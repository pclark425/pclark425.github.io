<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4209 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4209</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4209</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-280081994</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.07155v1.pdf" target="_blank">Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics</a></p>
                <p><strong>Paper Abstract:</strong> We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose.The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed. We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4\% accuracy. Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation. These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4209.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4209.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciRag RAG Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciRag Retrieval-Augmented Generation (RAG) Agent Suite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular RAG evaluation pipeline (SciRag) deployed across nine agent configurations to retrieve and synthesize information from a 5-paper cosmology corpus to answer factual, synthetic-reasoning, and analytical-interpretation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciRag RAG Agents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SciRag is a modular RAG implementation and benchmarking pipeline that (1) preprocesses a small document corpus (OCR or PDF), (2) indexes documents with vector stores or provider file-search, (3) retrieves top-k relevant chunks per query (top-k=20 by default), and (4) conditions high-performing LLM generators to produce concise answers. Implementations include commercial (OpenAI file search + GPT-4.1, VertexAI + Gemini), hybrid (ChromaDB with OpenAI/Gemini embeddings + Gemini generation), and academic (PaperQA2) configurations, all evaluated with temperature=0.01 and deterministic settings. Evaluation uses human expert labels and calibrated LLM-as-a-judge (LLMaaJ).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1, gemini-2.5 (various), OpenAI text-embedding-3-large, Google text-embedding-005, Gemini text-embedding-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>factual parameter extraction, empirical relationships and analytical interpretation (i.e., extraction of quantitative parameters, comparative model behaviour, and interpretation of simulation results)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples of QA tasks that require extracting quantitative information include: (i) impact of beam window functions on 2018 spectra in the Plik likelihood (factual parameter/impact extraction); (ii) which parameters and initial conditions are varied in CAMELS simulations and how they vary (explicit numeric/configuration extraction); (iii) comparing neural-network fits vs symbolic-regression equations for redshift evolution of cosmological quantities (analytical model comparison). No closed-form new physical laws are reported as discovered by SciRag in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Retrieval-augmented text-based extraction: semantic (and hybrid) retrieval of document chunks followed by LLM-conditioned answer generation; extraction relies on reading textual descriptions, tables, and cited numeric statements in the source papers (no automated equation parsing pipeline beyond retrieval and text summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human expert binary evaluation (945 answers; one domain expert) and calibrated LLM-as-a-Judge (LLMaaJ) validated against the human labels; concordance measured (Pearson r > 0.99 between evaluation methods).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy per agent (scaled 0-100). Example metrics: OpenAI best config 91.4% accuracy, OpenAIPDF/OpenAI range 89.5-91.4%, VertexAI 86.7%, HybridOAIGem 85.7%, HybridGemGem 84.8%, PaperQA2 81.9%, Gemini baseline 16.2%, Perplexity 17.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Top configuration correctness 91.4% (human-evaluated); other configurations ranged 16.2%–91.4% depending on architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The pipeline does not perform symbolic-equation extraction or automated discovery of new closed-form quantitative laws; extraction depends on quality of retrieval and summarization (summarization can dilute factual specifics), small corpus size (5 papers) may bias retrieval, and LLM hallucination/knowledge-cutoff issues remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against non-RAG baselines (Gemini, Perplexity) and academic RAG (PaperQA2); SciRag commercial RAGs outperformed baselines (e.g., OpenAI 91.4% vs Gemini 16.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4209.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4209.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2 Retrieval-Augmented Generative Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An academic RAG system that integrates retrieval, summarization, and GPT-4.1-based generation to answer scientific literature questions, used here as a baseline RAG tool for cosmology QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrievalaugmented generative agent for scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PaperQA2 is an academic RAG pipeline that uses GPT-4.1 for search, summarization, and answer generation, configured with evidence retrieval k=30 (or k=10 in a modified domain-adapted variant) and a maximum number of citations per response; it processes OCR-enhanced documents and returns summarized answers with citations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>originally general scientific domains (used in biology benchmarks); here applied to cosmology literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 (applied in this study's evaluation corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>knowledge synthesis and factual parameter extraction from text (not explicit symbolic-law extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>No explicit equations or new quantitative laws are reported; PaperQA2 is used to extract factual claims and synthesize content (achieved 81.9% human-evaluated accuracy in this benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Semantic retrieval of document chunks, summarization, and LLM generation conditioned on retrieved evidence (k=30 default; modified variant used k=10).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human expert binary evaluation and LLM-as-a-Judge calibration performed as part of the study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy: PaperQA2 81.9% (modified PaperQA2 reported 73.3% in some evaluations noted).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Approximately 81.9% correct across the 105-question benchmark (human-evaluated) for the standard PaperQA2 configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors note summarization steps in PaperQA2 may dilute specific factual information critical for precise scientific QA, causing lower performance relative to commercial RAG agents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to commercial (OpenAI, VertexAI) and hybrid RAG configurations; PaperQA2 underperforms top commercial systems by ~4.8–9.5 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4209.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4209.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Regression Equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodological approach to derive compact analytic expressions (equations) that model relationships in data; mentioned as an alternative to neural nets for modelling evolution of cosmological quantities with redshift in CAMELS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic regression (method)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Symbolic regression attempts to fit data with analytic mathematical expressions (equations) discovered by search/optimization procedures; in this paper it appears as a candidate model class (symbolic regression equations) to compare against neural networks for modelling cosmological quantity evolution with redshift.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cosmology / computational astrophysics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>mathematical equations / analytic relationships (symbolic laws) describing dependence of cosmological quantities on redshift</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Mentioned conceptually as a modeling approach (i.e., deriving equations from simulation or observational data) rather than an implemented extraction pipeline in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not implemented or validated in this paper; appears as an analytical-interpretation question type in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No experimental details provided here; symbolic regression is only referenced as a model comparison target and not executed within the study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Framed for comparison against neural networks in a benchmark question, but no empirical comparison results are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4209.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4209.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Hypothesis Generation (Ciucâ et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context/adversarial prompting for hypothesis generation (Ciucâ et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work showing LLMs, via in-context learning and adversarial prompting, can synthesize diverse astronomical information into coherent and innovative hypotheses, i.e., generate potential scientific relationships or patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMs with in-context learning + adversarial prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The referenced approach uses in-context learning and adversarial prompting strategies to steer LLMs to synthesize information from astronomy literature and generate novel hypotheses; in this paper it is cited as evidence that LLMs can produce coherent and innovative scientific hypotheses from text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astronomy / astrophysics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>hypothesis generation / pattern discovery (qualitative and potentially quantitative hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompting strategies (in-context examples and adversarial prompts) to induce LLMs to synthesize literature into hypotheses; emphasis on text synthesis rather than automated equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>The paper is cited as prior art; specific validation protocols are not detailed in this current RAG-evaluation paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited in the current work alongside general concerns about hallucination and evaluation scalability; specific limitations of the hypothesis-generation approach are not recapitulated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4209.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4209.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior project/paper proposing automated open-ended scientific discovery systems that aim to autonomously generate hypotheses and discoveries from data and literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (automated discovery framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as prior work towards fully automated, open-ended scientific discovery; the referenced work aims to build agents that autonomously generate hypotheses and perform discovery, but this RAG-evaluation paper only cites it in related work and does not implement or evaluate it.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific discovery / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>automated discovery of hypotheses / potentially quantitative relationships (as per the referenced work's goals)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned in context of prior efforts; the present paper highlights broader field limitations (hallucination, evaluation scalability) but does not provide details on the AI Scientist's empirical capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4209.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4209.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific RAG framework for astronomy that uses query expansion, reranking, and domain weighting to improve retrieval and support knowledge discovery in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pathfinder is described as a semantic literature-review framework implementing query expansion, reranking, and domain-specific weighting to enhance retrieval performance for astronomy literature; cited as an example of specialized retrieval infrastructure supporting knowledge discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astronomy / astrophysics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>facilitates discovery of relationships/patterns by improving retrieval (not an equation-extraction system per se)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Improved retrieval methods (query expansion, reranking, weighting) to surface relevant passages that could contain quantitative relationships; not described here as performing symbolic-law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as complementary work focusing on retrieval quality rather than automatic equation or law synthesis; details are in the cited work rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Paperqa: Retrievalaugmented generative agent for scientific research <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge <em>(Rating: 2)</em></li>
                <li>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy <em>(Rating: 2)</em></li>
                <li>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4209",
    "paper_id": "paper-280081994",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "SciRag RAG Agents",
            "name_full": "SciRag Retrieval-Augmented Generation (RAG) Agent Suite",
            "brief_description": "A modular RAG evaluation pipeline (SciRag) deployed across nine agent configurations to retrieve and synthesize information from a 5-paper cosmology corpus to answer factual, synthetic-reasoning, and analytical-interpretation questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciRag RAG Agents",
            "system_description": "SciRag is a modular RAG implementation and benchmarking pipeline that (1) preprocesses a small document corpus (OCR or PDF), (2) indexes documents with vector stores or provider file-search, (3) retrieves top-k relevant chunks per query (top-k=20 by default), and (4) conditions high-performing LLM generators to produce concise answers. Implementations include commercial (OpenAI file search + GPT-4.1, VertexAI + Gemini), hybrid (ChromaDB with OpenAI/Gemini embeddings + Gemini generation), and academic (PaperQA2) configurations, all evaluated with temperature=0.01 and deterministic settings. Evaluation uses human expert labels and calibrated LLM-as-a-judge (LLMaaJ).",
            "model_name": "GPT-4.1, gemini-2.5 (various), OpenAI text-embedding-3-large, Google text-embedding-005, Gemini text-embedding-001",
            "model_size": null,
            "scientific_domain": "astrophysics / cosmology",
            "number_of_papers": "5",
            "law_type": "factual parameter extraction, empirical relationships and analytical interpretation (i.e., extraction of quantitative parameters, comparative model behaviour, and interpretation of simulation results)",
            "law_examples": "Examples of QA tasks that require extracting quantitative information include: (i) impact of beam window functions on 2018 spectra in the Plik likelihood (factual parameter/impact extraction); (ii) which parameters and initial conditions are varied in CAMELS simulations and how they vary (explicit numeric/configuration extraction); (iii) comparing neural-network fits vs symbolic-regression equations for redshift evolution of cosmological quantities (analytical model comparison). No closed-form new physical laws are reported as discovered by SciRag in this paper.",
            "extraction_method": "Retrieval-augmented text-based extraction: semantic (and hybrid) retrieval of document chunks followed by LLM-conditioned answer generation; extraction relies on reading textual descriptions, tables, and cited numeric statements in the source papers (no automated equation parsing pipeline beyond retrieval and text summarization).",
            "validation_approach": "Human expert binary evaluation (945 answers; one domain expert) and calibrated LLM-as-a-Judge (LLMaaJ) validated against the human labels; concordance measured (Pearson r &gt; 0.99 between evaluation methods).",
            "performance_metrics": "Human-evaluated accuracy per agent (scaled 0-100). Example metrics: OpenAI best config 91.4% accuracy, OpenAIPDF/OpenAI range 89.5-91.4%, VertexAI 86.7%, HybridOAIGem 85.7%, HybridGemGem 84.8%, PaperQA2 81.9%, Gemini baseline 16.2%, Perplexity 17.1%.",
            "success_rate": "Top configuration correctness 91.4% (human-evaluated); other configurations ranged 16.2%–91.4% depending on architecture.",
            "challenges_limitations": "The pipeline does not perform symbolic-equation extraction or automated discovery of new closed-form quantitative laws; extraction depends on quality of retrieval and summarization (summarization can dilute factual specifics), small corpus size (5 papers) may bias retrieval, and LLM hallucination/knowledge-cutoff issues remain concerns.",
            "comparison_baseline": "Compared against non-RAG baselines (Gemini, Perplexity) and academic RAG (PaperQA2); SciRag commercial RAGs outperformed baselines (e.g., OpenAI 91.4% vs Gemini 16.2%).",
            "uuid": "e4209.0",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2 Retrieval-Augmented Generative Agent",
            "brief_description": "An academic RAG system that integrates retrieval, summarization, and GPT-4.1-based generation to answer scientific literature questions, used here as a baseline RAG tool for cosmology QA.",
            "citation_title": "Paperqa: Retrievalaugmented generative agent for scientific research",
            "mention_or_use": "use",
            "system_name": "PaperQA2",
            "system_description": "PaperQA2 is an academic RAG pipeline that uses GPT-4.1 for search, summarization, and answer generation, configured with evidence retrieval k=30 (or k=10 in a modified domain-adapted variant) and a maximum number of citations per response; it processes OCR-enhanced documents and returns summarized answers with citations.",
            "model_name": "GPT-4.1",
            "model_size": null,
            "scientific_domain": "originally general scientific domains (used in biology benchmarks); here applied to cosmology literature",
            "number_of_papers": "5 (applied in this study's evaluation corpus)",
            "law_type": "knowledge synthesis and factual parameter extraction from text (not explicit symbolic-law extraction)",
            "law_examples": "No explicit equations or new quantitative laws are reported; PaperQA2 is used to extract factual claims and synthesize content (achieved 81.9% human-evaluated accuracy in this benchmark).",
            "extraction_method": "Semantic retrieval of document chunks, summarization, and LLM generation conditioned on retrieved evidence (k=30 default; modified variant used k=10).",
            "validation_approach": "Human expert binary evaluation and LLM-as-a-Judge calibration performed as part of the study.",
            "performance_metrics": "Human-evaluated accuracy: PaperQA2 81.9% (modified PaperQA2 reported 73.3% in some evaluations noted).",
            "success_rate": "Approximately 81.9% correct across the 105-question benchmark (human-evaluated) for the standard PaperQA2 configuration.",
            "challenges_limitations": "Authors note summarization steps in PaperQA2 may dilute specific factual information critical for precise scientific QA, causing lower performance relative to commercial RAG agents.",
            "comparison_baseline": "Compared to commercial (OpenAI, VertexAI) and hybrid RAG configurations; PaperQA2 underperforms top commercial systems by ~4.8–9.5 percentage points.",
            "uuid": "e4209.1",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Symbolic Regression",
            "name_full": "Symbolic Regression Equations",
            "brief_description": "A methodological approach to derive compact analytic expressions (equations) that model relationships in data; mentioned as an alternative to neural nets for modelling evolution of cosmological quantities with redshift in CAMELS.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Symbolic regression (method)",
            "system_description": "Symbolic regression attempts to fit data with analytic mathematical expressions (equations) discovered by search/optimization procedures; in this paper it appears as a candidate model class (symbolic regression equations) to compare against neural networks for modelling cosmological quantity evolution with redshift.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "cosmology / computational astrophysics",
            "number_of_papers": null,
            "law_type": "mathematical equations / analytic relationships (symbolic laws) describing dependence of cosmological quantities on redshift",
            "law_examples": null,
            "extraction_method": "Mentioned conceptually as a modeling approach (i.e., deriving equations from simulation or observational data) rather than an implemented extraction pipeline in this work.",
            "validation_approach": "Not implemented or validated in this paper; appears as an analytical-interpretation question type in the benchmark.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "No experimental details provided here; symbolic regression is only referenced as a model comparison target and not executed within the study.",
            "comparison_baseline": "Framed for comparison against neural networks in a benchmark question, but no empirical comparison results are provided in this paper.",
            "uuid": "e4209.2",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "LLM Hypothesis Generation (Ciucâ et al.)",
            "name_full": "In-context/adversarial prompting for hypothesis generation (Ciucâ et al., 2023)",
            "brief_description": "Prior work showing LLMs, via in-context learning and adversarial prompting, can synthesize diverse astronomical information into coherent and innovative hypotheses, i.e., generate potential scientific relationships or patterns.",
            "citation_title": "Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy",
            "mention_or_use": "mention",
            "system_name": "LLMs with in-context learning + adversarial prompting",
            "system_description": "The referenced approach uses in-context learning and adversarial prompting strategies to steer LLMs to synthesize information from astronomy literature and generate novel hypotheses; in this paper it is cited as evidence that LLMs can produce coherent and innovative scientific hypotheses from text.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "astronomy / astrophysics",
            "number_of_papers": null,
            "law_type": "hypothesis generation / pattern discovery (qualitative and potentially quantitative hypotheses)",
            "law_examples": null,
            "extraction_method": "Prompting strategies (in-context examples and adversarial prompts) to induce LLMs to synthesize literature into hypotheses; emphasis on text synthesis rather than automated equation discovery.",
            "validation_approach": "The paper is cited as prior art; specific validation protocols are not detailed in this current RAG-evaluation paper.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited in the current work alongside general concerns about hallucination and evaluation scalability; specific limitations of the hypothesis-generation approach are not recapitulated here.",
            "comparison_baseline": null,
            "uuid": "e4209.3",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "The AI Scientist",
            "name_full": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "A referenced prior project/paper proposing automated open-ended scientific discovery systems that aim to autonomously generate hypotheses and discoveries from data and literature.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (automated discovery framework)",
            "system_description": "Cited as prior work towards fully automated, open-ended scientific discovery; the referenced work aims to build agents that autonomously generate hypotheses and perform discovery, but this RAG-evaluation paper only cites it in related work and does not implement or evaluate it.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general scientific discovery / machine learning",
            "number_of_papers": null,
            "law_type": "automated discovery of hypotheses / potentially quantitative relationships (as per the referenced work's goals)",
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Mentioned in context of prior efforts; the present paper highlights broader field limitations (hallucination, evaluation scalability) but does not provide details on the AI Scientist's empirical capabilities.",
            "comparison_baseline": null,
            "uuid": "e4209.4",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Pathfinder",
            "name_full": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy",
            "brief_description": "A domain-specific RAG framework for astronomy that uses query expansion, reranking, and domain weighting to improve retrieval and support knowledge discovery in literature.",
            "citation_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy",
            "mention_or_use": "mention",
            "system_name": "Pathfinder",
            "system_description": "Pathfinder is described as a semantic literature-review framework implementing query expansion, reranking, and domain-specific weighting to enhance retrieval performance for astronomy literature; cited as an example of specialized retrieval infrastructure supporting knowledge discovery.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "astronomy / astrophysics",
            "number_of_papers": null,
            "law_type": "facilitates discovery of relationships/patterns by improving retrieval (not an equation-extraction system per se)",
            "law_examples": null,
            "extraction_method": "Improved retrieval methods (query expansion, reranking, weighting) to surface relevant passages that could contain quantitative relationships; not described here as performing symbolic-law extraction.",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Mentioned as complementary work focusing on retrieval quality rather than automatic equation or law synthesis; details are in the cited work rather than this paper.",
            "comparison_baseline": null,
            "uuid": "e4209.5",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Paperqa: Retrievalaugmented generative agent for scientific research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy",
            "rating": 2,
            "sanitized_title": "harnessing_the_power_of_adversarial_prompting_and_large_language_models_for_robust_hypothesis_generation_in_astronomy"
        },
        {
            "paper_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy",
            "rating": 2,
            "sanitized_title": "pathfinder_a_semantic_framework_for_literature_review_and_knowledge_discovery_in_astronomy"
        }
    ],
    "cost": 0.014877749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics
9 Jul 2025</p>
<p>Xueqing Xu 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Boris Bolliet 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Adrian Dimitrov 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Andrew Laverick 
Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Francisco Villaescusa-Navarro 
Center for Computational Astrophysics
Flatiron Institute
New YorkNYUSA</p>
<p>Department of Astrophysical Sciences
Prince-ton University
PrincetonNJUSA</p>
<p>Licong Xu 
Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Institute of Astronomy
Uni-versity of Cambridge
CambridgeUnited Kingdom</p>
<p>Í Ñigo Zubeldia 
Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Institute of Astronomy
Uni-versity of Cambridge
CambridgeUnited Kingdom</p>
<p>Boris Bolliet</p>
<p>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics
9 Jul 2025712A7A925B42A9FA722DBEAE48F31A3FarXiv:2507.07155v1[astro-ph.IM]
We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose. 1The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed.We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4% accuracy.Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation.These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent 2 presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs.We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community. 3</p>
<p>Introduction</p>
<p>The rapid advancements of Large Language Models (LLMs) (Liu et al., 2024;Bai et al., 2025) have opened a new era in automated scientific discovery, where AI systems can conduct independent research and generate scientific insights (Lu et al., 2024).In cosmology, automated discovery systems are required to synthesize knowledge across collections of scientific literature, computational models, and observational datasets.The successful implementations require AI infrastructure capable of interacting with the knowledge ecosystem utilized by domain experts, and a specialized computational framework that constitutes the methodological foundation.In this work, we focus on the knowledge integration aspect of automated scientific discovery, specifically targeting the information overload in modern astronomy.</p>
<p>While LLMs have demonstrated impressive capabilities in scientific text analysis (Zhang et al., 2024), their deployment in critical research scenarios remains constricted (Fouesneau et al., 2024), by hallucination (Huang et al., 2025) and knowledge cut-off (Cheng et al., 2024).Retrieval-Augmented Generation (RAG) has emerged as a powerful tool to enhance LLMs' performance with external knowledge (Lewis et al., 2021) to meet scientific accuracy standards.The efficacy of this approach has been demonstrated in biology, where PaperQA2 RAG Agents (Lála et al., 2023;Skarlinski et al., 2024) achieve superhuman performance on LitQA2 (Futurehouse, 2024), a benchmark designed to evaluate knowledge synthesis in real research scenarios.Despite these successes in biology, systematic evaluation of RAG agents in astronomy remains limited by the lack of standardized benchmarks.As annotated by Bowman et al. (Bowman et al., 2015), developing human-annotated benchmarks for doctoral-level scientific research domains remains economically prohibitive.Consequently, evaluation of RAG agents in astronomy is constrained by the absence of authentic evaluation datasets that capture the complexity of real research scenarios.</p>
<p>To address these challenges, we introduce CosmoPaperQA, a high-quality benchmark dataset including 105 expert-curated question-answer pairs derived from five highly-cited cosmological literature.Unlike synthetic benchmarks, Cos-moPaperQA captures authentic research scenarios by extracting questions directly from research papers.</p>
<p>To facilitate a comprehensive and reproducible evaluation of CosmoPaperQA, we develop SciRag, a modular framework designed for systematic integration and benchmarking of multiple RAG Agents for scientific discovery.Our implementation enables evaluation across commercial APIs (Ope-nAI Assistant, VertexAI Assistant), hybrid architectures (ChromaDB with several embedding models), specialized academic tools (PaperQA2), and search-enhanced systems (Perplexity), providing empirical guidance for optimal RAG configuration selection in scientific contexts.</p>
<p>Our systematic evaluation across SciRag implementations reveals significant performance differences across four configuration categories, with commercial solutions (OpenAI Assistant: 89.5-91.4%,VertexAI Assistant: 86.7%) achieving the highest accuracy on CosmoPaperQA.Hybrid architectures (HybridOAIGem: 85.7%, HybridGemGem: 84.8%) show competitive performance while significantly reducing operational costs.Academic tools PaperQA2 (81.9%) show solid performance but lag behind commercial and hybrid SciRag Agents, while baseline approaches (Gemini Assistant: 16.2%, Perplexity Assistant: 17.1%) prove insufficient for expert-level scientific inquiry.</p>
<p>We present four primary contributions that collectively advance the state of RAG evaluation in cosmology:</p>
<p>Benchmark Development: We introduce CosmoPaperQA, a comprehensive benchmark dataset containing 105 expertvalidated question-answer pairs.Implementation Pipeline: We develop SciRag, a modular framework that enables systematic deployment and reproducible comparison of diverse RAG solutions.</p>
<p>Multi-System RAG Performance Analysis: We conduct a systematic evaluation of nine distinct RAG implementations utilizing high-performing LLMs and embedding models, revealing significant performance variations across different system architectures and cost-efficiency trade-offs for scientific applications.</p>
<p>Calibrated AI Judge Evaluation:</p>
<p>We introduce a LLMas-a-Judge (LLMaaJ) system that matches human expert assessment in astronomy, enabling scalable performance evaluation while maintaining the quality standards required for scientific applications.</p>
<p>Related Work</p>
<p>RAG Agents in Cosmology</p>
<p>Recent work has demonstrated the significant potential of LLMs in astronomical research contexts.Ciucȃ et al. (Ciucȃ et al., 2023) showed that through in-context learning and adversarial prompting, LLMs can synthesize diverse astronomical information into coherent and innovative hypotheses, while Shao et al. (Shao et al., 2024) demonstrated their effectiveness in extracting specialized knowledge entities from astrophysics journals using carefully designed prompting strategies.These capabilities have motivated the development of specialized RAG frameworks for astronomy, such as the pathfinder system by Iyer et al. (Iyer et al., 2024), which implements query expansion, reranking, and domain-specific weighting schemes to enhance retrieval performance in scientific applications.</p>
<p>However, the growing deployment of RAG systems in astronomy has highlighted the critical need for systematic evaluation methodologies.Wu et al. (Wu et al., 2024) addressed this challenge by proposing a dynamic evaluation framework using a Slack-based chatbot that retrieves information from arXiv astro-ph papers, emphasizing the importance of real-world user interactions over static benchmarks.While their approach provides valuable insights into user behavior and system usability, it relies on user feedback and reaction data rather than systematic performance assessment against validated ground-truth, highlighting a complementary need for standardized benchamrks that can provide consistent, reproducible evaluation metrics across different RAG implementations.</p>
<p>Benchmarks and Evaluation in Cosmology</p>
<p>Existing evaluation falls into two categories, each with some limitations:</p>
<p>Astronomy-Specific Knowledge Benchmarks: AstroM-Lab 1 (Ting et al., 2024) provides the first comprehensive astronomy-specific evaluation with 4425 AI-generated multiple-choice questions from Annual Review articles.While demonstrating significant performance variations between models with specialized astronomical knowledge, its multiple-choice format and automated question generation limit evaluation to content mastery rather than scientific inquiry workflows.Similarly, Astro-QA (Li et al., 2025) provides a structured evaluation with 3082 questions spanning diverse astronomical topics, demonstrating the application of LLMaaJ evaluation in astronomical contexts.However, its synthetic questions limit its ability to assess the complex, open-ended reasoning required for an authentic scientific research workflow.</p>
<p>General Scientific Evaluation: Broader scientific benchmarks like LitQA2 (Futurehouse, 2024) (Zhong et al., 2025), ScisummNet (Yasunaga et al., 2019) are designed for other scientific domains and may not capture astronomy-specific challenges such as mathematical reasoning about cosmological models, and interpretation of observational constraints.</p>
<p>Methodology</p>
<p>To enable AI systems to interact effectively with domain experts' knowledge bases in astrophysics, we present a comprehensive framework consisting of four integrated components designed to systematically evaluate RAG Agents.</p>
<p>CosmoPaperQA: Benchmark for Authentic Research Scenarios</p>
<p>To address the evaluation challenges identified in the previous section, we manually construct CosmoPaperQA.</p>
<p>We systematically selected five highly influential papers spanning critical areas of modern cosmology: the Planck 2018 cosmological parameters (Aghanim et al., 2020), CAMELS machine learning simulations (Villaescusa-Navarro et al., 2021;2022), local Hubble constant measurements (Riess et al., 2016), and recent Atacama Cosmology Telescope constraints (Calabrese et al., 2025).This curation ensures comprehensive coverage of observational, theoretical, and computational aspects of modern cosmological research.</p>
<p>A team of expert cosmologists generated 105 questionanswer pairs through a rigorous protocol designed to mir-ror research inquiries.The questions in our dataset span multiple complexity levels: (1) factual retrieval requiring specific parameter extraction, (2) synthetic reasoning requiring integration across multiple evidence sources, and (3) analytical interpretation requiring deep domain knowledge.</p>
<p>Each pair underwent expert validation to ensure scientific accuracy and representativeness of real research scenarios, distinguishing our benchmark from synthetic alternatives that lack authentic complexity.</p>
<p>Hence, CosmoPaperQA is designed for the following evaluations: zero-shot learning, answering without prior training on specific question types; open-ended questions, mirroring research scenarios; and multi-source knowledge synthesis, requiring integration across observational, theoretical, and computational domains.</p>
<p>SciRag: RAG Implementation Pipeline</p>
<p>Our preprocessing pipeline addresses the requirements of astronomical literature through multi-stage processing.Optical character recognition (OCR) integration using Mistral's advanced capabilities (Mistral AI, 2025)  All RAG systems perform retrieval over the complete corpus of 5 papers, regardless of which paper a specific question was derived from.This design tests the system's ability to identify and retrieve relevant information from the correct source paper among multiple cosmological documents.</p>
<p>We evaluate nine RAG implementations spanning commercial APIs (OpenAI, VertexAI), hybrid architectures (Chro-maDB with OpenAI/Gemini embeddings), academic tools (PaperQA2), and search-enhanced systems (Perplexity).All systems use temperature=0.01 and top-k=20 for consistent evaluation.Detailed analysis is in Appendix A.</p>
<p>Dual Evaluation Framework: Human Expert and Calibrated AI Assessment</p>
<p>To evaluate the quality of RAG Agents' responses in cosmological research contexts, we compare generated answers against expert-validated ground-truth responses to determine whether core factual claims in generated responses align with ground-truth.</p>
<p>While a single domain expert would be the optimal evaluator for this evaluation task, human-expert evaluation faces critical scalability limitations that make it impractical to evaluate across multiple RAG Agents.To address this scalability challenge, we implement a calibrated LLMaaJ system for automated response evaluation.However, we maintain scientific rigor by conducting parallel human expert evaluations on our benchmark results to validate the AI judges' performance and ensure assessment quality.Detailed evaluation setup is in Appendix B. After obtaining the scores, we scaled them to 0-100 for comparison between different system configurations.</p>
<p>Results</p>
<p>Human Evaluated Results</p>
<p>From the expert-evaluated results, we observe that the topperforming ones (OpenAIPDF, OpenAI, VertexAI) are all commercial RAGs, achieving 86.7-91.4% accuracy.Both hybrid implementations (HybridOAIGem: 85.7% , Hy-bridGemGem: 84.8% ) achieve performance competitive with commercial RAGs.PaperQA2 (81.90%) demonstrates solid performance but lags by 4.8-9.5 % compared to top performers.The poor performance of Perplexity Assistant (17.1%) and Gemini Assistant (16.2%) shows that unfiltered web search and non-RAG integration are insufficient for expert-level scientific inquiry, reinforcing the essential role of RAG Agents in scientific knowledge synthesis for autonomous scientific discovery.These clear performance distinctions between different system architectures validate CosmoPaperQA as an effective benchmark for distinguish-ing RAG agents' capabilities in authentic scientific research scenarios.</p>
<p>AI Evaluated Results</p>
<p>Evaluation Concordance: Both OpenAI and Gemini judges preserve the performance ranking observed in human evaluation.The performance gaps are preserved: baseline systems achieve 11.4-18.1% (OpenAI judge) and 16.2-31.4%(Gemini judge), while top-performing agents reach 80.0-84.8%(OpenAI judge) and 88.6-91.4% (Gemini judge).</p>
<p>Judge-Specific Patterns: The OpenAI judge demonstrates conservative scoring, consistently rating systems 2-8% lower than human experts across all categories.In contrast, the Gemini judge exhibits systematic overrating, scoring systems 5-15 percentage points higher than human evaluation (e.g., Gemini Baseline: 27.6% vs Human: 16.2%, Modified PaperQA2: 81.9% vs Human: 73.3%).This overrating pattern suggests that Gemini judge may be overly optimistic in assessing scientific accuracy.</p>
<p>For researchers seeking robust performance estimates, the OpenAI judge's conservative scoring provides a safer lower bound for system capabilities, while Gemini's optimistic scoring may overestimate real-world performance.Despite these systematic biases, the consistent ranking order across all three evaluation methods (Pearson r &gt; 0.99) demonstrates the robustness of our assessment framework.Ver-texAI demonstrates superior cost-efficiency while maintaining strong performance, while OpenAI achieves highest accuracy at a greater operational cost.Detailed cost analysis is provided in Appendix E.</p>
<p>Discussion and Future Work</p>
<p>While CosmoPaperQA represents a first step in systematic astronomical RAG evaluation, several design choices warrant discussion.Many questions explicitly reference their source papers (e.g., Cosmology From One Galaxy?questions mention the paper title, others reference Planck 2018 or ACT DR6).This was intentionally adopted to ensure clear answer provenance and facilitate rigorous evaluation.However, researchers typically formulate queries around scientific concepts without specifying source documents, and our explicit references may systematically improve RAG performance by providing retrieval cues.</p>
<p>Additionally, our five-paper corpus, while enabling expert evaluation, is more constrained than typical research contexts where systems must search thousands of papers or use web search, likely leading to degraded retrieval performance due to increased noise and irrelevant content.Future iterations should incorporate naturalistic question formulations and progressively larger document collections to test sys-tems' ability to identify relevant sources without explicit guidance and understand how accuracy scales with corpus size.</p>
<p>Our results also reveal important insights into retrieval mechanisms that drive performance differences.OpenAI Assistants (89.5-91.4%)use OpenAI's file search tool, which combines automatic query rewriting, parallel searches, keyword and semantic search, and result reranking.This multi-faceted approach outperforms simple semantic-only retrieval used in hybrid systems (84.8-85.7%).Future work should evaluate domain-specific retrieval enhancements such as hybrid sparse-dense methods, contextual chunk expansion, query decomposition strategies, and multi-hop reasoning approaches to further optimize RAG performance for scientific applications.</p>
<p>The calibrated LLMaaJ evaluators developed in this work enable the next phase of our research: building AI questioner systems that can automatically generate domain-specific questions.Our current dataset of 945 human-evaluated responses provides a valuable training foundation for developing such automated question generation capabilities, potentially scaling evaluation to much larger document corpora.</p>
<p>The evaluation framework could be extended to other scientific domains such as chemistry, biology, or materials science to demonstrate generalizability.Despite these limitations, our framework provides a foundation for more comprehensive astronomical RAG benchmarks.</p>
<p>Conclusion</p>
<p>We have evaluated 9 agent configurations on 105 Cosmology Question-Answer (QA) pairs that were built specifically for this purpose, based on 5 carefully selected papers.The papers were selected for their impact on the field and the quality of the presentation of their results, and their relevance to the autonomous discovery systems that we are building, e.g., cmbagent, presented in a companion paper.</p>
<p>The 9 agent configurations were manually evaluated by a human expert with more than 10 years of experience in the field, that is, a total of 945 generated answers were assessed.We find that currently the best RAG agent configuration uses OpenAI embedding and generative models, achieving 91.4% accuracy.VertexAI (86.7%) and hybrid architectures (84.8-85.7%)demonstrate competitive performance.These configurations outperform academic tools uch as PaperQA2 (81.9%), (Lála et al., 2023;Skarlinski et al., 2024), which we attribute to the summarization steps in such systems that may dilute specific factual information critical for our evaluation tasks.Notably, online tools like Perplexity perform poorly (17.1%), showing essentially no advantage over frontier LLMs without RAG (16.2%), indicating that unfiltered web search is insufficient for expert-level scientific inquiry.</p>
<p>Using our human evaluation results, we are able to calibrate evaluator agents which can be used as robust proxy for human evaluation.These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics and provide us with AI evaluators that can be scaled to much larger evaluation datasets.By themselves, our 945 manually evaluated QA pairs constitute a precious dataset that can serve for the calibration of future AI evaluator agents.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Detailed System Configurations</p>
<p>All generation agents are configured with a temperature of 0.01 for consistent, deterministic responses, and top-k=20 (retrieving the 20 most similar document chunks per query) excluding Gemini Assistant, PaperQA2 (both versions) and Perplexity Assistant.The implementation provides both semantic search and hybrid retrieval capabilities across different backends, with specific configurations optimized for each system's strengths.Here are the configurations that we use for each assistant.</p>
<p>OpenAI Assistant: Direct implementation of OpenAI vector stores with file search tool (providing automatic query rewriting, parallel searches, keyword+semantic search, and result reranking) with text-embedding-3-large (OpenAI, 2023) for embeddings and GPT-4.1 for generation, with configurable retrieval parameters (similarity threshold=0.5).</p>
<p>OpenAIPDF Assistant: Direct PDF processing implementation without OCR preprocessing, enabling comparison of raw PDF handling versus OCR-enhanced document processing.Identical configuration to OpenAI Assistant, but operates on unprocessed PDF documents.</p>
<p>VertexAI Assistant: Google Cloud implementation using Google's text-embedding-005 for embeddings and gemini-2.5flash-preview-05-20(Google DeepMind, 2025) for generation.Creates RAG corpora through Vertex AI infrastructure with automatic document ingestion from Google Cloud Storage buckets.Supports semantic search with configurable similarity thresholds (0.5).</p>
<p>Gemini Assistant: Direct integration with Google's Gemini model gemini-2.5-flash-preview-05-20for baseline comparison without specialized RAG infrastructure.</p>
<p>HybridGemGem Assistant: Dual-Gemini implementation using Gemini's text-embedding-001 for embedding, leading embedding model on MTEB (Muennighoff et al., 2023) 4 with ChromaDB storage and gemini-2.5-flash-preview-05-20for generation.Supports ChromaDB backends with semantic-only search.</p>
<p>HybridOAIGem Assistant: Cross-platform architecture identical to HybridGemGem but specifically configured with OpenAI embeddings (text-embedding-3-large) and gemini-2.5-flash-preview-05-20,enabling comparison of embeddinggeneration combinations.</p>
<p>PaperQA2: Standard academic RAG implementation utilizing GPT-4.1 across all components (search, summarization, retrieval), evidence retrieval k=30, maximum 5 citations per response (optimal settings from original work).Processes OCR-enhanced documents with semantic-only search.</p>
<p>Modified PaperQA2: Domain-adapted version with identical technical configuration but specialized astronomical prompts and cosmological citation protocols.Uses evidence retrieval k=10 (reduced from standard k=30) for more focused responses.</p>
<p>Perplexity Assistant: Web-search enabled system using sonar-reasoning-pro model with real-time access to current literature.No local vector storage -relies entirely on web retrieval.</p>
<p>This diverse implementation suite enables comprehensive comparison across commercial, academic, and hybrid approaches, providing empirical guidance for selecting optimal RAG configurations for autonomous scientific discovery workflows.</p>
<p>B. Evaluation Setup</p>
<p>A domain expert is provided (1) a question query, (2) an ideal solution validated by experts, and (3) an RAG Agent-generated response.Then, evaluation is based on</p>
<p>Correct (1): Generated responses demonstrate factual accuracy, and capture essential scientific understanding equivalent to the ideal answer.</p>
<p>Incorrect (0): Generated responses contain errors, contradict established scientific knowledge, or fail to include all the core concepts of ideal answers.</p>
<p>After obtaining the scores, we scaled them to 0-100 for comparison between different system configurations.</p>
<p>The cosmologist who evaluated the response is a domain expert with a PhD-level degree currently working as a researcher in astronomy, astrophysics, or physics.Together with this cosmologist, we designed the evaluation criteria and pipeline to ensure alignment with authentic research standards.In total, our expert evaluated 945 responses (9 systems × 105 questions) generated by RAG Agents.</p>
<p>We explored LLM-as-a-Judge (LLMaaJ) (Gu et al., 2025;Zheng et al., 2023), an AI-based evaluation system calibrated for scientific research queries, using a binary scoring protocol aligned with human expert methodology.Our prompting experiments in Appendix D revealed that chain-of-thought, which asks models to formulate their underlying reasoning process, typically enhances evaluation accuracy and improves concordance with field expert judgments.</p>
<p>To investigate the bias of the pipeline specifically, as LLM evaluators may prefer responses generated by themselves (Dai et al., 2024), we used two LLM-as-a-Judge settings.Given that majority of generation systems utilize either OpenAI or Gemini-based agents, with the exception of the Perplexity Agent, we used the OpenAI o3 mini and Gemini gemini-2.5-propreview-06-05,reasoning models for evaluation.</p>
<p>Research Papers Document</p>
<p>Preprocessing</p>
<p>SciRag</p>
<p>CosmoPaperQA</p>
<p>Retrieval Generation</p>
<p>Retrieved Chunks</p>
<p>Factual Retrieval</p>
<p>How large is the impact of beam window functions on the 2018 spectra in the baseline Plik likelihood?</p>
<p>Synthetic Reasoning</p>
<p>What parameters and initial conditions are varied in the simulations that are run in CAMELS and how are they varied for each simulation?</p>
<p>Analytical Interpretation</p>
<p>Are the neural networks or the symbolic regression equations better at modelling the evolution of cosmological quantities with the redshift in the CAMELS results?</p>
<p>C. RAG Prompts</p>
<p>Our modified PaperQA2 prompt priorities conciseness and domain specificity for efficient human evaluation.</p>
<p>Perplexity Assistants Prompt</p>
<p>You are a scientific literature search agent specializing in cosmology.</p>
<p>We perform retrieval on the following set of papers: {paper list} Your task is to answer questions using ONLY information from these specific papers.CRITICAL: Your answer section must contain no more than 3 sentences total.Count your sentences carefully.You must search your knowledge base calling your tool.The sources must be from the retrieval only.Your response must be in JSON format with exactly these fields:</p>
<p>-"answer": Your 1-3 sentence response with citations -"sources": Array of citation numbers used (e.g., ["1", "2"]) Gemini Assistant's approach to leveraging pre-trained knowledge of specific cosmological papers without requiring external retrieval mechanisms.</p>
<p>Gemini Assistant Prompt</p>
<p>You are a scientific literature agent specializing in cosmology.</p>
<p>You have access to the following key cosmology papers in your knowledge base: {paper list} Your task is to answer cosmology questions using your knowledge of these papers and general cosmology knowledge.</p>
<p>D. CoT Prompts</p>
<p>AI judges are given the following prompt:</p>
<p>Judge Prompt</p>
<p>You are an expert scientific evaluator assessing the quality of scientific responses against reference answers.</p>
<p>Your task is to evaluate responses using one critical criterion: ACCURACY (0-100): CRITICAL: Use ONLY these two scores for accuracy:</p>
<p>-100: The answer contains the core correct factual content, concepts, and conclusions from the ideal answer -0: The answer is fundamentally wrong or contradicts the ideal answer This is a BINARY evaluation -either the answer is essentially correct (100) or fundamentally incorrect (0).No partial credit or intermediate scores allowed.</p>
<p>EVALUATION GUIDELINES:</p>
<p>-Focus ONLY on whether the main scientific concepts and conclusions are correct -Check that the core factual claims from the ideal answer are present in the generated answer -Verify the overall conceptual direction and main conclusions align -Additional correct information beyond the ideal answer is acceptable -Only award 0 if the answer contradicts the ideal answer or gets the main concepts wrong -Award 100 if the answer captures the essential correct scientific understanding Provide your evaluation with the numerical score and detailed rationale explaining why you chose 100 or 0.""" Please evaluate this system's response against the ideal answer: QUESTION: {question} GENERATED ANSWER: {generated answer} IDEAL ANSWER: {ideal answer} Evaluate based on: Accuracy (0-100): How factually correct is the answer compared to the ideal?Use the evaluate response function to provide your structured evaluation with detailed rationale.</p>
<p>E. Cost Performance Analysis</p>
<p>Cost considerations are critical for scientific research deployment, where institutions face budget constraints and researchers require sustainable access to AI-powered literature analysis tools.While our evaluation represents a controlled academic setting, understanding cost-performance trade-offs enables informed decisions for scaling RAG systems across research groups, institutions, and broader scientific communities.</p>
<p>Figure 2 .
2
Figure 2. SciRag System Architecture and CosmoPaperQA Benchmark Overview.Our framework integrates document preprocessing, retrieval mechanisms, and multi-provider generation to enable systematic evaluation of RAG Agents on astronomical literature.</p>
<p>, ChemRAG-Toolkit
100OpenAI Judge Gemini Judge Human Eval (Reference)81.983.781.980.085.784.883.790.585.778.185.786.780.088.689.584.890.591.47573.371.465.7Accuracy (%)5031.427.62518.116.217.111.40Gemini (Baseline)PerplexityModified PaperQA2PaperQA2HybridGemGem HybridOAIGemVertexAIOpenAIOpenAIPDFFigure 1. Performance comparison of SciRag Agents across three evaluation methods. Vertical dashed lines separate different configu-
ration categories: baseline systems (Gemini, Perplexity), academic RAG tools (Modified PaperQA2, PaperQA2), hybrid architectures (HybridGemGem, HybridOAIGem, VertexAI), and commercial solutions (OpenAI, OpenAIPDF).The first two entries (Gemini Baseline and Perplexity) do not perform RAG but simply rely on pre-trained LLM knowledge and, for Perplexity, built-in retrieval tools.</p>
<p>Do not use any other sources or general knowledge beyond what these papers contain.
Instructions:1. Search for information relevant to the question within the specified papers2. Provide a CONCISE answer in EXACTLY 1-3 sentences. Do not exceed 3 sentences under any circumstances.3. Add numerical references [1], [2], [3], etc. corresponding to the paper numbers listed above4. If the papers don't contain sufficient information, state this clearly in 1-2 sentences maximum5. Focus ONLY on the most important quantitative results or key findings6. Be precise, direct, and avoid any unnecessary elaboration or context</p>
<p>You are a retrieval agent.You must add precise source from where you got the answer.Your answer should be in markdown format with the following structure: <strong>Answer</strong>:{answer} <strong>Sources</strong>:{sources} You must search your knowledge base calling your tool.The sources must be from the retrieval only.You must report the source names in the sources field, if possible, the page number, equation number, table number, section number, etc.
OpenAI/Vertex Assistants PromptInstructions: 1. Answer the question based on your knowledge of cosmology and the listed papers2. Provide a CONCISE answer in EXACTLY 1-2 sentences maximum3. Add numerical references [1], [2], [3], etc. when citing the specific papers listed above4. Focus ONLY on the most important quantitative results or key findings5. Be precise, direct, and avoid any unnecessary elaborationPaper reference guide:[1] -Planck 2018 cosmological parameters[2] -CAMELS machine learning cosmology simulations[3] -Single galaxy cosmology analysis[4] -Local Hubble constant measurement (Riess et al.)[5] -Atacama Cosmology Telescope DR6 resultsCRITICAL: Your answer must be no more than 2 sentences total. Count your sentences carefully.Your response must be in JSON format with exactly these fields:-"answer": Your 1-2 sentence response with citations-"sources": Array of paper citations [1]-[5] that are relevant to your answerOpenAI/VertexAI assistants use a tool-based retrieval approach with markdown formatting, emphasising precise source andknowledge integration.
 Retrieved on 30-05-2025 <br />
AcknowledgmentsThe work of BB was partially funded by an unrestricted gift from Google, the Cambridge Centre for Data-Driven Discovery Accelerate Programme and the Infosys-Cambridge AI Centre.We are very grateful to the referees and panel of the ICML 2025 ML4ASTRO workshop for reviewing and accepting our work.Author ContributionsXX led the work and wrote the paper.BB led the work, supervised XX and AD, and provided the human evaluation for all the 945 answers.AD created the CosmoPaperQA benchmark dataset.AL, FVN, LX and IZ provided crucial input at various stages of this work.Modified PaperQA2 PromptProvide a concise answer in 1-2 sentences maximum.Context (with relevance scores):{context} Question: {question} Write a concise answer based on the context, focusing on astronomical facts and concepts.If the context provides insufficient information, reply {CANNOT ANSWER PHRASE}.Write in the style of a scientific astronomy reference, with precise and factual statements.The context comes from a variety of sources and is only a summary, so there may be inaccuracies or ambiguities.{prior answer prompt} Answer (maximum one sentence):In contrast, the original prompt emphasizes comprehensive information synthesis, mandatory citation and Wikipedia-style formatting.PaperQA2 PromptAnswer the question below with the context.Write in the style of a Wikipedia article, with concise sentences and coherent paragraphs.The context comes from a variety of sources and is only a summary, so there may inaccuracies or ambiguities.If quotes are present and relevant, use them in the answer.This answer will go directly onto Wikipedia, so do not add any extraneous information.{prior answer prompt} Answer ({answer length}):The Hybrid SciRag assistant adopt a structured approach, requiring a JSON format return for consistent response parsing.Hybrid Assistants PromptYou are a helpful assistant.Answer based on the provided context.You must respond in valid JSON format with the following structure: { "answer": "your detailed answer here", "sources": ["source1", "source2", "source3"]} The sources must be from the <strong>Context</strong> material provided.Include source names, page numbers, equation numbers, table numbers, section numbers when available.Ensure your response is valid JSON only.The Perplexity assistant uses web search to specific papers while utilizing its real-time retrieval capabilities.For our 105-question evaluation, total costs ranged from $0.037 (VertexAI) to $5.12 (GPT-4.1 based systems), representing a 137× cost difference.The cost differences reflect underlying model pricing structures: GPT-4.1 costs $0.002 per 1K input tokens and $0.008 per 1K output tokens, while Gemini 2.5 Flash charges $0.00015 per 1K input tokens and $0.0006 per 1K output tokens.For a typical research corpus of 1,000 papers with 10,000 queries, projected costs would range from $35.7 (VertexAI) to $4,880 (OpenAI systems).Hybrid approaches (HybridOAIGem: $0.003182, HybridGemGem: $0.003806) provide compelling cost-performance balance, achieving 84.8-85.7%accuracy while reducing costs by 93% compared to OpenAI systems.This positions them as practical solutions for resource-constrained research environments requiring both high accuracy and operational sustainability.Figure3synthesizes these trade-offs across performance, cost efficiency, and overall value.While OpenAI systems achieve highest accuracy (89.5-91.4%),their poor cost efficiency limits practical deployment scalability.Conversely, VertexAI maximizes value by combining strong performance with exceptional cost efficiency, making it suitable for widespread institutional adoption.
Planck2018 results: Vi. cosmological parameters. N Aghanim, 10.1051/0004-6361/201833910Astronomy and Astrophysics. 1432-0746641A6September 2020</p>
<p>S Bai, Qwen2.5-vl technical report. 2025</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalSeptember 2015Association for Computational Linguistics</p>
<p>The atacama cosmology telescope: Dr6 constraints on extended cosmological models. E Calabrese, 2025</p>
<p>Dated data: Tracing knowledge cutoffs in large language models. J Cheng, M Marone, O Weller, D Lawrie, D Khashabi, B V Durme, 2024</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. I Ciucȃ, Y.-S Ting, S Kruk, K Iyer, 2023</p>
<p>Neural retrievers are biased towards llm-generated content. S Dai, Y Zhou, L Pang, W Liu, X Hu, Y Liu, X Zhang, G Wang, J Xu, 10.1145/3637528.3671882Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USA20249798400704901Association for Computing Machinery</p>
<p>What is the role of large language models in the evolution of astronomy research?. M Fouesneau, 2024</p>
<p>Litqa2: A scientific literature question answering dataset. Futurehouse, 2024</p>
<p>. Google Deepmind, Gemini, </p>
<p>J Gu, X Jiang, Z Shi, H Tan, X Zhai, C Xu, W Li, Y Shen, S Ma, H Liu, S Wang, K Zhang, Y Wang, W Gao, L Ni, J Guo, A survey on llm-as-a-judge. 2025</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, 10.1145/3703155ACM Transactions on Information Systems. 1558-2868432January 2025</p>
<p>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy. K G Iyer, 10.3847/1538-4365/ad7c43The Astrophysical Journal Supplement Series. 1538-4365275238November 2024</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Tau Yih, T Rocktäschel, S Riedel, D Kiela, 2021</p>
<p>An astronomical question answering dataset for evaluating large language models. J Li, F Zhao, P Chen, 10.1038/s41597-025-04613-9Scientific Data. 122025</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, 2024</p>
<p>Paperqa: Retrievalaugmented generative agent for scientific research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, 2023</p>
<p>Mistral ocr: Introducing the world's best document understanding api. A I Mistral, 2025. June 6, 2025</p>
<p>MTEB: Massive text embedding benchmark. N Muennighoff, N Tazi, L Magne, N Reimers, Proceedings of the 17th Conference of the European Chapter. A Vlachos, I Augenstein, the 17th Conference of the European Chapterthe Association for Computational Linguistics</p>
<p>Association for Computational Linguistics. Croatia Dubrovnik, doi: 10.18653May 2023</p>
<p>URL. </p>
<p>New embedding models and api updates. Openai, 2023</p>
<p>A 2.4. A G Riess, L M Macri, S L Hoffmann, D Scolnic, S Casertano, A V Filippenko, B E Tucker, M J Reid, D O Jones, J M Silverman, R Chornock, P Challis, W Yuan, P J Brown, R J Foley, 10.3847/0004-637X/826/1/56The Astrophysical Journal. 1538-4357826156July 2016</p>
<p>Astronomical knowledge entity extraction in astrophysics journal articles via large language models. W Shao, P Ji, D Fan, Y Hu, X Yan, C Cui, L Mi, L Chen, R Zhang, 2024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S G Rodriques, A D White, 2024</p>
<p>Astromlab 1: Who wins astronomy jeopardy!?. Y.-S Ting, 2024</p>
<p>The camels project: Cosmology and astrophysics with machine-learning simulations. F Villaescusa-Navarro, 10.3847/1538-4357/abf7baThe Astrophysical Journal. 1538-4357915171July 2021</p>
<p>Cosmology with one galaxy?. F Villaescusa-Navarro, 10.3847/1538-4357/ac5d3fThe Astrophysical Journal. 1538-43579292132April 2022</p>
<p>Designing an evaluation framework for large language models in astronomy research. J F Wu, 2024</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. M Yasunaga, J Kasai, R Zhang, A R Fabbri, I Li, D Friedman, D R Radev, 2019</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 2023</p>
<p>Benchmarking retrieval-augmented generation for chemistry. X Zhong, B Jin, S Ouyang, Y Shen, Q Jin, Y Fang, Z Lu, J Han, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5017 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5017</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5017</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-385c74957858e7d6856d48e72b5a902b4c1aa28c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/385c74957858e7d6856d48e72b5a902b4c1aa28c" target="_blank">Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution.</p>
                <p><strong>Paper Abstract:</strong> Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5017.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5017.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that runs multiple LLM agents (two debaters in a 'tit-for-tat' exchange) plus a judge that adaptively decides when to stop and extracts the final answer; promotes divergent chain-of-thoughts by forcing contradicting viewpoints and external feedback among agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (as backbone agents and judge in reported MAD runs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-Turbo API model (version reported: gpt-3.5-turbo-0301), used as both debaters and/or judge in experiments; API-based transformer LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Two debaters (affirmative and negative) exchange arguments in multiple iterations in a 'tit-for-tat' manner; a separate judge evaluates after each iteration in discriminative/extractive modes and can adaptively stop the debate. Diversity arises from opposing viewpoints and iterative refutation rather than repeated same-chain outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Common MT (Commonsense Machine Translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A Chinese→English translation benchmark focusing on lexical and syntactic ambiguities that require commonsense to resolve (lexical/contextless/contextual subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With GPT-3.5-Turbo backbone + MAD: Lexical COMET 82.0, BLEURT 70.9, HUMAN 3.78; Contextless COMET 84.8, BLEURT 73.7, HUMAN 3.67; Contextual COMET 85.3, BLEURT 74.0, HUMAN 3.67 (as reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>GPT-3.5-Turbo baseline (no MAD): Lexical COMET 80.3, BLEURT 68.2, HUMAN 3.14; +Self-Reflect: Lexical COMET 81.0, BLEURT 69.1, HUMAN 3.43 (Table 1). GPT-4 baseline: Lexical COMET 82.0, BLEURT 70.1, HUMAN 3.41.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MAD (diverse debate-based reasoning) substantially increases textual diversity and reduces commonsense translation bias relative to self-reflection and baseline sampling approaches; GPT-3.5-Turbo with MAD achieves higher human-rated translation quality than GPT-3.5 baseline and matches/exceeds GPT-4 on some Common MT metrics, indicating divergent multi-agent debate can be more effective than iterative self-refinement or single-chain methods for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MAD does not always beat stronger single-model baselines on all tasks (e.g., GPT-4 performance on some reasoning tasks); increasing number of debaters beyond two degraded performance for current LLMs due to long-context handling limits; excessive enforced disagreement ('must disagree on every point') was not optimal; forcing more iterations (no adaptive break) can harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5017.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5017.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD-CIAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate on Counter-Intuitive Arithmetic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of MAD on the Counter-Intuitive AR dataset where shallow intuition is often wrong and multi-step reasoning is required; debate encourages agents to surface non-intuitive reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-Turbo-0301 used as debaters/judge in MAD runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Two-agent iterative debate with judge; opposing viewpoints produce alternative chains-of-thought, the judge adjudicates or extracts the final answer after an adaptive number of rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Counter-Intuitive AR (CIAR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A 200-question dataset of arithmetic/word problems designed to lure intuitive but incorrect answers; requires multi-step logical reasoning to overcome traps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3.5-Turbo + MAD: Accuracy 37.0% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>GPT-3.5-Turbo baseline: 26.0%; +CoT: 28.0%; +Self-Consistency: 29.5%; +Self-Reflect: 27.5%; GPT-4 baseline: 51.0% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MAD (diverse debate) yielded the largest improvement over other GPT-3.5-based methods on CIAR, indicating that explicitly divergent multi-agent argumentation helps overcome intuitive traps better than single-agent self-refinement or single-chain CoT; however, MAD still underperforms GPT-4 on this dataset, showing that method alone doesn't fully close model-capability gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MAD (37.0%) remained substantially below GPT-4 (51.0%) on CIAR, indicating that model capacity can dominate method benefits; MAD improvements are relative to weaker backbone models and do not guarantee surpassing stronger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5017.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5017.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Reflection / Iterative Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative single-agent self-evaluation and refinement: an LLM generates an answer, produces feedback on its own output, and iteratively refines until satisfied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (as evaluated baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5-Turbo-0301 used to perform iterative self-reflection in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Reflect (iterative self-refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>A single model repeatedly critiques and revises its own answer; because both critique and revision originate from the same model and internal priors, the process tends to produce similar (low-diversity) chains and can suffer from 'Degeneration-of-Thought' where initial confident but incorrect views persist.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Counter-Intuitive AR and Common MT (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CIAR: counter-intuitive arithmetic problems; Common MT: translation with commonsense ambiguities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CIAR with GPT-3.5-Turbo backbone: Accuracy 27.5% (+Self-Reflect, Table 3). On Common MT (Lexical) +Self-Reflect: COMET 81.0, BLEURT 69.1, HUMAN 3.43 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to GPT-3.5-Turbo baseline (CIAR 26.0%) Self-Reflect gives a marginal improvement; MAD gives a larger improvement (37.0%). For Common MT, MAD overtakes Self-Reflect on HUMAN scores (3.78 vs 3.43).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-reflection (similar, single-agent iterative refinement) gives only marginal improvements and is prone to the Degeneration-of-Thought (DoT) effect, where the model sticks to its initial (possibly incorrect) stance and fails to generate truly novel corrective reasoning; multi-agent debate (diverse) mitigates DoT and yields larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-Reflect sometimes improves slightly over baseline but generally underperforms diverse approaches like MAD and sampling-based self-consistency for the evaluated tasks; Self-Reflect had lower diversity (Self-BLEU-derived diversity 19.3) and higher bias (29.0) versus MAD (diversity 49.7, bias 24.8) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5017.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5017.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (zero-shot CoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits step-by-step intermediate reasoning by appending triggers like 'Let's think step by step' to problems to get explicit chains-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-Turbo used with zero-shot CoT trigger to produce stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-agent generation of an explicit chain-of-thought (stepwise reasoning trace) produced deterministically by a prompt; similarity arises because the method produces a single principal reasoning path for a given prompt and decoding setting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Counter-Intuitive AR (CIAR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Counter-intuitive arithmetic reasoning benchmark requiring multi-step logical thinking to avoid intuitive traps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3.5-Turbo + CoT: Accuracy 28.0% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MAD (GPT-3.5-Turbo backbone): 37.0%; Self-Consistency: 29.5%; baseline GPT-3.5-Turbo: 26.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT provides modest improvements over the baseline but is outperformed by diverse methods (MAD) and sampling-ensemble methods (Self-Consistency) on CIAR; explicit single-chain prompts are insufficient to escape intuitive traps in many examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT alone improved accuracy only slightly (28% vs baseline 26%) and did not match performance of multi-path/diverse techniques like MAD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5017.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5017.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over sampled CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling-based ensemble over multiple chain-of-thought outputs, selecting the most frequent final answer by majority vote to increase robustness of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-Turbo used to sample multiple CoT responses and aggregate via majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple sampled chain-of-thought outputs (stochastic decoding) and take majority or consensus over final answers; diversity arises from sampling multiple reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Counter-Intuitive AR (CIAR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dataset of counter-intuitive arithmetic problems requiring careful multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3.5-Turbo + Self-Consistency: Accuracy 29.5% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MAD: 37.0%; CoT single-chain: 28.0%; baseline GPT-3.5-Turbo: 26.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling multiple chains and ensembling (Self-Consistency) improves over single-chain CoT and single-run baselines, but MAD's structured divergent debate produced larger gains on CIAR for the GPT-3.5 backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although Self-Consistency helps, it still fell short of MAD in the reported CIAR experiments, indicating that sampled chains aggregated by voting may be less effective than explicit adversarial debate for discovering non-intuitive reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5017.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5017.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rerank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rerank (QE-based selection from sampled translations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sample multiple translation candidates and select the best one using an external quality-estimation model (COMET-QE HUMANr in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-Turbo used to sample 4 translations; an external QE model (wmt21-comet-qe-da) ranks candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Rerank (sampling + external QE)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple (diverse) outputs and use an external quality estimation model to pick the best candidate, introducing diversity via sampling and an external evaluator to choose among alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Common MT (Commonsense Machine Translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chinese→English translation benchmark focusing on commonsense/ambiguity resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3.5-Turbo + Rerank (Table 1): Lexical COMET 80.9, BLEURT 68.6, HUMAN 3.16; Contextless COMET 84.5, BLEURT 73.2, HUMAN 3.46; Contextual COMET 85.3, BLEURT 73.9, HUMAN 3.58.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>GPT-3.5-Turbo baseline (no rerank): Lexical COMET 80.3, BLEURT 68.2, HUMAN 3.14; +MAD: Lexical HUMAN 3.78 (higher) and BLEURT 70.9 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reranking over sampled translations provides modest improvements over the baseline but is outperformed by MAD's debate-based divergence in human judgments for commonsense translation.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Rerank improved some automatic metrics but did not match MAD's gains on human evaluations; external QE selection can help but may not capture nuanced commonsense choices that MAD's adversarial exchange surfaced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5017.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5017.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAPS (Analyze before Translate - CoT-like for translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that instructs the model to analyze the source sentence's ambiguous parts before producing a translation, effectively a task-specific chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-Turbo used with MAPS-style prompts that ask the model to analyze then translate (chain-of-thought applied to translation).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>MAPS (analyze-before-translate chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Encourage internal stepwise analysis of ambiguities before generating translation; similarity arises because it’s a single-agent structured reasoning path, akin to CoT applied to translation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Common MT (Commonsense Machine Translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chinese→English translations with lexical and syntactic ambiguities requiring commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3.5-Turbo + MAPS (Table 1): Lexical COMET 81.9, BLEURT 70.1, HUMAN 3.43; Contextless COMET 84.2, BLEURT 73.5, HUMAN 3.45; Contextual COMET 85.2, BLEURT 74.0, HUMAN 3.56.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MAD: Lexical HUMAN 3.78 and BLEURT 70.9; MAPS has improvements over baseline but generally slightly below MAD on human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MAPS (single-agent analysis before translate) improves translation quality versus naive baseline and self-reflection in some metrics, but MAD (diverse debate) yields higher human scores and better mitigation of DoT.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MAPS gives gains but did not surpass MAD on human assessment; when translation requires exploring non-obvious alternatives, single-agent analysis may not be as effective as adversarial multi-agent debate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
                <li>Diving into the inter-consistency of large language models: An insightful analysis through debate <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5017",
    "paper_id": "paper-385c74957858e7d6856d48e72b5a902b4c1aa28c",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "MAD",
            "name_full": "Multi-Agent Debate",
            "brief_description": "A framework that runs multiple LLM agents (two debaters in a 'tit-for-tat' exchange) plus a judge that adaptively decides when to stop and extracts the final answer; promotes divergent chain-of-thoughts by forcing contradicting viewpoints and external feedback among agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (as backbone agents and judge in reported MAD runs)",
            "model_description": "OpenAI GPT-3.5-Turbo API model (version reported: gpt-3.5-turbo-0301), used as both debaters and/or judge in experiments; API-based transformer LLM.",
            "reasoning_method_name": "Multi-Agent Debate (MAD)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Two debaters (affirmative and negative) exchange arguments in multiple iterations in a 'tit-for-tat' manner; a separate judge evaluates after each iteration in discriminative/extractive modes and can adaptively stop the debate. Diversity arises from opposing viewpoints and iterative refutation rather than repeated same-chain outputs.",
            "task_name": "Common MT (Commonsense Machine Translation)",
            "task_description": "A Chinese→English translation benchmark focusing on lexical and syntactic ambiguities that require commonsense to resolve (lexical/contextless/contextual subsets).",
            "performance": "With GPT-3.5-Turbo backbone + MAD: Lexical COMET 82.0, BLEURT 70.9, HUMAN 3.78; Contextless COMET 84.8, BLEURT 73.7, HUMAN 3.67; Contextual COMET 85.3, BLEURT 74.0, HUMAN 3.67 (as reported in Table 1).",
            "comparison_with_other_method": true,
            "performance_other_method": "GPT-3.5-Turbo baseline (no MAD): Lexical COMET 80.3, BLEURT 68.2, HUMAN 3.14; +Self-Reflect: Lexical COMET 81.0, BLEURT 69.1, HUMAN 3.43 (Table 1). GPT-4 baseline: Lexical COMET 82.0, BLEURT 70.1, HUMAN 3.41.",
            "key_findings": "MAD (diverse debate-based reasoning) substantially increases textual diversity and reduces commonsense translation bias relative to self-reflection and baseline sampling approaches; GPT-3.5-Turbo with MAD achieves higher human-rated translation quality than GPT-3.5 baseline and matches/exceeds GPT-4 on some Common MT metrics, indicating divergent multi-agent debate can be more effective than iterative self-refinement or single-chain methods for this task.",
            "counter_examples_or_negative_results": "MAD does not always beat stronger single-model baselines on all tasks (e.g., GPT-4 performance on some reasoning tasks); increasing number of debaters beyond two degraded performance for current LLMs due to long-context handling limits; excessive enforced disagreement ('must disagree on every point') was not optimal; forcing more iterations (no adaptive break) can harm performance.",
            "uuid": "e5017.0",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MAD-CIAR",
            "name_full": "Multi-Agent Debate on Counter-Intuitive Arithmetic Reasoning",
            "brief_description": "Application of MAD on the Counter-Intuitive AR dataset where shallow intuition is often wrong and multi-step reasoning is required; debate encourages agents to surface non-intuitive reasoning paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (backbone)",
            "model_description": "OpenAI GPT-3.5-Turbo-0301 used as debaters/judge in MAD runs.",
            "reasoning_method_name": "Multi-Agent Debate (MAD)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Two-agent iterative debate with judge; opposing viewpoints produce alternative chains-of-thought, the judge adjudicates or extracts the final answer after an adaptive number of rounds.",
            "task_name": "Counter-Intuitive AR (CIAR)",
            "task_description": "A 200-question dataset of arithmetic/word problems designed to lure intuitive but incorrect answers; requires multi-step logical reasoning to overcome traps.",
            "performance": "GPT-3.5-Turbo + MAD: Accuracy 37.0% (Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "GPT-3.5-Turbo baseline: 26.0%; +CoT: 28.0%; +Self-Consistency: 29.5%; +Self-Reflect: 27.5%; GPT-4 baseline: 51.0% (Table 3).",
            "key_findings": "MAD (diverse debate) yielded the largest improvement over other GPT-3.5-based methods on CIAR, indicating that explicitly divergent multi-agent argumentation helps overcome intuitive traps better than single-agent self-refinement or single-chain CoT; however, MAD still underperforms GPT-4 on this dataset, showing that method alone doesn't fully close model-capability gaps.",
            "counter_examples_or_negative_results": "MAD (37.0%) remained substantially below GPT-4 (51.0%) on CIAR, indicating that model capacity can dominate method benefits; MAD improvements are relative to weaker backbone models and do not guarantee surpassing stronger LLMs.",
            "uuid": "e5017.1",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Reflect",
            "name_full": "Self-Reflection / Iterative Self-Refinement",
            "brief_description": "Iterative single-agent self-evaluation and refinement: an LLM generates an answer, produces feedback on its own output, and iteratively refines until satisfied.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (as evaluated baseline)",
            "model_description": "OpenAI GPT-3.5-Turbo-0301 used to perform iterative self-reflection in experiments.",
            "reasoning_method_name": "Self-Reflect (iterative self-refinement)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "A single model repeatedly critiques and revises its own answer; because both critique and revision originate from the same model and internal priors, the process tends to produce similar (low-diversity) chains and can suffer from 'Degeneration-of-Thought' where initial confident but incorrect views persist.",
            "task_name": "Counter-Intuitive AR and Common MT (evaluated as baseline)",
            "task_description": "CIAR: counter-intuitive arithmetic problems; Common MT: translation with commonsense ambiguities.",
            "performance": "On CIAR with GPT-3.5-Turbo backbone: Accuracy 27.5% (+Self-Reflect, Table 3). On Common MT (Lexical) +Self-Reflect: COMET 81.0, BLEURT 69.1, HUMAN 3.43 (Table 1).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to GPT-3.5-Turbo baseline (CIAR 26.0%) Self-Reflect gives a marginal improvement; MAD gives a larger improvement (37.0%). For Common MT, MAD overtakes Self-Reflect on HUMAN scores (3.78 vs 3.43).",
            "key_findings": "Self-reflection (similar, single-agent iterative refinement) gives only marginal improvements and is prone to the Degeneration-of-Thought (DoT) effect, where the model sticks to its initial (possibly incorrect) stance and fails to generate truly novel corrective reasoning; multi-agent debate (diverse) mitigates DoT and yields larger gains.",
            "counter_examples_or_negative_results": "Self-Reflect sometimes improves slightly over baseline but generally underperforms diverse approaches like MAD and sampling-based self-consistency for the evaluated tasks; Self-Reflect had lower diversity (Self-BLEU-derived diversity 19.3) and higher bias (29.0) versus MAD (diversity 49.7, bias 24.8) (Table 4).",
            "uuid": "e5017.2",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (zero-shot CoT prompting)",
            "brief_description": "Prompting technique that elicits step-by-step intermediate reasoning by appending triggers like 'Let's think step by step' to problems to get explicit chains-of-thought.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (reported)",
            "model_description": "GPT-3.5-Turbo used with zero-shot CoT trigger to produce stepwise reasoning.",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single-agent generation of an explicit chain-of-thought (stepwise reasoning trace) produced deterministically by a prompt; similarity arises because the method produces a single principal reasoning path for a given prompt and decoding setting.",
            "task_name": "Counter-Intuitive AR (CIAR)",
            "task_description": "Counter-intuitive arithmetic reasoning benchmark requiring multi-step logical thinking to avoid intuitive traps.",
            "performance": "GPT-3.5-Turbo + CoT: Accuracy 28.0% (Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "MAD (GPT-3.5-Turbo backbone): 37.0%; Self-Consistency: 29.5%; baseline GPT-3.5-Turbo: 26.0%.",
            "key_findings": "CoT provides modest improvements over the baseline but is outperformed by diverse methods (MAD) and sampling-ensemble methods (Self-Consistency) on CIAR; explicit single-chain prompts are insufficient to escape intuitive traps in many examples.",
            "counter_examples_or_negative_results": "CoT alone improved accuracy only slightly (28% vs baseline 26%) and did not match performance of multi-path/diverse techniques like MAD.",
            "uuid": "e5017.3",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over sampled CoT)",
            "brief_description": "Sampling-based ensemble over multiple chain-of-thought outputs, selecting the most frequent final answer by majority vote to increase robustness of reasoning.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (reported)",
            "model_description": "GPT-3.5-Turbo used to sample multiple CoT responses and aggregate via majority voting.",
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple sampled chain-of-thought outputs (stochastic decoding) and take majority or consensus over final answers; diversity arises from sampling multiple reasoning paths.",
            "task_name": "Counter-Intuitive AR (CIAR)",
            "task_description": "Dataset of counter-intuitive arithmetic problems requiring careful multi-step reasoning.",
            "performance": "GPT-3.5-Turbo + Self-Consistency: Accuracy 29.5% (Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "MAD: 37.0%; CoT single-chain: 28.0%; baseline GPT-3.5-Turbo: 26.0%.",
            "key_findings": "Sampling multiple chains and ensembling (Self-Consistency) improves over single-chain CoT and single-run baselines, but MAD's structured divergent debate produced larger gains on CIAR for the GPT-3.5 backbone.",
            "counter_examples_or_negative_results": "Although Self-Consistency helps, it still fell short of MAD in the reported CIAR experiments, indicating that sampled chains aggregated by voting may be less effective than explicit adversarial debate for discovering non-intuitive reasoning paths.",
            "uuid": "e5017.4",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Rerank",
            "name_full": "Rerank (QE-based selection from sampled translations)",
            "brief_description": "Sample multiple translation candidates and select the best one using an external quality-estimation model (COMET-QE HUMANr in this work).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (reported)",
            "model_description": "GPT-3.5-Turbo used to sample 4 translations; an external QE model (wmt21-comet-qe-da) ranks candidates.",
            "reasoning_method_name": "Rerank (sampling + external QE)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple (diverse) outputs and use an external quality estimation model to pick the best candidate, introducing diversity via sampling and an external evaluator to choose among alternatives.",
            "task_name": "Common MT (Commonsense Machine Translation)",
            "task_description": "Chinese→English translation benchmark focusing on commonsense/ambiguity resolution.",
            "performance": "GPT-3.5-Turbo + Rerank (Table 1): Lexical COMET 80.9, BLEURT 68.6, HUMAN 3.16; Contextless COMET 84.5, BLEURT 73.2, HUMAN 3.46; Contextual COMET 85.3, BLEURT 73.9, HUMAN 3.58.",
            "comparison_with_other_method": true,
            "performance_other_method": "GPT-3.5-Turbo baseline (no rerank): Lexical COMET 80.3, BLEURT 68.2, HUMAN 3.14; +MAD: Lexical HUMAN 3.78 (higher) and BLEURT 70.9 (Table 1).",
            "key_findings": "Reranking over sampled translations provides modest improvements over the baseline but is outperformed by MAD's debate-based divergence in human judgments for commonsense translation.",
            "counter_examples_or_negative_results": "Rerank improved some automatic metrics but did not match MAD's gains on human evaluations; external QE selection can help but may not capture nuanced commonsense choices that MAD's adversarial exchange surfaced.",
            "uuid": "e5017.5",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MAPS",
            "name_full": "MAPS (Analyze before Translate - CoT-like for translation)",
            "brief_description": "A method that instructs the model to analyze the source sentence's ambiguous parts before producing a translation, effectively a task-specific chain-of-thought.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (reported)",
            "model_description": "GPT-3.5-Turbo used with MAPS-style prompts that ask the model to analyze then translate (chain-of-thought applied to translation).",
            "reasoning_method_name": "MAPS (analyze-before-translate chain-of-thought)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Encourage internal stepwise analysis of ambiguities before generating translation; similarity arises because it’s a single-agent structured reasoning path, akin to CoT applied to translation.",
            "task_name": "Common MT (Commonsense Machine Translation)",
            "task_description": "Chinese→English translations with lexical and syntactic ambiguities requiring commonsense.",
            "performance": "GPT-3.5-Turbo + MAPS (Table 1): Lexical COMET 81.9, BLEURT 70.1, HUMAN 3.43; Contextless COMET 84.2, BLEURT 73.5, HUMAN 3.45; Contextual COMET 85.2, BLEURT 74.0, HUMAN 3.56.",
            "comparison_with_other_method": true,
            "performance_other_method": "MAD: Lexical HUMAN 3.78 and BLEURT 70.9; MAPS has improvements over baseline but generally slightly below MAD on human ratings.",
            "key_findings": "MAPS (single-agent analysis before translate) improves translation quality versus naive baseline and self-reflection in some metrics, but MAD (diverse debate) yields higher human scores and better mitigation of DoT.",
            "counter_examples_or_negative_results": "MAPS gives gains but did not surpass MAD on human assessment; when translation requires exploring non-obvious alternatives, single-agent analysis may not be as effective as adversarial multi-agent debate.",
            "uuid": "e5017.6",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Diving into the inter-consistency of large language models: An insightful analysis through debate",
            "rating": 2
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        }
    ],
    "cost": 0.016198999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</h1>
<p>Tian Liang ${ }^{13 <em>}$ Zhiwei $\mathrm{He}^{2 </em>}$ Wenxiang Jiao ${ }^{3 *}$ Xing Wang ${ }^{3 \dagger}$ Yan Wang ${ }^{3}$<br>Rui Wang ${ }^{2}$ Yujiu Yang ${ }^{1 \dagger}$ Shuming Shi ${ }^{3}$ Zhaopeng Tu ${ }^{3}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Shanghai Jiao Tong University ${ }^{3}$ Tencent AI Lab<br>{liangt21@mails, yang.yujiu@sz}.tsinghua.edu.cn {zwhe.cs}@sjtu.edu.cn<br>{joelwxjiao, brightxwang, brandenwang, zptu}@tencent.com</p>
<h4>Abstract</h4>
<p>Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counterintuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github. com/Skytliang/Multi-Agents-Debate.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable performance on general language tasks (Jiao et al., 2023; Wu et al., 2023; Bang</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Disagreement between two adjacent iterations with respect to the iteration of debate/self-reflection.
et al., 2023) but still struggle on complex reasoning tasks (Zhu et al., 2023a; Gou et al., 2023), which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. In particular, self-reflection (Madaan et al., 2024; Shinn et al., 2024), a concept that usually refers to the process of introspection and examination of a person's own thoughts, has been explored to solve intricate tasks that could be challenging for a zeroshot generation or even chain-of-thought (CoT) prompting (Wei et al., 2022). Specifically, selfreflection involves an iterative refinement process such that the LLM generates a new answer based on the answers and feedback in previous iterations and then provides feedback for the new answer. While self-reflection can be effective in creating better answers, it is highly dependent on the self-evaluation capabilities of LLMs, which are not formally guaranteed (Shinn et al., 2024).</p>
<p>In this work, we focus on the Degeneration-ofThought (DoT) problem in self-reflection, which is proposed and defined by us for the first time. Formally, DoT describes the following scenario:</p>
<p>Once the LLM-based agent has established confidence in its answers, it is unable to generate novel thoughts later through self-reflection even if the initial stance is incorrect.</p>
<p>To demonstrate this problem, we force the agents to engage in a debate or self-reflection for 5 rounds before reaching an answer. Next, we manually determine the disagreement as 1 and agreement as 0 between two adjacent iterations. We define the average disagreement in iteration $i$ as the percentage of opposition occurring between two debaters across multiple debates (or self-confliction in selfreflection). We show the trends in Figure 1. The low disagreement of self-reflection suggests that the LLM sticks to the incorrect answers predicted by CoT and is unable to engage in meaningful selfreflection.</p>
<p>There are various factors (Bortolotti, 2011; Keestra, 2017) that could result in DoT, and we outline three here: (1) Bias and Distorted Perception. Self-perception can be influenced by biases, preconceived notions, and distorted thinking patterns, which can be learned from the massive amount of data during pretraining. If an LLM's self-reflection is clouded by such biases or distorted thinking, it can lead to inaccurate conclusions instinctively. (2) Rigidity and Resistance to Change. Self-reflection often involves challenging one's beliefs, assumptions, and behaviors. If an LLM is resistant to change or holds rigid beliefs, it may struggle to engage in meaningful self-reflection that leads to better answers. (3) Limited External Feedback. Selfreflection is primarily an internal process, but external feedback can provide valuable perspectives and insights. Without considering external feedback, an LLM may miss important blind spots or alternative viewpoints that can enrich its self-reflection.</p>
<p>To address the DoT issue, we leverage another fundamental characteristic of human problemsolving, i.e., debate, to encourage divergent thinking in LLMs. Specifically, we propose the MAD framework, short for Multi-Agent Debate, where two agents express their own arguments in the state of "tit for tat" and a judge monitors and manages the debate process to obtain a final solution. The nature of MAD determines that (1) The distorted thinking of one agent can be corrected by the others; (2) The resistance to change of one agent will be complemented by the others; and (3) each agent can obtain external feedback from the others. Therefore, MAD is less susceptible to the factors of DoT , and can explore divergent chain-of-thoughts to achieve accurate answers.</p>
<p>We conducted experiments on both natural language generation and understanding through two challenging tasks, namely, Commonsense Machine</p>
<p>Translation (Common MT) and Counter-Intuitive Arithmetic Reasoning (Counter-Intuitive AR). The common characteristic of the two tasks is that our instincts are mostly incorrect based on only the superficial expressions of the questions, and deeper levels of contemplation are required for better answers. Experimental results demonstrate that our MAD framework outperforms the baseline methods, especially, GPT-3.5-Turbo with MAD can surpass the performance of GPT-4 on Common MT.</p>
<p>The contributions of this work are summarized as follows:</p>
<ul>
<li>We propose and define the Degeneration-ofThought (DoT) problem in self-reflection, and address it by proposing the Multi-Agent Debate (MAD) framework to explore divergent chain-of-thoughts.</li>
<li>We demonstrate the effectiveness of MAD on two challenging tasks, and find that GPT-3.5-Turbo with MAD can even surpass GPT-4 on the Common MT dataset.</li>
<li>Experimental results show that the adaptive break strategy and the modest level of "tit for tat" state are required for performance improvement. In addition, we find that the llm-based judge shows a preference to the side with the same LLM as the backbone.</li>
</ul>
<h2>2 Multi-Agent Debate Framework</h2>
<p>Figure 2 illustrates the general framework of MAD, where two debaters and a judge are involved in a debate to resolve a math problem while self-reflection descends into the trap of DoT. Generally, our MAD framework is composed of three components which are elaborated as follows:</p>
<p>Meta Prompts. We use meta prompts to introduce the topic to be solved, the number of debaters, the iteration limit, and other requirements. An exampe of meta prompts for the arithmetic reasoning task in Figure 2 is:</p>
<p>You are a debater. Hello and welcome to the debate competition. It's not necessary to fully agree with each other's perspectives, as our objective is to find the correct answer. The debate topic is stated as follows: $&lt;$ debate topic $&gt;$.</p>
<p>As seen, we require the agents to "tit for tat" (e.g. contents underlined in meta prompts above) so as to create an atmosphere of debate.</p>
<h1>Counter-Intuitive Question</h1>
<p>The two circles are externally tangent and there is no relative sliding. The radius of circle $A$ is $1 / 3$ the radius of circle B. Circle A rolls around circle B one trip back to its starting point. How many times will circle A revolve in total?</p>
<h2>Chain-of-Thought</h2>
<p>Let the radius of circle $A$ be $r$ and the radius of circle $B$ be $3 r$. The distance circle $A$ travels along the circumference of circle $B$ is equal to the circumference of circle $B$, which is $6 \pi r$. The number of revolutions $=(6 \pi r) /(2 \pi r)=6 \pi r /$ $2 \pi r=3$. So, circle $A$ will revolve 3 times in total as it rolls around circle $B$ once.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Framework of Multi-Agent Debate. Here we designate the devil ( $\boldsymbol{\sim}$ ) as the affirmative side while the angel ( $\boldsymbol{\theta}$ ) as the negative side. We want the angel to correct the devil's mistakes.</p>
<p>Debaters. There are $N$ debaters $D=\left{D_{i}\right}<em i="i">{i=1}^{N}$ involved in the framework. In each debate iteration, the debaters $D</em>(H)=h$. An example of a debater prompt appears below:}$ speak one by one in a fixed order and express their arguments based on the previous debate history $H$, i.e., $D_{i</p>
<ul>
<li>Prompt for Affirmative Debater ( $\boldsymbol{\mathcal { A }}$ )</li>
</ul>
<p>You are affirmative side. Please express your viewpoints.</p>
<ul>
<li>Prompt for Negative Debater ( $\boldsymbol{\mathcal { N }}$ )</li>
</ul>
<p>You are negative side. You disagree with the affirmative side's points. Provide your reasons and answer.</p>
<p>Judge. We also design a judge $J$ to manage and monitor the whole debate process. The judge contains two different modes: (a) Discrinative Mode, in which the judge $J$ decides whether the correct solution can be obtained after all the debaters finish their arguments in the current iteration:</p>
<p>$$
J_{d}(H)= \begin{cases}\text { True, } &amp; \text { solution obtained } \ \text { False, } &amp; \text { otherwise }\end{cases}
$$</p>
<p>If it is True, the debate is over. Otherwise, the debate continues. (b) Extractive Mode, in which
the judge $J$ needs to extract the final solution based on the whole debate history: $J_{e}(H)=a$, since no correct solution is identified within the iteration limit of debate. An example of a judge prompt ( $\boldsymbol{\Omega}$ ) appears below:</p>
<p>You are a moderator. There will be two debaters involved in a debate competition. They will present their answers and discuss their perspectives on the <debate topic>. At the end of each round, you will evaluate both sides' answers and decide which one is correct.</p>
<h2>3 Experiment</h2>
<h3>3.1 Challenging Testbeds</h3>
<p>We conduct experiments on two challenging tasks, namely, commonsense machine translation (i.e., Common MT), and counter-intuitive arithmetic reasoning (i.e., Counter-Intuitive AR), which require deep levels of contemplation for LLMs. Please refer to Appendix A for more details.</p>
<p>Commonsense Machine Translation The Common MT dataset is composed of Chinese $\Rightarrow$ English translation examples (He et al., 2020), which are used to examine three types of ambiguity resolution</p>
<p>abilities of translation models, covering lexical and contextless/contextual syntactic ambiguity. Within the challenging part of Common MT, the authentic translation of each source sentence requires a proper understanding of common sense knowledge. While these ambiguous sentences might appear to have a straightforward translation, such a literal interpretation is erroneous. Failure to address such ambiguities may result in inaccurate translations.</p>
<p>Counter-Intuitive Arithmetic Reasoning Previous studies on thinking hierarchy (Daniel, 2017) suggest that we humans have a fast and intuitive system and a slow and logical system, and tend to run the lower level system before the higher level one. Inspired by this, we created a more challenging dataset named Counter-Intuitive Arithmetic Reasoning (CIAR) to evaluate the reasoning abilities of LLMs at deep levels. Our Counter-Intuitive AR dataset contains 200 questions collected from elicitation questions (Kong et al., 2022) ${ }^{1}$, web data ${ }^{2}$ and additional manual derivatives of these questions. Compared to the commonly-used datasets, e.g., MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), our dataset presents two distinct challenges:</p>
<ul>
<li>Resistance to Intuition. The questions are embedded in hidden traps designed to elicit intuitive and appealing answers that are often incorrect. This feature evaluates the abilities of LLMs to resist the traps of superficial expressions.</li>
<li>Multi-Step Reasoning. Each correct answer within the dataset requires a rigorous multi-step reasoning process, thereby evaluating the capacity of LLMs to engage in complex decisionmaking and problem-solving.</li>
</ul>
<h3>3.2 Setups</h3>
<p>Input Format. Our experiments are performed in zero-shot instructions (setting temperature to 0 ). For all used datasets, we use a unified prompt to make LLMs give explanations and answers. We present the inputs to agents through <debate topic $>$ as mentioned in Section 2. For example, if we want to translate "吃掉敌人一个师" from Chinese to English, we will set the <debate topic> as "What is the correct English translation of the following Chinese text: 吃掉敌人一个师". For QA task, we employ the same prompt except set the $&lt;$ debate topic $&gt;$ to the arithmetic question.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Backbone Models. In this work, we mainly use three agents in our MAD framework, including two debaters (i.e., affirmative and negative) and a judge. We assess two open-source (i.e., vicuna-7b-v1.5-16k ${ }^{3}$ and vicuna-13b-v1.5-16k ${ }^{4}$ ) and two apibased LLMs (i.e., GPT-3.5-Turbo-0301 and GPT-4-0314).</p>
<p>Compared Methods. Generally, we compare our MAD framework with baseline models and Self-Reflect on both tasks. We also include other baseline methods individually, namely, Rerank and MAPS for Common MT, CoT and SelfConsistency for Counter-Intuitive AR. Below elaborates the details of them:</p>
<ul>
<li>Self-Reflect (Shinn et al., 2024): This approach requires the LLM to refine its translation until it deems the current output satisfactory.</li>
<li>Rerank (He et al., 2024): We sample the translations from the LLM for four times, from which we select the best candidate based on a quality estimation (QE) HUMANr ${ }^{5}$. This approach can be seen as analogous to self-consistency (Wang et al., 2022), where the majority voting is replaced by an external QE HUMANr.</li>
<li>MAPS (He et al., 2024): This method enables LLMs to mimic the human translation process: analyze before translate, which can be viewed as a chain-of-thought method applied to translation.</li>
<li>CoT (Kojima et al., 2022): This approach concatenates a trigger sentence "Let's think step by step" to the test question.</li>
<li>Self-Consistency (Wang et al., 2022): This method samples multiple responses and determines the final answer through a majority vote.</li>
</ul>
<p>All agents in our experimental setup, such as debaters and judge, are large language models. Here, we implement the methods on top of GPT-3.5-Turbo and Vicuna models.</p>
<p>Evaluation Metrics. For Counter-Intuitive AR, we report the accuracy (ACC) of predictions. For Common MT, we adopt automatic metrics</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Lexical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Contextless</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Contextual</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">HUMAN</td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">HUMAN</td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">HUMAN</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">3.41</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">3.65</td>
</tr>
<tr>
<td style="text-align: center;">Turbo</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">3.14</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">3.57</td>
</tr>
<tr>
<td style="text-align: center;">+ Rerank</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">3.16</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">3.58</td>
</tr>
<tr>
<td style="text-align: center;">+ MAPS</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">3.45</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">3.56</td>
</tr>
<tr>
<td style="text-align: center;">+ Self-Reflect</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr>
<td style="text-align: center;">+ MAD</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7b</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">3.23</td>
</tr>
<tr>
<td style="text-align: center;">+ MAD</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">3.27</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">2.81</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">3.04</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">3.37</td>
</tr>
<tr>
<td style="text-align: center;">+ MAD</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">2.96</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">3.45</td>
</tr>
</tbody>
</table>
<p>Table 1: Translation performance on Common MT. Note that Rerank and MAPS use the external quality estimation tool to select the best translation from multiple translation candidates. HUMAN: direct assessment of translation quality from human evaluators on a scale ranging from 1 to 5 .
like COMET $^{6}$ and BLEURT $^{7}$, which are widely adopted evaluation metrics for LLM-based translation literature (He et al., 2024; Hendy et al., 2023; Garcia et al., 2023; Pilault et al., 2023). In addition, we also employ professional human translators to directly assess the translation results, measuring translation quality on a scale ranging from 1 to 5 .</p>
<h3>3.3 Results on Common MT</h3>
<p>Results. In Common MT test set, we focus more on the translation accuracy of specific words and whether they conform to common sense. However, such minor variations at token level are difficult to reflect on automatic metrics. We therefore provide human HUMAN to evaluate these methods more accurately. Table 1 presents the experimental results. MAPS and Self-Reflec achieve improvements over baseline GPT-3.5-Turbo. Remarkably, our proposed MAD, by utilizing GPT-3.5 as the backbone model, has demonstrated significant advancements over GPT-4 across both automatic and human evaluation metrics.</p>
<p>Case Study. Table 2 shows example translations generated by baseline GPT-3.5-Turbo and the proposed MAD. We can find that the baseline GPT-3.5-Turbo (even the more powerful GPT-4) incorrectly translates the source words literally. Because of the DoT issue, Self-Reflect cannot rectify the literal translation. The proposed MAD framework, which explores divergent chain-of-thoughts,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Source | 呪掖数人一个师。 |
| :-- | :-- |
| Correct Ref. | Destroy a division of the enemy. |
| Incorrect Ref. | Eat up an enemy division. |
| GPT-4 | Eat up an enemy division. |
| GPT-3.5-Turbo | Eat up an enemy division. |
| + Self-Reflect | Eat up an enemy division. |
| + MAD | Eliminate an enemy division. |</p>
<p>Table 2: Example translations generated by different methods. Best viewed in color.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ACC (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">+ CoT</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: left;">+ Self-Consistency</td>
<td style="text-align: center;">29.5</td>
</tr>
<tr>
<td style="text-align: left;">+ Self-Reflect</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: left;">+ MAD</td>
<td style="text-align: center;">$\mathbf{3 7 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on Counter-Intuitive AR.
can generate the free translation of the underlined words within the source sentences.</p>
<h3>3.4 Results on Counter-Intuitive AR</h3>
<p>Results. Table 3 lists the results in terms of reasoning accuracy. We can observe that SelfReflect only marginally improves over the baseline GPT-3.5-Turbo, while CoT and Self-Consistency bring more improvements. Our MAD framework, though not as good as GPT-4, outperforms all the other compared methods based on GPT-3.5-Turbo, which further demonstrates its effectiveness. We also validate MAD on math and symbolic reasoning tasks and report our results in Appendix C.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Bias $\downarrow$</th>
<th style="text-align: left;">Diversity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Self-Reflect</td>
<td style="text-align: left;">29.0</td>
<td style="text-align: left;">19.3</td>
</tr>
<tr>
<td style="text-align: left;">MAD</td>
<td style="text-align: left;">24.8</td>
<td style="text-align: left;">49.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Mitigation of Degeneration-of-Thought.</p>
<p>Case Study. Figure 2 shows an example on Counter-Intuitive AR. We find both CoT and SelfReflect fail to reach the right answer by mistakenly outputting 3 . With divergent thinking, our MAD framework emerges "we need to consider both the rotation around circle $B$ and the rotation of circle A itself" and find the correct answer 4.</p>
<h2>4 Analysis</h2>
<p>In this section, we present a qualitative analysis to provide some insights how MAD works. Unless otherwise stated, we report the overall results on the Common MT dataset.</p>
<h3>4.1 Mitigation of DoT</h3>
<p>As mentioned in the Section 1, the DoT problem originates from three factors: (1) Bias and Distorted Perception, (2) Rigidity and Resistance to Change, and (3) Limited External Feedback. In our MAD framework, we introduce the views of other agents in the form of debates, solving the phenomenon of limited external feedback (problem 3). Next, this section will delve into the mitigation of problems 1 and 2 through experiments.</p>
<ul>
<li>Bias: We observe that LLMs often rely on direct intuition, which can lead to incorrect or inappropriate responses. To address this problem, we use human evaluation to determine the ambiguity error rate of LLMs' responses, examining if the LLM's output is biased.</li>
<li>Diversity: LLMs are resistant to changing their answers and lack diverse reflection. The diversity of the translations is evaluated using the Self-BLEU score (Yin et al., 2020). In other words, methods lacking diverse reflection produce more similar translation candidates. Consequently, higher Self-BLEU scores mean lower diversity. We calculate text diversity via:</li>
</ul>
<p>$$
\text { Diversity }=100-\text { Self_BLEU }\left(\text { Cand }<em 2="2">{1}, \text { Cand }</em>\right)
$$</p>
<p>In formula (2), candidates 1 and 2 represent the initial translation (base answer in Self-Reflection or affirmative side's response in MAD) and the current</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Judge LLM</th>
<th style="text-align: center;">COMET</th>
<th style="text-align: center;">HUMAN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vicuna-13b as Debaters</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">3.25</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo as Debaters</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">3.47</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">3.69</td>
</tr>
</tbody>
</table>
<p>Table 5: Translation performance with different judge.
translation (possible modified answer after SelfReflection or negative side's response in MAD).</p>
<p>As shown in Table 4, Bias and Rigidity are significant factors causing DoT. In addition, addressing these biases and stereotypes through self-reflection can be challenging. MAD framework effectively corrects inherent biases in translation, mitigates DoT, and considerably improves performance.</p>
<h3>4.2 Analysis of Judge</h3>
<p>In this section, we analyze the behavior of the judge for different settings of the debaters.</p>
<p>Strong debaters with a weak judge work better than the reverse. To understand the roles of debaters and judge in MAD, we employ various combinations of models to initialize the agents. Specifically, we utilize the smaller language model (vicuna-13b-v1.5-16k) as a judge to evaluate the debate results of the more powerful LLMs (GPT-3.5-Turbo), and vice versa.</p>
<p>The detailed experimental findings are presented in Table 5. The quality of the debaters' responses significantly impact the performance ceiling of MAD. Regardless of the model chosen for the judge, Turbo debaters consistently generate superior translations compared to Vicuna. In addition, the selection of the judge agent plays a secondary role. When Turbo debaters are involved, Vicuna, serving as the judge, underperforms Turbo across all test sets.</p>
<p>LLM may not act as an impartial judge when different LLMs are used as debaters. We study the behavior of agents by calculating how many times the judge chooses the answers of each debater as the final solution in different scenarios. The results are listed in Table 6 and we have the following observations:</p>
<ul>
<li>Same LLM for All Agents (Rows (1) and (2)):</li>
</ul>
<p>We find that the judge consistently favors the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Translation performance with respect to the debate level on Lexical.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ID</td>
<td>Jud</td>
<td>Debater</td>
<td></td>
<td>Winner</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Aff</td>
<td>Neg</td>
<td>Aff</td>
<td>Neg</td>
<td>Tie</td>
</tr>
<tr>
<td>①</td>
<td>Turbo</td>
<td>Turbo</td>
<td>Turbo</td>
<td>87</td>
<td>104</td>
<td>9</td>
</tr>
<tr>
<td>②</td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>67</td>
<td>124</td>
<td>9</td>
</tr>
<tr>
<td>③</td>
<td>GPT-4</td>
<td>Turbo</td>
<td>GPT-4</td>
<td>52</td>
<td>136</td>
<td>12</td>
</tr>
<tr>
<td>④</td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>Turbo</td>
<td>120</td>
<td>77</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Table 6: Number of times the judge chooses the answers of each debater based on different LLM.</p>
<p>negative side, which is believed to contribute to the performance improvement in MAD. When encountering complex tasks, the affirmative side tends to make mistakes that should be corrected by the opposing side to achieve improvements.</p>
<p>• Debaters of Different LLMs (Rows ③ and ④): We find that the judge shows a preference to the side with the same LLM as the backbone. This bias indicates that LLMs might not be a fair judge (Wang et al., 2023) when different LLMs are used for the agents.</p>
<h3>4.3 Analysis of Debaters</h3>
<p>In this section, we will discuss several factors of debaters that would affect the performance of MAD: <em>debater number</em>, <em>debate level</em>, and <em>debate iteration</em>.</p>
<p>Increasing the number of debaters fails when backbone LLMs are poor at long-text modeling. It seems intuitive that increasing the number of debaters would enhance diversity of thought and subsequently improve performance. However, as shown in Table 7, an increase in the number of debaters has resulted in varying degrees of performance reduction.</p>
<p>To address this issue, we manually analyze the debate processes in approximately 10% of the test</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Distribution of iteration rounds and a human score of each iteration subset.</p>
<table>
<thead>
<tr>
<th># of Debaters</th>
<th>COMET</th>
<th>HUMAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>2 (Default)</td>
<td>84.4</td>
<td>3.69</td>
</tr>
<tr>
<td>3</td>
<td>83.1</td>
<td>3.58</td>
</tr>
<tr>
<td>4</td>
<td>82.9</td>
<td>3.49</td>
</tr>
</tbody>
</table>
<p>Table 7: Translation performance with more debaters.</p>
<p>subset. As the number of debaters increases, the length and complexity of the text also increase. Such LLM-based debaters tend to forget the views of other debaters during the debate. Moreover, it becomes more challenging for the judge to extract information from the debates for summarization. This suggests that the key challenge of MAD with more debaters lies in the limitations of the LLMs to handle long texts (Liu et al., 2024).</p>
<p>Appropriate "tit for tat" is beneficial for effective debate. We then study how the intensity of "tit for tat" affects the performance of MAD. To achieve so, we design different instructions (see Table 11 in Appendix) to initialize the debaters' meta prompt. As shown in Figure 3, asking the debaters to "tit for tat" (i.e., higher disagreement) is necessary for MAD to achieve good performance. However, we find that "<em>must disagree with each other on every point</em>" (with a disagreement of 0.988) does not lead to the best performance. We speculate that continuous disagreement without finding common ground can contribute to polarization, where the debate becomes more about winning the argument than seeking truth or understanding. This can reinforce pre-existing biases and make it difficult to reach a meaningful consensus.</p>
<p>Complex questions require more iteration rounds of debate. In our experimental setup, we did not implement any additional stopping strate-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance with respect to the iteration of debate or self-reflection.
gies besides setting the maximum debate iteration to 3. In other words, the judge can take an adaptive break if it believes the optimal answer has already been obtained, efficiently ending the debate early.</p>
<p>To understand the distribution of iteration rounds and factors contributing to a longer debate process, we analyze the experimental results and present them in Figure 4. In the majority of cases, the optimal answer can be achieved through a single round of debate, demonstrating the efficiency of MAD. However, when translating more complex sentences (subsets with lower human scores), the judge requires additional iterations to gather adequate information from the debaters before making a final decision. We also find that our MAD framework consistently brings performance improvements across all the three subsets, demonstrating its effectiveness.</p>
<p>Adaptive break plays an important role to conclude the debate in the optimal moment. Intuitively, longer debates would encourage more diverse thinking. It raises the question of how the model's performance would be affected if constrained to conclude at a specific debate round. For each iteration, we force the judge $J$ to extract the final answer $\left(a=J_{e}(H)\right)$ instead of adaptively breaking the debate as in MAD.</p>
<p>As shown in figure 5, we can observe that MAD performs better than self-reflection as the iteration increases. However, the highest COMET score appears at the first iteration and is also lower than the result of the adaptive break. It indicates that, for most examples, MAD can generate good translations at the first iteration such that the debate should be stopped. Forcing the debate to continue will harm the translation results, which demonstrates the reasonableness of our adaptive break strategy.</p>
<h2>5 Related Work</h2>
<p>Chain-of-Thought Prompting. Recently, (Wei et al., 2022) has proposed chain-of-thought (CoT) prompting to improve the reasoning ability of LLMs. Specifically, CoT prompts LLMs to generate a series of intermediate steps that lead to the final answer of a multi-step problem. Most earlier work primarily concentrates on two main aspects: prompt design and decoding strategies. Zero-shot CoT (Kojima et al., 2022) employs the trigger sentence "Let's think step by step" to provide guidance for the decoding of LLMs. Advanced sampling strategies have been explored to improve CoT by generating diverse reasoning paths, e.g., SelfConsistency (Wang et al., 2022), Auto-CoT (Zhang et al., 2022), Active-Prompting (Diao et al., 2023), Complexity-based Consistency (Fu et al., 2022), Multi-Chain Reasoning (Yoran et al., 2023), and Progressive-Hint Prompting (Zheng et al., 2023).</p>
<p>With the emergence of powerful LLMs, approaches based on self-evaluation have attracted increasing attention. These approaches involve the generation of initial output, followed by evaluating the output to acquire feedback, which is then utilized to refine the output. Evaluation feedback can come from the model itself, e.g., Self-refine (Madaan et al., 2024) and Tree of Thoughts (Yao et al., 2024)) or external environments, e.g., QAaP (Zhu et al., 2023b) and Reflection (Shinn et al., 2024). The intuition behind these approaches involves the utilization of robust LLMs to mimic the human cognition process.</p>
<p>Generative Agents. Recently, LLM-based multiagent intelligent, e.g., Generative Agents (Park et al., 2023), Ghost in the Minecraft (Zhu et al., 2023c), GPT-Bargaining (Fu et al., 2023), has drawn significant attention for enabling simulations of human behavior. Our work follows this research line to address the DoT problem of LLMs. Concurrent with our work, a few studies (Xiong et al., 2023; Du et al., 2023) also explore the multi-agent debate framework to enhance the reasoning ability of LLMs. The main differences between our MAD framework and these works are: (1) we introduce an additional judge with an adaptive break mechanism to decide the optimal moment to conclude the debate; (2) our work aims to address the DoT problem, which is an inherent deficiency of LLMs; and (3) we empirically find that our MAD framework can yield enhanced performance by employing agents with the identical backbone LLM.</p>
<h2>6 Conclusion</h2>
<p>We propose and define the Degeneration-ofThought (DoT) problem in self-reflection, and address it by proposing the Multi-Agent Debate (MAD) framework to explore divergent chain-of-thoughts. We demonstrate the effectiveness of MAD on two challenging tasks and find that GPT-3.5-Turbo with MAD can even surpass GPT-4 on the Common MT dataset. Extensive analyses suggest that the adaptive break strategy of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Complex samples require more rounds of debate. More interestingly, we find that LLMs might not be a fair judge if different LLMs are used for agents.</p>
<p>Future work includes scheduling more agents in the debate in an appropriate manner, multi-agent intelligence for board games, and AI feedback for model alignment.</p>
<h2>Limitations</h2>
<p>A limitation of this work is that our method requires more time cost, as agents need to engage in multiple rounds of interaction to present and refute arguments. Moreover, current LLM-based agents may struggle to maintain coherence and relevance in long context scenarios, leading to potential misunderstandings and loss of context. Enhancing long-text modeling capability of large language models remains a future challenge. LLM-based judge may have a preference for outputs generated by itself. To mitigate this bias within the MAD framework, we recommend that all roles, including both the judge and debaters, utilize the same LLM, or alternatively, that the judge and debaters employ distinct LLMs.</p>
<h2>References</h2>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675-718.</p>
<p>Lisa Bortolotti. 2011. Does reflection lead to wise choices? Philosophical Explorations, 14(3):297313.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Kahneman Daniel. 2017. Thinking, fast and slow. Farrar, Straus and Giroux.</p>
<p>Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active prompting with chain-ofthought for large language models. arXiv preprint arXiv:2302.12246.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.</p>
<p>Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.</p>
<p>Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of fewshot learning for machine translation. In International Conference on Machine Learning, pages 10867-10878. PMLR.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing.</p>
<p>Jie He, Tao Wang, Deyi Xiong, and Qun Liu. 2020. The box is in the pen: Evaluating commonsense reasoning in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3662-3672, Online. Association for Computational Linguistics.</p>
<p>Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2024. Exploring humanlike translation strategy with large language models. Transactions of the Association for Computational Linguistics, 12:229-246.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical</p>
<p>Methods in Natural Language Processing (EMNLP), pages 523-533.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745.</p>
<p>Machiel Keestra. 2017. Metacognition and reflection by interdisciplinary experts: Insights from cognitive science and philosophy. Issues in Interdisciplinary Studies, 35:121-169.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Yuqing Kong, Yunqi Li, Yubo Zhang, Zhihuan Huang, and Jinzhao Wu. 2022. Eliciting thinking hierarchy without a prior. Advances in Neural Information Processing Systems, 35:13329-13341.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36.</p>
<p>Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.</p>
<p>Jonathan Pilault, Xavier Garcia, Arthur Bražinskas, and Orhan Firat. 2023. Interactive-chain-prompting: Ambiguity resolution for crosslingual conditional generation with interaction. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455-483.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. 2023. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. arXiv preprint arXiv:2303.13648.</p>
<p>Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. Diving into the inter-consistency of large language models: An insightful analysis through debate. arXiv preprint arXiv:2305.11595.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Haiyan Yin, Dingcheng Li, Xu Li, and Ping Li. 2020. Meta-cotgan: A meta cooperative training paradigm for improving adversarial text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9466-9473.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5942-5966.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Jiaxing Zhang, Yujiu Yang, et al. 2023a. Solving math word problems via cooperative reasoning induced language models. In The 61st Annual Meeting Of The Association For Computational Linguistics.</p>
<p>Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, JianGuang Lou, and Yujiu Yang. 2023b. Question answering as programming for solving time-sensitive questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12775-12790.</p>
<p>Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. 2023c. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144.</p>
<h1>A Challenging Testbeds</h1>
<p>We conduct experiments on two challenging tasks, namely, commonsense machine translation (i.e., Common MT), and counter-intuitive arithmetic reasoning (i.e., Counter-Intuitive AR), which require deep levels of contemplation for LLMs.</p>
<h2>A. 1 Commonsense Machine Translation</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Ambiguity Type</th>
<th style="text-align: center;">Source Sentence</th>
<th style="text-align: center;">Correct Reference</th>
<th style="text-align: center;">Incorrect Translation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical</td>
<td style="text-align: center;">吃掉敌人一个师。</td>
<td style="text-align: center;">Destroy a division of the enemy.</td>
<td style="text-align: center;">Eat up an enemy division.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">他喜欢吃苹果。</td>
<td style="text-align: center;">He likes to eat apples.</td>
<td style="text-align: center;">He likes to destory apples.</td>
</tr>
<tr>
<td style="text-align: center;">Contextless</td>
<td style="text-align: center;">正在手术的老健康的医生。</td>
<td style="text-align: center;">A healthy doctor is doing surgery.</td>
<td style="text-align: center;">What is undergoing surgery is a doctor who is healthy.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">正在手术的老生命垂危的病人。</td>
<td style="text-align: center;">What is undergoing surgery is a patient whose life is dying.</td>
<td style="text-align: center;">A patient whose life is dying is doing surgery.</td>
</tr>
<tr>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">当地震袭击中国时, 揭晰的更中国。</td>
<td style="text-align: center;">When the earthquake hit China, China was aided.</td>
<td style="text-align: center;">When the earthquake hit China, China has assisted.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">当地震袭击日本时, 揭晰的更中国。</td>
<td style="text-align: center;">When the earthquake hit Japan, China has assisted.</td>
<td style="text-align: center;">When the earthquake hit Japan, China was aided.</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of lexical, contextual and contextless syntactic ambiguity from the Common MT dataset. The underlined Chinese words are translated into the corresponding colored words in English. Best viewed in color.</p>
<p>The Common MT dataset is composed of Chinese $\Rightarrow$ English translation examples (He et al., 2020), which are used to examine three types of ambiguity resolution abilities of translation models. Specifically, The Common MT test set we used covers 200 examples of lexical ambiguity, 450 examples of contextless syntactic ambiguity, and 350 examples of contextual syntactic ambiguity. Within the challenging part of Common MT, the authentic translation of each source sentence requires a proper understanding of common sense knowledge. While these ambiguous sentences might appear to have a straightforward translation, such a literal interpretation is erroneous. Failure to identify and address such ambiguities may result in inaccurate translations.</p>
<p>Table 8 lists some examples of these three types of ambiguity. Lexical ambiguity refers to words with multiple meanings in different contexts. Contextless and contextual syntactic ambiguity involve sentences with multiple interpretations, which can be resolved by context or common sense. As the lexical ambiguity of "吃掉敌人一个师" shows, the source word "吃掉" should be translated to "destroy" rather than the straightforward translation "eat up" by considering the common sense in the real world.</p>
<h2>A. 2 Counter-Intuitive Arithmetic Reasoning</h2>
<p>Previous studies on thinking hierarchy (Daniel, 2017) suggest that we humans have a fast and intuitive system and a slow and logical system, and tend to run the lower level system before the higher level one. Inspired by this, we created a more challenging dataset named Counter-Intuitive Arithmetic Reasoning (CIAR) to evaluate the reasoning abilities of LLMs at deep levels.</p>
<p>Dataset Description. Our Counter-Intuitive AR dataset contains 200 questions collected from elicitation questions (Kong et al., 2022) ${ }^{8}$, web data ${ }^{9}$ and additional manual derivatives of these questions. Compared to the commonly-used datasets, e.g., MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), our dataset presents two distinct challenges:</p>
<ul>
<li>Resistance to Intuition. The questions in our dataset are embedded in hidden traps designed to elicit intuitive and appealing answers that are often incorrect. This feature evaluates the abilities of LLMs to resist the traps of superficial expressions.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Components</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">When Alice walks up the hill, her speed is $1 \mathrm{~m} / \mathrm{s}$ and when she goes down the <br> hill, her speed is $3 \mathrm{~m} / \mathrm{s}$. Then when Alice walks up and down the hill, what is <br> her average speed?</td>
</tr>
<tr>
<td style="text-align: left;">Correct Answer</td>
<td style="text-align: left;">$1.5 \mathrm{~m} / \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: left;">Explanation</td>
<td style="text-align: left;">If Alice covers a distance of d going up and down the hill, then her total distance <br> is 2 d . Her time going up the hill is $\mathrm{d} / 1=\mathrm{d}$, and her time going down the hill is <br> $\mathrm{d} / 3$. So, her total time is $\mathrm{d}+\mathrm{d} / 3=4 \mathrm{~d} / 3$. Therefore, her average speed is $2 \mathrm{~d} /$ <br> $(4 \mathrm{~d} / 3)=3 / 2 \mathrm{~m} / \mathrm{s}$.</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect Answer</td>
<td style="text-align: left;">$2 \mathrm{~m} / \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: left;">Explanation</td>
<td style="text-align: left;">Alice's average speed can be calculated by adding her speed going up the hill <br> and her speed going down the hill, and then dividing by 2. So, $(1 \mathrm{~m} / \mathrm{s}+3 \mathrm{~m} / \mathrm{s}) /$ <br> $2=2 \mathrm{~m} / \mathrm{s}$. Therefore, Alice's average speed is $2 \mathrm{~m} / \mathrm{s}$.</td>
</tr>
</tbody>
</table>
<p>Table 9: An example in Counter-Intuitive AR dataset.</p>
<ul>
<li>Multi-Step Reasoning. Each correct answer within the dataset requires a rigorous multi-step reasoning process, thereby evaluating the capacity of LLMs to engage in complex decision-making and problemsolving.</li>
</ul>
<p>Dataset Format. In our Counter-Intuitive AR dataset, each example contains three key components (see Table 9 for an example). We elaborate on the details below:</p>
<ul>
<li>Questions. The questions in our dataset are designed to stimulate counter-intuitive thinking, which aims to challenge conventional decision-making by presenting situations where the immediate, intuitive response is often incorrect.</li>
<li>Answers. Each question is provided with a correct answer, which requires deep comprehension of the question and commonsense knowledge. Additionally, we also provide a plausible yet incorrect answer for comparison.</li>
<li>Explanations. We offer comprehensive explanations for each correct answer, detailing the step-by-step rationale that leads to the right solution. We also provide the seemingly logical reasoning process behind incorrect answers. This reasoning process highlights the potential pitfalls and misconceptions during decision-making, especially when intuition is prioritized over rigorous logical reasoning.</li>
</ul>
<p>Experimental Settings. During our experiments, we did not utilize the explanations from the dataset. We provided detailed explanations to facilitate subsequent researchers to understand how the correct answer was derived.</p>
<h1>B Human Evaluation Details</h1>
<p>We implement human evaluation as follows:</p>
<ul>
<li>Human Score: We randomly shuffled the display order of the translated sentences from all methods in an anonymous manner. Then, employed three professional human translators (Krippendorff's Alpha $=$ 0.76 ) to directly assess all methods together. Finally, we calculated the average scores for each methods.</li>
<li>Bias: We also focus on whether the translation of specific words in CommonMT conforms to commonsense. Table 8 lists an example of lexical ambiguity, where the source word "无非" should be translated to "destroy" rather than the straightforward translation "eat up". Here, we asked the annotators to label each sentence as 1 (not conforming to commonsense) or 0 (conforming to commonsense), and report the degree of bias for the whole test set.</li>
</ul>
<h1>C Results on math and symbolic reasoning tasks</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Math Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Symbolic Reasoning (BBH)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GSM</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">Penguin</td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Colored Objects</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: left;">Self-Reflect</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">MAD</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">58.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparison of accuracy on math (e.g. GSM (Cobbe et al., 2021) and AddSub (Hosseini et al., 2014)) and symbolic reasoning (three datasets from Big-Bench (Srivastava et al., 2023; Suzgun et al., 2023)).</p>
<h2>D Prompts for Different Debate Levels</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: left;">Both sides must reach a full consensus on every point of the debate. Every statement <br> must be agreed upon by both sides.</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Most of the debate should be characterized by disagreements, but there may still be a <br> small amount of consensus on less significant points.</td>
</tr>
<tr>
<td style="text-align: center;">2 (Default)</td>
<td style="text-align: left;">It's not necessary to fully agree with each other's perspectives, as our objective is to <br> find the correct answer.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Both sides must disagree with each other on every point of the debate. There should <br> be no consensus whatsoever.</td>
</tr>
</tbody>
</table>
<p>Table 11: Prompts for different levels of "tit for tat" state. We modulate the level of "tit for tat" state outlined in Section 2 through appending natural language instructions to the debaters' meta prompt.</p>
<h2>E Extra Computational Cost</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Cost (# Generated Tokens)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Self-Reflect</td>
<td style="text-align: center;">$1.83 \times$</td>
</tr>
<tr>
<td style="text-align: left;">MAD</td>
<td style="text-align: center;">$2.46 \times$</td>
</tr>
</tbody>
</table>
<p>Table 12: Extra generated tokens on the counter-intuitive AR task.
We measure the inference cost by the number of generated tokens. As listed in Table 12, the selfreflection cost is 1.83 times that of the baseline COT method. Meanwhile, our MAD approach incurs an additional 0.63 times the inference cost. This increase in cost is deemed acceptable given the significant benefit our approach offers in enhancing LLMs' divergent thinking capabilities without requiring external signals (interaction with humans).</p>
<h1>F Debate Process</h1>
<h2>F. 1 Commonsense Machine Translation</h2>
<p>Table 13 presents the debate process of example translation discussed in Section 3.3. In this debate, both sides disputed the English translation of the Chinese sentence "吃掉敌人一个师". The affirmative side proposed "eat up an enemy division", advocating for a more literal translation to depict the consumption or devouring of the enemy. In contrast, the negative side suggested "eliminate an enemy division", arguing this expression better conveys the destruction of the enemy in a military context. After the debate, the judge favored "eliminate", as it accurately reflected the severity and intent of the original military language. This debate highlights the complexities of translation, where context and nuance significantly impact the selection of appropriate words. In response to this issue, our MAD method helps LLMs understand semantics correctly and generate highly authentic translations.</p>
<h2>$&lt;$ Debate Topic $&gt;$ : What is the correct English translation of the following text: 吃掉敌人一个师?</h2>
<p>I believe the correct translation of the Chinese text "吃掉敌人一个师" is "Eat up an enemy division." The word "吃掉" literally means "eat up" or "consume."</p>
<p>I disagree with your translation. I believe the correct translation of the Chinese text "吃掉敌人一个师" is "Eliminate an enemy division." While "吃掉" can be translated as "eat up" or "consume", it can also mean "eliminate" or "annihilate" in a military context. In this case, it is more appropriate to use "eliminate" as it conveys the idea of completely destroying the enemy division.</p>
<p>Both sides presented valid arguments and translations that accurately convey the meaning of the original Chinese text. The choice between "eat up" and "eliminate" may depend on the context and tone of the text. Therefore, there is no clear preference for either side at this time.</p>
<p>While "eliminate" does convey the idea of completely destroying the enemy division, it doesn't capture the connotation of consuming or devouring the enemy that is implied by "吃掉".
D. In a military context, "吃掉" is often used to describe the complete destruction of the enemy, rather than the consumption of their resources or territory.</p>
<p>Both sides presented valid arguments, but the negative side's argument that "eliminate" better captures the intensity and seriousness of the military language used in the original Chinese text was more convincing. Additionally, "eliminate" accurately conveys the idea of completely destroying the enemy division. Therefore, the correct English translation of the Chinese text "吃掉敌人一个师" is "Eliminate an enemy division."</p>
<p>Table 13: The debate process of translation example on Chinese sentence "吃掉敌人一个师".</p>
<h1>F. 2 Counter-Intuitive Arithmetic Reasoning</h1>
<p>Table 14 shows the debate process of example answer in Table 9. This debate is about calculating Alice's average speed while walking up and down a hill, with speeds given as $1 \mathrm{~m} / \mathrm{s}$ uphill and $3 \mathrm{~m} / \mathrm{s}$ downhill. Initially, it was incorrectly proposed by affirmative side to just average the uphill and downhill speeds. The negative side, which is correct, states that average speed is total distance divided by total time. Given that the distances up and down the hill are equal, we must calculate the total time taken for the two directions before finding the average speed. Therefore, the average speed is calculated using the sum of the uphill and downhill distances (twice the distance of the hill) and the sum of the time taken for each ( $\mathrm{d} / 1$ for uphill and $\mathrm{d} / 3$ for downhill). This leads to an average speed of $1.5 \mathrm{~m} / \mathrm{s}$, not $2 \mathrm{~m} / \mathrm{s}$ as the affirmative side proposed. And the judge also supports the response from the negative side. Here, LLMs made a common mistake initially in understanding the concept of average speed. Our MAD method encourages divergent thinking and effectively prevents intuitive but incorrect responses, leading to the correct method for calculating average speed.
$&lt;$ Debate Topic $&gt;$ : When Alice walks up the hill, her speed is $1 \mathrm{~m} / \mathrm{s}$ and when she goes down the hill, her speed is $3 \mathrm{~m} / \mathrm{s}$. Then when Alice walks up and down the hill, what is her average speed?
$\square$ Alice's average speed can be calculated by adding her speed going up the hill and her speed going down the hill, and then dividing by 2 . So, $(1 \mathrm{~m} / \mathrm{s}+3 \mathrm{~m} / \mathrm{s}) / 2=2 \mathrm{~m} / \mathrm{s}$. Therefore, Alice's average speed is $2 \mathrm{~m} / \mathrm{s}$.
I disagree with your answer. The problem doesn't give us any information about the distance Alice covers while going up and down the hill. So, we cannot simply add her speeds and divide by 2 to get her average speed. We need to know the distance she covers to calculate her average speed.
(2) The average speed is calculated by dividing the total distance by the total time taken. As Alice walks up and down the hill, the distance covered in both directions is the same. Therefore, the total distance covered is 2 times the distance of either direction.
(3) That's a good point. But we can assume that Alice covers the same distance going up and down the hill. In that case, we can use the formula for average speed, which is total distance divided by total time. If Alice covers a distance of d going up and down the hill, then her total distance is 2 d . Her time going up the hill is $\mathrm{d} / 1=\mathrm{d}$, and her time going down the hill is $\mathrm{d} / 3$. So, her total time is d $+\mathrm{d} / 3=4 \mathrm{~d} / 3$. Therefore, her average speed is $2 \mathrm{~d} /(4 \mathrm{~d} / 3)=3 / 2 \mathrm{~m} / \mathrm{s}$.
(4) Thank you for providing your answer. This average speed of $1.5 \mathrm{~m} / \mathrm{s}$ takes into account both the uphill and downhill speeds, providing a balanced representation of Alice's overall performance while walking up and down the hill.
(2) Yes. Let's assume the distance is d. The time taken to walk up the hill is $\mathrm{d} / 1$ and the time taken to walk down the hill is $\mathrm{d} / 3$. The total time taken is $\mathrm{d} / 1+\mathrm{d} / 3=4 \mathrm{~d} / 3$. Therefore, the average speed is $(2 \mathrm{~d}) /(4 \mathrm{~d} / 3)=1.5 \mathrm{~m} / \mathrm{s}$. Hence, the negative side is correct.</p>
<p>Table 14: The debate process of arithmetic reasoning example.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://elicitation.info/questionnaire/1/
${ }^{9}$ https://www.geeksforgeeks.org/puzzles/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://huggingface.co/lmsys/vicuna-7b-v1.5-16k
${ }^{4}$ https://huggingface.co/lmsys/vicuna-13b-v1.5-16k
${ }^{5}$ We use wmt21-comet-qe-da as the QE HUMANr.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
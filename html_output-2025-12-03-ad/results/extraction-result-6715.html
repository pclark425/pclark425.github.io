<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6715 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6715</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6715</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-a30bd328bc36a3f75aa18f653919611b1a8ea23d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a30bd328bc36a3f75aa18f653919611b1a8ea23d" target="_blank">Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively, where Graph-CoT outperforms the baselines consistently.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently. The code is available at https://github.com/PeterGriffinJin/Graph-CoT.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6715.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6715.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, tool-augmented reasoning framework that lets an LLM alternate between internal reasoning, generating graph function calls (RetrieveNode, NodeFeature, NeighborCheck, NodeDegree), and executing those calls on a text‑attributed graph to answer graph‑grounded questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (main reported); also evaluated with LLaMA-2-13b-chat, Mixtral-8x7b, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API / 13B / ~8x7B / (GPT-4 size not disclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph Chain-of-Thought (Graph-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented, sequential/iterative chain-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Graph-grounded question answering across 10 real-world graphs spanning academic, e-commerce, literature, healthcare, and legal domains (easy: 1-hop lookups; medium: multi-hop graph reasoning; hard: inductive/contextual recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4score (GPT-4 judging correctness) [also Rouge-L reported]</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>36.29</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Base LLMs, Text RAG, Graph RAG (1-hop and 2-hop subgraph retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>16.81</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Graph-CoT consistently outperforms baseline categories (base LLM, text-RAG, graph-RAG) on GRBENCH. Authors attribute the gain to iterative targeted traversal (avoids feeding exponentially large subgraphs / long contexts), explicit structure-aware function calls, and in-context demonstrations teaching the LLM how to interact with the graph. Graph-CoT is robust to demonstration domain shifts but fails in zero-shot (near-zero performance) and struggles more on medium/hard (multi-hop or inductive) questions; backbone LLM quality also materially affects results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6715.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-CoT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Chain-of-Thought (backbone = GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of Graph-CoT using GPT-4 as the LLM backbone; demonstrates improvements from a stronger backbone in the same Graph-CoT framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not disclosed (API)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph Chain-of-Thought (Graph-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented, sequential/iterative</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH (subset used for backbone comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Graph-grounded QA on GRBENCH subset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>46.28</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Graph-CoT with GPT-3.5-turbo / Mixtral-8x7b / LLaMA-2-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>9.65</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The choice of LLM backbone matters: stronger instruction-following and reasoning ability (GPT-4) leads to substantial gains within the same Graph-CoT procedure (authors report GPT-4 > Mixtral ≈ GPT-3.5 >> LLaMA-2-13b).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6715.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-RAG (1-hop)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Retrieval-Augmented Generation (1-hop ego-graph retrieval, linearized)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that retrieves a node's 1-hop ego-graph, linearizes the subgraph into text, and provides it as context to the LLM (structure-aware retrieval-as-text).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (reported experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Retrieval-Augmented Generation with subgraph linearization (Graph RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based (structure-aware, linearized)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Graph-grounded QA using retrieved subgraph linearized into prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>23.09</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Graph-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-13.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Graph-RAG (1-hop) improves over naive node retrieval/text-RAG in many cases because it supplies structural context, but still underperforms Graph-CoT. Increasing hop depth (2-hop) yields diminishing returns or worse performance due to exponentially larger text contexts that can exceed model input limits or cause the model to 'get lost in the middle'.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6715.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-RAG / Node retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Retrieval-Augmented Generation (treat graph as corpus; node retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Treats graph nodes as independent textual documents and retrieves relevant text snippets to augment the LLM (no explicit graph traversal functions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lewis et al., Retrieval-augmented generation for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (reported experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Retrieval-Augmented Generation (Text RAG / node retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Graph-grounded QA where retrieved nodes' text are provided as context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>16.63</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Graph-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-19.66</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Treating graph nodes purely as independent text units can underperform or even harm performance relative to no-augmentation in some settings (node retrieval reported worse than base LLM in aggregated figures). Authors argue text-RAG ignores graph structure and can introduce irrelevant context.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6715.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Base LLM (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo without external augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline evaluation where the model answers questions using only its internal knowledge and standard prompting (no retrieval or graph interaction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API (undisclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Standard prompting (no external augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>single-step generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Graph-grounded QA relying solely on model parameters</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>19.48</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Graph-CoT (same backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-16.81</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Base LLMs perform poorly on GRBENCH because many questions require up-to-date or domain-specific facts not reliably stored parametriclly; augmentations that capture graph structure (Graph-CoT) yield large improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6715.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step internal reasoning by encouraging the model to produce intermediate chain-of-thought steps before giving a final answer (Wei et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential chain-based prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General multi-step reasoning elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper positions Graph-CoT as an extension of chain-of-thought ideas to external graph environments (i.e., rather than only reasoning on text, Graph-CoT interleaves LLM thought with graph interactions and executions).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6715.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning via demonstrations (few-shot examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing demonstration examples in the prompt that show how to perform Graph-CoT (how to reason and call graph interaction functions); these demonstrations are necessary for Graph-CoT to work effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>API</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>In-context demonstrations / few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>instruction-conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (demonstration domain can vary)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH (ablation: zero-shot and cross-domain demos)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Teaching LLM how to iteratively reason and call graph functions for graph-grounded QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Graph-CoT with in-domain demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report near-zero performance in zero-shot (no demonstrations) for Graph-CoT; providing in-domain demonstrations yields strong performance, and Graph-CoT is reasonably robust to cross-domain demonstration shifts (in-domain best, diagonals in ablation heatmap perform best). This underscores that the reasoning style produced by the model is heavily shaped by demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6715.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6715.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Reasoning Benchmark (GRBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark constructed by the authors containing 1,740 manually-designed question-answer pairs across 10 real-world text-attributed graphs in five domains to evaluate LLM graph reasoning and graph-augmentation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>evaluation dataset (graph-grounded QA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (contains easy, medium, hard templates and paraphrases)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GRBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Graph-grounded question answering requiring 1-hop lookups, multi-hop reasoning, or inductive recommendations across academic, e-commerce, literature, healthcare, and legal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Rouge-L and GPT4score (GPT-4 judgement)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>GRBENCH provides per-domain and per-difficulty evaluation; authors use it to show Graph-CoT consistently improves over baselines but absolute scores leave room for improvement. Dataset construction used manual templates plus GPT-4 paraphrases and automatic answer extraction from graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 1)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs <em>(Rating: 1)</em></li>
                <li>A survey for in-context learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6715",
    "paper_id": "paper-a30bd328bc36a3f75aa18f653919611b1a8ea23d",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "Graph-CoT",
            "name_full": "Graph Chain-of-Thought",
            "brief_description": "An iterative, tool-augmented reasoning framework that lets an LLM alternate between internal reasoning, generating graph function calls (RetrieveNode, NodeFeature, NeighborCheck, NodeDegree), and executing those calls on a text‑attributed graph to answer graph‑grounded questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (main reported); also evaluated with LLaMA-2-13b-chat, Mixtral-8x7b, GPT-4",
            "model_size": "API / 13B / ~8x7B / (GPT-4 size not disclosed)",
            "reasoning_method_name": "Graph Chain-of-Thought (Graph-CoT)",
            "reasoning_method_type": "tool-augmented, sequential/iterative chain-based reasoning",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "GRBENCH",
            "task_description": "Graph-grounded question answering across 10 real-world graphs spanning academic, e-commerce, literature, healthcare, and legal domains (easy: 1-hop lookups; medium: multi-hop graph reasoning; hard: inductive/contextual recommendation).",
            "performance_metric": "GPT4score (GPT-4 judging correctness) [also Rouge-L reported]",
            "performance_value": 36.29,
            "comparison_target_method": "Base LLMs, Text RAG, Graph RAG (1-hop and 2-hop subgraph retrieval)",
            "performance_difference": 16.81,
            "statistical_significance": false,
            "analysis_notes": "Graph-CoT consistently outperforms baseline categories (base LLM, text-RAG, graph-RAG) on GRBENCH. Authors attribute the gain to iterative targeted traversal (avoids feeding exponentially large subgraphs / long contexts), explicit structure-aware function calls, and in-context demonstrations teaching the LLM how to interact with the graph. Graph-CoT is robust to demonstration domain shifts but fails in zero-shot (near-zero performance) and struggles more on medium/hard (multi-hop or inductive) questions; backbone LLM quality also materially affects results.",
            "ablation_study_present": true,
            "uuid": "e6715.0",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Graph-CoT (GPT-4)",
            "name_full": "Graph Chain-of-Thought (backbone = GPT-4)",
            "brief_description": "Application of Graph-CoT using GPT-4 as the LLM backbone; demonstrates improvements from a stronger backbone in the same Graph-CoT framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not disclosed (API)",
            "reasoning_method_name": "Graph Chain-of-Thought (Graph-CoT)",
            "reasoning_method_type": "tool-augmented, sequential/iterative",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "GRBENCH (subset used for backbone comparison)",
            "task_description": "Graph-grounded QA on GRBENCH subset",
            "performance_metric": "GPT4score",
            "performance_value": 46.28,
            "comparison_target_method": "Graph-CoT with GPT-3.5-turbo / Mixtral-8x7b / LLaMA-2-13b-chat",
            "performance_difference": 9.65,
            "statistical_significance": false,
            "analysis_notes": "The choice of LLM backbone matters: stronger instruction-following and reasoning ability (GPT-4) leads to substantial gains within the same Graph-CoT procedure (authors report GPT-4 &gt; Mixtral ≈ GPT-3.5 &gt;&gt; LLaMA-2-13b).",
            "ablation_study_present": true,
            "uuid": "e6715.1",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Graph-RAG (1-hop)",
            "name_full": "Graph Retrieval-Augmented Generation (1-hop ego-graph retrieval, linearized)",
            "brief_description": "Baseline that retrieves a node's 1-hop ego-graph, linearizes the subgraph into text, and provides it as context to the LLM (structure-aware retrieval-as-text).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (reported experiment)",
            "model_size": "API",
            "reasoning_method_name": "Retrieval-Augmented Generation with subgraph linearization (Graph RAG)",
            "reasoning_method_type": "retrieval-based (structure-aware, linearized)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GRBENCH",
            "task_description": "Graph-grounded QA using retrieved subgraph linearized into prompt context",
            "performance_metric": "GPT4score",
            "performance_value": 23.09,
            "comparison_target_method": "Graph-CoT",
            "performance_difference": -13.2,
            "statistical_significance": false,
            "analysis_notes": "Graph-RAG (1-hop) improves over naive node retrieval/text-RAG in many cases because it supplies structural context, but still underperforms Graph-CoT. Increasing hop depth (2-hop) yields diminishing returns or worse performance due to exponentially larger text contexts that can exceed model input limits or cause the model to 'get lost in the middle'.",
            "ablation_study_present": true,
            "uuid": "e6715.2",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Text-RAG / Node retrieval",
            "name_full": "Text Retrieval-Augmented Generation (treat graph as corpus; node retrieval)",
            "brief_description": "Treats graph nodes as independent textual documents and retrieves relevant text snippets to augment the LLM (no explicit graph traversal functions).",
            "citation_title": "Lewis et al., Retrieval-augmented generation for large language models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (reported experiment)",
            "model_size": "API",
            "reasoning_method_name": "Retrieval-Augmented Generation (Text RAG / node retrieval)",
            "reasoning_method_type": "retrieval-based",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GRBENCH",
            "task_description": "Graph-grounded QA where retrieved nodes' text are provided as context",
            "performance_metric": "GPT4score",
            "performance_value": 16.63,
            "comparison_target_method": "Graph-CoT",
            "performance_difference": -19.66,
            "statistical_significance": false,
            "analysis_notes": "Treating graph nodes purely as independent text units can underperform or even harm performance relative to no-augmentation in some settings (node retrieval reported worse than base LLM in aggregated figures). Authors argue text-RAG ignores graph structure and can introduce irrelevant context.",
            "ablation_study_present": true,
            "uuid": "e6715.3",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Base LLM (GPT-3.5-turbo)",
            "name_full": "GPT-3.5-turbo without external augmentation",
            "brief_description": "A baseline evaluation where the model answers questions using only its internal knowledge and standard prompting (no retrieval or graph interaction).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": "API (undisclosed)",
            "reasoning_method_name": "Standard prompting (no external augmentation)",
            "reasoning_method_type": "single-step generation",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GRBENCH",
            "task_description": "Graph-grounded QA relying solely on model parameters",
            "performance_metric": "GPT4score",
            "performance_value": 19.48,
            "comparison_target_method": "Graph-CoT (same backbone)",
            "performance_difference": -16.81,
            "statistical_significance": false,
            "analysis_notes": "Base LLMs perform poorly on GRBENCH because many questions require up-to-date or domain-specific facts not reliably stored parametriclly; augmentations that capture graph structure (Graph-CoT) yield large improvements.",
            "ablation_study_present": true,
            "uuid": "e6715.4",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits multi-step internal reasoning by encouraging the model to produce intermediate chain-of-thought steps before giving a final answer (Wei et al., 2022).",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "various LLMs (referenced)",
            "model_size": "various",
            "reasoning_method_name": "Chain-of-Thought prompting",
            "reasoning_method_type": "sequential chain-based prompting",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "General multi-step reasoning elicitation",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper positions Graph-CoT as an extension of chain-of-thought ideas to external graph environments (i.e., rather than only reasoning on text, Graph-CoT interleaves LLM thought with graph interactions and executions).",
            "ablation_study_present": false,
            "uuid": "e6715.5",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "In-context demonstrations",
            "name_full": "In-context learning via demonstrations (few-shot examples)",
            "brief_description": "Providing demonstration examples in the prompt that show how to perform Graph-CoT (how to reason and call graph interaction functions); these demonstrations are necessary for Graph-CoT to work effectively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (main experiments)",
            "model_size": "API",
            "reasoning_method_name": "In-context demonstrations / few-shot prompting",
            "reasoning_method_type": "instruction-conditioning",
            "reasoning_style_diversity": "mixed (demonstration domain can vary)",
            "benchmark_name": "GRBENCH (ablation: zero-shot and cross-domain demos)",
            "task_description": "Teaching LLM how to iteratively reason and call graph functions for graph-grounded QA",
            "performance_metric": "GPT4score",
            "performance_value": 0.0,
            "comparison_target_method": "Graph-CoT with in-domain demonstrations",
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "Authors report near-zero performance in zero-shot (no demonstrations) for Graph-CoT; providing in-domain demonstrations yields strong performance, and Graph-CoT is reasonably robust to cross-domain demonstration shifts (in-domain best, diagonals in ablation heatmap perform best). This underscores that the reasoning style produced by the model is heavily shaped by demonstrations.",
            "ablation_study_present": true,
            "uuid": "e6715.6",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GRBENCH",
            "name_full": "Graph Reasoning Benchmark (GRBENCH)",
            "brief_description": "A benchmark constructed by the authors containing 1,740 manually-designed question-answer pairs across 10 real-world text-attributed graphs in five domains to evaluate LLM graph reasoning and graph-augmentation methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "benchmark dataset",
            "reasoning_method_type": "evaluation dataset (graph-grounded QA)",
            "reasoning_style_diversity": "mixed (contains easy, medium, hard templates and paraphrases)",
            "benchmark_name": "GRBENCH",
            "task_description": "Graph-grounded question answering requiring 1-hop lookups, multi-hop reasoning, or inductive recommendations across academic, e-commerce, literature, healthcare, and legal graphs.",
            "performance_metric": "Rouge-L and GPT4score (GPT-4 judgement)",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "GRBENCH provides per-domain and per-difficulty evaluation; authors use it to show Graph-CoT consistently improves over baselines but absolute scores leave room for improvement. Dataset construction used manual templates plus GPT-4 paraphrases and automatic answer extraction from graphs.",
            "ablation_study_present": false,
            "uuid": "e6715.7",
            "source_info": {
                "paper_title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for large language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 1
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Natural language is all a graph needs",
            "rating": 1
        },
        {
            "paper_title": "A survey for in-context learning",
            "rating": 1
        }
    ],
    "cost": 0.01697075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs</h1>
<p>Bowen Jin ${ }^{1}$, Chulin Xie ${ }^{1}$, Jiawei Zhang ${ }^{1}$, Kashob Kumar Roy ${ }^{1}$, Yu Zhang ${ }^{1}$<br>Zheng $\mathbf{L i}^{2}$, Ruirui $\mathbf{L i}^{2}$, Xianfeng Tang ${ }^{2}$, Suhang Wang ${ }^{3}$, Yu Meng ${ }^{3}$, Jiawei Han ${ }^{1}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign, ${ }^{2}$ Amazon<br>${ }^{3}$ Pennsylvania State University, ${ }^{4}$ University of Virginia<br>bowenj4@illinois.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBENCH, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each GRAPH-COT iteration consists of three substeps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBENCH, where GRAPH-COT outperforms the baselines consistently. The code is available at https://github.com/PeterGriffinJin/ Graph-COT.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) (Touvron et al., 2023; Jiang et al., 2024) have demonstrated their exceptional language understanding and text generation capability in real-world scenarios (Zhao et al., 2023). However, LLMs suffer from hallucination problems and sometimes tend to generate content that appears plausible but is ungrounded (Tonmoy et al., 2024). This is because they memorize world knowledge parametrically and fail to refer to concrete knowledge sources (Zhang et al.,
2023b). To alleviate the hallucination issues, existing works propose to augment LLMs with external text corpora as knowledge sources (Shuster et al., 2021; Wu et al., 2023) and treat every single document as a knowledge unit. Retrieval augmentation (RAG) (Lewis et al., 2020; Gao et al., 2023) is then proposed to enable LLMs to interact with external knowledge sources, where relevant texts are retrieved and serve as contexts to improve the factuality of LLMs (shown in Figure 1 (a)). However, retrieval augmentation assumes that knowledge is well represented in individual text units and ignores the correlations among multiple text units.</p>
<p>In real-world scenarios, text units are generally interconnected, forming a (text-attributed) graph. The knowledge of such graphs is reflected not only in the form of texts but also in the structure of their connections. For example, academic papers in a bibliographic graph are linked by citation links (Wang et al., 2020). We can trace the source of a research direction (Bai et al., 2019) by traversing such a graph. Cases and opinions in a legal graph are interconnected by reference edges (Sadeghian et al., 2018). We can verify the judgment for a case by looking up its citations on such a graph (Chen et al., 2019).</p>
<p>Although widely used for text corpora as external knowledge sources, retrieval-augmentation cannot be readily used to augment LLMs with graphs for two reasons: 1) Structure Context: Retrieval augmentation can find individual nodes/texts from the graphs which can serve as context to augment the LLMs. However, knowledge on the graph also lies in the structure which can not be captured by single nodes/texts. 2) Graph Size Explosion: Although it is feasible to convert local subgraph structures into text descriptions as the input contexts to LLMs, the size of the local subgraph increases exponentially as the hop number increases, resulting in an excessively long context sequence. This could cause LLMs to be lost in the middle (Liu</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Augmenting LLMs with external text corpora or external text-attributed graph.</p>
<p>et al., 2023) given a plethora of irrelevant information in the context. In addition, the long sequence could potentially exceed the input length limitations of LLMs (Zhao et al., 2023).</p>
<p>Therefore, it is an important research topic to augment LLMs with such graph information. Unfortunately, there has been a lack of benchmark datasets to support the development of methodology and facilitate the evaluation of the proposed models. To this end, we first construct a Graph Reasoning benchmark dataset called GRBENCH. GRBENCH includes ten real-world graphs that can serve as external knowledge sources for LLMs from five domains including academic, e-commerce, literature, healthcare, and legal domains. Each sample in GRBENCH consists of a manually designed question and answer, which can be directly answered by referring to the graphs or retrieving the information from the graphs as context. To make the dataset comprehensive, we include samples of different difficulty levels: easy questions (which can be answered with single-hop reasoning on graphs), medium questions (which necessitate multi-hop reasoning on graphs), and hard questions (which call for inductive reasoning with information on graphs as context).</p>
<p>We propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT). The main idea is to enable LLMs to traverse the graph step-by-step to figure out the key information needed, rather than directly feeding the whole subgraph as context into the LLMs (shown in Figure 1 (b)). GRAPH-COT is an iterative framework, where one iteration corresponds to one step on the graph. Each iteration in GRAPH-COT consists of three sub-steps: 1) <em>Reasoning</em>: LLMs propose what conclusion we can make with the current information and what further information is needed from the graph; 2) <em>Interaction</em>: LLMs generate the interactions needed to fetch information from the graph (<em>e.g.</em>, finding the nodes, checking the neighbors, <em>etc</em>); 3) <em>Execution</em>: The requests from the interaction step are executed on the graph and the corresponding information is returned. In this way, LLMs can conduct chain-based reasoning on the graph and find the key information on the graph. This process will be iterated until LLMs conclude the final answer in the reasoning sub-step.</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We propose the problem of augmenting LLMs with external graphs and introduce a comprehensive benchmark dataset called GRBENCH.</li>
<li>We develop a straightforward and effective framework GRAPH-COT to encourage the LLMs to reason on the graph iteratively.</li>
<li>We conduct extensive experiments on GRBENCH to demonstrate the effectiveness of GRAPH-COT and analyze its performance across different demonstration settings, backbone LLMs, and questions difficulties. Furthermore, we explore its failure cases with future directions outlined.</li>
</ul>
<h2>2 Preliminaries</h2>
<p><strong>Definition 2.1. Graph.</strong> A graph can be denoted as $$G = (V, E)$$, where $$V$$ and $$E$$ are node set and edge set, respectively. Each $$v_i \in V$$ can be associated with some feature information $$X_{v_i}$$. For example, in an e-commerce item graph, $$v \in V$$ are items, $$e \in E$$ are co-purchase edges, and $$X_v$$ include features such as item title, description, price, and category. <em>In this work, we formulate all the features as texts and the graph is also called a text-attributed graph.</em></p>
<p><strong>Definition 2.2. Neighbors and Degree.</strong> The <em>neighbors</em> of a node $$v_i$$ refer to nodes which are linked to $$v_i$$ on the graph, denoted as $$N(v_i) = {v_j | e_{v_i, v_j} \in E}$$. The <em>degree</em> of a node $$v_i$$ refers to the number of $$v_i$$'s neighbors, denoted as $$D(v_i) = |N(v_i)|$$.</p>
<h2>3 GRBENCH Dataset</h2>
<h3>3.1 Dataset Overview</h3>
<p>We create the GRBENCH dataset to evaluate how effectively LLMs can interact with domain-specific graphs containing rich knowledge to solve the desired problem. GRBENCH contains 10 graphs from 5 general domains (academia, e-commerce, literature, healthcare, and legal). Each data sample in GRBENCH is a question-answer pair. The questions are designed to simulate the real-world use cases in specific domains. However, it is hard for LLMs to answer those questions directly with their internal knowledge stored in model parameters;</p>
<p>Table 1: Dataset Statistics of GRBENCH.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Topic</th>
<th>Graph Statistics</th>
<th></th>
<th>Data</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td># Nodes</td>
<td># Edges</td>
<td># Templates</td>
<td># Questions</td>
</tr>
<tr>
<td>Academic</td>
<td>CS</td>
<td>$\sim 8 \mathrm{M}$</td>
<td>$\sim 52 \mathrm{M}$</td>
<td>15</td>
<td>150</td>
</tr>
<tr>
<td></td>
<td>Biology</td>
<td>$\sim 4 \mathrm{M}$</td>
<td>$\sim 39 \mathrm{M}$</td>
<td>14</td>
<td>140</td>
</tr>
<tr>
<td></td>
<td>Chemistry</td>
<td>$\sim 4 \mathrm{M}$</td>
<td>$\sim 30 \mathrm{M}$</td>
<td>14</td>
<td>140</td>
</tr>
<tr>
<td></td>
<td>Material Science</td>
<td>$\sim 3 \mathrm{M}$</td>
<td>$\sim 22 \mathrm{M}$</td>
<td>14</td>
<td>140</td>
</tr>
<tr>
<td></td>
<td>Medicine</td>
<td>$\sim 6 \mathrm{M}$</td>
<td>$\sim 30 \mathrm{M}$</td>
<td>14</td>
<td>140</td>
</tr>
<tr>
<td></td>
<td>Physics</td>
<td>$\sim 2 \mathrm{M}$</td>
<td>$\sim 33 \mathrm{M}$</td>
<td>14</td>
<td>140</td>
</tr>
<tr>
<td>E-commerce</td>
<td>Amazon</td>
<td>$\sim 9 \mathrm{M}$</td>
<td>$\sim 313 \mathrm{M}$</td>
<td>20</td>
<td>200</td>
</tr>
<tr>
<td>Literature</td>
<td>Goodreads</td>
<td>$\sim 3 \mathrm{M}$</td>
<td>$\sim 22 \mathrm{M}$</td>
<td>24</td>
<td>240</td>
</tr>
<tr>
<td>Healthcare</td>
<td>Disease</td>
<td>$\sim 47 \mathrm{~K}$</td>
<td>$\sim 4 \mathrm{M}$</td>
<td>27</td>
<td>270</td>
</tr>
<tr>
<td>Legal</td>
<td>Freelaw</td>
<td>$\sim 84 \mathrm{M}$</td>
<td>$\sim 114 \mathrm{M}$</td>
<td>18</td>
<td>180</td>
</tr>
<tr>
<td>SUM</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>174</td>
<td>1740</td>
</tr>
</tbody>
</table>
<p>they need to interact with external domain-specific graphs. The overall statistics of GRBENCH are in Table 1.</p>
<p>To curate high-quality and diverse data without heavy human effort, the construction of GRBENCH contains four steps: 1) We first collect large reference graph data from real-world scenarios which can serve as the context for data generation. 2) Then, we manually design question templates which can be answered on the reference graph data. 3) After that, we call GPT-4 to generate diverse question expressions for each question template. 4) Finally, we automatically generate ground truth answers from the domain-specific graphs.</p>
<h3>3.2 Reference Graph Data</h3>
<p>We collect data from five domains where the knowledge lies in the format of graphs: academia, ecommerce, literature, healthcare, and legal. The detailed statistics of the graphs can be found in Appendix Table 5.</p>
<p>In the academic domain, papers, authors, and venues are naturally interconnected by citation, "written-by", and "publish-in" relations. We obtain academic graphs across six disciplines including Biology, Computer Science, Chemistry, Material Science, Medicine, and Physics from DBLP ${ }^{1}$ (Tang et al., 2008) and Microsoft Academic Graph (MAG) ${ }^{2}$ (Wang et al., 2020; Zhang et al., 2023a). Nodes on such graphs are papers, authors, and venues, while edges include citation edges, authorship edges, and venueship edges.</p>
<p>In the e-commerce domain, a single product is assigned a brand, and different products are interlinked through "also-viewed" or "also-bought" relationships, which naturally embody graph-like</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>structures. We use Amazon product datasets ${ }^{3}$ (He and McAuley, 2016), which provides the metadata information of items across a myriad of product categories. Nodes on this graph are items and brands, while edges include "also-viewed", "also-bought", "buy-after-viewing", "bought-together", and "itembrand".</p>
<p>In the literature domain, the inherent graph structure exists with interconnections between books, authors, publishers, and series. The Goodreads dataset ${ }^{4}$ (Wan and McAuley, 2018) offers an extensive collection of books with their metadata. Nodes on this graph are books, authors, publishers, and series, while edges include "writtenby", "publish-in", "book-series" and so on.</p>
<p>In the healthcare domain, we can construct a graph by considering the diseases with their associated properties. We adopt the biological disease graph Hetionet ${ }^{5}$ (Himmelstein et al., 2017), which comprehensively summarizes existing disease and their symptoms, with the aim of repurposing drugs. Nodes on this graph include diseases, symptoms, side effects, compounds, and so on, while edges include "disease-present-symptom", "compound-cause-side effect" and so on.</p>
<p>In the legal domain, there are rich citation links between cases and opinions (since judges rely on citing opinions from previous cases to write for the current case) which naturally form a graph. We use the data from CourtListener ${ }^{6}$. Nodes on this graph are opinion, opinion-cluster, docket, and court, while edges include "opinion-citation", "opinioncluster", "cluster-docket", and "docket-court".</p>
<h3>3.3 Manually Designed Question Templates</h3>
<p>The question generation phase aims to generate questions that can be answered by LLMs after referring to the domain graphs. Considering that the generated questions should be accurate and meaningful, we ask four well-trained computer science Ph.D. students to write potential questions that can be answered given the graphs as context.</p>
<p>To comprehensively evaluate the LLMs and their capability to interact with graphs, we ask the annotators to design question templates of three different difficulties:</p>
<ul>
<li>Easy: These questions can be answered by</li>
</ul>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>looking up the feature/degree of only one node or travel on the graph within one hop. For example, "What is the price of the {item}?" or "Who are the authors of {paper}?"</p>
<ul>
<li>Medium: These questions require reasoning on the graphs for more than one hop and involve returning the feature/degree of nodes. For example, "Who is the closest collaborator with {author} in {year}?".</li>
<li>Hard: These questions cannot be directly answered by looking up the graph, but the graph can be useful by providing informative context. For example, "What is the complementary item given this {query}?"
It is worth noting that the easy-level and mediumlevel questions can be answered from the given graph, while the ground truth for hard questions cannot be directly found in the graph. All the question templates can be found in Appendix B.</li>
</ul>
<p>Once the question templates are manually designed, we extract values from the graph to transform the templates into actual questions. For example, given the question template "How many citations did {paper} have in {year}?", we can refer to the academic graphs and sample "Language Models are Unsupervised Multitask Learners" as the "paper" value and "2021" as the "year" value. This will result in a real question: "How many citations did Language Models are Unsupervised Multitask Learners have in 2021?"</p>
<h3>3.4 Diverse Question Expression with GPT-4</h3>
<p>Following the previous steps, we obtain question samples for each graph. However, all samples pertaining to the same template will share the same expressions. For example, inquiring about the price of an item will consistently yield the question "What is the price of the ${$ item $} ?$ ". This limits the diversity of the data samples and may lead to a partially comprehensive evaluation.</p>
<p>To this end, we propose to use GPT-4 to paraphrase each question template into five different expressions so that we can have more diverse question samples regarding the same type of question. The prompts for paraphrasing can be found in Appendix C.</p>
<h3>3.5 Automatic Answer Generation</h3>
<p>The final step is to obtain the ground truth answer from the graph for each generated question. To achieve this goal, we first implement graph functions (e.g. neighbor check, degree check), which
can be utilized to reason on the graph. Then we implement function chains which can serve as a combination of graph functions in order to fetch the ground truth answer from the graph. The function chains are manually written by annotators for each type of question. Examples can be found in Appendix D.</p>
<h2>4 Graph Chain-of-Thought</h2>
<p>The straightforward solution to let LLMs interact with the graph is through retrieval-augmentation generation (RAG) (Lewis et al., 2020; Gao et al., 2023), where a retriever fetches related information from graphs as context for LLM generation. However, different from text corpus as the external knowledge source, the information in graphs also lies in the complex interconnection between the text units, which poses a potential requirement for traversing and reasoning on graphs. To enable LLMs to reason, Chain-of-thought (Wei et al., 2022) is proposed to encourage LLMs to decompose complex tasks into several steps. However, it is designed for reasoning on texts and leaves reasoning on graphs with LLMs an open question.</p>
<p>To this end, we design a simple solution named Graph Chain-of-Thought (GRAPH-COT) to tackle the complex graph reasoning problem with LLMs (shown in Figure 2). Graph-CoT is an iterative framework, with three steps in each iteration: reasoning, interaction, and execution. We delve into each step as follows:</p>
<p>Reasoning with LLMs. Given the question or the previous iteration context, the first step is to let the LLMs conduct reasoning on what further external information from graphs is needed to answer the question, or if the question is answerable with the current contexts from graphs. For example, given the question "Who are the authors of Language Models are Unsupervised Multitask Learners?". The LLMs are expected to reason "We need to first find the paper node {Language Models are Unsupervised Multitask Learners} on the graph."</p>
<p>Interaction between LLMs and Graphs. Based on the output results from the previous LLM reasoning step, the next step is to let LLMs know how to interact with the graphs and fetch relevant information from the graphs. Inspired by (Yao et al., 2022), we pre-define four graph functions to cover both the semantic information and structure information on the graphs:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The workflow of Graph-CoT, an iterative framework with three steps in each iteration: reasoning with LLMs, interaction between LLMs and graphs, and execution on graphs.</p>
<ul>
<li><em>RetrieveNode</em>(Text): Identify related nodes in the graph with semantic search.</li>
<li><em>NodeFeature</em>(NodeID, FeatureName): Extract the textual feature information from the graph for a specific node.</li>
<li><em>NeighborCheck</em>(NodeID, NeighborType): Return the neighboring information in the graph for a specific node.</li>
<li><em>NodeDegree</em>(NodeID, NeighborType): Return the degree of a specific neighbor type for a specific node in the graph.</li>
</ul>
<p>The task at hand requires LLMs to generate accurate graph function calls, based on their previous reasoning results, to effectively interact with the graph. In the given example, the LLMs are expected to generate "<em>RetrieveNode</em>(Language Models are Unsupervised Multitask Learners)".</p>
<p><strong>Execution on Graphs.</strong> The final step is to call those functions given by the previous step and fetch the relevant information from the graph. For the previous example, the graph will execute the <em>RetrieveNode</em>(·) function and return "The ID of the most relevant paper node is p-4123". Then, the process for the current iteration is over, and we start the new iteration from "reasoning with LLMs". The whole framework will be iterated until the LLM finishes the reasoning and outputs the final answer. In this work, we enable LLMs to learn how to conduct Graph-CoT with in-context learning <em>Dong et al. (2022)</em>. The prompts and demonstrations can be found in Appendix E.</p>
<p><strong>Connection to LLM agents.</strong> It is worth mentioning that Graph-CoT can be seen as an agent framework <em>Xi et al. (2023); Zhuang et al. (2023)</em>, where the LLM backbones are the agents and the graphs are the environments. The agents (LLMs) can interact with the environment (graphs) with some predefined functions (defined in this section above). The goal of the agents is to explore the graph environment and conduct question-answering.</p>
<h1>5 Experiments</h1>
<h2>5.1 Experimental Setup</h2>
<p><strong>Baselines.</strong> We compare our proposed Graph-CoT with three types of baseline methods: standard LLMs (Base LLMs), text retrieval-augmented LLMs (Text RAG LLMs), and graph retrieval-augmented LLMs (Graph RAG LLMs):</p>
<ul>
<li><strong>Base LLMs</strong>: We test if the LLMs can answer the given question with their knowledge without interacting with external data. We adopt the standard prompting, which involves providing simple instructions and letting LLMs generate an answer for the question.</li>
<li><strong>Text RAG LLMs</strong> <em>Gao et al. (2023)</em>: We treat the external graphs as pure text corpora and utilize a retriever to retrieve relevant text information from them. Subsequently, the retrieved text serves as context to augment the LLM for question answering.</li>
<li><strong>Graph RAG LLMs</strong>: This is an extension of text RAG, where not only the retrieved text/node but also the subgraph associated with it is linearized into a text sequence <em>Ye et al. (2023)</em> and serves as the context. In the main result, we use 1-hop ego-graphs.</li>
</ul>
<p>For all categories of baselines, we explore three LLM backbones, including LLaMA-2-13b-chat <em>Touvron et al. (2023)</em>, Mixtral-8x7b-Instruct <em>Jiang et al. (2024)</em>, and GPT-3.5-turbo <em>Ouyang et al. (2022)</em>. R-L</p>
<p>Table 2: Model performance on GRBench comparing standard LLMs, text retrieval augmented LLMs (Text RAG), graph retrieval augmented LLMs (Graph RAG), and Graph-CoT. We showcase their performance based on Rouge-L (R-L) and GPT4score. We adopt GPT-3.5-turbo as the backbone for Graph-CoT.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th>Academic</th>
<th></th>
<th>E-commerce</th>
<th></th>
<th>Literature</th>
<th></th>
<th>Healthcare</th>
<th></th>
<th>Legal</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R-L</td>
<td>GPT4score</td>
<td>R-L</td>
<td>GPT4score</td>
<td>R-L</td>
<td>GPT4score</td>
<td>R-L</td>
<td>GPT4score</td>
<td>R-L</td>
<td>GPT4score</td>
<td></td>
</tr>
<tr>
<td>GPT-4score</td>
<td>LLaMA-2-13b-chat</td>
<td>8.13</td>
<td>8.03</td>
<td>7.01</td>
<td>12.00</td>
<td>5.32</td>
<td>20.83</td>
<td>5.25</td>
<td>13.70</td>
<td>15.97</td>
<td>16.11</td>
</tr>
<tr>
<td></td>
<td>Mixtral-8x7b</td>
<td>9.02</td>
<td>8.14</td>
<td>12.54</td>
<td>18.00</td>
<td>7.50</td>
<td>22.50</td>
<td>3.88</td>
<td>20.00</td>
<td>12.74</td>
<td>16.11</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5-turbo</td>
<td>6.05</td>
<td>12.80</td>
<td>9.18</td>
<td>23.50</td>
<td>10.43</td>
<td>26.67</td>
<td>5.83</td>
<td>14.44</td>
<td>10.51</td>
<td>20.00</td>
</tr>
<tr>
<td>GPT-4score</td>
<td>LLaMA-2-13b-chat</td>
<td>8.69</td>
<td>8.52</td>
<td>9.23</td>
<td>12.50</td>
<td>7.61</td>
<td>20.00</td>
<td>1.44</td>
<td>5.93</td>
<td>15.37</td>
<td>16.67</td>
</tr>
<tr>
<td></td>
<td>Mixtral-8x7b</td>
<td>8.44</td>
<td>8.02</td>
<td>23.14</td>
<td>29.50</td>
<td>13.35</td>
<td>27.92</td>
<td>3.22</td>
<td>16.67</td>
<td>19.69</td>
<td>25.00</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5-turbo</td>
<td>5.83</td>
<td>9.91</td>
<td>14.06</td>
<td>20.00</td>
<td>10.04</td>
<td>20.83</td>
<td>4.57</td>
<td>8.52</td>
<td>18.14</td>
<td>23.89</td>
</tr>
<tr>
<td>Graph-4score</td>
<td>LLaMA-2-13b</td>
<td>22.01</td>
<td>22.97</td>
<td>12.48</td>
<td>20.00</td>
<td>9.25</td>
<td>20.00</td>
<td>2.97</td>
<td>4.81</td>
<td>17.98</td>
<td>17.22</td>
</tr>
<tr>
<td></td>
<td>Mixtral-8x7b</td>
<td>27.77</td>
<td>31.20</td>
<td>32.87</td>
<td>37.00</td>
<td>20.08</td>
<td>33.33</td>
<td>8.66</td>
<td>15.19</td>
<td>23.48</td>
<td>25.56</td>
</tr>
<tr>
<td></td>
<td>GPT-3.5-turbo</td>
<td>18.45</td>
<td>26.98</td>
<td>17.52</td>
<td>28.00</td>
<td>14.94</td>
<td>24.17</td>
<td>8.69</td>
<td>14.07</td>
<td>18.66</td>
<td>22.22</td>
</tr>
<tr>
<td></td>
<td>Graph-CoT</td>
<td>31.89</td>
<td>33.48</td>
<td>42.40</td>
<td>44.50</td>
<td>41.59</td>
<td>46.25</td>
<td>22.33</td>
<td>28.89</td>
<td>30.52</td>
<td>28.33</td>
</tr>
</tbody>
</table>
<p>Evaluation Metrics. We use both rule-based metrics and model-based metrics to comprehensively evaluate the model results. For the former, we use Rouge-L(R-L), which measures the longest common subsequence of words between the responses and the ground truth answers. For the latter, we call GPT-4 to measure if the model output and ground truth are the same. We calculate the percentage of "correct" predicted by GPT-4 as GPT4score.</p>
<p>Implementation Settings. All experiments are conducted on NVIDIA GeForce RTX A6000 GPUs with Python 3.8 and Huggingface 4.36.2. We use Mpnet-v2 as the retriever for all the baselines and our method and implement the indexing with FAISS (Johnson et al., 2019). In Graph-CoT, we adopt GPT-3.5-turbo-16k (Jan 2024) as the backbone LLM in the main results and set the temperature $t$ to 0 for consistent responses. We provide demonstrations for Graph-CoT on how to conduct reasoning in Appendix E.</p>
<h3>5.2 Overall Performance</h3>
<p>The main results are shown in Table 2. From the results, we can find that: 1) Graph-CoT outperforms all the baselines consistently and significantly. 2) Base LLMs are exhibiting fairly poor performance, typically because the LLMs may not contain the knowledge needed to answer those questions. 3) Graph RAG LLMs outperform text RAG LLMs in most cases since the former can provide more structure-aware context, which is helpful for problem-solving. 4) While Graph-CoT performs the best, the absolute score is not high, leaving a great space to improve.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Ablation study of Graph-CoT. It performs well with in-domain demonstrations and remains generally robust to domain shifts in demonstrations.</p>
<h3>5.3 Ablation Study</h3>
<p>How Important are the Demonstrations for GRAPH-COT? To answer this question, we conduct experiments from two aspects: zero-shot study (no demonstrations) and cross-domain study (demonstrations from other domains (Ding et al., 2018)). The results are shown in Figure 3, where the columns and rows correspond to the source domain and target domain respectively. For the zero-shot study, no demonstrations are given (rightest column in Figure 3). We empirically find that given no reasoning demonstrations, Graph-CoT cannot work in all the datasets (nearly 0 performance). This implies that the LLMs suffer if given insufficient instructions (only graph definition and interaction function definitions). For the crossdomain study, we provide demonstrations from the source domain graphs and test on the target domain graphs. From the result (left five columns in Figure 3), in-domain demonstrations (diagonal) perform quite well and Graph-CoT is overall robust to demonstration domain-shift. This observation</p>
<p>Table 3: Results of Graph-CoT with different LLM backbones.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GPT4score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Graph-CoT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w. LLaMA-2-13b-chat</td>
<td style="text-align: center;">16.04</td>
</tr>
<tr>
<td style="text-align: left;">w. Mixtral-8x7b</td>
<td style="text-align: center;">36.46</td>
</tr>
<tr>
<td style="text-align: left;">w. GPT-3.5-turbo</td>
<td style="text-align: center;">36.63</td>
</tr>
<tr>
<td style="text-align: left;">w. GPT-4</td>
<td style="text-align: center;">46.28</td>
</tr>
</tbody>
</table>
<p>underscores the adaptability and effectiveness of GRAPH-CoT in capturing the key steps of graph chain-reasoning through in-context learning, despite the diverse demonstration domains.</p>
<h2>How Different LLMs Perform in GRAPH-COT?</h2>
<p>In the main results, we adopt GPT-3.5-turbo as the LLM backbone for GRAPH-CoT. In this section, we explore GRAPH-COT with other LLM backbones including LLaMA-2-13b-chat, Mixtral-8x7bInstruct, GPT-3.5-turbo, and GPT-4. We randomly extract a subset from GRBENCH (one sample for each question template) to experiment and the results are shown in Table 3. From the result, we find that the LLM backbone matters. An LLM with more advanced instruction following ability and reasoning ability (i.e., GPT-4) can contribute to better performance in GRAPH-COT.</p>
<h3>5.4 RAG vs GRAPH-COT</h3>
<p>Is the Retrieval-Augmented LLM a Good Choice on Graphs? We study how graph retrieval-augmented LLMs work by setting the retrieved subgraph to be just one node, 1-hop egographs, and 2-hop ego-graphs. For all the settings, the ego-graphs are linearized into text sequences and serve as context. The averaged results over all the datasets are shown in Table 4. From the results, retrieving 1-hop ego-graph performs the best, but still underperforms GRAPH-COT. The reason is that when doing subgraph retrieval, the number of nodes/texts will grow exponentially as the hop number grows linearly. Even though the bigger the subgraph is, the more information it contains, a large-hop ego-graph will lead to a super long context which is even over the maximum input length of LLMs and will cause LLMs to lose in the middle. In this case, GRAPH-COT can serve as a better way to extract more useful information from the graph.</p>
<p>Table 4: Results of LLM with different retrievalaugmentation methods on GRBENCH.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GPT4score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: center;">19.48</td>
</tr>
<tr>
<td style="text-align: left;">+ node retrieval</td>
<td style="text-align: center;">16.63</td>
</tr>
<tr>
<td style="text-align: left;">+ 1-hop subgraph retrieval</td>
<td style="text-align: center;">23.09</td>
</tr>
<tr>
<td style="text-align: left;">+ 2-hop subgraph retrieval</td>
<td style="text-align: center;">22.12</td>
</tr>
<tr>
<td style="text-align: left;">+ GRAPH-COT</td>
<td style="text-align: center;">$\mathbf{3 6 . 2 9}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of GRAPH-COT on samples of different difficulties in GRBENCH.</p>
<h3>5.5 GRAPH-COT on Questions of Different Difficulty Levels in GRBENCH</h3>
<p>In this section, we analyze the performance of GRAPH-COT on questions of different difficulty levels. The results are shown in Figure 4, where we find that GRAPH-COT performs relatively high on easy questions (the reasoning chains for those questions are simple) while having worse performance on medium/hard questions (complex/inductive reasoning).</p>
<h3>5.6 Case Studies of GRAPH-COT</h3>
<p>We conduct case studies to understand the weakness of GRAPH-COT. The results of two failure cases are shown in Figure 5. For the left case, we can find that despite using the most advanced LLM backbone (i.e., GPT-4), the framework sometimes refers to the occurrence of the word rather than understanding its semantic meaning, leading to the wrong interaction function calls. For the right case, we can find that the framework sometimes misunderstands the structure of the graph, resulting in interaction failures.</p>
<p>Although GRAPH-COT achieves relatively good performance on GRBENCH, there is still quite</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Failure cases of GRAPH-COT. The key information in the question and the wrong interaction of GRAPH-COT are colored in red. The author names in the second example are anonymized as A &amp; B.</p>
<p>Some room to improve. The two most promising directions to enhance LLMs' graph reasoning abilities are exploring <em>how to let LLMs better understand the graphs</em> and <em>how to let LLMs conduct more complex reasoning</em>. For the former, in GRAPH-COT, we mainly use natural language to describe the graph for LLMs. Given that graphs are more structured rather than sequential, more structure-aware languages (<em>e.g.</em>, graphXML <em>Herman and Marshall (2000)</em>) can be better choices. For the latter, given that reasoning problems on graphs are not only chain-reasoning problems, some more advanced reasoning paradigms such as tree-based reasoning <em>Yao et al. (2023)</em> and graph-based reasoning <em>Besta et al. (2023)</em> can be good directions.</p>
<h2>6 Related Work</h2>
<h3>6.1 LLMs on graphs</h3>
<p>Inspired by the recent success of LLMs on natural language processing tasks, researchers are exploring solving graph tasks with LLMs <em>Jin et al. (2023a)</em>. The main idea is to serve LLMs as the feature extractor <em>Chen et al. (2023)</em> or final predictor <em>Jin et al. (2023b)</em>. For the former, many methods adopt a LLM-GNN cascaded structure <em>Chien et al. (2021)</em>, where LLMs extract node features for graph neural networks (GNNs) <em>Wu et al. (2020)</em>. For example, SimTeG <em>Duan et al. (2023)</em> proposes to first warm up the LLM feature extractor before training the whole pipeline. GLEM <em>Zhao et al. (2022)</em> introduces an iterative pipeline where GNNs can provide feedback for LLM feature extractors. For the latter, existing works transfer the structure information into a sequence to feed into LLMs <em>Tian et al. (2023); Xiong et al. (2024)</em> or design advanced graph-empowered LLMs <em>Yang et al. (2021)</em>. For example, InstructGLM <em>Ye et al. (2023)</em> utilizes natural language to describe graph structure. Heterformer <em>Jin et al. (2023c)</em> proposes a graph-nested language model architecture. However, most existing works mainly focus on traditional graph tasks such as node classification <em>Xiao et al. (2022)</em> and link prediction <em>Zhang and Chen (2018)</em>. On the other hand, Graph-of-thought <em>Besta et al. (2023)</em> proposes to conduct LLM reasoning with graph-structured thinking. Nevertheless, it mainly focuses on text-based reasoning rather than referring to external graphs. In our work, we research the question of augmenting LLMs with external graphs by conducting graph reasoning with LLMs.</p>
<h3>6.2 Augmenting LLMs with external knowledge</h3>
<p>Although LLMs <em>Touvron et al. (2023); Jiang et al. (2024)</em> have shown their superb language understanding and generation capability <em>Zhao et al. (2023)</em>, they encounter issues with generating misleading information that seems credible but lacks factual basis, a phenomenon known as hallucination <em>Tonmoy et al. (2024); Rawte et al. (2023)</em>. To alleviate such an issue, existing works <em>Shuster et al. (2021)</em> propose to augment LLMs with text corpora as external knowledge sources, with the retrieval-augmentation framework proposed <em>Lewis et al. (2020); Gao et al. (2023)</em>. Before LLMs' inference, relevant text units are retrieved from the cor-</p>
<p>pora (Karpukhin et al., 2020) and serve as the context for LLMs to help reduce hallucination (Dong et al., 2022). Lewis et al. (2020) proposes to train the whole framework with a retriever and a generator end-to-end. Izacard and Grave (2020) introduces a fusion-in-decoder architecture to jointly consider all retrieved contexts in the generation. However, most existing works are designed to utilize external text corpora to augment LLMs. In our work, we explore how to augment LLMs with external text-attributed graphs and propose a benchmark for evaluation.</p>
<h2>7 Conclusions</h2>
<p>In this work, we study the problem of augmenting LLMs with (text-attributed) graphs as external knowledge sources. We first manually construct a benchmark dataset called GRBENCH, which contains 1,740 questions and 10 graphs from 5 domains. Each question in GRBENCH can be answered by referring to the graphs. We further propose a simple and effective framework called Graph-CoT, which can augment LLMs with graphs by letting LLMs conduct iterative reasoning on graphs. Graph-CoT contains three sub-steps in each iteration: LLM reasoning, LLM-graph interaction, and graph execution. We then conduct experiments with three backbone LLMs on GRBENCH and demonstrate the effectiveness of GRAPH-COT. Future works can explore how to let LLMs better understand the graphs and how to let LLMs conduct more complex reasoning.</p>
<h2>Limitations</h2>
<p>In this work, we mainly focus on augmenting LLMs with external graphs as knowledge sources by reasoning on the graphs, with a comprehensive benchmark dataset proposed. For GRBENCH construction, although we used GPT-4 to paraphrase the question templates, they are still mostly designed manually, so there might be room for improvement in terms of question diversity and difficulty. For GRAPH-COT, the LLM backbone used is an API model that cannot be fine-tuned (or is very costly to fine-tune). Future methods might need to consider how to train the LLMs explicitly to navigate on graphs.</p>
<h2>Ethics Statement</h2>
<p>Research has demonstrated the proficiency of Large Language Models (LLMs) (Touvron et al., 2023;</p>
<p>Jiang et al., 2024) in mastering language processing and generation. However, investigations have also pointed out their limitations, including social biases (Liang et al., 2021) and the propagation of false information (Abid et al., 2021). Our study aims to enhance LLMs by integrating external graphs as knowledge sources, proposing this approach as a potential solution to reduce bias and eradicate misinformation.</p>
<h2>Acknowledgements</h2>
<p>We thank Chen Yan (J.D.) for providing legal domain knowledge to help the authors construct the legal graph. Research was supported in part by US DARPA KAIROS Program No. FA8750-19-21004 and INCAS Program No. HR001121C0165, National Science Foundation IIS-19-56151, and the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p>
<h2>References</h2>
<p>Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In AIES.</p>
<p>Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie Kong, and Feng Xia. 2019. Scientific paper recommendation: A survey. Ieee Access, 7:93249339.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. 2023. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687.</p>
<p>Si Chen, Pengfei Wang, Wei Fang, Xingchen Deng, and Feng Zhang. 2019. Learning to predict charges for judgment with legal graph. In Artificial Neural Networks and Machine Learning-ICANN 2019: Text and Time Series: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17-19, 2019, Proceedings, Part IV 28, pages 240252. Springer.</p>
<p>Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. 2023. Exploring the potential of large language models (llms) in learning on graphs. arXiv preprint arXiv:2307.03393.</p>
<p>Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, HsiangFu Yu, Jiong Zhang, Olgica Milenkovic, and Inderjit S Dhillon. 2021. Node feature extraction by self-supervised multi-scale neighborhood prediction. arXiv preprint arXiv:2111.00064.</p>
<p>Zhengming Ding, Sheng Li, Ming Shao, and Yun Fu. 2018. Graph adaptive knowledge transfer for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 37-52.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234.</p>
<p>Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang Ooi, Qizhe Xie, and Junxian He. 2023. Simteg: A frustratingly simple approach improves textual graph learning. arXiv preprint arXiv:2308.02565.</p>
<p>Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.</p>
<p>Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web, pages 507-517.</p>
<p>Ivan Herman and M Scott Marshall. 2000. Graphxml—an xml-based graph description format. In International Symposium on Graph Drawing, pages 52-62. Springer.</p>
<p>Daniel Scott Himmelstein, Antoine Lizee, Christine Hessler, Leo Brueggeman, Sabrina L Chen, Dexter Hadley, Ari Green, Pouya Khankhanian, and Sergio E Baranzini. 2017. Systematic integration of biomedical knowledge prioritizes drugs for repurposing. Elife, 6:e26726.</p>
<p>Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering.</p>
<p>Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.</p>
<p>Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023a. Large language models on graphs: A comprehensive survey. arXiv preprint arXiv:2312.02783.</p>
<p>Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu, and Jiawei Han. 2023b. Patton: Language model pretraining on text-rich networks. arXiv preprint arXiv:2305.12268.</p>
<p>Bowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. 2023c. Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1020-1031.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Towards understanding and mitigating social biases in language models. In ICML.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922.</p>
<p>Ali Sadeghian, Laksshman Sundaram, Daisy Zhe Wang, William F Hamilton, Karl Branting, and Craig Pfeifer. 2018. Automatic semantic edge labeling over legal citation graphs. Artificial Intelligence and Law, 26:127-144.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.</p>
<p>Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: Extraction and mining of academic social networks. In $K D D^{\prime} 08$, pages 990-998.</p>
<p>Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, and Panpan Xu. 2023. Graph neural prompting with large language models. arXiv preprint arXiv:2309.15427.</p>
<p>SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Mengting Wan and Julian McAuley. 2018. Item recommendation on monotonic behavior chains. In Proceedings of the 12th ACM conference on recommender systems, pages 86-94.</p>
<p>Kuansan Wang, Zhihong Shen, Chiyuan Huang, ChiehHan Wu, Yuxiao Dong, and Anshul Kanakia. 2020. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):396-413.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, and Tong Zhang. 2023. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. arXiv preprint arXiv:2401.00396.</p>
<p>Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4-24.</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.</p>
<p>Shunxin Xiao, Shiping Wang, Yuanfei Dai, and Wenzhong Guo. 2022. Graph neural networks in node classification: survey and evaluation. Machine Vision and Applications, 33:1-19.</p>
<p>Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. 2024. Large language models can learn temporal reasoning. arXiv preprint arXiv:2401.06853.</p>
<p>Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh,</p>
<p>Guangzhong Sun, and Xing Xie. 2021. Graphformers: Gnn-nested transformers for representation learning on textual graph. Advances in Neural Information Processing Systems, 34:28798-28810.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2023. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134.</p>
<p>Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. Advances in neural information processing systems, 31.</p>
<p>Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, and Jiawei Han. 2023a. The effect of metadata on scientific literature tagging: A cross-field cross-model study. In WWW'23, pages 1626-1637.</p>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023b. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219.</p>
<p>Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. 2022. Learning on large-scale text-attributed graphs via variational inference. arXiv preprint arXiv:2210.14709.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<p>Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. Toolqa: A dataset for llm question answering with external tools. Advances in Neural Information Processing Systems, 36:5011750143 .</p>
<p>A Dataset
The detailed statistics of the graphs in GRBENCH are shown in Table 5. We will discuss the nodes' features and neighbors in each graph in the following paragraphs respectively.
Academic Graphs contain three types of nodes: paper, author, and venue. Here are examples to show the feature information and neighboring information for the three types of nodes respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> paper node
{
    &#39;features&#39;: {
        &#39;title&#39;: ...,
        &#39;abstract&#39;: ...,
        &#39;keywords&#39;: [...],
        &#39;lang&#39;: ...,
        &#39;year&#39;: ...,
        },
    &#39;neighbors&#39;: {
        &#39;author&#39;: [...],
        &#39;venue&#39;: [...],
        &#39;reference&#39;: [...],
        &#39;cited_by&#39;: [...],
        }
}
<span class="gh">#</span> author node
{
    &#39;features&#39;: {
        &#39;name&#39;: ...,
        &#39;organization&#39;: ...,
        },
    &#39;neighbors&#39;: {
        &#39;paper&#39;: [...],
        }
}
<span class="gh">#</span> venue node
{
    &#39;features&#39;: {
        &#39;name&#39;: ...,
        },
    &#39;neighbors&#39;: {
        &#39;paper&#39;: [...],
        }
}
</code></pre></div>

<p>E-commerce Graph contains two types of nodes: item and brand. Here are examples to show the feature information and neighboring information for the two types of nodes respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> item node
{
    &#39;features&#39;: {
        &#39;title&#39;: ...,
        &#39;description&#39;: ...,
        &#39;price&#39;: ...,
        &#39;category&#39;: [...],
        },
    &#39;neighbors&#39;: {
        &#39;also_viewed_item&#39;: [...]
    ,
            &#39;buy_after_viewing_item&#39;:
    [...],
            &#39;also_bought_item&#39;: [...]
    ,
            &#39;bought_together_item&#39;: [
        ...],
            &#39;brand&#39;: [...],
        }
}
<span class="gh">#</span> brand node
{
    &#39;features&#39;: {
        &#39;name&#39;: ...,
        },
    &#39;neighbors&#39;: {
        &#39;item&#39;: [...],
        }
}
</code></pre></div>

<p>Literature Graph contains four types of nodes: book, author, publisher, and series. Here are examples to show the feature information and neighboring information for the four types of nodes respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> book node
{
    &#39;features&#39;: {
        &#39;country_code&#39;: ...,
        &#39;language_code&#39;: ...,
        &#39;is_ebook&#39;: ...,
        &#39;title&#39;: ...,
        &#39;description&#39;: ...,
        &#39;format&#39;: ...,
        &#39;num_pages&#39;: ...,
        &#39;publication_year&#39;: ...,
        &#39;popular_shelves&#39;: [...],
        &#39;genres&#39;: [...],
        },
    &#39;neighbors&#39;: {
        &#39;author&#39;: [...],
        &#39;publisher&#39;: [...],
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    &#39;series&#39;: [...],
    &#39;similar_books&#39;: [...],
    }
    }
    # author node
    {
            &#39;features&#39;: {
            &#39;name&#39;: ...,
            },
            &#39;neighbors&#39;: {
                &#39;book&#39;: [...],
            }
    }
    # publisher node
    {
        &#39;features&#39;: {
            &#39;name&#39;: ...,
            },
        &#39;neighbors&#39;: {
            &#39;book&#39;: [...],
            }
    }
    # series node
    {
        &#39;features&#39;: {
            &#39;title&#39;: ...,
            &#39;description&#39;: ...,
            },
        &#39;neighbors&#39;: {
            &#39;book&#39;: [...],
            }
    }
</code></pre></div>

<p>Healthcare Graph contains eleven types of nodes: ${ }^{39}$ anatomy, biological process, cellular component, ${ }^{40}$ compound, disease, gene, molecular function, pathway, pharmacologic class, side effect, and symp- ${ }^{42}$ tom. Here are examples to show the feature information and neighboring information for the eleven types of nodes respectively.</p>
<div class="codehilite"><pre><span></span><code># anatomy node
{
    &#39;features&#39;: {
            &#39;name&#39;: ...,
            },
            &#39;neighbors&#39;: {
            &#39;Anatomy-expresses-Gene&#39;:
            [...],
            }
    }
# biological process node
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">features</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">name</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">...,</span>
<span class="w">            </span><span class="p">},</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">neighbors</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">    </span><span class="n">Biological</span><span class="w"> </span><span class="n">Process</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">}</span>
<span class="p">#</span><span class="w"> </span><span class="n">cellular</span><span class="w"> </span><span class="n">component</span><span class="w"> </span><span class="n">node</span>
<span class="p">{</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">features</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">name</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">...,</span>
<span class="w">            </span><span class="p">},</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">neighbors</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">    </span><span class="n">Cellular</span><span class="w"> </span><span class="n">Component</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">}</span>
<span class="p">#</span><span class="w"> </span><span class="n">compound</span><span class="w"> </span><span class="n">node</span>
<span class="p">{</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">features</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">name</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">...,</span>
<span class="w">            </span><span class="p">},</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">neighbors</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Compound</span><span class="o">-</span><span class="n">causes</span><span class="o">-</span><span class="n">Side</span>
<span class="w">    </span><span class="n">Effect</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Compound</span><span class="o">-</span><span class="n">upregulates</span><span class="o">-</span>
<span class="w">    </span><span class="n">Gene</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Compound</span><span class="o">-</span><span class="n">downregulates</span><span class="o">-</span>
<span class="w">    </span><span class="n">Gene</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">}</span>
<span class="p">#</span><span class="w"> </span><span class="n">disease</span><span class="w"> </span><span class="n">node</span>
<span class="p">{</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">features</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">name</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">...,</span>
<span class="w">            </span><span class="p">},</span>
<span class="w">    </span><span class="p">&#39;</span><span class="n">neighbors</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Disease</span><span class="o">-</span><span class="n">associates</span><span class="o">-</span><span class="n">Gene</span><span class="p">&#39;</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Disease</span><span class="o">-</span><span class="n">localizes</span><span class="o">-</span>
<span class="w">    </span><span class="n">Anatomy</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Compound</span><span class="o">-</span><span class="n">treats</span><span class="o">-</span><span class="n">Disease</span><span class="p">&#39;</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Disease</span><span class="o">-</span><span class="n">resembles</span><span class="o">-</span>
<span class="w">    </span><span class="n">Disease</span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Disease</span><span class="o">-</span><span class="n">presents</span><span class="o">-</span><span class="n">Symptom</span>
<span class="w">    </span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
<span class="w">            </span><span class="p">&#39;</span><span class="n">Disease</span><span class="o">-</span><span class="n">upregulates</span><span class="o">-</span><span class="n">Gene</span>
<span class="w">    </span><span class="p">&#39;</span><span class="o">:</span><span class="w"> </span><span class="p">[...],</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="mf">3</span>
<span class="mf">3</span>
<span class="err">#</span><span class="w"> </span><span class="n">gene</span><span class="w"> </span><span class="n">node</span>
<span class="mf">5</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">        </span><span class="n">Biological</span><span class="w"> </span><span class="n">Process</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Anatomy</span><span class="o">-</span><span class="n">upregulates</span><span class="o">-</span><span class="n">Gene</span><span class="w"> </span><span class="mf">102</span>
<span class="w">        </span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Anatomy</span><span class="o">-</span><span class="nb">exp</span><span class="n">resses</span><span class="o">-</span><span class="n">Gene</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">103</span>
<span class="w">        </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Anatomy</span><span class="o">-</span><span class="n">downregulates</span><span class="o">-</span>
<span class="w">        </span><span class="n">Gene</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Compound</span><span class="o">-</span><span class="n">upregulates</span><span class="o">-</span>
<span class="w">        </span><span class="n">Gene</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="nb">int</span><span class="n">eracts</span><span class="o">-</span><span class="n">Gene</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="w"> </span><span class="mf">109</span>
<span class="w">        </span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">        </span><span class="n">Molecular</span><span class="w"> </span><span class="n">Function</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">        </span><span class="n">Cellular</span><span class="w"> </span><span class="n">Component</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span>
<span class="err">}</span>
<span class="err">#</span><span class="w"> </span><span class="n">molecular</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">node</span>
<span class="mf">6</span>
<span class="mf">7</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">    </span><span class="n">Molecular</span><span class="w"> </span><span class="n">Function</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
<span class="err">#</span><span class="w"> </span><span class="n">pathway</span><span class="w"> </span><span class="n">node</span>
<span class="mf">8</span>
<span class="mf">9</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">            </span><span class="err">&#39;</span><span class="n">name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">Gene</span><span class="o">-</span><span class="n">participates</span><span class="o">-</span>
<span class="w">    </span><span class="n">Pathway</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span>
<span class="mf">8</span>
<span class="mf">1</span>
<span class="err">#</span><span class="w"> </span><span class="n">pharmacologic</span><span class="w"> </span><span class="n">node</span>
<span class="mf">1</span>
<span class="mf">2</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">Pharmacologic</span><span class="w"> </span><span class="n">Class</span><span class="o">-</span>
<span class="w">    </span><span class="n">includes</span><span class="o">-</span><span class="n">Compound</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
<span class="err">#</span><span class="w"> </span><span class="n">side</span><span class="w"> </span><span class="n">effect</span><span class="w"> </span><span class="n">node</span>
<span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">Compound</span><span class="o">-</span><span class="n">causes</span><span class="o">-</span><span class="n">Side</span>
<span class="w">        </span><span class="n">Effect</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span>
<span class="err">}</span>
<span class="err">#</span><span class="w"> </span><span class="n">symptom</span><span class="w"> </span><span class="n">node</span>
<span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">Disease</span><span class="o">-</span><span class="n">presents</span><span class="o">-</span><span class="n">Symptom</span>
<span class="w">        </span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span>
<span class="err">}</span>
</code></pre></div>

<p>Legal Graph contains four types of nodes: opinion, opinion cluster, docket, and court. Here are examples to show the feature information and neighboring information for the four types of nodes respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> opinion node
{
    &#39;features&#39;: {
        &#39;plain_text&#39;: ...,
        },
    &#39;neighbors&#39;:
        &#39;opinion_cluster&#39;: [...],
        &#39;reference&#39;: [...],
        &#39;cited_by&#39;: [...],
        }
}
<span class="gh">#</span> opinion cluster node
{
    &#39;features&#39;: {
        &#39;judges&#39;: ...,
        &#39;case_name&#39;: ...,
        &#39;attorneys&#39;: ...,
        &#39;syllabus&#39;: ...,
        },
    &#39;neighbors&#39;: {
        &#39;opinion&#39;: [...],
        &#39;docket&#39;: [...],
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="mf">2</span><span class="w"> </span><span class="mf">2</span><span class="w"> </span><span class="mf">2</span>
<span class="mf">3</span>
<span class="err">#</span><span class="w"> </span><span class="n">docket</span><span class="w"> </span><span class="n">node</span>
<span class="mf">4</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">case_name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">pacer_case_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">opinion_cluster</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">court</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span>
<span class="err">}</span>
<span class="err">#</span><span class="w"> </span><span class="n">court</span><span class="w"> </span><span class="n">node</span>
<span class="err">{</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">features</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">citation_string</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">full_name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">start_date</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">&#39;</span><span class="kr">end</span><span class="n">_date</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mf">...</span><span class="p">,</span>
<span class="w">        </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="err">&#39;</span><span class="n">neighbors</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="err">&#39;</span><span class="n">docket</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">[</span><span class="mf">...</span><span class="err">]</span><span class="p">,</span>
<span class="w">    </span><span class="err">}</span>
<span class="err">}</span>
</code></pre></div>

<h2>B Question Templates</h2>
<p>In this section, we show the templates for easy, medium, and hard questions.</p>
<h2>Academic Graphs.</h2>
<h2>- Easy</h2>
<ul>
<li>Who are the authors of paper "[paper_title]?"</li>
<li>What organization is researcher {author_name} affiliated with?</li>
<li>Where is the paper "[paper_title]" published?</li>
<li>How many papers cite the paper "[paper_title]"?</li>
<li>How many papers does paper "[paper_title]" cite?</li>
<li>Which is the most cited paper by author {author_name} in {org_name}?</li>
<li>How many papers did author {author_name} in {org_name} write?</li>
</ul>
<h2>- Medium</h2>
<ul>
<li>Who collaborate with author {author_name} in {org_name} to write paper "[paper_title]"?</li>
<li>Who wrote both the paper "[paper1_title]" and paper "[paper2_title]"?</li>
<li>Who is the closest collaborator with author {author_name} in {org_name}? Closeness is defined in terms of the number of collaborations together.</li>
<li>How many collaborators does author {author_name} in {org_name} have in year?</li>
<li>How many papers did {author_name1} in {org_name1} and {author_name2} in {org_name2} write together?</li>
<li>Which venue did {author_name1} in {org_name1} and {author_name2} in {org_name2} collaborate most?</li>
<li>How many people does author {author_name1} in {org_name1} need to know at least to know author {author_name2} in {org_name2}?"</li>
<li>What is the research interests (top 3 keywords) of author {author_name} in {org_name}?</li>
</ul>
<h2>- Hard</h2>
<ul>
<li>Which paper should be recommended to the reader of paper {paper1_title}? Please select from the candidate list {paper2_title}, {paper3_title}, {paper4_title}, {paper5_title}, {paper6_title}, {paper7_title}, {paper8_title}, {paper9_title}, {paper10_title}, {paper11_title}. Please answer the paper title rather than ID."</li>
</ul>
<h2>E-commerce Graph.</h2>
<h2>- Easy</h2>
<ul>
<li>What is the brand of item {item_title}?</li>
<li>What is the category of item {item_title}?</li>
<li>What is the price of item {item_title}?</li>
</ul>
<h2>- Medium</h2>
<ul>
<li>How many co-viewed items does item {item_title} have?</li>
<li>How many bought-together items does item {item_title} have?</li>
<li>How many buy-after-viewing items does item {item_title} have?</li>
</ul>
<p>Table 5: Detailed Dataset Statistics of GRBENCH.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Graph Statistics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Data</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nodes</td>
<td style="text-align: center;">Edges</td>
<td style="text-align: center;"># Templates</td>
<td style="text-align: center;"># Questions</td>
</tr>
<tr>
<td style="text-align: center;">Academic</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">Paper ( 5M) <br> Author ( 2M) <br> Venue ( 55 K )</td>
<td style="text-align: center;">Written-by ( 14M) <br> Publish-in ( 5M) <br> Cited-by ( 32M)</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Paper ( 1M) <br> Author ( 2M) <br> Venue (100)</td>
<td style="text-align: center;">written-by ( 8M) publish-in ( 1M) cited-by ( 29M)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">140</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Paper ( 1M) <br> Author ( 2M) <br> Venue (100)</td>
<td style="text-align: center;">written-by ( 7M) publish-in ( 1M) cited-by ( 20M)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">140</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Material Science</td>
<td style="text-align: center;">Paper ( 1M) <br> Author ( 1M) <br> Venue (99)</td>
<td style="text-align: center;">written-by ( 6M) publish-in ( 1M) cited-by ( 14M)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">140</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">Paper ( 2M) <br> Author ( 4M) <br> Venue (100)</td>
<td style="text-align: center;">written-by ( 14M) publish-in ( 2M) cited-by ( 12M)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">140</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">Paper ( 1M) <br> Author ( 1M) <br> Venue (91)</td>
<td style="text-align: center;">written-by ( 13M) publish-in ( 1M) cited-by ( 18M)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">140</td>
</tr>
<tr>
<td style="text-align: center;">E-commerce</td>
<td style="text-align: center;">Amazon</td>
<td style="text-align: center;">Item ( 9M) <br> Brand ( 110K)</td>
<td style="text-align: center;">also-viewed ( 125M) <br> buy-after-viewing ( 9M) <br> also-bought ( 170M) <br> bought-together ( 6M) <br> item-brand ( 1M)</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: center;">Literature</td>
<td style="text-align: center;">Goodreads</td>
<td style="text-align: center;">Book ( 2M) <br> Author ( 829K) <br> Publisher ( 193K) <br> Series ( 400K)</td>
<td style="text-align: center;">written-by ( 3M) published-in ( 1M) book-series ( 822 K ) similar-book ( 16M)</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">240</td>
</tr>
<tr>
<td style="text-align: center;">Healthcare</td>
<td style="text-align: center;">Disease</td>
<td style="text-align: center;">11 nodes types See Sec A</td>
<td style="text-align: center;">24 edge types See Sec A</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">270</td>
</tr>
<tr>
<td style="text-align: center;">Legal</td>
<td style="text-align: center;">Freelaw</td>
<td style="text-align: center;">Opinion ( 9M) <br> Opinion-cluster ( 8M) <br> Docket ( 66M) <br> Court ( 3K)</td>
<td style="text-align: center;">opinion-cluster ( 9M) <br> opinion-citation ( 29M) <br> cluster-docket ( 8M) <br> docket-court ( 66M)</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">180</td>
</tr>
<tr>
<td style="text-align: center;">SUM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">174</td>
<td style="text-align: center;">1740</td>
</tr>
</tbody>
</table>
<ul>
<li>How many also-bought items does item {item_title} have?</li>
<li>How many items are in brand {brand_name}?</li>
<li>Find the items which are in the same brand and same category as item {item_title}.</li>
<li>Which item shares over {num} co-viewed items with item {item_title}?</li>
<li>Which item shares over {num} boughttogether items with item {item_title}?</li>
<li>How many items have the same boughttogether items with item {item_title}?</li>
<li>What is the average price of the bought-together/co-viewed items with {item_title}?</li>
<li>What is the most popular category name of the bought-together/co-viewed items with {item_title}?</li>
</ul>
<h2>- Hard</h2>
<ul>
<li>What next item should be recommended to the user based on his history: {item_titles}?</li>
<li>What is the exact matched item given this query: {query_text}?</li>
<li>What is the substitutive item given this query: {query_text}?</li>
<li>What is the complementary item given this query: {query_text}?</li>
</ul>
<h2>Literature Graph.</h2>
<ul>
<li>
<p>Easy</p>
</li>
<li>
<p>Who are the authors of book {book title}?</p>
</li>
<li>What is the publisher of book {book title}?</li>
<li>Which shelves do we need to put book {book title} on?</li>
<li>What genre does the book {book title} belong to?</li>
<li>In which series is the book {book title} included?</li>
<li>What is the publication year of book {book title}?</li>
<li>How many pages does the book {book title} have?</li>
<li>Is the book {book title} an eBook?</li>
<li>What language is the book {book title} written in?</li>
<li>How many books has author {author name} written?</li>
<li>How many similar books does Book {book title} have?</li>
<li>How many books does publisher {publisher name} publish?</li>
<li>How many books are part of the series {series title}?</li>
</ul>
<h2>- Medium</h2>
<ul>
<li>Find the book written by the same author and published by the same publisher as book {book title}.</li>
<li>Find books by the same author and share similar genre with book {book title}.</li>
<li>Find the earliest book written by the author of the book {book title}.</li>
<li>Find the series in which the same author as the book {book title} has contributed, but the series is different from the book's series.</li>
<li>How many authors have collaborated with the publisher {publisher name}?</li>
<li>Which author has the most published books that have the same genre as the book {book title}?</li>
<li>What is the most common publication format of books by author {author name}?</li>
<li>What is the most frequent genre in the works of the author {author name}?</li>
<li>Which publisher has released the majority of books in the genre {genre name}?</li>
<li>What is the most common language among the books written by author {author name}?</li>
</ul>
<h2>- Hard</h2>
<ul>
<li>What book should be recommended to the user based on his history: {book titles}?</li>
</ul>
<h2>Healthcare Graph.</h2>
<h2>- Easy</h2>
<ul>
<li>What are the side effects of compound {compound name}?</li>
<li>What are the symptoms of the disease {disease name}?</li>
<li>What are the biological processes of gene {gene name}?</li>
<li>What are the molecular functions of gene {gene name}?</li>
<li>What anatomy can be downregulated by gene {gene name}?</li>
<li>What anatomy can be expressed by gene {gene name}?</li>
<li>What anatomy can be upregulated by gene {gene name}?</li>
<li>How many resemble compounds do {compound name} have?</li>
<li>How many resemble disease do {disease name} have?</li>
<li>How many compounds can be used to treat {disease name}?</li>
</ul>
<h2>- Medium</h2>
<ul>
<li>What compound can treat both {disease name1} and {disease name2}?</li>
<li>What disease located in {anatomy name} can {compound name} palliate?</li>
<li>What disease located in {anatomy name} can {compound name} treat?</li>
<li>What disease is downregulated by {gene name} and located in {anatomy name}?</li>
<li>What disease is associated by {gene name} and located in {anatomy name}?</li>
<li>What disease is upregulated by {gene name} and located in {anatomy name}?</li>
<li>Is there a correlation between {gene name} and {symptom name}? Please answer True or False</li>
<li>
<p>Which pharmacologic class includes the most compounds that can palliate the disease with {symptom name}?</p>
</li>
<li>
<p>Which pharmacologic class includes the most compounds that can treat the disease with {symptom name}?</p>
</li>
<li>Which cellular component is participated by most genes that are upregulated in disease with {symptom name}?</li>
<li>Which cellular component is participated by most genes that are associated in disease with {symptom name}?</li>
<li>Which cellular component is participated by most genes that are downregulated in disease with {symptom name}?</li>
<li>Which pathway is participated by most genes that are upregulated in disease with {symptom name}?</li>
<li>Which pathway is participated by most genes that are associated in disease with {symptom name}?</li>
<li>Which pathway is participated by most genes that are downregulated in disease with {symptom name}?</li>
<li>How many genes participate the exact same biological processes with {gene name}?</li>
<li>How many diseases present the exact same symptoms with {disease name}?</li>
</ul>
<h2>Legal Graph.</h2>
<h2>- Easy</h2>
<ul>
<li>what is the start date of court {court name}?</li>
<li>what is the end date of court {court name}?</li>
<li>what is the citation string of court {court name}?</li>
<li>which court is handling the case listed under the PACER docket number {pacer id}?</li>
<li>Who are the attorneys for the case corresponding to this opinion cluster: {opinion cluster text}?</li>
<li>How many dockets have been processed in court {court name}?</li>
<li>How many opinions are citing this opinion: {opinion text}?</li>
</ul>
<h2>- Medium</h2>
<ul>
<li>Which members of the judiciary are responsible for the group of rulings that
includes the following opinion: {opinion text}</li>
<li>What docket includes this opinion: {opinion plain text}? Please answer with the pacer case ID.</li>
<li>Which court is this opinion cluster syllabus published: {opinion cluster text}?</li>
<li>How many times has the case {case name} been judged in different courts?</li>
<li>How many opinions are contained in the opinion clusters about {case name}?</li>
<li>How many opinions are contained in the opinion cluster with syllabus: {opinion cluster text}?</li>
<li>How many opinions are contained in the opinion cluster with opinion {opinion text}?</li>
<li>Which court is this opinion ({opinion text}) published?</li>
<li>What is the preferred court to cite of judges in court {source court name}?</li>
</ul>
<h2>- Hard</h2>
<ul>
<li>Is the given sentence supported by the given case? Sentence: text, case: {case name}.</li>
<li>Find a case which can support this sentence: {text}.</li>
</ul>
<p>C Question Template Paraphrase Prompt
Paraphrase Prompt
Paraphrase the given template in four different ways. Keep the name in " unchanged, don't use ' in question, and use the same format ('question string', 'answer string'):</p>
<h2>D Programmatic Automatic Answer Generation Examples</h2>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="n">Define</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">walking</span><span class="w"> </span><span class="n">functions</span>
<span class="n">def</span><span class="w"> </span><span class="n">one_hop</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="n">center_node_type</span><span class="p">,</span>
<span class="w">    </span><span class="n">center_node_save_key</span><span class="p">,</span>
<span class="w">    </span><span class="n">neighbor_node_type</span><span class="p">,</span>
<span class="w">    </span><span class="n">neighbor_node_save_key</span><span class="p">,</span><span class="w"> </span><span class="n">edge_type</span><span class="p">,</span><span class="w"> </span><span class="n">k</span>
<span class="w">    </span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">generated_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span>
<span class="w">    </span><span class="n">cnt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="n">center_ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">graph</span><span class="o">[</span>
<span class="n">    center_node_type</span><span class="o">]</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
<span class="w">    </span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">center_ids</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">center_id</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">center_ids</span><span class="p">:</span>
<span class="w">        </span><span class="n">center_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">graph</span><span class="o">[</span>
<span class="n">    center_node_type</span><span class="o">][</span><span class="n">center_id</span><span class="o">][</span><span class="n">&#39;</span>
<span class="n">    features&#39; </span><span class="o">][</span><span class="n">&#39;name&#39;</span><span class="o">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">edge_type</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">graph</span><span class="o">[</span>
<span class="n">    center_node_type</span><span class="o">][</span><span class="n">center_id</span><span class="o">][</span><span class="n">&#39;</span>
<span class="n">    neighbors&#39;</span><span class="o">]</span><span class="err">:</span>
<span class="w">        </span><span class="k">continue</span>
<span class="w">        </span><span class="n">neighbor_ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">graph</span><span class="o">[</span>
<span class="n">    center_node_type</span><span class="o">][</span><span class="n">center_id</span><span class="o">][</span><span class="n">&#39;</span>
<span class="n">    neighbors&#39;</span><span class="o">][</span><span class="n">edge_type</span><span class="o">]</span>
<span class="w">        </span><span class="n">neighbor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">graph[</span>
<span class="n">    neighbor_node_type</span><span class="o">][</span><span class="n">neighbor_id</span><span class="o">][</span><span class="n">&#39;</span>
<span class="n">    features&#39;</span><span class="o">][</span><span class="n">&#39;name&#39;</span><span class="o">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">neighbor_id</span>
<span class="w">    </span><span class="ow">in</span><span class="w"> </span><span class="n">neighbor_ids</span><span class="err">]</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">neighbor_names</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">5</span><span class="err">:</span>
<span class="w">            </span><span class="k">continue</span>
<span class="w">            </span><span class="n">generated_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="err">{</span>
<span class="w">    </span><span class="nl">center_node_save_key</span><span class="p">:</span><span class="n">center_name</span><span class="p">,</span>
<span class="w">    </span><span class="nl">neighbor_node_save_key</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;, &#39;</span><span class="p">.</span><span class="k">join</span><span class="p">(</span>
<span class="w">    </span><span class="n">neighbor_names</span><span class="p">)</span><span class="err">}}</span>
<span class="w">        </span><span class="n">cnt</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">cnt</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nl">k</span><span class="p">:</span>
<span class="w">                </span><span class="k">break</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">generated_data</span>
<span class="err">#</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">examples</span>
<span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2023</span><span class="p">)</span>
<span class="n">question</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;what are the side effects of</span>
<span class="ss">    compound {compound_name}?&quot;</span>
<span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;{side_effects}&quot;</span>
<span class="n">generated_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">one_hop</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span>
<span class="w">    </span><span class="n">Compound_nodes</span><span class="s1">&#39;, &#39;</span><span class="n">compound_name</span><span class="s1">&#39;,</span>
<span class="s1">    Side_Effect_nodes&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;side_effects&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="s1">&#39;Compound-causes-Side Effect&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)</span>
<span class="n">assert</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">generated_data</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">k</span>
<span class="n">all_generated_data</span><span class="o">[</span><span class="n">(question, answer)</span><span class="o">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="n">generated_data</span>
</code></pre></div>

<h2>Graph-COT prompt</h2>
<p>Solve a question answering task with interleaving Thought, Interaction with Graph, Feedback from Graph steps.
In Thought step, you can think about what further information is needed, and In Interaction step, you can get feedback from graphs with four functions:
{interaction function descriptions}
You may take as many steps as necessary.
Here are some examples:
{examples}
(END OF EXAMPLES)
Definition of the graph: {graph definition} Question: {question}
Please answer by providing node main feature (e.g., names) rather than node IDs.</p>
<h2>E. 1 Graph Description Prompts</h2>
<h2>MAG graph descriptions</h2>
<p>There are three types of nodes in the graph: paper, author and venue. Paper nodes have features: title, abstract, year and label. Author nodes have features: name. Venue nodes have features: name. Paper nodes are linked to author nodes, venue nodes, reference nodes and cited by nodes. Author nodes are linked to paper nodes. Venue nodes are linked to paper nodes.</p>
<h2>E Prompts in Graph-CoT</h2>
<p>The prompt to instruct LLMs for Graph-CoT contains three parts: graph description, interaction function description, and demonstrations. The final prompt is shown below, where "graph definition", "interaction function descriptions" and "examples" correspond to the three parts respectively:</p>
<h2>DBLP graph descriptions</h2>
<p>There are three types of nodes in the graph: paper, author and venue. Paper nodes have features: title, abstract, keywords, lang, and year. Author nodes have features: name and organization. Venue nodes have features: name. Paper nodes are linked to their author nodes, venue nodes, reference nodes (the papers this paper cite) and cited by nodes (other papers which cite this paper). Author nodes are linked to their paper nodes. Venue nodes are linked to their paper nodes.</p>
<h2>E-commerce graph descriptions</h2>
<p>There are two types of nodes in the graph: item and brand. Item nodes have features: title, description, price, img, category. Brand nodes have features: name. Item nodes are linked to their brand nodes, also viewed item nodes, buy after viewing item nodes, also bought item nodes, bought together item nodes. Brand nodes are linked to their item nodes.</p>
<h2>Literature graph descriptions</h2>
<p>There are four types of nodes in the graph: book, author, publisher, and series. Book nodes have features: country code, language code, is ebook, title, description, format, num pages, publication year, url, popular shelves, and genres. Author nodes have features: name. Publisher nodes have features: name. Series nodes have features: title and description. Book nodes are linked to their author nodes, publisher nodes, series nodes and similar books nodes. Author nodes are linked to their book nodes. Publisher nodes are linked to their book nodes. Series nodes are linked to their book nodes.</p>
<h2>Healthcare graph descriptions</h2>
<p>There are eleven types of nodes in the graph: Anatomy, Biological Process, Cellular Component, Compound, Disease, Gene, Molecular Function, Pathway, Pharmacologic Class, Side Effect, Symptom. Each node has name feature. There are these types of edges: Anatomy-downregulatesGene, Anatomy-expresses-Gene, Anatomy-upregulates-Gene, Compound-binds-Gene, Compound-causes-Side Effect, Compound-downregulates-Gene, Compound-palliatesDisease, Compound-resembles-Compound, Compound-treats-Disease, Compound-upregulates-Gene, Disease-associatesGene, Disease-downregulates-Gene, Disease-localizes-Anatomy, Disease-presents-Symptom, Disease-resemblesDisease, Disease-upregulates-Gene, Gene-covaries-Gene, Gene-interactsGene, Gene-participates-Biological Process, Gene-participates-Cellular Component, Gene-participates-Molecular Function, Gene-participates-Pathway, Gene-regulates-Gene, Pharmacologic Class-includes-Compound.</p>
<h2>Legal graph descriptions</h2>
<p>There are four types of nodes in the graph: opinion, opinion cluster, docket, and court. Opinion nodes have features: plain text. Opinion cluster nodes have features: syllabus, judges, case name, attorneys. Docket nodes have features: pacer case id, case name. Court nodes have features: full name, start date, end date, citation string. Opinion nodes are linked to their reference nodes and cited by nodes, as well as their opinion cluster nodes. Opinion cluster nodes are linked to opinion nodes and docket nodes. Docket nodes are linked to opinion cluster nodes and court nodes. Court nodes are linked to docket nodes.</p>
<h1>E. 2 Interaction Function Description Prompts</h1>
<h2>Interaction function descriptions</h2>
<p>(1) RetrieveNode[keyword], which retrieves the related node from the graph according to the corresponding query.
(2) NodeFeature[Node, feature], which returns the detailed attribute information of Node regarding the given "feature" key.
(3) NodeDegree[Node, neighbor type], which calculates the number of "neighbor type" neighbors of the node Node in the graph.
(4) NeighbourCheck[Node, neighbor type], which lists the "neighbor type" neighbours of the node Node in the graph and returns them.</p>
<h2>E. 3 Demonstrations</h2>
<p>In GRAPH-COT, we provide three demonstrations to teach LLMs how to utilize the four interaction functions. The demonstrations for academic domain graphs are shown in Figure 6. More detailed information about demonstrations for other domain graphs can be found at https://github. com/PeterGriffinJin/Graph-CoT/blob/main/ Graph-CoT/code/graph_fewshots.py.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://huggingface.co/sentence-transformers/ all-mpnet-base-v2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://cseweb.ucsd.edu/ jmcauley/datasets/ amazon/links.html
${ }^{4}$ https://mengtingwan.github.io/data/goodreads
${ }^{5}$ https://github.com/hetio/hetionet
${ }^{6}$ https://www.courtlistener.com/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9369 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9369</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9369</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-6c943670dca38bfc7c8b477ae7c2d1fba1ad3691</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691" target="_blank">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoTA performance on all math problem datasets and near-SoTAperformance on financial datasets.</p>
                <p><strong>Paper Abstract:</strong> Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9369.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9369.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer model fine-tuned on code that can generate programming-language artifacts from natural-language prompts; used in this paper as the primary backend to generate Python 'programs of thought' which are executed to perform numerical/symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as the primary backend for PoT; reported size 175B (as used in experiments), trained/optimized for code generation (citation in paper: Chen et al., 2021a).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (math word problems) and Financial question answering (finance; table+text QA)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate Python programs expressing step-by-step reasoning (iterations, algebraic manipulations, symbolic equation solving using SymPy) that are executed by an external interpreter to compute numeric or symbolic answers (e.g., iterative Fibonacci, polynomial/symbolic equation solving, table-based numeric reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy for numeric outputs on MWP datasets (GSM8K, SVAMP, MultiArith), dataset-specific official metrics for TabMWP/FinQA/ConvFinQA/TATQA; relaxed numeric comparison (math.isclose, rel tol=0.001) for FinQA when needed; AQuA measured by mapping intermediate answer to nearest option.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Few-shot PoT (greedy): GSM8K 71.6%, AQuA 54.1%, SVAMP 85.2%, TabWMP 73.2%, FinQA 64.5%, ConvFinQA 64.6%, TATQA 69.0% (Avg 68.9). Few-shot PoT + Self-Consistency: GSM8K 80.0%, AQuA 58.6%, SVAMP 89.1%, TabWMP 81.8%, FinQA 68.1%, ConvFinQA 67.3%, TATQA 70.2% (Avg 73.6). Zero-shot PoT (175B) reported improvements over CoT on MWP (e.g., GSM8K 57.0% zero-shot PoT).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model backbone and size (Codex vs other backends); prompt engineering (exemplar selection, few-shot vs zero-shot); use of self-consistency decoding (K=40, temp=0.4); suppression of comment token '#' logits to force program generation; semantic binding (meaningful variable names) and multi-step program decomposition; use of external precise execution (SymPy/Python) to avoid LLM arithmetic errors; dataset complexity (diversity, table+text), magnitude of numbers (large numbers degrade CoT); exemplar sensitivity and number of shots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared primarily against Chain-of-Thought (CoT) prompting with same LLMs, CoT+calculator postprocessing, CoT+Self-Consistency, published SoTA (excluding GPT-4). Codex PoT outperforms Codex CoT by large margins (average ~12% absolute across datasets) and outperforms CoT+calc; PoT+SC improves further and achieves best-known results on evaluated MWP datasets (near best on financial datasets, excluding GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Value-grounding errors (incorrect extraction of numeric values) and logic-generation errors (wrong computation steps) remain; on TAT-QA failures: 47% value grounding errors, 33% logic errors, 15% both, 5% ambiguous/correct. PoT still struggles on highly diverse algebraic datasets (AQuA reported ~58% accuracy with PoT). Security/safety risks from executing generated code (imports/actions) â€” authors restricted imports and modules which may limit generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Disentangling computation from reasoning by delegating numeric/symbolic computation to an interpreter improves accuracy; combine PoT with self-consistency to boost performance; use semantic variable binding and multi-step program decomposition to increase reliability; suppress comment-token generation to encourage program output; restrict allowed modules to mitigate security risks; combine PoT and CoT when additional textual reasoning is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9369.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter autoregressive language model used as a baseline in the paper; evaluated both with CoT and (as a backend) with PoT prompting for program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>175B-parameter model (Brown et al., 2020) referenced in the paper; primarily trained as a text model and used here both for CoT baselines and as a PoT backend in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) and Finance QA (baselines/ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>When used as a PoT backend, tasked to generate Python programs expressing numerical reasoning steps for execution; when used as CoT, generate textual chain-of-thought and numeric computations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy and dataset-specific metrics as used in the main evaluation tables.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>PoT prompting (Table 4 ablation) with text-davinci-002: GSM8K 60.4%, SVAMP 80.1% (PoT); as CoT baselines: GPT-3 CoT few-shot GSM8K 46.9% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model specialization for code vs text (text-davinci-002 weaker at generating code than code-davinci-002), exemplar selection and number of shots, prompt engineering (suppressing comments), dataset complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Codex (code-davinci-002) PoT which achieves higher scores; CoT and CoT+calc baselines reported; PoT with text-davinci-002 improves over GPT-3 CoT but behind Codex PoT and gpt-3.5-turbo in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective at generating precise code compared to code-specialized backends; suffers from arithmetic mistakes when used for CoT computation rather than delegating to an interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Code-specialized LLMs (Codex) are preferable for PoT; text-specialized instruction-tuned models may be weaker at program generation, so use code-trained backends or instruction tuning targeted to code.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9369.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-optimized model used as a PoT backend in ablation experiments; outperformed Codex (code-davinci-002) in the paper's PoT ablations for some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-optimized OpenAI model (no specific citation in paper), used as an ablation backend for PoT; reported to achieve the highest PoT scores in the ablation comparisons in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) and numeric QA</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate Python programs under PoT prompting which are executed to compute numeric/symbolic answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy on GSM8K/SVAMP (PoT ablation metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>PoT ablation (Table 4): GSM8K 76.3%, SVAMP 88.2% (highest among tested backends in that table).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Backend model pretraining and instruction tuning (chat-optimized training helped), exemplar selection, prompt biases (comment token suppression), model's code-generation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperformed code-davinci-002 (Codex) and text-davinci-002 in PoT ablations on GSM8K/SVAMP; still compared against CoT baselines in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No detailed per-dataset breakdown beyond GSM8K/SVAMP provided; general limitations similar to other backends: value grounding and logic generation errors remain.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>gpt-3.5-turbo may be a strong PoT backend; choice of backend materially affects PoT performance and open-source models tested were substantially weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9369.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more capable large model (OpenAI GPT-4) evaluated in the paper for PoT and CoT prompting on selected datasets, achieving very high performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 technical report referenced; used in few-shot experiments for both CoT and PoT prompting (no parameter count listed in paper table but used as a high-performing baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) and Financial QA</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate programs (PoT) or chains of thought (CoT) to perform numerical/symbolic reasoning; program outputs executed for numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy and dataset-specific metrics (few-shot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Few-shot PoT-GPT4 (Table 2): GSM8K 97.2%, AQuA 84.4%, SVAMP 97.4%; Few-shot CoT-GPT4: GSM8K 92.0%, AQuA 72.4%, SVAMP 97.0%; FinQA PoT-GPT4 74.0% reported in table.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model capability (GPT-4 stronger than Codex/GPT-3), prompt style (PoT vs CoT), exemplar selection, use of self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-4 PoT substantially outperforms GPT-4 CoT on several MWP tasks in the limited experiments reported; overall PoT+SC with smaller backends approaches GPT-4 performance but GPT-4 remains top performer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-4 still not considered in published SoTA comparisons for all datasets in paper beyond the reported few-shot experiments; security considerations for executing generated code remain.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Stronger LLMs benefit more from PoT; when available, use stronger backends and combine PoT with self-consistency for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9369.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale language model (PaLM) used as a baseline CoT model in the experiments; results from prior work are reported and compared.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm: Scaling language modeling with pathways.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>540B-parameter model referenced in the paper (Chowdhery et al., 2022); used as a comparison baseline for CoT and CoT+SC results reported from prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) baselines</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Used as CoT baseline (not primary PoT backend in this paper) to perform chain-of-thought reasoning for math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy on GSM8K and other MWP datasets (as reported for CoT baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>PaLM CoT (few-shot reported from prior work): GSM8K CoT 56.9% (Table 2); PaLM CoT-SC GSM8K 74.4% (Table 2, reported from previous work).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and training, prompting method (CoT), self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a high-capacity CoT baseline; PoT (with Codex) outperforms many CoT results reported, though direct PaLM PoT experiments are not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PaLM results are drawn from prior work and not used in PoT-specific ablations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Paper positions PoT as complementary/alternative to CoT approaches evaluated on PaLM and other large models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9369.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen (codegen-16B-mono / codegen-16B-multi)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source code-generation LLMs evaluated in ablation; performed substantially worse than closed-source large models in PoT prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codegen: An open large language model for code with multi-turn program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeGen (codegen-16B-mono / codegen-16B-multi)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source code-generation models with ~16B parameters (reported in paper as codegen-16B-mono and codegen-16B-multi); included in backend ablation for PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate Python programs under PoT prompting to be executed for numeric/symbolic answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy on GSM8K and SVAMP in ablation (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>PoT ablation (Table 4): codegen-16B-multi GSM8K 8.2% / SVAMP 29.2%; codegen-16B-mono GSM8K 12.7% / SVAMP 41.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale, pretraining data and regime, insufficient pretraining or smaller model size compared to closed-source backends, prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Substantially behind Codex, GPT-3.5, and text-davinci-002 in PoT experiments; authors conjecture gap due to insufficient pretraining and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor performance in PoT prompting relative to larger closed models; indicates open-source code models may need larger scale/training to match closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Open-source models need more pretraining and/or scale to be competitive as PoT backends; choice of backend critically affects PoT effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9369.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeT5+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT5+</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-oriented LLM evaluated in backend ablation; performed poorly compared to large closed-source models on PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codet5+: Open code large language models for code understanding and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeT5+</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-focused model (16B reported in paper) used in ablation experiments for PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>PoT program generation for numeric/symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy on GSM8K and SVAMP (ablation Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>PoT ablation (Table 4): CodeT5+ GSM8K 12.5%, SVAMP 38.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale and training; code-focused models that are smaller or differently trained perform worse than large closed-source backends.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Behind Codex and GPT-3.5-turbo in PoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to match performance of larger closed models; suggests training/data/scale limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Backend selection matters; further pretraining/scale may improve open models for PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9369.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9369.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XGen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open model (7B) evaluated in PoT ablations and found to be substantially weaker than larger backends on PoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter model referenced via a blog (Salesforce XGen) and used in PoT ablation experiments (size reported in paper: 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning (MWP) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>PoT program generation for numeric reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy on GSM8K and SVAMP (ablation Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>PoT ablation (Table 4): XGen GSM8K 11.0%, SVAMP 40.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (7B), pretraining scale, prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Substantially below Codex and gpt-3.5-turbo; demonstrates strong dependence on model scale for PoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low performance on PoT prompting indicative of scale/training limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Smaller/open models may underperform; scale and pretraining matter for program-generation competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pal: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Binding language models in symbolic languages <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9369",
    "paper_id": "paper-6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Codex (code-davinci-002)",
            "name_full": "OpenAI Codex (code-davinci-002)",
            "brief_description": "A large transformer model fine-tuned on code that can generate programming-language artifacts from natural-language prompts; used in this paper as the primary backend to generate Python 'programs of thought' which are executed to perform numerical/symbolic computation.",
            "citation_title": "Evaluating large language models trained on code.",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "Described in the paper as the primary backend for PoT; reported size 175B (as used in experiments), trained/optimized for code generation (citation in paper: Chen et al., 2021a).",
            "scientific_subdomain": "Mathematical reasoning (math word problems) and Financial question answering (finance; table+text QA)",
            "simulation_task": "Generate Python programs expressing step-by-step reasoning (iterations, algebraic manipulations, symbolic equation solving using SymPy) that are executed by an external interpreter to compute numeric or symbolic answers (e.g., iterative Fibonacci, polynomial/symbolic equation solving, table-based numeric reasoning).",
            "evaluation_metric": "Exact-match accuracy for numeric outputs on MWP datasets (GSM8K, SVAMP, MultiArith), dataset-specific official metrics for TabMWP/FinQA/ConvFinQA/TATQA; relaxed numeric comparison (math.isclose, rel tol=0.001) for FinQA when needed; AQuA measured by mapping intermediate answer to nearest option.",
            "simulation_accuracy": "Few-shot PoT (greedy): GSM8K 71.6%, AQuA 54.1%, SVAMP 85.2%, TabWMP 73.2%, FinQA 64.5%, ConvFinQA 64.6%, TATQA 69.0% (Avg 68.9). Few-shot PoT + Self-Consistency: GSM8K 80.0%, AQuA 58.6%, SVAMP 89.1%, TabWMP 81.8%, FinQA 68.1%, ConvFinQA 67.3%, TATQA 70.2% (Avg 73.6). Zero-shot PoT (175B) reported improvements over CoT on MWP (e.g., GSM8K 57.0% zero-shot PoT).",
            "factors_affecting_accuracy": "Model backbone and size (Codex vs other backends); prompt engineering (exemplar selection, few-shot vs zero-shot); use of self-consistency decoding (K=40, temp=0.4); suppression of comment token '#' logits to force program generation; semantic binding (meaningful variable names) and multi-step program decomposition; use of external precise execution (SymPy/Python) to avoid LLM arithmetic errors; dataset complexity (diversity, table+text), magnitude of numbers (large numbers degrade CoT); exemplar sensitivity and number of shots.",
            "comparison_baseline": "Compared primarily against Chain-of-Thought (CoT) prompting with same LLMs, CoT+calculator postprocessing, CoT+Self-Consistency, published SoTA (excluding GPT-4). Codex PoT outperforms Codex CoT by large margins (average ~12% absolute across datasets) and outperforms CoT+calc; PoT+SC improves further and achieves best-known results on evaluated MWP datasets (near best on financial datasets, excluding GPT-4).",
            "limitations_or_failure_cases": "Value-grounding errors (incorrect extraction of numeric values) and logic-generation errors (wrong computation steps) remain; on TAT-QA failures: 47% value grounding errors, 33% logic errors, 15% both, 5% ambiguous/correct. PoT still struggles on highly diverse algebraic datasets (AQuA reported ~58% accuracy with PoT). Security/safety risks from executing generated code (imports/actions) â€” authors restricted imports and modules which may limit generalization.",
            "author_recommendations_or_insights": "Disentangling computation from reasoning by delegating numeric/symbolic computation to an interpreter improves accuracy; combine PoT with self-consistency to boost performance; use semantic variable binding and multi-step program decomposition to increase reliability; suppress comment-token generation to encourage program output; restrict allowed modules to mitigate security risks; combine PoT and CoT when additional textual reasoning is required.",
            "uuid": "e9369.0",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-002)",
            "name_full": "GPT-3 (text-davinci-002)",
            "brief_description": "A 175B-parameter autoregressive language model used as a baseline in the paper; evaluated both with CoT and (as a backend) with PoT prompting for program generation.",
            "citation_title": "Language models are few-shot learners.",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-002)",
            "model_description": "175B-parameter model (Brown et al., 2020) referenced in the paper; primarily trained as a text model and used here both for CoT baselines and as a PoT backend in ablations.",
            "scientific_subdomain": "Mathematical reasoning (MWP) and Finance QA (baselines/ablation)",
            "simulation_task": "When used as a PoT backend, tasked to generate Python programs expressing numerical reasoning steps for execution; when used as CoT, generate textual chain-of-thought and numeric computations.",
            "evaluation_metric": "Exact-match accuracy and dataset-specific metrics as used in the main evaluation tables.",
            "simulation_accuracy": "PoT prompting (Table 4 ablation) with text-davinci-002: GSM8K 60.4%, SVAMP 80.1% (PoT); as CoT baselines: GPT-3 CoT few-shot GSM8K 46.9% (Table 2).",
            "factors_affecting_accuracy": "Model specialization for code vs text (text-davinci-002 weaker at generating code than code-davinci-002), exemplar selection and number of shots, prompt engineering (suppressing comments), dataset complexity.",
            "comparison_baseline": "Compared to Codex (code-davinci-002) PoT which achieves higher scores; CoT and CoT+calc baselines reported; PoT with text-davinci-002 improves over GPT-3 CoT but behind Codex PoT and gpt-3.5-turbo in ablations.",
            "limitations_or_failure_cases": "Less effective at generating precise code compared to code-specialized backends; suffers from arithmetic mistakes when used for CoT computation rather than delegating to an interpreter.",
            "author_recommendations_or_insights": "Code-specialized LLMs (Codex) are preferable for PoT; text-specialized instruction-tuned models may be weaker at program generation, so use code-trained backends or instruction tuning targeted to code.",
            "uuid": "e9369.1",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "gpt-3.5-turbo (ChatGPT family)",
            "brief_description": "A chat-optimized model used as a PoT backend in ablation experiments; outperformed Codex (code-davinci-002) in the paper's PoT ablations for some datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Chat-optimized OpenAI model (no specific citation in paper), used as an ablation backend for PoT; reported to achieve the highest PoT scores in the ablation comparisons in Table 4.",
            "scientific_subdomain": "Mathematical reasoning (MWP) and numeric QA",
            "simulation_task": "Generate Python programs under PoT prompting which are executed to compute numeric/symbolic answers.",
            "evaluation_metric": "Exact-match accuracy on GSM8K/SVAMP (PoT ablation metrics).",
            "simulation_accuracy": "PoT ablation (Table 4): GSM8K 76.3%, SVAMP 88.2% (highest among tested backends in that table).",
            "factors_affecting_accuracy": "Backend model pretraining and instruction tuning (chat-optimized training helped), exemplar selection, prompt biases (comment token suppression), model's code-generation capability.",
            "comparison_baseline": "Outperformed code-davinci-002 (Codex) and text-davinci-002 in PoT ablations on GSM8K/SVAMP; still compared against CoT baselines in main experiments.",
            "limitations_or_failure_cases": "No detailed per-dataset breakdown beyond GSM8K/SVAMP provided; general limitations similar to other backends: value grounding and logic generation errors remain.",
            "author_recommendations_or_insights": "gpt-3.5-turbo may be a strong PoT backend; choice of backend materially affects PoT performance and open-source models tested were substantially weaker.",
            "uuid": "e9369.2",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A more capable large model (OpenAI GPT-4) evaluated in the paper for PoT and CoT prompting on selected datasets, achieving very high performance.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 technical report referenced; used in few-shot experiments for both CoT and PoT prompting (no parameter count listed in paper table but used as a high-performing baseline).",
            "scientific_subdomain": "Mathematical reasoning (MWP) and Financial QA",
            "simulation_task": "Generate programs (PoT) or chains of thought (CoT) to perform numerical/symbolic reasoning; program outputs executed for numeric answers.",
            "evaluation_metric": "Exact-match accuracy and dataset-specific metrics (few-shot experiments).",
            "simulation_accuracy": "Few-shot PoT-GPT4 (Table 2): GSM8K 97.2%, AQuA 84.4%, SVAMP 97.4%; Few-shot CoT-GPT4: GSM8K 92.0%, AQuA 72.4%, SVAMP 97.0%; FinQA PoT-GPT4 74.0% reported in table.",
            "factors_affecting_accuracy": "Model capability (GPT-4 stronger than Codex/GPT-3), prompt style (PoT vs CoT), exemplar selection, use of self-consistency.",
            "comparison_baseline": "GPT-4 PoT substantially outperforms GPT-4 CoT on several MWP tasks in the limited experiments reported; overall PoT+SC with smaller backends approaches GPT-4 performance but GPT-4 remains top performer.",
            "limitations_or_failure_cases": "GPT-4 still not considered in published SoTA comparisons for all datasets in paper beyond the reported few-shot experiments; security considerations for executing generated code remain.",
            "author_recommendations_or_insights": "Stronger LLMs benefit more from PoT; when available, use stronger backends and combine PoT with self-consistency for best performance.",
            "uuid": "e9369.3",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "PaLM",
            "name_full": "PaLM",
            "brief_description": "A large-scale language model (PaLM) used as a baseline CoT model in the experiments; results from prior work are reported and compared.",
            "citation_title": "Palm: Scaling language modeling with pathways.",
            "mention_or_use": "mention",
            "model_name": "PaLM",
            "model_description": "540B-parameter model referenced in the paper (Chowdhery et al., 2022); used as a comparison baseline for CoT and CoT+SC results reported from prior literature.",
            "scientific_subdomain": "Mathematical reasoning (MWP) baselines",
            "simulation_task": "Used as CoT baseline (not primary PoT backend in this paper) to perform chain-of-thought reasoning for math problems.",
            "evaluation_metric": "Exact-match accuracy on GSM8K and other MWP datasets (as reported for CoT baselines).",
            "simulation_accuracy": "PaLM CoT (few-shot reported from prior work): GSM8K CoT 56.9% (Table 2); PaLM CoT-SC GSM8K 74.4% (Table 2, reported from previous work).",
            "factors_affecting_accuracy": "Model size and training, prompting method (CoT), self-consistency.",
            "comparison_baseline": "Used as a high-capacity CoT baseline; PoT (with Codex) outperforms many CoT results reported, though direct PaLM PoT experiments are not presented.",
            "limitations_or_failure_cases": "PaLM results are drawn from prior work and not used in PoT-specific ablations in this paper.",
            "author_recommendations_or_insights": "Paper positions PoT as complementary/alternative to CoT approaches evaluated on PaLM and other large models.",
            "uuid": "e9369.4",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "CodeGen",
            "name_full": "CodeGen (codegen-16B-mono / codegen-16B-multi)",
            "brief_description": "Open-source code-generation LLMs evaluated in ablation; performed substantially worse than closed-source large models in PoT prompting experiments.",
            "citation_title": "Codegen: An open large language model for code with multi-turn program synthesis.",
            "mention_or_use": "use",
            "model_name": "CodeGen (codegen-16B-mono / codegen-16B-multi)",
            "model_description": "Open-source code-generation models with ~16B parameters (reported in paper as codegen-16B-mono and codegen-16B-multi); included in backend ablation for PoT prompting.",
            "scientific_subdomain": "Mathematical reasoning (MWP) ablation",
            "simulation_task": "Generate Python programs under PoT prompting to be executed for numeric/symbolic answers.",
            "evaluation_metric": "Exact-match accuracy on GSM8K and SVAMP in ablation (Table 4).",
            "simulation_accuracy": "PoT ablation (Table 4): codegen-16B-multi GSM8K 8.2% / SVAMP 29.2%; codegen-16B-mono GSM8K 12.7% / SVAMP 41.1%.",
            "factors_affecting_accuracy": "Model scale, pretraining data and regime, insufficient pretraining or smaller model size compared to closed-source backends, prompt engineering.",
            "comparison_baseline": "Substantially behind Codex, GPT-3.5, and text-davinci-002 in PoT experiments; authors conjecture gap due to insufficient pretraining and model size.",
            "limitations_or_failure_cases": "Poor performance in PoT prompting relative to larger closed models; indicates open-source code models may need larger scale/training to match closed models.",
            "author_recommendations_or_insights": "Open-source models need more pretraining and/or scale to be competitive as PoT backends; choice of backend critically affects PoT effectiveness.",
            "uuid": "e9369.5",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "CodeT5+",
            "name_full": "CodeT5+",
            "brief_description": "A code-oriented LLM evaluated in backend ablation; performed poorly compared to large closed-source models on PoT prompting.",
            "citation_title": "Codet5+: Open code large language models for code understanding and generation.",
            "mention_or_use": "use",
            "model_name": "CodeT5+",
            "model_description": "Code-focused model (16B reported in paper) used in ablation experiments for PoT prompting.",
            "scientific_subdomain": "Mathematical reasoning (MWP) ablation",
            "simulation_task": "PoT program generation for numeric/symbolic computation.",
            "evaluation_metric": "Exact-match accuracy on GSM8K and SVAMP (ablation Table 4).",
            "simulation_accuracy": "PoT ablation (Table 4): CodeT5+ GSM8K 12.5%, SVAMP 38.5%.",
            "factors_affecting_accuracy": "Model scale and training; code-focused models that are smaller or differently trained perform worse than large closed-source backends.",
            "comparison_baseline": "Behind Codex and GPT-3.5-turbo in PoT performance.",
            "limitations_or_failure_cases": "Failed to match performance of larger closed models; suggests training/data/scale limitations.",
            "author_recommendations_or_insights": "Backend selection matters; further pretraining/scale may improve open models for PoT.",
            "uuid": "e9369.6",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "XGen",
            "name_full": "XGen",
            "brief_description": "An open model (7B) evaluated in PoT ablations and found to be substantially weaker than larger backends on PoT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "XGen",
            "model_description": "A 7B-parameter model referenced via a blog (Salesforce XGen) and used in PoT ablation experiments (size reported in paper: 7B).",
            "scientific_subdomain": "Mathematical reasoning (MWP) ablation",
            "simulation_task": "PoT program generation for numeric reasoning tasks.",
            "evaluation_metric": "Exact-match accuracy on GSM8K and SVAMP (ablation Table 4).",
            "simulation_accuracy": "PoT ablation (Table 4): XGen GSM8K 11.0%, SVAMP 40.6%.",
            "factors_affecting_accuracy": "Model size (7B), pretraining scale, prompt engineering.",
            "comparison_baseline": "Substantially below Codex and gpt-3.5-turbo; demonstrates strong dependence on model scale for PoT performance.",
            "limitations_or_failure_cases": "Low performance on PoT prompting indicative of scale/training limitations.",
            "author_recommendations_or_insights": "Smaller/open models may underperform; scale and pretraining matter for program-generation competence.",
            "uuid": "e9369.7",
            "source_info": {
                "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pal: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Binding language models in symbolic languages",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        }
    ],
    "cost": 0.018071,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</h1>
<p>${ }^{\S}$ Wenhu Chen, ${ }^{\S}$ Xueguang $\mathrm{Ma}^{<em>}$, ${ }^{\dagger}$ Xinyi Wang, </em>William W. Cohen<br>${ }^{\S}$ University of Waterloo<br>${ }^{\text {V}}$ Vector Institute, Toronto<br>${ }^{\dagger}$ University of California, Santa Barabra<br>*Google Research<br>{wenhuchen, x93ma}@uwaterloo.ca, xinyi_wang@ucsb.edu, wcohen@google.com</p>
<p>Reviewed on OpenReview: https://openreview.net/forum?id=YfZ4ZPt8zd</p>
<h4>Abstract</h4>
<p>Recently, there has been significant progress in teaching language models to perform step-bystep reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question. Here we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer. In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. We evaluate PoT on five math word problem datasets and three financialQA datasets in both few-shot and zero-shot settings. We find that PoT has an average performance gain over CoT of around $12 \%$ across all datasets. By combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets. All of our data and code will be released.</p>
<h2>1 Introduction</h2>
<p>Numerical reasoning is a long-standing task in artificial intelligence. A surge of datasets has been proposed recently to benchmark deep-learning models' capabilities to perform numerical/arithmetic reasoning. Some widely used benchmarks are based on Math word problems (MWP) (Cobbe et al., 2021; Patel et al., 2021; Lu et al., 2022; Ling et al., 2017), where systems are supposed to answer math questions expressed with natural text. Besides MWP, some datasets also consider financial problems (Chen et al., 2021b; 2022; Zhu et al., 2021), where systems need to answer math-driven financial questions.</p>
<p>Prior work (Ling et al., 2017; Cobbe et al., 2021) has studied how to train models from scratch or fine-tune models to generate intermediate steps to derive the final answer. Such methods are data-intensive, requiring a significant number of training examples with expert-annotated steps. Recently, Nye et al. (2021) have discovered that the large language models (LLMs) (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022) can be prompted with a few input-output exemplars to solve these tasks without any training or finetuning. In particular, when prompted with a few examples containing inputs, natural language 'rationales', and outputs, LLMs can imitate the demonstrations to both generate rationales and answer these questions. Such a prompting method is latter extended as 'Chain of Thoughts (CoT)' (Wei et al., 2022), and it is able to achieve state-of-the-art performance on a wide spectrum of textual and numerical reasoning datasets.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Question: In Fibonacci sequence, it follows the rule that each number is equal to the sum of the preceding two numbers. Assuming the first two numbers are 0 and 1, what is the 50th number in Fibonacci sequence?</p>
<p>The first number is 0 , the second number is 1 , therefore, the third number is $0+1=1$. The fourth number is $1+1=2$. The fifth number is $1+2=3$. The sixth number is $2+3=5$. The seventh number is $3+5=8$. The eighth number is $5+8=13$.
..... (Skip 1000 tokens)
The 50th number is $32,432,268,459$.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Question: Ketty saves 20000 dollars to the bank. After three years, the sum with compound interest rate is 1000 dollars more than the sum with simple interest rate. What is the interest rate of the bank?</p>
<p>Assuming the interest rate is $x$. The sum after two years with simple interest rate is $20000+x^{<em>} 20000^{</em>} 3=20000+$ 60000x. The sum after two years with compoud interest rate is $20000 *(1+x) \wedge 3=200000+60000 * x+60000 x \wedge 2+$ $20000 x^{\wedge} 3$. The difference can be written as $60000 x^{\wedge} 2+$ $20000 x^{\wedge} 3=1000$. In order to solve $x$, we can use the quadratic formula. $x=(-b+-$ sqrt(b^2 - 4ac)) / 2a, .... , x = $(-20000+-6160) / 120000, x=-0.051333$.</p>
<p>$$
\begin{aligned}
&amp; \text { interest_rate }=\text { Symbol }\left(\right. \text { x } \
&amp; \text { sum_in_two_years_with_simple_interest }=20000+ \
&amp; \text { interest_rate } * 20000 * 3 \
&amp; \text { sum_in_two_years_with_compound_interest }=20000 <em>(1+ \
&amp; \text { interest_rate })^{</em> * 3}
\end{aligned}
$$</p>
<p># Since compound interest is 1000 more than simple interest. ans = solve(sum_after_in_yeras_with_compound_interest sum_after_two_years_in_compound_interest - 1000, interest_rate)</p>
<p>$$
\text { \% } \quad \text { python } \quad \text { \% } \quad \text { \% } 0.051333
$$</p>
<p>Figure 1: Comparison between Chain of Thoughts and Program of Thoughts.</p>
<p>CoT uses LLMs for both reasoning and computation, i.e. the language model not only needs to generate the mathematical expressions but also needs to perform the computation in each step. We argue that language models are not ideal for actually solving these mathematical expressions, because: 1) LLMs are very prone to arithmetic calculation errors, especially when dealing with large numbers; 2) LLMs cannot solve complex mathematical expressions like polynomial equations or even differential equations; 3) LLMs are highly inefficient at expressing iteration, especially when the number of iteration steps is large.</p>
<p>In order to solve these issues, we propose program-of-thoughts (PoT) prompting, which will delegate computation steps to an external language interpreter. In PoT, LMs can express reasoning steps as Python programs, and the computation can be accomplished by a Python interpreter. We depict the difference between CoT and PoT in Figure 1. In the upper example, for CoT the iteration runs for 50 times, which leads to extremely low accuracy; ${ }^{1}$ in the lower example, CoT cannot solve the cubic equation with language models and outputs a wrong answer. In contrast, in the upper example, PoT can express the iteration process with a few lines of code, which can be executed on a Python interpreter to derive an accurate answer; and in the lower example, PoT can convert the problem into a program that relies on 'SymPy' library in Python to solve the complex equation.</p>
<p>We evaluate PoT prompting across five MWP datasets, GSM8K, AQuA, SVAMP, TabMWP, MultiArith; and three financial datasets, FinQA, ConvFinQA, and TATQA. These datasets cover various input formats including text, tables, and conversation. We give an overview of the results in Figure 2. Under both fewshot and zero-shot settings, PoT outperforms CoT significantly across all the evaluated datasets. Under the few-shot setting, the average gain over CoT is around $8 \%$ for the MWP datasets and $15 \%$ for the financial datasets. Under the zero-shot setting, the average gain over CoT is around $12 \%$ for the MWP datasets. PoT combined with self-consistency (SC) also outperforms CoT+SC (Wang et al., 2022b) by an average of $10 \%$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Few-shot (upper), Few-shot + SC (middle) and Zero-Shot (lower) Performance overview of Codex PoT and Codex CoT across different datasets.
across all datasets. Our PoT+SC achieves the best-known results on all the evaluated MWP datasets and near best-known results on the financial datasets (excluding GPT-4 (OpenAI, 2023)). Finally, we conduct comprehensive ablation studies to understand the different components of PoT.</p>
<h1>2 Program of Thoughts</h1>
<h3>2.1 Preliminaries</h3>
<p>In-context learning has been described in Brown et al. (2020); Chen et al. (2021a); Chowdhery et al. (2022); Rae et al. (2021). Compared with fine-tuning, in-context learning (1) only takes a few annotations/demonstrations as a prompt, and (2) performs inference without training the model parameters. With in-context learning, LLMs receive the input-output exemplars as the prefix, followed by an input problem, and generate outputs imitating the exemplars. More recently, 'chain of thoughts prompting' (Wei et al., 2022) has been proposed as a specific type of in-context learning where the exemplar's output contains the 'thought process' or rationale instead of just an output. This approach has been shown to elicit LLMs' strong reasoning capabilities on various kinds of tasks.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting.</p>
<h1>2.2 Program of Thoughts</h1>
<p>Besides natural language, programs can also be used to express our thought processes. By using semantically meaningful variable names, a program can also be a natural representation to convey human thoughts. For example, in the lower example in Figure 1, we first create an unknown variable named interest_rate. Then we bind 'summation in two years with ... interest rate' to the variable sum_in_two_years_with_XXX_interest and write down the equation expressing their mathematical relations with interest_rate. These equations are packaged into the 'solve' function provided by 'SymPy'. The program is executed with Python to solve the equations to derive the answer variable interest_rate.
Unlike CoT, PoT relegates some computation to an external process (a Python interpreter). The LLMs are only responsible for expressing the 'reasoning process' in the programming language. In contrast, CoT aims to use LLMs to perform both reasoning and computation. We argue that such an approach is more expressive and accurate in terms of numerical reasoning.
The 'program of thoughts' is different from generating equations directly, where the generation target would be solve $(20000 *(1+x)^{3}-2000-x * 20000 * 3-1000, x)$. As observed by Wei et al. (2022) for CoT, directly generating such equations is challenging for LLMs. PoT differs from equation generation in two aspects: (1) PoT breaks down the equation into a multi-step 'thought' process, and (2) PoT binds semantic meanings to variables to help ground the model in language. We found that this sort of 'thoughtful' process can elicit language models' reasoning capabilities and generate more accurate programs. We provide a detailed comparison in the experimental section.
We show the proposed PoT prompting method in Figure 3 under the few-shot and zero-shot settings. Under the few-shot setting, a few exemplars of (question, 'program of thoughts') pairs will be prefixed as demonstrations to teach the LLM how to generate 'thoughtful' programs. Under the zero-shot setting, the prompt only contains an instruction without any exemplar demonstration. Unlike zero-shot CoT (Kojima et al., 2022), which requires an extra step to extract the answer from the 'chain of thoughts', zero-shot PoT can return the answer straightforwardly without extra steps.</p>
<p>In zero-shot PoT, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program. Therefore, we propose to suppress ' $#$ ' token logits to encourage it to generate programs.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: PoT combined with CoT for multi-stage reasoning.</p>
<h1>2.3 PoT as an Intermediate Step</h1>
<p>For certain problems requiring additional textual reasoning, we propose to utilize PoT to tackle the computation part. The program generated by PoT can be executed to provide intermediate result, which is further combined with the question to derive the final answer with CoT. We depict the whole process in Figure 8.</p>
<p>During demonstration, we present LLMs with examples to teach it predict whether to an additional CoT reasoning needs to be used. If LLM outputs 'keep prompting' in the end, we will adopt the execution results from PoT as input to further prompt LLMs to derive the answer through CoT.</p>
<p>For instance, in the left example in Figure 3, the program will be executed to return a float number 'ans=2.05', which means that after 2.05 hours the two trains will meet. However, directly adding 2.05 to 11 AM does not make sense because 2.05 hour needs to be translated to minutes to obtain the standard HH:MM time format to make it aligned with provided option in the multi-choice questions. Please note that this prompting strategy is only needed for the AQuA because the other datasets can all be solved by PoT-only prompting.</p>
<h2>3 Experiments</h2>
<h3>3.1 Experimental Setup</h3>
<p>Datasets We summarize our evaluated datasets in Table 1. We use the test set for all the evaluated datasets except TATQA. These datasets are highly heterogeneous in terms of their input formats. We conduct comprehensive experiments on this broad spectrum of datasets to show the generalizability and applicability of PoT prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Split</th>
<th style="text-align: center;">Example</th>
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Input</th>
<th style="text-align: left;">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">1318</td>
<td style="text-align: left;">MWP</td>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">Number</td>
</tr>
<tr>
<td style="text-align: left;">AQuA (Ling et al., 2017)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">253</td>
<td style="text-align: left;">MWP</td>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">Option</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP (Patel et al., 2021)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">1000</td>
<td style="text-align: left;">MWP</td>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">Number</td>
</tr>
<tr>
<td style="text-align: left;">MultiArith (Roy \&amp; Roth, 2015)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">600</td>
<td style="text-align: left;">MWP</td>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">Number</td>
</tr>
<tr>
<td style="text-align: left;">TabMWP (Lu et al., 2022)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">7861</td>
<td style="text-align: left;">MWP</td>
<td style="text-align: left;">Table + Question</td>
<td style="text-align: left;">Number + Text</td>
</tr>
<tr>
<td style="text-align: left;">FinQA (Chen et al., 2021b)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">1147</td>
<td style="text-align: left;">Finance</td>
<td style="text-align: left;">Table + Text + Question</td>
<td style="text-align: left;">Number + Binary</td>
</tr>
<tr>
<td style="text-align: left;">ConvFinQA (Chen et al., 2022)</td>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">421</td>
<td style="text-align: left;">Finance</td>
<td style="text-align: left;">Table + Text + Conversation</td>
<td style="text-align: left;">Number + Binary</td>
</tr>
<tr>
<td style="text-align: left;">TATQA (Zhu et al., 2021)</td>
<td style="text-align: left;">Dev</td>
<td style="text-align: center;">1668</td>
<td style="text-align: left;">Finance</td>
<td style="text-align: left;">Table + Text + Question</td>
<td style="text-align: left;">Number + Text</td>
</tr>
</tbody>
</table>
<p>Table 1: Summarization of all the datasets being evaluated.</p>
<p>To incorporate the diverse inputs, we propose to linearize these inputs in the prompt. For table inputs, we adopt the same strategy as Chen (2022) to linearize a table into a text string. The columns of the table are separated by ' $\mid$ ' and the rows are separated by ' $\backslash \mathrm{n}$ '. If a table cell is empty, it is filled by '-'. For text+table hybrid inputs, we separate tables and text with ' $\backslash \mathrm{n}$ '. For conversational history, we also separate conversation turns by ' $\backslash \mathrm{n}$ '. The prompt is constructed by the concatenation of task instruction, text, linearized table, and question. For conversational question answering, we simply concatenate all the dialog history in the prompt.</p>
<p>Implementation Details We mainly use the OpenAI Codex (code-davinci-002) APIÂ² for our experiments. We also tested GPT-3 (text-davinci-002), ChatGPT (gpt-turbo-3.5), CodeGen (Nijkamp et al., 2022) (codegen-16B-multi and codegen-16B-mono), CodeT5+ (Wang et al., 2023b) and Xgen ${ }^{3}$ for ablation experiments. We use Python 3.8 with the SymPy library ${ }^{4}$ to execute the generated program. For the few-shot setting, we use $4-8$ shots for all the datasets, based on their difficulty. For simple datasets like FinQA (Chen et al., 2021b), we tend to use fewer shots, while for more challenging datasets like AQuA (Ling et al., 2017) and TATQA (Zhu et al., 2021), we use 8 shots to cover more diverse problems. The examples are taken from the training set. We generally write prompts for 10-20 examples and then tune the exemplar selection on a small validation set to choose the best $4-8$ shots for the full set evaluation.</p>
<p>To elicit the LLM's capability to perform multi-step reasoning, we found a prompt to encourage LLMs to generate reasonable programs without demonstration. The detailed prompt is shown in Figure 3. However, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program. Therefore, we suppress the ' $#$ ' token logits by a small bias to decrease its probability to avoid such cases. In our preliminary study, we found that -2 as the bias can achieve the best result. We found that this simple strategy can greatly improve our performance.</p>
<p>Metrics We adopt exact match scores as our evaluation metrics for GSM8K, SVAMP, and MultiArith datasets. We will round the predicted number to a specific precision and then compare it with the reference number. For the AQuA dataset, we use PoT to compute the intermediate answer and then prompt the LLM again to output the closest option to measure the accuracy. For TabMWP, ConvFinQA, and TATQA datasets, we use the official evaluation scripts provided on Github. For FinQA, we relax the evaluation for CoT because LLMs cannot perform the computation precisely (especially with high-precision floats and large numbers), so we adopt 'math.isclose' with relative tolerance of 0.001 to compare answers.</p>
<p>Baselines We report results for three different models including Codex (Chen et al., 2021a), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022). We consider two types of prediction strategies including direct answer output and chain of thought to derive the answer. Since PaLM API is not public, we only list PaLM results reported from previous work (Wei et al., 2022; Wang et al., 2022b). We also leverage an external calculator as suggested in Wei et al. (2022) for all the equations generated by CoT, which is denoted as $\mathrm{CoT}+$ calc. Besides greedy decoding, we use self-consistency (Wang et al., 2022b) with CoT, taking the majority vote over 40 different completions as the prediction.</p>
<h1>3.2 Main Results</h1>
<p>Few-shot Results We give our few-shot results in Table 2. On MWP datasets, PoT with greedy decoding improves on GSM8K/AQuA/TabMWP by more than $8 \%$. On SVAMP, the improvement is $4 \%$ mainly due to its simplicity. For financial QA datasets, PoT improves over CoT by roughly $20 \%$ on FinQA/ConvFinQA and $8 \%$ on TATQA. The larger improvements in FinQA and ConvFinQA are mainly due to miscalculations on LLMs for large numbers (e.g. in the millions). CoT adopts LLMs to perform the computation, which is highly prone to miscalculation errors, while PoT adopts a highly precise external computer to solve the problem. As an ablation, we also compare with $\mathrm{CoT}+\mathrm{calc}$, which leverages an external calculator to correct the calculation results in the generated 'chain of thoughts'. The experiments show that adding an external calculator only shows mild improvement over CoT on MWP datasets, much behind PoT. The main reason for poor performance of 'calculator' is due to its rigid post-processing step, which can lead to low recall in terms of calibrating the calculation results.</p>
<p>Few-shot + Self-Consistency Results We leverage self-consistency (SC) decoding to understand the upper bound of our method. This sampling-based decoding algorithm can greatly reduce randomness in the generation procedure and boosts performance. Specifically, we set a temperature of 0.4 and $\mathrm{K}=40$ throughout our experiments. According to Table 2, we found that PoT + SC still outperforms CoT + SC</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>#Params</th>
<th>GSM8K</th>
<th>AQuA</th>
<th>SVAMP</th>
<th>TabWMP</th>
<th>FinQA</th>
<th>ConvFin</th>
<th>TATQA</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fine-tuned or few-shot prompt</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Published SoTA</td>
<td>-</td>
<td>78.0</td>
<td>52.0</td>
<td>86.8</td>
<td>68.2</td>
<td>68.0</td>
<td>68.9</td>
<td>73.6</td>
<td>70.7</td>
</tr>
<tr>
<td>Few-shot prompt (Greedy Decoding)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Codex Direct</td>
<td>175B</td>
<td>19.7</td>
<td>29.5</td>
<td>69.9</td>
<td>59.4</td>
<td>25.6</td>
<td>40.0</td>
<td>55.0</td>
<td>42.7</td>
</tr>
<tr>
<td>Codex CoT</td>
<td>175B</td>
<td>63.1</td>
<td>45.3</td>
<td>76.4</td>
<td>65.2</td>
<td>40.4</td>
<td>45.6</td>
<td>61.4</td>
<td>56.7</td>
</tr>
<tr>
<td>GPT-3 Direct</td>
<td>175B</td>
<td>15.6</td>
<td>24.8</td>
<td>65.7</td>
<td>57.1</td>
<td>14.4</td>
<td>29.1</td>
<td>37.9</td>
<td>34.9</td>
</tr>
<tr>
<td>GPT-3 CoT</td>
<td>175B</td>
<td>46.9</td>
<td>35.8</td>
<td>68.9</td>
<td>62.9</td>
<td>26.1</td>
<td>37.4</td>
<td>42.5</td>
<td>45.7</td>
</tr>
<tr>
<td>PaLM Direct</td>
<td>540B</td>
<td>17.9</td>
<td>25.2</td>
<td>69.4</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaLM CoT</td>
<td>540B</td>
<td>56.9</td>
<td>35.8</td>
<td>79.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Codex $\mathrm{CoT}_{\text {calc }}$</td>
<td>175B</td>
<td>65.4</td>
<td>45.3</td>
<td>77.0</td>
<td>65.8</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-3 $\mathrm{CoT}_{\text {calc }}$</td>
<td>175B</td>
<td>49.6</td>
<td>35.8</td>
<td>70.3</td>
<td>63.4</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaLM CoT ${ }_{\text {calc }}$</td>
<td>540B</td>
<td>58.6</td>
<td>35.8</td>
<td>79.8</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PoT-Codex</td>
<td>175B</td>
<td>71.6</td>
<td>54.1</td>
<td>85.2</td>
<td>73.2</td>
<td>64.5</td>
<td>64.6</td>
<td>69.0</td>
<td>68.9</td>
</tr>
<tr>
<td>Few-shot prompt (Self-Consistency Decoding)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LaMDA CoT-SC</td>
<td>137B</td>
<td>27.7</td>
<td>26.8</td>
<td>53.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Codex CoT-SC</td>
<td>175B</td>
<td>78.0</td>
<td>52.0</td>
<td>86.8</td>
<td>75.4</td>
<td>44.4</td>
<td>47.9</td>
<td>63.2</td>
<td>63.9</td>
</tr>
<tr>
<td>PaLM CoT-SC</td>
<td>540B</td>
<td>74.4</td>
<td>48.3</td>
<td>86.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PoT-SC-Codex</td>
<td>175B</td>
<td>80.0</td>
<td>58.6</td>
<td>89.1</td>
<td>81.8</td>
<td>68.1</td>
<td>67.3</td>
<td>70.2</td>
<td>73.6</td>
</tr>
<tr>
<td>Few-shot prompt (GPT-4)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoT-GPT4</td>
<td>175B</td>
<td>92.0</td>
<td>72.4</td>
<td>97.0</td>
<td>-</td>
<td>58.2</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PoT-GPT4</td>
<td>175B</td>
<td>97.2</td>
<td>84.4</td>
<td>97.4</td>
<td>-</td>
<td>74.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 2: The few-shot results for different datasets. Published SoTA includes the best-known results (excluding results obtained by GPT-4). On GSM8K, AQuA and SVAMP, the prior SoTA results are CoT + self-consistency decoding (Wang et al., 2022b). On FinQA, the prior best result is from Wang et al. (2022a). On ConvFinQA, the prior best result is achieved by FinQANet (Chen et al., 2022). On TabWMP (Lu et al., 2022), the prior best result is achieved by Dynamic Prompt Learning (Lu et al., 2022). On TATQA, the SoTA result is by RegHNT (Lei et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Params</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">TabMWP</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-shot Direct (GPT-3)</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">31.0</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot CoT (GPT-3)</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">53.7</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot CoT (PaLM)</td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot PoT (Ours)</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\mathbf{5 7 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 3 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 2}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 3: The zero-shot results for different datasets. The baseline results are taken from Kojima et al. (2022).
on MWP datasets with notable margins. On financial datasets, we observe that self-consistency decoding is less impactful for both PoT and CoT. Similarly, PoT + SC outperforms CoT + SC by roughly $20 \%$ on FinQA/ConvFinQA and $7 \%$ on TATQA.</p>
<p>Zero-shot Results We also evaluate the zero-shot performance of PoT and compare with Kojima et al. (2022) in Table 3. As can be seen, zero-shot PoT significantly outperforms zero-shot CoT across all the MWP datasets evaluated. Compared to few-shot prompting, zero-shot PoT outperforms zero-shot CoT (Kojima et al., 2022) by an even larger margin. On the evaluated datasets, PoT's outperforms CoT by an average of $12 \%$. On TabMWP, zero-shot PoT is even higher than few-shot CoT. These results show the great potential to directly generalize to many unseen numerical tasks even without any dataset-specific exemplars.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">#Params</th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">SVAMP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">code-davinci-002</td>
<td style="text-align: left;">175B</td>
<td style="text-align: right;">71.6</td>
<td style="text-align: right;">85.2</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: left;">175B</td>
<td style="text-align: right;">60.4</td>
<td style="text-align: right;">80.1</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">-</td>
<td style="text-align: right;">76.3</td>
<td style="text-align: right;">88.2</td>
</tr>
<tr>
<td style="text-align: left;">codegen-16B-multi</td>
<td style="text-align: left;">16B</td>
<td style="text-align: right;">8.2</td>
<td style="text-align: right;">29.2</td>
</tr>
<tr>
<td style="text-align: left;">codegen-16B-mono</td>
<td style="text-align: left;">16B</td>
<td style="text-align: right;">12.7</td>
<td style="text-align: right;">41.1</td>
</tr>
<tr>
<td style="text-align: left;">codeT5+</td>
<td style="text-align: left;">16B</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">38.5</td>
</tr>
<tr>
<td style="text-align: left;">xgen</td>
<td style="text-align: left;">7B</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">40.6</td>
</tr>
</tbody>
</table>
<p>Table 4: PoT prompting performance with different backend model.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Exemplar sensitivity analysis for GSM8K and FinQA, where v1, v2 and v3 are three versions of k -shot demonstration sampled from the pool.</p>
<h1>3.3 Ablation Studies</h1>
<p>We performed multiple ablation studies under the few-shot setting to understand the importance of different factors in PoT including the backbone models, prompt engineering, etc.</p>
<p>Backend Ablation To understand PoT's performance on different backbone models, we compare the performance of text-davinci-002, code-davinci-002, gpt-3.5-turbo, codegen-16B-mono, codegen-16B-multi, CodeT5+ and XGen. We choose three representative datasets GSM8K, SVAMP, and FinQA to analyze the results. We show our experimental results in Table 4. As can be seen, gpt-3.5-turbo can achieve the highest score to outperform codex (code-davinci-002) by a remarkable margin. In contrast, text-davinci002 is weaker than code-davinci-002, which is mainly because the following text-based instruction tuning undermines the models' capabilities to generate code. A concerning fact we found is that the open source model like codegen Nijkamp et al. (2022) is significantly behind across different benchmarks. We conjecture that such a huge gap could be attributed to non-sufficient pre-training and model size.</p>
<p>Sensitivity to Exemplars To better understand how sensitive PoT is w.r.t different exemplars, we conduct a sensitivity analysis. Specifically, we wrote 20 total exemplars. For k-shot learning, we randomly sample $\mathrm{k}=(2,4,6,8)$ out of the 20 exemplars three times as $\mathrm{v} 1, \mathrm{v} 2$, and v 3 . We will use these randomly sampled exemplars as demonstrations for PoT. We summarize our sensitivity analysis in Figure 5. First of all, we found that increasing the number of shots helps more for GSM8K than FinQA. This is mainly due to the diversity of questions in GSM8K. By adding more exemplars, the language models can better generalize to diverse questions. Another observation is that when given fewer exemplars, PoT's performance variance is</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">GSM8K-Hard</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">ASDIV</th>
<th style="text-align: center;">ADDSUB</th>
<th style="text-align: center;">MULTIARITH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PaL</td>
<td style="text-align: center;">$\mathbf{7 2 . 0}$</td>
<td style="text-align: center;">61.2</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">$\mathbf{9 2 . 5}$</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: left;">PoT</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">$\mathbf{6 1 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 2}$</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">$\mathbf{9 9 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of PoT against contemporary work PaL (Gao et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">FinQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PoT</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">64.5</td>
</tr>
<tr>
<td style="text-align: left;">PoT - Binding</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">61.6</td>
</tr>
<tr>
<td style="text-align: left;">PoT - MultiStep</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">58.9</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison between PoT and equation generation on three different datasets.
larger. When $\mathrm{K}=2$, the performance variance can be as large as $7 \%$ for both datasets. With more exemplars, the performance becomes more stable.</p>
<p>Comparison with PaL We also compare PoT with another more recent related approach like PaL (Gao et al., 2022). According to to Table 5, we found that our method is in general better than PaL, especially on SVAMP and ASDIV. Our results are $6 \%$ higher than their prompting method.</p>
<p>Semantic Binding and Multi-Step Reasoning The two core properties of 'program of thoughts' are: (1) multiple steps: breaking down the thought process into the step-by-step program, (2) semantic binding: associating semantic meaning to the variable names. To better understand how these two properties contribute, we compared with two variants. One variant is to remove the semantic binding and simply use $a, b, c$ as the variable names. The other variant is to directly predict the final mathematical equation to compute the results. We show our findings in Table 6. As can be seen, removing the binding will in general hurt the model's performance. On more complex questions involving more variables like GSM8K, the performance drop is larger. Similarly, prompting LLMs to directly generate the target equations is also very challenging. Breaking down the target equation into multiple reasoning steps helps boost performance.</p>
<p>Breakdown Analysis We perform further analysis to determine which kinds of problems CoT and PoT differ most in performance. We use AQuA (Ling et al., 2017) as our testbed for this. Specifically, we manually classify the questions in AQuA into several categories including geometry, polynomial, symbolic, arithmetic, combinatorics, linear equation, iterative and probability. We show the accuracy for each subcategory in Figure 6. The major categories are (1) linear equations, (2) arithmetic, (3) combinatorics, (4) probability, and (5) iterative. The largest improvements of PoT are in the categories 'linear/polynomial equation', 'iterative', 'symbolic', and 'combinatorics'. These questions require more complex arithmetic or symbolic skills to solve. In contrast, on 'arithmetic', 'probability', and 'geometric' questions, PoT and CoT perform similarly. Such observation reflects our assumption that 'program' is more effective on more challenging problems.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: PoT and CoT's breakdown accuracy across different types of questions.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Error cases on TAT-QA dev set using PoT-greedy method.
Error Analysis We considered two types of errors: (1) value grounding error, and (2) logic generation error. The first type indicates that the model fails to assign correct values to the variables relevant to the question. The second type indicates that the model fails to generate the correct computation process to answer the question based on the defined variables. Figure 7 shows an example of each type of error. In the upper example, the model fetches the value of the variables incorrectly while the computation logic is correct. In the lower example, the model grounded relevant variables correctly but fails to generate proper computation logic to answer the question. We manually examined the errors made in the TAT-QA results. Among the 198 failure cases of numerical reasoning questions with the PoT (greedy) method, $47 \%$ have value grounding errors and $33 \%$ have logic errors. In $15 \%$ both types of errors occurred and in $5 \%$ we believe the answer is actually correct. We found that the majority of the errors are value grounding errors, which is also common for other methods such as CoT.</p>
<h1>4 Related Work</h1>
<h3>4.1 Mathematical Reasoning in NLP</h3>
<p>Mathematical reasoning skills are essential for general-purpose intelligent systems, which have attracted a significant amount of attention from the community. Earlier, there have been studies in understanding NLP models' capabilities to solve arithmetic/algebraic questions (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Roy \&amp; Roth, 2015; Ling et al., 2017; Roy \&amp; Roth, 2018). Recently, more challenging datasets (Dua et al., 2019; Saxton et al., 2019; Miao et al., 2020; Amini et al., 2019; Hendrycks et al., 2021; Patel et al., 2021) have been proposed to increase the difficulty, diversity or even adversarial robustness. LiLA (Mishra et al., 2022) proposes to assemble a large set of mathematical datasets into a unified dataset. LiLA also annotates Python programs as the generation target for solving mathematical problems. However, LiLA (Mishra et al., 2022) is mostly focused on dataset unification. Our work aims to understand how to generate 'thoughtful programs' to best elicit LLM's reasoning capability. Besides, we also investigate how to solve math problems without any exemplars. Austin et al. (2021) propose to evaluate LLMs' capabilities to synthesize code on two curated datasets MBPP and MathQA-Python.</p>
<h1>4.2 In-context Learning with LLMs</h1>
<p>GPT-3 (Brown et al., 2020) demonstrated a strong capability to perform few-shot predictions, where the model is given a description of the task in natural language with few examples. Scaling model size, data, and computing are crucial to enable this learning ability. Recently, Rae et al. (2021); Smith et al. (2022); Chowdhery et al. (2022); Du et al. (2022) have proposed to train different types of LLMs with different training recipes. The capability to follow few-shot exemplars to solve unseen tasks is not existent on smaller LMs, but only emerge as the model scales up (Kaplan et al., 2020). Recently, there have been several works (Xie et al., 2021; Min et al., 2022) aiming to understand how and why in-context learning works. Another concurrent work similar to ours is BINDER (Cheng et al., 2022), which applies Codex to synthesize 'soft' SQL queries to answer questions from tables.</p>
<h3>4.3 Chain of Reasoning with LLMs</h3>
<p>Although LLMs have demonstrated remarkable success across a range of NLP tasks, their ability to reason is often seen as a limitation. Recently, CoT (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022b) was proposed to enable LLM's capability to perform reasoning tasks by demonstrating 'natural language rationales'. Suzgun et al. (2022) have shown that CoT can already surpass human performance on challenging BIG-Bench tasks. Later on, several other works (Drozdov et al., 2022; Zhou et al., 2022; Nye et al., 2021) also propose different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps. ReAct Yao et al. (2022) propose to leverage external tools like search engine to enhance the LLM reasoning skills. Our method can be seen as augmenting CoT with external tools (Python) to enable robust numerical reasoning. Another contemporary work (Gao et al., 2022) was proposed at the same time as ours to adopt hybrid text/code reasoning to address math questions.</p>
<h3>4.4 Discussion about Contemporary Work</h3>
<p>Recently, there has been several follow-up work on top of PoT including self-critic (Gou et al., 2023), selfeval (Xie et al., 2023), plan-and-solve (Wang et al., 2023a). These methods propose to enhance LLMs' capabilities to solve math problems with PoT. self-critic (Gou et al., 2023) and self-eval (Xie et al., 2021) both adopt self-evaluation to enhance the robustness of the generated program. plan-and-solve (Wang et al., 2023a) instead adopt more detailed planning instruction to help LLMs create a high-level reasoning plan. These methods all prove to bring decent improvements over PoT on different math reasoning datasets.</p>
<p>Another line of work related to ours is Tool-use in transformer models (Schick et al., 2023; Paranjape et al., 2023). These work propose to adopt different tools to help the language models ground on external world. These work generalizes our Python program into more general API calls to include search engine, string extraction, etc. By generalization, LLMs can unlock its capabilities to solve more complex reasoning and grounding problems in real-world scenarios.</p>
<h2>5 Discussion</h2>
<p>In this work, we have verified that our prompting methods can work efficiently on numerical reasoning tasks like math or finance problem solving. We also study how to combine PoT with CoT to combine the merits of both prompting approaches. We believe PoT is suitable for problems which require highly symbolic reasoning skills. For semantic reasoning tasks like commonsense reasoning (StrategyQA), we conjecture that PoT is not the best option. In contrast, CoT can solve more broader reasoning tasks.</p>
<h2>6 Conclusions</h2>
<p>In this work, we investigate how to disentangle computation from reasoning in solving numerical problems. By 'program of thoughts' prompting, we are able to elicit LLMs' abilities to generate accurate programs to express complex reasoning procedure, while also allows computation to be separately handled by an external program interpreter. This approach is able to boost the performance of LLMs on several math datasets</p>
<p>significantly. We believe our work can inspire more work to combine symbolic execution with LLMs to achieve better performance on other symbolic reasoning tasks.</p>
<h1>Limitations</h1>
<p>Our work aims at combining LLM with symbolic execution to solve challenging math problems. PoT would require execution of 'generated code' from LLMs, which could contain certain dangerous or risky code snippets like 'import os; os.rmdir()', etc. We have blocked the LLM from importing any additional modules and restrict it to using the pre-defined modules. Such brutal-force blocking works reasonable for math QA, however, for other unknown symbolic tasks, it might hurt the PoT's generalization. Another limitation is that PoT still struggles with AQuA dataset with complex algebraic questions with only $58 \%$ accuracy. It's mainly due to the diversity questions in AQuA, which the demonstration cannot possibly cover. Therefore, the future research should discuss how to further prompt LLMs to generate code for highly diversified Math questions.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357-2367, 2019.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.</p>
<p>Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710, 2022 .</p>
<p>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. Finqa: A dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3697-3711, 2021b.</p>
<p>Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint arXiv:2210.03849, 2022.</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021 .</p>
<p>Andrew Drozdov, Nathanael SchÃ¤rli, Ekin AkyÃ¼rek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.</p>
<p>Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547-5569. PMLR, 2022.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378, 2019.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023 .</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pp. 523-533, 2014.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, $3: 585-597,2015$.</p>
<p>Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang Liu. Answering numerical reasoning questions in table-text hybrid contents with graph-based encoder and tree-based decoder. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 1379-1390, 2022.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, 2017.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, 2020.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology. org/2021.naacl-main.168.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015.</p>
<p>Subhro Roy and Dan Roth. Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics, 6:159-172, 2018.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>Bin Wang, Jiangzhou Ju, Yunlin Mao, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. A numerical reasoning question answering system with fine-grained retriever and the ensemble of multiple generators for finqa. arXiv preprint arXiv:2206.08506, 2022a.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2021.</p>
<p>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.</p>
<p>Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>
<p>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. $3277-3287,2021$.</p>
<h1>7 Appendix</h1>
<h3>7.1 PoT as intermediate step</h3>
<p>We demonstrate the workflow in Figure 8.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: We adopt PoT to prompt language models to first generate an intermediate answer and then continue to prompt large models to generate the final answer.</p>
<p>We write the pseudo code as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="cp"># Function PoT(Input) -&gt; Output</span>
<span class="cp"># Input: question</span>
<span class="cp"># Ouptut: program</span>
<span class="cp"># Function Prompt(Input) -&gt; Output</span>
<span class="cp"># Input: question + intermediate</span>
<span class="cp"># Ouptut: answer</span>
<span class="n">program</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PoT</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">exec</span><span class="w"> </span><span class="p">(</span><span class="n">program</span><span class="p">)</span>
<span class="nf">if</span><span class="w"> </span><span class="n">isintance</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span><span class="w"> </span><span class="n">dict</span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="n">ans</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">items</span><span class="p">()).</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="w">    </span><span class="n">extra</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&#39;according..to..the..program:..&#39;</span>
<span class="w">    </span><span class="n">extra</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">ans</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&#39;.\infty.&#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ans</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Prompt</span><span class="p">(</span><span class="n">question</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">extra</span><span class="p">)</span>
<span class="n">else</span><span class="o">:</span>
<span class="w">    </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ans</span>
<span class="kr">return</span><span class="w"> </span><span class="n">pred</span>
</code></pre></div>

<p>PoT as intermediate step is able to address more complex questions which require both symbolic and commonsense reasoning.</p>
<h3>7.2 Exemplars for Prompting</h3>
<p>To enable better reproducibility, we also put our prompts and exemplars for GSM8K dataset and AQuA dataset in the following pages:</p>
<p>Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $\$ 2$ per fresh duck egg. How much in dollars does she make every day at the farmers' market?
# Python code, return ans
total_eggs $=16$
eaten_eggs $=3$
baked_eggs $=4$
sold_eggs = total_eggs - eaten_eggs - baked_eggs
dollars_per_egg = 2
ans = sold_eggs * dollars_per_egg
Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?
# Python code, return ans
bolts_of_blue_fiber = 2
bolts_of_white_fiber = num_of_blue_fiber / 2
ans = bolts_of_blue_fiber + bolts_of_white_fiber
Question: Josh decides to try flipping a house. He buys a house for \$80,000 and then puts in \$50,000 in repairs. This increased the value of the house by $150 \%$. How much profit did he make?
# Python code, return ans
cost_of_original_house = 80000
increase_rate $=150 / 100$
value_of_house $=(1+$ increase_rate $) <em>$ cost_of_original_house
cost_of_repair $=50000$
ans = value_of_house - cost_of_repair - cost_of_original_house
Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25 cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?
# Python code, return ans
numb_of_chickens = 20
cups_for_each_chicken = 3
cups_for_all_chicken = num_of_chickens * cups_for_each_chicken
cups_in_the_morning = 15
cups_in_the_afternoon = 25
ans = cups_for_all_chicken - cups_in_the_morning - cups_in_the_afternoon
Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $\$ 5$, but every second glass costs only $60 \%$ of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?
# Python code, return ans
num_glasses $=16$
first_glass_cost $=5$
second_glass_cost $=5^{</em>} 0.6$
ans $=0$
for i in range(num_glasses):
if i $\% 2==0$ :
ans += first_glass_cost
else:
ans += second_glass_cost</p>
<p>Question: Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance?
# Python code, return ans
average_mile_per_hour = 4
total_trail_miles = 12
remaining_miles = total_trail_miles - 4 - 2
total_hours = total_trail_miles / average_mile_per_hour
remaining_hours = total_hours - 2
ans = remaining_miles / remaining_hours
Question: Carlos is planting a lemon tree. The tree will cost $\$ 90$ to plant. Each year it will grow 7 lemons, which he can sell for $\$ 1.5$ each. It costs $\$ 3$ a year to water and feed the tree. How many years will it tak
e before he starts earning money on the lemon tree?
# Python code, return ans
total_cost = 90
cost_of_watering_and_feeding = 3
cost_of_each_lemon = 1.5
num_of_lemon_per_year = 7
ans $=0$
while total_cost $&gt;0$ :
total_cost += cost_of_watering_and_feeding
total_cost -= num_of_lemon_per_year * cost_of_each_lemon
ans $+=1$
Question: When Freda cooks canned tomatoes into sauce, they lose half their volume. Each 16 ounce can of tomatoes that she uses contains three tomatoes. Freda's last batch of tomato sauce made 32 ounces of sauce. How many tomatoes did Freda use?
# Python code, return ans
lose_rate = 0.5
num_tomato_contained_in_per_ounce_sauce = 3 / 16
ounce_sauce_in_last_batch = 32
num_tomato_in_last_batch = ounce_sauce_in_last_batch * num_tomato_contained_in_per_ounce_sauce
ans = num_tomato_in_last_batch / (1 - lose_rate)
Question: Jordan wanted to surprise her mom with a homemade birthday cake. From reading the instructions, she knew it would take 20 minutes to make the cake batter and 30 minutes to bake the cake. The cake would require 2 hours to cool and an additional 10 minutes to frost the cake. If she plans to make the cake all on the same day, what is the latest time of day that Jordan can start making the cake to be ready to serve it at 5:00 pm?
# Python code, return ans
minutes_to_make_batter = 20
minutes_to_bake_cake = 30
minutes_to_cool_cake = 2 * 60
minutes_to_frost_cake = 10
total_minutes = minutes_to_make_batter + minutes_to_bake_cake + minutes_to_cool_cake +
minutes_to_frost_cake
total_hours = total_minutes / 60
ans $=5$ - total_hours</p>
<p># Write Python Code to solve the following questions. Store your result as a variable named 'ans'. from sympy import Symbol
from sympy import simplify
import math
from sympy import solve_it
# solve_it(equations, variable): solving the equations and return the variable value.
# Question: In a flight of 600 km , an aircraft was slowed down due to bad weather. Its average speed for the trip was reduced by $200 \mathrm{~km} / \mathrm{hr}$ and the time of flight increased by 30 minutes. The duration of the flight is:
# Answer option: ['A)1 hour', 'B)2 hours', 'C)3 hours', 'D)4 hours', 'E)5 hours']
duration = Symbol('duration', positive=True)
delay $=30 / 60$
total_disntace $=600$
original_speed = total_disntace / duration
reduced_speed = total_disntace / (duration + delay)
solution = solve_it(original_speed - reduced_speed - 200, duration)
ans = solution[duration]
# Question: M men agree to purchase a gift for Rs. D. If 3 men drop out how much more will each have to contribute towards the purchase of the gift?
# Answer options: ['A)D/(M-3)', 'B)MD/3', 'C)M/(D-3)', 'D)3D/(M2-3M)', 'E)None of these']
$\mathrm{M}=$ Symbol('M')
$D=$ Symbol('D')
cost_before_dropout $=D / M$
cost_after_dropout $=D /(M-3)$
ans=simplify(cost_after_dropout - cost_before_dropout)
# Question: A sum of money at simple interest amounts to Rs. 815 in 3 years and to Rs. 854 in 4 years. The sum is:
# Answer option: ['A)Rs. 650', 'B)Rs. 690', 'C)Rs. 698', 'D)Rs. 700', 'E)None of these']
deposit = Symbol('deposit', positive=True)
interest = Symbol('interest', positive=True)
money_in_3_years = deposit + 3 * interest
money_in_4_years = deposit + 4 * interest
solution = solve_it([money_in_3_years - 815, money_in_4_years - 854], [deposit, interest])
ans = solution[deposit]
# Question: Find out which of the following values is the multiple of $X$, if it is divisible by 9 and 12?
# Answer option: ['A)36', 'B)15', 'C)17', 'D)5', 'E)7']
options $=[36,15,17,5,7]$
for option in options:
if option $\% 9==0$ and option $\% 12==0$ :
ans = option
break</p>
<p># Question: 35\% of the employees of a company are men. 60\% of the men in the company speak French and $40 \%$ of the employees of the company speak French. What is $\%$ of the women in the company who do not speak French?
# Answer option: ['A)4\%', 'B)10\%', 'C)96\%', 'D)90.12\%', 'E)70.77\%']
num_women $=65$
men_speaking_french $=0.6 * 35$
employees_speaking_french $=0.4 * 100$
women_speaking_french = employees_speaking_french - men_speaking_french
women_not_speaking_french=num_women - women_speaking_french
ans = women_not_speaking_french / num_women
# Question: In one hour, a boat goes $11 \mathrm{~km} / \mathrm{hr}$ along the stream and $5 \mathrm{~km} / \mathrm{hr}$ against the stream. The speed of the boat in still water (in km/hr) is:
# Answer option: ['A)4 kmph', 'B)5 kmph', 'C)6 kmph', 'D)7 kmph', 'E) 8 kmph']
boat_speed = Symbol('boat_speed', positive=True)
stream_speed = Symbol('stream_speed', positive=True)
along_stream_speed $=11$
against_stream_speed $=5$
solution = solve_it([boat_speed + stream_speed - along_stream_speed, boat_speed - stream_speed against_stream_speed], [boat_speed, stream_speed])
ans = solution[boat_speed]
# Question: The difference between simple interest and C.I. at the same rate for Rs. 5000 for 2 years in Rs.72. The rate of interest is?
# Answer option: ['A)10\%', 'B)12\%', 'C)6\%', 'D)8\%', 'E)4\%']
interest_rate = Symbol('interest_rate', positive=True)
amount $=5000$
amount_with_simple_interest = amount * (1 + 2 * interest_rate / 100)
amount_with_compound_interest = amount * (1 + interest_rate / 100) ** 2
solution = solve_it(amount_with_compound_interest - amount_with_simple_interest - 72, interest_rate) ans = solution[interest_rate]
# Question: The area of a rectangle is 15 square centimeters and the perimeter is 16 centimeters. What are the dimensions of the rectangle?
# Answer option: ['A)2\&amp;4', 'B)3\&amp;5', 'C)4\&amp;6', 'D)5\&amp;7', 'E)6\&amp;8']
width = Symbol('width', positive=True)
height = Symbol('height', positive=True)
area $=15$
permimeter $=16$
solution = solve_it([width * height - area, 2 * (width + height) - permimeter], [width, height])
ans = (solution[width], solution[height])</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://openai.com/blog/openai-codex/
${ }^{3}$ https://blog.salesforceairesearch.com/xgen/
${ }^{4}$ https://www.sympy.org/en/index.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
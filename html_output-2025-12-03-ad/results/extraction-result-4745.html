<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4745 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4745</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4745</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-267334709</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.17686v3.pdf" target="_blank">Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors. In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4745.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4745.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deductive Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding method that integrates chain-of-thought with step-wise beam search and a learned deductive verifier: it samples multiple candidate reasoning steps (exploration) and uses a verifier to score/select the most deducible steps (exploitation), thereby reducing accumulative intermediate-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7b (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama 2 family, 7 billion parameter decoder-only transformer; used here as a primary backbone for generating candidate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Deductive Beam Search (DBS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>At each reasoning step, sample n candidate steps per beam (exploration) and score whole candidate chains with a learned deductive verifier that predicts whether a step is a logical consequence of its premises (exploitation). Step-wise beam search keeps top-m chains by deductive score and continues until termination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic reasoning (GSM8K and other arithmetic datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step math word problems requiring step-by-step deduction (datasets include GSM8K, SVAMP, AQuA, SingleEq, MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Arithmetic average improvement ≈ +5.3% (single-chain) for Llama2-7b compared to baseline decoding; GSM8K example: reported improvement of +9.2% for Llama2-7b (single-chain) in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Greedy / naive decoding: lower (DBS reported ≈ +5.3% over greedy on arithmetic average single-chain for Llama2-7b); Self-Consistency: DBS improves over naive self-consistency by ≈ +3.0% on arithmetic (multiple-chain setting).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining diverse sampling (multiple candidate steps) with a deductive verifier to select deducible chains gives consistent and sizable gains on arithmetic reasoning for a 7B model; this hybrid (explore diverse, exploit deducibility) outperforms both simple diverse aggregation (self-consistency) and similar single-chain decoding in aggregate while being more token-efficient than some verifier-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On some datasets requiring very few reasoning steps (e.g., SingleEq, StrategyQA) DBS with ChatGPT was slightly worse (-0.6% / -0.8%), suggesting that the multi-step exploratory paradigm can be less beneficial for very short reasoning chains or when the base model already solves the task reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4745.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4745.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBS-ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deductive Beam Search evaluated with gpt-3.5-turbo-instruct (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of DBS on ChatGPT (gpt-3.5-turbo-instruct) showing the method generalizes across model scales and families, yielding smaller but consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-instruct (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model (GPT-3.5 family) accessed via instruction-tuned chat interface; stronger base reasoning than smaller open models in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Deductive Beam Search (DBS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Same as above: sample multiple candidate steps per beam and rank chains by a learned deductive verifier to bias decoding toward logically deducible steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic, commonsense, and symbolic reasoning (GSM8K, StrategyQA, Coin Flip, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks spanning arithmetic word problems, commonsense multi-step questions, and symbolic token-manipulation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Arithmetic average improvement ≈ +3.2% (single-chain) for ChatGPT compared to baseline decoding; GSM8K reported improvement ≈ +7.0% for ChatGPT on that dataset in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Self-Consistency and greedy baselines: DBS provides smaller but consistent improvements over these on aggregate (multiple-chain arithmetic improvement ≈ +2.5% for ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DBS yields positive improvements on ChatGPT as well, though gains are smaller than on smaller models; the verifier-guided selection helps even when the underlying LM is strong, demonstrating cross-scale robustness of combining diverse sampling with deductive verification.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On tasks with very short reasoning chains (SingleEq, StrategyQA), DBS with ChatGPT was slightly worse (-0.6%/-0.8%), indicating that deductive beam exploration is not uniformly helpful across all task types and can marginally harm performance when little intermediate reasoning is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4745.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4745.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy decoding (single chain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard token- or step-level greedy decoding that selects at each step the highest-probability continuation and returns a single chain-of-thought without sampling or ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7b / Llama2-13b / Llama2-70b / ChatGPT (evaluated baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various decoder-only transformer LLMs of different parameter scales used as base generators in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Greedy single-chain CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate one chain deterministically by always selecting maximum-probability tokens/steps (no sampling or aggregation), yielding a single similar reasoning path per question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic, commonsense, symbolic reasoning benchmarks (same set as DBS experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step and sometimes single-step reasoning tasks used to evaluate different decoding/aggregation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used as a lower-cost baseline; paper reports that greedy decoding is the most token-economic but achieves lower accuracy than DBS and other ensemble/diverse methods (e.g., DBS improves over greedy by ~5.3% on arithmetic average for Llama2-7b in single-chain setting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DBS: higher (DBS reported ≈ +5.3% arithmetic single-chain for Llama2-7b); Self-Consistency: often better than greedy but less token-efficient than greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Greedy is cheapest in token cost but yields lower accuracy because it cannot explore alternative reasoning paths; DBS (diverse sampling + verification) trades modest extra cost for consistent accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Greedy decoding can sometimes be competitive on tasks requiring very short, straightforward chains (few-step problems), but in aggregate it underperforms verifier-guided diverse methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4745.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4745.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over sampled chain-of-thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Diverse sampling method that generates many reasoning chains via stochastic sampling and then aggregates final answers (e.g., majority vote) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2 family and ChatGPT (evaluated baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs used for sampling diverse chain-of-thought outputs; self-consistency aggregates final answers across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (ensemble over diverse chains)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample multiple complete rationales (diverse outputs via temperature/top-k/p); compute final answers for each and select the most consistent/majority answer (optionally with reranking).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic, commonsense, symbolic reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks used to test whether aggregating diverse answers improves final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Self-consistency improves over greedy but is outperformed by DBS in multiple-chain settings; paper reports DBS outperforms naive self-consistency by on average ≈ +3.0% on arithmetic tasks (multiple-chain setting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DBS: higher (≈ +3.0% arithmetic improvement over SC in multiple-chain setting); Greedy: lower than SC.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Diverse aggregation (self-consistency) helps reduce error relative to greedy sampling, but without addressing intermediate-step logical coherence; DBS's verifier-guided selection yields further gains on top of diversity because it targets deducibility of intermediate steps rather than only final-answer agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-consistency can be token-expensive; its benefits diminish on smaller models relative to verifier-guided methods (the paper notes self-evaluate patterns perform worse when model scale drops and consume excessive tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4745.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4745.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation guided beam search / SelfEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that use LLMs to score/rerank their own sampled reasoning steps or chains (self-evaluation) and optionally correct them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-instruct and Llama2 variants (used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used both to generate candidate chains and to perform self-evaluation/reranking of those candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-evaluation (LLM reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate candidates and ask the LLM itself (or an LLM) to rank/verify which chains/steps are correct; can be used for reranking or step-wise correction without an external verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic reasoning (GSM8K subset reported); general reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as a baseline to compare verifier-based external ranking versus LLM self-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SelfEval can be strong on large LMs but performs worse when model scale decreases; paper reports DBS outperforms SelfEval especially for smaller LMs and is far more token-efficient (SelfEval consumes excessive tokens during evaluation). Exact dataset metrics vary; DBS shows better accuracy than SelfEval on Llama2-7b and competitive or better on gpt-3.5-turbo-instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DBS: better accuracy and token efficiency vs SelfEval on smaller models; for gpt-3.5-turbo-instruct DBS outperforms DV while consuming ~80x fewer tokens (paper statement about token cost vs DV).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using the LLM itself as a verifier/reranker (self-evaluation) is less reliable than a dedicated learned deductive verifier, particularly for smaller models; external verifier + beam search (DBS) yields more verifiable score separation between correct and incorrect steps and better token-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-evaluation methods may be competitive on very large LMs, but the paper reports they degrade with smaller model sizes and require much higher token budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4745.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4745.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DV (Deductive Verification baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deductive Verification (prior work baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that applies a notion of deductive verification to CoT reasoning; used as a competitive baseline in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-instruct (reported comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same ChatGPT model family used as base generator; DV is an algorithmic/verification baseline rather than a model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Deductive Verification (prior baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Applies deductive verification ideas to chain-of-thought reasoning to detect incorrect steps; specifics from prior work used as a baseline here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (reported comparison) and other reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard arithmetic and reasoning benchmarks used to compare decoding/verification strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DBS outperforms DV on gpt-3.5-turbo-instruct while consuming substantially fewer tokens; the paper states DBS consumes ~80x fewer tokens than DV on gpt-3.5-turbo-instruct and still achieves better accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DBS: higher and more token efficient compared to DV in reported comparisons (gpt-3.5-turbo-instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DBS's combination of step-wise beam search and a trained deductive verifier (with synthetic + hard negative training) attains better accuracy and far better token-efficiency than the prior Deductive Verification baseline on the evaluated models/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No broad negative results reported against DV aside from the token-cost and DBS marginal superiority on accuracy in reported settings; exact per-dataset effect sizes vs DV not exhaustively enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning <em>(Rating: 2)</em></li>
                <li>Self-evaluation guided beam search for reasoning <em>(Rating: 2)</em></li>
                <li>Deductive verification of chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>How language model hallucinations can snowball <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4745",
    "paper_id": "paper-267334709",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "DBS",
            "name_full": "Deductive Beam Search",
            "brief_description": "A decoding method that integrates chain-of-thought with step-wise beam search and a learned deductive verifier: it samples multiple candidate reasoning steps (exploration) and uses a verifier to score/select the most deducible steps (exploitation), thereby reducing accumulative intermediate-step errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7b (evaluated)",
            "model_description": "Open-source Llama 2 family, 7 billion parameter decoder-only transformer; used here as a primary backbone for generating candidate reasoning steps.",
            "reasoning_method_name": "Deductive Beam Search (DBS)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "At each reasoning step, sample n candidate steps per beam (exploration) and score whole candidate chains with a learned deductive verifier that predicts whether a step is a logical consequence of its premises (exploitation). Step-wise beam search keeps top-m chains by deductive score and continues until termination.",
            "task_name": "Arithmetic reasoning (GSM8K and other arithmetic datasets)",
            "task_description": "Multi-step math word problems requiring step-by-step deduction (datasets include GSM8K, SVAMP, AQuA, SingleEq, MultiArith).",
            "performance": "Arithmetic average improvement ≈ +5.3% (single-chain) for Llama2-7b compared to baseline decoding; GSM8K example: reported improvement of +9.2% for Llama2-7b (single-chain) in the paper.",
            "comparison_with_other_method": true,
            "performance_other_method": "Greedy / naive decoding: lower (DBS reported ≈ +5.3% over greedy on arithmetic average single-chain for Llama2-7b); Self-Consistency: DBS improves over naive self-consistency by ≈ +3.0% on arithmetic (multiple-chain setting).",
            "key_findings": "Combining diverse sampling (multiple candidate steps) with a deductive verifier to select deducible chains gives consistent and sizable gains on arithmetic reasoning for a 7B model; this hybrid (explore diverse, exploit deducibility) outperforms both simple diverse aggregation (self-consistency) and similar single-chain decoding in aggregate while being more token-efficient than some verifier-based baselines.",
            "counter_examples_or_negative_results": "On some datasets requiring very few reasoning steps (e.g., SingleEq, StrategyQA) DBS with ChatGPT was slightly worse (-0.6% / -0.8%), suggesting that the multi-step exploratory paradigm can be less beneficial for very short reasoning chains or when the base model already solves the task reliably.",
            "uuid": "e4745.0",
            "source_info": {
                "paper_title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "DBS-ChatGPT",
            "name_full": "Deductive Beam Search evaluated with gpt-3.5-turbo-instruct (ChatGPT)",
            "brief_description": "Application of DBS on ChatGPT (gpt-3.5-turbo-instruct) showing the method generalizes across model scales and families, yielding smaller but consistent gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-instruct (ChatGPT)",
            "model_description": "OpenAI chat model (GPT-3.5 family) accessed via instruction-tuned chat interface; stronger base reasoning than smaller open models in the paper's comparisons.",
            "reasoning_method_name": "Deductive Beam Search (DBS)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Same as above: sample multiple candidate steps per beam and rank chains by a learned deductive verifier to bias decoding toward logically deducible steps.",
            "task_name": "Arithmetic, commonsense, and symbolic reasoning (GSM8K, StrategyQA, Coin Flip, etc.)",
            "task_description": "Benchmarks spanning arithmetic word problems, commonsense multi-step questions, and symbolic token-manipulation reasoning.",
            "performance": "Arithmetic average improvement ≈ +3.2% (single-chain) for ChatGPT compared to baseline decoding; GSM8K reported improvement ≈ +7.0% for ChatGPT on that dataset in the paper.",
            "comparison_with_other_method": true,
            "performance_other_method": "Self-Consistency and greedy baselines: DBS provides smaller but consistent improvements over these on aggregate (multiple-chain arithmetic improvement ≈ +2.5% for ChatGPT).",
            "key_findings": "DBS yields positive improvements on ChatGPT as well, though gains are smaller than on smaller models; the verifier-guided selection helps even when the underlying LM is strong, demonstrating cross-scale robustness of combining diverse sampling with deductive verification.",
            "counter_examples_or_negative_results": "On tasks with very short reasoning chains (SingleEq, StrategyQA), DBS with ChatGPT was slightly worse (-0.6%/-0.8%), indicating that deductive beam exploration is not uniformly helpful across all task types and can marginally harm performance when little intermediate reasoning is needed.",
            "uuid": "e4745.1",
            "source_info": {
                "paper_title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Greedy",
            "name_full": "Greedy decoding (single chain)",
            "brief_description": "Standard token- or step-level greedy decoding that selects at each step the highest-probability continuation and returns a single chain-of-thought without sampling or ensembling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2-7b / Llama2-13b / Llama2-70b / ChatGPT (evaluated baselines)",
            "model_description": "Various decoder-only transformer LLMs of different parameter scales used as base generators in the experiments.",
            "reasoning_method_name": "Greedy single-chain CoT",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Generate one chain deterministically by always selecting maximum-probability tokens/steps (no sampling or aggregation), yielding a single similar reasoning path per question.",
            "task_name": "Arithmetic, commonsense, symbolic reasoning benchmarks (same set as DBS experiments)",
            "task_description": "Multi-step and sometimes single-step reasoning tasks used to evaluate different decoding/aggregation strategies.",
            "performance": "Used as a lower-cost baseline; paper reports that greedy decoding is the most token-economic but achieves lower accuracy than DBS and other ensemble/diverse methods (e.g., DBS improves over greedy by ~5.3% on arithmetic average for Llama2-7b in single-chain setting).",
            "comparison_with_other_method": true,
            "performance_other_method": "DBS: higher (DBS reported ≈ +5.3% arithmetic single-chain for Llama2-7b); Self-Consistency: often better than greedy but less token-efficient than greedy.",
            "key_findings": "Greedy is cheapest in token cost but yields lower accuracy because it cannot explore alternative reasoning paths; DBS (diverse sampling + verification) trades modest extra cost for consistent accuracy gains.",
            "counter_examples_or_negative_results": "Greedy decoding can sometimes be competitive on tasks requiring very short, straightforward chains (few-step problems), but in aggregate it underperforms verifier-guided diverse methods.",
            "uuid": "e4745.2",
            "source_info": {
                "paper_title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency (majority-vote over sampled chain-of-thoughts)",
            "brief_description": "Diverse sampling method that generates many reasoning chains via stochastic sampling and then aggregates final answers (e.g., majority vote) to improve robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2 family and ChatGPT (evaluated baselines)",
            "model_description": "Same LLMs used for sampling diverse chain-of-thought outputs; self-consistency aggregates final answers across samples.",
            "reasoning_method_name": "Self-Consistency (ensemble over diverse chains)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample multiple complete rationales (diverse outputs via temperature/top-k/p); compute final answers for each and select the most consistent/majority answer (optionally with reranking).",
            "task_name": "Arithmetic, commonsense, symbolic reasoning benchmarks",
            "task_description": "Benchmarks used to test whether aggregating diverse answers improves final-answer accuracy.",
            "performance": "Self-consistency improves over greedy but is outperformed by DBS in multiple-chain settings; paper reports DBS outperforms naive self-consistency by on average ≈ +3.0% on arithmetic tasks (multiple-chain setting).",
            "comparison_with_other_method": true,
            "performance_other_method": "DBS: higher (≈ +3.0% arithmetic improvement over SC in multiple-chain setting); Greedy: lower than SC.",
            "key_findings": "Diverse aggregation (self-consistency) helps reduce error relative to greedy sampling, but without addressing intermediate-step logical coherence; DBS's verifier-guided selection yields further gains on top of diversity because it targets deducibility of intermediate steps rather than only final-answer agreement.",
            "counter_examples_or_negative_results": "Self-consistency can be token-expensive; its benefits diminish on smaller models relative to verifier-guided methods (the paper notes self-evaluate patterns perform worse when model scale drops and consume excessive tokens).",
            "uuid": "e4745.3",
            "source_info": {
                "paper_title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SelfEval",
            "name_full": "Self-Evaluation guided beam search / SelfEval",
            "brief_description": "Methods that use LLMs to score/rerank their own sampled reasoning steps or chains (self-evaluation) and optionally correct them.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-instruct and Llama2 variants (used as baselines)",
            "model_description": "LLMs used both to generate candidate chains and to perform self-evaluation/reranking of those candidates.",
            "reasoning_method_name": "Self-evaluation (LLM reranker)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Generate candidates and ask the LLM itself (or an LLM) to rank/verify which chains/steps are correct; can be used for reranking or step-wise correction without an external verifier.",
            "task_name": "Arithmetic reasoning (GSM8K subset reported); general reasoning benchmarks",
            "task_description": "Used as a baseline to compare verifier-based external ranking versus LLM self-ranking.",
            "performance": "SelfEval can be strong on large LMs but performs worse when model scale decreases; paper reports DBS outperforms SelfEval especially for smaller LMs and is far more token-efficient (SelfEval consumes excessive tokens during evaluation). Exact dataset metrics vary; DBS shows better accuracy than SelfEval on Llama2-7b and competitive or better on gpt-3.5-turbo-instruct.",
            "comparison_with_other_method": true,
            "performance_other_method": "DBS: better accuracy and token efficiency vs SelfEval on smaller models; for gpt-3.5-turbo-instruct DBS outperforms DV while consuming ~80x fewer tokens (paper statement about token cost vs DV).",
            "key_findings": "Using the LLM itself as a verifier/reranker (self-evaluation) is less reliable than a dedicated learned deductive verifier, particularly for smaller models; external verifier + beam search (DBS) yields more verifiable score separation between correct and incorrect steps and better token-efficiency.",
            "counter_examples_or_negative_results": "Self-evaluation methods may be competitive on very large LMs, but the paper reports they degrade with smaller model sizes and require much higher token budgets.",
            "uuid": "e4745.4",
            "source_info": {
                "paper_title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "DV (Deductive Verification baseline)",
            "name_full": "Deductive Verification (prior work baseline)",
            "brief_description": "Prior method that applies a notion of deductive verification to CoT reasoning; used as a competitive baseline in this paper's comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-instruct (reported comparison)",
            "model_description": "Same ChatGPT model family used as base generator; DV is an algorithmic/verification baseline rather than a model.",
            "reasoning_method_name": "Deductive Verification (prior baseline)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Applies deductive verification ideas to chain-of-thought reasoning to detect incorrect steps; specifics from prior work used as a baseline here.",
            "task_name": "GSM8K (reported comparison) and other reasoning benchmarks",
            "task_description": "Standard arithmetic and reasoning benchmarks used to compare decoding/verification strategies.",
            "performance": "DBS outperforms DV on gpt-3.5-turbo-instruct while consuming substantially fewer tokens; the paper states DBS consumes ~80x fewer tokens than DV on gpt-3.5-turbo-instruct and still achieves better accuracy.",
            "comparison_with_other_method": true,
            "performance_other_method": "DBS: higher and more token efficient compared to DV in reported comparisons (gpt-3.5-turbo-instruct).",
            "key_findings": "DBS's combination of step-wise beam search and a trained deductive verifier (with synthetic + hard negative training) attains better accuracy and far better token-efficiency than the prior Deductive Verification baseline on the evaluated models/datasets.",
            "counter_examples_or_negative_results": "No broad negative results reported against DV aside from the token-cost and DBS marginal superiority on accuracy in reported settings; exact per-dataset effect sizes vs DV not exhaustively enumerated in the paper.",
            "uuid": "e4745.5",
            "source_info": {
                "paper_title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning"
        },
        {
            "paper_title": "Self-evaluation guided beam search for reasoning",
            "rating": 2,
            "sanitized_title": "selfevaluation_guided_beam_search_for_reasoning"
        },
        {
            "paper_title": "Deductive verification of chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "deductive_verification_of_chainofthought_reasoning"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "How language model hallucinations can snowball",
            "rating": 1,
            "sanitized_title": "how_language_model_hallucinations_can_snowball"
        }
    ],
    "cost": 0.014068,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
19 Oct 2024</p>
<p>Tinghui Zhu 
Fudan University
The Ohio State University</p>
<p>Kai Zhang 
Fudan University
The Ohio State University</p>
<p>Jian Xie 
Fudan University
The Ohio State University</p>
<p>Yu Su 
Fudan University
The Ohio State University</p>
<p>Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning
19 Oct 2024441A399BD558F7D253A11208C9FD62A9arXiv:2401.17686v3[cs.CL]
Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning.However, previous methods often struggle to address reasoning errors in intermediate steps, which can lead to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs.Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation.Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities.Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic.Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales.Data and codes are released at https://github.com/OSU-NLP-Group/Deductive-Beam-Search.</p>
<p>Introduction</p>
<p>Question:</p>
<p>James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?</p>
<p>Answer:</p>
<p>Step 1: He runs 3 times a week, which is equal to 3 x 7 = 21 sprints.</p>
<p>Step 2: The distance he runs each sprint is 60 meters, so the total distance he runs is 60 x 21 = 1320 meters.Final Answer: 1320</p>
<p>Error Analysis</p>
<p>Step 1: Grounding error.7 does not come from premises.</p>
<p>Step 2: Accumulative error.Misled by errors in step 1. Machine reasoning has witnessed tremendous progress thanks to the emergence of Large Language Models (LLMs) (OpenAI, 2023;Google, 2023;Anil et al., 2023;Touvron et al., 2023;McIntosh et al., 2023).The power of LLMs activates the ability to conduct step-by-step chain-of-thought (CoT) reasoning (Wei et al., 2022b;a), significantly boosting the performance of reasoning tasks (Wang et al., 2022;Paul et al., 2023;Lyu et al., 2023).</p>
<p>Although CoT reasoning has demonstrated the superiority of step-by-step reasoning, its dependency on intermediate steps inevitably introduces accumulative errors (Du et al., 2023;Yu et al., 2023;Wu et al., 2024) in the process, as shown in Figure 1.Previous research that alleviates these errors lies in two main paradigms: 1) Answer aggregation across multiple rationales.They utilize majority voting (Wang et al., 2022) or deploy a verifier to score on each rationale (Li et al., 2023).However, these methods do not directly address errors in the reasoning process, undermining the reliability of their outcomes.2) Intermediate</p>
<p>Question:</p>
<p>James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?</p>
<p>The distance of each sprint is 60 meters.</p>
<p>Therefore, the total distance James runs in a week is 9 x 60 = 540 meters.</p>
<p>Figure 2: Overview of Deductive Beam Search.We illustrate the process under the configuration of beam size 2 and sampling times 2.</p>
<p>step correction.This line of works decomposes the reasoning path into reasoning steps and applies self-correction on each step (Weng et al., 2022;Ling et al., 2024;Paul et al., 2023;Xie et al., 2023).Yet, recent research finds that, without external feedback, LLMs tend to modify reasoning steps regardless of their correctness (Huang et al., 2023;Hong et al., 2023).</p>
<p>Previous works fail to address reasoning errors in intermediate steps, compromising the ability to conduct systematic reasoning.To mitigate this issue, we embrace the principle of deductive reasoning (Clark, 1969;Johnson-Laird, 1999;2010).In deductive reasoning, every step logically follows its premises, where a deducible reasoning step is termed a logical consequence (Dinkmeyer, 1976;Hanson, 1997).A key attribute of logical consequence is that if the premises hold, the deducible reasoning step is true, suggesting a correct outcome.Inspired by this attribute, we propose to navigate CoT towards a more deducible path.</p>
<p>Nonetheless, challenges arise when introducing the principle of deductive reasoning into CoT reasoning without changing the standard prompt paradigm and the parameters of LLMs. 1) Navigation on CoT reasoning.Since LLMs cannot always conduct correct deductive reasoning, they have to explore the potential reasoning space and choose those reasoning steps that are more likely to be deducible, which brings the trade-off between exploration and exploitation (Donoso et al., 2014;Dasgupta et al., 2019).2) Verification of deducibility.On one hand, previous research shows it is hard for LLMs to detect reasoning errors (Huang et al., 2023;Hong et al., 2023).On the other hand, symbolic reasoning engines (Cavada et al., 2014;Li et al., 2018) can reliably verify the correctness.However, transferring natural language to symbolic language without losing generality remains an unsolved problem in machine reasoning.</p>
<p>Confronted with these challenges, we propose Deductive Beam Search (DBS), adaptable to all models and settings.The overview of DBS is shown in Figure 2.For the trade-off challenge, we decompose the reasoning process into reasoning steps and incorporate stepwise beam search.In terms of the verification challenge, we propose a deductive verifier, which takes a reasoning step and its premises as inputs and outputs a deductive score, evaluating the logical coherence between them.Specifically, LLM samples a list of potential reasoning steps to explore.Then, our deductive verifier exploits by selecting steps that are more deducible.To train an effective verifier, we propose a scalable way of synthesizing fine-grained and diverse deductive reasoning errors without human annotation.Initially, the verifier is trained to verify heuristically synthesized wrong steps with typical reasoning error patterns.Subsequently, we ask LLMs to generate reasoning steps where false ones detected by our verifier serve as hard negatives.These hard negatives are adopted to train a deductive verifier with model feedback.</p>
<p>As we aim to enable LLMs to decode more deducible reasoning paths, DBS can be integrated with answer-aggregation-based methods.We evaluate our methods across 5 arithmetic reasoning tasks, 2 commonsense reasoning tasks, and 1 symbolic reasoning task in single chain setting and multiple chain setting.The improvements can be expected not only on models of all scales and diverse model families but also under different settings.Concretely, taking arithmetic reasoning tasks as an example, the average improvement is 5.3% / 3.2% on Llama2-7b / ChatGPT under single chain setting and 3.9% / 2.5% under multiple chain setting.Moreover, we comprehensively analyze our verifier, demonstrating its capability of detecting diverse and subtle reasoning errors and robustness on different model scales.</p>
<p>Deductive Beam Search</p>
<p>We begin by formulating multi-step CoT reasoning with step-wise beam search before describing DBS.For notation convenience, we denote [n] to be a set of natural numbers from 1 to n, and v
[n] = [v 1 , v 2 , ..., v n ] represents the first n elements of v, where v [0] = []
representing an empty sequence.Specifically, we denote tokens as y.</p>
<p>Multi-Step Chain-of-Thought Reasoning</p>
<p>Standard chain-of-thought reasoning (Wei et al., 2022b) generates the whole reasoning path for the final outcome.Formally, given the question q, CoT formulates the answer distribution Pr LM (a|q) as a product of the rationales generation distribution Pr LM (r [t] |q) and a final answer distribution Pr LM (a|r [t] ), which is:
Pr LM (a|q) = Pr LM (a|r [t] ) × Pr LM (r [t] |q),(1)
where r [t] = [r 1 , r 2 , ..., r t ] is a complete reasoning path, and t is the number of steps required to complete the reasoning process.Each r = [y 1 , y 2 , ...y l ] is an intermediate reasoning step, where l is its token length.</p>
<p>Problems in this setting lie in the complexity of navigating the generation of r [t] , which are sampled as a whole directly from language models, a process wherein errors can accumulate (Zhang et al., 2023a).To avoid error accumulation and navigate the reasoning process, we decompose the process of generating r [t] as:
Pr LM (r [t] |q) = Pr LM (r 1 |q) × t−1 ∏ i=1 Pr LM (r i+1 |q, r [i] ) = t−1 ∏ i=0
Pr LM (r i+1 |q, r [i] ).</p>
<p>(2)</p>
<p>As Equation 2 suggested, at timestamp i, the language model generates the next reasoning step r i based on previous premises, which is r i ∼ Pr LM (r i |q, r [i−1] ).This formulation follows the principle of deductive reasoning.</p>
<p>Step-wise Beam Search</p>
<p>Under beam size m, traditional beam search decodes at token level, which stores Topm candidate tokens, and uses them for future decoding.Formally, we denote the logprobability of LM generating the k-th tokens as ϕ(y k ) = log Pr LM (y k |y 1 , y 2 , ...,
y k−1 , x) = log Pr LM (y k |y [k−1] ,
x), and the log-probability of a solution at timestamp k as Φ(y
[k] ) = ∑ i∈[k] ϕ(y i ).
Given a set of m previous solutions at timestamp i as
Y i−1 = {y 1 [i−1] , y 2 [i−1] , ..., y m [i−1]
}, beam search generates as:
Y [i] = arg max y 1 [i] ,y 2 [i] ,...,y m [i] ∑ k∈[m] Φ(y k [i] ).(3)
However, in reasoning tasks, it is hard to verify whether a single token is deducible.Thus, we assign a reasoning step r as the minimal unit in step-wise beam search.Formally, we denote the log-probability of generating the k-th reasoning step as ψ(r k ) = Φ(y) and the log-probability of a solution at timestamp k to be Ψ(r
[k] ) = ∑ i∈[k] ψ(r i ). Given a set of m previous solutions at timestamp i as R [i−1] = {r 1 [i−1] , r 2 [i−1] , ..., r m [i−1] }, step-wise beam search infers as: R [i] = arg max r 1 [i] ,r 2 [i] ,...,r m [i] ∑ k∈[m] Ψ(r k [i] ).(4)
Combining multi-step CoT reasoning with step-wise beam search balances exploration and exploitation in reasoning tasks.However, confidence scores from language models cannot verify logical consequence between a reasoning step and its premises.To tackle this problem, we propose to constrain the step-wise beam search with deductive scores.</p>
<p>Deductive Verification Constrained Beam Search</p>
<p>To verify the logical coherence between a reasoning step and its premises, we propose to train a deductive verifier since LLM itself often fails to detect reasoning errors (Hong et al., 2023).Formally, given premises c [i] = [c 1 , c 2 , ..., c i ] and the candidate reasoning step r, the deductive score can be formulated as:
s = f (c [i] , r) = Pr f (r|c 1 , c 2 , ..., c i ),
where f is the deductive verifier function.The details of the deductive verifier are illustrated in Section 3.Then, we utilize the deductive verifier to constrain the step-wise beam search.To clearly illustrate the process, we show the case how, given one antecedent solution beam
r [i−1] ∈ R [i−1]
at timestamp i, reasoning steps are sampled and scored.</p>
<p>In the exploration phase of beam search, the language model samples a list of potential reasoning steps.Concretely, for sampling times n, the question q and r
[i−1] form the current context c [i] = [q, r [i−1] ],
and we can sample a set of n possible reasoning steps Ri = {r 1 , r 2 , ..., r n }, where r ∼ Pr LM (r|q,
r [i−1] ). Concatenating r ∈ Ri with r [i−1] generates candidate reasoning chains set R[i] = {[r [i−1] , r 1 ], [r [i−1] , r 2 ], ..., [r [i−1] , r n ]} at timestamp i.
In terms of exploitation, instead of using the language model probability Pr LM (r|c [i] ) to evaluate these reasoning steps, deductive verification scores S = {s 1 , s 2 , ..., s n } of candidate reasoning paths Ri are applied.Each score s j , j ∈ [n] is calculated by multiplying the score of r [i−1] and the score of each candidate reasoning step, that is:
s j = s([r [i−1] , r j ]) = s(r [i−1] ) × Pr f (r j |q, r [i−1] ) = i ∏ k=1 Pr f (r k |q, r [k−1] ),(5)
which follows the autoregressive factorization form and allows us to apply on the beam search algorithm.</p>
<p>Consequently, LM generates n times for each beam at each step, sampling a total number of m × n candidate reasoning steps.After scoring on these steps, the top m of them are selected according to the deductive score.This cycle of exploration and exploitation repeats until the final answer is generated or it reaches the upper limit of reasoning length.</p>
<p>Deductive Verifier</p>
<p>As stated above, a deductive verifier evaluates whether the reasoning step can be deduced from previous contexts, which resembles a natural language inference (NLI) task.Thus, we use deberta-v3-large (He et al., 2021), which achieves the best performance across various NLI benchmarks despite its small size, as the backbone.A small scalar head is adopted to predict deductive scores based on embedding the [CLS] token.</p>
<p>However, the difficulties of training a deductive verifier primarily reside in the training data quality and the training method.Changing one single token could lead to various errors, which is hard for any model to detect.Furthermore, the lack of high-quality false deductive reasoning step hinges the training of the verifier.To fully understand how LLMs make mistakes, we dive into the incorrect samples generated by LLMs.From the perspective of deductive reasoning, there are two main classes of reasoning errors: grounding errors and logic errors (Ling et al., 2024).Most grounding errors happening in the reasoning process can be detected by finding the contradiction between the context and the rationales, while the latter ones are illogical reasoning steps deduced from the previous context.</p>
<p>Thus, we propose a scalable and labor-free data construction method and a ranking-based training framework to teach the verifier to detect false reasoning steps.The whole training is divided into two stages.In stage 1, we heuristically corrupt gold reasoning steps to simulate typical false reasoning and train the verifier to detect them.In stage 2, the verifier trained from stage 1 is deployed to detect potential false reasoning steps generated by LLMs, bridging the gap between synthetic data and real-world data.Consequently, the model from stage 1 is continue-trained.</p>
<p>A General Deductive Verifier</p>
<p>In the first stage, we require the verifier to detect two general types of reasoning errors: grounding error and logic error.However, such fine-grained step-wise data is hard to annotate.Thus, we propose to synthesize false reasoning steps automatically.</p>
<p>Since it is hard to edit natural language to meet our demands, we turn to arithmetic reasoning, which can be viewed as a middle ground between symbols and natural language.</p>
<p>In terms of reasoning steps with grounding errors, we randomly replace one of the numbers on the left side of the equation in the gold reasoning step with numbers existing in previous contexts or randomly generated numbers to simulate false grounding or hallucinations.As for logic errors, we randomly select reasoning steps after the current gold reasoning step.Under this circumstance, the reasoning process is reversed and disrupted, making it a logic error.Moreover, to enhance the understanding of the model for this task, we use randomly selected reasoning steps across the whole dataset as an irrelevant false reasoning step.The examples of these errors are shown in Table 1.</p>
<p>To provide fine-grained supervision for error detection, we use margin ranking (Shashua &amp; Levin, 2002) to model the task.Specifically, given context c, gold reasoning step r, and three false reasoning steps r ′ 1 , r ′ 2 , and r ′ 3 , respectively representing grounding error, logic error, and irrelevant reasoning step, the verifier f scores all the candidates through s = f (c, r), which outputs four scores s, s ′ 1 , s ′ 2 , and s ′ 3 .Then, the loss of ranking these reasoning steps is formulated as the weighted sum of three margin ranking losses:
L = − 3 ∑ i=1 α i × (s − s ′ i − m i ),(6)
where m i is the hyper-parameter controlling the margin and α i weighs each loss.</p>
<p>Deductive Verifier with Model Feedback</p>
<p>In the first stage, we train a general deductive verifier, but the wrong samples synthesized heuristically are less diverse than the ones encountered during inference.To bridge the gap between synthesized data and real-world data, we use the verifier from stage 1 to detect false reasoning steps generated by an actual language model, where we choose Llama2-7b for the generation.The reason why we choose a relatively small language model for the generation is to maximize the diversity and the likelihood of generating incorrect steps.</p>
<p>To be concrete, given the verifier f 1 trained by stage 1, we feed context c into the LLM and sample 10 reasoning steps.Then, these steps are scored and ranked by f 1 .From this ranking, we select the reasoning step that exhibits the most significant decrease in the deductive score, designating it as the hard negative sample.We replace r 4 Experimental Setup</p>
<p>Reasoning Tasks</p>
<p>For our evaluation, we choose benchmarks from 3 different reasoning genres, namely, arithmetic reasoning, commonsense reasoning, and symbolic reasoning.These 3 types of reasoning tasks represent diverse reasoning paradigms.</p>
<p>Arithmetic Reasoning.Following Li et al. (2023) and Ling et al. (2024), we choose GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), AQuA (Ling et al., 2017), SingleEq (Koncel-Kedziorski et al., 2015), and MultiArith (Roy &amp; Roth, 2016) for evaluation.For AQuA, we evaluate the accuracy by comparing with the answer of the ground truth.</p>
<p>Commonsense Reasoning.Following Li et al. (2023), we use CommonsenseQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021).CommonsenseQA asks the model to choose the best answer from 5 choices, and StrategyQA asks for a True/False answer.</p>
<p>Symbolic Reasoning.We use the Coin Flip dataset (Wei et al., 2022b).The task is to determine which face of the coin is up after a series of operations.</p>
<p>Details</p>
<p>Language Models.We evaluate our method on models of various scales, including Llama2-7b, Llama2-13b, Llama2-70b (Touvron et al., 2023), and ChatGPT (gpt-3.5-turbo-instruct)(Ope-nAI, 2022).These models represent different levels of reasoning abilities.For the verifier, we choose deberta-v3-large as the backbone of our verifier.The training details are in Appendix B.</p>
<p>Prompts.For arithmetic reasoning tasks, we apply one prompt to all tasks.For commonsense reasoning tasks and the symbolic reasoning task, we write a prompt for each task to ensure the model can output the correct answer format.All methods are evaluated by the same prompt on each task.The details of prompts are in Appendix B.3.</p>
<p>Baselines.</p>
<p>To prove the effectiveness of DBS, we compare with greedy decoding algorithm (Jurafsky &amp; Martin) and self-consistency (Wang et al., 2022).For other SOTA baselines, we choose SelfEval (Xie et al., 2023) and Deductive Verification (Ling et al., 2024), which do not update the parameters of LLMs.The former represents SOTA decoding algorithm, while the latter stands for methods utilizing novel procedure design to conduct deductive reasoning.Since a full-scale experiment requires excessive token cost due to the extensive search and verification of these methods, we provide results on the GSM8K dataset.</p>
<p>Inference.During inference, we set beam size m to 5 and sampling times n to 10.For all models and baselines, we use their default parameter settings for generation.</p>
<p>Table 2 demonstrates the overall performance of the methods.We compare DBS with baselines under two paradigms: single chain setting and multiple chain setting.In multiple chain setting, the generated outcomes are integrated with self-consistency.Table 3 presents a comparative analysis of our approach against SOTA baselines.</p>
<p>Effectiveness</p>
<p>As shown in Table 2, DBS improves the performance across models of different scales and diverse reasoning tasks.For the single chain setting, the improvement is substantial.On arithmetic reasoning tasks, taking GSM8K as an example, we observe an increase from 7.6% to 16.6% across models of various scales.Specifically, with Llama2-7b and gpt-3.5-turboinstruct,DBS yields improvements of 9.2% and 7.0%, respectively, affirming the effectiveness of our proposed strategy.On commonsense reasoning tasks and symbolic reasoning tasks, we can expect an average increase of 2.5%/2.0%on models of all scales.</p>
<p>Regarding the multiple reasoning chain setting, DBS outperforms naive self-consistency.Concretely, we can see an average of 3.0% improvement on arithmetic reasoning tasks, 0.5% on commonsense reasoning tasks, and 1.0% on symbolic reasoning tasks, respectively.On the SingleEq and StrategyQA datasets, the performance of DBS on ChatGPT is slightly lower (-0.6%/-0.8%).These datasets require fewer reasoning steps for the final answer, as opposed to our paradigm of multiple reasoning steps.Nevertheless, the universal improvements demonstrate the effectiveness of our proposed method.The comparative results, grounded in accuracy and token cost metrics, substantiate our approach's effectiveness and token efficiency.Notably, the self-evaluate pattern performs worse when the scale of the LLM drops.Moreover, it consumes an excessive amount of tokens during evaluation.In contrast, DBS enhances the performance by approximately 10% on Llama2-7b and performs better than the baselines on gpt-3.5turbo-instructacross the paradigms of single and multiple reasoning chains.Furthermore, it consumes much fewer input and output tokens.Consuming 80 times less tokens, DBS outperforms DV on gpt-3.5-turbo-instruct.</p>
<p>Comparison with Current Solutions</p>
<p>Analysis</p>
<p>We conduct a detailed analysis to investigate the verifiability and efficiency of our proposed method.Moreover, we show how our method can adapt to different settings.</p>
<p>Verifier Analysis</p>
<p>Method MRR HITS@1 HITS@3 HITS@5 ChatGPT 0.48 0.32 0.49 0.67 Our Verifier 0.59 0.41 0.69 0.86 We comprehensively analyze the verifiability of our proposed verifier on performance (Table 4) and score distribution (Figure 3).</p>
<p>Empirical Study.</p>
<p>For empirical experiments, we test how our verifier will score the gold reasoning steps compared to synthesized reasoning steps.We randomly sample 500 context-step pairs from the test set of the  GSM8K dataset as gold reasoning steps.For each pair, we synthesize nine reasoning steps using Llama2-7b as inferior reasoning steps.The verifier's task is to rank them, and the performance is evaluated by four metrics, namely, Mean Reciprocal Rank (MRR), HITS@1/3/5.MRR evaluates the average rank of the gold reasoning step, and the HITS metrics reflect whether the gold reasoning step will be chosen under beam size setting 1/3/5.To compare with the self-evaluation pattern, we ask ChatGPT to rank these reasoning steps rather than predict scores, leveraging its inherent reranking capabilities (Ma et al., 2023).The results are listed in Table 4.Our verifier outperforms ChatGPT across all metrics, evidencing its capability.Notably, our verifier correctly identifies 86% of the gold reasoning steps within the top 5 positions out of 10 samples, affirming the deducibility of the reasoning paths decoded under m = 5, n = 10.</p>
<p>Distribution Analysis.To ascertain the reliability of our verifier, we compare the score distributions for correct and wrong predictions between the original LM confidence (gpt-3.5turbo-instruct)and our deductive verification score.We use results from greedy decoding, which naturally produces confidence scores from LM, and ask the verifier to score on them.Figure 3 shows the substantial difference between an LM confidence score and our deductive score.Notably, the LM confidence score demonstrates a mere 4% increase in scores of the correct reasoning paths, whereas our verifier exhibits a 17% increase.This significant difference proves the enhanced verifiability of our verification approach.</p>
<p>Cost Analysis</p>
<p>The cost of sampling multiple times is enormous.We analyze the cost of our methods under different settings and compare them with the baselines.Specifically, we compare our approach with greedy decoding, self-consistency, SelfEval (Xie et al., 2023), and DV (Ling et al., 2024).The results are presented in Table 3 and Table 5.Our analysis reveals that greedy decoding is the most token-economic, as it does not involve any form of sampling, but its performance lags.When m is constrained to 1, the token generation is minimized even further than that required by self-consistency strategies.Still, our method demonstrates higher accuracy.Moreover, DBS proves more effective and token-efficient under the same beam size than those leveraging LLMs' self-evaluation capabilities.Unlike arithmetic or symbolic reasoning tasks, commonsense reasoning tasks leverage the model's intrinsic commonsense.Thus, we introduce a prompt strategy, which asks the model to recall the necessary commonsense via greedy decoding, followed by DBS navigating the reasoning over the elicited contexts.We compare this setting to standard settings and baselines, and the results are shown in Table 6.The performance varies from different tasks, attributed to the distinct nature of the tasks.Concretely, StrategyQA typically requires a 2-3 step knowledge recall followed by a single reasoning step, whereas CSQA demands a 3-5</p>
<p>Commonsense Reasoning Task</p>
<p>Method</p>
<p>Question Greedy Ours James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?He runs 3 times a week, which is equal to 3 x 7 = 21 sprints.0.0102 The distance he runs each sprint is 60 meters, so the total distance is 60 x 21 = 1320 meters.0.4963 Final Answer:1320 Each sprint is 60 meters.0.8270 So he runs 60 x 3 = 180 meters each day.0.9643 He runs each sprint 3 times a week.0.9983 Therefore, he runs a total of 180 x 3 = 540 meters per week.step recall process alongside multiple reasoning steps.Our findings suggest that the recall prompt is more suitable for tasks demanding multi-step reasoning.</p>
<p>Case Study</p>
<p>Table 7 presents two GSM8K examples from Llama2-7b.The first example demonstrates a scenario where hallucination emerges under greedy decoding, which our verifier identifies with an extremely low score, thereby precluding its selection by our deductive decoding strategy.On the contrary, every reasoning step from our reasoning path is deduced from the previous context and is scored much higher than the incorrect steps.In the second example, a grounding error occurs in steps marked by red scores.Although the wrong reasoning steps resemble the correct ones, our verifier detects these minor errors.In both examples, the reasoning path generated by our decoding strategy initiates the reasoning by listing premises, followed by one reasoning step.This pattern strictly follows the principle of deductive reasoning, making the generated results more deducible.</p>
<p>Related Work</p>
<p>Answer Aggregation.Sampling techniques of language models, such as temperature sampling (Ackley et al., 1985), top-k sampling (Fan et al., 2018), and top-p sampling (Holtzman et al., 2019), bring diversity to the outcome but also uncertainty to the reasoning process, which is not favored in reasoning tasks.These methods aim to reduce uncertainty in the reasoning process by aggregating answers from sampled reasoning paths.After sampling diverse outputs from LLMs, Wang et al. (2022) propose to use consistency as the metric to aggregate the answers.Other methods evaluate whether the reasoning step can lead to the correct answer by training a verifier (Li et al., 2023;Wang et al., 2023).</p>
<p>Self-Evaluation.</p>
<p>Recent research on reducing reasoning errors is inclined to follow the self-verify-then-correct pattern (Dhuliawala et al., 2023;Weng et al., 2022;Zhang et al., 2023b;Ling et al., 2024;Miao et al., 2023).They design different procedures and prompts to achieve better performance.Taking two typical approaches as examples, Dhuliawala et al. (2023) design a chain-of-verification procedure to verify facts from its outputs, and Miao et al. (2023) ask LLM to detect errors in their step-by-step rationales.However, recent works (Huang et al., 2023;Hong et al., 2023;Xie et al., 2024) have pinpointed a critical limitation of LLMs in self-correction during reasoning tasks.Their findings suggest that LLMs indiscriminately alter reasoning steps without external feedback, irrespective of their initial accuracy.</p>
<p>Decoding Strategies.Conventional decoding strategies include greedy decoding (Teller, 2000), which selects tokens with the highest probabilities, and beam search (Graves, 2012), which stores candidate beams for future prediction.In the era of LLMs, these decoding strategies are implemented at a more coarse-grained level, especially on reasoning tasks involving multiple steps.They decompose the reasoning process into steps (Khot et al., 2022) and apply decoding or search algorithms (Yao et al., 2023;Xie et al., 2023).</p>
<p>Conclusions</p>
<p>In this paper, we aim to eliminate errors in intermediate reasoning steps in CoT reasoning, making it more reliable.To this end, we propose Deductive Beam Search that integrates CoT with step-wise beam search and scores each reasoning step with a deductive verifier, which verifies whether the reasoning step is a logical consequence.Beam search explores by sampling potential reasoning steps, while the verifier exploits by selecting the most deducible steps.To train such a verifier, we propose a scalable and labor-free data construction method.It initiates by heuristically introducing errors into gold reasoning steps and enhances the diversity and difficulty of training data by synthesizing hard negatives through the verifier trained on those typical wrong steps.Extensive experiments show our method's effectiveness across various model scales and diverse reasoning tasks without changing the standard CoT paradigm and parameters of LLMs.Further analysis proves the verifiability and robustness endowed by our verifier, thereby significantly improving the deducibility of the generated reasoning paths.</p>
<p>Jian Xie, Kexun Zhang, Jiangjie Chen, Siyu Yuan, Kai Zhang, Yikai Zhang, Lei Li, and Yanghua Xiao.Revealing the barriers of language agents in planning.arXiv preprint arXiv:2410.</p>
<p>A Extended Experiments</p>
<p>Ablation study on beam size.We conduct experiments on how beam size affects performance.Figure 4 shows the trend of DBS performance under single chain setting and multiple chain setting.The performance steadily grows when the beam size rises.</p>
<p>Verifier Robustness.To ensure that our verifier can equally verify reasoning steps generated by different models, we visualize the accuracy under different deductive score thresholds for Llama2-7b and ChatGPT, as depicted in Figure 5.The lines in the figure represent polynomial fits of the data.Their near-parallel alignment suggests the robustness of performance improvement across these models as the threshold increases.Intriguingly, these lines also offer insights into the inductive reasoning capabilities of the two models.Ideally, the accuracy approaches zero at the deductive score zero.However, the observed non-zero accuracy suggests the models' inductive reasoning capabilities.Although the reasoning process might not align with the deductive reasoning paradigm, LMs can still arrive at correct conclusions, likely by intuitively skipping over specific reasoning steps, which is the act of inductive reasoning.DBS Robustness.To demonstrate that DBS can sustain its accuracy as the length of the reasoning steps increases, we conducted experiments to analyze the impact of reasoning step length on the final outcome's accuracy.The results, presented in Figure 6, indicate that DBS consistently achieves higher accuracy, even when the reasoning process extends to 15 steps.</p>
<p>B Experimental Details</p>
<p>B.1 Training Data</p>
<p>At stage 1, we choose GSM8K dataset to train the general deductive verifier.The gold rationales provided are decomposed into sentences as gold reasoning steps.After filtering out some steps that cannot be altered into false reasoning steps, we construct a training dataset of 22,362 samples.At stage 2, the verifier from stage 1 is used to generate hard negative reasoning steps as stated in Sec. 3. We choose Llama2-7b as our language model to generate candidate false reasoning steps.For arithmetic reasoning and symbolic reasoning tasks, we Given the question, please give the rationales step by step and give a final answer.</p>
<p>Example 1: Question: Kate's hair is half as long as Emily's hair.Emily's hair is 6 inches longer than Logan's hair.If Logan hair is 20 inches, how many inches is Kate's hair?Answer: Emily's hair is 20-6 = 14 inches long.Kate's hair 14/2= 7 inches long.Final Answer:7</p>
<p>Example 2: Question: John puts $25 in his piggy bank every month for 2 years to save up for a vacation.He had to spend $400 from his piggy bank savings last week to repair his car.How many dollars are left in his piggy bank?Answer: He saved money for 2 years, which is equal to 12 x 2 = 24 months.The amount of money he saved is $25*24 = $600.But he spent some money so there is $600 -$400 = 200 left.Final Answer:200</p>
<p>B.2 Training Details</p>
<p>At stage 1, we finetune deberta-v3-large with learning rate 1 × 10 −5 and batch size 128.As for margins, we set the margins between the gold reasoning step and grounding error step/logic error step/irrelevant step to 0.3/0.6/0.9.At stage 2, we continue to finetune the verifier from stage 1 with learning rate 1 × 10 −6 and batch size 128.As for margins, we set the margins between the gold reasoning step and hard negative reasoning step/logic error step/irrelevant step to 0.3/0.6/0.9.</p>
<p>B.3 Prompts</p>
<p>For the results in Table 2, we use the following prompts:</p>
<p>• Arithmetic reasoning tasks share the same prompt, shown in Table 8.</p>
<p>• For StrategyQA, we use the prompt in Table 9.</p>
<p>• For CSQA, we use the prompt in Table 10.</p>
<p>• For Coin, we use the prompt in Table 11.</p>
<p>For the results in Table 6, we use the following prompts:</p>
<p>• For StrategyQA, we use prompt in Table 12.</p>
<p>• For CSQA, we use prompt in Table 13.</p>
<p>Example 1 Question: A coin is heads up.sager does not flip the coin.zyheir flips the coin.Is the coin still heads up?Answer: sager does not flip the coin, so the coin is heads up.zyheir flips the coins, so the coin is tails up.Final Answer: no Example 2 Question: A coin is heads up.mailey does not flip the coin.maurisa does not flip the coin.Is the coin still heads up?Answer: mailye does not flip the coin, so the coin is heads up.maurisa does not flip the coin, so the coin is heads up.Final Answer: yes</p>
<p>Figure 1 :
1
Figure 1: Example of error in an intermediate step leading to accumulative error from Llama2-7b.The dependency on intermediate steps introduces accumulative errors in the reasoning process.</p>
<p>as many sheep as Seattle.How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?Toulouse has twice as many sheep as Charleston, so Toulouse has 2 x 4 = 8 sheep.0.3149 Charleston has 4 times as many sheep as Seattle, so Charleston has 4 x 20 = 80 sheep.0.9877 So Toulouse has 8 + 80 = 88 sheep.0.3923 Seattle has 20 sheep.0.7479 Final Answer:88 + 20 + 88 = 196 Charleston has 4 times as many sheep as Seattle.Charleston has 20 x 4 = 80 sheep.0.9912 Toulouse has twice as many sheep as Charleston.Charleston has 80 sheep.Toulouse has 80 x 2 = 160 sheep.0.9736 Finally, add the number of sheep Toulouse, Charleston, and Seattle have.0.9981 So the total number of sheep they have together is 160 + 80 + 20 = 260 sheep.0.9924 Final Answer:260Table 7: Two GSM8K examples from Llama2-7b scored with our verifier.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Accuracy under different beam sizes on different models.</p>
<p>Figure 6 :
6
Figure 6: Accuracy under different reasoning step length.</p>
<p>use the MetaMathQA dataset to generate training data.For commonsense reasoning tasks, we use the StrategyQA dataset to generate training data.Finally, we train the arithmetic verifier on 150,000 samples and the commonsense verifier on 5,000 samples.</p>
<p>Table 1 :
1
Examples of heuristically synthesized false reasoning steps.
ContextTypeNot. Reasoning StepError ReasonRandy has some money.GoldrRandy has 5<em>4=20 dollars left after-He spent $10 buying hisbuying lunch.lunch. He spent a quarter of the money he had left on an ice cream cone. If the ice cream cone cost $5, what is the amount of money, in dollars, Randy had at first?Grounding Logic Irrelevantr ′ 1 r ′ 2 r ′ 3Randy has 10</em>4=40 dollars left after buying lunch. At first, Randy had a sum of 20+10=30 dollars. He eats 65 black cookies from the cookie jar, with 1/2 * 130 = 65.Minor token-level error, hard for models to detect. Logic-level error caused by reversed steps, not following deductive reasoning. Major error, completely incoherent with the context.</p>
<p>Table 2 :
2
The result comparison on arithmetic reasoning, commonsense reasoning, and symbolic reasoning tasks.The results represent accuracy (%) on each dataset.Bold indicates best results and underline indicates second bests.
MethodArithmetic ReasoningCommonsense ReasoningSymbolic ReasoningGSM8K SVAMP AQuA SingleEq MultiArith Avg.↑↓ StrategyQA CSQA Avg.↑↓ CoinAvg.↑↓Llama2-7bGreedy DBS22.0 31.249.0 55.03.2 5.767.5 69.068.3 74.4+5.364.0 66.466.9 67.0+2.653.8 51.6-2.2SC DBS + SC28.1 32.156.7 59.34.9 8.577.5 78.977.8 85.6+3.965.6 67.667.2 68.3+1.653.0 54.1+1.1Llama2-13bGreedy DBS35.6 43.252.3 58.02.8 6.172.2 76.770.6 85.6+7.266.2 64.653.2 53.7-0.660.2 61.2+1.0SC DBS + SC42.0 45.268.3 72.03.6 9.386.4 90.791.7 94.4+3.965.4 66.668.0 69.8+1.561.8 63.4+1.6Llama2-70bGreedy DBS41.7 58.351.3 61.710.1 10.170.0 78.970.6 90.6+11.269.8 70.659.4 62.4+1.971.2 80.4+8.7SC DBS + SC64.8 67.679.3 79.310.5 14.591.3 92.797.2 97.2+1.674.0 75.074.0 73.3+0.279.6 80.2+0.6ChatGPTGreedy DBS68.8 75.972.0 75.716.5 24.895.1 92.897.2 97.8+3.265.4 68.665.1 74.0+6.275.1 75.5+0.4SC DBS + SC81.3 83.581.3 82.720.2 28.897.6 97.098.3 99.4+2.570.6 69.875.4 78.3+1.178.9 79.5+0.6
′ 1 with the generated hard negative sample as r ′′ 1 , keeping the original way of generating r ′ 2 and r ′ 3 .Consequently, we continue training the verifier f 1 by Equation 6 with a smaller learning rate.</p>
<p>Table 4 :
4
Verification ability comparison of our verifier and ChatGPT.</p>
<p>Table 5 :
5
Cost Analysis of our method and other baseline methods.m represents beam size and n represents sampling times.</p>
<p>Table 6 :
6
Different prompt settings on commonsense reasoning tasks.We use Llama2-7b as the backbone for all methods.
StrategyQA CSQAGreedy64.0066.91DBS66.4066.99-w. recall prompt65.4066.67SC65.6067.24DBS67.6068.29-w. recall prompt65.8069.45</p>
<p>12409, 2024.Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie.Self-evaluation guided beam search for reasoning.In Thirty-seventh Conference on Neural Information Processing Systems, 2023.Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.Tree of thoughts: Deliberate problem solving with large language models.arXiv preprint arXiv:2305.10601,2023.Junchi Yu, Ran He, and Rex Ying.Thought propagation: An analogical approach to complex reasoning with large language models.arXiv preprint arXiv:2310.03965,2023.Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith.How language model hallucinations can snowball.arXiv preprint arXiv:2305.13534,2023a.Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao.Cumulative reasoning with large language models.arXiv preprint arXiv:2308.04371,2023b.
32DBS-single DBS-multiple Greedy30Accuracy26 282422123 Beam Size45</p>
<p>Table 8 :
8
Prompt for arithmetic reasoning tasks.</p>
<p>Table 11 :
11
Prompt for Coin.Given the question, output the rationale step by step and give the final answer (yes or no).
Example 1Question:Do hamsters provide food for any animals?Answer:Fact:Hamsters are prey animals.Prey are food for predators.Reasoning:Hamsters are food for some predators.Final answer: yesExample 2Question:Could a llama birth twice during War in Vietnam (1945-46)?Answer:Fact:The War in Vietnam was 6 months.The gestation period for a llama is 11 months, which is more than 6 months.Reasoning:A llama could not birth twice during War in Vietnam.Final answer: no</p>
<p>Table 12 :
12
Prompt for StrategyQA with prompt.</p>
<p>AcknowledgmentsThe authors would like to thank Jiahao Shi and Yikai Zhang from Fudan University, and Yifei Li from the Ohio State University, as well as the anonymous reviewers for their valuable comments.Published as a conference paper at COLM 2024 Given the question, output the rationale step by step and give the final answer (yes or no).
A learning algorithm for boltzmann machines. Geoffrey E David H Ackley, Terrence J Hinton, Sejnowski, Cognitive science. 911985</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>The nuxmv symbolic model checker. Roberto Cavada, Alessandro Cimatti, Michele Dorigatti, Alberto Griggio, Alessandro Mariotti, Andrea Micheli, Sergio Mover, Marco Roveri, Stefano Tonetta, Computer Aided Verification: 26th International Conference. 2014</p>
<p>Linguistic processes in deductive reasoning. Clark Herbert, Psychological review. 1969</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, Zeb Kurth-Nelson, arXiv:1901.08162Causal reasoning from meta-reinforcement learning. 2019arXiv preprint</p>
<p>Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston, arXiv:2309.114952023arXiv preprint</p>
<p>Logical consequences: A key to the reduction of disciplinary problems. The Phi Delta Kappan. Don Dinkmeyer, 1976</p>
<p>Foundations of human reasoning in the prefrontal cortex. Maël Donoso, Anne Ge Collins, Etienne Koechlin, Science. 2014</p>
<p>Minimizing the accumulated trajectory error to improve dataset distillation. Jiawei Du, Yidi Jiang, Joey Tianyi Vincent Yf Tan, Haizhou Zhou, Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsJuly 20181</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 2021Transactions of the Association for Computational Linguistics</p>
<p>Palm: Scaling language modeling with pathways. Google, Journal of Machine Learning Research. 2023</p>
<p>Sequence transduction with recurrent neural networks. Alex Graves, arXiv:1211.37112012arXiv preprint</p>
<p>The concept of logical consequence. The Philosophical Review, 1997. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. H William, Hanson, arXiv:2111.095432021arXiv preprint</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, International Conference on Learning Representations. 2019</p>
<p>A closer look at the self-verification abilities of large language models in logical reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, arXiv:2311.079542023arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Deductive reasoning. Annual Review of Psychology. P N Johnson-Laird, Phil Johnson-Laird. Deductive reasoning. WIREs Cognitive Science. 1999. 2010</p>
<p>Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. Daniel Jurafsky, James H Martin, </p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , 2015Transactions of the Association for Computational Linguistics</p>
<p>Shen Li, Hengru Xu, Zhengdong Lu, arXiv:1808.10326Generalize symbolic knowledge with neural rule engine. 2018arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics12023</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsJuly 20171</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. 202436</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Large language model is not a good few-shot information extractor, but a good reranker for hard samples! In Findings of the. Yubo Ma, Yixin Cao, Yong Hong, Aixin Sun, 2023. 2023Association for Computational Linguistics</p>
<p>From google gemini to openai q*(q-star): A survey of reshaping the generative artificial intelligence (ai) research landscape. Teo Timothy R Mcintosh, Tong Susnjak, Paul Liu, Malka N Watters, Halgamuge, arXiv:2312.108682023arXiv preprint</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. Ning Miao, Yee Whye Teh, Tom Rainforth, arXiv:2308.004362023arXiv preprint</p>
<p>Openai, Chatgpt, arXiv:2303.08774Gpt-4 technical report. 2022. 2023arXiv preprint</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, arXiv:2304.01904Refiner: Reasoning feedback on intermediate representations. 2023arXiv preprint</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, arXiv:1608.014132016arXiv preprint</p>
<p>Ranking with large margin principle: Two approaches. Amnon Shashua, Anat Levin, Advances in neural information processing systems. 200215</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition. Virginia Teller, 2000</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>How easily do irrelevant inputs skew the responses of large language models. Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao, arXiv:2404.033022024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
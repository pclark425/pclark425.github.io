<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8789 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8789</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8789</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-272690023</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.10294v2.pdf" target="_blank">MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> For the knowledge graph to text(KG-to-text) generation task, recent works have attempted to incorporate graph structure information into pre-trained language models(PLMs) to capture the structure information of knowledge graphs. However, these improvements only capture single-granularity structure information, either between entities in the original graph or between words within or across entities. Considering only entity-level structures neglects the semantic information between words, while focusing solely on word-level structures ignores the relationships between complete entities. Therefore, this paper proposes the Multi-Granularity Graph Structure Attention (MGSA), which integrates both granularities of information. The encoder of the model architecture features an entity-level structure encoding module, a word-level structure encoding module, and an aggregation module that synthesizes information from both structure. This allows the model to more comprehensively understand the information contained in the original knowledge graph, thereby improving the quality of the generated text. We evaluated the MGSA model on two popular KG-to-text generation benchmark datasets and achieved superior performance compared to models based on single-granularity structures.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8789.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8789.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGSA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Granularity Graph Structure Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KG-to-text model that injects multi-granularity graph structure into a pre-trained encoder (BART) via entity-level and word-level graph-structure attention modules and an aggregation module to improve text generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-granularity structure-aware linearization + attention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearize the KG into token sequences and augment the encoder with two parallel graph-structure attention encodings: (1) an entity-level encoding that builds a bipartite entity-relation graph and uses an adjacency matrix plus a relative-position matrix over node types, (2) a word-level encoding that expands nodes into word-nodes and uses a shortest-path-based relative position matrix R_N; final encoder representation is the aggregation of both granularities (weighted by hyperparameter λ).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (triples: head, relation, tail)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Concat triples into a token sequence using special tokens (<H>, <R>, <T>) for entity-level linearization; split entities/relations into words with markers ([N]) for word-level linearization; cluster triples by head-entity before concatenation to preserve topical locality.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge graph-to-text generation (KG-to-text) on WebNLG and EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>WebNLG: BLEU4=66.45, METEOR=46.93, ROUGEL=76.47; EventNarrative: BLEU4=35.22, METEOR=27.61, ROUGEL=64.46 (reported percentages/points as absolute metric scores).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms several single-granularity baselines: +2.34 BLEU4 and +1.90 ROUGEL over KGPT; surpasses JointGT, Graformer, GAP, UniD2T on BLEU4 by 0.53%, 5.30%, 0.25%, and 6.04% respectively on WebNLG; outperforms GAP on EventNarrative slightly (BLEU4 +0.14, METEOR +0.11).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simultaneously captures entity-level and intra-/inter-entity word-level structure, improving generation quality over single-granularity methods; integrates with PLMs (BART) to leverage pretrained linguistic knowledge; clustering triples by head helps preserve coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More complex encoder with multiple structure matrices and hyperparameters (e.g., λ); added computation and memory compared to pure linearization; optimal λ may require tuning (authors used λ=0.5 due to resource limits).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>For larger-scale KGs (many triples), generated texts exhibit missing content, hallucinations, and redundant expressions; model performance is sensitive to dataset quality (EventNarrative examples sometimes do not match KG well), and larger/complex graphs degrade fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8789.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGSA-Entity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MGSA Entity-level Structure Encoding Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Module that encodes entity- and relation-level graph structure by building a bipartite graph and injecting an adjacency matrix and a relative position matrix into attention computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity-level bipartite linearization + relative-position attention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Triples concatenated with tokens <H>, <R>, <T>; linear self-attention pooled (mean) to entity/relation vectors X_p; bipartite graph adjacency A used and a discrete relative position matrix R_E (values encode neighbor/entity-edge/edge-entity types) is added into scaled dot-product attention as an additive bias term mapped into vector space via γ.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entities and relation nodes; bipartite entity-relation graph)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Concatenate triples with <H>/<R>/<T> tokens (entity-level linearization), pool token-level encoder outputs into entity/relation node vectors, construct bipartite adjacency and relative-position matrices, compute graph-structure-aware attention, then remap node representations back to token space via gather().</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: removing entire entity-level module reduces BLEU4 from 66.45 (full) to 62.31 (w/o entity-level); models with only relative-position matrix or only adjacency matrix yield intermediate BLEU4 (relative-position-only ~64.86, adjacency-only ~65.34), indicating numeric impact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to methods that only add structure embeddings or design graph-aware modules (JointGT, GAP), entity-level encoding in MGSA combined with word-level encoding leads to better BLEU4/METEOR/ROUGEL; paper notes relative position encoding contributes more than adjacency in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly encodes inter-entity relation structure and relation-node roles; relative-position encoding (R_E) strongly improves performance (more than adjacency alone) and better preserves entity-level topology lost in flat linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Pooling from token to node-level (mean pooling) can lose fine-grained token-level information; constructing bipartite matrices and mapping back to token space adds complexity and potential mapping errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Ablation shows removing the entity-level module causes the largest drop in performance versus removing word-level module, indicating that if entity-level structure is missing or noisy, generation quality degrades significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8789.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGSA-Word</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MGSA Word-level Structure Encoding Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Module that encodes word-level semantic and structural relations by expanding entity/relation nodes into word-nodes and using shortest-path distances to form a relative position matrix for attention augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Word-level node expansion + shortest-path relative-position attention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Split each entity/relation into individual word nodes (marked with [N]) and build a word-level graph; compute shortest-path distances δ(n_i, n_j) between word nodes and derive R_N with special encoding for words from the same entity (encode(p) := sgn(p)*δ_max + p), unreachable pairs set to ∞; inject γ(R_N) into attention as an additive bias term.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs at token/word granularity (words within entities and relations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize each entity/relation to words, mark words with [N], concatenate words according to triple order to form X_N, compute word-level relative distance matrix from shortest-path distances in the word-graph, and add this encoded matrix into self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: removing word-level module reduces BLEU4 modestly (word-level removed BLEU4 ~65.58 vs full MGSA 66.45); removing word-level relative position matrix lowers performance by ~1 point compared to including it.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with Graformer and UniD2T which capture token/word-level structure, MGSA's combination with entity-level structure yields superior overall metrics; Graformer (word-level relative positions) performs worse than MGSA likely due to different encoder backbone (Transformer vs BART) and lack of entity-level encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures intra-entity semantic cohesion and inter-word structural paths, helping generate more semantically consistent phrases and maintain word-level relations that entity-level representations miss.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds substantial number of nodes (word-level expansion) increasing sequence length and compute; encoding of unreachable pairs as ∞ and special SAME encoding increases representation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Less impactful than entity-level module per ablation; when R_N is removed performance drops but not as much as removing entity-level info. Large graphs exacerbate missing/hallucination issues despite word-level encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8789.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Head-clustering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Head-entity Clustering of Triples (ordering heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing step that clusters triples by their head entities before linearization to keep information about the same entity adjacent in the input sequence, aligning linearized order with human topical coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Head-entity-based triple clustering + ordered linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Group triples that share the same head entity, and when linearizing the set of triples into the token sequence, place triples of the same head contiguously to preserve entity coherence and improve naturalness of generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Cluster triples by head entity before concatenation; then linearize each cluster in sequence (within clusters maintain triple word order) to form the input token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (improve fluency/coherence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No separate scalar metrics isolated, but authors state clustering preserves completeness and coherence and contributes to better text generation; experiments and case studies attribute improved alignment with human language habits to this step.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Most prior PLM-based linearization approaches use dataset-provided triple order or random order; MGSA's head-clustering is claimed to better match human topical discourse but no direct numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Better topical coherence and local completeness in generated descriptions; reduces interleaving of topics that can confuse decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Heuristic ordering that may not reflect optimal narrative order for all KGs; requires extra preprocessing and may hide inter-entity relations that cross clustered groups.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit numerical failure cases reported, but authors note that for large/misaligned KGs dataset quality still dominates performance and clustering cannot fully prevent hallucinations when reference text is not well aligned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8789.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-sequence Linearization of Knowledge Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common approach that transforms a KG (set of triples) into a flat token sequence used as input to pre-trained seq2seq models (e.g., BART, T5), often with special tokens indicating head, relation, tail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple-to-token linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Concatenate triples into a single token sequence, optionally with special markers (e.g., <H>, <R>, <T>) or entity/relation delimiters; feed directly into PLM encoder to perform seq2seq generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Concatenate triples (head, relation, tail) into tokens in arbitrary or dataset-provided order; may include separators/special tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation (used as baseline for PLM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BART-base baseline reported on WebNLG: BLEU4=64.55, METEOR=46.51, ROUGEL=75.13; T5-base: BLEU4=64.42, METEOR=46.58, ROUGEL=74.77 (from paper Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Linearization loses explicit graph structure; many works attempt to augment it with structure embeddings or graph-aware modules, and MGSA shows improvements over plain linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement and directly leverages PLM strengths; no graph encoder needed.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Loses structural/topological information in the KG; ordering ambiguity can hurt faithfulness and coherence; may produce lower-quality text than structure-aware methods.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors note that plain linearization causes loss of graph structure and alignment issues between KG and text; worse performance versus structure-aware approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8789.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KGPT: Knowledge-grounded pre-training for data-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that injects multi-level structure embeddings into the linearized token sequence for data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>KGPT: Knowledge-grounded pre-training for datato-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-level structure embeddings in linearized sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Adds additional learned embeddings to tokens in the linearized KG sequence to encode structural cues at multiple levels (token/entity/other) prior to PLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / structured data</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize triples into token sequence and augment token representations with structural embeddings representing multi-level information.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported on WebNLG: BLEU4=64.11, METEOR=46.30, ROUGEL=74.57 (Table II in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MGSA outperforms KGPT by +2.34 BLEU4 and +1.90 ROUGEL on WebNLG, suggesting MGSA's attention-based integration of structure is more effective than added embeddings alone.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves use of PLM while injecting structural signals without changing architecture dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Embedding-based injection may not fully capture topology or multi-hop relations; less effective than explicit graph-aware attention in MGSA (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper; MGSA results indicate KGPT underperforms when compared on same benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8789.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modeling graph structure via relative position for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that designs a relative graph position matrix to capture word-level structure information and integrates it into Transformer attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling graph structure via relative position for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Word-level relative position matrix for Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs a relative-position matrix over tokens/words derived from graph distances and injects it into Transformer attention to encode graph structure at word/token granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (word/token-level representation)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize entities/relations and compute pairwise relative positions/distances between tokens to produce a matrix added to scaled dot-product attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported from referenced results: Graformer BLEU4 ~61.15, METEOR=43.38 on WebNLG (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MGSA significantly outperforms Graformer (MGSA BLEU4 66.45 vs Graformer ~61.15 on WebNLG); paper suggests MGSA benefits also from using BART as backbone vs Graformer's Transformer backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures token-level distance structure and can be integrated into Transformer attention.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Single-granularity (word-level) misses entity-level topology; performance may depend on backbone model and integration choice.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper reports Graformer underperforms MGSA; no additional failure cases provided within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8789.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAP: A graph-aware language model framework for knowledge graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-aware attention/aggregation module that injects entity-level structure information into PLM-based encoders to improve KG-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GAP: A graph-aware language model framework for knowledge graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-aware attention module (entity-level)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Designs a module inside the encoder that uses graph connectivity to bias attention or aggregate node representations, focusing on entity-level structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entity-level graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize KG and add a graph-aware attention/aggregation module leveraging adjacency information to compute structure-aware encoder representations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported on WebNLG: BLEU4=66.20, METEOR=46.77, ROUGEL=76.36 (Table II); on EventNarrative GAP reported second-best BLEU4=35.08, METEOR=27.50 (Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MGSA slightly outperforms GAP on WebNLG and EventNarrative in aggregate metrics (MGSA BLEU4 66.45 vs GAP 66.20 on WebNLG; MGSA BLEU4 35.22 vs GAP 35.08 on EventNarrative).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly captures entity-level structure and has strong baseline performance, particularly effective on larger/complex KGs in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Focus on entity-level structure alone may miss word-level relations; MGSA's multi-granularity approach claims to improve further.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed beyond comparative metrics; MGSA authors note GAP is competitive but slightly weaker than MGSA in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8789.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JointGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JointGT: Graph-text joint representation learning for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that designs a structure-aware semantic aggregation module to capture entity-level structure information when converting KGs to text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structure-aware semantic aggregation (entity-level)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Integrates entity-level graph structure into encoder representations via a semantic aggregation module that combines graph and token sequence features.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entity-level)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize KG and use a joint representation module aligning graph nodes with token representations to aggregate structure-aware information.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported on WebNLG: BLEU4=65.92, METEOR=47.15, ROUGEL=76.10 (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MGSA outperforms JointGT modestly on BLEU4 (MGSA 66.45 vs JointGT 65.92) and competes on other metrics; JointGT focuses on entity-level structure while MGSA adds word-level structure as well.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves entity-level structural encoding and yields competitive metrics versus simpler baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Lacks simultaneous word-level structural modeling; MGSA authors argue multi-granularity yields further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper beyond comparative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8789.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8789.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniD2T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unifying Structured Data as Graph for Data-to-Text Pre-Training (UniD2T)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method for pre-training models on varied structured data by converting different structured inputs into unified graph representations and injecting structure-enhancement modules for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying Structured Data as Graph for Data-to-Text Pre-Training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Unified structured-data-as-graph + structure-enhancement module</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treats diverse structured inputs as graphs and uses a structure-enhancement module to capture word-level structure information across different data-to-text tasks, enabling unified pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General structured data represented as graphs (including KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Convert structured inputs into graph form, linearize tokens and inject structure-enhancement representations to aid PLM pretraining and downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text and KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UniD2T reported on WebNLG: BLEU4=60.41, METEOR=44.35, ROUGEL not reported in table for some baselines (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MGSA outperforms UniD2T on WebNLG; authors speculate UniD2T's multi-task pretraining across varied structured data may dilute performance on KG-specific tasks versus MGSA focused on KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Generalizable across structured data types and benefits from pretraining across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May underperform specialized KG-focused models on KG-to-text benchmarks; reported lower BLEU4 in this comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper; comparative metrics show lower scores vs MGSA on WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modeling graph structure via relative position for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>KGPT: Knowledge-grounded pre-training for datato-text generation <em>(Rating: 2)</em></li>
                <li>GAP: A graph-aware language model framework for knowledge graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Unifying Structured Data as Graph for Data-to-Text Pre-Training <em>(Rating: 2)</em></li>
                <li>Boosting KG-to-Text Generation via Multi-granularity Graph Representations <em>(Rating: 1)</em></li>
                <li>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8789",
    "paper_id": "paper-272690023",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "MGSA",
            "name_full": "Multi-Granularity Graph Structure Attention",
            "brief_description": "A KG-to-text model that injects multi-granularity graph structure into a pre-trained encoder (BART) via entity-level and word-level graph-structure attention modules and an aggregation module to improve text generation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Multi-granularity structure-aware linearization + attention",
            "representation_description": "Linearize the KG into token sequences and augment the encoder with two parallel graph-structure attention encodings: (1) an entity-level encoding that builds a bipartite entity-relation graph and uses an adjacency matrix plus a relative-position matrix over node types, (2) a word-level encoding that expands nodes into word-nodes and uses a shortest-path-based relative position matrix R_N; final encoder representation is the aggregation of both granularities (weighted by hyperparameter λ).",
            "graph_type": "Knowledge graphs (triples: head, relation, tail)",
            "conversion_method": "Concat triples into a token sequence using special tokens (&lt;H&gt;, &lt;R&gt;, &lt;T&gt;) for entity-level linearization; split entities/relations into words with markers ([N]) for word-level linearization; cluster triples by head-entity before concatenation to preserve topical locality.",
            "downstream_task": "Knowledge graph-to-text generation (KG-to-text) on WebNLG and EventNarrative",
            "performance_metrics": "WebNLG: BLEU4=66.45, METEOR=46.93, ROUGEL=76.47; EventNarrative: BLEU4=35.22, METEOR=27.61, ROUGEL=64.46 (reported percentages/points as absolute metric scores).",
            "comparison_to_others": "Outperforms several single-granularity baselines: +2.34 BLEU4 and +1.90 ROUGEL over KGPT; surpasses JointGT, Graformer, GAP, UniD2T on BLEU4 by 0.53%, 5.30%, 0.25%, and 6.04% respectively on WebNLG; outperforms GAP on EventNarrative slightly (BLEU4 +0.14, METEOR +0.11).",
            "advantages": "Simultaneously captures entity-level and intra-/inter-entity word-level structure, improving generation quality over single-granularity methods; integrates with PLMs (BART) to leverage pretrained linguistic knowledge; clustering triples by head helps preserve coherence.",
            "disadvantages": "More complex encoder with multiple structure matrices and hyperparameters (e.g., λ); added computation and memory compared to pure linearization; optimal λ may require tuning (authors used λ=0.5 due to resource limits).",
            "failure_cases": "For larger-scale KGs (many triples), generated texts exhibit missing content, hallucinations, and redundant expressions; model performance is sensitive to dataset quality (EventNarrative examples sometimes do not match KG well), and larger/complex graphs degrade fidelity.",
            "uuid": "e8789.0",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "MGSA-Entity",
            "name_full": "MGSA Entity-level Structure Encoding Module",
            "brief_description": "Module that encodes entity- and relation-level graph structure by building a bipartite graph and injecting an adjacency matrix and a relative position matrix into attention computations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Entity-level bipartite linearization + relative-position attention",
            "representation_description": "Triples concatenated with tokens &lt;H&gt;, &lt;R&gt;, &lt;T&gt;; linear self-attention pooled (mean) to entity/relation vectors X_p; bipartite graph adjacency A used and a discrete relative position matrix R_E (values encode neighbor/entity-edge/edge-entity types) is added into scaled dot-product attention as an additive bias term mapped into vector space via γ.",
            "graph_type": "Knowledge graphs (entities and relation nodes; bipartite entity-relation graph)",
            "conversion_method": "Concatenate triples with &lt;H&gt;/&lt;R&gt;/&lt;T&gt; tokens (entity-level linearization), pool token-level encoder outputs into entity/relation node vectors, construct bipartite adjacency and relative-position matrices, compute graph-structure-aware attention, then remap node representations back to token space via gather().",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Ablation: removing entire entity-level module reduces BLEU4 from 66.45 (full) to 62.31 (w/o entity-level); models with only relative-position matrix or only adjacency matrix yield intermediate BLEU4 (relative-position-only ~64.86, adjacency-only ~65.34), indicating numeric impact.",
            "comparison_to_others": "Compared to methods that only add structure embeddings or design graph-aware modules (JointGT, GAP), entity-level encoding in MGSA combined with word-level encoding leads to better BLEU4/METEOR/ROUGEL; paper notes relative position encoding contributes more than adjacency in this setup.",
            "advantages": "Explicitly encodes inter-entity relation structure and relation-node roles; relative-position encoding (R_E) strongly improves performance (more than adjacency alone) and better preserves entity-level topology lost in flat linearization.",
            "disadvantages": "Pooling from token to node-level (mean pooling) can lose fine-grained token-level information; constructing bipartite matrices and mapping back to token space adds complexity and potential mapping errors.",
            "failure_cases": "Ablation shows removing the entity-level module causes the largest drop in performance versus removing word-level module, indicating that if entity-level structure is missing or noisy, generation quality degrades significantly.",
            "uuid": "e8789.1",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "MGSA-Word",
            "name_full": "MGSA Word-level Structure Encoding Module",
            "brief_description": "Module that encodes word-level semantic and structural relations by expanding entity/relation nodes into word-nodes and using shortest-path distances to form a relative position matrix for attention augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Word-level node expansion + shortest-path relative-position attention",
            "representation_description": "Split each entity/relation into individual word nodes (marked with [N]) and build a word-level graph; compute shortest-path distances δ(n_i, n_j) between word nodes and derive R_N with special encoding for words from the same entity (encode(p) := sgn(p)*δ_max + p), unreachable pairs set to ∞; inject γ(R_N) into attention as an additive bias term.",
            "graph_type": "Knowledge graphs at token/word granularity (words within entities and relations)",
            "conversion_method": "Tokenize each entity/relation to words, mark words with [N], concatenate words according to triple order to form X_N, compute word-level relative distance matrix from shortest-path distances in the word-graph, and add this encoded matrix into self-attention.",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Ablation: removing word-level module reduces BLEU4 modestly (word-level removed BLEU4 ~65.58 vs full MGSA 66.45); removing word-level relative position matrix lowers performance by ~1 point compared to including it.",
            "comparison_to_others": "Compared with Graformer and UniD2T which capture token/word-level structure, MGSA's combination with entity-level structure yields superior overall metrics; Graformer (word-level relative positions) performs worse than MGSA likely due to different encoder backbone (Transformer vs BART) and lack of entity-level encoding.",
            "advantages": "Captures intra-entity semantic cohesion and inter-word structural paths, helping generate more semantically consistent phrases and maintain word-level relations that entity-level representations miss.",
            "disadvantages": "Adds substantial number of nodes (word-level expansion) increasing sequence length and compute; encoding of unreachable pairs as ∞ and special SAME encoding increases representation complexity.",
            "failure_cases": "Less impactful than entity-level module per ablation; when R_N is removed performance drops but not as much as removing entity-level info. Large graphs exacerbate missing/hallucination issues despite word-level encoding.",
            "uuid": "e8789.2",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Head-clustering",
            "name_full": "Head-entity Clustering of Triples (ordering heuristic)",
            "brief_description": "A preprocessing step that clusters triples by their head entities before linearization to keep information about the same entity adjacent in the input sequence, aligning linearized order with human topical coherence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Head-entity-based triple clustering + ordered linearization",
            "representation_description": "Group triples that share the same head entity, and when linearizing the set of triples into the token sequence, place triples of the same head contiguously to preserve entity coherence and improve naturalness of generated text.",
            "graph_type": "Knowledge graphs (triples)",
            "conversion_method": "Cluster triples by head entity before concatenation; then linearize each cluster in sequence (within clusters maintain triple word order) to form the input token sequence.",
            "downstream_task": "KG-to-text generation (improve fluency/coherence)",
            "performance_metrics": "No separate scalar metrics isolated, but authors state clustering preserves completeness and coherence and contributes to better text generation; experiments and case studies attribute improved alignment with human language habits to this step.",
            "comparison_to_others": "Most prior PLM-based linearization approaches use dataset-provided triple order or random order; MGSA's head-clustering is claimed to better match human topical discourse but no direct numeric comparison provided.",
            "advantages": "Better topical coherence and local completeness in generated descriptions; reduces interleaving of topics that can confuse decoder.",
            "disadvantages": "Heuristic ordering that may not reflect optimal narrative order for all KGs; requires extra preprocessing and may hide inter-entity relations that cross clustered groups.",
            "failure_cases": "No explicit numerical failure cases reported, but authors note that for large/misaligned KGs dataset quality still dominates performance and clustering cannot fully prevent hallucinations when reference text is not well aligned.",
            "uuid": "e8789.3",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Linearization (baseline)",
            "name_full": "Token-sequence Linearization of Knowledge Graphs",
            "brief_description": "A common approach that transforms a KG (set of triples) into a flat token sequence used as input to pre-trained seq2seq models (e.g., BART, T5), often with special tokens indicating head, relation, tail.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Triple-to-token linearization",
            "representation_description": "Concatenate triples into a single token sequence, optionally with special markers (e.g., &lt;H&gt;, &lt;R&gt;, &lt;T&gt;) or entity/relation delimiters; feed directly into PLM encoder to perform seq2seq generation.",
            "graph_type": "Knowledge graphs (triples)",
            "conversion_method": "Concatenate triples (head, relation, tail) into tokens in arbitrary or dataset-provided order; may include separators/special tokens.",
            "downstream_task": "KG-to-text generation (used as baseline for PLM fine-tuning)",
            "performance_metrics": "BART-base baseline reported on WebNLG: BLEU4=64.55, METEOR=46.51, ROUGEL=75.13; T5-base: BLEU4=64.42, METEOR=46.58, ROUGEL=74.77 (from paper Table II).",
            "comparison_to_others": "Linearization loses explicit graph structure; many works attempt to augment it with structure embeddings or graph-aware modules, and MGSA shows improvements over plain linearization.",
            "advantages": "Simple to implement and directly leverages PLM strengths; no graph encoder needed.",
            "disadvantages": "Loses structural/topological information in the KG; ordering ambiguity can hurt faithfulness and coherence; may produce lower-quality text than structure-aware methods.",
            "failure_cases": "Authors note that plain linearization causes loss of graph structure and alignment issues between KG and text; worse performance versus structure-aware approaches.",
            "uuid": "e8789.4",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "KGPT",
            "name_full": "KGPT: Knowledge-grounded pre-training for data-to-text generation",
            "brief_description": "A baseline method that injects multi-level structure embeddings into the linearized token sequence for data-to-text generation.",
            "citation_title": "KGPT: Knowledge-grounded pre-training for datato-text generation",
            "mention_or_use": "mention",
            "representation_name": "Multi-level structure embeddings in linearized sequence",
            "representation_description": "Adds additional learned embeddings to tokens in the linearized KG sequence to encode structural cues at multiple levels (token/entity/other) prior to PLM fine-tuning.",
            "graph_type": "Knowledge graphs / structured data",
            "conversion_method": "Linearize triples into token sequence and augment token representations with structural embeddings representing multi-level information.",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Reported on WebNLG: BLEU4=64.11, METEOR=46.30, ROUGEL=74.57 (Table II in paper).",
            "comparison_to_others": "MGSA outperforms KGPT by +2.34 BLEU4 and +1.90 ROUGEL on WebNLG, suggesting MGSA's attention-based integration of structure is more effective than added embeddings alone.",
            "advantages": "Preserves use of PLM while injecting structural signals without changing architecture dramatically.",
            "disadvantages": "Embedding-based injection may not fully capture topology or multi-hop relations; less effective than explicit graph-aware attention in MGSA (per paper).",
            "failure_cases": "Not detailed in this paper; MGSA results indicate KGPT underperforms when compared on same benchmarks.",
            "uuid": "e8789.5",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Graformer",
            "name_full": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "brief_description": "A method that designs a relative graph position matrix to capture word-level structure information and integrates it into Transformer attention.",
            "citation_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "mention_or_use": "mention",
            "representation_name": "Word-level relative position matrix for Transformer",
            "representation_description": "Constructs a relative-position matrix over tokens/words derived from graph distances and injects it into Transformer attention to encode graph structure at word/token granularity.",
            "graph_type": "Knowledge graphs (word/token-level representation)",
            "conversion_method": "Tokenize entities/relations and compute pairwise relative positions/distances between tokens to produce a matrix added to scaled dot-product attention.",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Reported from referenced results: Graformer BLEU4 ~61.15, METEOR=43.38 on WebNLG (Table II).",
            "comparison_to_others": "MGSA significantly outperforms Graformer (MGSA BLEU4 66.45 vs Graformer ~61.15 on WebNLG); paper suggests MGSA benefits also from using BART as backbone vs Graformer's Transformer backbone.",
            "advantages": "Captures token-level distance structure and can be integrated into Transformer attention.",
            "disadvantages": "Single-granularity (word-level) misses entity-level topology; performance may depend on backbone model and integration choice.",
            "failure_cases": "Paper reports Graformer underperforms MGSA; no additional failure cases provided within this paper.",
            "uuid": "e8789.6",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GAP",
            "name_full": "GAP: A graph-aware language model framework for knowledge graph-to-text generation",
            "brief_description": "A graph-aware attention/aggregation module that injects entity-level structure information into PLM-based encoders to improve KG-to-text generation.",
            "citation_title": "GAP: A graph-aware language model framework for knowledge graph-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "Graph-aware attention module (entity-level)",
            "representation_description": "Designs a module inside the encoder that uses graph connectivity to bias attention or aggregate node representations, focusing on entity-level structure.",
            "graph_type": "Knowledge graphs (entity-level graphs)",
            "conversion_method": "Linearize KG and add a graph-aware attention/aggregation module leveraging adjacency information to compute structure-aware encoder representations.",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Reported on WebNLG: BLEU4=66.20, METEOR=46.77, ROUGEL=76.36 (Table II); on EventNarrative GAP reported second-best BLEU4=35.08, METEOR=27.50 (Table III).",
            "comparison_to_others": "MGSA slightly outperforms GAP on WebNLG and EventNarrative in aggregate metrics (MGSA BLEU4 66.45 vs GAP 66.20 on WebNLG; MGSA BLEU4 35.22 vs GAP 35.08 on EventNarrative).",
            "advantages": "Directly captures entity-level structure and has strong baseline performance, particularly effective on larger/complex KGs in some settings.",
            "disadvantages": "Focus on entity-level structure alone may miss word-level relations; MGSA's multi-granularity approach claims to improve further.",
            "failure_cases": "Not detailed beyond comparative metrics; MGSA authors note GAP is competitive but slightly weaker than MGSA in reported experiments.",
            "uuid": "e8789.7",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "JointGT",
            "name_full": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "brief_description": "A model that designs a structure-aware semantic aggregation module to capture entity-level structure information when converting KGs to text.",
            "citation_title": "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs",
            "mention_or_use": "mention",
            "representation_name": "Structure-aware semantic aggregation (entity-level)",
            "representation_description": "Integrates entity-level graph structure into encoder representations via a semantic aggregation module that combines graph and token sequence features.",
            "graph_type": "Knowledge graphs (entity-level)",
            "conversion_method": "Linearize KG and use a joint representation module aligning graph nodes with token representations to aggregate structure-aware information.",
            "downstream_task": "KG-to-text generation",
            "performance_metrics": "Reported on WebNLG: BLEU4=65.92, METEOR=47.15, ROUGEL=76.10 (Table II).",
            "comparison_to_others": "MGSA outperforms JointGT modestly on BLEU4 (MGSA 66.45 vs JointGT 65.92) and competes on other metrics; JointGT focuses on entity-level structure while MGSA adds word-level structure as well.",
            "advantages": "Improves entity-level structural encoding and yields competitive metrics versus simpler baselines.",
            "disadvantages": "Lacks simultaneous word-level structural modeling; MGSA authors argue multi-granularity yields further gains.",
            "failure_cases": "Not detailed in this paper beyond comparative performance.",
            "uuid": "e8789.8",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "UniD2T",
            "name_full": "Unifying Structured Data as Graph for Data-to-Text Pre-Training (UniD2T)",
            "brief_description": "A method for pre-training models on varied structured data by converting different structured inputs into unified graph representations and injecting structure-enhancement modules for text generation.",
            "citation_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
            "mention_or_use": "mention",
            "representation_name": "Unified structured-data-as-graph + structure-enhancement module",
            "representation_description": "Treats diverse structured inputs as graphs and uses a structure-enhancement module to capture word-level structure information across different data-to-text tasks, enabling unified pretraining.",
            "graph_type": "General structured data represented as graphs (including KGs)",
            "conversion_method": "Convert structured inputs into graph form, linearize tokens and inject structure-enhancement representations to aid PLM pretraining and downstream generation.",
            "downstream_task": "Data-to-text and KG-to-text generation",
            "performance_metrics": "UniD2T reported on WebNLG: BLEU4=60.41, METEOR=44.35, ROUGEL not reported in table for some baselines (Table II).",
            "comparison_to_others": "MGSA outperforms UniD2T on WebNLG; authors speculate UniD2T's multi-task pretraining across varied structured data may dilute performance on KG-specific tasks versus MGSA focused on KGs.",
            "advantages": "Generalizable across structured data types and benefits from pretraining across many tasks.",
            "disadvantages": "May underperform specialized KG-focused models on KG-to-text benchmarks; reported lower BLEU4 in this comparison.",
            "failure_cases": "Not detailed in this paper; comparative metrics show lower scores vs MGSA on WebNLG.",
            "uuid": "e8789.9",
            "source_info": {
                "paper_title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_via_relative_position_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "KGPT: Knowledge-grounded pre-training for datato-text generation",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "GAP: A graph-aware language model framework for knowledge graph-to-text generation",
            "rating": 2,
            "sanitized_title": "gap_a_graphaware_language_model_framework_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
            "rating": 2,
            "sanitized_title": "unifying_structured_data_as_graph_for_datatotext_pretraining"
        },
        {
            "paper_title": "Boosting KG-to-Text Generation via Multi-granularity Graph Representations",
            "rating": 1,
            "sanitized_title": "boosting_kgtotext_generation_via_multigranularity_graph_representations"
        },
        {
            "paper_title": "GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism",
            "rating": 1,
            "sanitized_title": "grasame_injecting_tokenlevel_structural_information_to_pretrained_language_models_via_graphguided_selfattention_mechanism"
        }
    ],
    "cost": 0.01551675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation</p>
<p>Shanshan Wang 
Beijing Jiaotong University
BeijingChina</p>
<p>Chun Zhang chzhang1@bjtu.edu.cn 
Beijing Jiaotong University
BeijingChina</p>
<p>Ning Zhang nzhang1@bjtu.edu.cn 
Beijing Jiaotong University
BeijingChina</p>
<p>MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation
B104ABC81CDC757B32622429DAEBC75AKG-to-textMulti-granularityStructure attentionPLMs
The Knowledge Graph-to-Text Generation task aims to convert structured knowledge graphs into coherent and human-readable natural language text, a crucial step in making complex data accessible to non-expert users.Recent efforts in this field have focused on enhancing pre-trained language models (PLMs) by incorporating graph structure information to capture the intricate structure details of knowledge graphs.However, most of these approaches tend to capture only singlegranularity structure information, concentrating either on the relationships between entities within the original graph or on the relationships between words within the same entity or across different entities.This narrow focus results in a significant limitation: models that concentrate solely on entity-level structure fail to capture the nuanced semantic relationships between words, while those that focus only on word-level structure overlook the broader relationships between original entire entities.To overcome these limitations, this paper introduces the multigranularity graph structure attention (MGSA), which is based on PLMs.The encoder of the model architecture features an entity-level structure encoding module, a word-level structure encoding module, and an aggregation module that synthesizes information from both structure.This multi-granularity structure encoding approach allows the model to simultaneously capture both entity-level and word-level structure information, providing a more comprehensive understanding of the knowledge graph's structure information, thereby significantly improving the quality of the generated text.We conducted extensive evaluations of the MGSA model using two widely recognized KG-to-Text Generation benchmark datasets, WebNLG and EventNarrative, where it consistently outperformed models that rely solely on single-granularity structure information, demonstrating the effectiveness of our approach.</p>
<p>I. INTRODUCTION</p>
<p>Knowledge Graphs (KGs) [1]- [3] are structured data storage formats used to represent knowledge and information, with strong capabilities in data integration and information retrieval.Their graph-structured representation significantly enhances the reasoning capabilities between pieces of knowledge.However, the graph-structured nature of KGs differs greatly from natural language text, making it more difficult for humans to directly comprehend the information they contain.Therefore, the task of KG-to-text generation aims to transform structured KGs into human-readable natural language text, serving as a bridge between KGs and natural language.As illustrated in Fig. 1, a given KG and its corresponding natural language description are presented.The task of KG-to-text has a wide range of applications, such as question-answering systems [4], [5], dialogue generation or dialogue agents [6], and event narration [7], among others.</p>
<p>KG-to-text generation typically requires encoding the KG so that the model can understand the information it contains, thereby generating text that accurately describes the KG.Unlike AMR-to-Text Generation [8], which involves a more restrictive space where the graph follows predefined dense connection templates, the sparsity of KGs makes it difficult for typical text generation models to align the relationships between the KG and the target generated text.Given that PLMs have already learned rich linguistic knowledge, contextual information, and semantic relationships from large-scale pretraining corpora, recent works on KG-to-text generation [9]- [11] have achieved state-of-the-art (SOTA) results by leveraging PLMs.These models linearize the KG into a token sequence as input to the model, transforming the KG-to-text task into a sequence-to-sequence (seq2seq) task by fine-tuning PLMs or adding additional pre-training tasks.</p>
<p>Although fine-tuning PLMs has yielded encouraging results, several issues remain.First, linearizing the KG as input leads to the loss of graph structure information.Some works [10], [12]- [14] have attempted to add additional embedding information to the linearized sequence or introduce graphaware modules into the model's encoder to capture the graph structure.However, these methods either focus solely on entity-level or word-level structure, without simultaneously considering both granularities of structure information.Sec-arXiv:2409.10294v2[cs.CL] 23 Sep 2024 ond, when linearizing the KG, the order of the triples is randomly arranged based on the original dataset, without considering human language conventions.Our proposed model not only integrates both granularities of structure information but also clusters and arranges the linearized KG according to human language habits.</p>
<p>Our proposed MGSA model consists of three key modules: (1) Entity-level structure encoding module: A relative position matrix and an adjacency matrix are designed to capture entitylevel structure attention information.(2) Word-level structure encoding module: A word-level relative position matrix is designed to capture word-level structure information both within and between entities.(3) Aggregation module: The aggregation module integrates the outputs of the two granularity encoding modules, and its output is used as part of the input to the decoder for generating the target text.</p>
<p>We evaluated the MGSA model on two popular KG-to-text generation datasets, WebNLG [15] and EventNarrative [7], and it demonstrated better overall performance than the baseline models.Our main contributions are as follows:</p>
<p>• We propose a novel KG-to-text generation model based on multi-granularity graph structure attention.By integrating graph structure information at multiple granularities, the model fully understands the information contained in the KG, thereby generating higher-quality text.</p>
<p>• Based on our model, we further evaluated the effectiveness of the two granularity-level modules, providing additional evidence of the effectiveness of integrating multi-granularity structure information for encoding KG structure.</p>
<p>• Through case analysis, we explored the factors influencing the model's performance.In addition to intrinsic factors within the model, we discovered that external factors, such as the quality of the dataset, also impact the model's effectiveness.• Experiments conducted on two datasets achieved more competitive results compared to the baseline models.</p>
<p>II. RELATED WORK</p>
<p>A. KG-to-Text Generation KG-to-text generation is a subset of Data-to-Text generation [16] and a branch of Natural Language Generation (NLG) [17].Unlike traditional NLG tasks (seq2seq), where the input is a linear sequence, a KG is structured as a graph rather than a linear sequence.Therefore, models designed for conventional NLG tasks cannot be directly applied to KG-to-text generation.To address this challenge, recent work has primarily focused on two directions: one direction utilizes Graph Neural Networks (GNNs) [18]- [22], which encode node information in the graph through neighborhood aggregation and then decode the encoded results to generate the corresponding textual description.Alternatively, some approaches directly modify the encoder part of Transformer models [23], combining them with Graph Attention Networks (GATs) [24], enabling the model to directly compute self-attention information for all nodes in the graph.The other direction leverages the success of PLMs in NLG tasks, such as BART [25], T5 [26], and GPT [27].These methods fine-tune PLMs by linearizing the structured KG into a token sequence, using the linearized KG nodes as input to generate sentences, thus transforming the graph-to-text task into a seq2seq task.However, the drawback of this approach is that the original graph structure information is lost during the linearization process.</p>
<p>Although explicitly encoding graph structure information with GNNs has been shown to be effective, methods based on PLMs have demonstrated superior results compared to GNNs.Therefore, we also chose to explore the PLM-based approach for the KG-to-text generation task.</p>
<p>B. Structure Information for PLMs</p>
<p>Recent work has focused on injecting the structure information of KGs into PLMs.Studies such as [28] and [29] incorporate additional embeddings into the linearized sequence, primarily to capture token-level structure information in the KG.In [12] and [11], a relative distance matrix is designed based on the relative distances between words or entities, and this matrix is integrated into the attention mechanism of the encoder to capture word-level or entity-level structure information from the KG.Meanwhile, [10], [13], [30] design a graph structure-aware module within the encoder to capture entity-level structure information from the KG.Additionally, [31] and [32] combine GNNs with PLMs to align the entitylevel graph structure of KGs with the token-level semantic information in the linear sequence.</p>
<p>Overall, the aforementioned work either incorporates wordlevel structure information, entity-level structure information, or directly integrates entity-level structure information with the semantic information of linearized sequences.However, these approaches do not simultaneously consider the structure information between entities and the structure information between words within the same entity or across different entities in the KG.Inspired by [22] and [33], which explore multi-granularity structure information in KGs using GNNs, we also consider both entity-level and word-level structure information when incorporating graph structure into PLMs.Experimental results demonstrate the effectiveness of our model.</p>
<p>III. METHODOLOGY</p>
<p>A. Problem Definition and Model Architecture KG-to-text generation aims to convert a set of triples into natural language text.For a given KG G = {(h i , r ij , t j ) | h i , t j ∈ V, r ij ∈ R}, where V is the set of all nodes in the graph and R is the set of all relation labels in the graph, the triples are linearized into a token sequence G linear = (x 1 , x 2 , . . ., x n ) where x i represents the i-th token in the linear sequence and n represents the number of tokens in the sequence.The linearized sequence is input into the model to generate the corresponding text sequence T = (t 1 , t 2 , . . ., t k ), where t i represents the i-th token generated by the model and k represents the length of the text.The overall architecture of the model follows an encoderdecoder structure based on BART.As illustrated in Fig. 2, the encoder section is designed to fully capture the information embedded in the graph by utilizing two granularity-specific encoding modules: a word-level module and an entity-level module, along with an aggregation module.The entity-level module captures entity-level structure information using a relative position matrix between nodes in the graph, while the word-level module captures semantic information between words within the same node or across different nodes using a relative position matrix between words.The aggregation module is responsible for fusing the encoding vectors from both granularities.In the decoder section, the model adopts a standard Transformer [34] decoder architecture, where the output from the encoder serves as part of the hidden input to the decoder for generating the target text.</p>
<p>B. Entity-level Structure Encoding Module</p>
<p>This module encompasses the linearization of entity-level structure in the KG, the design of a relative position matrix, and the incorporation of this relative position matrix into the self-attention weight calculations to effectively capture the structure information.</p>
<p>1) Entity-level Linearization: For a given KG in the form of a set of triples, the linearization process follows the method in existing work [13].All triples are concatenated together, with special tokens <H>, <R>, and <T> used to denote the head, relation, and tail of each triple, respectively.</p>
<p>2) Linear Attention: The linearized sequence of the KG is then fed into the module, where it first undergoes self-attention computation.This process captures the global information of the linear sequence by calculating the global attention across all tokens in the sequence.Let the linearized sequence be denoted as X E , and the computation process of the linear self-attention is as follows:
X lin = σ QK ⊤ √ d V,(1)
where 3) Entity-level Graph Structure Attention: While linear attention primarily captures the global information of linear sequences, it is insufficient to obtain the structure information of entities and relations in the KG.To further capture the structure information between entities and relations, the vectors X lin ∈ R n×d obtained from linear attention are first transformed into entity and relation vectors X p ∈ R m×d through a pooling layer.
Q = X E W Q , K = X E W K , V = X E W V , and W Q , W K , W V are learnableX p = pooling(X lin )(2)
Here, n and m represent the number of tokens in the linear sequence and the number of entities and relations in the KG, respectively.The pooling operation employs mean pooling.</p>
<p>Transforming the KG into a bipartite graph, as illustrated in Fig. 3a.The nodes on the left side represent all entities from the original KG, while the nodes on the right side represent the relations.The relative position matrix R E is generated based on the connection types between nodes in the bipartite graph:
R E ij =      1, if i and j are neighboring entities, 2, if (i, j) is an (entity, edge) pair, 3, if (i, j) is an (edge, entity) pair,(3)
The connection types are defined based on the neighboring entities or relations in the original KG.By assigning different weights to different connection types, the relative position matrix R E is derived.Using this relative position matrix R E , the graph structure attention between entities is calculated:
X g = σ Q p K p⊤ √ d + A + γ R E V p ,(4)
Here Q p , K p , V p are constructed from X p by multiplying it with their corresponding learnable parameter W Q p , W K p , Fig. 3. Entity-level linearization and word-level linearization.</p>
<p>W V p .A ∈ R m×m is the adjacency matrix corresponding to the bipartite graph, and γ(•) maps the relative distances between entities or relations to a vector space.Finally, the entitylevel structure attention representation is combined with the linear attention representation through a residual connection, producing the output of the entity-level structure encoding module.</p>
<p>X E = gather(X g ) + X lin (5) Here, the function gather(•) remaps the m-dimensional node representations back to the n-dimensional token representation space.</p>
<p>C. Word-level Structure Encoding Module 1) Word-level Linearization: An entity or relation is usually composed of several words.In word-level linearization, these entities or relations are split into individual words, with each word marked by a special token [N].The words are then concatenated in the order of the original set of triples.</p>
<p>2) Word-level Graph Structure Attention: In the entitylevel bipartite graph, each node consists of several words.Based on this, the graph is transformed into a word-level graph, as illustrated in Fig. 3b, where each node from the original bipartite graph is split into multiple nodes, with each node containing only one word.When calculating word-level structure attention, the word-level graph is used to generate the word-level relative position matrix R N based on the shortest path distances between words:
R N ij =          ∞, if δ(n i , n j ) = ∞ and δ(n j , n i ) = ∞, encode(p), if (n i , n j ) ∈ SAME p , δ(n i , n j ), if δ(n i , n j ) ≤ δ(n j , n i ), −δ(n j , n i ), if δ(n i , n j ) &gt; δ(n j , n i ),(6)
Here, δ(n i , n j ) represents the shortest path distance from word node n i to word node n j .If n i to n j are unreachable from each other, their shortest path distance is set to ∞.The term SAME p indicates words that belong to the same entity.The function encode(•) maps the distance between words from the same entity to a value outside the range of the δ(•) function.The encoding function is defined as encode(p) := sgn(p) • δ max + p, where δ max is the diameter of the graph, representing the maximum shortest path length between two words.Let the linearized sequence be denoted as X N , the word-level graph structure attention is calculated as follows:
X W = σ Q N K N ⊤ √ d + γ R N V N ,(7)
Here, Q N , K N , V N are constructed from X N by multiplying it with their weight matrices.</p>
<p>D. Aggregation Module</p>
<p>After encoding through the two granularity-specific module, we obtain the entity-level and word-level graph structure encoding vectors, which are then concatenated:
c = X E ∥ λX W (8)
Here, λ is a hyperparameter that controls the contribution of the word-level structure encoding vector.For the vector c, the overall attention is calculated to fuse the graph structure information at both granularities as follows:
X c = σ Q c K c⊤ √ d V c ,(9)
Here, Q c , K c , V c are constructed from c by multiplying it with their weight matrices.The representation is then enhanced through a residual connection, followed by a two-layer feedforward network (FF) to adjust the dimensionality of the vector representation.Finally, the output representation O, which fuses the information from both granularities, is obtained through another residual connection and layer normalization (LN) as follows:
O = LN (F F (X c + c) + X c )(10)</p>
<p>E. Loss Function</p>
<p>The model's decoder adopts the standard Transformer decoder structure.For a given KG G, the loss for text generation is calculated using the negative log-likelihood (NLL) as follows:
L = − k i=1 log p (t i | t 1 , . . . , t i−1 ; G)(11)
Here, p represents the generation probability of each token.</p>
<p>IV. EXPERIMENTS A. Experiment Setup 1) Dataset: The experiments were conducted on two datasets designed for the KG-to-text generation task: WebNLG v2.0 and EventNarrative.WebNLG consists of crowdsourced RDF data manually created by human annotators.Each example in the dataset contains up to seven triples and one or more reference texts.For comparison with previous work, version 2.0 was used in our experimental analysis.EventNarrative extracts events from EventKG [35] and enriches each event with additional data from Wikidata [3], including related attributes and objects.Detailed textual descriptions related to these events are then obtained from Wikipedia.This process ensures a close correspondence between the KG and the text, resulting in a large-scale, high-quality parallel dataset.Due to computational resource limitations, ablation studies and further analysis experiments were only conducted on the WebNLG dataset, while the optimal model obtained from WebNLG was validated and tested on the EventNarrative dataset.Table I presents the official train/validation/test splits for both datasets.2) Data Processing: Before linearizing the KG, we clustered the set of triples based on their head entities.This ensures that information related to the same entity is not only adjacent in the graph structure but also in the linear sequence, thus preserving the completeness and coherence of the generated text in describing the entity.This approach aligns with human language habits-when we speak, we tend to focus on one topic at a time and move on to the next only after finishing the current one, rather than interweaving multiple topics in a disorganized manner.</p>
<p>3) Parameter Settings: In the experiments, we used the pre-trained bart-base checkpoint from Hugging Face to initialize the parameters of the entity-level module and wordlevel module.When training on the WebNLG dataset, we set the number of epochs to 40, the batch size to 16, the learning rate to 2e-5, the number of warm-up steps to 1600, the beam width to 5, and chose Adam as the optimizer with ϵ set to 1e-8.The maximum input length for the linear sequence was set to 256, and the maximum generation length for the text sequence was set to 128.The hyperparameter λ, which controls the word-level graph structure vectors, was set to 0.5.During evaluation, we followed existing KG-to-text work and used BLEU4 [36], METEOR [37], and ROUGEL [38] scores as evaluation metrics to analyze the model's performance.</p>
<p>4) Baseline Selection: When analyzing the experimental results, we selected KG-to-text generation models that linearize KGs on the same dataset for a fair comparison, including KGPT [28], JointGT [13], Graformer [12], GAP [10], and UniD2T [14].The KGPT model injects multi-level structure information into the linearized sequence using three special embeddings.The JointGT and GAP models design a structure-aware semantic aggregation module and a graphaware attention module, respectively, to capture entity-level structure information.For the JointGT model, we chose the BART-based pre-trained model consistent with this experiment.The Graformer model designs a relative graph position matrix based on the Transformer model to capture word-level structure information in the KG.The UniD2T model, targeting various structured data-to-text generation tasks, captures word-level structure information in the input graph through a designed structure-enhancement module.</p>
<p>B. Main Results</p>
<p>Table II and III present the experimental results of the proposed model on the WebNLG and EventNarrative datasets, along with the corresponding experimental data from baseline models.To ensure objectivity and fairness in comparison, all data are derived from the experimental results reported in the original papers.SOTA-NPT [39]   For the WebNLG dataset, our model outperforms the SOTA-NPT model, which has no pre-training, by 5.45%, 4.93%, and 5.47% in BLEU4, METEOR, and ROUGEL metrics, respectively.Additionally, Compared to the BART-base and T5-base models, it achieves approximately 2% and 1.5% improvements in BLEU4 and ROUGEL metrics, respectively, demonstrating that the incorporation of graph structure attention matrices enhances the performance of the KG-totext generation task.Compared to the KGPT model, which adds multi-level structure embeddings to the linearized sequence, our model achieves 2.34% and 1.90% improvements in BLEU4 and ROUGEL metrics, respectively.This indicates that our approach, which integrates multi-granularity graph structure attention, is more effective than adding multi-level embedding information.In the BLEU4 metric, our model surpasses models with single-granularity structure-aware modules such as JointGT, Graformer, GAP, and UniD2T by 0.53%, 5.30%, 0.25%, and 6.04%, respectively.This suggests that our multi-granularity graph structure attention matrices can more comprehensively capture both entity-level structure information and word-level semantic information, leading to a deeper understanding of the KG and the generation of higher-quality text.The significant improvement over Graformer may be attributed to our choice of the BART model, which enhances the KG-to-text generation task more effectively than the Transformer model.As for UniD2T, we speculate that our model's focus solely on graph-structured data-to-text generation tasks, without involving other structured data, contributes to its superior performance.</p>
<p>For the EventNarrative dataset, our model achieves the highest scores in BLEU4 and METEOR metrics, outperforming the second-best GAP model by 0.14% and 0.11%, respectively, and the second-highest score in the ROUGEL metric.Compared to the BART-base model, our model shows improvements of 3.84%, 0.93%, and 1.81% across the three evaluation metrics, further confirming that the integration of multi-granularity graph structure attention can achieve excellent performance even with more complex KGs.</p>
<p>Overall, built on top of BART, our model integrates multigranularity graph structure attention, effectively captures the structure information of the graph.This enables the model to extract information from the given KG at multiple granularities, resulting in the generation of higher-quality text.</p>
<p>C. Ablation Study</p>
<p>Due to the limitations of experimental resources, we conducted ablation studies only on the WebNLG dataset.Through experiments with single-granularity structure attention, we analyzed the impact of the entity-level structure encoding module and the word-level structure encoding module on the model's performance.1) Entity-level: In analyzing the impact of entity-level structure encoding on model performance, we conducted experiments on models where the entity-level structure encoding module was removed, the relative position encoding matrix was removed, the adjacency matrix was removed, and both the relative position encoding and adjacency matrices were removed.The experimental results are shown in Table IV.</p>
<p>From the results, it can be observed that both types of encoding contribute to improving the model's performance.Furthermore, the relative position matrix has a greater impact on the model's effectiveness than the adjacency matrix, with the model using only the relative position matrix outperforming the one using only the adjacency matrix by 0.52%.</p>
<p>2) Word-level: As shown in Table V, the difference between removing the word-level structure encoding module and removing only the word-level relative position matrix is not significant.However, when the relative position matrix</p>
<p>D. Impact of λ values</p>
<p>Additionally, we explored the impact of the λ value in the aggregation module on the model's performance.The Fig. 4 shows the trend of experimental results with λ values set to 0, 0.25, 0.50, 0.75, and 1.When λ = 0, it is equivalent to removing the word-level module.From λ = 0 to λ = 0.5, the experimental results show an upward trend, reaching the maximum at λ = 0.5.From λ = 0.5 to λ = 1, the experimental results gradually decline.Although the optimal model performance may not be achieved at λ = 0.5, due to resource limitations, we selected λ = 0.5 as the optimal value in our experiments.Overall, the experimental results indicate that the value of λ also affects the model's performance.</p>
<p>E. Case Study</p>
<p>To demonstrate the effectiveness of the proposed method, we analyzed the generated texts on the WebNLG and Event-Narrative datasets in Table VI.Two examples were provided for each dataset: one with a small-scale KGs (containing 1-4 triples) and one with a larger-scale KGs (containing more than 4 triples).As shown in the examples below, for smallscale KGs, the generated texts are almost identical to the reference texts in content and successfully convey the semantic knowledge contained in the set of triples.This explains why The 9th 2019 World Para Athletics Championships, the ninth edition of the para athletics World Championships, was held in Dubai, United Arab Emirates from 9 to 15 November 2019.Dubai was selected as the host city by the International Paralympic Committee (icc) on 9 August 2016.The 2019 championships was the first time that Dubai hosted the World Championships.The athletics federation of africa announced dubai as the venue for the event, and Dubai became the first city to bid for the championships in the same year.The event was scheduled to be held at the olympic the earlier experimental results yielded higher scores on the WebNLG dataset, which contains more small-scale KGs.However, for larger-scale KGs, the generated texts start to exhibit issues such as missing content, hallucinations, and redundant expressions.These issues are less apparent on the WebNLG dataset.For instance, when the number of triples reaches 7, there is only one case of missing content and hallucination, with the rest of the generated content almost semantically aligned with the reference text.However, the performance on the EventNarrative dataset is significantly worse; although only one triple's content is missing, the generated text almost fails to faithfully represent the given KG, and the reference text itself also fails to fully align with the given set of triples.From this analysis, we can conclude that the model's performance is influenced not only by the scale of the KG but also by the quality of the data.</p>
<p>V. CONCLUSION</p>
<p>This paper proposes a KG-to-text generation model based on multi-granularity graph structure attention, which simultaneously considers both entity-level and word-level structure information in the knowledge graph.The model primarily consists of three modules: an entity-level structure encoding module, a word-level structure encoding module, and an aggregation module that integrates structure encoding information from both granularities.Experimental evaluations conducted on two KG-to-text benchmark datasets demonstrate that the proposed model consistently outperforms baseline models (which utilize single-granularity structure).An analysis of the factors influencing the model further explains its effectiveness in generating text from knowledge graph.</p>
<p>Fig. 1 .
1
Fig. 1.The knowledge graph and its corresponding description text, with italic bold indicating relationship labels and colored highlights marking entities.</p>
<p>Fig. 2 .
2
Fig. 2. Overall model architecture.The input consists of two levels of granularity in the linearization process.The encoder module incorporates structure attention at both granularities, while the decoder module follows the standard Transformer decoder structure.</p>
<p>weight parameters.d is the dimensionality of the vectors.σ(•) denotes the softmax function.</p>
<p>Fig. 4 .
4
Fig. 4. Impact of λ values.</p>
<p>TABLE I DATASET
I
SPLITS FOR WEBNLG AND EVENTNARRATIVE.
DatasetTrainValidTestWebNLG34,3524,3164,224EventNarrative179,5431,00022,441</p>
<p>TABLE II PERFORMANCE
II
COMPARISON ON WEBNLG.
Model#PPre+BLEU4METEORROUGELSOTA-NPT-No61.0042.0071.00BART-base140MYes64.5546.5175.13T5-base220MYes64.4246.5874.77KGPT177MYes64.1146.3074.57JointGT160MYes65.9247.1576.10Graformer--61.1543.38-GAP153MNo66.2046.7776.36UniD2T-Yes60.4144.35-MGSA (ours)167MNo66.4546.9376.47</p>
<p>denotes the state-of-the-art (SOTA) model without any pre-training.BART-base and T5base are the unmodified base models.#P indicates the number of model parameters, and Pre+ denotes whether the model underwent additional pre-training.Bolded values represent the highest scores under the current evaluation metrics, while underlined values indicate the second-highest scores.</p>
<p>TABLE III PERFORMANCE
III
COMPARISON ON EVENTNARRATIVE.
ModelBLEU4METEORROUGELBART-base31.3826.6862.65T5-base12.8022.7752.06JointGT31.1926.5864.91GAP35.0827.5064.28MGSA (ours)35.2227.6164.46</p>
<p>TABLE IV EXPERIMENTAL
IVE/MAR EBLEU4METEORROUGELw/o--62.3144.5373.11w/✓×64.8645.3775.10w/×✓65.3446.4175.85w/××64.5345.0874.77w/ (MGSA)✓✓66.4546.9376.47
RESULTS WITH(W/) OR WITHOUT(W/O) ENTITY-LEVEL ENCODING MODULE(E/M).</p>
<p>is added, the model's performance improves by nearly 1% compared to the model without the relative position matrix.Additionally, by comparing the data in Table V with that in TableIV, we find that removing the entity-level module has a greater impact on the model's performance than removing the word-level module, indicating that the entity-level module contributes more to the model's overall improvement.
TABLE VEXPERIMENTAL RESULTS WITH OR WITHOUT WORD-LEVEL ENCODINGMODULE(W/M).W/MR WBLEU4METEORROUGELw/o-65.5846.3875.27w/×65.7746.5975.63w/ (MGSA)✓66.4546.9376.47</p>
<p>TABLE VI ANALYSIS
VI
OF GENERATION PERFORMANCE ON WEBNLG AND EVENTNARRATIVE DATASETS WITH DIFFERENT KNOWLEDGE GRAPH SCALES.The 2019 World Para Athletics Championships was a para athletics track and field event organised by the World Para Athletics subcommittee of the International Paralympic Committee.It was held in Dubai, United Arab Emirates from 7 to 15 November 2019.In July 2017 during 2017 World Para Athletics Championships there were reports and speculation that London could once again hold the games in 2019 due to the success of the 2017 event and the 2012 Summer Paralympics.As of day 9, Friday 15 November 2019.
DatasetKnowledge Graph TriplesReference TextGenerated TextWebNLG(Acharya Institute of Technology, affiliation, Visves-The Acharya Institute of Tech-The Acharya Institute of Technology(4 and 7 triples)varaya Technological University)nology 's campus is located inis located in Soldevanahalli, Acharya(Acharya Institute of Technology, country, India)Soldevanahalli, Acharya Dr. Sar-Dr. Sarvapalli Radhakrishnan Road,(Acharya Institute of Technology, campus, In Sol-vapalli Radhakrishnan Road, Hes-Hessarghatta Main Road, Bangalore -devanahalli, Acharya Dr. Sarvapalli Radhakrish-sarghatta Main Road, Bangalore560090, India. It is affiliated with thenan Road, Hessarghatta Main Road, Bangalore --560090, India. It is affiliatedVisvesvaraya Technological University560090.)with the Visvesvaraya Technolog-in Belgaum.(Visvesvaraya Technological University, city, Bel-ical University in Belgaum.gaum)(School of Business and Social Sciences at theThe School of Business and So-The School of Business and Social Sci-Aarhus University, city, Aarhus)cial Sciences at the Aarhus Univer-ences at the Aarhus University in Aarhus,(School of Business and Social Sciences at thesity is located in Aarhus, DenmarkDenmark was established in 1928. It is af-Aarhus University, affiliation, European Universityand it was established in 1928. Itsfiliated with the European University As-Association)dean is Thomas Pallesen and it hassociation and its dean is Thomas Palle-(School of Business and Social Sciences at the16,000 students. Its Latin name issen. There are 16000 students and 16000Aarhus University, dean, Thomas Pallesen)"Universitas Aarhusiensis". It is af-Latin speaking people.(School of Business and Social Sciences at thefiliated to the European UniversityAarhus University, number of students, 16000)Association.(School of Business and Social Sciences at theAarhus University, country, Denmark)(School of Business and Social Sciences atthe Aarhus University, latin name, UniversitasAarhusiensis)(School of Business and Social Sciences at theAarhus University, established, 1928)EventNarrative(2016 Tianjin Health Industry Park, sport, tennis)The 2016 Tianjin Health Indus-The 2016 Tianjin Health Industry Park(3 and 8 triples)(2016 Tianjin Health Industry Park, location, Tianjin)try Park was a professional tenniswas a professional tennis tournament(2016 Tianjin Health Industry Park, point in time,tournament played on outdoor hardplayed on outdoor hard courts. It took2016)courts. It took place in Tianjin,place in Tianjin, China, on 2016 -2016.China, on 2016-2016.(2019 World Para Athletics Championships, location,Dubai)(2019 World Para Athletics Championships, orga-nizer, International Paralympic Committee)(2019 World Para Athletics Championships, sport,para athletics)(2019 World Para Athletics Championships, editionnumber, 9)(2019 World Para Athletics Championships, country,United Arab Emirates)(2019 World Para Athletics Championships, point intime, 2019)(2019 World Para Athletics Championships, endtime, 15 November 2019)(2019 World Para Athletics Championships, follows,2017 World Para Athletics Championships)</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, 2007SpringerBerlin, Heidelberg; Berlin Heidelberginternational semantic web conference</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of data2008</p>
<p>Wikidata: a free collaborative knowledgebase. Denny Vrandečić, Markus Krötzsch, Communications of the ACM. 572014</p>
<p>Retrieve-rewrite-answer: A kg-to-text enhanced llms framework for knowledge graph question answering. Yike Wu, arXiv:2309.112062023arXiv preprint</p>
<p>Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models. Haoran Luo, arXiv:2310.089752023arXiv preprint</p>
<p>Knowledge Graph for NLG in the context of conversational agents. Hussam Ghanem, Massinissa Atmani, Christophe Cruz, arXiv:2307.015482023arXiv preprint</p>
<p>EventNarrative: A large-scale event-centric dataset for knowledge graph-to-text generation. Anthony Colas, arXiv:2111.002762021arXiv preprint</p>
<p>Amr-to-text generation with cache transition systems. Lisa Jin, Daniel Gildea, arXiv:1912.016822019arXiv preprint</p>
<p>Investigating pretrained language models for graph-to-text generation. Leonardo Fr Ribeiro, arXiv:2007.084262020arXiv preprint</p>
<p>GAP: A graph-aware language model framework for knowledge graph-to-text generation. Anthony Colas, Mehrdad Alvandipour, Daisy Zhe Wang, arXiv:2204.066742022arXiv preprint</p>
<p>Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction. Feng Zhao, Hongzhi Zou, Cheng Yan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, arXiv:2006.092422020arXiv preprint</p>
<p>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, arXiv:2106.105022021arXiv preprint</p>
<p>Unifying Structured Data as Graph for Data-to-Text Pre-Training. Shujie Li, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Creating training corpora for nlg microplanning. Claire Gardent, 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics2017. 2017</p>
<p>A Systematic Review of Data-to-Text NLG. Chinonso Osuji, Thiago Cynthia, Brian Castro Ferreira, Davis, arXiv:2402.084962024arXiv preprint</p>
<p>Pre-trained language models for text generation: A survey. Junyi Li, ACM Computing Surveys. 562024</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. Leonardo Fr Ribeiro, Transactions of the Association for Computational Linguistics. 82020</p>
<p>A comprehensive survey of graph neural networks for knowledge graphs. Zi Ye, IEEE Access. 102022</p>
<p>Boosting KG-to-Text Generation via Multi-granularity Graph Representations. Tianyu Yang, Yuxiang Zhang, Tao Jiang, 2022 International Joint Conference on Neural Networks (IJCNN). IEEE2022</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, arXiv:1904.023422019arXiv preprint</p>
<p>Graph attention networks. Petar Veličković, arXiv:1710.109032017arXiv preprint</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, arXiv:1910.134612019arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Journal of machine learning research. 211402020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, OpenAI blog. 192019</p>
<p>KGPT: Knowledge-grounded pre-training for datato-text generation. Wenhu Chen, arXiv:2010.023072020arXiv preprint</p>
<p>Graph-to-Text Generation Combining Directed and Undirected Structural Information in Knowledge Graphs. Hongda Gong, Shimin Shan, Hongkui Wei, 2023 5th International Conference on Natural Language Processing (ICNLP). IEEE2023</p>
<p>Improving plms for graph-to-text generation by relational orientation attention. Tao Wang, Neural Processing Letters. 552023</p>
<p>Exploring the Synergy of Dual-path Encoder and Alignment Module for Better Graph-to-Text Generation. Tianxin Zhao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)2024</p>
<p>GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism. Shuzhou Yuan, Michael Färber, arXiv:2404.069112024arXiv preprint</p>
<p>Enriched entity representation of knowledge graph for text generation. Kaile Shi, Complex &amp; Intelligent Systems. 92023</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Eventkg: A multilingual event-centric temporal knowledge graph. Simon Gottschalk, Elena Demidova, The Semantic Web: 15th International Conference, ESWC 2018. Heraklion, Crete, GreeceSpringer International PublishingJune 3-7, 2018. 201815</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, Proceedings of the ninth workshop on statistical machine translation. the ninth workshop on statistical machine translation2014</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, 2004Text summarization branches out</p>
<p>Handling rare items in datato-text generation. Anastasia Shimorina, Claire Gardent, Proceedings of the 11th international conference on natural language generation. the 11th international conference on natural language generation2018</p>            </div>
        </div>

    </div>
</body>
</html>
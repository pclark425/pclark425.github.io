<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8309 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8309</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8309</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276259361</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.07555v2.pdf" target="_blank">O1 Embedder: Let Retrievers Think Before Action</a></p>
                <p><strong>Paper Abstract:</strong> The growing power of large language models (LLMs) has revolutionized how people access and utilize information. Notably, the LLMs excel at performing fine-grained data representation, which facilitates precise retrieval of information. They also generate high-quality answers based on external references, enabling the production of useful knowledge. The recent introduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another leap forward, highlighting LLMs' ability to think progressively before delivering final answers. This breakthrough significantly improves the ability to address complex tasks, e.g., coding and math proofs. Inspired by this progress, we aim to develop similar capabilities for retrieval models, which hold great promise for tackling critical challenges in the field, including multi-task retrieval, zero-shot retrieval, and tasks requiring intensive reasoning of complex relationships. With this motivation, we propose a novel approach called O1 Embedder, which generates useful thoughts for the input query before making retrieval for the target documents. To realize this objective, we conquer two technical difficulties. First, we design a data synthesis workflow, creating training signals for O1 Embedder by generating initial thoughts from an LLM-expert and subsequently refining them using a retrieval committee. Second, we optimize the training process, enabling a pre-trained model to be jointly fine-tuned to generate retrieval thoughts via behavior cloning and perform dense retrieval through contrastive learning. Our approach is evaluated by comprehensive experiments, where substantial improvements are achieved across 12 popular datasets, spanning both in-domain and out-of-domain scenarios. These results highlight O1 Embedder's remarkable accuracy and generalizability, paving the way for the development of next-generation IR foundation models.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8309.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8309.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>O1 Embedder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>O1 Embedder (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense retrieval model that first generates multiple long-form 'thoughts' for a query (slow-thinking) and then produces thought-augmented embeddings via joint multi-task training (behavior cloning for generation + contrastive learning for embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>O1 Embedder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLM-based embedder (default backbone Llama-2-7B fine-tuned with LoRA) that implements two capabilities: M.think(q) to generate k independent long-form thoughts and M.embed(x) to produce embeddings (special <emb> token used). Aggregation uses mean-pooling over thought-augmented encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['generation of multiple independent long-form thoughts (multi-path thinking)', 'behavior cloning from LLM-expert thoughts (supervised generation)', 'committee-based refinement (majority voting among retrievers during data synthesis)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>O1 Embedder elicits multiple (k) independently generated long-form thoughts per query (analogous to sampling multiple reasoning paths). Training uses behavior cloning to teach the model to produce those thoughts and contrastive learning to learn embeddings for the thought-augmented query. The data pipeline refines candidate thoughts via a retrieval committee that scores candidate thoughts against the ground-truth document and selects the majority-voted thought for supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablations compare: (1) O1 Embedder with thinking (multiple thoughts) vs O1 Embedder w/o thinking (no thought generation) and (2) joint-trained O1 Embedder using generated thoughts vs stand-alone setup where an external generator (GPT-4o-mini) produces thoughts for an untrained retriever (RepLLaMA). The paper evaluates whether generated thoughts are usable without joint training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MS MARCO (in-domain passage retrieval); BEIR suite / nine out-of-domain QA datasets including NQ, HotPotQA, TREC-COVID, SciFact, FiQA, Touche, DBpedia, FEVER, CosQA (code search). Metrics: MRR@10, Recall@1k (MS MARCO); nDCG@10 (BEIR and others).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In-domain (MS MARCO dev): MRR@10 = 43.1 (reported +1.9% improvement over RepLLaMA reference). Recall@1k = 99.5. DL'19 nDCG@10 = 75.3 (+1.6 vs RepLLaMA in some comparisons); DL'20 nDCG@10 = 74.4 (+2.1). Out-of-domain (average nDCG@10 across nine BEIR QA datasets): O1 Embedder = 61.4 (average). O1 Embedder w/o thinking (w/o T) average = 59.1 (difference +2.3 nDCG points). Joint-training ablation: applying external thought generator (GPT-4o-mini) to RepLLaMA produced a small average change of -0.1 points; adding thinking to jointly trained O1 Embedder produced +2.2 average nDCG points (per Table 3). Per-dataset deltas for O1 Embedder (with T) vs O1 w/o T include NQ +3.9, HotPotQA +3.0, FiQA +1.6, TREC-COVID +1.1, SciFact +1.6, CosQA +1.2 (as reported in tables/ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Generated thoughts often surface patterns and facts absent from the raw query (case examples: Martin Luther King admission age; CosQA code snippet); attention analysis shows the <emb> token attends strongly to tokens in thoughts that match the positive document (e.g., 'Japan' in a Buck-Tick example), indicating thoughts supply missing matching signals. However, LLM-generated thoughts can be noisy/hallucinated, and domain-specialized datasets (medical/financial/scientific) benefit less. Joint training is important: naive use of externally generated thoughts can degrade performance on some specialized datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Diverse multi-thought generation (slow-thinking) combined with joint training significantly improves dense retrieval performance and generalization, especially for reasoning-intensive retrieval tasks; by contrast, applying externally generated thoughts to a retriever without joint training yields inconsistent or negative effects, demonstrating the need to train the embedder to exploit diverse reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8309.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RepLLaMA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RepLLaMA (LLM-based dense retriever, prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered dense retriever fine-tuned on MS MARCO (uses a Llama backbone) and used here as a competitive baseline and in ablation experiments where external thoughts were added at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RepLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based retriever fine-tuned on MS MARCO (in this paper RepLLaMA uses Llama-2-7B backbone consistent with O1 experiments); used as a baseline retrieval model and as the target retriever in a stand-alone-generator ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['standard dense retrieval (semantic matching); in ablation, fed externally generated long-form thoughts (from GPT-4o-mini) as additional context']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>RepLLaMA itself does not implement slow-thinking in this paper; it was tested in a setup where GPT-4o-mini generated thoughts for queries which were then used at retrieval time without joint training of RepLLaMA to exploit those thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (no internal multi-path reasoning applied in the experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Stand-alone generator experiment: GPT-4o-mini generated candidate thoughts which were appended to queries for RepLLaMA at inference; performance compared to RepLLaMA base and to O1 Embedder with joint training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same retrieval benchmarks: MS MARCO variants, BEIR datasets (NQ, HotPotQA, TREC-COVID, FiQA, SciFact, CosQA, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Applying external GPT-4o-mini thoughts to RepLLaMA produced mixed results: small improvements on some datasets (e.g., NQ +2.7, HotPotQA +3.2) but substantial declines on others (e.g., TREC-COVID -5.0, FiQA -5.6, SciFact -1.4). Average change reported: -0.1 points. This contrasts with O1 Embedder where integrated/joint training yields consistent improvements (+2.2 average).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>RepLLaMA cannot reliably exploit externally generated thoughts without joint training; noisy or hallucinated thought content can hurt specialized-domain retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Using externally generated reasoning outputs without adapting the retriever is insufficient and can be detrimental; retrievers need to be trained to ingest and exploit generated thoughts to gain consistent benefits from diverse reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8309.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini (external generator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (used as a stand-alone thought generator in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity external LLM used to generate candidate long-form thoughts that were provided to a separately trained retriever (RepLLaMA) in an ablation to test whether thoughts help without joint training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI model used as an external teacher/generator to produce long-form candidate thoughts for queries (same prompting template as used for O1 Embedder's data generation).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['long-form chain-like thought generation (prompted with instruction and examples)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-4o-mini was prompted to generate candidate thoughts for each query using instruction + in-context examples (Task: think about a plausible response). Several independently generated thoughts were produced and then used in a stand-alone retrieval setup.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple candidate thoughts produced)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Standalone generator experiment: GPT-4o-mini produced thoughts that were appended to queries for RepLLaMA inference; performance compared to baseline RepLLaMA and to joint-trained O1 Embedder.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same retrieval benchmarks (BEIR datasets and MS MARCO related evaluations used for ablation comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>As reported in Table 3, adding GPT-4o-mini generated thoughts to RepLLaMA led to dataset-dependent effects: gains on some (NQ +2.7, HotPotQA +3.2) but losses on others (TREC-COVID -5.0, FiQA -5.6); overall average change -0.1 nDCG points.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>External high-quality generators can produce useful reasoning paths, but without retriever adaptation the extra content can be noisy and reduce performance on domain-specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>High-quality external reasoning outputs are not sufficient by themselves; the retriever must be trained (jointly) to exploit these diverse reasoning outputs to obtain robust gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8309.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate reasoning from LLMs to improve performance on complex reasoning tasks by decomposing problems into chains of reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineering method (introduced in prior literature) which asks LLMs to produce intermediate reasoning steps before giving a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (decompositional step-by-step reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts the model to output an explicit chain of reasoning steps that lead from the query to an answer; often improves performance on multi-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single-style stepwise decomposition), but can be sampled multiple times to create diverse chains</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned as background; paper does not run experiments comparing CoT vs other methods directly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned in context of coding and mathematical proofs and general complex reasoning; no direct experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No new performance numbers in this paper; cited as foundational technique that inspired multi-thought generation.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper cites CoT as enabling progressive approximation to solutions and as conceptually related to O1 Embedder's thought generation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT is a key antecedent motivating generation of long-form thoughts; O1 Embedder adapts the idea to retrieval by producing multiple thoughts that reveal hidden matching signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8309.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling multiple reasoning paths + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that improves robustness by sampling multiple reasoning paths (chains) from an LLM and selecting final answers by majority voting across sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Inference technique that samples multiple diverse solution paths from an LLM and aggregates via majority voting to improve answer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sampling multiple reasoning chains and ensemble voting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The LLM is prompted multiple times to produce different chains; final answer selected by majority vote across sampled outputs, increasing robustness to sampling variance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Cited as related work and conceptual precursor to multi-thought generation; no direct experimental comparison inside this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Referenced as successful on complex problems (coding, math proofs); not directly benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not evaluated in this paper; referenced conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper draws analogy between self-consistency's multi-path sampling and O1 Embedder generating multiple thoughts for coverage of useful patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Sampling multiple reasoning paths and aggregating (self-consistency) is an established technique that motivates O1 Embedder's multiple-thought generation and majority-vote selection in data synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8309.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM reasoning framework that organizes candidate intermediate steps in tree structures, allowing evaluation, backtracking, and search across branches to explore solution space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that extends CoT by structuring intermediate reasoning steps as nodes in a tree, enabling exploration and pruning/backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['structured search over branching candidate intermediate steps with backtracking (tree search)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each node the model generates multiple candidate next thoughts and uses feasibility/evaluation heuristics to expand promising branches and prune dead ends, effectively performing guided search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (explicit multi-branch exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned as related work and conceptual inspiration; no experiments comparing ToT to O1 Embedder within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Cited as improving exploratory reasoning tasks like puzzles, code, math proofs; not benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No new performance numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Used to motivate richer thinking paradigms; O1 Embedder uses multiple independent thoughts but does not implement tree search/backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>ToT and similar structured multi-path reasoning frameworks motivate the paper's design to generate multiple long-form thoughts; O1 Embedder adopts multiple independent thoughts rather than full tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8309.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph of Thoughts (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning paradigm that represents intermediate reasoning steps as nodes in a directed acyclic graph, allowing merging and refinement of reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extends tree-based reasoning to DAGs so that different reasoning branches can merge and reuse intermediate steps, enabling more flexible refinement strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['DAG-structured multi-step reasoning with merging/refinement']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Allows combining and refining previously generated reasoning steps across branches; cited as an advancement over tree-based methods for complex problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multi-path with merging capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned in related work context only; no experiment or ablation comparing GoT vs other reasoning styles in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Referenced as enabling more sophisticated reasoning on elaborate problems; not tested in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Serves as background conceptual precedent; O1 Embedder's multi-thought generation is related but does not implement graph-structured search or merging operations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>GoT is cited as part of the LLM reasoning literature that inspired slow-thinking approaches; the paper does not directly evaluate graph-structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8309.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8309.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70B-Instruct (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-70B-Instruct (used as teacher model for thought generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open-source instruct-tuned LLM used in the paper as the teacher to generate candidate thoughts during the data synthesis (exploration) phase.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruct-tuned model (70B) used as the teacher/generator to produce multiple candidate thoughts per query during data production; prompts included instruction and in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['long-form thought generation (instruction + examples prompting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Used to explore multiple candidate long-form thoughts per query (exploration stage of data synthesis); candidates are later refined via a retrieval committee.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (multiple candidates sampled per query)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used in data production (exploration-refinement) pipeline; candidates evaluated by retrieval committee and majority voting to produce training supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Data synthesis for MS MARCO training set; downstream evaluation on MS MARCO dev and BEIR datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No direct performance metric for the teacher model itself; its generations form training data that contributed to O1 Embedder's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Teacher generations produce patterns that align with positive documents; to avoid trivial solutions, training excludes using rephrased positive documents as thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Using a strong instruction-tuned LLM as a generator in an exploration-refinement pipeline produces thought supervision that, when used in joint training, improves retrieval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'O1 Embedder: Let Retrievers Think Before Action', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8309",
    "paper_id": "paper-276259361",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "O1 Embedder",
            "name_full": "O1 Embedder (this paper)",
            "brief_description": "A dense retrieval model that first generates multiple long-form 'thoughts' for a query (slow-thinking) and then produces thought-augmented embeddings via joint multi-task training (behavior cloning for generation + contrastive learning for embeddings).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "O1 Embedder",
            "model_description": "Fine-tuned LLM-based embedder (default backbone Llama-2-7B fine-tuned with LoRA) that implements two capabilities: M.think(q) to generate k independent long-form thoughts and M.embed(x) to produce embeddings (special &lt;emb&gt; token used). Aggregation uses mean-pooling over thought-augmented encodings.",
            "reasoning_methods": [
                "generation of multiple independent long-form thoughts (multi-path thinking)",
                "behavior cloning from LLM-expert thoughts (supervised generation)",
                "committee-based refinement (majority voting among retrievers during data synthesis)"
            ],
            "reasoning_methods_description": "O1 Embedder elicits multiple (k) independently generated long-form thoughts per query (analogous to sampling multiple reasoning paths). Training uses behavior cloning to teach the model to produce those thoughts and contrastive learning to learn embeddings for the thought-augmented query. The data pipeline refines candidate thoughts via a retrieval committee that scores candidate thoughts against the ground-truth document and selects the majority-voted thought for supervision.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Ablations compare: (1) O1 Embedder with thinking (multiple thoughts) vs O1 Embedder w/o thinking (no thought generation) and (2) joint-trained O1 Embedder using generated thoughts vs stand-alone setup where an external generator (GPT-4o-mini) produces thoughts for an untrained retriever (RepLLaMA). The paper evaluates whether generated thoughts are usable without joint training.",
            "task_or_benchmark": "MS MARCO (in-domain passage retrieval); BEIR suite / nine out-of-domain QA datasets including NQ, HotPotQA, TREC-COVID, SciFact, FiQA, Touche, DBpedia, FEVER, CosQA (code search). Metrics: MRR@10, Recall@1k (MS MARCO); nDCG@10 (BEIR and others).",
            "performance_results": "In-domain (MS MARCO dev): MRR@10 = 43.1 (reported +1.9% improvement over RepLLaMA reference). Recall@1k = 99.5. DL'19 nDCG@10 = 75.3 (+1.6 vs RepLLaMA in some comparisons); DL'20 nDCG@10 = 74.4 (+2.1). Out-of-domain (average nDCG@10 across nine BEIR QA datasets): O1 Embedder = 61.4 (average). O1 Embedder w/o thinking (w/o T) average = 59.1 (difference +2.3 nDCG points). Joint-training ablation: applying external thought generator (GPT-4o-mini) to RepLLaMA produced a small average change of -0.1 points; adding thinking to jointly trained O1 Embedder produced +2.2 average nDCG points (per Table 3). Per-dataset deltas for O1 Embedder (with T) vs O1 w/o T include NQ +3.9, HotPotQA +3.0, FiQA +1.6, TREC-COVID +1.1, SciFact +1.6, CosQA +1.2 (as reported in tables/ablation).",
            "qualitative_findings": "Generated thoughts often surface patterns and facts absent from the raw query (case examples: Martin Luther King admission age; CosQA code snippet); attention analysis shows the &lt;emb&gt; token attends strongly to tokens in thoughts that match the positive document (e.g., 'Japan' in a Buck-Tick example), indicating thoughts supply missing matching signals. However, LLM-generated thoughts can be noisy/hallucinated, and domain-specialized datasets (medical/financial/scientific) benefit less. Joint training is important: naive use of externally generated thoughts can degrade performance on some specialized datasets.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Diverse multi-thought generation (slow-thinking) combined with joint training significantly improves dense retrieval performance and generalization, especially for reasoning-intensive retrieval tasks; by contrast, applying externally generated thoughts to a retriever without joint training yields inconsistent or negative effects, demonstrating the need to train the embedder to exploit diverse reasoning outputs.",
            "uuid": "e8309.0",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RepLLaMA (baseline)",
            "name_full": "RepLLaMA (LLM-based dense retriever, prior work)",
            "brief_description": "An LLM-powered dense retriever fine-tuned on MS MARCO (uses a Llama backbone) and used here as a competitive baseline and in ablation experiments where external thoughts were added at inference time.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RepLLaMA",
            "model_description": "LLM-based retriever fine-tuned on MS MARCO (in this paper RepLLaMA uses Llama-2-7B backbone consistent with O1 experiments); used as a baseline retrieval model and as the target retriever in a stand-alone-generator ablation.",
            "reasoning_methods": [
                "standard dense retrieval (semantic matching); in ablation, fed externally generated long-form thoughts (from GPT-4o-mini) as additional context"
            ],
            "reasoning_methods_description": "RepLLaMA itself does not implement slow-thinking in this paper; it was tested in a setup where GPT-4o-mini generated thoughts for queries which were then used at retrieval time without joint training of RepLLaMA to exploit those thoughts.",
            "reasoning_diversity": "similar (no internal multi-path reasoning applied in the experiments)",
            "reasoning_diversity_experimental_setup": "Stand-alone generator experiment: GPT-4o-mini generated candidate thoughts which were appended to queries for RepLLaMA at inference; performance compared to RepLLaMA base and to O1 Embedder with joint training.",
            "task_or_benchmark": "Same retrieval benchmarks: MS MARCO variants, BEIR datasets (NQ, HotPotQA, TREC-COVID, FiQA, SciFact, CosQA, etc.).",
            "performance_results": "Applying external GPT-4o-mini thoughts to RepLLaMA produced mixed results: small improvements on some datasets (e.g., NQ +2.7, HotPotQA +3.2) but substantial declines on others (e.g., TREC-COVID -5.0, FiQA -5.6, SciFact -1.4). Average change reported: -0.1 points. This contrasts with O1 Embedder where integrated/joint training yields consistent improvements (+2.2 average).",
            "qualitative_findings": "RepLLaMA cannot reliably exploit externally generated thoughts without joint training; noisy or hallucinated thought content can hurt specialized-domain retrieval.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Using externally generated reasoning outputs without adapting the retriever is insufficient and can be detrimental; retrievers need to be trained to ingest and exploit generated thoughts to gain consistent benefits from diverse reasoning outputs.",
            "uuid": "e8309.1",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o-mini (external generator)",
            "name_full": "GPT-4o-mini (used as a stand-alone thought generator in ablation)",
            "brief_description": "A high-capacity external LLM used to generate candidate long-form thoughts that were provided to a separately trained retriever (RepLLaMA) in an ablation to test whether thoughts help without joint training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "OpenAI model used as an external teacher/generator to produce long-form candidate thoughts for queries (same prompting template as used for O1 Embedder's data generation).",
            "reasoning_methods": [
                "long-form chain-like thought generation (prompted with instruction and examples)"
            ],
            "reasoning_methods_description": "GPT-4o-mini was prompted to generate candidate thoughts for each query using instruction + in-context examples (Task: think about a plausible response). Several independently generated thoughts were produced and then used in a stand-alone retrieval setup.",
            "reasoning_diversity": "diverse (multiple candidate thoughts produced)",
            "reasoning_diversity_experimental_setup": "Standalone generator experiment: GPT-4o-mini produced thoughts that were appended to queries for RepLLaMA inference; performance compared to baseline RepLLaMA and to joint-trained O1 Embedder.",
            "task_or_benchmark": "Same retrieval benchmarks (BEIR datasets and MS MARCO related evaluations used for ablation comparisons).",
            "performance_results": "As reported in Table 3, adding GPT-4o-mini generated thoughts to RepLLaMA led to dataset-dependent effects: gains on some (NQ +2.7, HotPotQA +3.2) but losses on others (TREC-COVID -5.0, FiQA -5.6); overall average change -0.1 nDCG points.",
            "qualitative_findings": "External high-quality generators can produce useful reasoning paths, but without retriever adaptation the extra content can be noisy and reduce performance on domain-specific tasks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "High-quality external reasoning outputs are not sufficient by themselves; the retriever must be trained (jointly) to exploit these diverse reasoning outputs to obtain robust gains.",
            "uuid": "e8309.2",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step intermediate reasoning from LLMs to improve performance on complex reasoning tasks by decomposing problems into chains of reasoning steps.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought prompting",
            "model_description": "Prompt-engineering method (introduced in prior literature) which asks LLMs to produce intermediate reasoning steps before giving a final answer.",
            "reasoning_methods": [
                "chain-of-thought (decompositional step-by-step reasoning)"
            ],
            "reasoning_methods_description": "CoT prompts the model to output an explicit chain of reasoning steps that lead from the query to an answer; often improves performance on multi-step problems.",
            "reasoning_diversity": "similar (single-style stepwise decomposition), but can be sampled multiple times to create diverse chains",
            "reasoning_diversity_experimental_setup": "Mentioned as background; paper does not run experiments comparing CoT vs other methods directly.",
            "task_or_benchmark": "Mentioned in context of coding and mathematical proofs and general complex reasoning; no direct experiments in this paper.",
            "performance_results": "No new performance numbers in this paper; cited as foundational technique that inspired multi-thought generation.",
            "qualitative_findings": "Paper cites CoT as enabling progressive approximation to solutions and as conceptually related to O1 Embedder's thought generation.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "CoT is a key antecedent motivating generation of long-form thoughts; O1 Embedder adapts the idea to retrieval by producing multiple thoughts that reveal hidden matching signals.",
            "uuid": "e8309.3",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (sampling multiple reasoning paths + majority voting)",
            "brief_description": "A method that improves robustness by sampling multiple reasoning paths (chains) from an LLM and selecting final answers by majority voting across sampled chains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Self-Consistency",
            "model_description": "Inference technique that samples multiple diverse solution paths from an LLM and aggregates via majority voting to improve answer robustness.",
            "reasoning_methods": [
                "sampling multiple reasoning chains and ensemble voting"
            ],
            "reasoning_methods_description": "The LLM is prompted multiple times to produce different chains; final answer selected by majority vote across sampled outputs, increasing robustness to sampling variance.",
            "reasoning_diversity": "diverse (multiple sampled reasoning paths)",
            "reasoning_diversity_experimental_setup": "Cited as related work and conceptual precursor to multi-thought generation; no direct experimental comparison inside this paper.",
            "task_or_benchmark": "Referenced as successful on complex problems (coding, math proofs); not directly benchmarked here.",
            "performance_results": "Not evaluated in this paper; referenced conceptually.",
            "qualitative_findings": "Paper draws analogy between self-consistency's multi-path sampling and O1 Embedder generating multiple thoughts for coverage of useful patterns.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Sampling multiple reasoning paths and aggregating (self-consistency) is an established technique that motivates O1 Embedder's multiple-thought generation and majority-vote selection in data synthesis.",
            "uuid": "e8309.4",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Tree of Thoughts (ToT)",
            "name_full": "Tree of Thoughts",
            "brief_description": "An LLM reasoning framework that organizes candidate intermediate steps in tree structures, allowing evaluation, backtracking, and search across branches to explore solution space.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Tree of Thoughts",
            "model_description": "Method that extends CoT by structuring intermediate reasoning steps as nodes in a tree, enabling exploration and pruning/backtracking.",
            "reasoning_methods": [
                "structured search over branching candidate intermediate steps with backtracking (tree search)"
            ],
            "reasoning_methods_description": "At each node the model generates multiple candidate next thoughts and uses feasibility/evaluation heuristics to expand promising branches and prune dead ends, effectively performing guided search.",
            "reasoning_diversity": "diverse (explicit multi-branch exploration)",
            "reasoning_diversity_experimental_setup": "Mentioned as related work and conceptual inspiration; no experiments comparing ToT to O1 Embedder within this paper.",
            "task_or_benchmark": "Cited as improving exploratory reasoning tasks like puzzles, code, math proofs; not benchmarked here.",
            "performance_results": "No new performance numbers in this paper.",
            "qualitative_findings": "Used to motivate richer thinking paradigms; O1 Embedder uses multiple independent thoughts but does not implement tree search/backtracking.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "ToT and similar structured multi-path reasoning frameworks motivate the paper's design to generate multiple long-form thoughts; O1 Embedder adopts multiple independent thoughts rather than full tree search.",
            "uuid": "e8309.5",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Graph of Thoughts (GoT)",
            "name_full": "Graph of Thoughts",
            "brief_description": "A reasoning paradigm that represents intermediate reasoning steps as nodes in a directed acyclic graph, allowing merging and refinement of reasoning paths.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Graph of Thoughts",
            "model_description": "Extends tree-based reasoning to DAGs so that different reasoning branches can merge and reuse intermediate steps, enabling more flexible refinement strategies.",
            "reasoning_methods": [
                "DAG-structured multi-step reasoning with merging/refinement"
            ],
            "reasoning_methods_description": "Allows combining and refining previously generated reasoning steps across branches; cited as an advancement over tree-based methods for complex problem solving.",
            "reasoning_diversity": "diverse (multi-path with merging capabilities)",
            "reasoning_diversity_experimental_setup": "Mentioned in related work context only; no experiment or ablation comparing GoT vs other reasoning styles in this paper.",
            "task_or_benchmark": "Referenced as enabling more sophisticated reasoning on elaborate problems; not tested in this paper.",
            "performance_results": "None reported in this paper.",
            "qualitative_findings": "Serves as background conceptual precedent; O1 Embedder's multi-thought generation is related but does not implement graph-structured search or merging operations.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "GoT is cited as part of the LLM reasoning literature that inspired slow-thinking approaches; the paper does not directly evaluate graph-structured reasoning.",
            "uuid": "e8309.6",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3.1-70B-Instruct (teacher)",
            "name_full": "Llama-3.1-70B-Instruct (used as teacher model for thought generation)",
            "brief_description": "A large open-source instruct-tuned LLM used in the paper as the teacher to generate candidate thoughts during the data synthesis (exploration) phase.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct",
            "model_description": "Large instruct-tuned model (70B) used as the teacher/generator to produce multiple candidate thoughts per query during data production; prompts included instruction and in-context examples.",
            "reasoning_methods": [
                "long-form thought generation (instruction + examples prompting)"
            ],
            "reasoning_methods_description": "Used to explore multiple candidate long-form thoughts per query (exploration stage of data synthesis); candidates are later refined via a retrieval committee.",
            "reasoning_diversity": "diverse (multiple candidates sampled per query)",
            "reasoning_diversity_experimental_setup": "Used in data production (exploration-refinement) pipeline; candidates evaluated by retrieval committee and majority voting to produce training supervision.",
            "task_or_benchmark": "Data synthesis for MS MARCO training set; downstream evaluation on MS MARCO dev and BEIR datasets.",
            "performance_results": "No direct performance metric for the teacher model itself; its generations form training data that contributed to O1 Embedder's gains.",
            "qualitative_findings": "Teacher generations produce patterns that align with positive documents; to avoid trivial solutions, training excludes using rephrased positive documents as thoughts.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Using a strong instruction-tuned LLM as a generator in an exploration-refinement pipeline produces thought supervision that, when used in joint training, improves retrieval performance.",
            "uuid": "e8309.7",
            "source_info": {
                "paper_title": "O1 Embedder: Let Retrievers Think Before Action",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 1,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        }
    ],
    "cost": 0.0193145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>O1 Embedder: Let Retrievers Think Before Action
12 Feb 2025</p>
<p>Ruiran Yan yanruiran@mail.ustc.edu.cn 
Zheng Liu zhengliu1026@gmail.com 
Defu Lian liandefu@ustc.edu.cn </p>
<p>University of Science and Technology of China Hefei
China</p>
<p>Beijing Academy of Artifical Intelligence
BeijingChina</p>
<p>University of Science and Technology of China Hefei
China</p>
<p>O1 Embedder: Let Retrievers Think Before Action
12 Feb 20253EA6C0E3DCA30D0CD7614985DB77F5DBarXiv:2502.07555v2[cs.CL]Dense RetrievalEmbedding ModelLarge Language Models
The growing power of large language models (LLMs) has revolutionized how people access and utilize information.Notably, the LLMs excel at performing fine-grained data representation, which facilitates precise retrieval of information.They also generate highquality answers based on external references, enabling the production of useful knowledge.The recent introduction of reasoning models, like OpenAI O1 1 and DeepSeek R1 2 , marks another leap forward, highlighting LLMs' ability to think progressively before delivering final answers.This breakthrough significantly improves the ability to address complex tasks, e.g., coding and math proofs.Inspired by this progress, we aim to develop similar capabilities for retrieval models, which hold great promise for tackling critical challenges in the field, including multi-task retrieval, zero-shot retrieval, and tasks requiring intensive reasoning of complex relationships.With this motivation, we propose a novel approach called O1 Embedder, which generates useful thoughts for the input query before making retrieval for the target documents.To realize this objective, we conquer two technical difficulties.First, we design a data synthesis workflow, creating training signals for O1 Embedder by generating initial thoughts from an LLM-expert and subsequently refining them using a retrieval committee.Second, we optimize the training process, enabling a pre-trained model to be jointly fine-tuned to generate retrieval thoughts via behavior cloning and perform dense retrieval through contrastive learning.Our approach is evaluated by comprehensive experiments, where substantial improvements are achieved across 12 popular datasets, spanning both in-domain and out-of-domain scenarios.These results highlight O1 Embedder's remarkable accuracy and generalizability, paving the way for the development of next-generation IR foundation models.</p>
<p>Introduction</p>
<p>Information retrieval (IR) is fundamental to many important applications, such as search engines and question answering systems [23,24].Recently, it draws even higher attention because of its critical role in augmenting large language models (LLMs) with external knowledge [36,67,69], a paradigm known as retrievalaugmented generation (RAG) [12,65,66].Over the past decade, IR techniques have experienced tremendous progresses.One important breakthrough is made by dense retrieval, where relevant data can be effectively retrieved via vector search.With the popularity of open-source models in this field [40,55,61], dense retrieval has become a go-to option for realizing retrieval applications in reality.</p>
<p>However, dense retrieval is still subject to many limitations in this stage.First, existing methods struggle with zero-shot retrieval tasks in unseen scenarios, which differ significantly from their source domains.For instance, a well-trained embedding model from general datasets is prone to a limited performance when applied to a specialized problem, like medical or legal case retrieval [31,39,50].Second, the existing models are insufficient to discriminate complex relationships, as they cannot be identified directly from semantic meaning.For example, the retrieval of useful code snippets for a computer program, or the retrieval of evidence to a multi-hop reasoning problem [19,27,32,63].</p>
<p>The above challenges can benefit from a prior deep reasoning process, instead of making direct judgment of relevance.Recently, remarkable advancements were made in this direction with the introduction of reasoning LLMs, such as OpenAI O1, O3, and DeepSeek R1 [14].Particularly, when a complex task is presented, the LLM is prompted to generate long-form thoughts about the problem in the first place.Through this process, the LLM can progressively get close to the correct solution, thus enabling the production of high-quality answers in the end.This operation is conceptualized as the test-time-scaling by recent studies [44], which has driven major improvements in solving complex problems, such as coding and mathematical proofs.</p>
<p>With the above inspiration, we propose O1 Embedder, which is designed to introduce a slow-thinking capability for embedding models, akin to that of LLMs.Our approach integrates two essential functions within a single model: Thinking and Embedding.First, it generates useful thoughts towards the input query, which explicitly uncovers the hidden information needs about the query.Secondly, it produces a discriminative embedding for the query and the generated thoughts.By incorporating both elements, the resulting embedding enables precise retrieval of relevant documents that are challenging to identify using the query alone.</p>
<p>The training of O1 Embedder is technically challenging given the absence of appropriate long-form thought data for embedding models.To solve this problem, we introduce a Data Synthesis method following an "Exploration-Refinement" process.First, we prompt an LLM to explore initial thoughts for a query.Next, we employ a retrieval committee to refine the initial thoughts.Each committee member scores the relevance between initial thoughts and the target document, which indicates their usefulness in retrieving the target document.With the collection of all members' scoring results, the golden thought is selected by majority voting and added to the training set.As such, we automatically create long-form thoughts of the best retrieval utility for O1 Embedder.</p>
<p>Building on well-curated long-form thought data, we introduce a Multi-Task Training Method, which fine-tunes a pre-trained LLM into O1 Embedder.Our method introduces two parallel training tasks.One applies supervised fine-tuning for the LLM, which enables the generation of optimal thoughts for an input query.The other one employs contrastive learning, which produces discriminative embeddings to retrieve relevant documents for a thoughtaugmented query.With proper optimizations in loss computation and training workflow, these two tasks are seamlessly unified in a cost-efficient manner, leading to effective development of thinking and embedding capabilities throughout the training.</p>
<p>The effectiveness of O1 Embedder is comprehensive evaluated.In our experiment, O1 Embedder achieves a substantial improvement over existing methods across a broad range of retrieval tasks, especially those requiring complex reasoning.O1 Embedder also demonstrates a strong generalization ability when applied to out-of-domain scenarios.Finally, O1 Embedder well maintains a superior performance across various LLM backbones, like Llama [13,49], Mistral [21], and Qwen [13].Our model and code will be made publicly available to advance research in this field.</p>
<p>To summarize, the main contributions of this paper are highlighted by the following perspectives.</p>
<p> We propose O1 Embedder, which generates useful thoughts about the input query before producing discriminative embeddings for dense retrieval.To the best of our knowledge, O1 Embedder is the first practice to equip embedding models with thinking capabilities, offering promising insights for future research. We design an exploration-refinement approach, producing long-form thoughts with the optimal retrieval utility.We also optimize the multitask training method, which effectively fine-tunes a pre-trained LLM into O1 Embedder. We perform comprehensive experiments for a detailed analysis O1 Embedder, whose result verifies the effectiveness and broad applicability of our method.pre-trained models, such as BERT and RoBERTa [8,34], for dense retrieval, which already demonstrated competitive performance compared to traditional methods like BM25.At the same time, the scope of dense retrieval was substantially expanded thanks to the adoption of multi-lingual [4,20] and multi-modal pre-trained models [56,68].The introduction of more advanced training strategies, such as retrieval-oriented adaptation [35,53,60], hard negative mining [62], batch size expansion [42], and knowledge distillation from cross-encoders [17], has continually contributed to the improvement of dense retrieval's performance.</p>
<p>In addition to the improvement on retrieval accuracy, it becomes increasingly emphasized to develop multi-task retrievers for general-purpose retrieval applications.Recent studies showed that the retrievers' generalization ability can be substantially enhanced by scaling-up the training scale [45] and model architecture [41].Based on these inspirations, people have made significant expansion of pre-training and fine-tuning tasks, leading to a series of popular retrievers for general-purpose applications, such as BGE, E5, and GTE [33,52].Meanwhile, people also introduce large language models (LLMs) as the retrievers' backbones, which brings forth significant improvements in retrieval performance.For example, RepLLaMA presents a powerful dense retriever by directly fine-tuning a pre-trained Llama [55].Llama2Vec further enhances RepLLaMA by incorporating unsupervised adaptation of the pretrained Llama [28].Promptriver [58], built on RepLLaMA, equips the retrieval model with the capability to follow instructions.Methods like NV-Embed and ICL-Embedder achieves additional improvement through continual training with extensive fine-tuning data [26,29].Today, LLM-powered retrievers have dominated nearly all major benchmarks in IR-related evaluation.</p>
<p>Despite these remarkable advancements, existing methods are primarily designed for direct semantic matching in popular applications like web search and question-answering.They still face challenges with zero-shot retrieval in completely new scenarios that differ significantly from their source domains [11,69].In addition, they are insufficient for more complex retrieval tasks which require intensive reasoning to identify semantic relationships [46].</p>
<p>LLMs' Reasoning Ability</p>
<p>The reasoning capabilities of large language models (LLMs) have been significantly enhanced with techniques that simulate humanlike problem-solving processes.A major breakthrough in this area is Chain-of-Thought (CoT) [57], which prompts LLMs to tackle complex problems by decomposing them into multiple reasoning steps.Building on this progress, the Self-Consistency method improves reasoning robustness by sampling multiple reasoning paths from the LLM and selecting the final answer through majority voting [9].For scenarios requiring more exploratory reasoning, the Tree of Thoughts (ToT) method [64] extends CoT by structuring the problem-solving process as a tree.At each node, the LLM generates candidate intermediate steps, evaluates their feasibility, and backtracks from dead ends.Further advancing this paradigm, Graph of Thoughts (GoT) [2] replaces the tree structure with a directed acyclic graph (DAG), enabling LLMs to merge or refine reasoning steps as needed.The reasoning capability of large language models (LLMs), or the "think before action" workflow, represents a new paradigm that sets them apart from traditional language models.In addition to the usual strategies of scaling model size, datasets, and training computation [16,22], the expansion of inference computation, or test-time scaling [5,43,59], becomes another important factor in driving the improvement of LLMs.This capability has been significantly enhanced and showcased by recent reasoning-capable LLMs, such as OpenAI's O1 and O3, DeepSeek's R1 [14], and Gemini 2.0 3 .These models adopt a "slow-thinking" approach when handling complex problems: instead of providing an immediate answer, they first generate verbose, structured reasoning before arriving at a final solution.This method has allowed LLMs to achieve elite-level performance in areas like coding and mathematical proofs.</p>
<p>The reasoning capability also offers a significant advantage in addressing the challenges posed by traditional retrieval methods.However, current embedding models primarily focus on generating discriminative data representations, which leaves the development of reasoning capabilities largely unexplored.</p>
<p>Method</p>
<p>In this section, we first present the problem formulation of O1 Embedder.We then introduce the two main technical contributions of this work: the production of long-form thought data for O1 Embedder, and the multi-task training process of O1 Embedder.</p>
<p>Problem Formulation</p>
<p>Dense retrieval measures the relevance between query and document based on their embedding similarity.Given an query  and document , an embedding model (M) is used to encode them into latent representations   and   :    M (),    M ().To retrieve the relevant document  * from a massive dataset , the following nearest neighbor condition needs to be satisfied:
 * = argmax. {  ,   |   },(1)
where  indicates the inner-product operator.As discussed, traditional embedding models are insufficient to handle the challenges regarding zero-shot or complex retrieval problems.Our method tackles the above challenge by introducing the thinking operation to embedding models.That is to say, the embedding model M is equipped with two functionalities: thinking M.think() and embedding M.embed().For an input query , the embedding 3 https://deepmind.google/technologies/gemini/flash-thinking/model generates its thoughts () on how to address the information needs of the query in the first place:
   M.think(),  = 1, ..., (2)
By revealing critical semantic matching patterns with relevant documents, the generated thoughts are expected facilitate the retrieval process for complex queries.In this place, a total of  thoughts are independently generated with respect to the query, which enables useful patterns to be comprehensively covered.</p>
<p>On top of the embedding model M and an aggregation function Agg, the query and its thoughts are jointly transformed into a unified embedding, called the thought-augmented embedding ( v ):
v  Agg.(, {  }  ; M.embed)(3)
As a result, the relevance between query and document is computed with the thought-augmented embedding:  v ,   .Finally, our problem is formulated as the joint training of thinking and embedding capability of model M, such that the end-to-end retrieval performance is optimized.</p>
<p>Data Production</p>
<p>The training of O1 Embedder involves two types of data.One is used for the embedding capability, which is made up of queries and their relevant documents, i.e., q-doc tuples.The other one is used for the thinking capability, which includes queries and their thoughts, i.e., q-thought tuples.Unlike q-doc tuples which have been widely existed, there are no available q-thought tuples in reality.To resolve this problem, we propose a data synthesis pipeline, leveraging LLMs' readily equipped reasoning capacity to generate such datasets.Our method follows an "explorationrefinement" workflow, as demonstrated in Figure 2. First, we employ a LLM to explore candidate thoughts for a given query .To facilitate proper generations from the LLM, the system prompt is formulated with the following template, where both instruction and examples are incorporated:
Prompt = Task : {Ins}; Examples : {E}; Query : {q} (4)
The instruction is used to explicitly declare the demand for the thinking task; for example, "think about a plausible response to address the query".While the examples are introduced to demonstrate the form of desirable output.In this place, we randomly select  samples from the training set of q-doc dataset:
E = {Query :   , Response :   }  .(5)
Note that although the relevant document  for query  may seem like a trivial solution, it is unsuitable to serve as a thought.This is because the generated thought will be further used in embedding task, whose goal is to discriminate the relevant document  based on the thought-augmented embedding.If  (or any rephrased version of it) is used, the training process would be circumvented, ultimately leading to the collapse of the embedding task.The generated thoughts may not always enhance retrieval performance due to potential hallucinations by the LLM.To ensure the inclusion of useful thoughts, the exploration process is repeated multiple times, generating several independent thoughts for the given input query.To identify the most useful thoughts, a qualitycontrol mechanism is introduced to filter the generated candidates.Specifically, we employ a diverse set of retrievers, denoted as .In the second step, the retrieval committee is employed to evaluate the candidates by making comparison with the ground-truth document, i.e. the retrieval target.Finally, the candidate thought receiving the maximum votes is selected and incorporated to the training data.</p>
<p>For each retriever   , a similarity score is computed between a relevant document  and a thought   :   (  , ).The thought with the highest similarity score is selected by each retriever, i.e.,  *   argmax({  (  , )} =1... ).Finally, a majority voting process is conducted to determine the most useful thought.The thought that receives the highest number of nominations from the retrievers is selected as the final result:   Voting{ *  }   .By applying the above data synthesis workflow to an existing qdoc dataset:  = {(  ,   )}  , we can obtain an thought-augmented dataset composed of q-thought-doc triplets: D = {(  ,   ,   )}  , which offers long-form thoughts of the optimal retrieval utility.</p>
<p>Multi-Task Training</p>
<p>The O1 embedder is built upon a pre-trained LLM, leveraging the model's inherent generation and reasoning abilities.Additionally, LLMs also show strong potential for fine-tuning as discriminative embedding models [37,70].We apply the following multitask training for a pre-trained LLM backbone, which establishes its thinking capability via supervised behavior cloning and embedding ability through contrastive learning.</p>
<p>Behavior cloning.</p>
<p>The foundation model is trained to generate thoughts for the input query through supervised fine-tuning.Given the dataset of q-thought-doc triplets: D = {(  ,   ,   )}  , a training sample   is formulated with the template below:
  = <query>  <thought>  </s>,(6)
where <query>, <thought>, </s> are the special tokens to landmark the query, thought, and the completion of generation, respectively.With the formulation of above training samples, the model is fine-tuned w.r.t. the following generation loss:
L  =      |  |+|  | =|  | log  ( , | ,&lt;  ),(7)
where the next-token-prediction loss is minimized for each of the tokens starting from the beginning of the thought (i.e.  |  |).</p>
<p>Contrastive learning.</p>
<p>The pre-trained LLM is also fine-tuned to distinguish relevant documents from a query based on its generated embeddings.Traditionally, LLM-based embedders utilize the </s> token for text embedding.However, the </s> token has been designated to indicate the completion of generation process in our approach.As a result, incorporating an extra embedding task could lead to the collapse of training process.To avoid mutual interference between the two parallel training tasks, we employ another special token <emb> and append it to the end of input  (following </s>) to compute the text embedding:
   LLM(; <emb>) <a href="8">1</a>
This simple modification substantially increases the compatibility, which is crucial to maintain the successful running of joint training process.Considering that people may want to leverage thoughtaugmented embedding to handle complex retrieval tasks while still relying on basic query embedding to process simple retrieval tasks, we generate the two forms of query embeddings simultaneously to identify the relevant document   .As a result, we perform the following composite contrastive learning, where two contrastive losses are calculated based on the query and the thoughtaugmented query of each training sample, respectively:
L  =    exp(       )   exp(       ) + exp(  q    )   exp(  q    )(9)
In this place, q indicates the thought-augmented query: q    +  ,   stands for the collection of negative samples, including both in-batch negative samples and hard negative samples introduced by a pre-trained embedder.</p>
<p>Joint training.</p>
<p>The model is trained to minimize the linear combination of generation loss and contrastive loss:
L = L  + (1  )L (10)
To enable precise retrieval in downstream scenarios, contrastive learning must be conducted with a large training batch.However,  the native parallel running of two training tasks requires significant GPU memory, which severely limits the achievable batch size.To address this challenge, we propose a memory-efficient joint training to handle both tasks (Figure 3).Specifically, for each training sample   = (  ,   ,   ), we encode it once and share the encoding result between the two tasks.The generative loss is calculated based on each of the output embeddings from   tokens; while the contrastive loss is derived based on the output embeddings from <emb> tokens.This allows the generation task to consume almost no additional memory when trained alongside contrastive learning task, thereby enabling a substantial increase in batch size.</p>
<p>O1 Embedder</p>
<p>Retrieval</p>
<p>The well-trained O1 embedder M is applied for retrieval tasks through thinking and embedding.First, O1 embedder is prompted to generated multiple thoughts towards the input query which comprehensively uncover the query's hidden information needs:    M.think(q),  = 1, ..., .Next, the thought-augmented queries are independently encoded and aggregated.In this place, we simply adopt mean pooling as the aggregation function in Eq. 3, which produces the following thought-augmented embedding:
v    M. enc(,   ) (11)
Finally, the top-N documents  * are retrieved based on their embedding similarity with v :  *  top-N({ v ,   | }).</p>
<p>Experiment</p>
<p>We make comprehensive evaluation of our approach with a focus on the following research questions.With these research questions, we design our experimental studies, whose settings are presented as follows.</p>
<p>Settings</p>
<p>4.1.1Datasets.O1 embedder is trained by the thought-augmented queries created from the MS MARCO (passage retrieval) dataset [1].The well-trained model is evaluated based on both in-domain and out-of-domain datasets.For in-domain evaluation, we utilize MS MARCO (dev), TREC DL19 [7], and TREC DL20 [6] datasets.For outof-domain evaluation, we incorporate the following eight questionanswering datasets from BEIR [47], including SciFact [51], TREC-COVID [50], DBpedia [15], NQ [25], HotPotQA [63], FiQA [39], Touche [3], FEVER [48], along with a popular dataset on codesearch: CosQA [18].All of these datasets consist of asymmetric retrieval tasks, where the query and document are presented in very different forms.As a result, the relationships between query and document can be more effectively identified through appropriate reasoning.We exclude common paraphrasing datasets, such as Quora, as they only involve simple similarity comparisons.Following prior works [38,58], we use MRR@10 and Recall@1k as metrics for MS MARCO-related tasks, and NDCG@10 for other datasets.Evaluations on o.o.d.datasets strictly adhere to the BEIR protocol, which prohibits task-specific fine-tuning.</p>
<p>4.1.2Baselines.We choose a wide variety of popular retrievers as our baselines, such as BM25, a commonly used sparse retrieval method, and ANCE [62], TAS-B [17], coCondenser [10], SimLM [53] which fine-tune BERT-based pre-trained models using MS MARCO dataset.We also introduce the LLM-based methods, including Re-pLLaMA [54], Promptriver [58].RepLLaMA fine-tunes a pre-trained LLM on MS MARCO, resulting in superior retrieval performance across various downstream tasks.While Promptriever builds on RepLLaMA by enhancing the model's instruction-following capabilities, which further improves the retrieval performance on top of tailored prompts.Both LLM-based methods are fine-tuned from the same pre-trained backbone (Llama-2-7B) as our default setting, thus ensuring a fair comparison in terms of model scale.Note that we exclude several other popular retrievers from our evaluation, including BGE [61], E5 [52], M3 [4], and recent LLM-based methods like E5-Mistral [55], ICL-Embedder [30], and NV-Embed [26].These models utilize significantly more training data beyond MS MARCO (the only training dataset used by our method and our included baselines), many having strong overlaps with the evaluation tasks on BEIR.This overlap makes it difficult to assess zero-shot retrieval performance in out-of-distribution (o.o.d.) settings.Additionally, these methods rely on different training datasets, which makes it impossible to maintain a fair comparison.</p>
<p>Implementation Details.</p>
<p>During the data preparation stage, we leverage a powerful open-source LLM: Llama-3.1-70B-Instruct 4 , to generated the candidate thoughts.We employ BM25, BGE-ENlarge-v1.5,GTE-large, and Stella-EN-1.5B-v5, to serve the retrieval committee.The evaluation is primary made based on a Llama-2-7B backbone [49], with other alternative LLMs analyzed in the extended study.The training process follows RepLLaMA's recipe [38], where all projection layers (q_proj k_proj v_proj o_proj gate_proj down_proj up_proj) of the LLM are fine-tuned via LoRA, with rank set to 32 and alpha set to 64.We used BF16 for training, with learning rate set to 1  10 4 .The training process takes place on 8xA800 GPUs, with a batch size set to 64 (8 per-device).We introduce 15 hard negatives for each query.The maximum query length was set to 32, and the maximum passage length was set to 192.The max tokens were set to 256 for thought generation. 4https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct</p>
<p>Main Results</p>
<p>4.2.1</p>
<p>In-domain Performance.The in-domain evaluation results are presented in Table 1, where the O1 Embedder outperforms all other models across all evaluation metrics, including MRR@10, Recall@1k, and nDCG@10, on the MSMARCO, DL'19, and DL'20 datasets.Specifically, our model achieves an MRR@10 of 43.1 (+1.9% improvement) and a Recall@1k of 99.5 on MSMARCO, along with nDCG@10 scores of 75.3 (+1.0%improvement) and 74.4 (2.1% improvement) on DL'19 and DL'20, respectively, compared to Re-pLLaMA.These results demonstrate the effectiveness of our model and its enhanced reasoning capabilities in retrieval tasks.Furthermore, when comparing model sizes, it is clear that larger models, such as the LLM-based RepLLaMA and Promptriever, generally outperform smaller BERT-based models.For example, RepLLaMA achieves an nDCG@10 of 74.3 on DL'19 and 72.1 on DL'20, surpassing all smaller models.The O1 Embedder without thinking (O1 embedder w/o T), a variation that disables the reasoning operation, yields similar results as RepLLaMA, suggesting that the embedding capability is effectively established through multi-task training.However, with the addition of thinking, the full O1 Embedder demonstrates a substantial improvement, highlighting the power of the "thinking" enhancement.2, where the O1 Embedder consistently outperforms across all nine datasets.On average, our model achieves a 2.3% improvement over the baseline, which is a significant boost and underscores the strong generalization capabilities of our approach.Additionally, for each of the nine datasets, our model sets the highest performance except for FiQA, where it falls behind CPT-XL 175B.This highlights that our model excels across various scenarios, especially the retrieval tasks involving complex reasoning, such as HotPotQA (multi-hop question-answering) and CosQA (code-search).</p>
<p>Several interesting conclusions can be derived from the table.The thinking mechanism leads to more significant improvements on certain OpenQA datasets.For instance, the NQ dataset shows a 3.9% improvement, while HotPotQA sees a 3.0% boost.However, while there are improvements in some specialized domains, they are not as substantial.For example, in TREC-Covid (medical domain), FiQA (financial domain), and SciFact (scientific domain), the improvements brought by the thinking mechanism are only 0.9%, 1.7%, and 1.6%, respectively.We believe this is because LLMs perform well on general OpenQA questions after training on large-scale corpora, but often lack specialized domain data.As a result, the generated content can sometimes be noisy or even hallucinated.While the thinking mechanism still provides some enhancement, its impact is not as pronounced in these domains.Thus, refining the generated thoughts will be an important issue for future research.</p>
<p>Impact of Thought.</p>
<p>The comparisons between the O1 Embedder and the O1 Embedder w/o T in Tables 1 and 2 clearly demonstrate the significant impact of incorporating the thinking mechanism.Specifically, on the MS MARCO benchmark (Table 1), the O1 Embedder shows substantial improvements across all metrics, with notable gains in MRR@10 (+1.4%) and nDCG@10 for both DL'19 (+1.6%) and DL'20 (+2.1%).Similarly, in the zero-shot evaluation (Table 2), the O1 Embedder outperforms its counterpart across all datasets, achieving an average nDCG@10 score of 61.4-2.3 points higher than the O1 Embedder w/o T. This performance boost can be attributed to the thinking mechanism, which allows the model to generate additional contextual information during the encoding process.By incorporating this supplementary information, the model enhances its understanding of the user's query, leading to more accurate and effective retrieval.Additionally, it is worth noting that even in its "fast" mode (represented by O1 Embedder w/o T), the model achieves results comparable to those of RepLLaMA.This highlights that our model maintains competitive retrieval performance, even without the generated reasoning, underscoring its flexibility and adaptability.</p>
<p>4.2.4</p>
<p>Summary for main result.Based on the above discussion, we come to the following conclusions in response to RQ 1-3:</p>
<p>Con 1.Our model significantly outperforms both BERT-based and LLM-based models for in-domain evaluations, indicating that the model's thinking and embedding capabilities are effectively learned through the multi-task training process.Con 2. Our model well maintains superior performances throughout different o.o.d.tasks, which verifies our strong generalization ability.This advantage is especially pronounced when handling those challenging problems.Con 3. The thinking operation brings forth significant help, as it substantially improves upon the retrieval performance from the vanilla embedding model.</p>
<p>Extended Analysis</p>
<p>With the verification of O1 Embedder's overall effectiveness in the previous discussions, we perform extended studies in this section, where the following detailed problems are analyzed: RQ 4. How much does our joint multi-task training strategy contribute to the model's performance?RQ 5. Whether our method stays robust to different model architectures and parameter settings?RQ 6.Why does our method achieves such significant improvements in its retrieval performance?The corresponding issues are discussed in subsections 4. of thinking and embedding ability, O1 embedder achieves a substantial improvement in retrieval performance.In contrast, RepLLaMA directly leverages the thoughts generated by GPT-4o-mini (using the same prompt as O1 embedder), which leads to a suboptimal performance due to the incompatibility of the two modules.In this table, "Base" denotes retrieval directly with query, "with T" denotes retrieval using the query with thought, "" denotes the improvement.</p>
<p>Joint training.</p>
<p>To analyze the impact from joint training, we make analysis on whether existing retrieval models can make effective use of the generated thoughts in a training-free manner.</p>
<p>For this purpose, we introduce a stand-alone generator for a welltrained retriever, which generates thoughts for its presented queries.</p>
<p>In our experiment, we leverage RepLLaMA as the retriever and GPT-4o-mini as the generator for our experiments.The results of this approach are shown in Table 3.While incorporating thoughts results in mild improvements on datasets like NQ and HotPotQA, it causes declines on others, such as Trec-Covid and FiQA.Overall, this approach leads to only minor gains on some tasks but results in a slight decrease of 0.1 in overall performance.In contrast, our model consistently improves the retrieval performance across all datasets.This suggests that, with joint training, our model can better utilize the generated thoughts.The untrained RepLLaMA model, however, appears to be negatively impacted by the potential noise within the generated thoughts, leading to worse results, particularly in specialized domains like Trec-Covid, FiQA, and SciFact.In brief, the above observation indicates that our method not only generates useful thoughts for retrieval, but also learns to make effective use of the thoughts through joint training.</p>
<p>Robustness.</p>
<p>In this section, we verify the robustness of our method from two perspectives.First, we implement our method based on different pre-trained architectures, including Llama, Mistral, Qwen.Second, we also introduce LLMs of different sizes (ranging from 0.5B to 8B) for evaluation.Impact of different model backbone.The previous experiments primarily used Llama-2-7B as the backbone, which is consistent with RepLLaMA and promptriever.To verify the generalizability of our approach, we repeat the same experiment with implementations on different backbone LLMs.The results in Table 4 demonstrate our effectiveness across different settings.Notably, our approach well maintains a strong retrieval performance in both indomain and out-of-domain evaluations.This observation suggests that our method is generally effective with various architectures, regardless of their difference in pre-trained capabilities.</p>
<p>Impact of different model sizes.We can also clearly observe the significant impact of model size on performance in Table 4.As the size of the Qwen2.5 models decreases, there is a noticeable drop in effectiveness across all evaluated datasets.While the 3B model performs similarly to the 7B model, further reductions in size lead to more pronounced performance declines.This is partly because larger models tend to have better generalization capabilities, benefiting from training on more extensive corpora during pre-training.It's worth noting that even a 1.5B model, through retrieval with thought, performs comparably to the 7B RepLLaMA, which further highlights the advantage of our approach.</p>
<p>Query: what age was martin luther king when he was admitted Thought: Martin Luther King Jr. was admitted to Morehouse College in Atlanta, Georgia at the age of 15.He attended the college from 1944 to 1948, where he earned a Bachelor of Arts degree in sociology.Positive Doc: Dr. Martin L. King, Jr. and His Mentors: A Message for America Today If it was not for Benjamin Mays... Benjamin Mays was the president of Morehouse College in Atlanta when he met Martin Luther King, Jr.In 1944, Martin Luther King was admitted to the college at age 15. ... Table 5: Examples of the original query, thinking content and the positive document.The similar patterns between gnenrated content and the groundtruth are marked in green.</p>
<p>Case Study.</p>
<p>In Table 5, we demonstrate an example of the generated thought and the ground-truth document to a complex multi-hop query.In this case, the query asks about Martin Luther King's age upon his admission to college.Our thought generated effectively uncovers useful contextual information, highlighting that King was admitted to Morehouse College at the age of 15 in 1944.This generated content not only answers the query directly but also enriches the context by providing additional details about the college and the timeframe of his attendance.By generating such useful patterns, the embedding model can obtain crucial information related to the query, which results in a more precise retrieval result.We include more case analysis for O1 Embedder in Appendix D and E.</p>
<p>Conclusion</p>
<p>In this paper, we introduce O1 Embedder, a novel retrieval model that performs slow-thinking before executing retrieval actions.This approach allows the model to better understand the underlying information needs within the query, aiding in the identification of semantic relevance for complex retrieval tasks.Leveraging our tailored data production method, we generate long-form thoughts optimized for retrieval utility.Additionally, our proposed multi-task training method effectively establishes both the model's thinking and embedding capabilities.We conduct comprehensive experiments on popular evaluation benchmarks, and the results demonstrate that O1 Embedder significantly outperforms existing methods, achieving substantial improvements in retrieval performance across both in-domain and out-of-domain scenarios.Our work lays the foundation for future research in advanced retrieval models with reasoning capabilities.Future directions include expanding the reasoning process to multi-round interactions, exploring lightweight distillation techniques, and applying the approach to other retrieval tasks.The O1 Embedder marks a promising paradigm for next-generation IR systems, showcasing the potential of integrating the classic dense retrieval methods with large language models' outstanding reasoning abilities.</p>
<p>A Prompts</p>
<p>System Prompt You are a helpful assistant.Your anwer should be follow the task description.Do not ask the user for further clarification.Don't repeat the query, just give the response.Response Physical medicine, also known as physiatry, is a branch of medicine that focuses on helping patients with physical disabilities or impairments to achieve maximum function and independence.Physiatrists, or physical medicine doctors, use a multidisciplinary approach to diagnose and treat a wide range of conditions, including musculoskeletal disorders, neurological conditions, and chronic pain.Physical medicine doctors use various techniques, such as physical therapy, occupational therapy, and pain management, to help patients recover from injuries or illnesses, manage chronic conditions, and improve their overall quality of life.They may also use advanced technologies, such as electromyography (EMG) and nerve conduction studies, to diagnose and treat conditions.Some common conditions treated by physical medicine doctors include back pain, arthritis, spinal cord injuries, stroke, and traumatic brain injuries.Physical medicine doctors work with patients to develop personalized treatment plans that address their unique needs and goals, with the aim of restoring function, reducing pain, and improving overall well-being.Table 6: Example of the Prompt and the Response of the teacher model for data generation.For presentation purposes, we use <omit> to omit the long documentation in the example.Table 6 shows the example of complete prompts and the response of the teacher model used to generate the training data.For each specific query, we randomly select 3 query and positive document pair to form the in-context examples.During the data generation process, we found that a significant portion of the responses duplicated the query provided.So we add "Don't repeat the query, just give the response."in the system prompts.</p>
<p>B Detailed Scores of Different Base Models</p>
<p>Table 7 provides the detailed scores of different base model for zero-shot evaluation in section 4.3.2.We can see that although the results of different base models fluctuate a bit in each dataset, there is not much difference in the overall performance of models of the same scale.While when the model parameters decreased, there was a significant decrease in the final results.This suggests that larger models are more capable of generating and exploiting high-quality thoughts and have greater generalization.The Algorithm 1 outlined in the Data production process in section 3.2.For each query-document pair (  ,   ) in the dataset, the algorithm first samples  examples from  to create a prompt, which includes specific instructions.It then generates  candidate thoughts   by formatting the prompt with the sampled examples.Next, for each retrieval model  , it calculates the similarity scores between the candidate thoughts and the document   , selecting the thought with the highest score as  *  .Finally, the algorithm aggregates these top thoughts through a majority voting mechanism to determine the final thought   .The enhanced dataset D is Table 7: Detailed scores of different base model for zero-shot evaluation in section 4.3.2.</p>
<p>C Algorithm</p>
<p>then constructed, consisting of the original queries, the generated thoughts, and their corresponding positive documents.</p>
<p>D Additional Case analyses &amp; Attention</p>
<p>Query: Hayden is a singer-songwriter from Canada, but where does Buck-Tick hail from?Thought: Buck-Tick is a Japanese rock band, and its members are from various parts of Japan and their music is a unique blend of alternative rock, gothic rock, and visual kei styles.</p>
<p>Positive Doc: Buck-Tick Buck-Tick (stylized as BUCK-TICK) is a Japanese rock band, formed in Fujioka, Gunma in 1983.The group has consisted of ...  To better understand why thought can better help with retrieval, we further analyzed the attention scores of the model for the simple example in Table 8.We calculate the attention weight of the <emb> token with each token   in the thought:
_ (  ) =   exp (  &lt;&gt;     ) | |
=1 exp (  &lt;&gt;     ) where   &lt;&gt; and    are the "query" vector and the th token's "key" vector of the th attention head.|| is the total length of input sequence.</p>
<p>To enhance the clarity of our findings, we present the top 20 attention scores from the "<emb>" token in the generated thought, illustrated in Figure 4.The results reveal that the model assigns high scores to similar patterns found between the thought and the positive document, indicating its effectiveness in recognizing and focusing on relevant generated patterns.Notably, the word "Japan" emerges with the highest attention score, despite not appearing in the original query.This highlights the critical role of the generated thought in the retrieval process, as it enables the model to supplement essential information that may be absent from the query.</p>
<p>E Complex Retrieval Example</p>
<p>We present another case from CosQA dataset demonstrating the capabilities of O1 embedder in Table 9, to effectively generate and retrieve relevant code snippet.The query posed, "python check if a directory is writable" illustrates a common programming challenge.The O1 embedder responds by generating a comprehensive thought that not only provides a direct solution using the os.access() function but also includes an example code snippet.This case highlights the model's ability to synthesize information and present it in an accessible format, thereby aiding more accurate query representation.Additionally, the thought generated by the model incorporates alternative methods, such as verifying if the path is indeed a directory before checking write permissions.This demonstrates the O1 embedder's depth of understanding by providing multiple approaches to the problem.Through this case, we illustrate that the O1 embedder is capable of producing coherent, contextually relevant outputs that significantly improve the utility of code retrieval systems.</p>
<p>Figure 1 :
1
Figure 1: O1 Embedder.First of all, the model generates the thoughts about the question (thinking).Next, the model produces the embedding for dense retrieval (retrieval).</p>
<p>Question:Figure 2 :
2
Figure2: The production of thought data.In the first step, the LLM is prompted to generate candidates thoughts about the input question based on the instruction and in-context examples.In the second step, the retrieval committee is employed to evaluate the candidates by making comparison with the ground-truth document, i.e. the retrieval target.Finally, the candidate thought receiving the maximum votes is selected and incorporated to the training data.</p>
<p>Figure 3 :
3
Figure 3: Training and Retrieval process of O1 Embedder.During the training process, O1 embedder minimizes two losses: the generation loss while decoding the thought, and the contrastive loss while discriminating the target document.During the retrieval process, multiple thoughts are generated for the query.The thoughts are used to produce thought-augmented queries, which are independently encoded by O1 Embedder and aggregated for retrieval.</p>
<p>RQ 1 .
1
Whether O1 Embedder can outperform popular baseline retrievers after fine-tuning?RQ 2. Whether O1 Embedder can be effectively generalized to outof-domain scenarios?RQ 3. How much does the thinking operation help?</p>
<ol>
<li>2 . 2 O
22
.O.D. Performance.The out-of-distribution (o.o.d.) evaluation results are shown in Table</li>
</ol>
<p>Figure 4 :
4
Figure 4: Top 20 Attention score from <emb> token in the thought</p>
<p>Table 1 :
1
In-domain evaluation results evaluated on MS MARCO, DL'19, and DL'20.O1 embedder w/o T indicates the variational form of O1 embedder, which disables the thinking operation but uses the same embedding model.
,</p>
<p>Table 2 :
2
Out-of-domain evaluation results measured by nDCG@10.
MethodModel size T-Covid NQ HQA FiQA Touche DBPedia FEVER SciFact CosQA AverageContriever0.1B59.649.8 63.832.923.041.375.867.714.247.6CPT-L6B56.2-64.845.230.941.275.674.4--CPT-XL175B64.9-68.851.229.143.277.575.4--OpenAI-Ada-002-81.348.2 65.441.128.040.277.373.628.953.7RepLLaMA7B84.762.4 68.545.830.543.783.475.632.358.5Promptriver7B84.662.6 69.546.632.045.282.876.332.859.2O1 embedder w/o T7B84.562.9 69.845.033.844.482.575.832.959.1O1 embeder7B85.666.8 72.846.636.747.384.977.434.161.4</p>
<p>Table 3 :
3
3.1, 4.3.2, and 4.3.3,respectively.Exploration of joint training.With the joint training
RepLLaMAO1 embedderbase with Tbase with TDL'1974.372.4-1.9 73.775.3+1.6DL'2072.172.6+0.5 72.374.4+2.1Trec-Covid 84.779.7-5.0 84.585.6+1.1NQ62.465.1+2.7 62.966.8+3.9HotPotQA 68.571.7+3.2 69.872.8+3.0FiQA45.840.2-5.6 45.046.6+1.6Touche30.533.3+2.8 33.836.7+2.9DBPedia43.744.2+0.5 44.447.3+2.9FEVER83.485.5+2.1 82.585.0+2.5SciFact75.674.2-1.4 75.877.4+1.6CosQA32.333.0+0.7 32.934.1+1.2Avg61.261.1-0.1 61.663.8+2.2</p>
<p>Table 4 :
4
The impact from using different backbone models of variant pre-trained architectures and model sizes.The detailed scores for the zero-shot evaluation are presented in Appendix B. O1 Embedder well maintains a strong performance throughout these settings.
in-domaino.o.d.MS MARCO DL'19 DL'20 AVGLlama-2-7B43.175.374.461.4Mistralv0.3-7B43.577.075.661.4Llama-3.1-8B43.576.274.561.6Qwen2.5-7B43.376.474.761.2Qwen2.5-3B42.576.374.560.3Qwen2.5-1.5B41.974.073.558.7Qwen2.5-0.5B40.573.671.455.4</p>
<p>Enhanced dataset D = {(  ,   ,   )}  1: Initialize D   2: for each (  ,   )   do   Prompt. (, ,   )    Voting( *  )   12: D  D  {(  ,   ,   )}
3:for  = 1 to  do4:  SampleExamples()5:6:   . ()7:end for8:for  = 1 to || do9:   10: end for11:13: end for14: returnD
Algorithm 1 Data Production Process Require: Query-document dataset  = {(  ,   )}  Teacher model for generating thoughts  Retrieval models  = { 1 ,  2 , ...,  | | } Example count , candidate thoughts  Prompt tamplate: Prompt, Instruction:  Functions:  SampleExamples: Samples  examples from     : Similarity score function of Retrieval model   Voting: majority voting function Ensure: *   argmax({  (  , )} =1... )</p>
<p>Table 8 :
8
Another Example from HotPotQA (zero-shot).
Top 20 Attention Values from <emb>0.0250.005 0.010 0.015 0.020 Attention Value0.000Japan Buck styles . Tick from Japanese -a its and i of band is , visual of rock blend
print('The directory is not writable')Positive Doc: def _writable_dir(path): """ Whether <code>path</code>is a directory, to which the user has write access.""" return os.path.isdir(path)and os.access(path, os.W_OK) Table9: An example of O1 embedder solving a complex code retrieval problem.
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew Mcnamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, Tong Wang, arXiv:1611.09268[cs.CLMS MARCO: A Human Generated MAchine Reading COmprehension Dataset. 2018</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Overview of Touch 2020: argument retrieval. Alexander Bondarenko, Maik Frbe, Meriem Beloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association. Proceedings. Thessaloniki, GreeceSpringer2020. September 22-25, 20202020</p>
<p>Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, 10.48550/arXiv.2402.03216arXiv:2402.03216BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. 2024</p>
<p>Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou, arXiv:2403.02419[cs.LG2024</p>
<p>Overview of the TREC 2020 deep learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, arXiv:2102.07662[cs.IR2021</p>
<p>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees, arXiv:2003.07820[cs.IROverview of the TREC 2019 deep learning track. 2020</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. Luyu Gao, Jamie Callan, 10.18653/v1/2022.acl-long.203Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>
<p>Precise zero-shot dense retrieval without relevance labels. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, arXiv:2212.104962022. 2022arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023. 2023arXiv preprint</p>
<p>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, Danny Livshits, David Wyatt, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Francisco Radenovic, Frank Guzmn, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Govind Lewis Anderson, Graeme Thattai, Gregoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel Ibarra, Ishan Kloumann, Ivan Misra, Jack Evtimov, Jade Zhang, Jaewon Copet, Jan Lee, Jana Geffert, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Kalyan Jia, Karthik Vasuden Alwala, Kartikeya Prasad, Kate Upasani, Ke Plawiak, Kenneth Li, Kevin Heafield, Khalid Stone, Krithika El-Arini, Kshitiz Iyer, Kuenley Malik, Kunal Chiu, Kushal Bhalla, Lauren Lakhotia, Laurens Rantala-Yeary, Lawrence Van Der Maaten, Liang Chen, Liz Tan, Louis Jenkins, Lovish Martin, Lubo Madaan, Lukas Malo, Lukas Blecher, Luke Landzaat, Madeline De Oliveira, Mahesh Muzzi, Mannat Pasupuleti, Manohar Singh, Marcin Paluri, Maria Kardas, Mathew Tsimpoukelli, Mathieu Oldham, Maya Rita, Melanie Pavlova, Mike Kambadur, Min Lewis, Mitesh Kumar Si, Mona Singh, Naman Hassan, Narjes Goyal, Nikolay Torabi, Nikolay Bashlykov, Niladri Bogoychev, Ning Chatterji, Olivier Zhang, Onur Duchenne, Patrick elebi, Pengchuan Alrassy, Pengwei Zhang, Petar Li, Peter Vasic, Prajjwal Weng, Pratik Bhargava, Praveen Dubal, Punit Krishnan, Puxin Singh Koura, Qing Xu, Qingxiao He, Ragavan Dong, Raj Srinivasan, Ramon Ganapathy, Ricardo Silveira Calderer, Robert Cabral, Roberta Stojnic, Rohan Raileanu, Rohit Maheswari, Rohit Girdhar, Romain Patel, Ronnie Sauvestre, Roshan Polidoro, Ross Sumbaly, Ruan Taylor, Rui Silva, Rui Hou, Saghar Wang, Sahana Hosseini, Sanjay Chennabasappa, Sean Singh, Bell, Sonia Seohyun, Sergey Kim, Shaoliang Edunov, Sharan Nie, Sharath Narang, Sheng Raparthy, Shengye Shen, Shruti Wan, Shun Bhosale, Simon Zhang, Soumya Vandenhende, Spencer Batra, Sten Whitman, Stephane Sootla, Suchin Collot, Sydney Gururangan, Tamar Borodinsky, Tara Herman, Tarek Fowler, Thomas Sheasha, Thomas Georgiou, Tobias Scialom, Todor Speckbacher, Tong Mihaylov, Ujjwal Xiao, Vedanuj Karn, Vibhor Goswami, Vignesh Gupta, Viktor Ramanathan, Vincent Kerkez, Virginie Gonguet, Vish Do, Vtor Vogeti, Vladan Albiero, Weiwei Petrovic, Wenhan Chu, Wenyin Xiong, Whitney Fu, Xavier Meers, Xiaodong Martinet, Xiaofang Wang, Wang, Ellen Xiaoqing, Xide Tan, Xinfeng Xia, Xuchao Xie, Xuewei Jia, Yaelle Wang, Yashesh Goldschlag, Yasmine Gaur, Yi Babaei, Yiwen Wen, Yuchen Song, Yue Zhang, Yuning Li, Zacharie Delpierre Mao, Zheng Coudert, Zhengxing Yan, Zoe Chen, Aaditya Papakipos, Aayushi Singh, Abha Srivastava, Adam Jain, Adam Kelsey, Adithya Shajnfeld, Adolfo Gangidi, Ahuva Victoria, Ajay Goldstand, Ajay Menon, Alex Sharma, Alexei Boesenberg, Allie Baevski, Amanda Feinstein, Amit Kallet, Amos Sangani, Anam Teo, Andrei Yunus, Andres Lupu, Annie Alvarado, Annie Dong, Anuj Franco, Aparajita Goyal, Arkabandhu Saraf, Ashley Chowdhury, Ashwin Gabriel, Assaf Bharambe, Azadeh Eisenman, Beau Yazdan, Ben James, Benjamin Maurer, Bernie Leonhardi, Beth Huang, Beto Loyd, Bhargavi De Paola, Bing Paranjape, Bo Liu, Boyu Wu, Braden Ni, Bram Hancock, Brandon Wasti, Brani Spence, Brian Stojkovic, Britt Gamido, Carl Montalvo, Carly Parker, Catalina Burton, Ce Mejia, Changhan Liu, Changkyu Wang, Chao Kim, Chester Zhou, Ching-Hsiang Hu, Chris Chu, Chris Cai, Christoph Tindal, Dustin Feichtenhofer ; Duc Le, Edward Holland, Eissa Dowling, Elaine Jamil, Eleonora Montgomery, Emily Presani, Emily Hahn, Eric-Tuan Wood, Erik Le, Esteban Brinkman, Evan Arcaute, Evan Dunbar, Fei Smothers, Felix Sun, Feng Kreuk, Filippos Tian, Firat Kokkinos, Francesco Ozgenel, Frank Caggioni, Frank Kanayet, Gabriela Medina Seide, Gabriella Florez, Gada Schwarz, Georgia Badeer, Gil Swee, Grant Halpern, Grigory Herman, Sizov, Guangyi, Guna Zhang, Hakan Lakshminarayanan, Hamid Inan, Han Shojanazeri, Hannah Zou, Hanwen Wang, Haroun Zha, Harrison Habeeb, Helen Rudolph, Henry Suk, Hunter Aspegren, Hongyuan Goldman, Ibrahim Zhan, Igor Damlaj, Igor Molybog, Ilias Tufanov, Irina-Elena Leontiadis, Itai Veliche, Jake Gat, James Weissman, James Geboski, Janice Kohli, Japhet Lam, Jean-Baptiste Asher, Jeff Gaya, Jeff Marcus, Jennifer Tang, Jenny Chan, Jeremy Zhen, Jeremy Reizenstein, Jessica Teboul, Jian Zhong, Jingyi Jin, Joe Yang, Jon Cummings, Jon Carvill, Jonathan Shepard, Jonathan Mcphie, Stephanie Torres, Stephen Max, Steve Chen, Steve Kehoe, Sudarshan Satterfield, Sumit Govindaprasad, Summer Gupta, Sungmin Deng, Sunny Cho, Suraj Virk, Sy Subramanian, Sydney Choudhury, Tal Goldman, Tamar Remez, Glaser ; Nam, Wang Yu, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy. Victoria Ajayi, Victoria Montanez, Vijai Mohan, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani; Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang,; Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar; Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt; Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta; Ye Hu, Ye Jia, Ye Qi, Yenda LiSargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,Parth Parekh, Paul Saab, Pavan Balaji. Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen. Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025. 2025arXiv preprint</p>
<p>DBpedia-Entity v2: A Test Collection for Entity Search. Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Erik Svein, Alexander Bratsberg, Jamie Kotov, Callan, 10.1145/3077136.3080751Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information RetrievalShinjuku, Tokyo, Japan; New York, NY, USAAssociation for Computing Machinery2017SIGIR '17)</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022. 2022arXiv preprint</p>
<p>Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. Sebastian Hofsttter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, Allan Hanbury, 10.48550/arXiv.2104.06967arXiv:2104.069672021</p>
<p>CoSQA: 20,000+ Web Queries for Code Search and Question Answering. Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2021.acl-long.442Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>CodeSearchNet challenge: Evaluating the state of semantic code search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1909.094362019. 2019arXiv preprint</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, arXiv:2112.09118[cs.IRUnsupervised Dense Information Retrieval with Contrastive Learning. 2022</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825[cs.CLMistral 7B. Renard Llio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothe Wang, William El Lacroix, Sayed, 2023</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020. 2020arXiv preprint</p>
<p>Dense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas Ouz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.049062020. 2020arXiv preprint</p>
<p>Information retrieval on the web. Mei Kobayashi, Koichi Takeda, ACM computing surveys (CSUR). 322000. 2000</p>
<p>Natural Questions: A Benchmark for Question Answering Research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 72019. 2019</p>
<p>Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, arXiv:2405.17428NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models. 2024. 2024arXiv preprint</p>
<p>Hyunji Lee, Sohee Yang, Hanseok Oh, Minjoon Seo, arXiv:2204.13596[cs.IRGenerative Multihop Retrieval. 2022</p>
<p>Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval. Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, Defu Lian, 10.18653/v1/2024.acl-long.191Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Making text embedders few-shot learners. Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, Zheng Liu, arXiv:2409.157002024. 2024arXiv preprint</p>
<p>Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, Zheng Liu, 10.48550/arXiv.2409.15700arXiv:2409.15700Making Text Embedders Few-Shot Learners. 2024</p>
<p>AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels. Lei Li, Xiangxu Zhang, Xiao Zhou, Zheng Liu, arXiv:2410.20050[cs.IR2024</p>
<p>Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, Ruiming Tang, arXiv:2407.02883[cs.IRCoIR: A Comprehensive Benchmark for Code Information Retrieval Models. 2024</p>
<p>Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, arXiv:2308.03281Towards general text embeddings with multi-stage contrastive learning. 2023. 2023arXiv preprint</p>
<p>Yinhan Liu, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019364arXiv preprint</p>
<p>RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. Zheng Liu, Shitao Xiao, Yingxia Shao, Zhao Cao, 10.18653/v1/2023.acl-long.148Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Information Retrieval Meets Large Language Models. Zheng Liu, Yujia Zhou, Yutao Zhu, Jianxun Lian, Chaozhuo Li, Zhicheng Dou, Defu Lian, Jian-Yun Nie, 10.1145/3589335.3641299Companion Proceedings of the ACM Web Conference 2024. Singapore, Singapore; New York, NY, USAAssociation for Computing Machinery2024WWW '24)</p>
<p>Large language models as foundations for next-gen dense retrieval: A comprehensive empirical assessment. Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, Kang Liu, arXiv:2408.121942024. 2024arXiv preprint</p>
<p>Fine-Tuning LLaMA for Multi-Stage Text Retrieval. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin, 10.1145/3626772.3657951Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24). the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Www'18 open challenge: financial opinion mining and question answering. Macedo Maia, Siegfried Handschuh, Andr Freitas, Brian Davis, Ross Mcdermott, Manel Zarrouk, Alexandra Balahur, Companion proceedings of the the web. 2018. 2018</p>
<p>Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, Lilian Weng, 10.48550/arXiv.2201.10005arXiv:2201.10005Text and Code Embeddings by Contrastive Pre-Training. 2022</p>
<p>Large Dual Encoders Are Generalizable Retrievers. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, Yinfei Yang, 10.18653/v1/2022.emnlp-main.669Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang, arXiv:2010.081912020. 2020arXiv preprint</p>
<p>Nikhil Sardana, Jacob Portes, Sasha Doubov, Jonathan Frankle, arXiv:2401.00448Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. 2023. 2023arXiv preprint</p>
<p>Scaling llm testtime compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.033142024. 2024arXiv preprint</p>
<p>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-Tau Yih, Noah A Smith, Luke Zettlemoyer, Tao Yu, arXiv:2212.09741One embedder, any task: Instruction-finetuned text embeddings. 2022. 2022arXiv preprint</p>
<p>Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-Yu Wang, Haisu Liu, Quan Shi, Zachary S Siegel, Michael Tang, arXiv:2407.12883Bright: A realistic and challenging benchmark for reasoning-intensive retrieval. 2024. 2024arXiv preprint</p>
<p>BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. Nandan Thakur, Nils Reimers, Andreas Rckl, Abhishek Srivastava, Iryna Gurevych, 10.48550/arXiv.2104.08663arXiv:2104.086632021</p>
<p>FEVER: a Large-scale Dataset for Fact Extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/v1/N18-1074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Marilyn Walker, Ji Heng, Amanda Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288[cs.CLSergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic</p>
<p>TREC-COVID: constructing a pandemic information retrieval test collection. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, Lucy Lu, Wang , 10.1145/3451964.3451965SI-GIR Forum. 541122021. Feb. 2021</p>
<p>Fact or Fiction: Verifying Scientific Claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, 10.18653/v1/2020.emnlp-main.609Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv:2212.03533Text embeddings by weakly-supervised contrastive pre-training. 2022. 2022arXiv preprint</p>
<p>SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, 10.48550/arXiv.2207.02578arXiv:2207.025782023</p>
<p>Text Embeddings by Weakly-Supervised Contrastive Pre-training. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, 10.48550/arXiv.2212.03533arXiv:2212.035332024</p>
<p>Improving Text Embeddings with Large Language Models. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv:2401.003682023. 2023arXiv preprint</p>
<p>Uniir: Training and benchmarking universal multimodal information retrievers. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, Wenhu Chen, European Conference on Computer Vision. Springer2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models. Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, Jack Hessel, 10.48550/arXiv.2409.11136arXiv:2409.111362024</p>
<p>Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24. 2024</p>
<p>RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder. Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao, 10.18653/v1/2022.emnlp-main.35Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>C-Pack: Packed Resources For General Chinese Embeddings. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, Jian-Yun Nie, 10.1145/3626772.3657878Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information RetrievalWashington DC USAACM2024</p>
<p>Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk, 10.48550/arXiv.2007.00808arXiv:2007.008082020</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.096002018. 2018arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, Jian-Yun Nie, arXiv:2310.07554Retrieve anything to augment large language models. 2023. 2023arXiv preprint</p>
<p>Retrieval augmented generation (rag) and beyond: A comprehensive survey on how to make your llms use external data more wisely. Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K Qiu, Lili Qiu, arXiv:2409.149242024. 2024arXiv preprint</p>
<p>Dense text retrieval based on pretrained language models: A survey. Jing Wayne Xin Zhao, Ruiyang Liu, Ji-Rong Ren, Wen, ACM Transactions on Information Systems. 422024. 2024</p>
<p>VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval. Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, Yongping Xiong, arXiv:2406.042922024. 2024arXiv preprint</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.071072023. 2023arXiv preprint</p>
<p>Large Language Models for Information Retrieval: A Survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, Ji-Rong Wen, 10.48550/arXiv.2308.07107arXiv:2308.071072024</p>
<p>Trec-Covid Nq Hotpotqa, FiQA Touche DBPedia FEVER SciFact CosQA Average O1 Embedder. Llama3.1-8B</p>
<p>. Embedder, Mistralv0.3-7B</p>
<p>O1 Embedder. </p>
<p>. Embedder, Qwen2.5-1.5B</p>
<p>. Embedder, Qwen2.5-0.5B</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2827 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2827</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2827</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-271903203</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.09559v1.pdf" target="_blank">HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HiAgent, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HiAgent prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HiAgent achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HiAgent consistently improves performance across various steps, highlighting its robustness and generalizability. Project Page: https://github.com/HiAgent2024/HiAgent .</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2827.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2827.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HIAGENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Working Memory Management Agent (HIAGENT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent framework that manages in-trial (working) memory hierarchically by generating subgoals as memory chunks, summarizing completed subgoal trajectories, and supporting on-demand retrieval of detailed past trajectories to solve long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HIAGENT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>HIAGENT prompts the base LLM to (1) propose a subgoal g_i before generating grounded actions, (2) keep detailed action-observation pairs only for the currently active subgoal, (3) when a subgoal is judged complete, summarize that subgoal's trajectory into a concise summarized observation s_i and replace the detailed action-observation pairs in working memory with the (g_i, s_i) pair, and (4) allow the LLM to request retrieval of a past subgoal's detailed trajectory on-demand via a retrieval function (e.g., Action: "retrieve(subgoal_id)"). The working memory at time t is formalized as m_t = (g_0, s_0, ..., g_{n-1}, s_{n-1}, g_n, a_{n0}, o_{n1}, ...), i.e., past subgoals are stored as subgoal + summarized observation while the current subgoal retains fine-grained trajectory details. Observation summarization is implemented via the same LLM (GPT-4) using prompts that ask for a concise summary plus a flag indicating whether the subgoal is met. Trajectory retrieval returns the full action-observation pairs for a requested subgoal and is issued by generating a retrieval action.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>GPT-4 (gpt-4-turbo) (used as both policy and summarizer; temperature=0, top_p=1)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (hierarchical, subgoal-chunked) with retrieval-augmented access to detailed trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Hierarchical working-memory where each past subgoal is stored as a (subgoal, summarized-observation) chunk produced by an LLM summarizer S(g_i, trajectory). The current subgoal's detailed action-observation trajectory is retained verbatim in the context. A retrieval module allows on-demand replacement of a summarized chunk with the detailed trajectory for a selected past subgoal. Summarized observations include an assessment of whether the subgoal has been achieved. The architecture is implemented via prompt engineering (subgoal-generation prompt, summarization prompt) and context manipulation: upon subgoal completion the system 'obscures' detailed pairs and appends the summarized pair; when retrieval is requested, the system injects the saved detailed pairs back into the LLM context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>No hard fixed numeric capacity specified; designed to reduce working-context token usage by summarization. Experiments used a maximum of 30 steps per episode; HIAGENT reduced context tokens by 35.02% on average relative to the STANDARD full-history baseline (i.e., baseline = 100%, HIAGENT ≈ 64.98% of baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>On-demand retrieval initiated by the LLM: the LLM outputs a retrieval action (e.g., retrieve(subgoal_id)), and the system injects the detailed action-observation trajectory for that subgoal back into the context. Retrieval selection is therefore LLM-driven (implicit importance/need-based retrieval), not a learned vector-similarity retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Detailed trajectories are appended to working memory during execution of the current subgoal; when the LLM determines the subgoal is complete, a summarization step S(g_i, trajectory) runs and the detailed trajectory is replaced/obscured by the summarized observation s_i. Summaries also indicate subgoal completion status. Retrieval can re-expose detailed trajectories if the LLM requests them later.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>AgentBoard tasks (Ma et al. 2024), including Jericho (text-based adventure) plus four other long-horizon agent tasks: Blocksworld, Gripper, Tyreworld, Barman</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Long-horizon multi-step planning tasks (typically >20 steps), partially observable POMDP-style environments, require sequential planning, grounding of subgoals to executable actions, puzzle-style manipulations (Blocksworld), object transport (Gripper), multi-step procedural task (Tyreworld: changing a tire), multi-step object-manipulation and mixing (Barman), and narrative/text interaction in Jericho (text-adventure).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Overall (averaged across five tasks): Success Rate = 42.0% (absolute), Progress Rate = 62.55% (absolute), Average Steps = 22.61 steps (mean), Context usage ≈ 64.98% of STANDARD (i.e., -35.02%), Run time ≈ 80.58% of STANDARD (i.e., -19.42%). Per-task highlights reported in the paper: Blocksworld SR 60.0% (vs STANDARD 30.0%), PR 80.0% (vs 35.0%), Average Steps 18.6 (vs 25.0). Tyreworld SR 60.0% (vs STANDARD 10.0%), PR 75.83% (vs 39.28%), Average Steps 19.0 (vs 28.4). Barman SR 30.0% (vs 10.0%). Jericho SR 10.0% (vs 5.0%). Gripper SR unchanged at 50.0% in this evaluation though context tokens were cut by >50% for that task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (STANDARD full-history working memory): Overall Success Rate = 21.0%, Progress Rate = 38.61%, Average Steps = 26.41 steps, Context tokens = 100% baseline, Runtime = 100% baseline. For Tyreworld specifically: STANDARD SR 10.0%, PR 39.28%, Steps 28.4; HIAGENT markedly improved these numbers (see performance_with_memory).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Ablations on Tyreworld: removing Observation Summarization (w/o OS) caused a large performance drop (SR fell from 60% to 30%: -30 percentage points; PR dropped by ~7.6 points) and increased context and runtime; removing Trajectory Retrieval (w/o TR) reduced SR by 10 percentage points (60% → 50%) and increased average steps by ~1.2; removing both modules produced a major decline (SR down to 30%). A comparison to Task Decomposition (generating subgoals but NOT obscuring past trajectories) showed that task decomposition alone improved over STANDARD (SR up to 40% from 10% in Tyreworld) but was still ~20 percentage points lower than HIAGENT and incurred increased runtime and context (runtime +5.7%, context +12.8%), indicating that the summarization + retrieval memory management is crucial beyond decomposition alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>The paper compares HIAGENT's hierarchical working-memory management primarily against (a) STANDARD full-history working memory and (b) a Task-Decomposition variant that generates subgoals but keeps full past trajectories. It does not present comparisons to vector-database retrieval or other long-term memory architectures for these text games. Result: HIAGENT (hierarchical chunking + summarization + retrieval) outperforms both alternatives in success/progress rates and context/runtime efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Chunking working memory around subgoals and summarizing completed subgoal trajectories (while retaining the detailed trajectory only for the active subgoal) significantly reduces redundant context, improves LLM decision executability on long-horizon tasks (HIAGENT maintains >80% action executability at long step counts vs. STANDARD which drops dramatically), doubles overall success rate versus the STANDARD full-history baseline, reduces required steps and runtime, and provides robust scaling with episode length. On-demand retrieval of detailed past trajectories is important for diagnosing failures and further improves success rate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2827.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2827.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STANDARD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STANDARD full-history prompting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The common agent prompting strategy that appends every action-observation pair from the current trial into the LLM context (working memory) and generates one action per step from that full context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>STANDARD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each timestep the LLM is prompted with the instruction + the entire sequence of past action-observation pairs (m_std_t = (o_t, a_{t-1}, o_{t-1}, ..., a_0, o_0)) and asked to output the next executable action. The full in-trial history is always included in the context; no explicit summarization or retrieval module is used. This is the paper's baseline agent for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>GPT-4 (gpt-4-turbo) (used as the agent policy in the experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (full-history context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Simple append-all history: every action-observation pair from the current trial is concatenated into the LLM prompt/context on each decision step; no summarization or hierarchical chunking, no selective retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Limited implicitly by the LLM context window; experiments used episodes up to 30 steps. In the paper STANDARD context usage is treated as the 100% baseline for reporting context efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>None beyond raw inclusion of full past history (recency via ordering in context).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>After each executed action the resulting observation is appended to the context (immediate addition every step).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Same AgentBoard tasks as HIAGENT: Blocksworld, Gripper, Tyreworld, Barman, Jericho (including Jericho text-adventure)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same long-horizon, partially-observable, multi-step planning tasks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Overall (averaged across five tasks): Success Rate = 21.0%, Progress Rate = 38.61%, Average Steps = 26.41 steps. Per-task examples: Blocksworld SR 30.0% PR 35.0% Steps 25.0; Tyreworld SR 10.0% PR 39.28% Steps 28.4; Barman SR 10.0%; Jericho SR 5.0%; Gripper SR 50.0%. Context tokens and runtime are used as the 100% baseline for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared directly in experiments to HIAGENT (hierarchical working memory) and to a Task-Decomposition variant; STANDARD performed substantially worse on long-horizon tasks, especially as episode length increased (drop in action executability).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Using the full uncompressed trial history as working memory leads to redundant contexts that degrade LLM performance on long-horizon tasks: longer histories lower action executability and reduce progress/success rate, showing the need for more efficient working-memory management strategies such as HIAGENT's subgoal-chunking and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2827.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2827.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaskDecomp (w.TD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task Decomposition baseline (generate subgoals but keep full trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline variant where the LLM is prompted to generate subgoals before grounding actions (task decomposition), but unlike HIAGENT it does not obscure detailed past trajectory information (keeps full-history context).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Task Decomposition (w.TD)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The LLM first produces subgoals and then generates actions to achieve each subgoal, similar to HIAGENT's decomposition step, but the past subgoals' detailed action-observation pairs remain in context (no summarization/obscuring). Thus decomposition is applied but working memory remains full-history.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>GPT-4 (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (full-history) despite decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Same full-history context as STANDARD but with an extra prompt stage to generate subgoals before grounding actions; no summarization or retrieval module to hide past trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Implicitly limited by LLM context; in experiments this variant used same episode limits (30 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>None (detailed past trajectories remain in the context already).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Past actions/observations are appended at each step as in STANDARD; subgoals are generated but past detailed trajectories are retained.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Evaluated on the same AgentBoard tasks; the paper reports Tyreworld results for this variant explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same long-horizon tasks; the variant isolates the effect of decomposition vs. memory summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example (Tyreworld): Task Decomposition (w.TD) achieved SR = 40.0% (vs STANDARD 10.0% and HIAGENT 60.0%), Progress Rate ≈ 67.4% (vs STANDARD ~39.3% and HIAGENT ~75.8%), Average Steps ≈ 22.8 (improved vs STANDARD 28.4) but with increased context usage (+12.8% over STANDARD) and increased runtime (+5.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Task decomposition alone provides notable improvements over STANDARD (e.g., +30 percentage points SR in Tyreworld) but is still significantly worse than HIAGENT; moreover, keeping full trajectories increases context tokens and runtime, indicating that task decomposition benefits are amplified when combined with HIAGENT-style summarization and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Direct comparison in the paper shows that decomposition + full-history (w.TD) is inferior to hierarchical summarization + retrieval (HIAGENT) in both effectiveness and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Task decomposition improves performance relative to naive full-history prompting, but efficient working-memory management (summarization + on-demand retrieval) is necessary to realize further gains in long-horizon tasks and reduce context/runtime overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents <em>(Rating: 2)</em></li>
                <li>Interactive fiction games: A colossal adventure <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in LLM Agents (Yao et al. 2022b) <em>(Rating: 1)</em></li>
                <li>Think-in-memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2827",
    "paper_id": "paper-271903203",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "HIAGENT",
            "name_full": "Hierarchical Working Memory Management Agent (HIAGENT)",
            "brief_description": "An LLM-based agent framework that manages in-trial (working) memory hierarchically by generating subgoals as memory chunks, summarizing completed subgoal trajectories, and supporting on-demand retrieval of detailed past trajectories to solve long-horizon tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HIAGENT",
            "agent_description": "HIAGENT prompts the base LLM to (1) propose a subgoal g_i before generating grounded actions, (2) keep detailed action-observation pairs only for the currently active subgoal, (3) when a subgoal is judged complete, summarize that subgoal's trajectory into a concise summarized observation s_i and replace the detailed action-observation pairs in working memory with the (g_i, s_i) pair, and (4) allow the LLM to request retrieval of a past subgoal's detailed trajectory on-demand via a retrieval function (e.g., Action: \"retrieve(subgoal_id)\"). The working memory at time t is formalized as m_t = (g_0, s_0, ..., g_{n-1}, s_{n-1}, g_n, a_{n0}, o_{n1}, ...), i.e., past subgoals are stored as subgoal + summarized observation while the current subgoal retains fine-grained trajectory details. Observation summarization is implemented via the same LLM (GPT-4) using prompts that ask for a concise summary plus a flag indicating whether the subgoal is met. Trajectory retrieval returns the full action-observation pairs for a requested subgoal and is issued by generating a retrieval action.",
            "base_llm": "GPT-4 (gpt-4-turbo) (used as both policy and summarizer; temperature=0, top_p=1)",
            "uses_memory": true,
            "memory_type": "working memory (hierarchical, subgoal-chunked) with retrieval-augmented access to detailed trajectories",
            "memory_architecture": "Hierarchical working-memory where each past subgoal is stored as a (subgoal, summarized-observation) chunk produced by an LLM summarizer S(g_i, trajectory). The current subgoal's detailed action-observation trajectory is retained verbatim in the context. A retrieval module allows on-demand replacement of a summarized chunk with the detailed trajectory for a selected past subgoal. Summarized observations include an assessment of whether the subgoal has been achieved. The architecture is implemented via prompt engineering (subgoal-generation prompt, summarization prompt) and context manipulation: upon subgoal completion the system 'obscures' detailed pairs and appends the summarized pair; when retrieval is requested, the system injects the saved detailed pairs back into the LLM context.",
            "memory_capacity": "No hard fixed numeric capacity specified; designed to reduce working-context token usage by summarization. Experiments used a maximum of 30 steps per episode; HIAGENT reduced context tokens by 35.02% on average relative to the STANDARD full-history baseline (i.e., baseline = 100%, HIAGENT ≈ 64.98% of baseline).",
            "memory_retrieval_method": "On-demand retrieval initiated by the LLM: the LLM outputs a retrieval action (e.g., retrieve(subgoal_id)), and the system injects the detailed action-observation trajectory for that subgoal back into the context. Retrieval selection is therefore LLM-driven (implicit importance/need-based retrieval), not a learned vector-similarity retrieval.",
            "memory_update_strategy": "Detailed trajectories are appended to working memory during execution of the current subgoal; when the LLM determines the subgoal is complete, a summarization step S(g_i, trajectory) runs and the detailed trajectory is replaced/obscured by the summarized observation s_i. Summaries also indicate subgoal completion status. Retrieval can re-expose detailed trajectories if the LLM requests them later.",
            "text_game_benchmark": "AgentBoard tasks (Ma et al. 2024), including Jericho (text-based adventure) plus four other long-horizon agent tasks: Blocksworld, Gripper, Tyreworld, Barman",
            "game_characteristics": "Long-horizon multi-step planning tasks (typically &gt;20 steps), partially observable POMDP-style environments, require sequential planning, grounding of subgoals to executable actions, puzzle-style manipulations (Blocksworld), object transport (Gripper), multi-step procedural task (Tyreworld: changing a tire), multi-step object-manipulation and mixing (Barman), and narrative/text interaction in Jericho (text-adventure).",
            "performance_with_memory": "Overall (averaged across five tasks): Success Rate = 42.0% (absolute), Progress Rate = 62.55% (absolute), Average Steps = 22.61 steps (mean), Context usage ≈ 64.98% of STANDARD (i.e., -35.02%), Run time ≈ 80.58% of STANDARD (i.e., -19.42%). Per-task highlights reported in the paper: Blocksworld SR 60.0% (vs STANDARD 30.0%), PR 80.0% (vs 35.0%), Average Steps 18.6 (vs 25.0). Tyreworld SR 60.0% (vs STANDARD 10.0%), PR 75.83% (vs 39.28%), Average Steps 19.0 (vs 28.4). Barman SR 30.0% (vs 10.0%). Jericho SR 10.0% (vs 5.0%). Gripper SR unchanged at 50.0% in this evaluation though context tokens were cut by &gt;50% for that task.",
            "performance_without_memory": "Baseline (STANDARD full-history working memory): Overall Success Rate = 21.0%, Progress Rate = 38.61%, Average Steps = 26.41 steps, Context tokens = 100% baseline, Runtime = 100% baseline. For Tyreworld specifically: STANDARD SR 10.0%, PR 39.28%, Steps 28.4; HIAGENT markedly improved these numbers (see performance_with_memory).",
            "has_ablation_study": true,
            "memory_ablation_results": "Ablations on Tyreworld: removing Observation Summarization (w/o OS) caused a large performance drop (SR fell from 60% to 30%: -30 percentage points; PR dropped by ~7.6 points) and increased context and runtime; removing Trajectory Retrieval (w/o TR) reduced SR by 10 percentage points (60% → 50%) and increased average steps by ~1.2; removing both modules produced a major decline (SR down to 30%). A comparison to Task Decomposition (generating subgoals but NOT obscuring past trajectories) showed that task decomposition alone improved over STANDARD (SR up to 40% from 10% in Tyreworld) but was still ~20 percentage points lower than HIAGENT and incurred increased runtime and context (runtime +5.7%, context +12.8%), indicating that the summarization + retrieval memory management is crucial beyond decomposition alone.",
            "comparison_with_other_memory_types": "The paper compares HIAGENT's hierarchical working-memory management primarily against (a) STANDARD full-history working memory and (b) a Task-Decomposition variant that generates subgoals but keeps full past trajectories. It does not present comparisons to vector-database retrieval or other long-term memory architectures for these text games. Result: HIAGENT (hierarchical chunking + summarization + retrieval) outperforms both alternatives in success/progress rates and context/runtime efficiency.",
            "key_findings_about_memory_effectiveness": "Chunking working memory around subgoals and summarizing completed subgoal trajectories (while retaining the detailed trajectory only for the active subgoal) significantly reduces redundant context, improves LLM decision executability on long-horizon tasks (HIAGENT maintains &gt;80% action executability at long step counts vs. STANDARD which drops dramatically), doubles overall success rate versus the STANDARD full-history baseline, reduces required steps and runtime, and provides robust scaling with episode length. On-demand retrieval of detailed past trajectories is important for diagnosing failures and further improves success rate.",
            "uuid": "e2827.0",
            "source_info": {
                "paper_title": "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "STANDARD",
            "name_full": "STANDARD full-history prompting baseline",
            "brief_description": "The common agent prompting strategy that appends every action-observation pair from the current trial into the LLM context (working memory) and generates one action per step from that full context.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "STANDARD",
            "agent_description": "At each timestep the LLM is prompted with the instruction + the entire sequence of past action-observation pairs (m_std_t = (o_t, a_{t-1}, o_{t-1}, ..., a_0, o_0)) and asked to output the next executable action. The full in-trial history is always included in the context; no explicit summarization or retrieval module is used. This is the paper's baseline agent for comparisons.",
            "base_llm": "GPT-4 (gpt-4-turbo) (used as the agent policy in the experiments)",
            "uses_memory": true,
            "memory_type": "working memory (full-history context)",
            "memory_architecture": "Simple append-all history: every action-observation pair from the current trial is concatenated into the LLM prompt/context on each decision step; no summarization or hierarchical chunking, no selective retrieval.",
            "memory_capacity": "Limited implicitly by the LLM context window; experiments used episodes up to 30 steps. In the paper STANDARD context usage is treated as the 100% baseline for reporting context efficiency.",
            "memory_retrieval_method": "None beyond raw inclusion of full past history (recency via ordering in context).",
            "memory_update_strategy": "After each executed action the resulting observation is appended to the context (immediate addition every step).",
            "text_game_benchmark": "Same AgentBoard tasks as HIAGENT: Blocksworld, Gripper, Tyreworld, Barman, Jericho (including Jericho text-adventure)",
            "game_characteristics": "Same long-horizon, partially-observable, multi-step planning tasks used in the paper.",
            "performance_with_memory": "Overall (averaged across five tasks): Success Rate = 21.0%, Progress Rate = 38.61%, Average Steps = 26.41 steps. Per-task examples: Blocksworld SR 30.0% PR 35.0% Steps 25.0; Tyreworld SR 10.0% PR 39.28% Steps 28.4; Barman SR 10.0%; Jericho SR 5.0%; Gripper SR 50.0%. Context tokens and runtime are used as the 100% baseline for comparisons.",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": "Compared directly in experiments to HIAGENT (hierarchical working memory) and to a Task-Decomposition variant; STANDARD performed substantially worse on long-horizon tasks, especially as episode length increased (drop in action executability).",
            "key_findings_about_memory_effectiveness": "Using the full uncompressed trial history as working memory leads to redundant contexts that degrade LLM performance on long-horizon tasks: longer histories lower action executability and reduce progress/success rate, showing the need for more efficient working-memory management strategies such as HIAGENT's subgoal-chunking and summarization.",
            "uuid": "e2827.1",
            "source_info": {
                "paper_title": "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "TaskDecomp (w.TD)",
            "name_full": "Task Decomposition baseline (generate subgoals but keep full trajectories)",
            "brief_description": "A baseline variant where the LLM is prompted to generate subgoals before grounding actions (task decomposition), but unlike HIAGENT it does not obscure detailed past trajectory information (keeps full-history context).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Task Decomposition (w.TD)",
            "agent_description": "The LLM first produces subgoals and then generates actions to achieve each subgoal, similar to HIAGENT's decomposition step, but the past subgoals' detailed action-observation pairs remain in context (no summarization/obscuring). Thus decomposition is applied but working memory remains full-history.",
            "base_llm": "GPT-4 (gpt-4-turbo)",
            "uses_memory": true,
            "memory_type": "working memory (full-history) despite decomposition",
            "memory_architecture": "Same full-history context as STANDARD but with an extra prompt stage to generate subgoals before grounding actions; no summarization or retrieval module to hide past trajectories.",
            "memory_capacity": "Implicitly limited by LLM context; in experiments this variant used same episode limits (30 steps).",
            "memory_retrieval_method": "None (detailed past trajectories remain in the context already).",
            "memory_update_strategy": "Past actions/observations are appended at each step as in STANDARD; subgoals are generated but past detailed trajectories are retained.",
            "text_game_benchmark": "Evaluated on the same AgentBoard tasks; the paper reports Tyreworld results for this variant explicitly.",
            "game_characteristics": "Same long-horizon tasks; the variant isolates the effect of decomposition vs. memory summarization.",
            "performance_with_memory": "Example (Tyreworld): Task Decomposition (w.TD) achieved SR = 40.0% (vs STANDARD 10.0% and HIAGENT 60.0%), Progress Rate ≈ 67.4% (vs STANDARD ~39.3% and HIAGENT ~75.8%), Average Steps ≈ 22.8 (improved vs STANDARD 28.4) but with increased context usage (+12.8% over STANDARD) and increased runtime (+5.7%).",
            "performance_without_memory": null,
            "has_ablation_study": true,
            "memory_ablation_results": "Task decomposition alone provides notable improvements over STANDARD (e.g., +30 percentage points SR in Tyreworld) but is still significantly worse than HIAGENT; moreover, keeping full trajectories increases context tokens and runtime, indicating that task decomposition benefits are amplified when combined with HIAGENT-style summarization and retrieval.",
            "comparison_with_other_memory_types": "Direct comparison in the paper shows that decomposition + full-history (w.TD) is inferior to hierarchical summarization + retrieval (HIAGENT) in both effectiveness and efficiency.",
            "key_findings_about_memory_effectiveness": "Task decomposition improves performance relative to naive full-history prompting, but efficient working-memory management (summarization + on-demand retrieval) is necessary to realize further gains in long-horizon tasks and reduce context/runtime overhead.",
            "uuid": "e2827.2",
            "source_info": {
                "paper_title": "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents",
            "rating": 2,
            "sanitized_title": "agentboard_an_analytical_evaluation_board_of_multiturn_llm_agents"
        },
        {
            "paper_title": "Interactive fiction games: A colossal adventure",
            "rating": 2,
            "sanitized_title": "interactive_fiction_games_a_colossal_adventure"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in LLM Agents (Yao et al. 2022b)",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_llm_agents_yao_et_al_2022b"
        },
        {
            "paper_title": "Think-in-memory",
            "rating": 1,
            "sanitized_title": "thinkinmemory"
        }
    ],
    "cost": 0.01685275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HIAGENT: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model
18 Aug 2024</p>
<p>Mengkang Hu 
The University of Hong Kong</p>
<p>Tianxing Chen 
The University of Hong Kong</p>
<p>Qiguang Chen 
Harbin Institution of Technology</p>
<p>Yao Mu 
The University of Hong Kong</p>
<p>Wenqi Shao 
Shanghai Artificial Intelligence Laboratory</p>
<p>Ping Luo 
The University of Hong Kong</p>
<p>HIAGENT: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model
18 Aug 2024A584B36E000B54E29113A975F3AAD6DEarXiv:2408.09559v1[cs.CL]
Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks.The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of actionobservation pairs.We categorize memory into two types: crosstrial memory, accumulated across multiple attempts, and intrial memory (working memory), accumulated within a single attempt.While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored.Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks.Inspired by human problem-solving strategies, this paper introduces HIAGENT, a framework that leverages subgoals as memory chunks to manage the working memory of LLMbased agents hierarchically.Specifically, HIAGENT prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal.Experimental results across five long-horizon tasks demonstrate that HIAGENT achieves a twofold increase in success rate and reduces the average number of steps required by 3.8.Additionally, our analysis shows that HIAGENT consistently improves performance across various steps, highlighting its robustness and generalizability.</p>
<p>Introduction</p>
<p>Owing to the development of powerful reasoning capabilities of Large Language Models (LLMs) in recent years (OpenAI 2022(OpenAI , 2023;;Meta AI 2024;Touvron et al. 2023;Jiang et al. 2023), LLM-based agents have demonstrated significant potential in various applications (Xie et al. 2023;Wang et al. 2024;Xi et al. 2023), such as software development (Hong et al. 2023;Bairi et al. 2024), robotic planning (Yao et al. 2022b;Puig et al. 2018;Singh et al. 2023;Huang et al. 2022a), simulating human behavior (Park et al. 2023), etc.Typically, an LLM-based agent refers to an interactive system that processes environmental observations, maintains context across multiple rounds of dialogue, and outputs executable Figure 1: Top right: A commonly adopted paradigm STAN-DARD for LLM-based agents includes: i) prompts LLMs to generate one action; ii) executes the generated action and then append the obtained observation to the LLM's context (working memory); and iii) generates the next action.Bottom: Instead of incorporating all historical action-observation pairs into the working memory, HIAGENT leverage subgoals as memory chunks, with a summarized observation as the observation for each memory chunk.HIAGENT achieves an average success rate improvement of twofold (42 vs. 21) across five long-horizon tasks.</p>
<p>actions tailored to completing a given task.Memory is one of the critical components of LLM-based agents, involving how agents store and utilize past experiences.When handling a specific task, an agent's memory can be divided into crosstrial and in-trial memory (also as known as working memory).Cross-trial memory typically consists of the historical trajectory information accumulated across multiple attempts at the current task.In contrast, in-trial memory pertains to the information relevant to the current trial.While many papers have explored leveraging cross-trial memory to optimize agent performance (Shinn et al. 2024;Zhao et al. 2024;Guo et al. 2023), few have investigated ways to better utilize working memory.Existing LLM-based agent literature primarily employs the STANDARD strategy illustrated in Figure 1, where all action-observation pairs in working memory are directly incorporated into the context when prompting LLMs (Liu et al. 2023c;Ma et al. 2024;Yao et al. 2022b).Although this approach transmits the historical information to the LLM as comprehensively as possible, it encounters issues in longhorizon agent tasks.Such tasks typically require the agent to perform numerous actions to complete the task, resulting in an extensive working memory.This lengthy working memory creates a redundant context, hindering LLMs from maintaining coherent strategies and making accurate predictions over extended periods.</p>
<p>Drawing on principles of cognitive science (Newell, Simon et al. 1972;Anderson 2013), humans typically decompose a complex problem into multiple subproblems, addressing each individually.Each subproblem is treated as a memory "chunk," thereby reducing the cognitive load on working memory (Miller 1956).By focusing on the results of completed subproblems rather than their detailed execution, humans effectively manage cognitive resources and improve their efficiency in solving complex, long-horizon tasks.Inspired by human cognition and problem-solving strategies, we propose a sophisticated hierarchical working memory management framework HIAGENT tailored for long-horizon agent tasks.The core idea of HIAGENT is to trigger LLMs to generate subgoals, with each subgoal serving as a chunk of the working memory.Specifically, as shown in Figure 2, we first prompt the LLM to generate a subgoal, then create actions to achieve the subgoal and store the corresponding actionobservation pairs in a memory chunk.Once the subgoal is completed, we summarize the memory chunk and append the subgoal-observation pair to the working memory.In a word, HIAGENT triggers LLMs to proactively decide to replace previous subgoals with summarized observations while retaining only the action-observation pairs relevant to the current subgoal.To provide more flexible working memory management, we also introduce a trajectory retrieval module, which can retrieve the detailed trajectory information of specific past subgoals when necessary.</p>
<p>To validate the effectiveness and efficiency of HIAGENT, we conducted experiments on five long-horizon agent tasks from AgentBoard (Ma et al. 2024).The experimental results show that the success rate of HIAGENT is twice that of the STANDARD strategy, and it exceeds the STANDARD strategy by 23.94% in progress rate.Additionally, HIAGENT is more efficient than STANDARD strategy, reducing the average number of steps to complete tasks by 3.8, the context length by 35.02%, and the run time by 19.42%.Furthermore, to demonstrate that redundant context impairs the performance of LLM-based agents in long-horizon tasks, we compared HIAGENT to a method that generates subgoals without disregarding the detailed trajectory information of past subgoals.Experimental results show that HIAGENT improved the success rate by 20% while reducing both runtime and the number of steps.By analyzing model performance across varying step counts, we found that HIAGENT not only consistently outperformed STANDARD on progress rate but also showed a higher likelihood of generating executable actions as the number of steps increased.</p>
<p>Preliminary</p>
<p>Large Language Model based Agent</p>
<p>Large Language Model (LLM) based agents are intelligent autonomous systems designed to perform complex tasks.These tasks can be formalized as a partially observable Markov decision process (POMDP), characterized by the tuple (S, O, A, T, R), where: S denotes the state space; O represents the observation space; A signifies the action space; T : S ×A → S embodies the transition function; R : S ×A → R encapsulates the reward function; An LLM-based agent operates as a policy π(a t |I, o t , a t−1 , o t−1 , . . ., a 0 , o 0 ), which, given the historical action-observation pairs and instructions I (encompassing in-context examples, environmental descriptions, etc.), generates an executable action a t ∈ A. Each action precipitates a new state s t+1 ∈ S and a subsequent observation o t+1 ∈ O.This iterative interaction persists until either task completion or the agent reaches a predetermined maximum number of steps.</p>
<p>Working Memory</p>
<p>From the cognitive science perspective, working memory enables individuals to hold and manipulate information in real-time, facilitating complex cognitive tasks such as reasoning, comprehension, and learning (Newell, Simon et al. 1972;Anderson 2013).In LLM-based agents, we define working memory as the essential historical information required by the LLM at a given moment t to complete the current task.Effective working memory management allows for better integrating past experiences and current stimuli, leading to more informed and accurate decisions.It can be likened to the human process of attentional control and cognitive updating, which involves selectively focusing on relevant information, filtering out distractions, and continually updating the mental workspace with new and pertinent data.The STANDARD approach in Figure 1 stores all historical action-observation pairs in working memory, i.e., m std t = (o t , a t−1 , o t−1 , . . ., a 0 , o 0 ).Although this provides the LLM with comprehensive information, it also introduces redundancy, complicating the LLM's processing.</p>
<p>Methodology</p>
<p>Overview</p>
<p>The core idea of HIAGENT is to employ subgoals for hierarchical management of working memory.More specifically, as is shown in Figure 2, the process of HIAGENT can be described as follows: (1) Before generating specific grounded actions, we prompt the LLM to first formulate a subgoal g i .Each subgoal serves as a milestone within the overall task.(2) Subsequently, the LLM generates precise actions to accomplish this subgoal.(3) Upon the LLM's determination that a particular subgoal has been fulfilled, we synthesize the corresponding actionobservation pairs into a summarized observation s i ( §3.3).We then obscure the action-observation pairs within the context, substituting them with s i .Consequently, the working memory of HIAGENT can be formalized as m t = (g 0 , s 0 , ...., g n−1 , s n−1 , g n , a n0 , o n1 , ...).( 4) Additionally, we have incorporated a retrieval module to facilitate more flexible memory management( §3.4).For instance, if the q th subgoal is retrieved, we input the detailed action-observation pairs into the context rather than the summarized observation, i.e., m ′ t = (g 0 , s 0 , ...., g q , a q0 , a q0 , ..., g n , a n0 , o n0 , ...).</p>
<p>Subgoal-based Hierarchical Working Memory</p>
<p>As is shown in Figure 2, at each time step, the LLM can either generate the next action for the current subgoal or generate a new subgoal when it determines that the existing subgoal has been accomplished.For the current subgoal, the agent retains all action-observation pairs, providing a detailed context for immediate decision-making.For past subgoals, only a summarized version of the observations is kept.This subgoalbased hierarchical management approach in HIAGENT is deeply motivated by cognitive science principles, drawing parallels with human cognition and problem-solving strategies (Newell, Simon et al. 1972;Anderson 2013).Employing subgoals to compartmentalize action-observation pairs can be conceptualized as a form of chunking methodology.In human cognition, chunking allows individuals to group related information into meaningful units, thereby overcoming working memory limitations (Miller 1956).Similarly, HIAGENT utilizes subgoals as cognitive chunks, encapsulating related actions and observations.This chunking mechanism enables the system to handle complex sequences of information more effectively, reducing cognitive load and enhancing overall performance.Furthermore, by generating subgoals before specific actions, the system mimics the human tendency to break down larger objectives into more manageable components.This methodology enhances computational efficiency and aligns with established theories of human information processing.</p>
<p>Observation Summarization</p>
<p>The process of observation summarization can be formalized as s i = S(g i , o 0 , a 0 , ..., o t ), where S can be implemented using either a Large Language Model (LLM) or alternative text summarization models.This function encapsulates the synthesis of historical observations and actions, contextualized by the current subgoal, to produce a concise representation of the agent's state.Furthermore, a crucial component of the summarized observation is assessing whether the current subgoal has been achieved.This evaluation serves as a pivotal guide for future subgoal generation, facilitating adaptive and goal-oriented behavior in the agent's decision-making process.By doing so, the agent can maintain a condensed yet informative context, balancing the need for historical information with efficiency.The example prompt is as follows:</p>
<p>You are an advanced AI system tasked with summarizing and analyzing a series of action-observation pairs (trajectories) and determining whether a specific subgoal has been met.</p>
<p>Your goal is to create a summary that captures all essential information, decisions, and outcomes from the given trajectories, and indicate whether the subgoal has been met based on the summarized observations.</p>
<p>If there are no valid actions taken, you need to analyze the reason.</p>
<h3>Instructions:</h3>
<ol>
<li>
<p>Provide a summarized observation related to the subgoal in a concise manner.</p>
</li>
<li>
<p>Determine whether the subgoal has been met.</p>
</li>
</ol>
<p>Trajectory Retrieval</p>
<p>Despite the summarization, there may be instances where detailed past trajectory information becomes crucial for immediate decision-making.For instance, when a past subgoal execution fails, we need detailed trajectory information to determine the cause of failure.Moreover, reviewing past successful experiences can also increase the likelihood of success when facing novel challenges and scenarios.To address this, we introduce a trajectory retrieval module.To address this, we introduce a trajectory retrieval module.When the LLM determines that detailed information from a past subgoal is necessary, it generates a retrieval function to recall the complete action-observation pairs for that subgoal, analogous to the way to generate actions.This selective retrieval allows the agent to access detailed historical data on-demand without consistently carrying the full context.Baselines STANDARD prompting strategy is a predominantly used method in current LLM-based agent literature (Yao et al. 2022b;Ma et al. 2024;Liu et al. 2023c).</p>
<p>It operates by taking one action followed by one observation, providing a comparative baseline for evaluating the performance of HIAGENT.</p>
<p>Implementation Details The implementation of evaluation tasks is based on AgentBoard (Ma et al. 2024).We set a maximum step limit of 30 for task configuration and provide one in-context example for each task.We employ GPT-4 (gpt-4-turbo)1 as the LLM backbone for our experiments, serving both as the agent policy and the observation summarization model.We set the temperature hyperparameter for LLM inference to 0 and topp to 1. Detailed prompt examples are provided in the Appendix B. We can draw several conclusions from previous discussions:</p>
<p>Main Results</p>
<p>As shown in</p>
<p>(1) HIAGENT is more effective than STANDARD, achieving huge improvements on both success rate and progress rate.</p>
<p>(2) HIAGENT is also more efficient than STANDARD, requiring fewer steps to complete tasks, utilizing shorter context lengths, and achieving faster runtime.</p>
<p>Analysis</p>
<p>To gain deeper insights into our approach, we explored the following research questions:</p>
<p>(1) Are all modules effective for HIAGENT?</p>
<p>(2) Is HIAGENT consistently superior to the baseline at different steps?</p>
<p>(3) Is improvement of HIAGENT solely derived from task decomposition?(4) How effective are the frameworks in generating executable actions?</p>
<p>(5) Are the observed performance improvements in HIAGENT statistically significant compared to STANDARD?</p>
<p>Answer 1: All Modules in HIAGENT are Effective for HIAGENT</p>
<p>In this section, we conducted albation study to explore whether Observation Summarization and Trajectory Retrieval are effective.</p>
<p>Observation Summarization is effective.We heuristically use the observation corresponding to the last action as the summarized observation when removing the Observation Summarization module.As is shown in Table 2 ("w/o OS"), there is a significant decline in performance across all metrics.Specifically, the success rate and progress rate were significantly impacted, decreasing by 30% and 7.6%, respectively.It indicates that the observation summarization module can comprehensively aggregate the detailed information within a trajectory, thereby aiding the reasoning of an LLM-based agent.</p>
<p>Trajectory Retrieval is also crucial for performance enhancement.We hide all the detailed trajectory information of previous subgoals at each time step to verify the effectiveness of Trajectory Retrieval.According to the results in Table 2 ("w/o TR"), the success rate decreased by 10%, and the average steps increased by 1.2.This is because, while trajectory retrieval lengthens the reasoning steps of the LLM, it allows the agent to flexibly retrieve past trajectories under certain subgoals, which is more beneficial for identifying errors in previous actions.</p>
<p>The combination of Observation Summarization and Trajectory Retrieval yields significant improvement.We conducted an experiment where both modules were removed Therefore, a pertinent question arises: "Is the performance improvement attributed to HIAGENT merely related to task decomposition, rather than efficient working memory management?"To address this question, we implemented a new method that prompts the LLM to generate a subgoal before generating executable actions, followed by generating actions to achieve this subgoal.Unlike HIAGENT, this approach does not obscure the detailed trajectory information of previous subgoals.The experimental results, detailed in Table 3, indicate that although task decomposition can lead to a performance improvement (30% in success rate), the success rate is still 20% lower than HIAGENT.Additionally, solely using task decomposition introduces inefficiencies, increasing runtime by 5.7% and context length by 12.8%.In summary, HIAGENT is more efficient and effective than task decomposition alone.</p>
<p>Answer 4: HIAGENT is effective in generating executable actions even under long steps</p>
<p>LLM-based agents sometimes generate actions that cannot be executed, such as attempting to retrieve objects from a closed container.This is typically due to LLMs' poor reasoning abilities.To investigate this, we calculated the proportion of executable actions generated by the model at each timestep, referred to as executability.As shown in Figure 4, HIAGENT is more likely to generate executable actions than STANDARD, further demonstrating the effectiveness of HI-AGENT.Additionally, we observed that STANDARD is more   prone to generating non-executable actions when the steps are longer (e.g., in the blocksworld, when the steps exceed 20, executability drops below 10%).This is because, as the working memory increases, the ability of LLMs to generate executable actions decreases.In contrast, HIAGENT maintains over 80% executability even with longer steps, indicating that the robustness to long steps is a key factor in the strong performance on long-horizon tasks.</p>
<p>Answer 5:</p>
<p>The observed performance improvements in HIAGENT are statistically significant compared to STANDARD</p>
<p>To validate the statistical significance of the improvements in both effectiveness and efficiency, we selected the Progress Rate and Average Steps metrics for analysis.We employed the Wilcoxon signed-rank test (Woolson 2005) for this purpose due to its suitability for comparing paired samples.This non-parametric test helps assess whether the observed differences are likely due to chance or represent a genuine effect.The results of our analysis are as follows: (i) For the Progress Rate, the test statistic is 144.0 with a p-value of 2.38 × 10 −5 , indicating a statistically significant difference between HIA-GENT and STANDARD; (ii) For the Average Steps, the test statistic is 112.5 with a p-value of 0.0016, also demonstrating a statistically significant difference.These results confirm that the observed improvements in both effectiveness and efficiency are not due to random variation, underscoring the superiority of HIAGENT.</p>
<p>6 Related Work  HIAGENT distinguishes itself from the literature by not only utilizing planning to enhance task performance but also by using subgoals as memory chunks to manage working memory hierarchically.This approach brings context efficiency and surpasses methods that rely solely on planning, as discussed in Section 5.3.</p>
<p>Memory.The memory module in LLM-based agents is analogous to the human memory system, which is responsible for encoding, storing, and retrieving information (Zhang et al. 2024).The memory modules are typically divided into long-term memory and short-term memory.Long-term memory can usually be stored in an external database, while short-term memory (also known as working memory) is typically used directly as the context input of LLMs.Most current research papers primarily focus on managing long-term memory (Alonso et al. 2024;Maharana et al. 2024;Chen et al. 2024;Xiao et al. 2024;Yuan et al. 2023;Wang et al. 2023c;Majumder et al. 2023;Hu et al. 2023a;Hao et al. 2024;Lanchantin et al. 2024;Tu et al. 2023;Liang et al. 2023;Kagaya et al. 2024).Pioneer works include Memorybank (Zhong et al. 2024), with its global-level summaries, has made significant strides in distilling conversations into coherent narratives.</p>
<p>Other works, such as Think-in-memory (Liu et al. 2023b) and the Retroformer (Yao et al. 2023), incorporated summary modules to manage long-term memories.Unlike these works, our study investigates how optimizing the management of working memory can enhance agent performance.Another line of research involves modifying the structure of transformers to enable large language models (LLMs) to process longer contexts, thereby extending their working memory capabilities (Zhou et al. 2023c;Chevalier et al. 2023;Bertsch et al. 2024;Ruoss et al. 2023;Beltagy, Peters, and Cohan 2020;An et al. 2023).However, existing research has identified that LLMs encounter attention loss issues with lengthy texts (Liu et al. 2024a).Consequently, we believe that investigating more efficient management of working memory remains a valuable endeavor.</p>
<p>Conclusion</p>
<p>This paper proposes HIAGENT, a hierarchical framework that utilizes subgoals to manage the working memory of Large Language Model(LLM)-based agents.HIAGENT aims to address the poor performance of LLM-based agents when handling long-horizon tasks.Experimental results from five long-horizon agent tasks demonstrate that HIAGENT outperforms the baseline model across all tasks, with an overall success rate more than double that of the baseline model.Furthermore, HIAGENT is more efficient, accomplishing tasks with fewer steps, in less runtime, and using shorter context.We also conducted an ablation study to verify the effectiveness of the individual modules of HIAGENT.A series of analysis experiments demonstrate that as the number of steps increases, HIAGENT more effectively generates executable actions and consistently outperforms STANDARD in terms of progress rate.Additionally, we conducted a statistical test to validate the statistical significance of the improvements introduced by HIAGENT.We believe HIAGENT is an effective and flexible framework that can be integrated into other agent frameworks.In the future, we hope HIAGENT can inspire more creative ideas on effectively managing the working memory of LLM-based agents.</p>
<p>Action List</p>
<ol>
<li>
<p>open <container>: The precondition for this action is that the container is unlocked and closed.The effect of this action is that the container is open and not closed.</p>
</li>
<li>
<p>close <container>: The precondition for this action is that the container is open.The effect of this action is that the container is closed and not open.</p>
</li>
<li>
<p>fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open.The effect of this action is that the object is held by the agent and not inside the container.4. put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open.The effect of this action is that the object is inside the container and not held by the agent.5. loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground.The effect of this action is that the nut on hub is loose and not tight.6. tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground.The effect of this action is that the nut on hub is tight and not loose.7. jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack.It requires the agent to have a jack and for the hub to be on the ground.After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.8. jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack.It requires the agent to have the hub off the ground.After performing this action, the hub will be back on the ground and the agent will have the jack.9. undo <nut> <hub>: This action undo the fastening of a nut on a hub.The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose.The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.10. do-up <nut> <hub>: This action fasten a nut on a hub.The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened.The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut.</p>
</li>
<li>
<p>remove-wheel <wheel> <hub>: This action removes a wheel from a hub.It can only be performed if the hub is not on the ground, the wheel is currently on the hub, and the hub is unfastened.After the action is performed, the agent will have the removed wheel and the hub will be free, meaning that the wheel is no longer on the hub.12. put-on-wheel <wheel> <hub>: This action puts a wheel onto a hub.It can only be performed if the agent has the wheel, the hub is free, the hub is unfastened, and the hub is not on the ground.After the action is performed, the wheel will be on the hub, the hub will no longer be free, and the agent will no longer have the wheel.13. inflate <wheel>: This action inflates a wheel using a pump.It can only be performed if the agent has a pump, the wheel is not inflated, and the wheel is intact.After the action is performed, the wheel will be inflated.</p>
</li>
</ol>
<p>Goal example</p>
<p>w1 is in boot.</p>
<p>Observation example</p>
<p>Boot is closed.Boot is unlocked.Hub the-hub1 is fastened.Hub the-hub1 is on the ground.Jack is in boot.Pump is in boot.R1 is in boot.The nut nuts1 on the hub the-hub1 is tight.Wheel r1 is intact.Wheel r1 is not inflated.Wheel w1 is on hub the-hub1.Wrench is in boot.</p>
<p>Action example</p>
<p>Open boot.</p>
<p>A.4 Barman</p>
<p>Action List 1. <hand> grasp <container>: Grasp a container 2. <hand> leave <container>: Leave a container on the table 3. fill-shot <shot> <ingredient> <hand1> <hand2> <dis-penser>: Fill a shot glass with an ingredient from dispenser 4. refill-shot <shot> <ingredient> <hand1> <hand2> <dispenser>: Refill a shot glass with an ingredient from dispenser 5. empty-shot <hand> <shot> <beverage>: Empty a shot glass 6. clean-shot <shot> <beverage> <hand1> <hand2>: Clean a shot glass 7. pour-shot-to-clean-shaker <shot> <ingredient> <shaker> <hand1> <level1> <level2>: Pour an ingredient from a shot glass to a clean shaker from level1 to level2 8. pour-shot-to-used-shaker <shot> <ingredient> <shaker> <hand1> <level1> <level2>: Pour an ingredient from a shot glass to a used shaker from level1 to level2 9. empty-shaker <hand> <shaker> <cocktail> <level1> <level2>: Empty a shaker containing cocktail from level1 to level2 10. clean-shaker <hand1> <hand2> <shaker>: Clean a shaker 11. shake <cocktail> <ingredient1> <ingredient2> <shaker> <hand1> <hand2>: Shake a cocktail in a shaker 12. pour-shaker-to-shot <beverage> <shot> <hand> <shaker> <level1> <level2>: Pour a beverage from a shaker to a shot glass from level1 to level2</p>
<p>Goal example shot1 contains cocktail1.</p>
<p>Observation example</p>
<p>Goal example</p>
<p>You are the warrior Link that needs to save the princess from the castle.</p>
<p>Observation example</p>
<p>You are at the path leading to the castle.The castle is to your north.There is a barrel in front of you.put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open.The effect of this action is that the object is inside the container and not held by the agent.loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground.The effect of this action is that the nut on hub is loose and not tight.tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground.The effect of this action is that the nut on hub is tight and not loose.jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack.It requires the agent to have a jack and for the hub to be on the ground.After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack.It requires the agent to have the hub off the ground.After performing this action, the hub will be back on the ground and the agent will have the jack.undo <nut> <hub>: This action undo the fastening of a nut on a hub.The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose.The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.</p>
<p>Action example</p>
<p>do-up <nut> <hub>: This action fasten a nut on a hub.The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened.The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut.remove-wheel <wheel> <hub>: This action removes a wheel from a hub.It can only be performed if the hub is not on the ground, the wheel is currently on the hub, and the hub is unfastened.After the action is performed, the agent will have the removed wheel and the hub will be free, meaning that the wheel is no longer on the hub.put-on-wheel <wheel> <hub>: This action puts a wheel onto a hub.It can only be performed if the agent has the wheel, the hub is free, the hub is unfastened, and the hub is not on the ground.After the action is performed, the wheel will be on the hub, the hub will no longer be free, and the agent will no longer have the wheel.inflate <wheel>: This action inflates a wheel using a pump.It can only be performed if the agent has a pump, the wheel is not inflated, and the wheel is intact.After the action is performed, the wheel will be inflated.</p>
<p>Here are examples:</p>
<p>The goal is to satisfy the following conditions: w1 is in boot.put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open.The effect of this action is that the object is inside the container and not held by the agent.loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground.The effect of this action is that the nut on hub is loose and not tight.tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground.The effect of this action is that the nut on hub is tight and not loose.jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack.It requires the agent to have a jack and for the hub to be on the ground.After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack.It requires the agent to have the hub off the ground.After performing this action, the hub will be back on the ground and the agent will have the jack.undo <nut> <hub>: This action undo the fastening of a nut on a hub.The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose.The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.do-up <nut> <hub>: This action fasten a nut on a hub.The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened.The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut.remove-wheel <wheel> <hub>: This action removes a wheel from a hub.It can only be performed if the hub is not on the ground, the wheel is currently on the hub, and the hub is unfastened.After the action is performed, the agent will have the removed wheel and the hub will be free, meaning that the wheel is no longer on the hub.</p>
<p>put-on-wheel <wheel> <hub>: This action puts a wheel onto a hub.It can only be performed if the agent has the wheel, the hub is free, the hub is unfastened, and the hub is not on the ground.After the action is performed, the wheel will be on the hub, the hub will no longer be free, and the agent will no longer have the wheel.inflate <wheel>: This action inflates a wheel using a pump.It can only be performed if the agent has a pump, the wheel is not inflated, and the wheel is intact.After the action is performed, the wheel will be inflated.</p>
<p>Note: A subgoal is a milestone goal that you need to complete in order to achieve the final goal.When there is an unfinished subgoal, you need to ground the given subgoal to corresponding executable actions for solving the given task in the following format: "Action: action".When there is no current subgoal or you believe the previous subgoal has been completed (based on past actions and observations), you need to output the next subgoal to be completed and its first action in the following format: "Subgoal: subgoal Action: action".You cannot output two subgoals consecutively.Detailed trajectory information (action-observation pair) of previously satisfied subgoals will be hidden for context efficiency.If you believe that the detailed trajectory information of a particular subgoal is crucial for the current subgoal, you can use Action: "retrieve(subgoal_id)" to obtain the detailed trajectory information.</p>
<p>Here are examples:</p>
<p>The goal is to satisfy the following conditions: w1 is in boot.(Note you need to open boot first so that you can extract tools from it.)</p>
<p>Observation: Boot is closed.Boot is unlocked.Hub the-hub1 is fastened.Hub the-hub1 is on the ground.Jack is in boot.Pump is in boot.R1 is in boot.The nut nuts1 on the hub the-hub1 is tight.Wheel r1 is intact.Wheel r1 is not inflated.Wheel w1 is on hub the-hub1.Wrench is in boot.</p>
<p>Figure 2 :
2
Figure 2: An overview of the process of HIAGENT.</p>
<p>3 .
3
Do not output anything except whether summary and subgoal are met.Your output should be only one line.Do not output things like '##Summary', '#</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Progress rate at different steps.</p>
<p>(Hausknecht et al. 2020periments on five longhorizon agent tasks, which typically require more than 20 steps: (i) Blocksworld requires the model to arrange the blocks into a specified target configuration by executing a series of moves; (ii) Gripper involves moving objects between different rooms; (iii) Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire; (iv) Barman emulates a bartender's tasks in mixing cocktails, including combining various ingredients, shakers, and garnishing drinks; (v) Jericho(Hausknecht et al. 2020) is a suite of text-based adventure game environments designed to evaluate agents' ability to navigate and interact with fictional worlds.More details can be found in Appendix A.
4 Experiments4.1 Experimental Setup
(Ma et al. 2024)tion Metrics We use multiple metrics to evaluate both the effectiveness and efficiency of LLM-based agents in solving long-horizon tasks: (i) Progress Rate(Ma et al. 2024)evaluates the advancement toward task completion.Specifically, a task consists of multiple goal conditions, and the progress rate is the proportion of goal conditions fulfilled by the model out of the total number of goal conditions.(ii) Success Rate measures the percentage of successful task completions.The success rate is 1 when the progress rate is 1. (iii) Average Steps counts the steps taken to complete the task; (iv) Context Efficiency is defined as the mean number of tokens in the in-trial context across all steps required to complete a given task.(v) Run Time evaluates the time required to complete tasks.</p>
<p>Table 1 ,
1
HIAGENT demonstrated substantial advancements over STANDARD.Overall, in terms of effectiveness, it increased the success rate by 21% and the progress rate by 23.94%.Regarding task execution efficiency, it reduced the average number of steps to completion by 3.8, decreased the number of context tokens consumed by 35%, and reduced the run time by 19.42%.Furthermore, in certain tasks (blocksworld, barman, jericho), HIAGENT even achieved more than double the progress rate improvement while maintaining efficiency.In tyreworld, the model not only achieved a 50% improvement in success rate but also reduced the average number of steps by 9.4.Although the progress rate slightly decreased by 1.5% in the gripper task, context token usage was reduced by over 50%.</p>
<p>Table 1 :
1
Performance of STANDARD and HIAGENT on 5 long-horizon agent tasks.We report on four metrics: Success Rate (SR), Progress Rate (PR), Average Steps (Steps), and Context Efficiency (Context), Run Time (Time).The symbol ↑ indicates that a higher value for the metric is preferable, while ↓ signifies that a lower value is considered better.In the Overall section, the result is obtained by averaging the values of a certain metric across various tasks.+21.00 62.55 +23.94 22.61 -3.80 64.98% -35.02% 80.58% -19.42% to validate the functionality and effectiveness of the combined Observation Summarization and Trajectory Retrieval modules.As shown in Table2("w/o OS &amp; TR"), there is a noticeable performance decline compared to HIAGENT, with the success rate decreasing by 20%.This decline is also evident when compared to the individual ablations of the Observation Summarization and Trajectory Retrieval modules, highlighting a substantial reduction in progress rate in their absence.
SR ↑PR ↑Steps ↓Context ↓Time ↓BlocksworldSTANDARD 30.0035.0025.00100%100%HIAGENT60.00 +30.00 80.00 +45.00 18.60 -6.40 67.46% -32.54% 63.47% -36.53%GripperSTANDARD 50.0087.7525.20100%100%HIAGENT50.00 +0.0086.25 -1.5024.80 -0.40 49.99% -50.01% 70.46% -29.54%TyreworldSTANDARD 10.0039.2828.40100%100%HIAGENT60.00 +50.00 75.83 +36.55 19.00 -9.473.58% -26.42% 77.58% -22.42%BarmanSTANDARD 10.0017.5026.85100%100%HIAGENT30.00 +20.00 40.83 +23.33 24.5 -2.3567.02% -32.98% 95.54% -4.46%JerichoSTANDARD 5.0013.5126.60100%100%HIAGENT10.00 +5.0029.85 +16.34 26.15 -0.45 66.86% -33.14% 95.85% -4.15%OverallSTANDARD 21.0038.6126.41100%100%HIAGENT 42.00 5.2 Answer 2: HIAGENT is consistently superiorto STANDARD at different steps5.3 Answer 3: The improvement in HIAGENT isnot solely attributed to task decomposition
(Zhou et al. 2022;Yin et al. 2023as been employed in numerous studies and has demonstrated considerable per-formance advantages(Zhou et al. 2022;Yin et al. 2023).</p>
<p>Table 2 :
2
Ablation study of HIAGENT on tyreworld."w/o OS" refers to removing the Observation Summarization module introduced by Section 3.3."w/o TR" refers to removing the Trajectory Retrieval module introduced by Section 3.4."w/o TR &amp; OS" refers to removing both modules.
ModelSR ↑PR ↑Steps ↓Context ↓Time ↓HIAGENT60.075.819.0100.0%100.0%w/o OS30.0 -30.0 68.2 -7.624.2 +5.2 110.8% +10.8% 122.5% +22.5%w/o TR50.0 -10.0 76.9 +1.1 21.2 +2.2 105.0% +5.0%107.5% +7.5%w/o OS &amp; TR 30.0 -30.0 62.4 -13.4 26.2 +7.2 107.2% +7.2%121.2% +21.2%</p>
<p>Table 3 :
3
Experimental results on tyreworld."w.TD" refers to Task Decomposition, i.e., having the LLM generate subgoals without concealing detailed trajectory information of previous subgoals.
ModelSR ↑PR ↑Steps ↓Context ↓Time ↓STANDARD10.039.328.4100%100%w. TD40.0 +30.0 67.4 +28.1 22.8 -5.6 112.8% +12.8% 105.7% +5.7%w. HIAGENT 60.0 +50.0 75.8 +36.5 19.0 -9.4 73.6% -26.4%77.6% -22.4%Additionally, lots of works explored the application of LLM-based agents in the field of multi-agent systems (Hong et al.2023; Zhang et al. 2023a; Wu et al. 2023; Li et al. 2023a;Chen et al. 2023). This paper introduces a working memorymanagement framework HIAGENT that can be universally ap-plied to enhance the performance of other agent frameworks.For example, ReAct (Yao et al. 2022b) introduces a methodwhere the LLM generates a chain of thought (Wei et al. 2022)before generating actions, and the trajectory formed by thetriplet of "(thought, action, observation)" can be managedusing HIAGENT. Additionally, HIAGENT has the potential toalleviate information management challenges in multi-agentframeworks (Hong et al. 2023).po-tential solutions, and achieving a desired goal. This cognitiveability is fundamental to human-level intelligence and hasbeen a focal point of research in various domains, includ-ing robotics (Hu et al. 2023b; Huang et al. 2022a; Singhet al. 2023; Brohan et al. 2023; Valmeekam et al. 2024; Puiget al. 2018), travel planning (Xie et al. 2024), warehouse-level coding (Bairi et al. 2024), tool use (Liu et al. 2024b)and so on. Least-to-most (Zhou et al. 2022) and Plan-and-solve (Wang et al. 2023a) propose decomposing a complexquestion into a series of sub-questions. However, when an-swering each sub-question, it inputs all previous answersinto the LLM, leading to context inefficiency. Lumos (Yinet al. 2023) and XAgent (Team 2023) introduce an indepen-dent planning module for generating subgoals and use fullcontext in the grounding module to complete each subgoal.
(Yao et al. 2024;Zhang et al. 2023b;Xu et al. 2023;gence, representing a systematic approach to achieving gHuang et al. , 2022b;;Liu et al. 2023a;Guan et al. 2023;Zhao, Lee, and Hsu 2024;Ruan et al. 2023;Aghzal, Plaku, and Yao 2023) 2023d; Huang et al. 2023Huang et al. , 2022b;;Liu et al. 2023a;Guan et al. 2023;Zhao, Lee, and Hsu 2024;Ruan et al. 2023;Aghzal, Plaku, and Yao 2023).It involves breaking down complex tasks into manageable sub-tasks, searching for</p>
<p>Shaker1 is on the table.Shot1 is clean.Shot1 is empty.Shot1 is on the table.Shot2 is clean.Shot2 is empty.Shot2 is on the table.Shot3 is clean.Shot3 is empty.Shot3 is on the table.Shot4 is clean.Shot4 is empty.Shot4 is on the table.
Cocktail1 part1 ingredient is ingredient1. Cocktail1 part2ingredient is ingredient3. Cocktail2 part1 ingredient is ingre-dient2. Cocktail2 part2 ingredient is ingredient3. Cocktail3part1 ingredient is ingredient1. Cocktail3 part2 ingredient isingredient2. Dispenser1 dispenses ingredient1. Dispenser2dispenses ingredient2. Dispenser3 dispenses ingredient3.Left hand is empty. Level l0 is next to level l1. Level l1 isnext to level l2. Right hand is empty. Shaker1 is at emptylevel l0. Shaker1 is at level l0. Shaker1 is clean. Shaker1 isempty. Action exampleright grasp shot1.A.5 JerichoAction List1. Inventory: check things you are carrying2. Look: check your surroundings3. Examine <place/obj>: check the details of something4. Take <obj>: pickup obj5. Put down <obj>: leave a obj at your current place.6. Drop <obj>7. Check valid actions: Check actions you can use8. South: go south9. North: go north10. East: go east11. West: go west12. Up: go up13. Down: go down14. Check valid actions (Other available actions)</p>
<p>The precondition for this action is that the container is open.The effect of this action is that the container is closed and not open.fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open.The effect of this action is that the object is held by the agent and not inside the container.
B Prompt ExamplesB.1 STANDARDEnvironment ImplementationYour goal is to replace flat tyres with intact tyres onthe hubs. Remember to open boot first to get toolsyou need. Intact tyres should be inflated. The nutsshould be tight on the hubs. The flat tyres, wrench,jack, and pump should be in the boot. The bootshould be closed.There are 13 actions defined in this domain:open <container>: The precondition for this action isthat the container is unlocked and closed. The effectof this action is that the container is open and notclosed.close <container>:Examine barrel</p>
<p>(Note you need to open boot first so that you can extract tools from it.)The precondition for this action is that the container is open.The effect of this action is that the container is closed and not open.fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open.The effect of this action is that the object is held by the agent and not inside the container.
of this action is that the container is open and notclosed.close <container>:Observation: Boot is closed. Boot is unlocked.Hub the-hub1 is fastened. Hub the-hub1 is on theground. Jack is in boot. Pump is in boot. R1 is inboot. The nut nuts1 on the hub the-hub1 is tight.Wheel r1 is intact. Wheel r1 is not inflated. Wheelw1 is on hub the-hub1. Wrench is in boot.B.2 HIAGENTEnvironment ImplementationYour goal is to replace flat tyres with intact tyres onthe hubs. Remember to open boot first to get toolsyou need. Intact tyres should be inflated. The nutsshould be tight on the hubs. The flat tyres, wrench,jack, and pump should be in the boot. The bootshould be closed.There are 13 actions defined in this domain:open <container>: The precondition for this action isthat the container is unlocked and closed. The effect
We utilized the model via OpenAI API service.
To conduct a more granular study of HIAGENT's performance, we present the progress rate at different step counts (in intervals of 5 steps) in Figure3. The experimental results indicate that overall, HIAGENT consistently achieves a higher progress rate at each step than STANDARD (f). Additionally, it is noteworthy that HIAGENT benefits more from an increased number of steps, whereas STANDARD does not. For example, in the blocksworld task (a) and barman task (b), STANDARD shows no progress rate increase between steps 15-25, whereas HIAGENT exhibits continuous growth. This further demonstrates HIAGENT's advantage in handling long-horizon agent tasks.
Action: check valid actions.Observation: Valid actions are: Open boot.Check valid actions.Look around.Action: Open boot.Observation: Boot is open.Action: fetch wrench from boot Observation: You have wrench.Action: check valid actions Observation: Valid actions are: Fetch pump from boot., Fetch r1 from boot., Put-away wrench in boot., Close boot., Fetch jack from boot., Loosen the nut nuts1 on the hub the-hub1., check valid actions Action: Loosen the nut nuts1 on the hub the-hub1.Observation: The nut nuts1 on the hub the-hub1 is loose.Action: fetch jack from boot Observation: You have jack.Action: jack-up the hub the-hub1 Observation: Hub the-hub1 is not on the ground.Action: Undo the fastening of the nut nuts1 on the hub the-hub1.Observation: Hub the-hub1 is unfastened.Action: Remove-wheel w1 from the hub the-hub1.Observation: You have w1.Action: Put-away w1 in boot Observation: W1 is in boot.Goal is completed.You should perform actions to accomplish the goal: The goal is to satisfy the following conditions: Wheel r1 is inflated., r1 is on the-hub1., w1 is in boot.You should use the following commands for help when your action cannot be understood: check valid actions Observation: Boot is closed.Boot is unlocked.Hub the-hub1 is fastened.Hub the-hub1 is on the ground.Jack is in boot.Pump is in boot.R1 is in boot.The nut nuts1 on the hub the-hub1 is tight.W1 is on the-hub1.Wheel r1 is intact.Wheel r1 is not inflated.Wrench is in boot.Action: Open boot.Observation: Boot is open.Action: jack-up the-hub1 Observation: The action is not valid and therefore takes no effect.Please check valid actions.Action: jack-up the hub the-hub1 Observation: The action is not valid and therefore takes no effect.Please check valid actions.Action: Loosen the nut nuts1 on the hub the-hub1.Observation: The action is not valid and therefore takes no effect.Please check valid actions.Action: Loosen the nut nuts1 on the hub the-hub1.Observation: The action is not valid and therefore takes no effect.Please check valid actions.Action: Loosen the nut nuts1 on the hub the-hub1.Observation: The action is not valid and therefore takes no effect.Please check valid actions.Action: Fetch wrench from boot.Observation: You have wrench.Reproducibility Checklist1.This paper:(a) Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes) (b) Clearly delineates statements that are opinions, hypotheses, and speculations from objective facts and results (yes) (c) Provides well-marked pedagogical references for lessfamiliar readers to gain background necessary to replicate the paper (yes)2. Does this paper make theoretical contributions?(no) 3. Does this paper rely on one or more datasets?(yes) (a) A motivation is given for why the experiments are conducted on the selected datasets (yes) (b) All novel datasets introduced in this paper are included in a data appendix.(NA) (c) All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes.After the unstack action, the arm will be holding the top block, the top block will no longer be on top of the bottom block, and the bottom block will be clear.Goal example b1 is on b2., b2 is on b3.Observation example b1 is on the table.b2 is on the table.B3 is on the table.Robot arm is empty.The b1 is clear.The b2 is clear.The b3 is clear.Action example pickup b2.A.2 GripperAction List 1. move <room1> <room2>: This action allows the robot to move from one room to another.The action has a single precondition, which is that the robot is currently in a room.The effect of this action is to move the robot to another room and to remove the fact that it is in the original room.2. pick <obj> <room> <gripper>: This action allows the robot to pick up an object using the gripper.The action has three preconditions: (1) the object is located in a room (2) the robot is currently in the same room and (3) the gripper is free (i.e., not holding any object).The effect of this action is to update the state of the world to show that the robot is carrying the object using the gripper, the object is no longer in the room, and the gripper is no longer free.3. drop <obj> <room> <gripper>: This action allows the robot to drop an object that it is carrying.The action has two preconditions: (1) the robot is currently carrying the object using the gripper, and (2) the robot is currently in a room.The effect of this action is to update the state of the world to show that the robot is no longer carrying the object using the gripper, the object is now located in the room, and the gripper is now free.Goal example ball1 is at roomb., ball2 is at roomb., ball3 is at roomb., ball4 is at room.Observation exampleBall1 is a ball.Ball1 is carrying right.Ball2 is a ball.Ball2 is at rooma.Ball3 is a ball.Ball3 is at rooma.Ball4 is a ball.Ball4 is at rooma.Left is a gripper.Left is free.Right is a gripper.Robby is at rooma.Room rooma Room roomb.Action examplePick up ball1 at rooma with arm right.A.3 Tyreworld
Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. M Aghzal, E Plaku, Z Yao, arXiv:2310.032492023arXiv preprint</p>
<p>Toward Conversational Agents with Context and Time Sensitive Long-term Memory. N Alonso, T Figliolia, A Ndirango, B Millidge, C An, S Gong, M Zhong, M Li, J Zhang, L Kong, X Qiu, arXiv:2406.00057arXiv:2307.11088L-eval: Instituting standardized evaluation for long context language models. 2024. 2023arXiv preprint</p>
<p>The architecture of cognition. J R Anderson, 2013Psychology Press</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. R Bairi, A Sonwane, A Kanade, A Iyer, S Parthasarathy, S Rajamani, B Ashok, S Shet, I Beltagy, M E Peters, A Cohan, A Bertsch, U Alon, G Neubig, M Gormley, A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, arXiv:2004.05150Advances in Neural Information Processing Systems. PMLR2024. 2020. 2024. 20231arXiv preprintConference on robot learning</p>
<p>N Chen, H Li, J Huang, B Wang, J Li, arXiv:2402.11975Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations. 2024arXiv preprint</p>
<p>W Chen, Y Su, J Zuo, C Yang, C Yuan, C Qian, C.-M Chan, Y Qin, Y Lu, R Xie, arXiv:2308.10848Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. 2023arXiv preprint</p>
<p>Adapting language models to compress contexts. A Chevalier, A Wettig, A Ajith, D Chen, M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, arXiv:2305.14788arXiv:1810.08272Babyai: A platform to study the sample efficiency of grounded language learning. 2023. 2018arXiv preprint</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. J Guo, N Li, J Qi, H Yang, R Li, Y Feng, S Zhang, M Xu, S Hao, T Liu, Z Wang, Z Hu, arXiv:2312.17259Advances in neural information processing systems. 2023. 202436arXiv preprintEmpowering Working Memory for Large Language Model Agents</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>S Hong, X Zheng, J Chen, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, arXiv:2308.00352Metagpt: Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. C Hu, J Fu, C Du, S Luo, J Zhao, H Zhao, M Hu, Y Mu, X Yu, M Ding, S Wu, W Shao, Q Chen, B Wang, Y Qiao, P Luo, W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2306.03901arXiv:2310.08582Tree-planner: Efficient close-loop task planning with large language models. PMLR2023a. 2023b. 2022aarXiv preprintChatdb: Augmenting llms with databases as their symbolic memory. International conference on machine learning</p>
<p>Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control. W Huang, F Xia, D Shah, D Driess, A Zeng, Y Lu, P Florence, I Mordatch, S Levine, K Hausman, B Ichter, arXiv:2303.008552023</p>
<p>W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, P Sermanet, N Brown, T Jackson, L Luu, S Levine, K Hausman, B Ichter, G Lample, L Saulnier, arXiv:2207.05608arXiv:2310.06825Inner Monologue: Embodied Reasoning through Planning with Language Models. A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, Lengyel, G2022b. 2023arXiv preprintMistral 7B</p>
<p>Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. T Kagaya, T J Yuan, Y Lou, J Karlekar, S Pranata, A Kinose, K Oguri, F Wick, Y You, arXiv:2402.036102024arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. H Kang, C Xiong, J Lanchantin, S Toshniwal, J Weston, S Sukhbaatar, arXiv:2406.10291arXiv:2406.01623ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents. G Li, H Hammoud, H Itani, D Khizbullin, B Ghanem, 2024. 2024. 202436arXiv preprintLearning to reason and memorize with self-notes</p>
<p>M Li, Y Zhao, B Yu, F Song, H Li, H Yu, Z Li, F Huang, Y Li, arXiv:2304.08244Api-bank: A comprehensive benchmark for tool-augmented llms. 2023barXiv preprint</p>
<p>Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. X Liang, B Wang, H Huang, S Wu, P Wu, L Lu, Z Ma, Z Li, arXiv-23042023arXiv e-prints</p>
<p>Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system. X V Lin, C Wang, L Zettlemoyer, M D Ernst, arXiv:1802.089792018arXiv preprint</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>L Liu, X Yang, Y Shen, B Hu, Z Zhang, J Gu, G Zhang, arXiv:2311.08719Think-in-memory: Recalling and postthinking enable llms with long-term memory. 2023barXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, Transactions of the Association for Computational Linguistics. 122024a</p>
<p>X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, arXiv:2308.03688arXiv:2406.03807Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering. 2023c. 2024barXiv preprintAgentbench: Evaluating llms as agents</p>
<p>C Ma, J Zhang, Z Zhu, C Yang, Y Yang, Y Jin, Z Lan, L Kong, J He, arXiv:2401.13178AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. 2024arXiv preprint</p>
<p>A Maharana, D.-H Lee, S Tulyakov, M Bansal, F Barbieri, Y Fang, arXiv:2402.17753Evaluating very long-term conversational memory of llm agents. 2024arXiv preprint</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. B P Majumder, B D Mishra, P Jansen, O Tafjord, N Tandon, L Zhang, C Callison-Burch, P Clark, arXiv:2310.101342023arXiv preprint</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. A I Meta, 2024</p>
<p>The magical number seven, plus or minus two: Some limits on our capacity for processing information. G A Miller, Psychological review. 632811956</p>
<p>RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis. Y Mu, J Chen, Q Zhang, S Chen, Q Yu, C Ge, R Chen, Z Liang, M Hu, C Tao, arXiv:2402.161172024aarXiv preprint</p>
<p>Embodiedgpt: Vision-language pre-training via embodied chain of thought. Y Mu, Q Zhang, M Hu, W Wang, M Ding, J Jin, B Wang, J Dai, Y Qiao, P Luo, A Newell, H A Simon, arXiv:2303.08774arXiv:2406.12373WebCanvas: Benchmarking Web Agents in Online Environments. Prentice-hall Englewood Cliffs2024b. 1972. 2022. 202436arXiv preprintAdvances in Neural Information Processing Systems</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Virtualhome: Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Randomized positional encodings boost length generalization of transformers. J Ruan, Y Chen, B Zhang, Z Xu, T Bao, G Du, S Shi, H Mao, X Zeng, R Zhao, A Ruoss, G Delétang, T Genewein, J Grau-Moya, R Csordás, M Bennani, S Legg, J Veness, N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, arXiv:2308.03427arXiv:2305.16843Advances in Neural Information Processing Systems. 2023. 2023. 202436arXiv preprintReflexion: Language agents with verbal reinforcement learning</p>
<p>M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, C H Wu, J Washington, C Sadler, B M Chao, W.-L Su, Y , arXiv:2212.04088LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. 2023. 2023. 2023Team, X. 2023. XAgent: An Autonomous Agent for Complex Task Solving</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288arXiv:2304.14106Chatlog: Recording and analyzing chatgpt across time. 2023. 2023arXiv preprint</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. K Valmeekam, M Marquez, A Olmo, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 202436</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R K Lee, .-W Lim, E.-P ; Wang, Z Liu, J Chen, Y Yuan, L Peng, H Ji, H , arXiv:2305.04091arXiv:2309.10691Mint: Evaluating llms in multi-turn interaction with tools and language feedback. Wang, X2023a. 2023barXiv preprint</p>
<p>Jarvis-1: Openworld multi-task agents with memory-augmented multimodal language models. Z Wang, S Cai, A Liu, Y Jin, J Hou, B Zhang, H Lin, Z He, Z Zheng, Y Yang, arXiv:2311.05997arXiv:2302.015602023d. Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, 2023carXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>R F Woolson, Wilcoxon signed-rank test. Encyclopedia of Biostatistics. 20058</p>
<p>M Wu, T Zhu, H Han, C Tan, X Zhang, W Chen, arXiv:2405.08355Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark. 2024arXiv preprint</p>
<p>Q Wu, G Bansal, J Zhang, Y Wu, S Zhang, E Zhu, B Li, L Jiang, X Zhang, C Wang, arXiv:2308.08155Autogen: Enabling next-gen llm applications via multi-agent conversation framework. 2023arXiv preprint</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. C Xiao, P Zhang, X Han, G Xiao, Y Lin, Z Zhang, Z Liu, S Han, M Sun, arXiv:2402.046172024arXiv preprint</p>
<p>J Xie, K Zhang, J Chen, T Zhu, R Lou, Y Tian, Y Xiao, Y Su, arXiv:2402.01622Travelplanner: A benchmark for real-world planning with language agents. 2024arXiv preprint</p>
<p>T Xie, F Zhou, Z Cheng, P Shi, L Weng, Y Liu, T J Hua, J Zhao, Q Liu, C Liu, arXiv:2310.10634Openagents: An open platform for language agents in the wild. 2023arXiv preprint</p>
<p>Rewoo: Decoupling reasoning from observations for efficient augmented language models. B Xu, Z Peng, B Lei, S Mukherjee, Y Liu, D Xu, arXiv:2305.183232023arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yang, B Zhao, C Xie, S Yao, H Chen, J Yang, K Narasimhan, arXiv:2402.09404AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability. 2024. 2022a35arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, W Yao, S Heinecke, J C Niebles, Z Liu, Y Feng, L Xue, R Murthy, Z Chen, J Zhang, D Arpit, arXiv:2210.03629arXiv:2311.05657Retroformer: Retrospective large language agents with policy gradient optimization. 2022b. 2023. 2023arXiv preprintLumos: Learning agents with unified data, modular design, and open-source llms</p>
<p>Evolving Large Language Model Assistant with Long-Term Conditional Memory. R Yuan, S Sun, Z Wang, Z Cao, W Li, H Zhang, W Du, J Shan, Q Zhou, Y Du, J B Tenenbaum, T Shu, C Gan, arXiv:2312.17257arXiv:2307.02485Building cooperative embodied agents modularly with large language models. 2023. 2023aarXiv preprint</p>
<p>Reasoning over hierarchical question decomposition tree for explainable question answering. J Zhang, S Cao, T Zhang, X Lv, J Shi, Q Tian, J Li, L Hou, arXiv:2305.150562023barXiv preprint</p>
<p>Z Zhang, X Bo, C Ma, R Li, X Chen, Q Dai, J Zhu, Z Dong, J.-R Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Z Zhao, W S Lee, D Hsu, W Zhong, L Guo, Q Gao, H Ye, Y Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 202436Advances in Neural Information Processing Systems</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. A Zhou, K Yan, M Shlapentokh-Rothman, H Wang, Y.-X Wang, arXiv:2310.044062023aarXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.10625arXiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2022. 2023barXiv preprint</p>
<p>Action: Put-away w1 in boot Observation: W1 is in boot. Goal is completed. You should perform actions to accomplish the goal: The goal is to satisfy the following conditions: Wheel r1 is inflated., r1 is on the-hub1., w1 is in boot. You should use the following commands for help when your action cannot be understood: check valid actions Observation: Boot is closed. W Zhou, Y E Jiang, P Cui, T Wang, Z Xiao, Y Hou, R Cotterell, M Sachan, arXiv:2305.13304.1Subgoal is met. 6 Subgoal: Unfasten the nut nuts1 and remove wheel w1. 2023carXiv preprintFetch jack from boot., Loosen the nut nuts1 on the hub the-hub1., check valid actions Action: Loosen the nut nuts on the hub the-hub1. Observation: The nut nuts1 on the hub the-hub1 is loose. Subgoal is met. 5 Subgoal: Fetch the jack from the boot and jack up the hub. Action: fetch jack from boot Observation: You have jack. Boot is unlocked. Hub the-hub1 is fastened. Hub the-hub1 is on the ground. Jack is in boot. Pump is in boot. R1 is in boot. The nut nuts1 on the hub the-hub1 is tight. W1 is on the-hub1. Wheel r1 is intact. Wheel r1 is not inflated. Wrench is in boot. 1 Subgoal: Open the boot to access the tools. Observation: Boot is open. 2 Subgoal: Fetch the wrench from the boot. Observation: You have wrench</p>
<p>Loosen the nut on the hub the-hub1. Observation: The nut nuts1 on the hub the-hub1 is loose. Subgoal, </p>
<p>Fetch the jack from the boot and jack up the hub the-hub1. Observation: You have jack and hub the-hub1 is elevated. Subgoal, Subgoal is met</p>
<p>Action: undo nuts1 on the-hub1 Observation: Hub the-hub1 is unfastened. You have nuts1. Action: Remove-wheel w1 from the-hub1 Observation: The-hub1 is free. Subgoal, 1Unfasten the nut nuts1 and remove wheel w1 from the hub the-hub1</p>            </div>
        </div>

    </div>
</body>
</html>
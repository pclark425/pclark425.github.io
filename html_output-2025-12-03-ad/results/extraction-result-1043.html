<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1043 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1043</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1043</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-258309175</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.12877v1.pdf" target="_blank">Proximal Curriculum for Reinforcement Learning Agents</a></p>
                <p><strong>Paper Abstract:</strong> We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1043.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1043.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointMass-s (PPO + ProCuRL-val)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PointMass sparse-reward environment trained with PPO and Proximal Curriculum (ProCuRL-val)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 2D point-mass navigation environment with sparse (binary) goal reward and a 3-dimensional context controlling gate position, gate width, and friction; trained using PPO with the ProCuRL-val curriculum that uses critic value estimates to select tasks near the agent's Zone of Proximal Development.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL agent trained with Proximal Policy Optimization (PPO). ProCuRL-val selects tasks by computing V_t(s)*(1 - V_t(s)) from the agent's critic (normalized to [0,1]) and sampling with a softmax temperature β.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointMass-s (sparse)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D point-mass navigation task where the agent must move through a gate to a goal; reward is binary (1 for reaching goal, 0 otherwise). Context vector c ∈ R^3 controls C-GatePosition, C-GateWidth, and C-Friction. A pool of 100 tasks was sampled uniformly over context space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dim = R^4, action dim = R^2, context dim = 3 (gate position, gate width, friction), pool size = 100, reward sparsity (binary goal reward).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (continuous state/action, sparse reward, contextual variation across 100 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of task instances in pool (100) and continuous context variation across R^3 (gate position, gate width, friction); tasks sampled uniformly from context space.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean reward on the training pool (uniform performance), reported as expected mean episodic reward / success probability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProCuRL-val: mean reward ≈ 0.71 ± 0.18 at 1M training steps (reported in ablation table for PointMass-s, γ2/γ1=1.0). In a train/test summary ProCuRL-val: train 0.71, test 0.65 (numbers reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper does not present a formal empirical law relating environment complexity to variation; instead it focuses on curricula that adapt to task difficulty (probability-of-success) within a varied task pool. It reports that ProCuRL-val shifts selection gradually toward harder contexts (e.g., gates closer to edges, narrower gates) as training progresses, illustrating an interaction where curriculum progression increases difficulty while operating over a fixed variation pool.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (ProCuRL-val; critic-value based soft selection), compared to IID sampling and other curricula (SPDL, SPaCE, PLR); training optimizer: PPO, 1M environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>A train/test comparison was reported: ProCuRL-val achieved train ≈ 0.71 and test ≈ 0.65 on a held-out PointMass-s test set (100 tasks), indicating reasonable generalization from the training pool to the test pool.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training runs used 1,000,000 environment steps; ProCuRL-val has similar sample complexity to state-of-the-art baselines (does not require additional teacher rollouts), whereas ProCuRL-env required additional environment steps for task evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ProCuRL-val accelerates learning compared to baselines in PointMass-s by focusing training on tasks with intermediate success probability (PoS_t*(1-PoS_t) proxy using critic), the curriculum shifts to progressively harder contexts, and ProCuRL-val achieves competitive train/test performance while remaining sample-efficient (no extra teacher rollouts).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1043.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1043.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointMass-d (PPO + ProCuRL-val)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PointMass dense-reward environment trained with PPO and ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the PointMass environment with dense reward (squared-exponential decay with distance to goal) and the same 3-dimensional context; trained with PPO and the ProCuRL-val curriculum using normalized critic values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep RL agent trained with PPO; ProCuRL-val uses the critic V_t(s) normalized to [0,1] to compute selection weights V_t(s)*(1-V_t(s)).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>PointMass-d (dense)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same 2D point-mass navigation but with dense reward that decays with squared distance to goal; contexts control gate position, gate width, friction. Pool size = 100 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dim = R^4, action dim = R^2, context dim = 3, pool size = 100, dense reward shaping (continuous reward function).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (dense reward reduces sparsity but continuous dynamics remain)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Pool size 100, continuous context variation in R^3 (same contextual params as PointMass-s).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean reward on the training pool (expected episodic return)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper shows ProCuRL methods outperform or match baselines in PointMass-d in convergence plots; explicit numeric 1M-step ProCuRL-val value not isolated in main text tables as clearly as PointMass-s, but convergence plots indicate improved learning speed and final reward compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors show curricula generated by ProCuRL shift selection across context dimensions (e.g., gate width/position) as learning progresses; they emphasize that curriculum can handle both sparse and dense reward variants by normalizing critic values. No explicit quantitative law linking complexity to variation is given.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (ProCuRL-val with critic normalization), PPO optimizer, 1M training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Similar sample complexity to baselines (ProCuRL-val does not require extra teacher rollouts); training experiments used 1M environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ProCuRL-val is applicable to dense-reward settings after critic value normalization; it retains the proximal (neither-too-hard nor-too-easy) selection principle and shows empirical speedups over baselines in PointMass-d experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1043.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1043.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BasicKarel (PPO + ProCuRL-val)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BasicKarel program-synthesis inspired environment trained with PPO and ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete, symbolic grid-manipulation environment inspired by Karel where each task is a (initial, final) grid configuration pair; rewards are goal-based (binary) and the pool contains 24,000 tasks; trained with PPO and ProCuRL-val curriculum using critic estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent using PPO training on a discrete action set {move, turnLeft, turnRight, pickMarker, putMarker, finish}; curriculum selection uses critic-derived normalized V_t(s)*(1-V_t(s)).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated/virtual agent (symbolic grid world)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BasicKarel</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Episodic grid-world tasks (4x4 grids) where the agent must transform an initial grid into a target grid using basic commands; episodes terminate on crashes or finish; reward is binary (1 for successful transformation). The training pool comprises 24,000 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Discrete state space with up to 88-dimensional encoding reported in paper (state dim = 88), action space size = 6, pool size = 24,000, solution trajectory length can vary widely; reward is sparse binary.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (large discrete task pool, combinatorial state space, long solution trajectories possible)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Large pool size = 24,000 distinct tasks (discrete contexts), variation across initial/final grid configurations and features like number of walls, distractor markers, required marker actions, trajectory length.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean reward on the training pool (expected episodic return / success rate); also reported train/test mean rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProCuRL-val achieved competitive performance: from ablation/general results ProCuRL-val mean reward ≈ 0.71 at 1M steps (ablation table shows 0.71 ± 0.05 for BasicKarel at 1M for γ2/γ1=1.0); train/test summary shows ProCuRL-val train ≈ 0.91 and test ≈ 0.90 (numbers reported in the paper summary table).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper documents that ProCuRL-val curriculum tends to select tasks that become progressively harder (e.g., longer solution trajectories, tasks requiring marker actions, more distractors and walls), demonstrating curriculum-driven navigation of the high-variation discrete task pool. The authors note trade-offs: ProCuRL-env obtains stronger performance but at the cost of many additional environment steps; ProCuRL-val offers good performance while being computationally cheaper.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (ProCuRL-val using critic V_t normalization), PPO optimizer; compared to IID, SPDL, SPaCE, PLR baselines. Training budgets reported at 1M environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported train/test summary indicates ProCuRL-val generalizes well on BasicKarel: train ≈ 0.91 and test ≈ 0.90, suggesting little overfitting and good transfer from training pool to held-out test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training runs used 1,000,000 environment steps; ProCuRL-val does not require additional teacher rollouts and so retains baseline-like sample efficiency; authors note ProCuRL-val has favorable computational complexity relative to SPDL/SPaCE for large pools because it only needs critic forward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ProCuRL-val scales to large discrete pools (24k tasks) and produces curricula that move toward harder discrete tasks; it achieves high uniform performance across the pool and good generalization, while being computationally efficient (critic-only evaluations) compared to teacher-rollout-based ProCuRL-env and heavier baselines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1043.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1043.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BallCatching (PPO + ProCuRL-val)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BallCatching robot task trained with PPO and ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated robotic catching task where an agent must move a robot to catch a thrown ball; reward is sparse but non-binary (reward for catching, penalty for excessive motion); context vector is R^3 (throw distance and goal position); trained using PPO with ProCuRL-val curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated robot controller trained with PPO; task selection via ProCuRL-val using critic-derived normalized values to prefer tasks in the proximal difficulty zone.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BallCatching</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Agent must direct a robot to catch a ball thrown from varying distances; reward is sparse/non-binary (positive for catch, penalties for excessive motion). Context c ∈ R^3 controls throw distance and goal position; pool size = 100 uniformly sampled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dim = R^21, action dim = R^5, context dim = 3, pool size = 100, sparse reward with continuous penalties, episode length unspecified but non-trivial dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (high-dimensional state and action spaces, dynamic physics, mixed sparse/dense reward components)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Pool size 100, continuous context variation across throw distance and goal positions; variation in dynamics due to different initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean reward on the training pool (expected episodic return)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports ProCuRL variants outperform or match baselines in BallCatching convergence plots; explicit numeric 1M-step ProCuRL-val value not printed clearly in main snippets, but empirical claim is improved learning speed and performance relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors note ProCuRL-val and ProCuRL-env help the agent focus on intermediate-difficulty dynamics across the varied ball-throw contexts; no explicit numeric relation between complexity and variation is provided, but the curriculum adapts selection based on critic success estimates across the variation set.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (ProCuRL-val), PPO optimizer, compared to SPDL, SPaCE, PLR and IID; pool-based contextual RL with 100 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used 1M environment steps; ProCuRL-val required no extra teacher rollouts so sample complexity comparable to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ProCuRL-val accelerates learning in a high-dimensional dynamic task with mixed reward signals by selecting contexts of intermediate difficulty; demonstrates applicability of the proximal curriculum idea to robotics-like simulated tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1043.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1043.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AntGoal (PPO + ProCuRL-val)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo AntGoal environment trained with PPO and ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MuJoCo Ant agent with a goal-conditioned task on a flat 2D plane, rewarded for moving toward a goal (goal reward increases exponentially when nearer); trained with PPO and curriculum ProCuRL-val over a pool of goal locations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO agent with ProCuRL-val</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated MuJoCo ant (continuous-control agent) trained with PPO; ProCuRL-val uses critic value estimates normalized to select mid-difficulty goal-starting contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic locomotion agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AntGoal</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>MuJoCo Ant adapted with a goal on a 2D surface; reward includes an exponential goal-distance component plus control and contact costs; context is goal location in R^2 sampled uniformly from a circle; pool size = 50; episodes length = 200 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dim ≈ R^29, action dim = R^8, context dim = 2 (goal position), pool size = 50, long episodic horizon (H=200), hybrid reward (goal + control costs).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (high-dimensional continuous control, long horizon, hybrid reward terms)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Pool size 50, continuous variation in goal location sampled from circle around ant; contextual variation limited to spatial goal locations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium (smaller pool but continuous spatial variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean reward on the training pool (episodic return)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports ProCuRL variants match or outperform baselines in AntGoal convergence plots; specific numeric values for ProCuRL-val at 1M steps are not cleanly enumerated in the provided text, but performance improvement versus baselines is claimed.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The curriculum gradually biases selection over goal contexts as agents improve; authors highlight the method's applicability to long-horizon control but do not quantify a formulaic relationship between environment complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (ProCuRL-val), PPO optimizer, 1M training steps; compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used 1M environment steps; ProCuRL-val does not add teacher rollouts and thus has similar sample complexity to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ProCuRL-val is applicable to long-horizon continuous-control tasks with hybrid rewards; it achieves competitive or better learning efficiency versus baselines while avoiding extra environment sampling required by rollout-based teacher methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1043.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1043.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforce CB-theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theoretical contextual-bandit derivation using Reinforce agent with softmax</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical analysis of a simplified independent-task setting (contextual bandits) using a Reinforce agent with a softmax policy, showing that expected learning progress for a task is proportional to PoS_t(s)*(PoS*(s)-PoS_t(s)), motivating the ProCuRL objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reinforce agent (theoretical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A policy-gradient Reinforce agent with softmax parameterization analyzed in a contextual bandit (H=1) scenario; updates follow Reinforce gradient steps and expected improvement C_t(s) is derived analytically.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>theoretical/simulated agent (contextual bandit model)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Contextual bandit (independent tasks special case)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>An MDP reduced to contextual bandits: goal g, two actions a1,a2, transition to goal with probability p_rand(s) only if a1 chosen; reward 1 only at goal. Each starting state s represents an independent task; analysis assumes independent-task learning and existence of a target parameter θ*.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Simplified: H=1, |A|=2, PoS defined as p_rand(s)*π(a1|s); complexity measured via PoS and whether PoS* =1; not a high-dimensional environment.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low (analytic toy model)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation across tasks encoded in p_rand(s) and initial PoS values; number of tasks arbitrary but analysis is per-task.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>variable (theoretical parameter p_rand(s) can vary per task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Expected improvement in the training objective C_t(s) (a proxy for learning progress), analytically derived</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Analytic expression: C_t(s) = 2 * η_t * p * (1 - p / p*). For p* = p_rand(s) and p = PoS_t(s), simplifies to C_t(s) = 2 * η_t * p * (1 - p/p*). For p* =1, C_t(s) ∝ p*(1-p). (Theorem 1 in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Direct theoretical relationship: expected improvement C_t(s) is maximized for intermediate PoS_t(s) values (neither too small nor too large), formalizing the ZPD intuition. This ties per-task 'difficulty' (PoS) to learning progress; the analysis is per-task and assumes independent tasks, so it doesn't directly quantify environment-level complexity vs variation trade-offs but gives a task-level relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Analytic study of policy-gradient updates in an independent-task contextual-bandit setting; motivates PoS-based curriculum selection.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable (theoretical derivation); learning-rate η_t appears in analytic expression.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Theorem 1 shows that, in a simplified Reinforce contextual bandit, the expected improvement from training on task s is proportional to PoS_t(s) * (PoS*(s) - PoS_t(s)), i.e., largest for intermediate PoS, providing theoretical justification for selecting tasks in the Zone of Proximal Development (ZPD).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1043.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1043.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstract performance-parameterized agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstract agent model with direct PoS parameterization (theoretical)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toy agent model that directly parameterizes its per-task performance (PoS) in Θ = [0,1]^{|S_init|} and updates PoS toward target PoS*, used to show that maximizing PoS_t*(PoS* - PoS_t) maximizes expected improvement under plausible update dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Abstract PoS-parameterized agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An abstract learner where parameter θ[s] = PoS_θ(s); update rule for a picked task increases θ[s] towards θ*[s] more on successes (factor α) than failures (factor β). Used to derive expected improvement expression.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>theoretical/abstract agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Independent-task setting (abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Each task s is independent; PoS_θ(s) is the direct performance parameter for that task; environment complexity abstracted away; serves as an analytical model to justify curriculum objective.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Not directly applicable; model abstracts complexity into scalar PoS per task and target PoS* per task.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>abstract (not quantified)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Encoded as per-task PoS* (p*) and current PoS p; variation across tasks reflected by distribution of p* and p across pool.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>abstract</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Expected improvement C_t(s) in L1-distance to θ* (proxy for learning progress)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Theorem 2: C_t(s) = α * p * (p* - p) + β * (1 - p) * (p* - p). For α=1, β=0 this reduces to p*(p* - p), matching the ProCuRL objective shape.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>At the abstract level, maximal expected improvement occurs for tasks with intermediate p (PoS) values; this formalizes the proximal (ZPD) selection principle across a varied task pool but does not map to explicit environment complexity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Analytic toy update rule reflecting policy-gradient-like asymmetry between success/failure updates; used to justify curriculum objective.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable (theoretical model); analytic expressions relate improvement to PoS parameters and update magnitudes α, β.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple abstract model yields the same proximal objective (PoS*(p* - p)) as optimal for maximizing expected per-step improvement when success updates are stronger than failure updates, providing additional theoretical support for ProCuRL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reverse Curriculum Generation for Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Automatic Goal Generation for Reinforcement Learning Agents <em>(Rating: 2)</em></li>
                <li>Self-Paced Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Self-Paced Context Evaluation for Contextual Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Prioritized Level Replay <em>(Rating: 1)</em></li>
                <li>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1043",
    "paper_id": "paper-258309175",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PointMass-s (PPO + ProCuRL-val)",
            "name_full": "PointMass sparse-reward environment trained with PPO and Proximal Curriculum (ProCuRL-val)",
            "brief_description": "A simulated 2D point-mass navigation environment with sparse (binary) goal reward and a 3-dimensional context controlling gate position, gate width, and friction; trained using PPO with the ProCuRL-val curriculum that uses critic value estimates to select tasks near the agent's Zone of Proximal Development.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with ProCuRL-val",
            "agent_description": "A deep RL agent trained with Proximal Policy Optimization (PPO). ProCuRL-val selects tasks by computing V_t(s)*(1 - V_t(s)) from the agent's critic (normalized to [0,1]) and sampling with a softmax temperature β.",
            "agent_type": "simulated agent",
            "environment_name": "PointMass-s (sparse)",
            "environment_description": "A 2D point-mass navigation task where the agent must move through a gate to a goal; reward is binary (1 for reaching goal, 0 otherwise). Context vector c ∈ R^3 controls C-GatePosition, C-GateWidth, and C-Friction. A pool of 100 tasks was sampled uniformly over context space.",
            "complexity_measure": "State dim = R^4, action dim = R^2, context dim = 3 (gate position, gate width, friction), pool size = 100, reward sparsity (binary goal reward).",
            "complexity_level": "medium (continuous state/action, sparse reward, contextual variation across 100 tasks)",
            "variation_measure": "Number of task instances in pool (100) and continuous context variation across R^3 (gate position, gate width, friction); tasks sampled uniformly from context space.",
            "variation_level": "medium",
            "performance_metric": "Mean reward on the training pool (uniform performance), reported as expected mean episodic reward / success probability",
            "performance_value": "ProCuRL-val: mean reward ≈ 0.71 ± 0.18 at 1M training steps (reported in ablation table for PointMass-s, γ2/γ1=1.0). In a train/test summary ProCuRL-val: train 0.71, test 0.65 (numbers reported in paper).",
            "complexity_variation_relationship": "The paper does not present a formal empirical law relating environment complexity to variation; instead it focuses on curricula that adapt to task difficulty (probability-of-success) within a varied task pool. It reports that ProCuRL-val shifts selection gradually toward harder contexts (e.g., gates closer to edges, narrower gates) as training progresses, illustrating an interaction where curriculum progression increases difficulty while operating over a fixed variation pool.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (ProCuRL-val; critic-value based soft selection), compared to IID sampling and other curricula (SPDL, SPaCE, PLR); training optimizer: PPO, 1M environment steps.",
            "generalization_tested": true,
            "generalization_results": "A train/test comparison was reported: ProCuRL-val achieved train ≈ 0.71 and test ≈ 0.65 on a held-out PointMass-s test set (100 tasks), indicating reasonable generalization from the training pool to the test pool.",
            "sample_efficiency": "Training runs used 1,000,000 environment steps; ProCuRL-val has similar sample complexity to state-of-the-art baselines (does not require additional teacher rollouts), whereas ProCuRL-env required additional environment steps for task evaluation.",
            "key_findings": "ProCuRL-val accelerates learning compared to baselines in PointMass-s by focusing training on tasks with intermediate success probability (PoS_t*(1-PoS_t) proxy using critic), the curriculum shifts to progressively harder contexts, and ProCuRL-val achieves competitive train/test performance while remaining sample-efficient (no extra teacher rollouts).",
            "uuid": "e1043.0"
        },
        {
            "name_short": "PointMass-d (PPO + ProCuRL-val)",
            "name_full": "PointMass dense-reward environment trained with PPO and ProCuRL-val",
            "brief_description": "A variant of the PointMass environment with dense reward (squared-exponential decay with distance to goal) and the same 3-dimensional context; trained with PPO and the ProCuRL-val curriculum using normalized critic values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with ProCuRL-val",
            "agent_description": "Deep RL agent trained with PPO; ProCuRL-val uses the critic V_t(s) normalized to [0,1] to compute selection weights V_t(s)*(1-V_t(s)).",
            "agent_type": "simulated agent",
            "environment_name": "PointMass-d (dense)",
            "environment_description": "Same 2D point-mass navigation but with dense reward that decays with squared distance to goal; contexts control gate position, gate width, friction. Pool size = 100 tasks.",
            "complexity_measure": "State dim = R^4, action dim = R^2, context dim = 3, pool size = 100, dense reward shaping (continuous reward function).",
            "complexity_level": "medium (dense reward reduces sparsity but continuous dynamics remain)",
            "variation_measure": "Pool size 100, continuous context variation in R^3 (same contextual params as PointMass-s).",
            "variation_level": "medium",
            "performance_metric": "Mean reward on the training pool (expected episodic return)",
            "performance_value": "Paper shows ProCuRL methods outperform or match baselines in PointMass-d in convergence plots; explicit numeric 1M-step ProCuRL-val value not isolated in main text tables as clearly as PointMass-s, but convergence plots indicate improved learning speed and final reward compared to baselines.",
            "complexity_variation_relationship": "Authors show curricula generated by ProCuRL shift selection across context dimensions (e.g., gate width/position) as learning progresses; they emphasize that curriculum can handle both sparse and dense reward variants by normalizing critic values. No explicit quantitative law linking complexity to variation is given.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (ProCuRL-val with critic normalization), PPO optimizer, 1M training steps.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Similar sample complexity to baselines (ProCuRL-val does not require extra teacher rollouts); training experiments used 1M environment steps.",
            "key_findings": "ProCuRL-val is applicable to dense-reward settings after critic value normalization; it retains the proximal (neither-too-hard nor-too-easy) selection principle and shows empirical speedups over baselines in PointMass-d experiments.",
            "uuid": "e1043.1"
        },
        {
            "name_short": "BasicKarel (PPO + ProCuRL-val)",
            "name_full": "BasicKarel program-synthesis inspired environment trained with PPO and ProCuRL-val",
            "brief_description": "A discrete, symbolic grid-manipulation environment inspired by Karel where each task is a (initial, final) grid configuration pair; rewards are goal-based (binary) and the pool contains 24,000 tasks; trained with PPO and ProCuRL-val curriculum using critic estimates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with ProCuRL-val",
            "agent_description": "An RL agent using PPO training on a discrete action set {move, turnLeft, turnRight, pickMarker, putMarker, finish}; curriculum selection uses critic-derived normalized V_t(s)*(1-V_t(s)).",
            "agent_type": "simulated/virtual agent (symbolic grid world)",
            "environment_name": "BasicKarel",
            "environment_description": "Episodic grid-world tasks (4x4 grids) where the agent must transform an initial grid into a target grid using basic commands; episodes terminate on crashes or finish; reward is binary (1 for successful transformation). The training pool comprises 24,000 tasks.",
            "complexity_measure": "Discrete state space with up to 88-dimensional encoding reported in paper (state dim = 88), action space size = 6, pool size = 24,000, solution trajectory length can vary widely; reward is sparse binary.",
            "complexity_level": "high (large discrete task pool, combinatorial state space, long solution trajectories possible)",
            "variation_measure": "Large pool size = 24,000 distinct tasks (discrete contexts), variation across initial/final grid configurations and features like number of walls, distractor markers, required marker actions, trajectory length.",
            "variation_level": "high",
            "performance_metric": "Mean reward on the training pool (expected episodic return / success rate); also reported train/test mean rewards.",
            "performance_value": "ProCuRL-val achieved competitive performance: from ablation/general results ProCuRL-val mean reward ≈ 0.71 at 1M steps (ablation table shows 0.71 ± 0.05 for BasicKarel at 1M for γ2/γ1=1.0); train/test summary shows ProCuRL-val train ≈ 0.91 and test ≈ 0.90 (numbers reported in the paper summary table).",
            "complexity_variation_relationship": "Paper documents that ProCuRL-val curriculum tends to select tasks that become progressively harder (e.g., longer solution trajectories, tasks requiring marker actions, more distractors and walls), demonstrating curriculum-driven navigation of the high-variation discrete task pool. The authors note trade-offs: ProCuRL-env obtains stronger performance but at the cost of many additional environment steps; ProCuRL-val offers good performance while being computationally cheaper.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (ProCuRL-val using critic V_t normalization), PPO optimizer; compared to IID, SPDL, SPaCE, PLR baselines. Training budgets reported at 1M environment steps.",
            "generalization_tested": true,
            "generalization_results": "Reported train/test summary indicates ProCuRL-val generalizes well on BasicKarel: train ≈ 0.91 and test ≈ 0.90, suggesting little overfitting and good transfer from training pool to held-out test tasks.",
            "sample_efficiency": "Training runs used 1,000,000 environment steps; ProCuRL-val does not require additional teacher rollouts and so retains baseline-like sample efficiency; authors note ProCuRL-val has favorable computational complexity relative to SPDL/SPaCE for large pools because it only needs critic forward passes.",
            "key_findings": "ProCuRL-val scales to large discrete pools (24k tasks) and produces curricula that move toward harder discrete tasks; it achieves high uniform performance across the pool and good generalization, while being computationally efficient (critic-only evaluations) compared to teacher-rollout-based ProCuRL-env and heavier baselines.",
            "uuid": "e1043.2"
        },
        {
            "name_short": "BallCatching (PPO + ProCuRL-val)",
            "name_full": "BallCatching robot task trained with PPO and ProCuRL-val",
            "brief_description": "A simulated robotic catching task where an agent must move a robot to catch a thrown ball; reward is sparse but non-binary (reward for catching, penalty for excessive motion); context vector is R^3 (throw distance and goal position); trained using PPO with ProCuRL-val curriculum.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with ProCuRL-val",
            "agent_description": "Simulated robot controller trained with PPO; task selection via ProCuRL-val using critic-derived normalized values to prefer tasks in the proximal difficulty zone.",
            "agent_type": "simulated robotic agent",
            "environment_name": "BallCatching",
            "environment_description": "Agent must direct a robot to catch a ball thrown from varying distances; reward is sparse/non-binary (positive for catch, penalties for excessive motion). Context c ∈ R^3 controls throw distance and goal position; pool size = 100 uniformly sampled tasks.",
            "complexity_measure": "State dim = R^21, action dim = R^5, context dim = 3, pool size = 100, sparse reward with continuous penalties, episode length unspecified but non-trivial dynamics.",
            "complexity_level": "high (high-dimensional state and action spaces, dynamic physics, mixed sparse/dense reward components)",
            "variation_measure": "Pool size 100, continuous context variation across throw distance and goal positions; variation in dynamics due to different initial conditions.",
            "variation_level": "medium",
            "performance_metric": "Mean reward on the training pool (expected episodic return)",
            "performance_value": "Paper reports ProCuRL variants outperform or match baselines in BallCatching convergence plots; explicit numeric 1M-step ProCuRL-val value not printed clearly in main snippets, but empirical claim is improved learning speed and performance relative to baselines.",
            "complexity_variation_relationship": "Authors note ProCuRL-val and ProCuRL-env help the agent focus on intermediate-difficulty dynamics across the varied ball-throw contexts; no explicit numeric relation between complexity and variation is provided, but the curriculum adapts selection based on critic success estimates across the variation set.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (ProCuRL-val), PPO optimizer, compared to SPDL, SPaCE, PLR and IID; pool-based contextual RL with 100 tasks.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Training used 1M environment steps; ProCuRL-val required no extra teacher rollouts so sample complexity comparable to baselines.",
            "key_findings": "ProCuRL-val accelerates learning in a high-dimensional dynamic task with mixed reward signals by selecting contexts of intermediate difficulty; demonstrates applicability of the proximal curriculum idea to robotics-like simulated tasks.",
            "uuid": "e1043.3"
        },
        {
            "name_short": "AntGoal (PPO + ProCuRL-val)",
            "name_full": "MuJoCo AntGoal environment trained with PPO and ProCuRL-val",
            "brief_description": "A MuJoCo Ant agent with a goal-conditioned task on a flat 2D plane, rewarded for moving toward a goal (goal reward increases exponentially when nearer); trained with PPO and curriculum ProCuRL-val over a pool of goal locations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PPO agent with ProCuRL-val",
            "agent_description": "Simulated MuJoCo ant (continuous-control agent) trained with PPO; ProCuRL-val uses critic value estimates normalized to select mid-difficulty goal-starting contexts.",
            "agent_type": "simulated robotic locomotion agent",
            "environment_name": "AntGoal",
            "environment_description": "MuJoCo Ant adapted with a goal on a 2D surface; reward includes an exponential goal-distance component plus control and contact costs; context is goal location in R^2 sampled uniformly from a circle; pool size = 50; episodes length = 200 steps.",
            "complexity_measure": "State dim ≈ R^29, action dim = R^8, context dim = 2 (goal position), pool size = 50, long episodic horizon (H=200), hybrid reward (goal + control costs).",
            "complexity_level": "high (high-dimensional continuous control, long horizon, hybrid reward terms)",
            "variation_measure": "Pool size 50, continuous variation in goal location sampled from circle around ant; contextual variation limited to spatial goal locations.",
            "variation_level": "low-to-medium (smaller pool but continuous spatial variation)",
            "performance_metric": "Mean reward on the training pool (episodic return)",
            "performance_value": "Paper reports ProCuRL variants match or outperform baselines in AntGoal convergence plots; specific numeric values for ProCuRL-val at 1M steps are not cleanly enumerated in the provided text, but performance improvement versus baselines is claimed.",
            "complexity_variation_relationship": "The curriculum gradually biases selection over goal contexts as agents improve; authors highlight the method's applicability to long-horizon control but do not quantify a formulaic relationship between environment complexity and variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (ProCuRL-val), PPO optimizer, 1M training steps; compared to baselines.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Training used 1M environment steps; ProCuRL-val does not add teacher rollouts and thus has similar sample complexity to baselines.",
            "key_findings": "ProCuRL-val is applicable to long-horizon continuous-control tasks with hybrid rewards; it achieves competitive or better learning efficiency versus baselines while avoiding extra environment sampling required by rollout-based teacher methods.",
            "uuid": "e1043.4"
        },
        {
            "name_short": "Reinforce CB-theory",
            "name_full": "Theoretical contextual-bandit derivation using Reinforce agent with softmax",
            "brief_description": "A theoretical analysis of a simplified independent-task setting (contextual bandits) using a Reinforce agent with a softmax policy, showing that expected learning progress for a task is proportional to PoS_t(s)*(PoS*(s)-PoS_t(s)), motivating the ProCuRL objective.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reinforce agent (theoretical)",
            "agent_description": "A policy-gradient Reinforce agent with softmax parameterization analyzed in a contextual bandit (H=1) scenario; updates follow Reinforce gradient steps and expected improvement C_t(s) is derived analytically.",
            "agent_type": "theoretical/simulated agent (contextual bandit model)",
            "environment_name": "Contextual bandit (independent tasks special case)",
            "environment_description": "An MDP reduced to contextual bandits: goal g, two actions a1,a2, transition to goal with probability p_rand(s) only if a1 chosen; reward 1 only at goal. Each starting state s represents an independent task; analysis assumes independent-task learning and existence of a target parameter θ*.",
            "complexity_measure": "Simplified: H=1, |A|=2, PoS defined as p_rand(s)*π(a1|s); complexity measured via PoS and whether PoS* =1; not a high-dimensional environment.",
            "complexity_level": "low (analytic toy model)",
            "variation_measure": "Variation across tasks encoded in p_rand(s) and initial PoS values; number of tasks arbitrary but analysis is per-task.",
            "variation_level": "variable (theoretical parameter p_rand(s) can vary per task)",
            "performance_metric": "Expected improvement in the training objective C_t(s) (a proxy for learning progress), analytically derived",
            "performance_value": "Analytic expression: C_t(s) = 2 * η_t * p * (1 - p / p*). For p* = p_rand(s) and p = PoS_t(s), simplifies to C_t(s) = 2 * η_t * p * (1 - p/p*). For p* =1, C_t(s) ∝ p*(1-p). (Theorem 1 in paper.)",
            "complexity_variation_relationship": "Direct theoretical relationship: expected improvement C_t(s) is maximized for intermediate PoS_t(s) values (neither too small nor too large), formalizing the ZPD intuition. This ties per-task 'difficulty' (PoS) to learning progress; the analysis is per-task and assumes independent tasks, so it doesn't directly quantify environment-level complexity vs variation trade-offs but gives a task-level relationship.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Analytic study of policy-gradient updates in an independent-task contextual-bandit setting; motivates PoS-based curriculum selection.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Not applicable (theoretical derivation); learning-rate η_t appears in analytic expression.",
            "key_findings": "Theorem 1 shows that, in a simplified Reinforce contextual bandit, the expected improvement from training on task s is proportional to PoS_t(s) * (PoS*(s) - PoS_t(s)), i.e., largest for intermediate PoS, providing theoretical justification for selecting tasks in the Zone of Proximal Development (ZPD).",
            "uuid": "e1043.5"
        },
        {
            "name_short": "Abstract performance-parameterized agent",
            "name_full": "Abstract agent model with direct PoS parameterization (theoretical)",
            "brief_description": "A toy agent model that directly parameterizes its per-task performance (PoS) in Θ = [0,1]^{|S_init|} and updates PoS toward target PoS*, used to show that maximizing PoS_t*(PoS* - PoS_t) maximizes expected improvement under plausible update dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Abstract PoS-parameterized agent",
            "agent_description": "An abstract learner where parameter θ[s] = PoS_θ(s); update rule for a picked task increases θ[s] towards θ*[s] more on successes (factor α) than failures (factor β). Used to derive expected improvement expression.",
            "agent_type": "theoretical/abstract agent",
            "environment_name": "Independent-task setting (abstract)",
            "environment_description": "Each task s is independent; PoS_θ(s) is the direct performance parameter for that task; environment complexity abstracted away; serves as an analytical model to justify curriculum objective.",
            "complexity_measure": "Not directly applicable; model abstracts complexity into scalar PoS per task and target PoS* per task.",
            "complexity_level": "abstract (not quantified)",
            "variation_measure": "Encoded as per-task PoS* (p*) and current PoS p; variation across tasks reflected by distribution of p* and p across pool.",
            "variation_level": "abstract",
            "performance_metric": "Expected improvement C_t(s) in L1-distance to θ* (proxy for learning progress)",
            "performance_value": "Theorem 2: C_t(s) = α * p * (p* - p) + β * (1 - p) * (p* - p). For α=1, β=0 this reduces to p*(p* - p), matching the ProCuRL objective shape.",
            "complexity_variation_relationship": "At the abstract level, maximal expected improvement occurs for tasks with intermediate p (PoS) values; this formalizes the proximal (ZPD) selection principle across a varied task pool but does not map to explicit environment complexity metrics.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Analytic toy update rule reflecting policy-gradient-like asymmetry between success/failure updates; used to justify curriculum objective.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Not applicable (theoretical model); analytic expressions relate improvement to PoS parameters and update magnitudes α, β.",
            "key_findings": "A simple abstract model yields the same proximal objective (PoS*(p* - p)) as optimal for maximizing expected per-step improvement when success updates are stronger than failure updates, providing additional theoretical support for ProCuRL.",
            "uuid": "e1043.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reverse Curriculum Generation for Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "reverse_curriculum_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "Automatic Goal Generation for Reinforcement Learning Agents",
            "rating": 2,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        },
        {
            "paper_title": "Self-Paced Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "selfpaced_deep_reinforcement_learning"
        },
        {
            "paper_title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "selfpaced_context_evaluation_for_contextual_reinforcement_learning"
        },
        {
            "paper_title": "Prioritized Level Replay",
            "rating": 1,
            "sanitized_title": "prioritized_level_replay"
        },
        {
            "paper_title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design",
            "rating": 2,
            "sanitized_title": "emergent_complexity_and_zeroshot_transfer_via_unsupervised_environment_design"
        }
    ],
    "cost": 0.020969500000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Proximal Curriculum for Reinforcement Learning Agents</p>
<p>Georgios Tzannetos gtzannet@mpi-sws.org 
Max Planck Institute for Software Systems
Max Planck Institute for Software Systems
The Alan Turing Institute Adish Singla
Max Planck Institute for Software Systems</p>
<p>Bárbara Gomes bgomesr@mpi-sws.org 
Max Planck Institute for Software Systems
Max Planck Institute for Software Systems
The Alan Turing Institute Adish Singla
Max Planck Institute for Software Systems</p>
<p>Ribeiro 
Max Planck Institute for Software Systems
Max Planck Institute for Software Systems
The Alan Turing Institute Adish Singla
Max Planck Institute for Software Systems</p>
<p>Parameswaran Kamalaruban kparameswaran@turing.ac.uk 
Max Planck Institute for Software Systems
Max Planck Institute for Software Systems
The Alan Turing Institute Adish Singla
Max Planck Institute for Software Systems</p>
<p>Proximal Curriculum for Reinforcement Learning Agents</p>
<p>We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents. arXiv:2304.12877v1 [cs.LG] 25 Apr 2023 2021). Despite extensive research on curriculum design for the RL setting, existing techniques typically have limited theoretical underpinnings or require domain-specific hyperparameter tuning.In this paper, we are interested in developing a principled curriculum strategy for the RL setting that is broadly applicable to many domains with minimal tuning of hyperparameters. To this end, we rely on the Zone of Proximal Development (ZPD) concept from the educational psychology literature(Vygotsky &amp; Cole, 1978;Chaiklin, 2003). The ZPD concept, when applied in terms of learning progress, suggests that progress is maximized when the learner is presented with tasks that lie in the proximal zone, i.e., tasks that are neither too hard nor too easy. This idea of proximal zone can be captured using a notion of probability of success score PoS πt (s) w.r.t. the learner's current policy π t for any given task s. Building on this idea, we mathematically derive an intuitive curriculum strategy by analyzing two simple learning settings. Our main results and contributions are as follows:I. We propose a curriculum strategy, ProCuRL, inspired by the ZPD concept. ProCuRL formalizes the idea of picking tasks that are neither too hard nor too easy for the learner in the form of selection strategy arg max s PoS πt (s) · PoS * (s) − PoS πt (s) , where PoS * (s) corresponds to the probability of success score w.r.t. an optimal policy (Section 3.1).II. We derive ProCuRL under two specific learning settings where we analyze the effect of picking a task on the agent's learning progress (Section 3.2).III. We present a practical variant of ProCuRL, namely ProCuRL-val, that can be easily integrated with deep RL frameworks with minimal hyperparameter tuning (Section 3.3).IV. We empirically demonstrate the effectiveness of ProCuRL-val over state-of-the-art baselines in accelerating the training process of deep RL agents in a variety of environments (Section 4). 1Related WorkCurriculum strategies based on domain knowledge. Early works on curriculum design for the supervised learning setting typically order the training examples in increasing difficulty(Elman, 1993;Bengio et al., 2009;Schmidhuber, 2013;Zaremba &amp; Sutskever, 2014). This easy-to-hard design principle has been utilized in the hand-crafted curriculum approaches for the RL setting(Asada et al., 1996;Wu &amp; Tian, 2016). Moreover, there have been recent works on designing greedy curriculum strategies for the imitation learning setting based on the iterative machine teaching framework(Liu et al., 2017;Yang et al., 2018;Zhu et al., 2018;Kamalaruban et al., 2019;Yengera et al., 2021). However, these approaches require domain-specific expert knowledge for designing difficulty measures.Curriculum strategies based on ZPD concept. In the pedagogical setting, it has been realized that effective teaching provides tasks that are neither too hard nor too easy for the human learner. This intuition of providing tasks from a particular range of difficulties is conceptualized in the ZPD concept(Vygotsky &amp; Cole, 1978;Chaiklin, 2003;Oudeyer et al., 2007;Baranes &amp; Oudeyer, 2013;Zou et al., 2019). In the RL setting, several curriculum strategies that have been proposed are inherently based on the ZPD concept(Florensa et al., 2017;Wöhlke et al., 2020). A common underlying theme in both Florensa et al. (2017) and Florensa et al. (2018) is that they choose the next task (starting or goal state) for the learner uniformly at random from the set {s : r min ≤ PoS πt (s) ≤ r max }. Here, the threshold values r min and r max require tuning according to the learner's progress and specific to the domain. Wöhlke et al. (2020) propose a unified framework for the learner's performance-based starting state curricula in RL. In particular, the starting state selection policy of Wöhlke et al. (2020), P s (0) t = s ∝ G(PoS πt (s)) for some function G, accommodates existing curriculum generation methods like Florensa et al. (2017); Graves et al. (2017). Despite promising empirical results, theoretical analysis of the impact of the chosen curriculum on the RL agent's learning progress is still missing in the aforementioned works.Curriculum strategies based on self-paced learning (SPL). In the supervised learning setting, the curriculum strategies using the SPL concept optimize the trade-off between exposing the learner to all 1 Github repo: https://github.com/machine-teaching-group/tmlr2023_proximal-curriculum-rl.</p>
<p>Introduction</p>
<p>Recent advances in deep reinforcement learning (RL) have demonstrated impressive performance in games, continuous control, and robotics (Mnih et al., 2015;Lillicrap et al., 2015;Silver et al., 2017;Levine et al., 2016). Despite these remarkable successes, a broader application of RL in real-world domains is often very limited. For example, training RL agents in contextual multi-task settings and goal-based tasks with sparse rewards still remains challenging (Hallak et al., 2015;Kirk et al., 2021;Andrychowicz et al., 2017;Florensa et al., 2017;Riedmiller et al., 2018).</p>
<p>Inspired by the importance of curricula in pedagogical domains, there is a growing interest in leveraging curriculum strategies when training machine learning models in challenging domains. In the supervised learning setting, such as image classification, the impact of the order of presented training examples has been studied both theoretically and empirically Zhou &amp; Bilmes, 2018;Zhou et al., 2021;Elman, 1993;Bengio et al., 2009;Zaremba &amp; Sutskever, 2014). Recent works have also studied curriculum strategies for learners in sequential-decision-making settings, such as imitation learning (where the agent learns from demonstrations) and RL (where the agent learns from rewards). In the imitation learning setting, recent works have proposed greedy curriculum strategies for picking the next training demonstration according to the agent's learning progress (Kamalaruban et al., 2019;Yengera et al., 2021). In the RL setting, several curriculum strategies have been proposed to improve sample efficiency, e.g., by choosing an appropriate next starting state or goal state for the task to train on (Wöhlke et al., 2020;Florensa et al., 2017;Racanière et al., 2020;Riedmiller et al., 2018;Klink et al., 2020a;b;Eimer et al., available training examples and selecting examples in which it currently performs well (Kumar et al., 2010;Jiang et al., 2015). In SPDL (Klink et al., 2020b;a;2022) and SPaCE (Eimer et al., 2021), the authors have adapted the concept of SPL to the RL setting by controlling the intermediate task distribution with respect to the learner's current training progress. However, SPDL and SPaCE differ in their mode of operation and their objective. SPDL considers the procedural task generation framework where tasks of appropriate difficult levels can be synthesized, as also considered in Florensa et al. (2017;. In contrast, SPaCE considers a pool-based curriculum framework for picking suitable tasks, as popular in the supervised learning setting. Further, SPDL considers the objective of a targeted performance w.r.t. a target distribution (e.g., concentrated distribution on hard tasks); in contrast, SPaCE considers the objective of uniform performance across a given pool of tasks. Similar to SPaCE, in our work, we consider the pool-based setting with uniform performance objective. Both SPDL and SPaCE serve as state-of-the-art baselines in our experimental evaluation. In terms of curriculum strategy, SPDL operates by solving an optimization problem at each step to pick a task (Klink et al., 2021); SPaCE uses a ranking induced by the magnitude of differences in current/previous critic values at each step to pick a task (Eimer et al., 2021). In the appendix, we have also provided some additional information on hyperparameters for SPDL and SPaCE.</p>
<p>Other automatic curriculum strategies. There are other approaches for automatic curriculum generation, including: (i) by formulating the curriculum design problem with the use of a meta-level Markov Decision Process (Narvekar et al., 2017;Narvekar &amp; Stone, 2019); (ii) by learning how to generate training tasks similar to a teacher (Dendorfer et al., 2020;Matiisen et al., 2019;Turchetta et al., 2020); (iii) by leveraging self-play as a form of curriculum generation (Sukhbaatar et al., 2018); (iv) by using the disagreement between different agents trained on the same tasks (Zhang et al., 2020); (v) by picking the starting states based on a single demonstration (Salimans &amp; Chen, 2018;Resnick et al., 2018); and (vi) by providing agents with environment variations that are at the frontier of an agent's capabilities, e.g., Unsupervised Environment Design methods (Dennis et al., 2020;Jiang et al., 2021a;Parker-Holder et al., 2022). We refer the reader to recent surveys on curriculum design for the RL setting (Narvekar et al., 2020;Portelas et al., 2021;Weng, 2020).</p>
<p>Formal Setup</p>
<p>In this section, we formalize our problem setting based on prior work on teacher-student curriculum learning (Matiisen et al., 2019).</p>
<p>MDP environment.</p>
<p>We consider a learning environment defined as a Markov Decision Process (MDP) M := (S, A, T , H, R, S init ). Here, S and A denote the state and action spaces, T : S × S × A → [0, 1] is the transition dynamics, H is the maximum length of the episode, and R : S × A → R is the reward function. The set of initial states S init ⊆ S specifies a fixed pool of tasks, i.e., each starting state s ∈ S init corresponds to a unique task. Note that the above environment formalism is quite general enough to cover many practical settings, including the contextual multi-task MDP setting (Hallak et al., 2015). 2 RL agent and training process. We consider an RL agent acting in this environment via a policy π : S × A → [0, 1] that is a mapping from a state to a probability distribution over actions. 3 Given a task with the corresponding starting state s ∈ S init , the agent attempts the task via a trajectory rollout obtained by executing its policy π from s in the MDP M. The trajectory rollout is denoted as ξ = {(s (τ ) , a (τ ) , R(s (τ ) , a (τ ) ))} τ =0,1,...,h with s (0) = s and for some h ≤ H. The agent's performance on task s is measured via the value function V π (s) := E h τ =0 R(s (τ ) , a (τ ) ) π, M, s (0) = s . Then, the uniform performance of the agent over the pool of tasks S init is given by
V π := E s∼Uniform(Sinit) [V π (s)].
The training process of the agent involves an interaction between two components: a student component that is responsible for policy update and a teacher component that is responsible for task selection. The interaction happens in discrete steps, indexed by t = 1, 2, . . ., and is formally described in Algorithm 1.</p>
<p>Let π end denote the agent's final policy at the end of training. The training objective is to ensure that the uniform performance of the policy π end is -near-optimal, i.e., (max π V π − V π end ) ≤ . In the following two paragraphs, we discuss the student and teacher components in detail.</p>
<p>Student component.</p>
<p>We consider a parametric representation for the RL agent, whose current knowledge is parameterized by θ ∈ Θ ⊆ R d and each parameter θ is mapped to a policy π θ : S ×A → [0, 1]. At step t, the student component updates the knowledge parameter based on the following quantities: the current knowledge parameter θ t , the task picked by the teacher component, and the rollout
ξ t = {(s (τ ) t , a (τ ) t , R(s (τ ) t , a (τ ) t ))} τ .
Then, the updated knowledge parameter θ t+1 is mapped to the agent's policy given by π t+1 := π θt+1 . As a concrete example, the knowledge parameter of the Reinforce agent (Sutton et al., 1999) 
is up- dated as θ t+1 ← θ t + η t · h−1 τ =0 G (τ ) t · g (τ ) t , where η t is the learning rate, G (τ ) t = h τ =τ R(s (τ ) t , a (τ ) t ), and g (τ ) t = ∇ θ log π θ (a (τ ) t |s (τ ) t ) θ=θt .
Teacher component. At step t, the teacher component picks a task with the corresponding starting state s (0) t for the student component to attempt via a trajectory rollout (see line 3 in Algorithm 1). The sequence of tasks (curriculum) picked by the teacher component affects the performance improvement of the policy π t . The main focus of this work is to develop a teacher component to achieve the training objective in both a computational and a sample-efficient manner.</p>
<p>Algorithm 1 RL Agent Training as Interaction between Teacher-Student Components 1: Input: RL agent's initial policy π 1 2: for t = 1, 2, . . . do 3: Teacher component picks a task with the corresponding starting state s (0) t .</p>
<p>4:</p>
<p>Student component attempts the task via a trajectory rollout ξ t using the policy π t from s (0) t .</p>
<p>5:</p>
<p>Student component updates the policy to π t+1 . 6: Output: RL agent's final policy π end ← π t+1 .</p>
<p>Proximal Curriculum Strategy</p>
<p>In Section 3.1, we propose a curriculum strategy for the goal-based setting. In Section 3.2, we show that the proposed curriculum strategy can be mathematically derived by analyzing simple learning settings. In Section 3.3, we present our final curriculum strategy that is applicable in general settings.</p>
<p>Curriculum Strategy for the Goal-based Setting</p>
<p>Here, we introduce our curriculum strategy for the goal-based setting using the notion of probability of success scores.</p>
<p>Goal-based setting. In this setting, the reward function R is goal-based, i.e., the agent gets a reward of 1 only at the goal states and 0 at other states; moreover, any action from a goal state also leads to termination. For any task with the corresponding starting state s ∈ S init , we say that the attempted rollout ξ succeeds in the task if the final state of ξ is a goal state. Formally, succ(ξ; s) is an indicator function whose value is 1 when the rollout ξ succeeds in task s, and 0 otherwise. Furthermore, for an agent with policy π, we have that V π (s) := E succ(ξ; s) π, M is equal to the total probability of reaching a goal state by executing the policy π starting from s ∈ S init . Probability of success. We begin by assigning a probability of success score for any task with the corresponding starting state s ∈ S init w.r.t. any parameterized policy π θ in the MDP M.</p>
<p>Definition 1. For any given knowledge parameter θ ∈ Θ and any starting state s ∈ S init , we define the probability of success score PoS θ (s) as the probability of successfully solving the task s by executing the policy π θ in the MDP M. For the goal-based setting, we have PoS θ (s) = V π θ (s).</p>
<p>With the above definition, the probability of success score for any task s ∈ S init w.r.t. the agent's current policy π t is given by PoS t (s) := PoS θt (s). Further, we define PoS * (s) := max θ∈Θ PoS θ (s).</p>
<p>Curriculum strategy. Based on the notion of probability of success scores that we defined above, we propose the following curriculum strategy:
s (0) t ← arg max s∈Sinit PoS t (s) · PoS * (s) − PoS t (s) ,(1)
i.e., at step t, the teacher component picks a task associated with the starting state s (0) t according to Eq. 1. The term PoS t (s) · (PoS * (s) − PoS t (s)) can be interpreted as the geometric mean of two quantities: the learner's probability of solving the task and the expected regret of the learner on this task. In the following subsection, we show that the above curriculum strategy can be derived by considering simple learning settings, such as contextual bandit problems with Reinforce agent; these derivations provide insights about the design of the curriculum strategy.</p>
<p>Theoretical Justifications for the Curriculum Strategy</p>
<p>To derive our curriculum strategy for the goal-based setting, we additionally consider independent tasks where any task s (0) t picked from the pool S init at step t only affects the agent's knowledge component corresponding to that task. Further, we assume that there exists a knowledge parameter θ * ∈ Θ such that π θ * ∈ arg max π V π , and π θ * is referred to as the target policy. Then, based on the work of ; Kamalaruban et al. (2019);Yengera et al. (2021), we investigate the effect of picking a task s (0) t at step t on the convergence of the agent's parameter θ t towards the target parameter θ * . Under a smoothness condition on the value function of the form |V π θ − V π θ | ≤ L · θ − θ 1 , ∀θ, θ ∈ Θ for some L &gt; 0, we can translate the parameter convergence (θ t → θ * ) into the performance convergence (V π θ t → V π θ * ). Thus, we define the improvement in the training objective at step t as
∆ t (θ t+1 θ t , s (0) t , ξ t ) := [ θ * − θ t 1 − θ * − θ t+1 1 ].
(2)</p>
<p>In the above objective, we use the 1 -norm because our theoretical analysis considers the independent task setting mentioned above. Further, we define the expected improvement in the training objective at step t due to picking the task s (0) t as follows:
C t (s (0) t ) := E ξt|s (0) t ∆ t (θ t+1 |θ t , s (0) t , ξ t ) .(3)
Note that the above quantity is an approximation of the expected learning progress measure as defined in Graves et al. (2017). In the following subsection, we justify our proposed curriculum strategy by analyzing the above quantity for a specific agent model under the independent task setting. More concretely, for the specific setting considered in Section 3.2.1, Theorem 1 implies that picking tasks based on the curriculum strategy given in Eq. 1 maximizes the expected value of the objective in Eq. 2. In the appendix, we provide an additional justification by considering an abstract agent model with a direct performance parameterization.</p>
<p>Reinforce Agent with Softmax Policy Parameterization</p>
<p>We consider the Reinforce agent model with the following softmax policy parameterization: for any θ ∈ R |S|·|A| , we parameterize the policy as π θ (a|s) ∝ exp(θ[s, a]), ∀s ∈ S, a ∈ A. In the following, we consider a problem instance involving a pool of contextual bandit tasks (a special case of independent task setting). Consider an MDP M with g ∈ S as the goal state for all tasks, S init = S \ {g}, A = {a 1 , a 2 }, and H = 1. We define the reward function as follows: R(s, a) = 0, ∀s ∈ S \ {g}, a ∈ A and R(g, a) = 1, ∀a ∈ A. For a given probability mapping p rand : S → [0, 1], we define the transition dynamics as follows: T (g|s, a 1 ) = p rand (s), ∀s ∈ S; T (s|s, a 1 ) = 1 − p rand (s), ∀s ∈ S; and T (s|s, a 2 ) = 1, ∀s ∈ S. Then, for the Reinforce agent under the above setting, the following theorem quantifies the expected improvement in the training objective at step t:</p>
<p>Theorem 1. Consider the Reinforce agent with softmax policy parameterization under the independent task setting as described above. Let s (0) t be the task picked at step t with PoS θt (s
(0) t ) = p and PoS θ * (s (0) t ) = p * . Then, we have: C t (s (0) t ) = 2 · η t · p · 1 − p p * ,
where η t is the learning of the Reinforce agent.</p>
<p>For the above setting with p rand (s) = 1, ∀s ∈ S, max s∈Sinit C t (s) is equivalent to max s∈Sinit PoS t (s) · (1 − PoS t (s)). This means that for the case of PoS * (s) = 1, ∀s ∈ S init , the curriculum strategy given in Eq. 1 can be seen as greedily optimizing the expected improvement in the training objective at step t given in Eq. 3.</p>
<p>Curriculum Strategy for General Settings</p>
<p>Next, we discuss various practical issues in directly applying the curriculum strategy in Eq. 1 for general settings, and introduce several design choices to address these issues.</p>
<p>Softmax selection. When training deep RL agents, it is typically useful to allow some stochasticity in the selected batch of tasks. Moreover, the arg max selection in Eq. 1 is brittle in the presence of any approximation errors in computing PoS(·) values. To tackle this issue, we replace arg max selection in Eq. 1 with softmax selection and sample according to the following distribution:
P s (0) t = s ∝ exp β · PoS t (s) · PoS * (s) − PoS t (s) ,(4)
where β is a hyperparameter. Here, PoS t (s) values are computed for each s ∈ S init using rollouts obtained via executing the policy π t in M; PoS * (s) values are assumed to be provided as input.</p>
<p>PoS * (·) is not known. Since the target policy π θ * is unknown, it is not possible to compute the PoS * (s) values without additional domain knowledge. In our experiments, we resort to simply setting PoS * (s) = 1, ∀s ∈ S init in Eq. 4 -the rationale behind this choice is that we expect the ideal π θ * to succeed in all the tasks in the pool. 4 This brings us to the following curriculum strategy referred to as ProCuRL-env in our experimental evaluation:
P s (0) t = s ∝ exp β · PoS t (s) · 1 − PoS t (s) .(5)
Computing PoS t (·) is expensive. It is expensive (sample inefficient) to estimate PoS t (s) over the space S init using rollouts of the policy π t . To tackle this issue, we replace PoS t (s) with values V t (s) obtained from the critic network of the RL agent. This brings us to the following curriculum strategy referred to as ProCuRL-val in our experimental evaluation:
P s (0) t = s ∝ exp β · V t (s) · 1 − V t (s) .(6)
Extension to non-binary or dense reward settings. The current forms of ProCuRL-val in Eq. 6 and ProCuRL-env in Eq. 5 are not directly applicable for settings where the reward is non-binary or dense.</p>
<p>To deal with this issue in ProCuRL-val, we replace V t (s) values from the critic in Eq. 6 with normalized values given by V t (s) = Vt(s)−Vmin Vmax−Vmin clipped to the range [0, 1]. Here, V min and V max could be provided as input based on the environment's reward function; alternatively we can dynamically set V min and V max during the training process by taking min-max values of the critic for states S init at step t. To deal with this issue in ProCuRL-env, we replace PoS t (s) values from the rollouts in Eq. 5 with normalized values V t (s) as above. Algorithm 2 in the appendix provides a complete pseudo-code for the RL agent training with ProCuRL-val in this general setting.</p>
<p>Experimental Evaluation</p>
<p>In this section, we evaluate the effectiveness of our curriculum strategies on a variety of domains w.r.t. the uniform performance of the trained RL agent over the training pool of tasks. Additionally, we consider the following two metrics in our evaluation: (i) total number of environment steps incurred jointly by the teacher and the student components at the end of the training process; (ii) total clock time required for the training process. Throughout all the experiments, we use the PPO method from Stable-Baselines3 library for policy optimization (Schulman et al., 2017;Raffin et al., 2021).</p>
<p>Environment</p>
<p>Reward Context State Action Pool size
PointMass-s binary R 3 R 4 R 2 100 PointMass-d non-binary R 3 R 4 R 2 100
BasicKarel binary 24000 {0, 1} 88 6 24000</p>
<p>BallCatching non-binary 
R 3 R 21 R 5 100 AntGoal non-binary R 2 R 29 R 8 50(</p>
<p>Environments</p>
<p>We consider 5 different environments in our evaluation, as described in the following paragraphs. Figure 1 provides a summary and illustration of these environments.</p>
<p>PointMass-s and PointMass-d.</p>
<p>Based on the work of Klink et al. (2020b), we consider a contextual PointMass environment where an agent navigates a point mass through a gate of a given size towards a goal in a two-dimensional space. More concretely, we consider two settings: (i) PointMass-s environment corresponds to a goal-based (i.e., binary and sparse) reward setting where the agent receives a reward of 1 only if it successfully moves the point mass to the goal position; (ii) PointMass-d environment corresponds to a dense reward setting as used by Klink et al. (2020b) where the reward values decay in a squared exponential manner with increasing distance to the goal. Here, the contextual variable c ∈ R 3 controls the position of the gate (C-GatePosition), the width of the gate (C-GateWidth), and the friction coefficient of the ground (C-Friction). We construct the training pool of tasks by uniformly sampling 100 tasks over the space of possible tasks (here, each task corresponds to a different contextual variable).</p>
<p>BasicKarel. This environment is inspired by the Karel program synthesis domain (Bunel et al., 2018), where the goal of an agent is to transform an initial grid into a final grid configuration by a sequence of commands. In our BasicKarel environment, we do not allow any programming constructs such as conditionals or loops and limit the commands to the "basic" actions given by A = {move, turnLeft, turnRight, pickMarker, putMarker, finish}. A task in this environment corresponds to a pair of initial grid and final grid configurations; the environment is episodic with goal-based (i.e., binary and sparse) reward setting where the agent receives a reward of 1 only if it successfully transforms the task's initial grid into the task's final grid. Here, the contextual variable is discrete, where each task can be considered as a discrete context. We construct the training pool of tasks by sampling 24000 tasks; additional details are provided in the appendix.</p>
<p>BallCatching. This environment is the same used in the work of Klink et al. (2020b); here, an agent needs to direct a robot to catch a ball thrown towards it. The reward function is sparse and non-binary, only rewarding the robot when it catches the ball and penalizing it for excessive movements. The contextual vector c ∈ R 3 captures the distance to the robot from which the ball is thrown and its goal position in a plane that intersects the base of the robot. We construct the training pool of tasks by uniformly sampling 100 tasks over the space of possible tasks.</p>
<p>AntGoal. This environment is adapted from the original MuJoCo Ant environment (Todorov et al., 2012). In our adaptation, we additionally have a goal on a flat 2D surface, and an agent is rewarded for moving an ant robot towards the goal location. This goal-based reward term replaces the original reward term of making the ant move forward; also, this reward term increases exponentially when the ant moves closer to the goal location. We keep the other reward terms, such as control and contact costs, similar to the original MuJoCo Ant environment. The environment is episodic with a length of 200 steps. The goal location essentially serves as a contextual variable in R 2 . We construct the training pool of tasks by uniformly sampling 50 goal locations from a circle around the ant.</p>
<p>These environments are goal-based and have an implicit way of defining a successful trajectory. Typically, success is defined as a reward signal to the agent for approaching the goal, as done by Klink et al. (2020b) for PointMass, BallCatching, and AntGoal. As future work, it would also be interesting to investigate the effect of our curriculum strategy on RL algorithms designed for the same goal-based setting but without assuming that a goal proximity function is defined in the environment (Ding et al., 2019;Eysenbach et al., 2022;Lin et al., 2019).</p>
<p>Curriculum Strategies Evaluated</p>
<p>Variants of our curriculum strategy. We consider the curriculum strategies ProCuRL-val and ProCuRL-env from Section 3.3. Since ProCuRL-env uses policy rollouts to estimate PoS t (s) in Eq. 5, it requires environment steps for selecting tasks in addition to environment steps for training. To compare ProCuRL-val and ProCuRL-env in terms of trade-off between performance and sample efficiency, we introduce a variant ProCuRL-env x where x controls the budget of the total number of steps used for estimation and training. In Figure 3, variants with x ∈ {2, 4} refer to a total budget of about x million environment steps when training comprises of 1 million steps.</p>
<p>State-of-the-art baselines. SPDL (Klink et al., 2020b) and SPaCE (Eimer et al., 2021) are state-of-theart curriculum strategies for contextual RL. We adapt the implementation of an improved version of SPDL, presented in Klink et al. (2021), to work with a discrete pool of tasks. We also introduce a variant of SPaCE, namely SPaCE-alt, by adapting the implementation of Eimer et al. (2021) to sample the next training task as P s Jiang et al., 2021b) is a state-of-the-art curriculum strategy originally designed for procedurally generated content settings. We adapt the implementation of PLR for the contextual RL setting operating on a fixed pool of tasks and include it as an additional baseline.
(0) t = s ∝ exp β · V t (s) − V t−1 (s) . PLR (
Prototypical baselines. IID strategy randomly samples the next task from the pool; note that IID serves as a competitive baseline since we consider the uniform performance objective. We introduce two additional variants of ProCuRL-env, namely Easy and Hard, to understand the importance of the two terms PoS t (s) and 1 − PoS t (s) in Eq. 5. Easy samples tasks as P s (0) t = s ∝ exp β · PoS t (s) , and</p>
<p>Hard samples tasks as P s
(0) t = s ∝ exp β · 1 − PoS t (s) .</p>
<p>Results</p>
<p>Convergence behavior and curriculum plots. As shown in Figure 2, the RL agents trained using the variants of our curriculum strategy, ProCuRL-env and ProCuRL-val, either match or outperform the agents trained with state-of-the-art and prototypical baselines in all the environments. Figures 4 and 5 visualize the curriculums generated by ProCuRL-env, ProCuRL-val, and IID; the trends for ProCuRLval generally indicate a gradual shift towards harder tasks across different contexts. The increasing trend in Figure 4a corresponds to a preference shift towards tasks with the gate positioned closer to the edges; the decreasing trend in Figure 4b corresponds to a preference shift towards tasks with narrower gates. For BasicKarel, the increasing trends in Figures 5a and 5b correspond to a preference towards tasks with longer solution trajectories and tasks requiring a marker to be picked or put, respectively. In Figures 5c and 5d, tasks with a distractor marker (C-DistractorMarker) and tasks with more walls (C-Walls) are increasingly selected while training. Figure 3, we compare curriculum strategies considered in our experiments w.r.t. different metrics. ProCuRL-val has similar sample complexity as state-of-the-art baselines since it does not require additional environment steps for the teacher component. ProCuRL-val performs better compared to SPDL, SPaCE and PLR in terms of computational complexity. The effect of that is more evident as the pool size increases. The reason is that ProCuRL-val only requires forward-pass operation on the critic-model to obtain value estimates for each task in the pool. SPDL and SPaCE not only require the same forward-pass operations, but SPDL does an additional optimization step, and SPaCE requires a task ordering step. As for PLR, it has an additional computational overhead for scoring the sampled tasks. In terms of agent's performance, our curriculum strategies exceed or match these baselines at different training segments. Even though ProCuRL-env consistently surpasses all the other variants in terms of perfor-  Figure 2: Performance comparison of RL agents trained using different curriculum strategies described in Section 4.2. The performance is measured as the mean reward (±1 standard error) on the training pool of tasks. The results are averaged over 20 random seeds for PointMass-s and PointMass-d, 10 random seeds for BasicKarel and BallCatching, and 5 random seeds for AntGoal. The plots are smoothed across 5 evaluation snapshots happening at over 25000 training steps.</p>
<p>Metrics comparison. In</p>
<p>Method Env</p>
<p>PointMass-s BasicKarel Hard 0.01 ± 0.01 0.00 ± 0.00 0.01 ± 0.01 37.0 ± 0.7 332 0.01 ± 0.00 0.01 ± 0.00 0.01 ± 0.00 35.2 ± 3.9 197 Figure 3: Comparison of different curriculum strategies described in Section 4.2 under the following metrics: (i) performance (mean reward ± t×standard error, where t is the value from the t-distribution table for 95% confidence (Beyer, 2019)) of the RL agent at 0.25, 0.5, and 1 million training steps; (ii) total number of environment steps incurred at the end of 1 million training steps (this captures the sample efficiency of a curriculum strategy); (iii) total clock time in minutes at the end of 1 million training steps (this captures the computational efficiency of a curriculum strategy).</p>
<p>mance, its teacher component requires a lot of additional environment steps. Regarding the prototypical baselines in Figure 3, we make the following observations: (a) IID is a strong baseline in terms of sample and computational efficiency; however, its performance tends to be unstable in PointMass-s environment because of high randomness; (b) Easy performs well in PointMass-s because of the presence of easy tasks in the task space of this environment, but, performs quite poorly in BasicKarel; (c) Hard consistently fails in both the environments. to competitive performance, and ProCuRL-val is robust even for noise levels up to = 0.2. Further, we conduct an ablation study on the form of our curriculum objective presented in Eq. 1. More specifically, we consider the following generalized variant of Eq. 1 with parameters γ 1 and γ 2 : s (0) t ← arg max s∈Sinit PoS t (s)·(γ 1 · PoS * (s)−γ 2 ·PoS t (s)) . In our experiments, we consider the following range of γ 2 /γ 1 ∈ {0.6, 0.8, 1.0, 1.2, 1.4}. The results are reported in the appendix. From the reported results, we note that our default curriculum strategy in Eq. 1 (corresponding to γ 2 /γ 1 = 1.0) leads to competitive performance.</p>
<p>Concluding Discussions</p>
<p>We proposed a novel curriculum strategy for deep RL agents inspired by the ZPD concept. We mathematically derived our strategy by analyzing simple learning settings and empirically demonstrated its effectiveness in a variety of complex domains. Here, we discuss a few limitations of our work and outline a plan on how to address them in future work. First, experimental results show that different variants of our proposed curriculum provide an inherent trade-off between runtime and performance; it would be interesting to systematically study these variants to obtain a more effective curriculum strategy across different metrics. Second, it would be interesting to extend our curriculum strategy to sparse reward environments with high-dimensional context space; in particular, our curriculum strategy requires estimating the probability of success of all tasks in the pool when sampling a new task which is challenging in these environments. Third, extending the theoretical analysis of the curriculum strategy from independent task settings to correlated task settings would be an interesting avenue to explore; this could involve developing a generalized version of ProCuRL curriculum strategy using a distance metric over the context space (Klink et al., 2022;Huang et al., 2022).</p>
<p>A Table of Contents</p>
<p>In this section, we give a brief description of the content provided in the appendices of the paper.</p>
<p>• Appendix B provides a proof for Theorem 1 and an additional theoretical justification for our curriculum strategy. (Section 3.2)</p>
<p>• Appendix C provides additional details and results for experimental evaluation. (Section 4)</p>
<p>B Theoretical Justifications for the Curriculum Strategy -Proof and Additional Justification (Section 3.2) B.1 Proof of Theorem 1</p>
<p>Proof. For the contextual bandit setting described in Section 3.2.1, the Reinforce learner's update rule reduces to the following:
θ t+1 ← θ t + η t · 1{s (1) t = g} · ∇ θ log π θ (a (0) t |s (0) t ) θ=θt . In particular, for s (0) t = s
and a (0) t = a 1 , we update:
θ t+1 [s, a 1 ] ← θ t [s, a 1 ] + η t · 1{s
(1)
t = g} · (1 − π θt (a 1 |s)) θ t+1 [s, a 2 ] ← θ t [s, a 2 ] − η t · 1{s (1) t = g} · (1 − π θt (a 1 |s)) and we set θ t+1 [s, ·] ← θ t [s, ·] when s (0) t = s or a (0) t = a 1 . Let s (0)
t = s, and consider the following: π θt (a 1 |s)).
∆ t (θ t+1 θ t , s, ξ t ) = θ * − θ t 1 − θ * − θ t+1 1 = θ * [s, ·] − θ t [s, ·] 1 − θ * [s, ·] − θ t+1 [s, ·] 1 = {θ * [s, a 1 ] − θ t [s, a 1 ] + θ t [s, a 2 ] − θ * [s, a 2 ]} − {θ * [s, a 1 ] − θ t+1 [s, a 1 ] + θ t+1 [s, a 2 ] − θ * [s, a 2 ]} = θ t+1 [s, a 1 ] − θ t [s, a 1 ] + θ t [s, a 2 ] − θ t+1 [s, a 2 ] = 2 · η t · 1{a (0) t = a 1 , s (1) t = g} · (1 −
For the contextual bandit setting, the probability of success is given by PoS θ (s) = V π θ (s) = p rand (s) · π θ (a 1 |s), ∀s ∈ S. We assume that ∃ θ * such that π θ * (a 1 |s) → 1; here, π θ * is the target policy. With the above definition, the probability of success scores for any task associated with the starting state s ∈ S init w.r.t. the target and agent's current policies (at any step t) are respectively given by PoS * (s) = PoS θ * (s) = p rand (s) = p * and PoS t (s) = PoS θt (s) = p rand (s) · π θt (a 1 |s) = p. Now, we consider the following:
C t (s) = E ξt|s ∆ t (θ t+1 θ t , s, ξ t ) = E ξt|s 2 · η t · 1{a (0) t = a 1 , s (1) t = g} · (1 − π θt (a 1 |s)) = 2 · η t · p rand (s) · π θt (a 1 |s) · (1 − π θt (a 1 |s)) = 2 · η t · p · 1 − p p * .</p>
<p>B.2 Abstract Agent with Direct performance Parameterization</p>
<p>We consider an abstract agent model with the following direct performance parameterization: for any θ ∈ Θ = [0, 1] |Sinit| , we have PoS θ (s) = θ[s], ∀s ∈ S init . 5 Under this model, the agent's current knowledge θ t at step t is encoded directly by its probability of success scores {PoS θt (s) | s ∈ S init }. The target knowledge parameter θ * is given by {PoS θ * (s) | s ∈ S init }. Under the independent task setting, we design an update rule for the agent to reflect the characteristics of the policy gradient style update. In particular, for s = s
(0) t ∈ S init , we update θ t+1 [s] ← θ t [s] + α · succ(ξ t ; s) · (θ * [s] − θ t [s]) + β · (1 − succ(ξ t ; s)) · (θ * [s] − θ t [s]),
where α, β ∈ [0, 1] and α &gt; β. For s ∈ S init and s = s
(0) t , we maintain θ t+1 [s] ← θ t [s]
. Importantly, α &gt; β implies that the agent's current knowledge for the picked task is updated more when the agent succeeds in that task compared to the failure case. The update rule captures the following idea: when picking a task that is "too easy", the progress in θ t towards θ * is minimal since (θ * [s] − θ t [s]) is low; similarly, when picking a task that is "too hard", the progress in θ t towards θ * is minimal since β · (θ * [s] − θ t [s]) is low for β 1. This idea aligns with the ZPD concept in terms of the learning progress (Vygotsky &amp; Cole, 1978;Chaiklin, 2003). For the abstract agent under the above setting, the following theorem quantifies the expected improvement in the training objective at step t:</p>
<p>Theorem 2. Consider the abstract agent with direct performance parameterization under the independent task setting as described above. Let s (0) t be the task picked at step t with PoS θt (s
(0) t ) = p and PoS θ * (s (0) t ) = p * . Then, we have: C t (s (0) t ) = α · p · (p * − p) + β · (1 − p) · (p * − p).
Proof. Let s (0) t = s ∈ S init , and consider the following:
∆ t (θ t+1 θ t , s, ξ t ) = θ * − θ t 1 − θ * − θ t+1 1 = θ t+1 [s] − θ t [s] = α · succ(ξ t ; s) · (θ * [s] − θ t [s]) + β · (1 − succ(ξ t ; s)) · (θ * [s] − θ t [s]).
For the abstract learner model defined in Section B.2, we have PoS θ (s) = V π θ (s) = θ[s], for any s ∈ S init . Then, the probability of success scores for any task s ∈ S init w.r.t. the target and agent's current policies (at any step t) are respectively given by PoS * (s) = PoS θ * (s) = θ * [s] = p * and PoS t (s) = PoS θt (s) = θ t [s] = p. Now, we consider the following:
C t (s) = E ξt|s ∆ t (θ t+1 θ t , s, ξ t ) = E ξt|s [α · succ(ξ t ; s) · (θ * [s] − θ t [s]) + β · (1 − succ(ξ t ; s)) · (θ * [s] − θ t [s])] = α · θ t [s] · (θ * [s] − θ t [s]) + β · (1 − θ t [s]) · (θ * [s] − θ t [s]) = α · p · (p * − p) + β · (1 − p) · (p * − p).
For the above setting with α = 1 and β = 0, max s∈Sinit C t (s) is equivalent to max s∈Sinit PoS t (s) · (PoS * (s) − PoS t (s)). This, in turn, implies that the curriculum strategy given in Eq. 1 can be seen as greedily optimizing the expected improvement in the training objective at step t given in Eq. 3.</p>
<p>C Experimental Evaluation -Additional Details (Section 4)</p>
<p>C.1 Environments</p>
<p>BasicKarel. This environment is inspired by the Karel program synthesis domain (Bunel et al., 2018), where the goal of an agent is to transform an initial grid into a final grid configuration by a sequence of commands. In the BasicKarel environment, we do not allow any programming constructs such as conditionals or loops and limit the commands to the "basic" actions given by the action space A = {move, turnLeft, turnRight, pickMarker, putMarker, finish}. A task in this environment corresponds to a pair of initial grid and final grid configurations. It consists of an avatar, walls, markers, and empty grid cells, and each element has a specific location in the grid. The avatar is characterized by its current location and orientation. Its orientation can be any direction {North, East, South, West}, and its location can be any grid cell, except from grid cells where a wall is located. The state space S of BasicKarel is any possible configuration of the avatar, walls, and markers in a pair of grids. The avatar can move around the grid and is directed via the basic Karel commands, i.e., the action space A. While the avatar moves, if it hits a wall or the grid boundary, it "crashes" and the episode terminates. If pickMarker is selected when no marker is present, the avatar "crashes" and the program ends. Likewise, if the putMarker action is taken and a marker is already present, the avatar "crashes" and the program terminates. The finish action indicates the end of the sequence of actions, i.e., the episode ends after encountering this action. To successfully solve a BasicKarel task, the sequence of actions must end with a finish, and there should be no termination via "crashes". Based on this environment, we created a multi-task dataset that consists of 24000 training tasks and 2400 test tasks. All the generated tasks have a grid size of 4 × 4.</p>
<p>C.2 Evaluation Setup</p>
<p>Hyperparameters of PPO method. We use the PPO method from Stable-Baselines3 library with a basic MLP policy for all the conducted experiments (Schulman et al., 2017;Raffin et al., 2021 Figure 6: Different hyperparameters of the PPO method used in the experiments for each environment.</p>
<p>Compute resources.</p>
<p>All the experiments were conducted on a cluster of machines with CPUs of model Intel Xeon Gold 6134M CPU @ 3.20GHz.</p>
<p>C.3 Curriculum Strategies Evaluated</p>
<p>Variants of the curriculum strategy. Algorithm 2 provides a complete pseudo-code for the RL agent using PPO method when trained with ProCuRL-val in the general setting of non-binary or dense rewards (see Section 3.3). In Eq. 1 and Algorithm 1, we defined t at an episodic level; however, in Algorithm 2, t denotes an environment step (in the context of the PPO method). For ProCuRL-env, in line 24 of Algorithm 2, we estimate the probability of success for all the tasks using the additional rollouts obtained by executing the current policy in M.</p>
<p>To achieve the constrained budget of evaluation steps in ProCuRL-env x (with x ∈ {2, 4}), we reduce the frequency of updating PoS t since this is the most expensive operation for ProCuRL-env requiring additional rollouts for each task. On the other hand, ProCuRL-val updates PoS t by using the values obtained from forward-pass on the critic model -this update happens whenever the critic model is updated (every 2048 training steps for BasicKarel). This higher frequency of updating PoS t in ProCuRL-val is why it is slower than ProCuRL-env x (with x ∈ {2, 4}) for BasicKarel. Note that the relative frequency of updates for PointMass is different in comparison to BasicKarel because of very different pool sizes. Hence, the behavior in total clock times is different.</p>
<p>Algorithm 2 RL agent using PPO method when trained with ProCuRL-val in the general setting 1: Input: RL algorithm PPO, rollout buffer D 2: Hyperparameters: policy update frequency N steps , number of epochs N epochs , number of minibatches N batch , parameter β, V min , and V max 3: Initialization: randomly initialize policy π 1 and critic V 1 ; set normalized probability of success scores V 1 (s) = 0 and PoS * (s) = 1, ∀s ∈ S init 4: for t = 1, . . . , T do 5:</p>
<p>// add an environment step to the buffer 6:</p>
<p>observe the state s t , and select the action a t ∼ π t (s t ) 7:</p>
<p>execute the action a t in the environment 8:</p>
<p>observe reward r t , next state s t+1 , and done signal d t+1 to indicate whether s t+1 is terminal 9: store (s t , a t , r t , s t+1 , d t+1 ) in the rollout buffer D if d t+1 = true then 12: reset the environment state 13: maintain the previous values π t+1 ← π t , V t+1 ← V t , and V t+1 ← V t 27: Output: policy π T Hyperparameters of curriculum strategies. In Figure 7, we report the hyperparameters of each curriculum strategy used in the experiments (for each environment). Below, we provide a short description of these hyperparameters:
sample next task s t+1 from P s t+1 = s ∝ exp β · V t (s) · (1 − V t (s))
5. η and κ parameters as used in SPaCE (Eimer et al., 2021).</p>
<ol>
<li>
<p>V LB performance threshold as used in SPDL (Klink et al., 2021).</p>
</li>
<li>
<p>ρ staleness coefficient and β PLR temperature parameter for score prioritization as used in PLR (Jiang et al., 2021b). </p>
</li>
</ol>
<p>Method</p>
<p>C.4 Additional Results</p>
<p>Ablation and robustness experiments. We conduct additional experiments to evaluate the robustness of ProCuRL-val w.r.t. different values of β and different -level noise in V t (s) values. The results are reported in Figure 8. Further, we conduct an ablation study on the form of our curriculum objective presented in Eq. 1. More specifically, we consider the following generalized variant of Eq. 1 with parameters γ 1 and γ 2 :
s (0) t ← arg max s∈Sinit PoS t (s) · γ 1 · PoS * (s) − γ 2 · PoS t (s)(7)
In our experiments, we consider the following range of γ 2 /γ 1 ∈ {0.6, 0.8, 1.0, 1.2, 1.4}. Our default curriculum strategy in Eq. 1 essentially corresponds to γ 2 /γ 1 = 1.0. The results are reported in Figure 9.</p>
<p>Performance on test set. In Figure 10, we report the performance of the trained models in the training set and a test set for comparison purposes. For PointMass-S, we constructed a separate test set of 100 tasks by uniformly picking tasks from the task space. For BasicKarel, we have a train and test dataset of 24000 and 2400 tasks, respectively.</p>
<p>Pool of harder tasks. We sought to assess the effectiveness of ProCuRL-val on tasks where IID does not perform well. To demonstrate this, we construct a more challenging set of tasks for the PointMasss environment. We generate half of these tasks by uniformly sampling over the context space. The remaining tasks are sampled from a bi-modal Gaussian distribution, where the means of the contexts [C-GatePosition, C-GateWidth] are [−3, 1] and [3,1] for the two modes, respectively. In Figure 11, we present the results for ProCuRL-val and IID, and in Figure 12 the different distributions. We present the results for the PointMass-s environment and BasicKarel environment. We report the mean reward (± t×standard error, where t is the value from the t-distribution table for 95% confidence) at 0.25, 0.5, and 1 million training steps averaged over 20 and 10 random seeds, respectively.</p>
<p>Method Env</p>
<p>PointMass-s BasicKarel</p>
<p>ProCuRL-val Performance Performance 0.25M 0.5M 1M 0.25M 0.5M 1M γ 2 /γ 1 = 0.6 0.33 ± 0.14 0.50 ± 0.14 0.55 ± 0.13 0.08 ± 0.04 0.21 ± 0.07 0.41 ± 0.08 γ 2 /γ 1 = 0.8 0.26 ± 0.15 0.43 ± 0.17 0.55 ± 0.20 0.11 ± 0.03 0.36 ± 0.05 0.64 ± 0.07 γ 2 /γ 1 = 1.0 0.48 ± 0.15 0.64 ± 0.17 0.71 ± 0.18 0.06 ± 0.03 0.30 ± 0.08 0.71 ± 0.05 γ 2 /γ 1 = 1.2 0.42 ± 0.19 0.55 ± 0.19 0.65 ± 0.17 0.07 ± 0.04 0.30 ± 0.11 0.72 ± 0.08 γ 2 /γ 1 = 1.4 0.39 ± 0.16 0.59 ± 0.17 0.59 ± 0.15 0.04 ± 0.02 0.21 ± 0.08 0.71 ± 0.06 Figure 9: Performance comparison of the generalized form of our curriculum strategy presented in Eq. 7 w.r.t. different values of γ 2 /γ 1 . We present the results for the PointMass-s environment and BasicKarel environment. We report the mean reward (± t×standard error, where t is the value from the t-distribution table for 95% confidence) at 0.25, 0.5, and 1 million training steps averaged over 20 and 10 random seeds, respectively.  Figure 10: Performance of the curriculum strategies, discussed in Section 4.2, in the training set and a test set. We report the performance, i.e., expected mean reward, of the best model obtained during training for all the methods. The training steps to achieve this performance is shown in parenthesis for each environment (M is 10 6 steps). We present the results for the PointMass-s environment and BasicKarel environment and report the mean reward averaged over 20 and 10 random seeds, respectively.</p>
<p>Method Env</p>
<p>PointMass-s BasicKarel</p>
<p>Method Env</p>
<p>PointMass-s Performance 0.25M 0.5M 1M 1.5M 2M</p>
<p>ProCuRL-val 0.07 ± 0.07 0.19 ± 0.11 0.40 ± 0.15 0.46 ± 0.16 0.49 ± 0.16 IID 0.01 ± 0.01 0.03 ± 0.03 0.05 ± 0.06 0.04 ± 0.04 0.03 ± 0.03 Figure 11: Performance comparison of our curriculum strategy, ProCuRL-val, and IID in a pool of harder tasks for the PointMass-s environment. We report the mean reward (± t×standard error, where t is the value from the t-distribution table for 95% confidence) at 0.25, 0.5, 1, 1.5 and 2 million training steps averaged over 20 random seeds.   </p>
<p>Figure 1 :
1(a) shows complexity of the environments w.r.t. the reward signals, context variation, state space, action space, and the pool size of the tasks used for training. (b) shows illustration of the environments (from left to right): PointMass, BasicKarel, BallCatching, and AntGoal. Details are provided in Section 4.1.</p>
<p>32 ± 0.12 0.47 ± 0.15 0.69 ± 0.13 1.0 ± 0.0 20 0.05 ± 0.03 0.23 ± 0.05 0.70 ± 0.04 1.0 ± 0.0 79 IID 0.27 ± 0.15 0.34 ± 0.17 0.36 ± 0.19 1.0 ± 0.0 20 0.03 ± 0.02 0.15 ± 0.06 0.64 ± 0.08 1.0 ± 0.0 34 Easy 0.37 ± 0.13 0.44 ± 0.12 0.50 ± 0.11 17.1 ± 2.3 154 0.04 ± 0.01 0.07 ± 0.02 0.11 ± 0.03 22.6 ± 0.9 126</p>
<p>Figure 4 :Figure 5 :
45Ablation and robustness experiments. We conduct additional experiments to evaluate the robustness of ProCuRL-val w.r.t. different values of β and different -level noise in V t (s) values. The results are reported in the appendix. From the reported results, we note that picking a value for β in the range from 10 to 30 leads (a-c) Curriculum visualization of ProCuRL-env, ProCuRL-val, and IID in the PointMass-s environment; these plots show the moving average variation of the context variables of every 100 tasks picked by curriculum strategies during the training process (a picked task involves multiple training steps shown on the x-axis of plots). The increasing trend in (a) corresponds to a preference shift towards tasks with the gate positioned closer to the edges; the decreasing trend in (b) corresponds to a shift towards tasks with narrower gates. (d-f) Illustrative tasks used during the training process for ProCuRL-val (M is 10 6 ). (a-d) Curriculum visualization of ProCuRL-env, ProCuRL-val, and IID in the BasicKarel environment; these plots show the moving average variation of the context variables of every 500 tasks picked. The increasing trends in (a-d) correspond to a preference towards tasks: (a) with longer trajectories, (b) requiring a marker action, (c) with more distractor markers, and (d) with more walls. (e-i) Illustrative tasks used during the training process at different steps for ProCuRL-val (M is 10 6 ).</p>
<p>and V t (s) update 15: if t%N steps = 0 then 16: set π ← π t and V ← V t 17: for e = 1, . . . , N epochs do 18: for b = 1, . . . , N batch do 19: sample b-th minibatch of N steps /N batch transitions B = {(s, a, r, s , d)} from D 20: update policy and critic using PPO algorithm π , V ← PPO(π , V , B) 21: set π t+1 ← π and V t+1 ← V 22: empty the rollout buffer D 23: // normalization for the environments with non-binary or dense rewards 24:update V t+1 (s) ← Vt+1(s)−Vmin Vmax−Vmin , ∀s ∈ S init using forward passes on critic</p>
<p>03 Figure 8 :
03849 ± 0.16 0.61 ± 0.18 0.68 ± 0.18 0.04 ± 0.02 0.26 ± 0.08 0.74 ± 0.Robustness of ProCuRL-val w.r.t. different values of β and different -level noise in V t (s) values.</p>
<p>) Distribution for uniform pool of tasks</p>
<p>) Distribution for harder pool of tasks</p>
<p>Figure 12 :
12(a) shows the distribution of context values used to generate the uniform pool of tasks for the main experimental; (b) the distribution of context values that is used to generate a harder pool of tasks.</p>
<p>Hyperparameters PointMass-s PointMass-d BasicKarel BallCatching AntGoal Figure 7: We present the hyperparameters of the different curriculum strategies for all five environments. For SPDL, we choose the best performing V LB in the non-binary environments from the following sets: set {1, 3.5, 10, 20, 30, 40} for PointMass-D; set {20, 25, 30, 35, 42.5} for BallCatching; set {50, 100, 200, 300, 400} for AntGoal. For PLR, we choose the best performing pair (β PLR , ρ) for each environment fromProCuRL-env 
β 
20 
10 
10 
10 
10 </p>
<p>N pos 
5120 
5120 
102400 
20480 
81920 </p>
<p>c rollouts 
20 
20 
20 
20 
20 </p>
<p 0_="0," 300="300">{V min , V max } 
n/a {V min,t , V max,t } 
n/a 
n/a </p>
<p>ProCuRL-val 
β 
20 
10 
10 
10 
10 </p>
<p>N pos 
1024 
1024 
2048 
5120 
1024 </p>
<p 0_="0," 300="300">{V min , V max } 
n/a {V min,t , V max,t } 
n/a 
{0, 60} </p>
<p>SPaCE 
η 
0.1 
0.1 
0.5 
0.1 
0.1 </p>
<p>κ 
1 
1 
64 
1 
1 </p>
<p>N pos 
1024 
1024 
2048 
5120 
1024 </p>
<p>SPaCE-alt 
β 
20 
10 
10 
10 
10 </p>
<p>N pos 
1024 
1024 
2048 
5120 
1024 </p>
<p>SPDL 
V LB 
0.5 
3.5 
0.5 
30 
100 </p>
<p>N pos 
1024 
1024 
2048 
5120 
1024 </p>
<p>PLR 
ρ 
0.5 
0.9 
0.9 
0.7 
0.3 </p>
<p>β PLR 
0.1 
0.3 
0.1 
0.3 
0.1 </p>
<p>N pos 
1024 
1024 
2048 
5120 
1024 </p>
<p>the set {0.1, 0.3, 0.5, 0.7, 0.9} × {0.1, 0.3, 0.5, 0.7, 0.9}. </p>
<p>Train Set Test Set Train Set Test SetPerformance (1M) 
Performance (2M) </p>
<p>ProCuRL-env 
0.84 
0.78 
0.92 
0.90 </p>
<p>ProCuRL-val 
0.71 
0.65 
0.91 
0.90 </p>
<p>SPaCE 
0.34 
0.28 
0.65 
0.64 </p>
<p>SPaCE-alt 
0.47 
0.40 
0.82 
0.81 </p>
<p>SPDL 
0.55 
0.48 
0.88 
0.87 </p>
<p>PLR 
0.69 
0.60 
0.88 
0.88 </p>
<p>IID 
0.39 
0.32 
0.90 
0.89 </p>
<p>In this setting, for a given set of contexts C, the pool of tasks is given by {Mc = (S, A, Tc, H, Rc, S init ) : c ∈ C}. Our environment formalism (MDP M) covers this setting as follows: S = S × C; S init = S init × C; T ((s , c)|(s, c), a) = Tc(s |s, a) and R((s, c), a) = Rc(s, a), ∀s,s ∈ S, a ∈ A, c ∈ C.3 For general finite-horizon MDPs, including the time step as part of the state is important. However, to avoid complicating the notation with additional indexing, we have assumed that the time step is implicitly included in the state.
This simple choice leads to competitive performance in a variety of environments used in our experiments. However, the above choice could lead to a suboptimal strategy for specific scenarios, e.g., when all PoS * (s) are below 0.5. It would be interesting to investigate alternative strategies to estimate PoS * (s) during the training process, e.g., using top K% rollouts obtained by executing the current policy πt starting from s.
In this setting, we abstract out the policy π θ and directly map the "parameter" θ to a vector of "performance on tasks" PoS θ . Then, we choose the parameter space as Θ = [0, 1] S init (where d = S init ) and define PoS θ = θ. Thus, an update in the "parameter" θ is equivalent to an update in the "performance on tasks" PoS θ .
AcknowledgmentsParameswaran Kamalaruban acknowledges support from The Alan Turing Institute. Funded/Co-funded by the European Union (ERC, TOPS, 101039090). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.
. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, Hindsight Experience Replay. In NeurIPS. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In NeurIPS, 2017.</p>
<p>Purposive Behavior Acquisition for a Real Robot by Vision-based Reinforcement Learning. Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, Koh Hosoda, Machine learning. 232-3Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, and Koh Hosoda. Purposive Behavior Acquisition for a Real Robot by Vision-based Reinforcement Learning. Machine learning, 23(2-3):279-303, 1996.</p>
<p>Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots. Adrien Baranes, Pierre-Yves Oudeyer, Robotics and Autonomous Systems. 611Adrien Baranes and Pierre-Yves Oudeyer. Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.</p>
<p>Curriculum Learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, ICML. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum Learning. In ICML, 2009.</p>
<p>Handbook of Tables for Probability and Statistics. H William, Beyer, CRC PressWilliam H Beyer. Handbook of Tables for Probability and Statistics. CRC Press, 2019.</p>
<p>Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis. Rudy Bunel, Matthew J Hausknecht, Jacob Devlin, Rishabh Singh, Pushmeet Kohli, In ICLR. Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis. In ICLR, 2018.</p>
<p>The Zone of Proximal Development in Vygotsky's Analysis of Learning and Instruction. Vygotsky's Educational Theory in Cultural Context. Seth Chaiklin, 39Seth Chaiklin. The Zone of Proximal Development in Vygotsky's Analysis of Learning and Instruction. Vygotsky's Educational Theory in Cultural Context, pp. 39, 2003.</p>
<p>Goal-GAN: Multimodal Trajectory Prediction based on Goal Position Estimation. Patrick Dendorfer, Aljosa Osep, Laura Leal-Taixé, ACCV. Patrick Dendorfer, Aljosa Osep, and Laura Leal-Taixé. Goal-GAN: Multimodal Trajectory Prediction based on Goal Position Estimation. In ACCV, 2020.</p>
<p>Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine, NeurIPS. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design. In NeurIPS, 2020.</p>
<p>Goal-conditioned Imitation Learning. Yiming Ding, Carlos Florensa, Pieter Abbeel, Mariano Phielipp, NeurIPS. Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned Imitation Learning. In NeurIPS, 2019.</p>
<p>Self-Paced Context Evaluation for Contextual Reinforcement Learning. Theresa Eimer, André Biedenkapp, Frank Hutter, Marius Lindauer, ICML. 2021Theresa Eimer, André Biedenkapp, Frank Hutter, and Marius Lindauer. Self-Paced Context Evaluation for Contextual Reinforcement Learning. In ICML, 2021.</p>
<p>Learning and Development in Neural Networks: The Importance of Starting Small. Cognition. Jeffrey L Elman, 48Jeffrey L Elman. Learning and Development in Neural Networks: The Importance of Starting Small. Cog- nition, 48(1):71-99, 1993.</p>
<p>Contrastive Learning as Goal-conditioned Reinforcement Learning. Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, Ruslan Salakhutdinov, NeurIPS. 2022Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Ruslan Salakhutdinov. Contrastive Learning as Goal-conditioned Reinforcement Learning. In NeurIPS, 2022.</p>
<p>Reverse Curriculum Generation for Reinforcement Learning. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel, CORL. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse Curriculum Generation for Reinforcement Learning. In CORL, 2017.</p>
<p>Automatic Goal Generation for Reinforcement Learning Agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, ICML. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic Goal Generation for Reinforce- ment Learning Agents. In ICML, 2018.</p>
<p>Automated Curriculum Learning for Neural Networks. Alex Graves, G Marc, Jacob Bellemare, Rémi Menick, Koray Munos, Kavukcuoglu, Alex Graves, Marc G Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu. Automated Curricu- lum Learning for Neural Networks. In ICML, 2017.</p>
<p>Contextual Markov Decision Processes. CoRR. Assaf Hallak, Dotan Di Castro, Shie Mannor, abs/1502.02259Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual Markov Decision Processes. CoRR, abs/1502.02259, 2015.</p>
<p>Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation. Peide Huang, Mengdi Xu, Jiacheng Zhu, Laixi Shi, Fei Fang, Ding Zhao, NeurIPS. 2022Peide Huang, Mengdi Xu, Jiacheng Zhu, Laixi Shi, Fei Fang, and Ding Zhao. Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation. In NeurIPS, 2022.</p>
<p>Self-Paced Curriculum Learning. Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, Alexander G Hauptmann, AAAI. Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-Paced Curriculum Learning. In AAAI, 2015.</p>
<p>Edward Grefenstette, and Tim Rocktäschel. Replay-Guided Adversarial Environment Design. Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, NeurIPS. Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, and Tim Rock- täschel. Replay-Guided Adversarial Environment Design. In NeurIPS, 2021a.</p>
<p>Prioritized Level Replay. Minqi Jiang, Edward Grefenstette, Tim Rocktäschel, ICML. Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized Level Replay. In ICML, 2021b.</p>
<p>Interactive Teaching Algorithms for Inverse Reinforcement Learning. Parameswaran Kamalaruban, Rati Devidze, Volkan Cevher, Adish Singla, IJCAI. Parameswaran Kamalaruban, Rati Devidze, Volkan Cevher, and Adish Singla. Interactive Teaching Algo- rithms for Inverse Reinforcement Learning. In IJCAI, 2019.</p>
<p>A Survey of Generalisation in Deep Reinforcement Learning. Robert Kirk, Amy Zhang, Edward Grefenstette, Tim Rocktäschel, abs/2111.09794CoRRRobert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A Survey of Generalisation in Deep Reinforcement Learning. CoRR, abs/2111.09794, 2021.</p>
<p>Self-Paced Contextual Reinforcement Learning. Pascal Klink, Hany Abdulsamad, Boris Belousov, Jan Peters, CORL. Pascal Klink, Hany Abdulsamad, Boris Belousov, and Jan Peters. Self-Paced Contextual Reinforcement Learning. In CORL, 2020a.</p>
<p>Self-Paced Deep Reinforcement Learning. Pascal Klink, D&apos; Carlo, Jan R Eramo, Joni Peters, Pajarinen, NeurIPS. Pascal Klink, Carlo D'Eramo, Jan R Peters, and Joni Pajarinen. Self-Paced Deep Reinforcement Learning. In NeurIPS, 2020b.</p>
<p>A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. Pascal Klink, Hany Abdulsamad, Boris Belousov, D&apos; Carlo, Jan Eramo, Joni Peters, Pajarinen, Journal of Machine Learning Research. 22Pascal Klink, Hany Abdulsamad, Boris Belousov, Carlo D'Eramo, Jan Peters, and Joni Pajarinen. A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. Journal of Machine Learning Research, 22:182-1, 2021.</p>
<p>Curriculum Reinforcement Learning via Constrained Optimal Transport. Pascal Klink, Haoyi Yang, Carlo D Eramo, Jan Peters, Joni Pajarinen, ICML. 2022Pascal Klink, Haoyi Yang, Carlo D'Eramo, Jan Peters, and Joni Pajarinen. Curriculum Reinforcement Learning via Constrained Optimal Transport. In ICML, 2022.</p>
<p>Self-Paced Learning for Latent Variable Models. Benjamin M Pawan Kumar, Daphne Packer, Koller, NeurIPS. M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-Paced Learning for Latent Variable Models. In NeurIPS, 2010.</p>
<p>End-to-end Training of Deep Visuomotor Policies. Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel, Journal of Machine Learning Research. 171Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334-1373, 2016.</p>
<p>. P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, abs/1509.02971Continuous Control with Deep Reinforcement Learning. CoRR. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Sil- ver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR, abs/1509.02971, 2015.</p>
<p>X Lin, H Baweja, D Held, Reinforcement Learning without Ground-Truth State. ICML'19 Workshop on Multi-Task and Lifelong Reinforcement Learning. X. Lin, H. Baweja, and D. Held. Reinforcement Learning without Ground-Truth State. ICML'19 Workshop on Multi-Task and Lifelong Reinforcement Learning, 2019.</p>
<p>Iterative Machine Teaching. Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, M James, Le Rehg, Song, Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and Le Song. Iterative Machine Teaching. In ICML, 2017.</p>
<p>Teacher-Student Curriculum Learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, IEEE Transactions on Neural Networks and Learning Systems. 319Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-Student Curriculum Learning. IEEE Transactions on Neural Networks and Learning Systems, 31(9):3732-3740, 2019.</p>
<p>Human-Level Control Through Deep Reinforcement Learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, Nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-Level Control Through Deep Reinforcement Learning. Nature, 518(7540):529-533, 2015.</p>
<p>Learning Curriculum Policies for Reinforcement Learning. Sanmit Narvekar, Peter Stone, AAMAS. Sanmit Narvekar and Peter Stone. Learning Curriculum Policies for Reinforcement Learning. In AAMAS, 2019.</p>
<p>Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning. Sanmit Narvekar, Jivko Sinapov, Peter Stone, IJCAI. Sanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning. In IJCAI, 2017.</p>
<p>Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, E Matthew, Peter Taylor, Stone, Journal of Machine Learning Research. 21Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey. Journal of Machine Learning Research, 21:1-50, 2020.</p>
<p>Intrinsic Motivation Systems for Autonomous Mental Development. Pierre-Yves Oudeyer, Frdric Kaplan, Verena V Hafner, IEEE Transactions on Evolutionary Computation. 112Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic Motivation Systems for Autonomous Mental Development. IEEE Transactions on Evolutionary Computation, 11(2):265-286, 2007.</p>
<p>Evolving Curricula with Regret-Based Environment Design. Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rocktäschel, abs/2203.01302CoRRJack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, and Tim Rocktäschel. Evolving Curricula with Regret-Based Environment Design. CoRR, abs/2203.01302, 2022.</p>
<p>Automatic Curriculum Learning for Deep RL: A Short Survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, IJCAI. 2021Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Automatic Curriculum Learning for Deep RL: A Short Survey. In IJCAI, 2021.</p>
<p>Automated Curricula Through Setter-Solver Interactions. Sébastien Racanière, K Andrew, Adam Lampinen, Santoro, P David, Vlad Reichert, Timothy P Firoiu, Lillicrap, ICLR. Sébastien Racanière, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, and Timothy P Lillicrap. Automated Curricula Through Setter-Solver Interactions. In ICLR, 2020.</p>
<p>Stable-Baselines3: Reliable Reinforcement Learning Implementations. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, Noah Dormann, Journal of Machine Learning Research. 22268Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Re- search, 22(268):1-8, 2021.</p>
<p>Man muss immer umkehren. Cinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, Joan Bruna Backplay, abs/1807.06919Cinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, and Joan Bruna. Backplay:" Man muss immer umkehren". CoRR, abs/1807.06919, 2018.</p>
<p>Learning by Playing Solving Sparse Reward Tasks from Scratch. A Martin, Roland Riedmiller, Thomas Hafner, Michael Lampe, Jonas Neunert, Tom Degrave, Vlad Van De Wiele, Nicolas Mnih, Jost Tobias Heess, Springenberg, ICML. Martin A Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by Playing Solving Sparse Reward Tasks from Scratch. In ICML, 2018.</p>
<p>Learning Montezuma's Revenge from a Single Demonstration. Tim Salimans, Richard Chen, abs/1812.03381CoRRTim Salimans and Richard Chen. Learning Montezuma's Revenge from a Single Demonstration. CoRR, abs/1812.03381, 2018.</p>
<p>Powerplay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem. Jürgen Schmidhuber, Frontiers in Psychology. 4313Jürgen Schmidhuber. Powerplay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem. Frontiers in Psychology, 4:313, 2013.</p>
<p>Proximal Policy Optimization Algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.06347CoRRJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Opti- mization Algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Mastering the Game of Go Without Human Knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Nature. 5507676David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the Game of Go Without Human Knowledge. Nature, 550(7676):354-359, 2017.</p>
<p>Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data. Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, Jeffrey Clune, ICML. Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, and Jeffrey Clune. Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data. In ICML, 2020.</p>
<p>Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Robert Fergus, In ICLR. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Robert Fergus. Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. In ICLR, 2018.</p>
<p>Policy Gradient Methods for Reinforcement Learning with Function Approximation. S Richard, David Sutton, Satinder Mcallester, Yishay Singh, Mansour, NeurIPS. Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In NeurIPS, 1999.</p>
<p>Mujoco: A Physics Engine for Model-based Control. Emanuel Todorov, Tom Erez, Yuval Tassa, IROS. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-based Control. In IROS, 2012.</p>
<p>Safe Reinforcement Learning via Curriculum Induction. Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, Alekh Agarwal, NeurIPS. Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe Reinforcement Learning via Curriculum Induction. In NeurIPS, 2020.</p>
<p>Mind in Society: Development of Higher Psychological Processes. Lev Semenovich Vygotsky, Michael Cole, Harvard University PressLev Semenovich Vygotsky and Michael Cole. Mind in Society: Development of Higher Psychological Pro- cesses. Harvard University Press, 1978.</p>
<p>Theory of Curriculum Learning with Convex Loss Functions. Daphna Weinshall, Dan Amir, abs/1812.03472CoRRDaphna Weinshall and Dan Amir. Theory of Curriculum Learning with Convex Loss Functions. CoRR, abs/1812.03472, 2018.</p>
<p>Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks. Daphna Weinshall, Gad Cohen, Dan Amir, ICML. Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks. In ICML, 2018.</p>
<p>Curriculum for Reinforcement Learning. lilianweng.github.io. Lilian Weng, Lilian Weng. Curriculum for Reinforcement Learning. lilianweng.github.io, 2020. URL https:// lilianweng.github.io/posts/2020-01-29-curriculum-rl/.</p>
<p>A Performance-Based Start State Curriculum Framework for Reinforcement Learning. Jan Wöhlke, Felix Schmitt, Herke Van Hoof, AAMAS. Jan Wöhlke, Felix Schmitt, and Herke van Hoof. A Performance-Based Start State Curriculum Framework for Reinforcement Learning. In AAMAS, 2020.</p>
<p>Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning. Yuxin Wu, Yuandong Tian, ICLR. Yuxin Wu and Yuandong Tian. Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning. In ICLR, 2016.</p>
<p>. Scott Cheng-Hsin, Yue Yang, Yu, Pei Givchi, Wai Keen Wang, Patrick Vong, Shafto, Optimal Cooperative Inference. In AISTATS. Scott Cheng-Hsin Yang, Yue Yu, arash Givchi, Pei Wang, Wai Keen Vong, and Patrick Shafto. Optimal Cooperative Inference. In AISTATS, 2018.</p>
<p>Curriculum Design for Teaching via Demonstrations: Theory and Applications. Rati Gaurav Raju Yengera, Parameswaran Devidze, Adish Kamalaruban, Singla, NeurIPS. 2021Gaurav Raju Yengera, Rati Devidze, Parameswaran Kamalaruban, and Adish Singla. Curriculum Design for Teaching via Demonstrations: Theory and Applications. In NeurIPS, 2021.</p>
<p>Learning to Execute. CoRR, abs/1410. Wojciech Zaremba, Ilya Sutskever, 4615Wojciech Zaremba and Ilya Sutskever. Learning to Execute. CoRR, abs/1410.4615, 2014.</p>
<p>Automatic Curriculum Learning Through Value Disagreement. Yunzhi Zhang, Pieter Abbeel, Lerrel Pinto, NeurIPS. Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic Curriculum Learning Through Value Disagree- ment. In NeurIPS, 2020.</p>
<p>Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity. Tianyi Zhou, Jeff Bilmes, In ICLR. Tianyi Zhou and Jeff Bilmes. Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity. In ICLR, 2018.</p>
<p>Curriculum Learning by Optimizing Learning Dynamics. Tianyi Zhou, Shengjie Wang, Jeff Bilmes, AISTATS. 2021Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Curriculum Learning by Optimizing Learning Dynamics. In AISTATS, 2021.</p>
<p>An Overview of Machine Teaching. Xiaojin Zhu, Adish Singla, Sandra Zilles, Anna N Rafferty, abs/1801.05927CoRRXiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Rafferty. An Overview of Machine Teaching. CoRR, abs/1801.05927, 2018.</p>
<p>Towards Helping Teachers Select Optimal Content for Students. Xiaotian Zou, Wei Ma, Zhenjun Ma, Ryan S Baker, AIED. Xiaotian Zou, Wei Ma, Zhenjun Ma, and Ryan S Baker. Towards Helping Teachers Select Optimal Content for Students. In AIED, 2019.</p>
<p>For ProCuRL-env, we set N pos higher than N steps since obtaining rollouts to update V t (s) is expensive. For all the other curriculum strategies, we set N pos = N steps . For SPaCE, N pos controls how frequently the current task dataset is updated based on their curriculum. For SPDL, N pos controls how often we perform the optimization step to update the distribution for selecting tasks. N pos parameter controls the frequency at which V t is updatedN pos parameter controls the frequency at which V t is updated. For ProCuRL-env, we set N pos higher than N steps since obtaining rollouts to update V t (s) is expensive. For all the other curriculum strategies, we set N pos = N steps . For SPaCE, N pos controls how frequently the current task dataset is updated based on their curriculum. For SPDL, N pos controls how often we perform the optimization step to update the distribution for selecting tasks.</p>
<p>c rollouts determines the number of additional rollouts required to compute the probability of success score for each task (only for ProCuRL-env). c rollouts determines the number of additional rollouts required to compute the probability of success score for each task (only for ProCuRL-env).</p>
<p>V max } are used in the environments with non-binary or dense rewards to obtain the normalized values V (s) (see Section 3.3). {V min. In Figure 7, {V min,t , V max,t } denote the min-max values of the critic for states S. init at step t{V min , V max } are used in the environments with non-binary or dense rewards to obtain the normalized values V (s) (see Section 3.3). In Figure 7, {V min,t , V max,t } denote the min-max values of the critic for states S init at step t.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3871 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3871</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3871</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-268247854</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.02839v4.pdf" target="_blank">An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge models based on open-source LLMs for evaluation. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of LLM-as-a-Judge. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness and adaptability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3871.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3871.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned judge models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned LLM-based judge models (e.g., JudgeLM, PandaLM, Auto-J, Prometheus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source foundation models fine-tuned on judge/evaluation datasets to act as automated evaluators of LLM outputs; optimized on specific annotation schemes and benchmarks and evaluated across multiple meta-evaluation datasets in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge (fine-tuned judge models validated on multiple meta-evaluation benchmarks and cross-scheme tests)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>General LLM response evaluation (pairwise selection, pointwise grading, multi-turn evaluation, bias/adversarial and aspect-specific evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Paper reports these fine-tuned judges achieve high accuracy on their own in-domain test sets (e.g., JudgeLM-7B JudgeLM-test accuracy 82.39, F1 72.97; Auto-J-13B Auto-J-test accuracy 77.79, F1 62.64) and in some in-domain cases comparable to or close to GPT-4, but the paper does not report a single clear numeric LLM-vs-human agreement percentage across all settings; it states proprietary LLMs (e.g., GPT-4) have "high agreement with human evaluators" but gives no unified human-agreement rate in text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Fine-tuned judges often lose general instruction-following and flexible evaluation capabilities; they behave like task-specific classifiers tied to the prompt/training scheme, rely on superficial features (verbosity/format), and do not generalize to unseen evaluation schemes or aspect-specific instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited to their fine-tuning/evaluation scheme (cross-scheme performance drops sharply); severely biased toward superficial quality (favoring verbose or well-formatted but incorrect answers); incapable of reliable aspect-specific evaluation (helpfulness, factuality, safety, toxicity); fail to benefit from prompt-engineering strategies (CoT/ICL) and sometimes degrade under those strategies; overfitting to fine-tuning data and degenerating into a task-specific classifier; high similarity/correlation among different fine-tuned classifiers but divergence from GPT-4 judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Strength: very strong in-domain performance on the datasets they were fine-tuned on (sometimes matching or slightly surpassing closed-source models on those specific testsets); computationally open/controllable and reproducible (privacy and reproducibility advantages compared to closed APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use with caution: limit use to scenarios with substantial overlap to their fine-tuning data or schemes; monitor domain/task overlap with fine-tuning; do not treat them as universal substitutes for GPT-4; consider combining with or validating against stronger proprietary judges or human evaluation for out-of-domain or aspect-specific assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3871.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3871.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comparison on adversarial bias (LLMBar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMBar adversarial bias benchmark results (Neig., Manu., GPTO., GPTI. splits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of judge models on LLMBar shows pronounced bias of fine-tuned judges toward superficial answer quality; GPT-4 performs substantially better and is less biased.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Bias and adversarial testsets (LLMBar) comparing fine-tuned judges versus GPT-4 on paired outputs where an incorrect answer has better superficial quality than a correct answer</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Bias/fairness in evaluator judgments (preference for superficial features over correctness/instruction-following)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (as reference in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported per-model accuracies on LLMBar subsets (examples from Table 4): JudgeLM-7B: [overall subset numbers shown e.g., 62.0 / 23.1 / 26.1 / 46.8 / 28.3 across splits]; PandaLM-7B and Auto-J-13B similarly low on adversarial splits; GPT-4-1106: e.g., 93.5 / 64.2 / 76.6 / 76.6 / 75.0. (Paper shows fine-tuned judges perform very poorly on adversarial subsets, sometimes worse than random.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Fine-tuned judges over-weight superficial features (verbosity, fluency, format) and can prefer incorrect but superficially stronger outputs; GPT-4 is less reliant on such superficial cues and preserves preference for correctness/instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuned judges can fail catastrophically on adversarial examples that decouple superficial quality and correctness, exhibiting accuracy below reasonable baselines and sometimes approaching random-guess levels on those splits.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>None reported where fine-tuned judges outperform humans on these adversarial bias tests; GPT-4 shows substantially higher robustness on LLMBar.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not trust fine-tuned judge outputs on adversarial or bias-sensitive datasets without human validation; include adversarial tests (like LLMBar) in meta-evaluation pipelines to detect superficiality bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3871.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3871.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aspect-specific evaluation (HaluEval, ToxicChat, SALAD-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained aspect evaluations on factuality (HaluEval), toxicity (ToxicChat), and safety (SALAD-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper evaluates judge models on fine-grained aspect benchmarks and finds fine-tuned judges perform substantially worse than GPT-4, often failing to learn correlations between aspects and evaluation results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge performing aspect-specific scoring (factuality, toxicity, safety) using aspect-specific prompts or original prompts</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Factuality detection, toxicity detection, safety evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (as reference in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Reported model accuracies (examples from Table 3): Auto-J-13B on HaluEval-QA ~58.30 accuracy; Prometheus-7B HaluEval-QA ~47.90; GPT-4-1106 HaluEval-QA ~72.50 (paper shows GPT-4 substantially higher accuracy on several aspect datasets). No single human-LLM agreement aggregate is given; numbers above are evaluator-vs-benchmark accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Fine-tuned judges lack reliable aspect-specific discernment (they miss factual errors, toxicity/safety nuances) and do not improve when provided aspect-specific prompts, indicating loss of general instruction-understanding ability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fail to perform fine-grained aspect-wise evaluation; Prometheus — despite being designed for fine-grained evaluation — obtains inferior results, indicating failure to learn aspect correlations; aspect-specific prompts often have little effect on these fine-tuned judges.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Fine-tuned judges sometimes achieve moderate accuracy on some in-domain aspect splits, but overall they lag behind GPT-4 on aspect-specific benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not rely solely on fine-tuned judges for aspect-specific evaluation; validate with human annotators or stronger judges (e.g., GPT-4) for safety/factuality/toxicity assessments; perform threshold tuning when converting scores to binary labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3871.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3871.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting strategies (CoT and ICL) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of chain-of-thought (CoT) and in-context learning (ICL) prompting on evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper compares how prompting strategies (CoT, ICL) affect fine-tuned judges versus proprietary models: GPT-3.5/GPT-4 benefit substantially, while fine-tuned judges rarely benefit and may degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Evaluation with/without CoT and ICL across judge models, including fine-tuned judges and GPT variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Meta-evaluation performance improvement via prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5, GPT-4 (used as comparisons in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Quantitative effects shown in tables: e.g., JudgeLM-7B w/o CoT JudgeLM-test accuracy 82.74 (F1 72.64) ; with original CoT 81.68 (71.59); with ICL 68.57 (58.52) — demonstrating degradation under ICL; GPT-3.5/GPT-4 gain meaningful improvements with CoT/ICL in the paper's experiments (e.g., GPT-3.5 and GPT-4 accuracy/PCC improvements cited).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Proprietary models can incorporate CoT/ICL to improve evaluation judgments, while fine-tuned judges frequently ignore CoT instructions or stick to their learned output format, showing lost instruction-following flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fine-tuned judges sometimes cannot produce chain-of-thought even when prompted and can suffer sizeable performance drops when CoT/ICL is applied; CoT-annotated fine-tuning (even from high-quality CoT sources) degraded performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Proprietary models (GPT-3.5/GPT-4) show clear improvements from CoT and ICL, demonstrating their adaptability to prompting strategies — a strength relative to fine-tuned judges.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Avoid relying on CoT/ICL to fix limitations of fine-tuned judges; if CoT/ICL is desired, prefer using proprietary/generalist models that retain instruction-following and flexible prompting abilities; be wary of fine-tuning that hard-codes output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3871.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3871.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-specific classifier phenomenon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degeneration of fine-tuned judge models into task-specific classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>After supervised fine-tuning on a single task/scheme, foundation models become overfit task-specific classifiers that lose generalization, instruction-following, and benefit from generative capabilities or prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Analysis of fine-tuned vs. classification-formulated judges, cross-model correlation analyses (F1 and Pearson between model predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Meta-evaluation model behavior and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Paper reports high mutual agreement/correlation among different classification-style fine-tuned judges (F1 and Pearson correlations shown in Figures 2 and 3), and notably lower correlation between those classifiers and GPT-4; no single human agreement metric given here for this phenomenon.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Classification-style judges produce similar outputs to each other (high inter-classifier correlation) but differ systematically from GPT-4 judgments; models lose generative/instruction-generalization ability and behave like narrow classifiers tied to their supervision signals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Overfitting to supervision leads to inability to adapt to new evaluation schemes or novel prompts; generative capability of LLMs does not translate to improved evaluation quality once fine-tuned into classifier form; encoder-only classifiers (e.g., DeBERTa) can match LLM-based classifiers on these tasks, implying the power comes from the supervision not generative modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Classifier form can be efficient and attain strong in-domain accuracy; DeBERTa-classification attained comparable performance to LLM-based classifiers on the studied testsets, showing that simpler architectures can suffice for narrow judge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Avoid over-reliance on a single-task fine-tuned judge for general evaluation; increase training diversity if attempting to make a generalist judge, and validate cross-scheme and aspect-specific performance; use human evaluation or robust proprietary judges for tasks beyond the fine-tuning scope.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>JudgeLM : Fine-tuned large language models are scalable judges <em>(Rating: 2)</em></li>
                <li>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Prometheus: Inducing finegrained evaluation capability in language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3871",
    "paper_id": "paper-268247854",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "Fine-tuned judge models",
            "name_full": "Fine-tuned LLM-based judge models (e.g., JudgeLM, PandaLM, Auto-J, Prometheus)",
            "brief_description": "Open-source foundation models fine-tuned on judge/evaluation datasets to act as automated evaluators of LLM outputs; optimized on specific annotation schemes and benchmarks and evaluated across multiple meta-evaluation datasets in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge (fine-tuned judge models validated on multiple meta-evaluation benchmarks and cross-scheme tests)",
            "task_or_domain": "General LLM response evaluation (pairwise selection, pointwise grading, multi-turn evaluation, bias/adversarial and aspect-specific evaluation)",
            "llm_model_name": null,
            "agreement_rate": "Paper reports these fine-tuned judges achieve high accuracy on their own in-domain test sets (e.g., JudgeLM-7B JudgeLM-test accuracy 82.39, F1 72.97; Auto-J-13B Auto-J-test accuracy 77.79, F1 62.64) and in some in-domain cases comparable to or close to GPT-4, but the paper does not report a single clear numeric LLM-vs-human agreement percentage across all settings; it states proprietary LLMs (e.g., GPT-4) have \"high agreement with human evaluators\" but gives no unified human-agreement rate in text.",
            "qualitative_differences": "Fine-tuned judges often lose general instruction-following and flexible evaluation capabilities; they behave like task-specific classifiers tied to the prompt/training scheme, rely on superficial features (verbosity/format), and do not generalize to unseen evaluation schemes or aspect-specific instructions.",
            "limitations_or_failure_cases": "Limited to their fine-tuning/evaluation scheme (cross-scheme performance drops sharply); severely biased toward superficial quality (favoring verbose or well-formatted but incorrect answers); incapable of reliable aspect-specific evaluation (helpfulness, factuality, safety, toxicity); fail to benefit from prompt-engineering strategies (CoT/ICL) and sometimes degrade under those strategies; overfitting to fine-tuning data and degenerating into a task-specific classifier; high similarity/correlation among different fine-tuned classifiers but divergence from GPT-4 judgments.",
            "counterexamples_or_strengths": "Strength: very strong in-domain performance on the datasets they were fine-tuned on (sometimes matching or slightly surpassing closed-source models on those specific testsets); computationally open/controllable and reproducible (privacy and reproducibility advantages compared to closed APIs).",
            "recommendations_or_best_practices": "Use with caution: limit use to scenarios with substantial overlap to their fine-tuning data or schemes; monitor domain/task overlap with fine-tuning; do not treat them as universal substitutes for GPT-4; consider combining with or validating against stronger proprietary judges or human evaluation for out-of-domain or aspect-specific assessments.",
            "citation": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)",
            "uuid": "e3871.0",
            "source_info": {
                "paper_title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Comparison on adversarial bias (LLMBar)",
            "name_full": "LLMBar adversarial bias benchmark results (Neig., Manu., GPTO., GPTI. splits)",
            "brief_description": "Evaluation of judge models on LLMBar shows pronounced bias of fine-tuned judges toward superficial answer quality; GPT-4 performs substantially better and is less biased.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Bias and adversarial testsets (LLMBar) comparing fine-tuned judges versus GPT-4 on paired outputs where an incorrect answer has better superficial quality than a correct answer",
            "task_or_domain": "Bias/fairness in evaluator judgments (preference for superficial features over correctness/instruction-following)",
            "llm_model_name": "GPT-4 (as reference in paper)",
            "agreement_rate": "Reported per-model accuracies on LLMBar subsets (examples from Table 4): JudgeLM-7B: [overall subset numbers shown e.g., 62.0 / 23.1 / 26.1 / 46.8 / 28.3 across splits]; PandaLM-7B and Auto-J-13B similarly low on adversarial splits; GPT-4-1106: e.g., 93.5 / 64.2 / 76.6 / 76.6 / 75.0. (Paper shows fine-tuned judges perform very poorly on adversarial subsets, sometimes worse than random.)",
            "qualitative_differences": "Fine-tuned judges over-weight superficial features (verbosity, fluency, format) and can prefer incorrect but superficially stronger outputs; GPT-4 is less reliant on such superficial cues and preserves preference for correctness/instruction-following.",
            "limitations_or_failure_cases": "Fine-tuned judges can fail catastrophically on adversarial examples that decouple superficial quality and correctness, exhibiting accuracy below reasonable baselines and sometimes approaching random-guess levels on those splits.",
            "counterexamples_or_strengths": "None reported where fine-tuned judges outperform humans on these adversarial bias tests; GPT-4 shows substantially higher robustness on LLMBar.",
            "recommendations_or_best_practices": "Do not trust fine-tuned judge outputs on adversarial or bias-sensitive datasets without human validation; include adversarial tests (like LLMBar) in meta-evaluation pipelines to detect superficiality bias.",
            "citation": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)",
            "uuid": "e3871.1",
            "source_info": {
                "paper_title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Aspect-specific evaluation (HaluEval, ToxicChat, SALAD-Bench)",
            "name_full": "Fine-grained aspect evaluations on factuality (HaluEval), toxicity (ToxicChat), and safety (SALAD-Bench)",
            "brief_description": "Paper evaluates judge models on fine-grained aspect benchmarks and finds fine-tuned judges perform substantially worse than GPT-4, often failing to learn correlations between aspects and evaluation results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge performing aspect-specific scoring (factuality, toxicity, safety) using aspect-specific prompts or original prompts",
            "task_or_domain": "Factuality detection, toxicity detection, safety evaluation",
            "llm_model_name": "GPT-4 (as reference in paper)",
            "agreement_rate": "Reported model accuracies (examples from Table 3): Auto-J-13B on HaluEval-QA ~58.30 accuracy; Prometheus-7B HaluEval-QA ~47.90; GPT-4-1106 HaluEval-QA ~72.50 (paper shows GPT-4 substantially higher accuracy on several aspect datasets). No single human-LLM agreement aggregate is given; numbers above are evaluator-vs-benchmark accuracy.",
            "qualitative_differences": "Fine-tuned judges lack reliable aspect-specific discernment (they miss factual errors, toxicity/safety nuances) and do not improve when provided aspect-specific prompts, indicating loss of general instruction-understanding ability.",
            "limitations_or_failure_cases": "Fail to perform fine-grained aspect-wise evaluation; Prometheus — despite being designed for fine-grained evaluation — obtains inferior results, indicating failure to learn aspect correlations; aspect-specific prompts often have little effect on these fine-tuned judges.",
            "counterexamples_or_strengths": "Fine-tuned judges sometimes achieve moderate accuracy on some in-domain aspect splits, but overall they lag behind GPT-4 on aspect-specific benchmarks.",
            "recommendations_or_best_practices": "Do not rely solely on fine-tuned judges for aspect-specific evaluation; validate with human annotators or stronger judges (e.g., GPT-4) for safety/factuality/toxicity assessments; perform threshold tuning when converting scores to binary labels.",
            "citation": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)",
            "uuid": "e3871.2",
            "source_info": {
                "paper_title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Prompting strategies (CoT and ICL) comparison",
            "name_full": "Effects of chain-of-thought (CoT) and in-context learning (ICL) prompting on evaluators",
            "brief_description": "Paper compares how prompting strategies (CoT, ICL) affect fine-tuned judges versus proprietary models: GPT-3.5/GPT-4 benefit substantially, while fine-tuned judges rarely benefit and may degrade.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Evaluation with/without CoT and ICL across judge models, including fine-tuned judges and GPT variants",
            "task_or_domain": "Meta-evaluation performance improvement via prompt engineering",
            "llm_model_name": "GPT-3.5, GPT-4 (used as comparisons in experiments)",
            "agreement_rate": "Quantitative effects shown in tables: e.g., JudgeLM-7B w/o CoT JudgeLM-test accuracy 82.74 (F1 72.64) ; with original CoT 81.68 (71.59); with ICL 68.57 (58.52) — demonstrating degradation under ICL; GPT-3.5/GPT-4 gain meaningful improvements with CoT/ICL in the paper's experiments (e.g., GPT-3.5 and GPT-4 accuracy/PCC improvements cited).",
            "qualitative_differences": "Proprietary models can incorporate CoT/ICL to improve evaluation judgments, while fine-tuned judges frequently ignore CoT instructions or stick to their learned output format, showing lost instruction-following flexibility.",
            "limitations_or_failure_cases": "Fine-tuned judges sometimes cannot produce chain-of-thought even when prompted and can suffer sizeable performance drops when CoT/ICL is applied; CoT-annotated fine-tuning (even from high-quality CoT sources) degraded performance in experiments.",
            "counterexamples_or_strengths": "Proprietary models (GPT-3.5/GPT-4) show clear improvements from CoT and ICL, demonstrating their adaptability to prompting strategies — a strength relative to fine-tuned judges.",
            "recommendations_or_best_practices": "Avoid relying on CoT/ICL to fix limitations of fine-tuned judges; if CoT/ICL is desired, prefer using proprietary/generalist models that retain instruction-following and flexible prompting abilities; be wary of fine-tuning that hard-codes output formats.",
            "citation": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)",
            "uuid": "e3871.3",
            "source_info": {
                "paper_title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Task-specific classifier phenomenon",
            "name_full": "Degeneration of fine-tuned judge models into task-specific classifiers",
            "brief_description": "After supervised fine-tuning on a single task/scheme, foundation models become overfit task-specific classifiers that lose generalization, instruction-following, and benefit from generative capabilities or prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Analysis of fine-tuned vs. classification-formulated judges, cross-model correlation analyses (F1 and Pearson between model predictions)",
            "task_or_domain": "Meta-evaluation model behavior and generalization",
            "llm_model_name": null,
            "agreement_rate": "Paper reports high mutual agreement/correlation among different classification-style fine-tuned judges (F1 and Pearson correlations shown in Figures 2 and 3), and notably lower correlation between those classifiers and GPT-4; no single human agreement metric given here for this phenomenon.",
            "qualitative_differences": "Classification-style judges produce similar outputs to each other (high inter-classifier correlation) but differ systematically from GPT-4 judgments; models lose generative/instruction-generalization ability and behave like narrow classifiers tied to their supervision signals.",
            "limitations_or_failure_cases": "Overfitting to supervision leads to inability to adapt to new evaluation schemes or novel prompts; generative capability of LLMs does not translate to improved evaluation quality once fine-tuned into classifier form; encoder-only classifiers (e.g., DeBERTa) can match LLM-based classifiers on these tasks, implying the power comes from the supervision not generative modeling.",
            "counterexamples_or_strengths": "Classifier form can be efficient and attain strong in-domain accuracy; DeBERTa-classification attained comparable performance to LLM-based classifiers on the studied testsets, showing that simpler architectures can suffice for narrow judge tasks.",
            "recommendations_or_best_practices": "Avoid over-reliance on a single-task fine-tuned judge for general evaluation; increase training diversity if attempting to make a generalist judge, and validate cross-scheme and aspect-specific performance; use human evaluation or robust proprietary judges for tasks beyond the fine-tuning scope.",
            "citation": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4 (2025)",
            "uuid": "e3871.4",
            "source_info": {
                "paper_title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "JudgeLM : Fine-tuned large language models are scalable judges",
            "rating": 2,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        },
        {
            "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
            "rating": 2,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_at_evaluating_instruction_following"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Prometheus: Inducing finegrained evaluation capability in language models",
            "rating": 2,
            "sanitized_title": "prometheus_inducing_finegrained_evaluation_capability_in_language_models"
        }
    ],
    "cost": 0.0146045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4
30 May 2025</p>
<p>Hui Huang huanghui@stu.hit.edu.cn 
Faculty of Computing
Harbin Institute of Technology
HarbinChina</p>
<p>Xingyuan Bu xingyuanbu@gmail.com 
School of Computer Science
Beijing Institute of Technology
BeijingChina</p>
<p>Hongli Zhou 
Faculty of Computing
Harbin Institute of Technology
HarbinChina</p>
<p>Yingqi Qu 
Baidu Inc
BeijingChina</p>
<p>Jing Liu 
Baidu Inc
BeijingChina</p>
<p>Muyun Yang yangmuyun@hit.edu.cn 
Faculty of Computing
Harbin Institute of Technology
HarbinChina</p>
<p>Bing Xu 
Faculty of Computing
Harbin Institute of Technology
HarbinChina</p>
<p>Tiejun Zhao 
Faculty of Computing
Harbin Institute of Technology
HarbinChina</p>
<p>An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4
30 May 20254BF8535840FEFA9F13D618F1CDC24AB2arXiv:2403.02839v4[cs.CL]
Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs.Many studies have fine-tuned judge models based on opensource LLMs for evaluation.While the finetuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of LLM-as-a-Judge.Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness and adaptability.We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations 1 .</p>
<p>Introduction</p>
<p>Recently, the evaluation for Large-scale Language Models (LLMs) has drawn significant attention (Liang et al., 2022;Chang et al., 2023;He et al., 2024;Gu et al., 2025;He et al., 2025).Some research has proposed LLM-as-a-Judge (Li et al., 2023b;Zheng et al., 2023), namely utilizing proprietary LLMs, especially GPT-4 (Achiam et al., 2023), to evaluate the LLM's response.By defining evaluation schemes in the prompt template, proprietary LLMs can provide an accurate evaluation with high agreement with human evaluators.</p>
<p>However, relying on external API for evaluation may introduce consideration about privacy leakage, and the opacity of API models also challenges the evaluation reproducibility.To address these issues, several fine-tuned judge models are proposed (Zhu et al., 2024;Wang et al., 2024;Ke et al., 2024), relying on open-source foundation models and data constructed from either GPT-4 or human annotation, as shown in Figure 1.These models are validated on their respective meta-evaluation benchmarks, where the finetuned models exhibit performance on par with GPT-3.5 and GPT-4, leading to the affirmation of their evaluation capability.</p>
<p>In this paper, we conduct an empirical study for the evaluation capability of judge models.Experiment results indicate that while the fine-tuned judge models achieve superior accuracy on their respective in-domain test sets, they still exhibit limitations compared with close-sourced proprietary models:</p>
<p>• The fine-tuned judge model is constrained by specific evaluation scheme;</p>
<p>• The fine-tuned judge model is biased towards superficial quality;</p>
<p>• The fine-tuned judge model is incapable of aspect-specific evaluation;</p>
<p>• The fine-tuned judge model can not benefit from prompting strategies;</p>
<p>We argue that these limitations primarily stem from the fine-tuning process, where the foundation model is transformed into a task-specific classifier overfitted to the fine-tuning data.To draw a conclusion, the fine-tuned judge model cannot serve as a general substitute for GPT-4 in terms of LLM evaluation.It is advisable to exercise caution when leveraging them for evaluation in real applications, watching for the overlap between the evaluation scenario and the fine-tuning process.</p>
<p>2 How Far can Fine-tuned Judges Go?</p>
<p>In this section, we make a comprehensive empirical study based on four representative fine-tuned judge models in Table 1 1 , and reveal there exist several limitations about their evaluation capabilities.</p>
<p>Constrained by Evaluation Scheme</p>
<p>One of the most appealing attributes of LLMs is their generalization ability, enabling them to execute various tasks defined by various instructions (Zhu et al., 2023).Under the case of LLM evaluation, the instruction can also be formed in various schemes: pairwise selection, pointwise grading, chain-of-thought evaluation, etc.Since different judge models are fine-tuned on different schemes, we would like to verify their capability on uncovered schemes.Specifically, we apply their pub-licly released checkpoints, and cross-validate the judge models on each other's testsets.We also validate the models on MT-bench (Zheng et al., 2023), which is a multi-turn meta-evaluation dataset.</p>
<p>As shown in Table 2, all four models perform the best on their own training schemes, respectively, with results comparable with GPT-4.However, if we employ a model on an evaluation scheme where it is not trained, the evaluation performance would drop by a large margin.On the contrary, close-sourced proprietary models such as GPT-3.5 or GPT-4 consistently exhibit superior performance across various evaluation schemes.</p>
<p>Biased Towards Superficial Quality</p>
<p>Recently, there has been a lot of research on the bias of LLM-based evaluators, namely the evaluator would favor more verbose answers, or answers with similar format (Wang et al., 2023b;Saito et al., 2023).Subsequently, Zeng et al. (2023) proposed LLMBar as a testbed for the fairness of evaluators.It comprises four adversarial testsets (Neig., Manu., GPTO., GPTI.) with paired outputs of a correct answer and an incorrect answer with better superficial quality (e.g., more fluent, more verbose, etc.).</p>
<p>We evaluate the judge models on LLMBar.As shown in Table 4, the fine-tuned judge models perform poorly on adversarial testsets, even worse than random-guess.This notifies that they are severely biased toward superficial quality such as formality or verbosity, while neglecting crucial properties such as instruction following, resulting in the preference for incorrect answers.On the other hand, GPT-4 does not over-rely on the superficial features and achieves decent accuracy on LLMBar.</p>
<p>Incapable of Aspect-specific Evaluation</p>
<p>LLM evaluation covers various aspects such as helpfulness, safety, etc.In this part, we would like to assess the evaluation capability of judge models on fine-grained aspects, based on the following datasets: 1) HaluEval (Li et al., 2023a): for factuality evaluation; 2) ToxicChat (Lin et al., 2023): for toxicity evaluation; 3) SALAD-Bench (Li et al., 2024b): for safety evaluation.</p>
<p>As can be seen from Table 3, the fine-tuned judges fall far behind on all fine-grained aspects.It deserves to notice that while Prometheus is designed for fine-grained evaluation, it obtains an inferior performance on both benchmarks, which notifies that it failed to learn the correlation between fine-grained aspects and evaluation results.</p>
<p>For the purpose of comparison, we also apply Auto-J and Prometheus with their original prompt on aspect-specific evaluation.As can be seen in Table 3, to our surprise, their performance remains roughly the same compared with aspect-specific prompts, notifying that both models have lost the general instruction-understanding ability, therefore the aspect-specific prompt is not taking effect.</p>
<p>Can not Benefit from CoT and ICL</p>
<p>One of the most appealing features of LLM is it can benefit from delicate prompt engineering.Various strategies have been proposed to improve the LLM's capability on various tasks, including text evaluation.In this section, we select two representative strategies, namely In-context Learning (ICL) (Dong et al., 2023) and Chain-of-Thought Prompting (CoT) (Wei et al., 2022), to further improve the evaluation capability of the judge models.</p>
<p>As shown in Table 7, while the close-sourced proprietary models are improved by a large margin through both prompt engineering strategies, the fine-tuned judges hardly benefit from these strategies, sometimes even experiencing severe performance decline.Specifically, in the case of CoT prompting, despite we modified the prompts for JudgeLM and PandaLM to generate CoT firstly, both models failed to produce CoT and adhered to their original output format, as they have lost their general instruction-following ability.</p>
<p>We also evaluated the impact of different CoT sources based on JudgeLM-7B.We first swapped the positions of scores and CoT in the training data, and then fine-tuned the base model with or without CoT using the same hyperparameters.Additionally, we utilized o1-preview-09122 to generate CoT for the original scores through hint-driven prompting (Srivastava et al., 2023), and subsequently finetuned the model with this annotated CoT.</p>
<p>As demonstrated in Table 7: Results of evaluators with ICL and CoT.We did not apply GPT-4 on JudgeLM-test as the annotation of JudgeLM-test is conducted with GPT-4 without ICL and CoT.We only apply ICL on Auto-J as the original prompt of Auto-J comprises CoT.</p>
<p>compared to the model fine-tuned without CoT.Notably, the o1-generated CoT led to a more severe performance drop.This clearly indicates that even high-quality CoT did not introduce any improvement to the fine-tuned judge.</p>
<p>3 The Essence of Fine-tuned Judge: A Task-specific Classifier</p>
<p>Combining all the limitations revealed in our experiments, we would like to claim that after the finetuning process on a single task, the judge model has degenerated into a task-specific classifier, which is overfitted to the training data.To support this, we   fine-tune three groups of judges based on the four groups of data as listed in Table 1 3 : 1. Vicuna-generation (Chiang et al., 2023): It formulates the evaluation task in a generationstyle, and the prediction head reuses the pretrained language model head;</p>
<p>Vicunaclassification</p>
<p>DeBERTaclassification</p>
<p>Vicunageneration</p>
<p>Vicunageneration</p>
<p>Vicunaclassification</p>
<p>DeBERTaclassification</p>
<p>DeBERTaclassification</p>
<p>Vicunaclassification</p>
<p>Vicunageneration</p>
<p>Vicunageneration</p>
<p>Vicunaclassification</p>
<p>DeBERTaclassification</p>
<ol>
<li>
<p>Vicuna-classification: It formulates the evaluation task as classification or regression, and the prediction head is newly initialized as a linear projection layer;</p>
</li>
<li>
<p>DeBERTa-classification: It also formulates as a classification task, based on DeBERTaV3large (He et al., 2023), which is 20 times smaller than the 7B version of Vicuna;</p>
</li>
</ol>
<p>As shown in Table 5, the classification model performs equally well as the generation model.The formidable generative capabilities of LLMs hardly bring any improvement to the evaluation, as they are fitting to the same group of data.Moreover, the DeBERTa-based classifier achieves comparable performance with the LLM-based evaluators4 , which might be argued for that the encoder-only architecture is more suitable for classification.</p>
<p>We also analyze the correlation between different predictions made by different evaluators.As shown in Figure 2 and 3, the correlation among different classification models is much closer than their correlation with GPT-4.Different as they are in architectures, all three models are inherently classifiers fitting to the same set of supervision, leading to similar evaluation outcomes.</p>
<p>Although prior research on instruction-tuning all emphasizes the importance of data diversity (Zhou et al., 2023;Lu et al., 2024), the fine-tuning of judges is doing the opposite thing.Therefore, after fine-tuning for a single task with a fixed prompt template, the model lost its generalization ability, and degenerate into a task-specific classifier, which exhibits several limitations due to overfitting.</p>
<p>Conclusion</p>
<p>Although the fine-tuned models demonstrate superior performance on in-domain test sets, they still have several limitations compared to GPT-4.While increasing the fine-tuning data could possibly mitigate some of the limitations, as the potential of LLM extends beyond boundaries, there will always be new domains and tasks that are not covered by the fine-tuning scope.Therefore, the fine-tuned judge model cannot replace GPT-4 as a universal evaluator for LLMs, and should be used judiciously by watching the domain and task adaptability.</p>
<p>Limitations</p>
<p>Our work still has some limitations: 1) Due to time limitation, we did not present a possible solution to presented from Figure 13 to 16 on MT-Bench, which are all adapted to multi-turn evaluation.We adopt the prompts presented in Figure 21 and Figure 22 for chain-of-thought prompting.</p>
<p>For Section 2.2, we adopt the prompts presented in Figure 5, 7, 9 and 11, as LLMBar is in the form of pair-wise selection.</p>
<p>For Section 2.3, we adopt the prompts presented in Figure 17 to 20 for JudgeLM, PandaLM and Auto-J, respectively.For Prometheus, as its original prompt comprises of scoring rubrics, we simply define the corresponding rubrics for different benchmarks.As HaluEval and ToxicChat are both binary classifications, we apply Auto-J and Prometheus with pointwise grading and conduct a grid search to determine the classification threshold.On the other hand, as SALAD-Bench is a pairwise classification, we apply pairwise selection models, namely JudgeLM, PandaLM, and Auto-J to select a better response.</p>
<p>Figure 1 :
1
Figure 1: The general training and inference procedure of fine-tuned judge models.</p>
<p>Figure 2 :
2
Figure 2: The F1 score between the predictions of different evaluators on JudgeLM testset.</p>
<p>Figure 3 :
3
Figure 3: The pearson coefficient between the predictions of different evaluators on Prometheus testset.</p>
<p>Figure 5 :
5
Figure 5: Prompt template for JudgeLM applied for pairwise selection.</p>
<p>Figure 6 :
6
Figure 6: Prompt template for JudgeLM applied for pointwise grading.</p>
<p>Figure 7 :
7
Figure 7: Prompt template for PandaLM applied for pairwise selection.</p>
<p>Figure 8 :
8
Figure 8: Prompt template for PandaLM applied for pointwise grading.</p>
<p>Figure 9 :
9
Figure 9: Prompt template for Auto-J applied for pairwise selection.</p>
<p>Figure 10 :
10
Figure 10: Prompt template for Auto-J applied for pointwise grading.</p>
<p>Figure 11 :
11
Figure 11: Prompt template for Prometheus applied for pairwise selection.</p>
<p>Figure 12 :
12
Figure 12: Prompt template for Prometheus applied for pointwise grading.</p>
<p>Figure 13 :
13
Figure 13: Prompt template for JudgeLM applied for multi-turn grading.</p>
<p>Figure 14 :
14
Figure 14: Prompt template for PandaLM applied for multi-turn grading.</p>
<p>Figure 15 :
15
Figure 15: Prompt template for Auto-J applied for multi-turn grading.</p>
<p>Figure 16 :
16
Figure 16: Prompt template for Prometheus applied for multi-turn grading.</p>
<p>Figure 17 :
17
Figure 17: Prompt template for JudgeLM applied on SALAD-Bench.</p>
<p>Figure 18 :
18
Figure 18: Prompt template for Auto-J applied on HaluEval.</p>
<p>Figure 19 :
19
Figure 19: Prompt template for Auto-J applied on ToxicChat.</p>
<p>Figure 20 :
20
Figure 20: Prompt template for Auto-J applied on SALAD-Bench.</p>
<p>Figure 21 :
21
Figure 21: Prompt template for JudgeLM applied with chain-of-thought prompting.</p>
<p>Figure 22 :
22
Figure 22: Prompt template for PandaLM applied with chain-of-thought prompting.</p>
<p>Table 1 :
1
Detailed statistics of the four fine-tuned judge models, which is the foundation of our empirical study.
ModelFoundationInstructionResponseAnnotation Evaluation Scheme TestsetJudgeLMVicunaInstruct Datasets11 modelsGPT-4Pairwise GradingGPT-4(Zhu et al., 2024)(Alpaca-GPT4,(Alpaca,Vicuna...)Dolly-15K...)PandaLMLLaMAAlpaca 52K5 modelsGPT3.5Pairwise SelectionHuman(Wang et al., 2024)(LLaMA, Bloom...)Auto-JLLaMA2-chat Preference DatasetsPreference Datasets HumanPairwise SelectionHuman(Li et al., 2024a)(Chatbot Arena,Pointwise GradingOpenAI WebGPT...)PrometheusLLaMA2-chat GPT-4 GeneratedGPT-4 GeneratedGPT-4Pointwise GradingGPT-4(Kim et al., 2024)ModelJudgeLM-test accuracy F1PandaLM-test accuracy F1Auto-J-test agreement PCC-ind PCC-ood accuracy F1 Prometheus-test MT-BenchJudgeLM-7B82.3972.9768.1765.1845.30.3980.38448.748.7PandaLM-7B66.4456.0168.9760.9540.00.4170.38655.246.8Auto-J-13B77.7962.6472.1764.1053.60.6140.59151.743.7Prometheus-13B24.5823.3929.0327.9216.20.8640.86953.247.1+grade-twice54.2450.0445.2543.5847.8----Deepseek-V379.2368.2775.9771.2557.00.7340.741--GPT-4-mini79.1768.3176.5771.7957.40.7070.705--GPT-3.5-061372.5751.4064.3646.4042.70.6360.563--GPT-4-110684.2472.8375.7871.5156.90.7420.74366.961.9</p>
<p>Table 2 :
2
Results of evaluators on different evaluation schemes.Notice JudgeLM-test, PandaLM-test, Auto-J-test are pairwise selection, Prometheus-test is pointwise grading, and MT-Bench is multi-turn evaluation.</p>
<p>Table 3 :
3
Results of evaluators on aspect-specific evaluation.w/o adapt denotes using the original prompt without adaptation to the specific aspect.For more details please refer to A.2.
ModelHaluEval-QA accuracy F1HaluEval-Sum accuracy F1HaluEval-Dial accuracy F1ToxicChat accuracy F1SALAD-Bench accuracy F1JudgeLM-7B--------82.4557.44PandaLM-B--------57.0337.23Auto-J-13B58.3056.0353.1043.3463.1062.9087.4052.2486.8852.66w/o adapt59.6057.3853.4743.5564.5063.7187.7051.1571.7747.86Prometheus-7B47.9045.8444.5040.3851.0045.1777.1058.14--w/o adapt48.9045.1046.6036.4353.4050.2481.2061.87--GPT-3.5-061357.5057.1062.6060.2772.1072.0895.1080.8095.5461.70GPT-4-110672.5072.5072.0071.4484.5084.7894.5082.7898.7565.55ModelLLMBar Natu. Neig. GPTI. GPTO. Manu.JudgeLM-7B 62.0 23.1 26.146.828.3PandaLM-7B 59.0 16.5 21.742.626.1Auto-J-13B70.0 20.9 21.746.823.9Prometheus-7B 53.0 22.4 17.427.732.6GPT-4-1106 93.5 64.2 76.676.675.0</p>
<p>Table 4 :
4
Accuracy of evaluators on bias evaluation.</p>
<p>Table 5 :
5
Table 6, fine-tuning with either the original CoT or the o1-generated CoT resulted in a degradation of model performance Comparison of generation and classification-based evaluators.Results with † are from evaluating the four publicly released models on their respective testsets, and results with ‡ are from evaluating models trained by us.
ModelJudgeLM-test PandaLM-test Auto-J-test Prometheus-testaccuracy F1 accuracy F1 agreement PCC-ind PCC-oodReleased Models  †82.39 72.97 68.97 60.9553.60.8640.869Vicuna-generation  ‡82.44 71.77 72.37 60.7847.60.8260.815Vicuna-classification  ‡82.16 70.07 70.87 60.3446.80.8460.831DeBERTa-classification  ‡ 81.30 68.34 72.27 51.7531.70.8350.813GPT-3.5-061372.57 51.40 64.36 46.4042.70.6360.563GPT-4-1106-preview84.24 72.83 75.78 71.5156.90.7420.743ModelMethodJudgeLM-test PandaLM-test Auto-J-test Salad-bench accuracy F1 acuracy F1 agreement accuracy F1w/o CoT82.74 72.64 72.67 69.3244.2585.57 57.35JudgeLM-7Bw/ original CoT 82.26 67.41 70.77 64.2441.4575.15 50.59w/ o1 CoT77.96 65.43 66.46 62.9537.9072.39 47.91</p>
<p>Table 6 :
6
Comparison of different CoT sources on JudgeLM-7B.
ModelJudgeLM-test PandaLM-test accuracy F1 accuracy F1JudgeLM-7B82.3972.9768.1765.18+ CoT81.6871.5968.0364.42+ ICL68.5758.5241.1440.39PandaLM-7B66.4456.0168.9760.95+ CoT65.8556.5968.0360.42+ ICL66.1655.9468.9759.40Auto-J-13B77.7962.6472.1764.10+ ICL76.2059.1268.3758.44GPT-3.5-061372.5751.4064.3646.40+ CoT75.2460.7169.9763.66+ ICL69.3857.4670.6756.12GPT-4-110684.2472.8375.7871.51+ CoT--77.0871.77+ ICL--64.8656.20
Codes are openly available at https://github.com/ HuihuiChyan/UnlimitedJudge.
We make minimal change to the predefined prompts to adapt the judge model to different schemes. Please refer to Appendix A.2 for detailed implementations.
platform.openai.com/docs/models/o1
Please refer to Appendix A.1 for training details.
The only exception is on Auto-J-test, which is possibly due to a large proportion of the test data exceeds
.mitigate the limitations of fine-tuned judge models. We will investigate related method in the future. 2) The work ofZeng et al. (2023) is only a general assessment of evaluator bias, and we did not include fine-grained assessment for different biases, such as position bias(Wang et al., 2023a), verbosity bias(Saito et al., 2023), etc.  3) Due to time constraints, we did not incorporate manual inspection into the meta-evaluation process. Including human evaluators would enhance the credibility of our claims.
Please refer to the class AutoModelForSequence Classification in Huggingface library for more details.
AcknowledgementsThis work is supported by National Natural Science Foundation of China (62276077, 62376075,  62376076).A AppendixA.1 Training SettingsAs mentioned in Section 2, we fine-tune our judge models based on the four groups of data (JudgeLM(Zhu et al., 2024), PandaLM(Wang et al., 2024), Auto-J(Li et al., 2024a), Prometheus(Kim et al., 2024)), both in generation-style and in classification-style, for the purpose of comparison.We train all the models on NVIDIA A100-80GB GPUs with Huggingface-transformers(Wolf et al., 2020)and DeepSpeed(Rasley et al., 2020).Detailed hyperparameters are presented in Table8.Notice when comparing generation and classification models, we adopt the same prompt template and same hyper-parameters, with the only difference lying in the prediction method, as illustrated in Figure4.For generation model, the prediction head reused the pretrained language model head and is trained akin to the process of language modeling.For classification (regression) model, the prediction head is newly initialized as a linear projection layer, and is decoupled from the language modeling process 5 .A.2 Prompt TemplatesAs mentioned in Section 2, we take the publicly released checkpoints of the four fine-tuned judge models and validate their performance.To make a fair comparison, we make minimal modifications to their pre-defined prompts, to adapt them to different scenarios, as listed as follows.For Section 2.1, we adopt the prompts presented in Figure5to 12 for cross validation.Notice for JudgeLM and PandaLM, their predefined prompts are in the form of pairwise selection, and we make slight modifications to apply them on pointwise grading.For Prometheus, the predefined prompt is in the form of pointwise grading, and we make slight modifications to apply it on pairwise selection.For Auto-J, they predefined prompts both for pairwise selection and pointwise grading.We also adopt the prompts
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 2023</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, 2023A survey on in-context learning</p>
<p>Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, and Bo Zheng. 2025. Chinesesimplevqa -"see the world, discover knowledge": A chinese factuality evaluation for large vision language models. </p>
<p>DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. Pengcheng He, Jianfeng Gao, Weizhu Chen, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, arXiv:2411.07140Chinese simpleqa: A chinese factuality evaluation for large language models. 2024arXiv preprint</p>
<p>Can large language models detect errors in long chain-of-thought reasoning?. Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, Bo Zheng, 2025</p>
<p>CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation. Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Generative judge for evaluating alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Pengfei Liu, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>HaluEval: A large-scale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, 10.18653/v1/2023.emnlp-main.397Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Salad-bench: A hierarchical and comprehensive safety benchmark for large language models. Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao, 2024b</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023b</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang, 10.18653/v1/2023.findings-emnlp.311Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<h1>instag: Instruction tagging for analyzing supervised fine-tuning of large language models. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin ; Chuanqi Tan, Chang Zhou, Jingren Zhou, The Twelfth International Conference on Learning Representations. Junyang Lin,. 2024</h1>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto, arXiv:2310.10076Verbosity bias in preference labeling by large language models. 2023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023aarXiv preprint</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, ArXiv, abs/2305.179262023b</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, 2024</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, arXiv:2310.07641Evaluating large language models at evaluating instruction following. 2023arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>
<p>LIMA: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Promptbench: A unified library for evaluation of large language models. Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie, arXiv:2312.079102023arXiv preprint</p>
<p>JudgeLM : Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
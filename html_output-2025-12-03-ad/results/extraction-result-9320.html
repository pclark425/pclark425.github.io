<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9320 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9320</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9320</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-272880971</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.16635v2.pdf" target="_blank">Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper proposes a novel prompting approach, Judgment of Thought (JoT), specifically tailored for binary logical reasoning tasks. Despite advances in prompt engineering, existing approaches still face limitations in handling complex logical reasoning tasks. To address these issues, JoT introduces a multi-agent approach with three specialized roles$\unicode{x2010}$$\unicode{x2010}$$\unicode{x2010}$lawyer, prosecutor, and judge$\unicode{x2010}$$\unicode{x2010}$$\unicode{x2010}$where a high-level model acts as the judge, and lower-level models serve as lawyer and prosecutor to systematically debate and evaluate arguments. Experimental evaluations on benchmarks such as BigBenchHard and Winogrande demonstrate JoT's superior performance compared to existing prompting approaches, achieving notable improvements, including 98\% accuracy in Boolean expressions. Also, our ablation studies validate the critical contribution of each role, iterative refinement loops, and feedback mechanisms. Consequently, JoT significantly enhances accuracy, reliability, and consistency in binary reasoning tasks and shows potential for practical applications.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9320.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9320.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoT (GPT-3.5) — Boolean Expressions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment of Thought prompting applied with GPT-3.5-turbo on Boolean Expressions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's multi-agent JoT prompt (lawyer, prosecutor, judge; iterative debate with feedback) applied to Boolean logical formula evaluation using GPT-3.5-turbo produced very high accuracy and F1, substantially outperforming zero-shot, few-shot, chain-of-thought, self-consistency, and a prior debate baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Boolean Expressions (BigBenchHard subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of logical Boolean formulas (binary true/false decisions).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Judgment of Thought (JoT): three roles (lawyer argues True, prosecutor argues False, judge (higher-level model) evaluates), iterative loops (default reporting used 3 loops), role-specific system messages (3 utterances per lawyer/prosecutor, 3 judgments per judge), temperature=1, top-p=1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, Chain-of-Thought (CoT), Self-Consistency (SC), Debate (Khan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 98%, F1: 0.98</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 67%/0.76; Few-shot: 55%/0.55; CoT: 47%/0.29; SC: 43%/0.17; Khan et al. (Debate): 81%/0.84 (all reported for GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+31% accuracy vs Zero-shot; +43% vs Few-shot; +51% vs CoT; +55% vs SC; +17% vs prior Debate baseline (Khan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper attributes JoT's gains to structured adversarial roles that force opposing, explicitly adversarial reasoning, iterative judge feedback that corrects logical gaps, and the judge's higher-capability evaluation yielding more logically consistent final decisions. JoT's explicit requirement for stepwise argumentation prevents shallow persuasive but logically flawed claims from winning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations averaged over 10 runs; iterative prompting methods (including JoT) used 3 reasoning samples by default for reported comparisons; models run with default parameters (temperature=1, top-p=1); the JoT role instructions (lawyer/prosecutor/judge) and three-turn speaking structure are provided in Appendix A.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9320.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoT (GPT-3.5) — Causal Judgment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment of Thought prompting applied with GPT-3.5-turbo on Causal Judgment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>JoT applied to cause-effect binary judgments with GPT-3.5-turbo improved accuracy relative to standard prompting methods, by leveraging iterative adversarial argument and judge feedback to clarify causal relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Causal Judgment (BigBenchHard subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary reasoning about cause-effect relationships (determine whether a causal claim holds).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Judgment of Thought (JoT) with 3 iterative loops; lawyer/prosecutor/judge role prompts; default model sampling (temperature=1, top-p=1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, CoT, SC, Khan et al. debate</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 74%, F1: 0.72</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 62%/0.55; Few-shot: 61%/0.61; CoT: 61%/0.52; SC: 59%/0.48; Khan et al.: 61%/0.61 (GPT-3.5-turbo values from Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+12% accuracy vs Zero-shot; +13% vs SC; +13% vs CoT; +13% vs Few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue iterative, adversarial reasoning and judge feedback help disambiguate causal chains and address counterarguments, improving robustness in causal inference where single-agent linear reasoning misses alternative interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Averaged over 10 runs; JoT iterative loops set to 3 for reported comparisons; same few-shot examples used across Few-shot/CoT/SC baselines (generated by Zeroshot CoT); models run with default parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9320.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoT (GPT-3.5) — Navigate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment of Thought prompting applied with GPT-3.5-turbo on Navigate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>JoT improves binary spatial-instruction reasoning (Navigate task) substantially relative to standard prompting, by using multi-turn adversarial argumentation to track and reconcile spatial constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Navigate (BigBenchHard subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary reasoning based on spatial instructions (deciding true/false or correct/incorrect navigation outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT (lawyer/prosecutor/judge), iterative loops (3), role-specific system prompts, models run at default sampling settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, CoT, SC, Khan et al.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 88%, F1: 0.87</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 54%/0.18; Few-shot: 55%/0.12; CoT: 57%/0.04; SC: 56%/0.00; Khan et al.: 60%/0.63 (GPT-3.5-turbo values)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+34% accuracy vs Zero-shot; +28% vs Khan et al.; +31% vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>JoT's step-by-step debate helps keep track of spatial constraints and counterpoints, reducing omission errors common in single-pass reasoning; iterative judge feedback corrects misinterpretations of spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>10-run averages; iterative methods used 3 samples in reported comparisons; default sampling params (temperature=1, top-p=1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9320.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoT (GPT-3.5) — Web of Lies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment of Thought prompting applied with GPT-3.5-turbo on Web of Lies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On tasks requiring validation of linked truth claims (Web of Lies), JoT's multi-round adversarial prompting yields large gains for GPT-3.5-turbo by improving chain-tracking and contradiction detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Web of Lies (BigBenchHard subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary validation of interconnected statements (identify truthfulness within chains of claims).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT with 3 iterative loops; explicit lawyer/prosecutor/judge role prompts; default sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, CoT, SC, Khan et al.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 90%, F1: 0.91</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 47%/0.18; Few-shot: 51%/0.35; CoT: 44%/0.20; SC: 46%/0.07; Khan et al.: 53%/0.49 (GPT-3.5-turbo values)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+43% accuracy vs Zero-shot; +37% vs Khan et al.; +46% vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Iterative adversarial arguments plus judge feedback enable better tracking of dependencies across statements and systematic rebuttal of inconsistencies, improving detection of webs of misinformation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported numbers are averages over 10 runs; iterative methods used 3 samples; JoT enforces explicit adversarial roles and three-turn speaking procedures per agent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9320.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoT (GPT-3.5) — Formal Fallacies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment of Thought prompting applied with GPT-3.5-turbo on Formal Fallacies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>JoT aids detection of flawed logical arguments (formal fallacies) with moderate gains over standard prompting by focusing on rebuttal and explicit logical critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Formal Fallacies (BigBenchHard subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary identification of flawed logical arguments (detecting formal fallacies).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT multi-agent iterative prompting (3 loops), role-specific instructions, default sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, CoT, SC, Khan et al.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 77%, F1: 0.77</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 45%/0.58; Few-shot: 50%/0.31; CoT: 57%/0.30; SC: 54%/0.23; Khan et al.: 60%/0.66 (GPT-3.5-turbo values)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+32% accuracy vs Zero-shot; +17% vs Khan et al.; +20% vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The rebuttal structure of JoT encourages focused examination of argumentative structure, improving identification of subtle formal flaws; judge feedback helps surface missed logical steps.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Averages over 10 runs; JoT used 3 loops in reported comparisons; models run with default temperature/top-p.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9320.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JoT (GPT-3.5) — Winogrande</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment of Thought prompting applied with GPT-3.5-turbo on Winogrande</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>JoT improves pronoun resolution (Winogrande) for GPT-3.5 by using multi-perspective argumentation to resolve ambiguous references, yielding substantial accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Winogrande</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Pronoun resolution benchmark designed to test commonsense reasoning about ambiguous references.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT (lawyer/prosecutor/judge), iterative loops (3), role-specific prompts, default sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, CoT, SC, Khan et al.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 89%, F1: 0.89</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 60%/0.60; Few-shot: 58%/0.60; CoT: 54%/0.57; SC: 63%/0.64; Khan et al.: 59%/0.43 (GPT-3.5-turbo values)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+29% accuracy vs Zero-shot; +26% vs SC; +35% vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multi-perspective adversarial reasoning surfaces alternative antecedents and counterarguments, improving context disambiguation; judge adjudication consolidates the strongest reasoning path.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>10-run averages; iterative methods used 3 samples; same few-shot examples shared across Few-shot/CoT/SC baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9320.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought negative example (GPT-3.5, Boolean)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting on GPT-3.5-turbo for Boolean Expressions (example of degraded performance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In the reported experiments, Chain-of-Thought (CoT) prompting sometimes decreased performance relative to simpler prompts (example: Boolean Expressions on GPT-3.5), illustrating that CoT is not universally beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Boolean Expressions (illustrative negative effect)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Logical formula evaluation; binary true/false decisions where CoT produced worse results than zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-Thought (few-shot CoT examples encouraging step-by-step internal reasoning), same few-shot examples as used across baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot, Few-shot, SC, Debate, JoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 47%, F1: 0.29 (CoT on GPT-3.5-turbo, Boolean Expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot: 67%/0.76; JoT: 98%/0.98; Khan et al.: 81%/0.84</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-20% accuracy for CoT vs Zero-shot on this task (47% vs 67%) and -51% vs JoT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper suggests that single-agent linear explanatory CoT can miss counterarguments or alternative interpretations, permitting shallow reasoning chains that harm performance on some binary logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Values from Table 1; experiments averaged over 10 runs; same few-shot examples used across Few-shot/CoT/SC baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9320.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Role Ablation (Without Lawyer/Prosecutor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study removing either the prosecutor or the lawyer from JoT (reported results)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Removing either adversarial role (prosecutor or lawyer) from JoT generally reduced accuracy and F1 across evaluated datasets, showing both roles contribute complementary information to final decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude models (3-Haiku, 3.5-Haiku) / reported JoT ablation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (Boolean Expressions, Causal Judgment, Navigate, Web of Lies, Formal Fallacies, Winogrande)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary logical reasoning tasks (as in BigBenchHard subset and Winogrande); ablation compares full JoT vs JoT without one adversarial role.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT with either prosecutor removed or lawyer removed (role-specific prompts otherwise same); iterative feedback included.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Full JoT (lawyer+prosecutor+judge) vs JoT-without-prosecutor vs JoT-without-lawyer</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples (Table 2): Boolean expressions — Without Prosecutor: 95%/0.96; Without Lawyer: 95%/0.95; JoT: 98%/0.98. Causal judgment — Without Prosecutor: 68%/0.70; Without Lawyer: 68%/0.53; JoT: 74%/0.72. Navigate — 72%/0.76; 65%/0.72; JoT 88%/0.87. Web of Lies — 69%/0.74; 64%/0.55; JoT 90%/0.91. Formal fallacies — 65%/0.66; 68%/0.56; JoT 77%/0.77. Winogrande — 85%/0.86; 82%/0.82; JoT 89%/0.89.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>In all listed cases, full JoT > JoT-without-role (example deltas vary per task; see performance field for numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Typical drops when removing a role: up to ~23 percentage points (e.g., Navigate: 88% -> 65% when lawyer removed) on some tasks; magnitude varies by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors conclude the lawyer and prosecutor provide complementary adversarial perspectives; removing either reduces the breadth or critical-contrast of arguments, leading to poorer judge decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported in Table 2 (Claude-model-based ablation reported); experiments averaged as in main setup; the paper emphasizes role interaction importance; exact model mapping per row corresponds to Claude-model evaluations reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9320.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Loop Iteration Effect (JoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of increasing number of iterative loops in JoT (1, 3, 5 iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Performance generally improved or remained stable as JoT increased the number of judge-lawyer-prosecutor iterations from 1 to 3 to 5, with larger gains on more complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JoT evaluated across reported models (GPT-3.5, GPT-4o, Claude variants) — aggregated reporting</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (Boolean Expressions, Causal Judgment, Navigate, Web of Lies, Formal Fallacies, Winogrande)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary logical reasoning tasks; testing sensitivity of JoT to number of iterative feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT with varying loop counts (1, 3, 5 iterations); other prompt structure unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 4 examples: Boolean expressions — 1 iter: 98%/0.98; 3 iters: 98%/0.98; 5 iters: 99%/0.99. Causal judgment — 65%/0.60; 74%/0.72; 74%/0.73. Navigate — 87%/0.83; 88%/0.87; 91%/0.89. Web of lies — 87%/0.88; 90%/0.91; 91%/0.91. Formal fallacies — 70%/0.67; 77%/0.77; 78%/0.78. Winogrande — 87%/0.87; 89%/0.89; 89%/0.89.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Examples: Causal judgment +9 percentage points accuracy from 1 to 3 iterations (65% -> 74%); Navigate +4 points from 1 to 5 iterations (87% -> 91%); Web of Lies +4 points (87% -> 91%).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Iterative refinement with judge feedback allows correction of earlier logical gaps and progressive strengthening of arguments, particularly useful on tasks where reasoning steps build on one another (causal judgment, Web of Lies). Gains are smaller on simple tasks (Boolean expressions already near ceiling).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Loop counts explicitly tested at 1, 3, and 5; other conditions held constant; reported values are from Table 4 in the paper; experiments averaged over multiple runs per earlier protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9320.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9320.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feedback Ablation (With vs Without Feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of including iterative judge feedback in JoT (with feedback vs without feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including judge feedback in JoT improved accuracy and F1 across evaluated tasks compared to disabling feedback, indicating feedback is a meaningful component of JoT's presentation format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JoT evaluated across reported models (aggregated reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (Boolean Expressions, Causal Judgment, Navigate, Web of Lies, Formal Fallacies, Winogrande)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary logical reasoning tasks; direct test of whether explicit feedback loops improve outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>JoT with iterative judge feedback vs JoT without any iterative feedback (single-step or no-feedback variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 5 examples: Boolean expressions — Without Feedback: 96%/0.97; With Feedback: 98%/0.98. Causal judgement — 69%/0.63 vs 74%/0.72. Navigate — 87%/0.83 vs 88%/0.87. Web of Lies — 87%/0.88 vs 90%/0.91. Formal fallacies — 70%/0.70 vs 77%/0.77. Winogrande — 87%/0.87 vs 89%/0.89.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Examples: Boolean +2% accuracy; Causal +5% accuracy; Formal fallacies +7% accuracy; Web of Lies +3% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Judge feedback identifies logical weaknesses and forces the arguing agents to address gaps, improving argument quality and final judgments. The benefit is greater in complex tasks where initial arguments are more likely to miss key steps.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Paper compares with/without feedback using JoT prompt variants; reported numbers in Table 5; other experimental parameters (runs, sampling) follow main setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Debating with more persuasive llms leads to more truthful answers <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Winogrande: An adversarial winograd schema challenge at scale <em>(Rating: 1)</em></li>
                <li>BigBench-Hard <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9320",
    "paper_id": "paper-272880971",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "JoT (GPT-3.5) — Boolean Expressions",
            "name_full": "Judgment of Thought prompting applied with GPT-3.5-turbo on Boolean Expressions",
            "brief_description": "The paper's multi-agent JoT prompt (lawyer, prosecutor, judge; iterative debate with feedback) applied to Boolean logical formula evaluation using GPT-3.5-turbo produced very high accuracy and F1, substantially outperforming zero-shot, few-shot, chain-of-thought, self-consistency, and a prior debate baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Boolean Expressions (BigBenchHard subset)",
            "task_description": "Evaluation of logical Boolean formulas (binary true/false decisions).",
            "presentation_format": "Judgment of Thought (JoT): three roles (lawyer argues True, prosecutor argues False, judge (higher-level model) evaluates), iterative loops (default reporting used 3 loops), role-specific system messages (3 utterances per lawyer/prosecutor, 3 judgments per judge), temperature=1, top-p=1.",
            "comparison_format": "Zero-shot, Few-shot, Chain-of-Thought (CoT), Self-Consistency (SC), Debate (Khan et al.)",
            "performance": "accuracy: 98%, F1: 0.98",
            "performance_comparison": "Zero-shot: 67%/0.76; Few-shot: 55%/0.55; CoT: 47%/0.29; SC: 43%/0.17; Khan et al. (Debate): 81%/0.84 (all reported for GPT-3.5-turbo)",
            "format_effect_size": "+31% accuracy vs Zero-shot; +43% vs Few-shot; +51% vs CoT; +55% vs SC; +17% vs prior Debate baseline (Khan et al.)",
            "explanation_or_hypothesis": "The paper attributes JoT's gains to structured adversarial roles that force opposing, explicitly adversarial reasoning, iterative judge feedback that corrects logical gaps, and the judge's higher-capability evaluation yielding more logically consistent final decisions. JoT's explicit requirement for stepwise argumentation prevents shallow persuasive but logically flawed claims from winning.",
            "null_or_negative_result": false,
            "experimental_details": "Evaluations averaged over 10 runs; iterative prompting methods (including JoT) used 3 reasoning samples by default for reported comparisons; models run with default parameters (temperature=1, top-p=1); the JoT role instructions (lawyer/prosecutor/judge) and three-turn speaking structure are provided in Appendix A.",
            "uuid": "e9320.0",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "JoT (GPT-3.5) — Causal Judgment",
            "name_full": "Judgment of Thought prompting applied with GPT-3.5-turbo on Causal Judgment",
            "brief_description": "JoT applied to cause-effect binary judgments with GPT-3.5-turbo improved accuracy relative to standard prompting methods, by leveraging iterative adversarial argument and judge feedback to clarify causal relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Causal Judgment (BigBenchHard subset)",
            "task_description": "Binary reasoning about cause-effect relationships (determine whether a causal claim holds).",
            "presentation_format": "Judgment of Thought (JoT) with 3 iterative loops; lawyer/prosecutor/judge role prompts; default model sampling (temperature=1, top-p=1).",
            "comparison_format": "Zero-shot, Few-shot, CoT, SC, Khan et al. debate",
            "performance": "accuracy: 74%, F1: 0.72",
            "performance_comparison": "Zero-shot: 62%/0.55; Few-shot: 61%/0.61; CoT: 61%/0.52; SC: 59%/0.48; Khan et al.: 61%/0.61 (GPT-3.5-turbo values from Table 1)",
            "format_effect_size": "+12% accuracy vs Zero-shot; +13% vs SC; +13% vs CoT; +13% vs Few-shot",
            "explanation_or_hypothesis": "Authors argue iterative, adversarial reasoning and judge feedback help disambiguate causal chains and address counterarguments, improving robustness in causal inference where single-agent linear reasoning misses alternative interpretations.",
            "null_or_negative_result": false,
            "experimental_details": "Averaged over 10 runs; JoT iterative loops set to 3 for reported comparisons; same few-shot examples used across Few-shot/CoT/SC baselines (generated by Zeroshot CoT); models run with default parameters.",
            "uuid": "e9320.1",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "JoT (GPT-3.5) — Navigate",
            "name_full": "Judgment of Thought prompting applied with GPT-3.5-turbo on Navigate",
            "brief_description": "JoT improves binary spatial-instruction reasoning (Navigate task) substantially relative to standard prompting, by using multi-turn adversarial argumentation to track and reconcile spatial constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Navigate (BigBenchHard subset)",
            "task_description": "Binary reasoning based on spatial instructions (deciding true/false or correct/incorrect navigation outcomes).",
            "presentation_format": "JoT (lawyer/prosecutor/judge), iterative loops (3), role-specific system prompts, models run at default sampling settings.",
            "comparison_format": "Zero-shot, Few-shot, CoT, SC, Khan et al.",
            "performance": "accuracy: 88%, F1: 0.87",
            "performance_comparison": "Zero-shot: 54%/0.18; Few-shot: 55%/0.12; CoT: 57%/0.04; SC: 56%/0.00; Khan et al.: 60%/0.63 (GPT-3.5-turbo values)",
            "format_effect_size": "+34% accuracy vs Zero-shot; +28% vs Khan et al.; +31% vs CoT",
            "explanation_or_hypothesis": "JoT's step-by-step debate helps keep track of spatial constraints and counterpoints, reducing omission errors common in single-pass reasoning; iterative judge feedback corrects misinterpretations of spatial relations.",
            "null_or_negative_result": false,
            "experimental_details": "10-run averages; iterative methods used 3 samples in reported comparisons; default sampling params (temperature=1, top-p=1).",
            "uuid": "e9320.2",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "JoT (GPT-3.5) — Web of Lies",
            "name_full": "Judgment of Thought prompting applied with GPT-3.5-turbo on Web of Lies",
            "brief_description": "On tasks requiring validation of linked truth claims (Web of Lies), JoT's multi-round adversarial prompting yields large gains for GPT-3.5-turbo by improving chain-tracking and contradiction detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Web of Lies (BigBenchHard subset)",
            "task_description": "Binary validation of interconnected statements (identify truthfulness within chains of claims).",
            "presentation_format": "JoT with 3 iterative loops; explicit lawyer/prosecutor/judge role prompts; default sampling.",
            "comparison_format": "Zero-shot, Few-shot, CoT, SC, Khan et al.",
            "performance": "accuracy: 90%, F1: 0.91",
            "performance_comparison": "Zero-shot: 47%/0.18; Few-shot: 51%/0.35; CoT: 44%/0.20; SC: 46%/0.07; Khan et al.: 53%/0.49 (GPT-3.5-turbo values)",
            "format_effect_size": "+43% accuracy vs Zero-shot; +37% vs Khan et al.; +46% vs CoT",
            "explanation_or_hypothesis": "Iterative adversarial arguments plus judge feedback enable better tracking of dependencies across statements and systematic rebuttal of inconsistencies, improving detection of webs of misinformation.",
            "null_or_negative_result": false,
            "experimental_details": "Reported numbers are averages over 10 runs; iterative methods used 3 samples; JoT enforces explicit adversarial roles and three-turn speaking procedures per agent.",
            "uuid": "e9320.3",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "JoT (GPT-3.5) — Formal Fallacies",
            "name_full": "Judgment of Thought prompting applied with GPT-3.5-turbo on Formal Fallacies",
            "brief_description": "JoT aids detection of flawed logical arguments (formal fallacies) with moderate gains over standard prompting by focusing on rebuttal and explicit logical critique.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Formal Fallacies (BigBenchHard subset)",
            "task_description": "Binary identification of flawed logical arguments (detecting formal fallacies).",
            "presentation_format": "JoT multi-agent iterative prompting (3 loops), role-specific instructions, default sampling.",
            "comparison_format": "Zero-shot, Few-shot, CoT, SC, Khan et al.",
            "performance": "accuracy: 77%, F1: 0.77",
            "performance_comparison": "Zero-shot: 45%/0.58; Few-shot: 50%/0.31; CoT: 57%/0.30; SC: 54%/0.23; Khan et al.: 60%/0.66 (GPT-3.5-turbo values)",
            "format_effect_size": "+32% accuracy vs Zero-shot; +17% vs Khan et al.; +20% vs CoT",
            "explanation_or_hypothesis": "The rebuttal structure of JoT encourages focused examination of argumentative structure, improving identification of subtle formal flaws; judge feedback helps surface missed logical steps.",
            "null_or_negative_result": false,
            "experimental_details": "Averages over 10 runs; JoT used 3 loops in reported comparisons; models run with default temperature/top-p.",
            "uuid": "e9320.4",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "JoT (GPT-3.5) — Winogrande",
            "name_full": "Judgment of Thought prompting applied with GPT-3.5-turbo on Winogrande",
            "brief_description": "JoT improves pronoun resolution (Winogrande) for GPT-3.5 by using multi-perspective argumentation to resolve ambiguous references, yielding substantial accuracy gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Winogrande",
            "task_description": "Pronoun resolution benchmark designed to test commonsense reasoning about ambiguous references.",
            "presentation_format": "JoT (lawyer/prosecutor/judge), iterative loops (3), role-specific prompts, default sampling.",
            "comparison_format": "Zero-shot, Few-shot, CoT, SC, Khan et al.",
            "performance": "accuracy: 89%, F1: 0.89",
            "performance_comparison": "Zero-shot: 60%/0.60; Few-shot: 58%/0.60; CoT: 54%/0.57; SC: 63%/0.64; Khan et al.: 59%/0.43 (GPT-3.5-turbo values)",
            "format_effect_size": "+29% accuracy vs Zero-shot; +26% vs SC; +35% vs CoT",
            "explanation_or_hypothesis": "Multi-perspective adversarial reasoning surfaces alternative antecedents and counterarguments, improving context disambiguation; judge adjudication consolidates the strongest reasoning path.",
            "null_or_negative_result": false,
            "experimental_details": "10-run averages; iterative methods used 3 samples; same few-shot examples shared across Few-shot/CoT/SC baselines.",
            "uuid": "e9320.5",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Chain-of-Thought negative example (GPT-3.5, Boolean)",
            "name_full": "Chain-of-Thought prompting on GPT-3.5-turbo for Boolean Expressions (example of degraded performance)",
            "brief_description": "In the reported experiments, Chain-of-Thought (CoT) prompting sometimes decreased performance relative to simpler prompts (example: Boolean Expressions on GPT-3.5), illustrating that CoT is not universally beneficial.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "task_name": "Boolean Expressions (illustrative negative effect)",
            "task_description": "Logical formula evaluation; binary true/false decisions where CoT produced worse results than zero-shot.",
            "presentation_format": "Chain-of-Thought (few-shot CoT examples encouraging step-by-step internal reasoning), same few-shot examples as used across baselines.",
            "comparison_format": "Zero-shot, Few-shot, SC, Debate, JoT",
            "performance": "accuracy: 47%, F1: 0.29 (CoT on GPT-3.5-turbo, Boolean Expressions)",
            "performance_comparison": "Zero-shot: 67%/0.76; JoT: 98%/0.98; Khan et al.: 81%/0.84",
            "format_effect_size": "-20% accuracy for CoT vs Zero-shot on this task (47% vs 67%) and -51% vs JoT",
            "explanation_or_hypothesis": "The paper suggests that single-agent linear explanatory CoT can miss counterarguments or alternative interpretations, permitting shallow reasoning chains that harm performance on some binary logical tasks.",
            "null_or_negative_result": true,
            "experimental_details": "Values from Table 1; experiments averaged over 10 runs; same few-shot examples used across Few-shot/CoT/SC baselines.",
            "uuid": "e9320.6",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Role Ablation (Without Lawyer/Prosecutor)",
            "name_full": "Ablation study removing either the prosecutor or the lawyer from JoT (reported results)",
            "brief_description": "Removing either adversarial role (prosecutor or lawyer) from JoT generally reduced accuracy and F1 across evaluated datasets, showing both roles contribute complementary information to final decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude models (3-Haiku, 3.5-Haiku) / reported JoT ablation",
            "model_size": null,
            "task_name": "Multiple (Boolean Expressions, Causal Judgment, Navigate, Web of Lies, Formal Fallacies, Winogrande)",
            "task_description": "Binary logical reasoning tasks (as in BigBenchHard subset and Winogrande); ablation compares full JoT vs JoT without one adversarial role.",
            "presentation_format": "JoT with either prosecutor removed or lawyer removed (role-specific prompts otherwise same); iterative feedback included.",
            "comparison_format": "Full JoT (lawyer+prosecutor+judge) vs JoT-without-prosecutor vs JoT-without-lawyer",
            "performance": "Examples (Table 2): Boolean expressions — Without Prosecutor: 95%/0.96; Without Lawyer: 95%/0.95; JoT: 98%/0.98. Causal judgment — Without Prosecutor: 68%/0.70; Without Lawyer: 68%/0.53; JoT: 74%/0.72. Navigate — 72%/0.76; 65%/0.72; JoT 88%/0.87. Web of Lies — 69%/0.74; 64%/0.55; JoT 90%/0.91. Formal fallacies — 65%/0.66; 68%/0.56; JoT 77%/0.77. Winogrande — 85%/0.86; 82%/0.82; JoT 89%/0.89.",
            "performance_comparison": "In all listed cases, full JoT &gt; JoT-without-role (example deltas vary per task; see performance field for numbers).",
            "format_effect_size": "Typical drops when removing a role: up to ~23 percentage points (e.g., Navigate: 88% -&gt; 65% when lawyer removed) on some tasks; magnitude varies by dataset.",
            "explanation_or_hypothesis": "The authors conclude the lawyer and prosecutor provide complementary adversarial perspectives; removing either reduces the breadth or critical-contrast of arguments, leading to poorer judge decisions.",
            "null_or_negative_result": false,
            "experimental_details": "Reported in Table 2 (Claude-model-based ablation reported); experiments averaged as in main setup; the paper emphasizes role interaction importance; exact model mapping per row corresponds to Claude-model evaluations reported in the paper.",
            "uuid": "e9320.7",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Loop Iteration Effect (JoT)",
            "name_full": "Effect of increasing number of iterative loops in JoT (1, 3, 5 iterations)",
            "brief_description": "Performance generally improved or remained stable as JoT increased the number of judge-lawyer-prosecutor iterations from 1 to 3 to 5, with larger gains on more complex tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "JoT evaluated across reported models (GPT-3.5, GPT-4o, Claude variants) — aggregated reporting",
            "model_size": null,
            "task_name": "Multiple (Boolean Expressions, Causal Judgment, Navigate, Web of Lies, Formal Fallacies, Winogrande)",
            "task_description": "Binary logical reasoning tasks; testing sensitivity of JoT to number of iterative feedback loops.",
            "presentation_format": "JoT with varying loop counts (1, 3, 5 iterations); other prompt structure unchanged.",
            "comparison_format": null,
            "performance": "Table 4 examples: Boolean expressions — 1 iter: 98%/0.98; 3 iters: 98%/0.98; 5 iters: 99%/0.99. Causal judgment — 65%/0.60; 74%/0.72; 74%/0.73. Navigate — 87%/0.83; 88%/0.87; 91%/0.89. Web of lies — 87%/0.88; 90%/0.91; 91%/0.91. Formal fallacies — 70%/0.67; 77%/0.77; 78%/0.78. Winogrande — 87%/0.87; 89%/0.89; 89%/0.89.",
            "performance_comparison": null,
            "format_effect_size": "Examples: Causal judgment +9 percentage points accuracy from 1 to 3 iterations (65% -&gt; 74%); Navigate +4 points from 1 to 5 iterations (87% -&gt; 91%); Web of Lies +4 points (87% -&gt; 91%).",
            "explanation_or_hypothesis": "Iterative refinement with judge feedback allows correction of earlier logical gaps and progressive strengthening of arguments, particularly useful on tasks where reasoning steps build on one another (causal judgment, Web of Lies). Gains are smaller on simple tasks (Boolean expressions already near ceiling).",
            "null_or_negative_result": false,
            "experimental_details": "Loop counts explicitly tested at 1, 3, and 5; other conditions held constant; reported values are from Table 4 in the paper; experiments averaged over multiple runs per earlier protocol.",
            "uuid": "e9320.8",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Feedback Ablation (With vs Without Feedback)",
            "name_full": "Effect of including iterative judge feedback in JoT (with feedback vs without feedback)",
            "brief_description": "Including judge feedback in JoT improved accuracy and F1 across evaluated tasks compared to disabling feedback, indicating feedback is a meaningful component of JoT's presentation format.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "JoT evaluated across reported models (aggregated reporting)",
            "model_size": null,
            "task_name": "Multiple (Boolean Expressions, Causal Judgment, Navigate, Web of Lies, Formal Fallacies, Winogrande)",
            "task_description": "Binary logical reasoning tasks; direct test of whether explicit feedback loops improve outcomes.",
            "presentation_format": "JoT with iterative judge feedback vs JoT without any iterative feedback (single-step or no-feedback variants).",
            "comparison_format": null,
            "performance": "Table 5 examples: Boolean expressions — Without Feedback: 96%/0.97; With Feedback: 98%/0.98. Causal judgement — 69%/0.63 vs 74%/0.72. Navigate — 87%/0.83 vs 88%/0.87. Web of Lies — 87%/0.88 vs 90%/0.91. Formal fallacies — 70%/0.70 vs 77%/0.77. Winogrande — 87%/0.87 vs 89%/0.89.",
            "performance_comparison": null,
            "format_effect_size": "Examples: Boolean +2% accuracy; Causal +5% accuracy; Formal fallacies +7% accuracy; Web of Lies +3% accuracy.",
            "explanation_or_hypothesis": "Judge feedback identifies logical weaknesses and forces the arguing agents to address gaps, improving argument quality and final judgments. The benefit is greater in complex tasks where initial arguments are more likely to miss key steps.",
            "null_or_negative_result": false,
            "experimental_details": "Paper compares with/without feedback using JoT prompt variants; reported numbers in Table 5; other experimental parameters (runs, sampling) follow main setup.",
            "uuid": "e9320.9",
            "source_info": {
                "paper_title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Debating with more persuasive llms leads to more truthful answers",
            "rating": 2,
            "sanitized_title": "debating_with_more_persuasive_llms_leads_to_more_truthful_answers"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Winogrande: An adversarial winograd schema challenge at scale",
            "rating": 1,
            "sanitized_title": "winogrande_an_adversarial_winograd_schema_challenge_at_scale"
        },
        {
            "paper_title": "BigBench-Hard",
            "rating": 1,
            "sanitized_title": "bigbenchhard"
        }
    ],
    "cost": 0.021332249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models
21 May 2025</p>
<p>Sungjune Park 
Soongsil University
Korea</p>
<p>Heehwan Kim 
Soongsil University
Korea</p>
<p>Haehyun Cho haehyun@ssu.ac.kr 
Soongsil University
Korea</p>
<p>Daeseon Choi 
Soongsil University
Korea</p>
<p>Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models
21 May 2025CD61E4E627F322BEC00F01DC82902423arXiv:2409.16635v2[cs.AI]
This paper proposes a novel prompting approach, Judgment of Thought (JoT), specifically tailored for binary logical reasoning tasks.Despite advances in prompt engineering, existing approaches still face limitations in handling complex logical reasoning tasks.To address these issues, JoT introduces a multiagent approach with three specialized roleslawyer, prosecutor, and judge-where a highlevel model acts as the judge, and lower-level models serve as lawyer and prosecutor to systematically debate and evaluate arguments.Experimental evaluations on benchmarks such as BigBenchHard and Winogrande demonstrate JoT's superior performance compared to existing prompting approaches, achieving notable improvements, including 98% accuracy in Boolean expressions.Also, our ablation studies validate the critical contribution of each role, iterative refinement loops, and feedback mechanisms.Consequently, JoT significantly enhances accuracy, reliability, and consistency in binary reasoning tasks and shows potential for practical applications.</p>
<p>Introduction</p>
<p>Recent advances in AI and natural language processing (NLP) have brought major changes to many industries (Vaswani et al., 2017;Peters et al., 2018;Devlin et al., 2019).In particular, Large Language Models (LLMs) have shown impressive performance on a wide range of language tasks, such as text generation, translation, and sentiment analysis (Floridi and Chiriatti, 2020;Touvron et al., 2023;Zhao et al., 2023;Chang et al., 2024;Achiam et al., 2023).These models are trained on massive datasets and have learned to understand and generate language in flexible, general-purpose ways (Zhang et al., 2024).</p>
<p>However, to get high-quality results from LLMs, it's important to carefully design the input textknown as a prompt (Wahle et al., 2024).The way a prompt is written can greatly affect how accurate, helpful, or logical the model's output is (Benedetto et al., 2024).This practice, called prompt engineering, helps guide LLMs to produce responses that match the user's goals (Schulhoff et al., 2024;Sahoo et al., 2024;Wang et al., 2024).</p>
<p>Many prompting approaches have been proposed to improve reasoning quality.These include zero-shot and few-shot prompting, and Chain-of-Thought (CoT) prompting, which encourages the model to explain its reasoning step by step (Wei et al., 2021;Kaplan et al., 2020;Touvron et al., 2023;Wei et al., 2022;Wang et al., 2022).More recently, Khan et al. (2024) showed that debatestyle prompting, where a stronger model argues against a weaker one, can lead to more accurate answers-especially when evaluated by a third model acting as a judge.Despite these advances, current prompting methods still have limitations: especially for binary decisions that require careful reasoning.Tasks involving subtle logic, ambiguity, or conflicting claims often lead to inconsistent or incorrect answers.Existing methods do not always handle disagreements well or allow for step-by-step resolution of complex issues.</p>
<p>To address these challenges, we propose a new prompting framework called Judgment of Thought (JoT).JoT is designed for binary logical reasoning and introduces three roles: a lawyer, a prosecutor, and a judge.These roles engage in a structured, debate-style dialogue where the lawyer argues for a position, the prosecutor argues against it, and the judge evaluates both sides to reach a final decision.</p>
<p>We evaluate JoT on benchmark datasets such as BigBenchHard and Winogrande.The results show that JoT consistently outperforms existing prompting methods in BigBenchHard and Winogrande.Notably, JoT achieved remarkable performance metrics, including 98% accuracy on the Boolean Expressions task, 90% accuracy on the challenging Web of Lies task, and 88% accuracy on the Navigate task, clearly emphasizing its strengths in complex logical reasoning scenarios.Importantly, these performance outcomes were consistently observed across different model architectures including OpenAI models as well as Anthropic Claude models, highlighting JoT's robust generalizability.We also conduct ablation studies to better understand the contribution of each component.These experiments confirmed that all parts of JoT-the lawyer, prosecutor, and judge roles, as well as the iterative loops and feedback mechanism-are important for producing strong and reliable reasoning.</p>
<p>In summary, JoT offers a new approach to prompting LLMs for binary decision-making.Our evaluation results demonstrate that JoT produces more accurate and consistent results, advancing the state of prompt engineering for complex binary reasoning tasks.(The source code and data will be openly available upon publication.)</p>
<p>Background</p>
<p>LLMs are trained on massive, general-purpose datasets (Floridi and Chiriatti, 2020;Touvron et al., 2023;Zhao et al., 2023;Chang et al., 2024;Anthropic, 2024).To get specific, accurate, or nuanced outputs, how we ask a question really matters (Nan et al., 2023).Also, prompt engineering, the practice of designing and refining input prompts to guide a LLM, shapes how LLMs "think" by influencing tone, structure, depth, and style, making it essential for precision and control in AIgenerated responses (Schulhoff et al., 2024;Grabb, 2023).To systematically guide LLM behavior, researchers have proposed a variety of prompting strategies-such as zero-shot, few-shot, and chainof-thought -each offering distinct advantages for improving task alignment, reasoning quality, and output consistency (Achiam et al., 2023).</p>
<p>Prompting strategies differ not only in format but also in the type of reasoning they activate in language models.Zero-shot prompting tasks the model with solving a problem based solely on a textual instruction, relying entirely on its internalized knowledge (Wei et al., 2021).Few-shot prompting extends this approach by incorporating a small number of input-output examples within the prompt, thereby guiding the model toward the desired task behavior and output format (Kaplan et al., 2020;Touvron et al., 2023).Chainof-thought prompting further extends the few-shot paradigm by encouraging intermediate reasoning steps, enabling the model to better handle tasks requiring logical inference or multi-hop reasoning (Wei et al., 2022;Madaan et al., 2023).Empirical results consistently show that chain-of-thought prompting improves performance on tasks such as mathematical problem solving and commonsense QA.To further enhance this reasoning process, selfconsistency prompting improves the reliability of chain-of-thought outputs by sampling multiple reasoning paths and selecting the most consistent final answer (Wang et al., 2022).In addition, various prompting strategies exist each tailored to specific purposes and modalities (Guo et al., 2024;Li et al., 2023;Cao et al., 2023;Ha et al., 2023).</p>
<p>Despite these advancements, existing prompt engineering methods still face significant limitations in complex binary inference tasks involving sub-tle logical reasoning, ambiguous contexts, or contentious decisions.Current approaches lack robust mechanisms for effectively resolving interpretive conflicts or systematically evaluating competing lines of reasoning, often resulting in suboptimal or inconsistent performance.Motivation.Recent work by Khan et al. (Khan et al., 2024) demonstrates the effectiveness of structured debates between large language models (LLMs).Their framework poses a question to two expert LLMs assigned opposing answers, prompting each to generate persuasive arguments before presenting the exchange to a weaker judgeeither a less capable model or a human without access to source material.This debate-based setup enables the judge to identify the more truthful position based on the merits of the arguments alone, without requiring ground-truth labels or external evidence.The study shows that when debaters are optimized for persuasiveness, judges can reliably favor correct over incorrect answers, achieving 76% accuracy on the HARD subset of QuALITY dataset (Pang et al., 2022).This approach highlights the promise of multi-agent prompting as a scalable strategy for supporting logical inferences.</p>
<p>While the debate framework proposed by Khan et al. (Khan et al., 2024) shows that having two models argue can lead to more truthful answers, the study demonstrate that it has several limitationsespecially for tasks that require careful logical reasoning about yes/no questions.Because both models play similar roles in the debate, their arguments can become vague or repetitive, without clear responsibilities for how they should argue.This becomes problematic in real-world questions such as "surveillance programs violate privacy rights" where one side should provide strong evidence and the other should point out flaws or alternatives.In addition, the framework produces a final decision, but the reasoning process is not clearly structured or easy to interpret.This makes it difficult to use in domains where transparency and explanation are essential, such as policy, law, or scientific argumentation.Moreover, the format does not require models to reason step by step or follow a consistent logical structure, and thus, persuasive, but shallow, claims can still win the debate.</p>
<p>Inspired by this prior work and aiming to overcome the limitations, we introduce a new prompting framework designed to support more reliable and interpretable logical inference in binary decision-making tasks.The lawyer and prosecutor use lower-level models to argue different aspects of a problem.The judge uses a higher-level model to evaluate these arguments and deliver a comprehensive judgment.This process enables thorough analysis from multiple perspectives, leading to balanced solutions for complex problems.</p>
<p>Judgment of Thought (JoT)</p>
<p>In this section, we introduce a novel prompting approach, Judgment of Thought (JoT).The overall structure and workflow of JoT are illustrated in Figure 2. JoT mimics deliberative human reasoning (e.g., legal or debate settings) to support more intuitive, transparent, and trustworthy decision-making for end users.In JoT, each unit-the lawyer, prosecutor, and judge-is prompted using role-specific system instructions, detailed in the Appendix A. This structured role design encourages the generation of logically coherent, step-by-step arguments rather than superficially persuasive claims.The framework follows an iterative process in which a higher-level model (e.g., GPT-4 Omni) is assigned to the judge role, while lower-level models (e.g., GPT-3.5-turbo)serve as the lawyer and prosecutor.This configuration allows the judge to critically evaluate the submitted arguments, with an emphasis on assessing both their logical structure and argumentation.</p>
<p>Initially, each role receives a tailored system message: the lawyer and prosecutor are explicitly instructed to systematically advocate for the True and False positions, respectively, on a given task.The lawyer generates arguments supporting the truthfulness of the statement, while the prosecutor provides arguments opposing it.Subsequently, both units present their reasoning clearly to the judge.The judge then analyzes the logical coherence with the provided arguments and gives feedback highlighting their strengths and weaknesses.</p>
<p>In each iterative loop, the lawyer and prosecutor incorporate the judge's feedback and the opposing unit's arguments to refine their reasoning, systematically addressing identified logical gaps and reinforcing argumentative depth.These refined arguments are again presented to the judge for further evaluation.This loop continues iteratively, allowing the judge to progressively identify the most logically robust arguments.In the final loop, the lawyer and prosecutor present their concluding arguments, explicitly integrating insights from previous evaluations and rebuttals.Throughout this iterative process, users gain clear visibility into the evolution of logical reasoning underpinning the judge's decisions.In summary, the JoT prompting has the following attractive properties.Balanced Reasoning: JoT assigns reasoning tasks to distinct roles, reducing bias and ensuring balanced consideration of both sides in binary tasks.Logical Consistency: By explicitly enforcing adversarial reasoning and direct comparison of opposing viewpoints, JoT mitigates the risk of inconsistent or contradictory outputs.Iterative Refinement: JoT supports multi-round feedback and revision, allowing arguments to evolve and strengthen over time.Interpretability: JoT exposes each agent's reasoning, along with the judge's evaluation rationale, providing transparent visibility into the model's logical decision-making process.Modularity and Flexibility: JoT's modular architecture allows independent improvement or customization of individual roles.</p>
<p>Evaluation</p>
<p>We evaluate the JoT by answering the following research questions: (1) How does JoT perform across different types of logical reasoning (e.g., causal inference, Boolean logic, fallacy detection)?</p>
<p>(2) Does JoT outperform existing prompting methods in logical reasoning tasks?(3) How do the structural components of the JoT framework contribute to its overall performance in logical reasoning tasks?</p>
<p>Evaluation Setup</p>
<p>We conducted systematic performance evaluations of Judgement of Thought (JoT) across diverse logical reasoning tasks.Experiments were conducted using GPT-3.5-turbo(OpenAI, 2023) and GPT-4o (Omni) (Hurst et al., 2024), which were selected for their differing capability levels to enable a robust comparison of each prompting method.Also Claude-3-Haiku, and Claude-3.5-Haiku(Anthropic, 2024) were used to further evaluate the generalizability and consistency of the results across models from different providers.All models were run with default parameters (temperature=1, top-p=1).</p>
<p>Our evaluation dataset comprised two main components.First, we used Winogrande (Sakaguchi et al., 2021), a benchmark designed to assess largescale pronoun resolution.Second, we adopted a subset of binary reasoning tasks from the BigBench-Hard dataset (Srivastava et al., 2022), chosen for their emphasis on complex language understanding and logical reasoning.Specifically, we evaluated JoT on the following BigBenchHard tasks: Boolean Expressions: logical formula evaluation, Causal Judgment: reasoning over cause-effect relations, Formal Fallacies: identifying flawed logical arguments, Web of Lies: validating the truthfulness of interconnected statements, Navigate: spatial reasoning based on instructions.These tasks test reasoning capabilities and serve as a strong benchmark for evaluating JoT's effectiveness.</p>
<p>In addition, we compared Judgement of Thought (JoT) with several established prompting approaches: Zero-shot, Few-shot, Chain-of-Thought (CoT), Self-Consistency (SC), and Debate (as proposed by Khan et al.).These baselines were selected based on their demonstrated strengths in handling logical reasoning tasks.The evaluation was conducted by averaging results over 10 runs.</p>
<p>For iterative prompting methods (SC, Khan et al., and JoT), the number of reasoning samples (for-loop parameter) was uniformly set to 3, ensuring methodological consistency across approaches.Using 3 samples in iterative prompting methods strikes a balance between computational efficiency and accuracy, offering a reasonable tradeoff between cost and performance.Furthermore, the same few-shot examples-generated by Zeroshot CoT-were used across the Few-shot, CoT, and SC settings to maintain consistency and enable fair comparisons.</p>
<p>We employed two evaluation metrics: Accuracy and F1 Score.Accuracy measured the proportion of correct predictions, while F1 Score captured the harmonic mean of precision and recall.Together, these metrics offered a comprehensive view of each method's performance, highlighting their respective strengths and limitations across various tasks and datasets.</p>
<p>Evaluation Result on Benchmarks</p>
<p>We report the evaluation results of Judgement of Thought (JoT) and the other prompting approaches based on accuracy and F1 score, using the Big-BenchHard and Winogrande datasets.The results using GPT 3.5-Turbo and GPT-4o are summarized in Table 1.Also evaluation results using Claude models are provided in Table 5.Furthermore, Appendix B presents a detailed output variability through 16 resampling runs to illustrate the consistency of each approach's behavior.Summary of results using GPT models.Overall, the evaluation shows that JoT significantly improves logical reasoning across a variety of tasks.Its step-by-step, role-based structure supports deeper analysis, organized rebuttals, and more reliable decisions-highlighting both its innovative design and practical value.Boolean Expressions.JoT achieved an accuracy of 98% and an F1 score of 0.98, significantly surpassing all other methods.This strong performance is due to JoT's debate-style approach, which clearly presents opposing arguments and helps resolve logical ambiguities through step-by-step reasoning.Causal Judgment.JoT achieved 74% accuracy and an F1 score of 0.72, outperforming the next best method, Self-Consistency, which scored 67%.JoT's structured dialogue helps make causal relationships clearer, leading to more accurate identification of cause-and-effect patterns.Navigate.JoT showed strong reasoning skills, with 88% accuracy and an F1 score of 0.87.Its stepby-step approach helps the model keep track of and interpret spatial instructions more effectively, improving its performance on navigation tasks.Web of Lies.In tasks that require evaluating complex chains of truth, JoT achieved 90% accuracy and an F1 score of 0.91.Its multi-turn feedback process helps the model better track and analyze connected statements, making it reliable.Formal Fallacies.JoT scored 77% in both accuracy and F1, showing that it can effectively detect and analyze logical fallacies.Although this is the lowest score among the benchmarks, JoT's rebuttal process encourages careful examination of flawed reasoning, which contributes to its solid performance on this challenging task.Winogrande.JoT achieved 89% accuracy and an F1 score of 0.89 on pronoun resolution tasks, outperforming other methods.Its argument-based, multi-perspective approach helps the model better understand context and resolve ambiguous references more accurately.Summary of results using Claude models.Although the overall scores are lower than those obtained using GPT models, JoT consistently outperformed other prompting strategies across all evaluated benchmarks in both accuracy and F1 score, demonstrating its strong ability to support structured and reliable logical reasoning.</p>
<p>It is worth noting that Self-Consistency builds on Chain-of-Thought (CoT) by running it multiple times and choosing the majority answer.However, because this method is computationally expensive, we excluded it from this evaluation.Case studies.Figure 4 illustrates the logical reasoning process of JoT.As shown in the examples, each role generated logically coherent, step-bystep arguments, while the judge critically evaluated these arguments-focusing on both the strength of the reasoning and the argumentation.</p>
<p>Ablation Study on JoT</p>
<p>Role Contributions in JoT.To better understand the individual contributions of the lawyer and prosecutor roles in the JoT framework, we conducted an ablation study by removing each role in turn.The results are summarized in Table 3, which compares performance changes across reasoning tasks.</p>
<p>Overall, removing either the prosecutor or the lawyer resulted in a notable drop in performance.The ablation study confirms that both roles are integral and complementary in JoT's reasoning process.Their interaction is crucial for achieving high performance across logical reasoning tasks.Effect of Loop Iterations.We further explored how varying the number of iterative loops in JoT affects performance.Table 4 shows results for loop counts of 1, 3, and 5 iterations.</p>
<p>In summary, increasing the number of loops in JoT generally led to better or stable performance across tasks.These results highlight the effectiveness of JoT's iterative mechanism in improving the precision, robustness, and consistency of logical reasoning.tasks.While the impact is minimal in simpler tasks like Boolean expressions, feedback proves especially beneficial in more complex settings such as causal judgment.Even in tasks where gains are marginal, the feedback improves performance, confirming its overall value as a mechanism for  strengthening logical decision-making within JoT.</p>
<p>Discussion</p>
<p>Comparison JoT with CoT and Debate.In comparing the proposed JoT framework with existing methodologies such as CoT and structured Debate (Khan et al.), several key differences emerge that underscore JoT's advantages in binary logical reasoning tasks, as illustrated in Figure 4.</p>
<p>CoT typically relies on a single agent generating a linear, one-sided rationale.This linear reasoning process could overlook potential counterarguments, reducing robustness and comprehensiveness.On the other hand, the structured Debate method proposed by Khan et al. employs a high-capability model as the debater and a lower-capability model as the judge.While the study was designed to ex-plore the question, "Can weaker models assess the correctness of stronger models?", the asymmetry between debater and judge could cause that the weaker judge may be persuaded by well-articulated but logically flawed arguments.</p>
<p>In contrast, JoT uses an adversarial reasoning process with three clearly defined roles-lawyer, prosecutor, and judge-each with a specific task.These roles take turns interacting with each other in multiple rounds, helping to gradually refine their arguments.This makes JoT more effective for accurate and thorough reasoning in complex binary decision-making tasks.Improvement of Feedback.While JoT demonstrates strong performance across a variety of reasoning tasks, our analysis suggests that its feedback mechanism can be further refined to enhance effectiveness in more complex domains.Currently, the judge's feedback is rule-based and follows a fixed structure in every iteration.This static format may limit the model's ability to adapt to specific task challenges or increase attention to unclear or incomplete arguments from earlier rounds.</p>
<p>One possible improvement is to make the feedback more adaptive.The judge could adjust its level of detail or focus based on the strength of the previous arguments.For example, using dynamic prompting strategies that revise evaluation criteria or add counterexamples could help the model reason more effectively.</p>
<p>Another promising direction is to introduce memory-augmented feedback.Instead of only considering the latest exchange, the judge could keep track of earlier inconsistencies or missed points across multiple rounds.This could lead to stronger reasoning, particularly in tasks like causal judgment or formal fallacies, where logical steps build on one another.</p>
<p>In summary, while JoT's feedback mechanism is already effective, we believe that introducing more flexible, context-aware feedback strategies could further improve its reasoning quality and adaptability across a wide range of tasks.Real-World Application.Although JoT has shown strong performance on benchmark tasks, its effectiveness in real-world scenarios remains less certain.The current experiments were conducted on controlled datasets (BigBenchHard and Winogrande), which differ significantly from the ambiguity and unpredictability often found in realworld applications.</p>
<p>Practical reasoning tasks frequently involve incomplete, noisy, or ambiguous inputs-conditions that were not fully represented in our evaluation.The absence of tests in applied domains limits our understanding of JoT's robustness and utility in handling real-world tasks.</p>
<p>To address this gap, future research should investigate JoT's adaptability to real-world tasks by incorporating domain-specific knowledge and contextual reasoning.One promising direction is integrating JoT with Domain-Specific Retrieval-Augmented Generation (DS-RAG) (Siriwardhana et al., 2023), which enables models to retrieve and incorporate relevant external information.This could significantly improve JoT's performance in specialized domains such as law, or cybersecurity.</p>
<p>In addition, enhancing computational efficiency is critical to enabling JoT's deployment in realtime or large-scale settings.Making JoT more lightweight and responsive will be essential for its application in high-throughput or latency-sensitive environments.</p>
<p>Extending JoT to real-world contexts will require both architectural improvements and integration with domain-specific tools.Doing so will be key to validating its practical value, reliability, and scalability beyond controlled benchmarks.</p>
<p>Conclusion</p>
<p>In this paper, we proposed Judgment of Thought (JoT), a novel prompting framework designed for binary logical reasoning.JoT introduces an adversarial reasoning process involving three distinct roles-lawyer, prosecutor, and judge-to promote accuracy, consistency, and interpretability.Our evaluation results demonstrated that JoT outperforms existing prompting approaches across multiple benchmark tasks.Also, ablation studies showed the importance of JoT's core design elements.</p>
<p>Future work should focus on extending JoT to real-world applications by incorporating Domain-Specific Retrieval-Augmented Generation (DS-RAG) methods and improving computational efficiency.These advancements will be essential for scaling JoT to complex, dynamic environments and ensuring its practical reliability and effectiveness.</p>
<p>Prompt Engineering.Prompt engineering alone is often insufficient to guarantee consistent performance, as prompting methods remain vulnerable to prompt sensitivity, poor generalization to unseen tasks, and unpredictable model behavior in complex or ambiguous scenarios.Because the system often uses large models with multiple rounds of sampling to get strong results, it may not be practical in settings where efficiency and cost matter.Open-Source Model Generalizability.This study evaluated JoT using closed-source models (Ope-nAI's GPT series and Claude), which may limit into its performance and generalizability when applied to open-source models.Future research should include evaluations on open-source models to comprehensively assess JoT's broader applicability and reliability across various modeling environments.Real-World Application.This study primarily relied on benchmark datasets such as BigBenchHard and Winogrande.Real-world scenarios typically involve more complex, noisy, or ambiguous data, which might affect JoT's practical performance.Future work should validate JoT on real-world tasks to better understand its robustness and effectiveness in applied contexts.Computational Cost.JoT employs a multi-agent approach with iterative loops, making it computationally resource-intensive.This characteristic could limit its applicability in resource-constrained environments.Optimizing JoT to balance computational efficiency and performance is an important direction for future research.</p>
<p>Figure 5: Boxplots illustrating the resampling results, comparing the variability and robustness of existing prompt engineering techniques and JoT.Self-Consistency was excluded from this comparison due to its reliance on repeated executions, which incur substantial computational costs.For a detailed comparison of trends between Self-Consistency and other methods, please refer to Table 1</p>
<p>Figure 1 :
1
Figure 1: Comparison of Judgment of Thought (ours) with recent prompting strategies.</p>
<p>Figure 2 :
2
Figure 2: Judgment of Thought (JoT) Architecture.It consists of three roles: lawyer, prosecutor, and judge.The lawyer and prosecutor use lower-level models to argue different aspects of a problem.The judge uses a higher-level model to evaluate these arguments and deliver a comprehensive judgment.This process enables thorough analysis from multiple perspectives, leading to balanced solutions for complex problems.</p>
<p>Figure 3 :
3
Figure 3: Case studies highlighting how JoT resolves binary reasoning tasks through adversarial dialogue.</p>
<p>Figure 4 :
4
Figure 4: Comparative illustration of the reasoning paradigms in CoT, Debate (Khan et al.), and the proposed Judgment of Thought(ours) frameworks.</p>
<p>Table 1 :
1
Accuracy/F1 Score Comparison Across Different Benchmarks and Models Using Various Prompt Engineering Method and the Proposed JoT Method.For SC, Khan et al., and JoT, 3 loops were used in all cases.
DatasetModelZero-shot Few-shotCoTSCKhan et al.JoTBoolean expressionsGPT-3.5-Turbo 67%/0.76 55%/0.55 47%/0.29 43%/0.17 81%/0.84 98%/0.98 GPT-4o 86%/0.89 91%/0.93 84%/0.88 87%/0.90Causal judgementGPT-3.5-Turbo 62%/0.55 61%/0.61 61%/0.52 59%/0.48 61%/0.61 74%/0.72 GPT-4o 66%/0.71 65%/0.65 63%/0.60 67%/0.65BigBenchHardNavigateGPT-3.5-Turbo 54%/0.18 55%/0.12 57%/0.04 56%/0.00 60%/0.63 88%/0.87 GPT-4o 68%/0.48 62%/0.27 63%/0.30 64%/0.31Web of liesGPT-3.5-Turbo 47%/0.18 51%/0.35 44%/0.20 46%/0.07 53%/0.49 90%/0.91 GPT-4o 54%/0.44 49%/0.50 44%/0.46 51%/0.53Formal fallaciesGPT-3.5-Turbo 45%/0.58 50%/0.31 57%/0.30 54%/0.23 60%/0.66 77%/0.77 GPT-4o 52%/0.62 61%/0.61 61%/0.61 57%/0.56WinograndeGPT-3.5-Turbo 60%/0.60 58%/0.60 54%/0.57 63%/0.64 59%/0.43 89%/0.89 GPT-4o 82%/0.83 77%/0.77 82%/0.83 82%/0.83DatasetModelZero-shot Few-shotCoTKhan et al.JoTBoolean expressions3-Haiku 3.5-Haiku62%/0.65 57%/0.49 66%/0.67 76%/0.81 76%/0.81 80%/0.8345%/0.3486%/0.88Causal judgement3-Haiku 3.5-Haiku61%/0.38 64%/0.57 59%/0.44 57%/0.47 63%/0.39 63%/0.6054%/0.2667%/0.66BigBenchHardNavigate Sport understanding3-Haiku 3.5-Haiku 3-Haiku 3.5-Haiku54%/0.47 58%/0.09 56%/0.08 61%/0.42 63%/0.59 63%/0.53 71%/0.72 74%/0.74 61%/0.49 70%/0.77 78%/0.80 81%/0.8265%/0.49 44%/0.0069%/0.66 82%/0.79Web of lies3-Haiku 3.5-Haiku47%/0.33 45%/0.50 52%/0.57 53%/0.32 54%/0.51 48%/0.5057%/0.2558%/0.50Formal fallacies3-Haiku 3.5-Haiku57%/0.58 49%/0.47 45%/0.30 54%/0.36 60%/0.38 56%/0.4157%/0.2571%/0.69Winogrande3-Haiku 3.5-Haiku58%/0.55 58%/0.56 66%/0.59 62%/0.60 62%/0.58 70%/0.6860%/0.4671%/0.63</p>
<p>Table 2 :
2
Accuracy/F1 Score Comparison Across Different Benchmarks and Models Using Various Prompt Engineering Method and the Proposed JoT Method in Claude models.For SC, Khan et al., and JoT, 3 loops were used.
DatasetWithout ProsecutorWithout LawyerJoTBoolean expressions95%/0.9695%/0.95 98%/0.98BigCausal judgement68%/0.7068%/0.53 74%/0.72BenchNavigate72%/0.7665%/0.72 88%/0.87HardWeb of lies69%/0.7464%/0.55 90%/0.91Formal fallacies65%/0.6668%/0.56 77%/0.77Winogrande85% / 0.86 82% / 0.82 89%/0.89</p>
<p>Table 3 :
3
Ablation Study on Accuracy/F1 Score: Effect of Removing the Lawyer or Prosecutor from JoT.Effect of Feedback.We investigated the impact of feedback within the JoT framework by comparing performance with and without iterative feedback.As shown in Table5, the results demonstrate that incorporating feedback within the JoT framework generally leads to improved performance across
Dataset1 Iteration 3 Iterations 5 IterationsBoolean expressions98%/0.98 98%/0.9899%/0.99BigCausal judgement65%/0.60 74%/0.7274%/0.73BenchNavigate 87%/0.83 88%/0.8791%/0.89HardWeb of lies87%/0.88 90%/0.9191%/0.91Formal fallacies70%/0.67 77%/0.7778%/0.78Winogrande87%/0.87 89%/0.8989%/0.89Table 4: Ablation Study on Accuracy/F1 Score: Effectof Increasing Loop Iterations in JoT.</p>
<p>Table 5 :
5
Ablation Study on Accuracy/F1 Score: Effect of Feedback in JoT.
DatasetWithout FeedbackWith FeedbackBoolean expressions96%/0.9798%/0.98BigCausal judgement69%/0.6374%/0.72BenchNavigate87%/0.8388%/0.87HardWeb of lies87%/0.8890%/0.91Formal fallacies70%/0.7077%/0.77Winogrande87%/0.8789%/0.89
A Used prompts for JoT Lawyer: Role: You are an expert lawyer specialized in logical reasoning.Your task is to argue persuasively that the correct answer to the given input is <Positive Position>.You will address the judge directly and present logical arguments and evidence.Procedure: You have a total of 3 opportunities to speak, each with a clear purpose: 1.First utterance: Briefly analyze the input, describe its key logical characteristics, and outline your main arguments supporting a <Positive Position>response. 2. Second utterance: Logically counter the prosecutor's arguments, clearly addressing any concerns or questions raised by the judge.Reinforce your arguments with logical precision.3. Final utterance: Concisely summarize the strongest logical points, reiterate how you've effectively countered the prosecution, and firmly establish why the answer must be <Positive Position>.Style: Be concise, highly structured, and persuasive.Clearly address all potential doubts raised by the prosecutor or judge.Prosecutor: Role: You are an expert prosecutor specialized in logical reasoning.Your task is to argue persuasively that the correct answer to the given input is <Negative Position>.You will address the judge directly and present logical arguments and evidence.Procedure: You have a total of 3 opportunities to speak, each with a clear purpose: 1.First utterance: Briefly analyze the input, describe its key logical characteristics, and outline your main arguments supporting a <Negative Position>response. 2. Second utterance: Logically counter the lawyer's arguments, clearly addressing any concerns or questions raised by the judge.Reinforce your arguments with logical precision.3. Final utterance: Concisely summarize the strongest logical points, reiterate how you've effectively countered the lawyer's arguments, and firmly establish why the answer must be <Negative Position>.Style: Be concise, highly structured, and persuasive.Clearly address all potential doubts raised by the lawyer or judge.Judge: Role: You are an expert judge specialized in logical reasoning.Your task is to carefully analyze the given input and the logical arguments provided by both a lawyer (arguing for <Positive Position>) and a prosecutor (arguing for <Negative Position>, then decisively determine whether the correct answer is <Positive Position>or <Negative Position>.Important: You must remain strictly neutral, unbiased, and objective.Base your decision solely on logical strength and coherence of the presented arguments, disregarding personal beliefs or external biases.Procedure: You will issue three judgments in total.For each judgment, you must: 1. Analyze the input thoroughly along with the arguments presented by both the lawyer and the prosecutor.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Introducing the next generation of claude. Anthropic, 2024</p>
<p>Using llms to simulate students' responses to exam questions. Luca Benedetto, Giovanni Aradelli, Antonia Donvito, Alberto Lucchetti, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024Andrea Cappelli, and Paula Buttery</p>
<p>Beautiful-Prompt: Towards automatic prompt engineering for text-to-image synthesis. Tingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, 10.18653/v1/2023.emnlp-industry.1Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. the 2023 Conference on Empirical Methods in Natural Language Processing: Industry TrackAssociation for Computational LinguisticsJun Huang. 2023Singapore</p>
<p>Yidong Wang, and 1 others. 2024. A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, ACM Transactions on Intelligent Systems and Technology. 153</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines. Luciano Floridi, Massimo Chiriatti, 202030</p>
<p>Sample design engineering: An empirical study on designing better fine-tuning samples for information extraction with LLMs. Declan Grabb, ; Biyang Guo, He Wang, Wenyilin Xiao, Hong Chen, Zhuxin Lee, Songqiao Han, Hailiang Huang, 10.18653/v1/2024.emnlp-industry.43Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track. the 2024 Conference on Empirical Methods in Natural Language Processing: Industry TrackMiami, Florida, USAssociation for Computational Linguistics2023. 20246The impact of prompt engineering in large language model performance: a psychiatric example</p>
<p>Meta-learning of prompt generation for lightweight prompt engineering on language-modelas-a-service. Hyeonmin Ha, Jihye Lee, Wookje Han, Byung-Gon Chun, 10.18653/v1/2023.findings-emnlp.159Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023Association for Computational Linguistics</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Hayes, arXiv:2410.21276Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Debating with more persuasive llms leads to more truthful answers. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Tim Samuel R Bowman, Ethan Rocktäschel, Perez, arXiv:2402.067822024arXiv preprint</p>
<p>Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, Brian Ichter, arXiv:2312.04474Chain of code: Reasoning with a language model-augmented code emulator. 2023arXiv preprint</p>
<p>What makes chain-of-thought prompting effective? a counterfactual study. Aman Madaan, Katherine Hermann, Amir Yazdanbakhsh, 10.18653/v1/2023.findings-emnlp.101Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Enhancing text-to-sql capabilities of large language models: A study on prompt design strategies. Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, Dragomir Radev, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Gpt-3.5-turbo. 2023OpenAI</p>
<p>QuALITY: Question answering with long input texts, yes!. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Samuel Bowman, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Deep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. 2024arXiv preprint</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 202164</p>
<p>Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, H Han, arXiv:2406.06608The prompt report: A systematic survey of prompting techniques. 20245arXiv preprintSevien Schulhoff, and 1 others</p>
<p>Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering. Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, Suranga Nanayakkara, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Gupta, arXiv:2206.04615Adrià Garriga-Alonso, and 1 others. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Paraphrase types elicit prompt engineering capabilities. Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp, 10.18653/v1/2024.emnlp-main.617Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Investigating the impact of prompt engineering on the performance of large language models for standardizing obstetric diagnosis text: comparative study. Lei Wang, Wenshuai Bi, Suling Zhao, Yinyao Ma, Longting Lv, Chenwei Meng, Jingru Fu, Hanlin Lv, JMIR Formative Research. 8e532162024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, arXiv:2109.01652arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235and 1 others</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, 10.18653/v1/2024.emnlp-main.498Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zhang, arXiv:2303.18223Zican Dong, and 1 others. 2023. A survey of large language models. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
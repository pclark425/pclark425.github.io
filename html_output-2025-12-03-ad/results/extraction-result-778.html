<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-778 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-778</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-778</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-254069888</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.16002v1.pdf" target="_blank">DiffG-RL: Leveraging Difference between State and Common Sense</a></p>
                <p><strong>Paper Abstract:</strong> Taking into account background knowledge as the context has always been an important part of solving tasks that involve natural language. One representative example of such tasks is text-based games, where players need to make decisions based on both description text previously shown in the game, and their own background knowledge about the language and common sense. In this work, we investigate not simply giving common sense, as can be seen in prior research, but also its effective usage. We assume that a part of the environment states different from common sense should constitute one of the grounds for action selection. We propose a novel agent, DiffG-RL, which constructs a Difference Graph that organizes the environment states and common sense by means of interactive objects with a dedicated graph encoder. DiffG-RL also contains a framework for extracting the appropriate amount and representation of common sense from the source to support the construction of the graph. We validate DiffG-RL in experiments with text-based games that require common sense and show that it outperforms baselines by 17% of scores. The code is available at https://github.com/ibm/diffg-rl</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e778.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e778.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiffG-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiffG-RL (Difference Graph Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL agent for text-based games that constructs a Difference Graph linking extracted environment state nodes and external common-sense graph triples, encodes that graph with a GIN-like graph encoder, and selects actions via a learned scorer over admissible textual commands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DiffG-RL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DiffG-RL has four main components: (1) Environment States Extractor (EE) that parses each textual observation (using an AMR parser) to extract interactive objects and their current state nodes and maintain them in a dynamic knowledge-graph-like belief state; (2) Common Sense Extractor (CE) that retrieves and filters triples from an external common-sense graph source (Visual Genome) via three substeps: Extract-by-Meaning (embedding-similarity matching), Narrow-by-Circumstance (keep "interactive object -> location" style triples), and Transform-into-Grounded-Representation (TGR) that aligns extracted triple terms to in-game entity representations; (3) Difference Encoder (DE) that builds a per-interactive-object Difference Graph containing the interactive-object node, its current state nodes and the grounded common-sense nodes, encodes node text with a bidirectional GRU node encoder, and iteratively aggregates neighbors using a GIN-inspired graph neural network with distinct learnable parameters per node type (interactive object, state, common-sense node); (4) Action Selector (AS) that encodes admissible textual commands with a shared bidirectional GRU, concatenates command vectors with the difference-graph output d_t, and scores/selects actions via an MLP scorer (trained with reinforcement learning). The agent is trained end-to-end (policy learned with RL) and uses the Difference Graph representation as the basis for planning/selecting actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) games / TWC dataset (text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Choice-based text games framed as a POMDP: at each step the agent receives a textual observation and a set of admissible short textual commands. Challenges include partial observability (agent cannot see full world state), long-term dependencies, sparse rewards, many interactive objects and large combinatorial action spaces, and requirement for commonsense knowledge about object-location relations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Visual Genome (VG) scene-graph commonsense knowledge base (triples); GloVe word embeddings for semantic matching; AMR parser for extracting states from text. (Baselines and related methods referenced also use ConceptNet and OpenIE.)</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured triples (subject, relation, object) from Visual Genome transformed into grounded entity nodes; vector embeddings (from GloVe) used for similarity matching; graph nodes/edges (Difference Graph) as structured data; textual command vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A dynamic, graph-structured belief state: environment extractor produces entity and interactive-object nodes (tracked across steps) using AMR; common-sense triples are grounded and added as nodes associated with each interactive object to form a Difference Graph per object. Node textual content is encoded with bidirectional GRUs; graph node features are iteratively updated via a GIN-like aggregation (separate learnable matrices per node type) to form the belief representation used for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the Environment States Extractor parses the current observation (AMR) and updates entities E_t, interactive objects I_t, and their current state nodes (locations etc.). The Common Sense Extractor provides a set of grounded common-sense triples (after EbM, NbC, TGR) which are aligned to in-game entity representations. The Difference Graph for each interactive object is reconstructed/updated with current-state nodes and matched common-sense nodes; node features are re-encoded and aggregated with the graph encoder so the belief state reflects both observed state and relevant tool-derived (common-sense) expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (reinforcement learning) that conditions on a graph-structured belief (Difference Graph) to produce action probabilities; the Difference Graph provides an explicit representation that functions as a planning substrate (graph-based reasoning) rather than explicit search/A*.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is performed by selecting discrete textual move commands (e.g., "go north", "go south") via the learned policy conditioned on the Difference Graph; no explicit shortest-path algorithm is used — navigation decisions emerge from the graph-encoded belief and trained scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>On the TWC OUT test set (Hard difficulty, normalized score 0–1): DiffG-RL achieves 0.35 ± 0.02 (normalized score). This is with the full pipeline (EE + CE + DE + AS) using Visual Genome common-sense and the Difference Graph.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly representing differences between observed environment state and external common-sense in a Difference Graph substantially improves sample efficiency and generalization on partially observable text-based tasks; grounding and filtering commonsense (EbM, NbC, TGR) is necessary to keep extraction tractable; the Difference Graph Encoder (DE) contributes more to performance than CE alone; common-sense graph nodes are explicitly incorporated into the belief and thus directly influence action selection rather than being aggregated into a single undifferentiated vector.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e778.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Actor-Critic (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that constructs a dynamic knowledge graph belief state from textual observations (using OpenIE) and uses that graph representation within an actor-critic RL agent to prune and score natural-language actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs and maintains a dynamic knowledge graph extracted from observation text (OpenIE / information extraction) that contains entities and relations; the graph is used as the agent's belief/state representation which conditions a policy network (actor-critic) to score admissible textual commands. The KG is updated as observations arrive.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (used as a baseline on TWC in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same TWC/choice-based text-game setup as DiffG-RL: partially observable textual environment with admissible commands.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenIE (information extraction) to build the knowledge graph from text; GloVe embeddings used in implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured triples / knowledge-graph nodes and edges derived from textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic knowledge graph constructed from OpenIE extractions and updated per observation; graph nodes represent entities and relations and function as the agent's belief/state.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Observation text is processed by OpenIE to extract triples which are added/updated in the KG belief. The KG is maintained across timesteps to reflect observed entity states.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (actor-critic) conditioned on a knowledge-graph belief; not explicit symbolic search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Selects textual navigation commands via the learned policy using the KG-conditioned scorer; no explicit path planner reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>On TWC OUT Hard (normalized score): 0.33 ± 0.01 (reported in the paper as the KG-A2C baseline performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-structured belief (KG) helps prune action space and is a strong baseline; however, naive combinations with external common-sense can be brittle without an explicit mechanism to represent differences between observed state and commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e778.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWC agent-CN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TWC agent (using ConceptNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that uses external common-sense knowledge from ConceptNet to bias or constrain action selection in TextWorld Commonsense tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TWC agent-CN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A baseline agent from prior work that integrates multiple common-sense graphs (e.g., ConceptNet) with state information to improve exploration and action selection; in earlier implementations common-sense triples are aggregated to influence policy (the paper notes aggregation into single vectors can over-bias decisions).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable TWC text tasks requiring commonsense object-location relations; large action spaces and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet knowledge graph (triples).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured triples (subject, relation, object) and aggregated vector representations derived from them.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Aggregated common-sense vector(s) combined with state features; not explicitly a per-object grounded difference graph (as criticism in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Common-sense triples are retrieved and combined (in earlier TWC formulations, possibly aggregated) and used alongside state representation to guide action selection; paper notes unclear correspondence between states and common-sense in that approach.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy using combined state and common-sense representations; can perform look-ahead planning in prior work but lacks explicit per-object difference representation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Selects move commands via policy; issues reported with repeated move commands and failure to reach correct rooms in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>On TWC OUT Hard (normalized score): 0.29 ± 0.02 (reported in the paper for TWC agent-CN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using common-sense as an undifferentiated aggregate can help but may also bias actions incorrectly (e.g., when commonsense for one object affects another); explicit state-to-commonsense correspondence (as in DiffG-RL) improves robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e778.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWC agent-VG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TWC agent (using Visual Genome)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A TWC baseline that uses Visual Genome scene-graph triples as the external common-sense source, shown to be more grounded for object-location relations than ConceptNet in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Commonsense Knowledge from Scene Graphs for Textual Environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TWC agent-VG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as the TWC agent but uses Visual Genome (VG) scene-graph triples as the common-sense source; the agent incorporates VG-derived triples into its representation to bias action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text-based TWC games requiring object-location commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Visual Genome (VG) scene-graph triples.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured triples (scene-graph triplets), possibly aggregated vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Combined state representation with VG-derived common-sense information; prior TWC approach does not explicitly align each common-sense triple per-object in a difference graph.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>VG triples are filtered/matched to in-game entities (prior work used exact matches; this paper extends that idea by using embedding similarity) and used to influence policy.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy conditioned on state and VG common-sense; not an explicit search-based planner.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Chooses textual move commands via policy. Reported to sometimes fail on OUT set due to overfitting in training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>On TWC OUT Hard (normalized score): 0.25 ± 0.01 (reported baseline using Visual Genome).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a more grounded common-sense source (VG) can improve behavior, but without explicit state-to-common-sense alignment (difference representation), agents may still overfit or misapply commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e778.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWC agent-VG+KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive combination of TWC agent-VG with KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A straightforward method that concatenates or combines the TWC agent using Visual Genome commonsense with KG-A2C's environment knowledge graph without an explicit difference-encoding mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TWC agent-VG+KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Naive fusion of an environment-state knowledge graph (KG-A2C) and external VG common-sense by combining their representations without a dedicated Difference Graph encoder or CE filtering; used here as a baseline to show that naive combination can be worse than using either alone when scale increases.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text games; combining large state graphs and many common-sense triples increases representational load.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenIE-derived state KG and Visual Genome common-sense triples.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graphs/triples and aggregated vector representations.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A union/concatenation of state KG and common-sense features without an explicit per-object difference representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Both kinds of graph-derived features are updated/maintained and fed to the policy, but without explicit alignment leading to potential interference.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy conditioned on combined representations; no special planner.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Selects textual movement commands using the combined representation; reported to perform poorly on harder tasks compared to DiffG-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>On TWC OUT Hard (normalized score): 0.25 ± 0.01 (reported in paper as TWC agent-VG+KG-A2C).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Naive combination of large state graphs and commonsense graphs can hurt performance (vulnerable to scale and interference); motivates the need for explicit alignment (Difference Graph) and commonsense filtering/grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e778.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BiKE (Bidirectional Knowledge Exchange)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related method that shares information between the environment state graph and a common-sense graph via a bidirectional attention mechanism, focusing primarily on similar node information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BiKE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Links state and common-sense graphs with bidirectional attention to share information; emphasizes shared/similar nodes between the two graphs rather than explicitly representing differences.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based game environments (referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable textual game settings where state and external knowledge graphs are available.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>State knowledge graph and external common-sense graphs (e.g., ConceptNet/VG) via attention mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graphs and attention-weighted feature vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Two graphs (state and common-sense) with bidirectional attention to exchange information; does not explicitly encode state-common-sense differences.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Information is shared between graphs via attention based on node similarity; details are referenced but not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy using combined attention-mediated graph features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Related approach that couples state and common-sense but focuses on shared information rather than the differences; mentioned as complementary but limited in addressing difference-driven action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e778.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-DQN (Knowledge Graph DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses OpenIE to build a knowledge graph of the game's belief state and uses that graph to constrain and inform action selection within a DQN framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a knowledge graph from text observations (OpenIE) and uses it as the agent's state representation inside a DQN (deep Q-network) architecture to handle large textual action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable textual games where building an explicit KG helps pruning the action space.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenIE for extraction of triples.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured triples / knowledge graph nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Dynamic knowledge graph belief state created from OpenIE extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>KG updated stepwise with OpenIE extractions; used to provide state features for the DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Value-based RL (DQN) conditioned on KG features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates utility of dynamic KG belief states for text-based RL; cited as prior art for using extracted structured beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e778.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e778.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Worldformer / GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Worldformer and GATA (dynamic belief-graph models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that learn and use dynamic belief graphs or world models to predict next-step belief graphs jointly with action generation, thereby providing an explicit learned belief update/prediction mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning knowledge graph-based world models of textual environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Worldformer / GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Worldformer trains world models where the agent jointly predicts the next-step belief graph and the next action; GATA uses self-supervised learning to construct and update belief graphs. Both maintain explicit dynamic belief graph representations and learn to update/predict them.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based environments (referenced as prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable textual games requiring maintenance and prediction of belief states.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internal learned models to predict belief graphs; may use OpenIE/KG construction as pre-processing in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Predicted graph structures (nodes/edges) and associated embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Learned dynamic belief graphs / world models that can be predicted forward one step to support planning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Learned next-step belief graph prediction (world-model) that augments observation-based updates.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-based flavor (world-model prediction) combined with learned policy/action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows that predicting/updating belief graphs explicitly can improve generalization; cited in this paper as complementary and motivating richer belief representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DiffG-RL: Leveraging Difference between State and Common Sense', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games. <em>(Rating: 2)</em></li>
                <li>Learning knowledge graph-based world models of textual environments. <em>(Rating: 2)</em></li>
                <li>Commonsense Knowledge from Scene Graphs for Textual Environments. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-778",
    "paper_id": "paper-254069888",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "DiffG-RL",
            "name_full": "DiffG-RL (Difference Graph Reinforcement Learning)",
            "brief_description": "An RL agent for text-based games that constructs a Difference Graph linking extracted environment state nodes and external common-sense graph triples, encodes that graph with a GIN-like graph encoder, and selects actions via a learned scorer over admissible textual commands.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DiffG-RL",
            "agent_description": "DiffG-RL has four main components: (1) Environment States Extractor (EE) that parses each textual observation (using an AMR parser) to extract interactive objects and their current state nodes and maintain them in a dynamic knowledge-graph-like belief state; (2) Common Sense Extractor (CE) that retrieves and filters triples from an external common-sense graph source (Visual Genome) via three substeps: Extract-by-Meaning (embedding-similarity matching), Narrow-by-Circumstance (keep \"interactive object -&gt; location\" style triples), and Transform-into-Grounded-Representation (TGR) that aligns extracted triple terms to in-game entity representations; (3) Difference Encoder (DE) that builds a per-interactive-object Difference Graph containing the interactive-object node, its current state nodes and the grounded common-sense nodes, encodes node text with a bidirectional GRU node encoder, and iteratively aggregates neighbors using a GIN-inspired graph neural network with distinct learnable parameters per node type (interactive object, state, common-sense node); (4) Action Selector (AS) that encodes admissible textual commands with a shared bidirectional GRU, concatenates command vectors with the difference-graph output d_t, and scores/selects actions via an MLP scorer (trained with reinforcement learning). The agent is trained end-to-end (policy learned with RL) and uses the Difference Graph representation as the basis for planning/selecting actions.",
            "environment_name": "TextWorld Commonsense (TWC) games / TWC dataset (text-based games)",
            "environment_description": "Choice-based text games framed as a POMDP: at each step the agent receives a textual observation and a set of admissible short textual commands. Challenges include partial observability (agent cannot see full world state), long-term dependencies, sparse rewards, many interactive objects and large combinatorial action spaces, and requirement for commonsense knowledge about object-location relations.",
            "is_partially_observable": true,
            "external_tools_used": "Visual Genome (VG) scene-graph commonsense knowledge base (triples); GloVe word embeddings for semantic matching; AMR parser for extracting states from text. (Baselines and related methods referenced also use ConceptNet and OpenIE.)",
            "tool_output_types": "Structured triples (subject, relation, object) from Visual Genome transformed into grounded entity nodes; vector embeddings (from GloVe) used for similarity matching; graph nodes/edges (Difference Graph) as structured data; textual command vectors.",
            "belief_state_mechanism": "A dynamic, graph-structured belief state: environment extractor produces entity and interactive-object nodes (tracked across steps) using AMR; common-sense triples are grounded and added as nodes associated with each interactive object to form a Difference Graph per object. Node textual content is encoded with bidirectional GRUs; graph node features are iteratively updated via a GIN-like aggregation (separate learnable matrices per node type) to form the belief representation used for action selection.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the Environment States Extractor parses the current observation (AMR) and updates entities E_t, interactive objects I_t, and their current state nodes (locations etc.). The Common Sense Extractor provides a set of grounded common-sense triples (after EbM, NbC, TGR) which are aligned to in-game entity representations. The Difference Graph for each interactive object is reconstructed/updated with current-state nodes and matched common-sense nodes; node features are re-encoded and aggregated with the graph encoder so the belief state reflects both observed state and relevant tool-derived (common-sense) expectations.",
            "planning_approach": "Learned policy (reinforcement learning) that conditions on a graph-structured belief (Difference Graph) to produce action probabilities; the Difference Graph provides an explicit representation that functions as a planning substrate (graph-based reasoning) rather than explicit search/A*.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation is performed by selecting discrete textual move commands (e.g., \"go north\", \"go south\") via the learned policy conditioned on the Difference Graph; no explicit shortest-path algorithm is used — navigation decisions emerge from the graph-encoded belief and trained scorer.",
            "performance_with_tools": "On the TWC OUT test set (Hard difficulty, normalized score 0–1): DiffG-RL achieves 0.35 ± 0.02 (normalized score). This is with the full pipeline (EE + CE + DE + AS) using Visual Genome common-sense and the Difference Graph.",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Explicitly representing differences between observed environment state and external common-sense in a Difference Graph substantially improves sample efficiency and generalization on partially observable text-based tasks; grounding and filtering commonsense (EbM, NbC, TGR) is necessary to keep extraction tractable; the Difference Graph Encoder (DE) contributes more to performance than CE alone; common-sense graph nodes are explicitly incorporated into the belief and thus directly influence action selection rather than being aggregated into a single undifferentiated vector.",
            "uuid": "e778.0",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "Knowledge Graph Actor-Critic (KG-A2C)",
            "brief_description": "A baseline agent that constructs a dynamic knowledge graph belief state from textual observations (using OpenIE) and uses that graph representation within an actor-critic RL agent to prune and score natural-language actions.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Constructs and maintains a dynamic knowledge graph extracted from observation text (OpenIE / information extraction) that contains entities and relations; the graph is used as the agent's belief/state representation which conditions a policy network (actor-critic) to score admissible textual commands. The KG is updated as observations arrive.",
            "environment_name": "Text-based games (used as a baseline on TWC in this paper)",
            "environment_description": "Same TWC/choice-based text-game setup as DiffG-RL: partially observable textual environment with admissible commands.",
            "is_partially_observable": true,
            "external_tools_used": "OpenIE (information extraction) to build the knowledge graph from text; GloVe embeddings used in implementations.",
            "tool_output_types": "Structured triples / knowledge-graph nodes and edges derived from textual observations.",
            "belief_state_mechanism": "Dynamic knowledge graph constructed from OpenIE extractions and updated per observation; graph nodes represent entities and relations and function as the agent's belief/state.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Observation text is processed by OpenIE to extract triples which are added/updated in the KG belief. The KG is maintained across timesteps to reflect observed entity states.",
            "planning_approach": "Learned policy (actor-critic) conditioned on a knowledge-graph belief; not explicit symbolic search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Selects textual navigation commands via the learned policy using the KG-conditioned scorer; no explicit path planner reported.",
            "performance_with_tools": "On TWC OUT Hard (normalized score): 0.33 ± 0.01 (reported in the paper as the KG-A2C baseline performance).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Graph-structured belief (KG) helps prune action space and is a strong baseline; however, naive combinations with external common-sense can be brittle without an explicit mechanism to represent differences between observed state and commonsense.",
            "uuid": "e778.1",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "TWC agent-CN",
            "name_full": "TWC agent (using ConceptNet)",
            "brief_description": "An agent that uses external common-sense knowledge from ConceptNet to bias or constrain action selection in TextWorld Commonsense tasks.",
            "citation_title": "Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations.",
            "mention_or_use": "use",
            "agent_name": "TWC agent-CN",
            "agent_description": "A baseline agent from prior work that integrates multiple common-sense graphs (e.g., ConceptNet) with state information to improve exploration and action selection; in earlier implementations common-sense triples are aggregated to influence policy (the paper notes aggregation into single vectors can over-bias decisions).",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "Partially observable TWC text tasks requiring commonsense object-location relations; large action spaces and sparse rewards.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet knowledge graph (triples).",
            "tool_output_types": "Structured triples (subject, relation, object) and aggregated vector representations derived from them.",
            "belief_state_mechanism": "Aggregated common-sense vector(s) combined with state features; not explicitly a per-object grounded difference graph (as criticism in this paper).",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Common-sense triples are retrieved and combined (in earlier TWC formulations, possibly aggregated) and used alongside state representation to guide action selection; paper notes unclear correspondence between states and common-sense in that approach.",
            "planning_approach": "Learned policy using combined state and common-sense representations; can perform look-ahead planning in prior work but lacks explicit per-object difference representation.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Selects move commands via policy; issues reported with repeated move commands and failure to reach correct rooms in some cases.",
            "performance_with_tools": "On TWC OUT Hard (normalized score): 0.29 ± 0.02 (reported in the paper for TWC agent-CN).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Using common-sense as an undifferentiated aggregate can help but may also bias actions incorrectly (e.g., when commonsense for one object affects another); explicit state-to-commonsense correspondence (as in DiffG-RL) improves robustness.",
            "uuid": "e778.2",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "TWC agent-VG",
            "name_full": "TWC agent (using Visual Genome)",
            "brief_description": "A TWC baseline that uses Visual Genome scene-graph triples as the external common-sense source, shown to be more grounded for object-location relations than ConceptNet in this domain.",
            "citation_title": "Commonsense Knowledge from Scene Graphs for Textual Environments.",
            "mention_or_use": "use",
            "agent_name": "TWC agent-VG",
            "agent_description": "Same architecture as the TWC agent but uses Visual Genome (VG) scene-graph triples as the common-sense source; the agent incorporates VG-derived triples into its representation to bias action selection.",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "Partially observable text-based TWC games requiring object-location commonsense.",
            "is_partially_observable": true,
            "external_tools_used": "Visual Genome (VG) scene-graph triples.",
            "tool_output_types": "Structured triples (scene-graph triplets), possibly aggregated vectors.",
            "belief_state_mechanism": "Combined state representation with VG-derived common-sense information; prior TWC approach does not explicitly align each common-sense triple per-object in a difference graph.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "VG triples are filtered/matched to in-game entities (prior work used exact matches; this paper extends that idea by using embedding similarity) and used to influence policy.",
            "planning_approach": "Learned policy conditioned on state and VG common-sense; not an explicit search-based planner.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Chooses textual move commands via policy. Reported to sometimes fail on OUT set due to overfitting in training.",
            "performance_with_tools": "On TWC OUT Hard (normalized score): 0.25 ± 0.01 (reported baseline using Visual Genome).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Using a more grounded common-sense source (VG) can improve behavior, but without explicit state-to-common-sense alignment (difference representation), agents may still overfit or misapply commonsense.",
            "uuid": "e778.3",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "TWC agent-VG+KG-A2C",
            "name_full": "Naive combination of TWC agent-VG with KG-A2C",
            "brief_description": "A straightforward method that concatenates or combines the TWC agent using Visual Genome commonsense with KG-A2C's environment knowledge graph without an explicit difference-encoding mechanism.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "TWC agent-VG+KG-A2C",
            "agent_description": "Naive fusion of an environment-state knowledge graph (KG-A2C) and external VG common-sense by combining their representations without a dedicated Difference Graph encoder or CE filtering; used here as a baseline to show that naive combination can be worse than using either alone when scale increases.",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "Partially observable text games; combining large state graphs and many common-sense triples increases representational load.",
            "is_partially_observable": true,
            "external_tools_used": "OpenIE-derived state KG and Visual Genome common-sense triples.",
            "tool_output_types": "Structured graphs/triples and aggregated vector representations.",
            "belief_state_mechanism": "A union/concatenation of state KG and common-sense features without an explicit per-object difference representation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Both kinds of graph-derived features are updated/maintained and fed to the policy, but without explicit alignment leading to potential interference.",
            "planning_approach": "Learned policy conditioned on combined representations; no special planner.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Selects textual movement commands using the combined representation; reported to perform poorly on harder tasks compared to DiffG-RL.",
            "performance_with_tools": "On TWC OUT Hard (normalized score): 0.25 ± 0.01 (reported in paper as TWC agent-VG+KG-A2C).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Naive combination of large state graphs and commonsense graphs can hurt performance (vulnerable to scale and interference); motivates the need for explicit alignment (Difference Graph) and commonsense filtering/grounding.",
            "uuid": "e778.4",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "BiKE",
            "name_full": "BiKE (Bidirectional Knowledge Exchange)",
            "brief_description": "A related method that shares information between the environment state graph and a common-sense graph via a bidirectional attention mechanism, focusing primarily on similar node information.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "BiKE",
            "agent_description": "Links state and common-sense graphs with bidirectional attention to share information; emphasizes shared/similar nodes between the two graphs rather than explicitly representing differences.",
            "environment_name": "Text-based game environments (referenced in related work)",
            "environment_description": "Partially observable textual game settings where state and external knowledge graphs are available.",
            "is_partially_observable": true,
            "external_tools_used": "State knowledge graph and external common-sense graphs (e.g., ConceptNet/VG) via attention mechanisms.",
            "tool_output_types": "Structured graphs and attention-weighted feature vectors.",
            "belief_state_mechanism": "Two graphs (state and common-sense) with bidirectional attention to exchange information; does not explicitly encode state-common-sense differences.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Information is shared between graphs via attention based on node similarity; details are referenced but not elaborated in this paper.",
            "planning_approach": "Learned policy using combined attention-mediated graph features.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Related approach that couples state and common-sense but focuses on shared information rather than the differences; mentioned as complementary but limited in addressing difference-driven action selection.",
            "uuid": "e778.5",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "KG-DQN (Knowledge Graph DQN)",
            "brief_description": "An approach that uses OpenIE to build a knowledge graph of the game's belief state and uses that graph to constrain and inform action selection within a DQN framework.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "Builds a knowledge graph from text observations (OpenIE) and uses it as the agent's state representation inside a DQN (deep Q-network) architecture to handle large textual action spaces.",
            "environment_name": "Text-based games (general)",
            "environment_description": "Partially observable textual games where building an explicit KG helps pruning the action space.",
            "is_partially_observable": true,
            "external_tools_used": "OpenIE for extraction of triples.",
            "tool_output_types": "Structured triples / knowledge graph nodes and edges.",
            "belief_state_mechanism": "Dynamic knowledge graph belief state created from OpenIE extractions.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "KG updated stepwise with OpenIE extractions; used to provide state features for the DQN.",
            "planning_approach": "Value-based RL (DQN) conditioned on KG features.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Demonstrates utility of dynamic KG belief states for text-based RL; cited as prior art for using extracted structured beliefs.",
            "uuid": "e778.6",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Worldformer / GATA",
            "name_full": "Worldformer and GATA (dynamic belief-graph models)",
            "brief_description": "Approaches that learn and use dynamic belief graphs or world models to predict next-step belief graphs jointly with action generation, thereby providing an explicit learned belief update/prediction mechanism.",
            "citation_title": "Learning knowledge graph-based world models of textual environments.",
            "mention_or_use": "mention",
            "agent_name": "Worldformer / GATA",
            "agent_description": "Worldformer trains world models where the agent jointly predicts the next-step belief graph and the next action; GATA uses self-supervised learning to construct and update belief graphs. Both maintain explicit dynamic belief graph representations and learn to update/predict them.",
            "environment_name": "Text-based environments (referenced as prior work)",
            "environment_description": "Partially observable textual games requiring maintenance and prediction of belief states.",
            "is_partially_observable": true,
            "external_tools_used": "Internal learned models to predict belief graphs; may use OpenIE/KG construction as pre-processing in some setups.",
            "tool_output_types": "Predicted graph structures (nodes/edges) and associated embeddings.",
            "belief_state_mechanism": "Learned dynamic belief graphs / world models that can be predicted forward one step to support planning.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": "Learned next-step belief graph prediction (world-model) that augments observation-based updates.",
            "planning_approach": "Model-based flavor (world-model prediction) combined with learned policy/action generation.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Shows that predicting/updating belief graphs explicitly can improve generalization; cited in this paper as complementary and motivating richer belief representations.",
            "uuid": "e778.7",
            "source_info": {
                "paper_title": "DiffG-RL: Leveraging Difference between State and Common Sense",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations.",
            "rating": 2,
            "sanitized_title": "efficient_textbased_reinforcement_learning_by_jointly_leveraging_state_and_commonsense_graph_representations"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces.",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Learning knowledge graph-based world models of textual environments.",
            "rating": 2,
            "sanitized_title": "learning_knowledge_graphbased_world_models_of_textual_environments"
        },
        {
            "paper_title": "Commonsense Knowledge from Scene Graphs for Textual Environments.",
            "rating": 2,
            "sanitized_title": "commonsense_knowledge_from_scene_graphs_for_textual_environments"
        }
    ],
    "cost": 0.017998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DiffG-RL: Leveraging Difference between State and Common Sense
29 Nov 2022</p>
<p>Tsunehiko Tanaka tsunehiko@fuji.waseda.jp 
Daiki Kimura 
Michiaki Tatsubori </p>
<p>Waseda University</p>
<p>IBM Research</p>
<p>IBM Research</p>
<p>DiffG-RL: Leveraging Difference between State and Common Sense
29 Nov 2022D516CECA23D503ABA30E8FB0DB3DE9F4arXiv:2211.16002v1[cs.CL]
Taking into account background knowledge as the context has always been an important part of solving tasks that involve natural language.One representative example of such tasks is text-based games, where players need to make decisions based on both description text previously shown in the game, and their own background knowledge about the language and common sense.In this work, we investigate not simply giving common sense, as can be seen in prior research, but also its effective usage.We assume that a part of the environment states different from common sense should constitute one of the grounds for action selection.We propose a novel agent, DiffG-RL, which constructs a Difference Graph that organizes the environment states and common sense by means of interactive objects with a dedicated graph encoder.DiffG-RL also contains a framework for extracting the appropriate amount and representation of common sense from the source to support the construction of the graph.We validate DiffG-RL in experiments with text-based games that require common sense and show that it outperforms baselines by 17% of scores.The code is available at https://github.com/ibm/diffg-rl* Work done during an internship at IBM Research.Action Plans 1. take dirty fork from floor 2. insert dirty fork into dishwasher DiffG-RL dirty fork floor dishwasher Common Sense dirty fork dishwasher in Observation You've entered a kitchen.Here's a dishwasher and a fridge.You see a dirty fork on the floor.</p>
<p>Introduction</p>
<p>Taking into account background knowledge as the context has always been an important yet challenging part of solving tasks that involve natural language.One illustrative example of such challenges is text-based games.Text-based games are computer games where game states and action spaces are represented in pure texts.To play them, players have to not only understand in-game texts correctly but also make appropriate action decisions from given options according to the context.Computational agents required to solve such games naturally arise in the form of natural language processing Figure 1: An example of text-based games.Our proposed DiffG-RL summarizes the difference between the environment states and common sense in a graph and uses it as a basis for predicting a plan of action.</p>
<p>(NLP) systems trained with reinforcement learning (RL) algorithms.However, the intrinsic properties of text-based games such as partial observability, long-term dependencies, sparse reward signals, and large action spaces make it extremely challenging for RL agents to learn.Specifically, the chance of agents discovering optimal actions from the vast combinatorial action spaces is astronomically low.</p>
<p>Previous approaches (Ammanabrolu and Riedl, 2019b;Murugesan et al., 2021b;Sahith et al., 2020;Murugesan et al., 2021a;Kimura et al., 2020;Tanaka et al., 2022) have used external knowledge to constraint agents' action outputs in order to shrink the size of search space.Recently, Murugesan et al. (2021a) and Tanaka et al. (2022) utilized human common sense which improved sample efficiency and enabled agents to perform look-ahead planning.However, these approaches have not yet clarified how common sense should actually be used.Specifically, a huge amount of common sense is given at once, regardless of the environment states, and the correspondence between the states and common sense is unclear.This prevents agents from learning which common sense to use in which state, and the agents remember only the results after using common sense.</p>
<p>In this paper, as usage of common sense, we assume that differences between environment states and common sense can provide a basis for action selection and further improve sample efficiency.For example, consider the environment state of "dirty fork on the floor" and the common sense of "dirty fork should be in the dishwasher".The difference between floor and dishwasher in the location of the dirty fork helps agents plan their actions to pick it up from the floor and put it in the dishwasher.We construct a difference graph that maps environment states to common sense and explicitly represents their differences.Further, we develop an encoder dedicated to this graph and propose an agent that can effectively concentrate on learning which common sense to use in which state.An example is shown in Fig. 1.</p>
<p>Inevitable problems in constructing the difference graph are extracting the right amount and unifying the representation.For the first problem, large amounts of common sense cannot be encoded, and small amounts are insufficient for learning.In prior work, the amount of common sense is reduced by extracting only common sense that contains representations that exactly match the objects that appear in the environment, but in some tasks, there may be no common sense available due to mismatches of representations.In contrast, we extract the appropriate amounts of common sense based on semantics and the circumstances, independent of linguistic representations.For the second problem, to help agents recognize the difference between common sense and environmental states, their representations should be aligned.Therefore, we propose an extraction framework for common sense that includes an acquisition of appropriate amounts based on meanings and circumstances and a representation transformation that facilitates the mapping to environment states.</p>
<p>Our contributions in this work are as follows.</p>
<p>(1) We introduce a difference graph with an explicit representation of the difference between the environment states and common sense and a novel agent with a dedicated graph encoder.(2) We develop a framework for extracting common sense from sources to facilitate comparing the environment states with common sense.(3) We perform experiments with text-based games and demonstrate that our approach outperforms baselines, and evaluate the effect of each component in our approach through ablation studies.</p>
<p>Background</p>
<p>Text-based Games: Text-based games can be formally framed as partially observable Markov decision processes (POMDPs), represented as a 7tuple of S, T, A, Ω, O, R, γ denoting the set of environment states, conditional transition probabilities, actions, observations, conditional observation probabilities, reward function, and discount factor.We target choice-based games, where the player receives a textual observation o t ∈ Ω and sends a short textual phrase from action choices A to the environment as an action a t .Most text-based games contain entities (e 1 , e 2 , ..., e M ∈ E t ) such as items and location, and players often take actions on themselves ("go east") or on items ("take dirty fork").Common Sense and Text-based Games: Given common sense, an agent receives an observation o t to determine the next action by comparing it with the common sense.Common sense is represented by an external knowledge graph stored as triplets of subject, relationship, object , which is called a common sense graph.While the recently proposed TWC agent (Murugesan et al., 2021a) uses multiple graphs obtained from ConceptNet (Speer et al., 2017) and combines them, (Tanaka et al., 2022) showed that just a single graph can suffice if Visual Genome (VG) (Krishna et al., 2017) is used, as it contains more grounded graphs.We therefore use VG as a common sense source.There are two challenges when it comes to using VG with common sense: first, how to extract common sense from the source, and second, how agents use the common sense.In this work, we propose two methods to individually address these challenges.Environment States Extractor: In this study, the current states of the environment are extracted from the observation o t .Since states in text-based games have a graph-like structure, prior works (Ammanabrolu and Riedl, 2019a; Ammanabrolu and Hausknecht, 2020;Murugesan et al., 2021b;Adhikari et al., 2020) have represented environment states as a dynamic knowledge graph.KG-A2C introduced interactive objects (io 1 , io 2 , ..., io P ∈ I t ), which are items that allow agents to interact directly with the surrounding environment.We connect these interactive objects to their state nodes (e.g., the locations they are in) in the knowledge graph and then separate them from the entities (I t ⊂ E t ).We build on this to extract the states of interactive objects from observation text by using the Abstract Meaning Representation (AMR) parser (Zhou et al., 2021).Nodes connected to the interactive objects by positional relationships ("on," "in," Etc.) from AMR are extracted as the current states.We also add a node representing the player "You" and use it when the interactive objects are in the inventory.At each step, the entities E t , the interactive objects I t , and their current states are updated on the basis of the observations, and the states of all interactive objects are tracked.</p>
<p>3 DiffG-RL</p>
<p>In this section, we first present an overview of our proposed agent, called DiffG-RL, and then propose a framework for the extraction of common sense to facilitate the comparison of differences between environment states and common sense.Finally, we introduce the Difference Graph and its encoder, which provides a representation of the difference.</p>
<p>Model Overview</p>
<p>Our DiffG-RL agent with common sense for textbased games consists of the four components shown in Fig. 2.</p>
<p>Environment States Extractor (EE):</p>
<p>A component that extracts interactive objects and their current states from the observation texts o t .The details were described in Section 2.</p>
<p>Common Sense Extractor (CE):</p>
<p>A framework for extracting the appropriate amount and representation of common sense to facilitate comparisons of the environment states and common sense.</p>
<p>Difference Encoder (DE):</p>
<p>A graph encoder and a node encoder of a difference graph that organize the environment states and common sense for interactive objects.</p>
<p>Action Selector (AS): A component that includes an encoder of admissible commands</p>
<p>A t and a selector that determines the next action a t+1 from the output of the difference graph encoder and the command encoder.</p>
<p>Common Sense Extractor</p>
<p>Giving the common sense all at once (e.g., all the triples of graphs in ConceptNet) is excessive and inefficient for solving the tasks.We have also not identified any way to encode it all at once so far.</p>
<p>Existing research (Murugesan et al., 2021a;Tanaka et al., 2022) has narrowed it down to triples of common sense graphs (c 1 , c 2 , ..., c N ∈ C) that perfectly match the entities (e 1 , e 2 , ..., e M ∈ E) in text-based games, but common sense rarely has such a representation.We extract common sense based on word meaning and the circumstances of games, independent of linguistic representation, to broaden the scope of common sense extraction.In addition, the difference graph described in Section 3.3 needs unified representations of environment states and common sense to help agents understand the correspondences between them.To extract the appropriate amount and representation of common sense, we propose a framework consisting of three components: extracting by meaning (EbM), narrowing by circumstances (NbC), and transforming into grounded representation (TGR).</p>
<p>Extracting by Meaning</p>
<p>As the first step to extract triples of common sense graphs by meaning, we utilize the similarity between vectors obtained by word embedding instead of spell matching.The similarity sim is represented as
sim(s i , e j ) = s i • e j |s i ||e j | ,(1)
where s i is a subject in a triple of common sense graph c i , and e j represents an entity in text-based games.The bolded terms also represent vectors obtained by word embedding.If the similarity is greater than a preset threshold, it is considered to have a similar meaning.We calculate this for all combinations and then replace s i with an object in the triple of common sense graph and calculate them again.If both s i and o i are similar to one of E, its triple of common sense graph c i is extracted.</p>
<p>Note that because this component relaxes the constraints on common sense much more than with exact matching, the number of extracted triples of common sense graphs will be enormous. 1In most cases, it is necessary to use it in combination with the twc components introduced in 3.2.2 and 3.2.3.</p>
<p>Narrowing by Circumstances</p>
<p>We leave only triples of graphs that are in line with the circumstances of games, i.e. "interactive object → object's state".In many text-based games, "interactive object → location" (e.g., dirty fork → dishwasher) remains, while "location → location" (agents do not move the dishwasher into the fridge) etc. is removed.</p>
<p>Transforming into Grounded Representation</p>
<p>We transform the subject and object in the extracted triple of common sense graph into the entities to which they correspond in the first component, the EbM (Section 3.2.1).In the case of Eq. 1, s i is transformed into e j .This eliminates the influence of differences between the extracted common sense and games' representations and clarifies the correspondence between the environment states and common sense.</p>
<p>Difference Encoder</p>
<p>Difference Graph</p>
<p>We introduce the difference graph to represent the difference between the environment states and common sense to select common sense according to the states and to obtain the basis for the next action plan, as shown in Fig. 3.We define the difference graph as a representation of the situation where "an interactive object should be placed at A based on common sense, but is now placed at B" (a dirty fork should be placed at the dishwasher but is currently on the floor).The outputs of the current state extractor and the common sense extractor are organized by interactive objects (io 1 , io 2 , ..., io P ∈ I t ).The difference graph contains three types of nodes, with multiple current state nodes (st 1 , st 2 , ..., st U ∈ U(p)) and common sense nodes (co 1 , co 2 , ..., co V ∈ V(p)) corresponding to one interactive object node (io p ).For edges, there are two types: interactive object-current state and interactive object-common sense.After the TGR (Section 3.2.3), the common sense node has the same representation as the entities in games.</p>
<p>The difference graph is updated in accordance with the observation texts at each time step.</p>
<p>Node Encoder</p>
<p>We convert the words in a node of the difference graph into a series of vectors by word embedding and obtain a fixed-length vector using the node encoder.We use the fixed-length vector as the initial feature of each node in the difference graph encoder.We use bidirectional GRU (Cho et al., 2014) for the node encoder.</p>
<p>Difference Graph Encoder</p>
<p>We develop a graph encoder to encode the difference graph.Similar to recent graph neural networks, we update the features of a node by aggregating the features of its neighbors.The aggregate of our encoder is based on the Graph Isomorphism Network (GIN) (Keyulu et al., 2019) and is calcu-lated as
h (k) iop = MLP{φ(1 + W I )h (k−1) iop + stu∈ U (p) φ(W ST h (k−1) stu ) + cov∈V(p) φ(W CO h (k−1) cov )},(2)
where h
(k)
X represents the feature of X node with k iterations of the aggregation, φ represents an activation function, and MLP represents multi-layer perceptrons.</p>
<p>To distinguish between the three types of node and represent the difference between the current state and common sense, different learnable parameters are provided for each type: W I , W ST , and W CO .Since the actions are based on the interactive objects, the encoder only aggregates for the interactive object io p .In GIN, one MLP is used after the product with the learnable parameter because MLPs can represent a composition of functions, but we add an activation function φ for output simplification and training stability.</p>
<p>The aggregation can be repeated to reflect the features of distant nodes.As a results, the difference graph encoder can handle the environment states and common sense even if they become subgraphs consisting of multiple nodes.Note that we assume the maximum distance of 1 from the interactive object in the following experiments.</p>
<p>Action Selector</p>
<p>The action selector calculates the probability of each action from the concatenation of the vector representation a i t of the admissible command a i t ∈ A t and the output d t of the difference graph encoder.a i t is obtained by word embedding and the command encoder, similar to the node encoder in the difference graph encoder.We use the bidirectional GRU for the command encoder and share weight with the node encoder.The scorer consists of two MLP layers, a dropout layer, and an activation layer and calculates the probability
p a i t = Scorer(a i t , d t ).
4 Experiments</p>
<p>Environment</p>
<p>We conduct experiments with the TWC game (Murugesan et al., 2021a) to verify the difference between common sense and the environment states.The goal of the TWC game is to tidy up a house by putting items where they should be and requires common sense about the relationships between objects and their locations.We generate a new game set using the scripts from the original TWC study (Murugesan et al., 2021a).There are three difficulty levels depending on the number of rooms and the number of interactive objects, as shown in Tab. 1.Because the agent performance is affected by the number of objects/rooms, we unify the different numbers of these included in the same difficulty level in the original dataset.</p>
<p>To test the generalization performance, we introduce a supervised learning paradigm and split the dataset into three subsets: train, test, and valid.The original dataset (Murugesan et al., 2021a) contains only five games each in the train and test sets, but we generate 100 games and split them into train : test : valid = 50 : 40 : 10.</p>
<p>The TWC game contains two test sets: an IN set with the same entities as the train set and an OUT set consisting of entities that do not appear in the train set.The OUT set cannot be solved simply by memorizing the results (i.e., pairs of interactive objects and locations) after using common sense.Depending on the situation, such as the type of room the agents are in now, the same thing may be placed in different places between the train set and the OUT set.Therefore, the OUT set is used for the validation and ablation study because it is suitable for evaluating the ability to use common sense in a given situation.</p>
<p>Methods and Metrics</p>
<p>We use four baselines.</p>
<p>• KG-A2C (Ammanabrolu and Hausknecht, 2020) is a method with our implementation that organizes the environment states obtained from observation texts into a knowledge graph.</p>
<p>• TWC agent-CN (Murugesan et al., 2021a) is a method that uses common sense obtained from ConceptNet.• TWC agent-VG (Tanaka et al., 2022) is a method that utilizes the same model as TWC agent-CN but with VG as the common sense source.</p>
<p>• TWC agent-VG+KG-A2C is a method that naively combines the TWC agent-VG and KG-A2C without our proposed difference encoder and common sense extractor.</p>
<p>We evaluate performance on the normalized score computed by dividing the actual score by the maximum possible score.The scores range from 0 to 1, and higher is better.</p>
<p>Implementation and Training Details</p>
<p>In all methods, we use GloVe (Pennington et al., 2014) provided by GENSIM 2 for word embedding.The hidden size is set to 512 for DiffG-RL and to 300 for the baselines.The activation function is ELU (Clevert et al., 2016) for DiffG-RL and ReLU (Nair and Hinton, 2010) for the baselines.The threshold of similarity for the EbM (Section 3.2.1) is set to 0.3.</p>
<p>For training, we optimize all models for 100 epochs with Adam (Kingma and Ba, 2015) optimizer using a learning rate of 3.0 × 10 −5 and the default hyperparameters in PyTorch (Paszke et al., 2017).For tests, the model with the largest normalized score and the smallest number of steps in the validation is used.DiffG-RL can train 100 epochs in two and a half days using a single NVIDIA TI-TAN X (Pascal) GPU.</p>
<p>General Results</p>
<p>Table 2 lists   proach (DiffG-RL) trained for each difficulty level.DiffG-RL using the difference graph outperforms the baselines in all results.Specifically, DiffG-RL improves 40% in the hard level using the OUT set from TWC agent-VG, which uses only common sense, and 17% in the easy level using the OUT set from KG-A2C (previous SOTA), which uses only a knowledge graph.We can observe high performances on OUT, which cannot be solved by simply memorizing the results after using common sense in the training.This suggests that the representation of the difference between the environment states and common sense in the proposed approach contributes to learning how to use common sense.</p>
<p>Table 2 also shows that TWC agent-VG+KG-A2C struggles on the hard difficulty level and is less than or equal to KG-A2C and TWC agent-VG, which use only a knowledge graph of the environment states or common sense (not both).We believe this approach is vulnerable to an increase in the environment states and common sense as the number of interactive object increases.In con- trast, DiffG-RL shows a solid improvement over KG-A2C and TWC agent-VG, indicating that it is able to effectively utilize a combination of the environment states and common sense to deal with increased interactive objects.</p>
<p>Figure 4 shows the training curves at the medium level for the baselines and DiffG-RL, where it is clear that DiffG-RL performs the best.We believe the difference between the environment states and common sense has a positive impact on the decision making and improves the sample efficiency.</p>
<p>Considering both Tab. 2 and Fig. 4 together, in the medium level games, we can see that TWC agent-VG has a low performance on both test sets, despite its high performance in the training.In contrast, DiffG-RL performs well on all of the training and the two test sets.This reinforces our intuition that the difference graph of our proposed approach improves the generalization performance.</p>
<p>Ablation Study</p>
<p>Effect of Components: We investigate the effect of the components in DiffG-RL, and show the results in Tab. 3. Based on the field of information retrieval, we add precision and recall to the metric to evaluate how well the extraction method can extract the common sense needed to achieve the game goals.We first introduce the concept of a goal graph g 1 , g 2 , ..., gL ∈ G directly connecting interactive objects and their goal locations.The precision is calculated by dividing the number of triples of common sense graphs that correspond to the triple of goal graph C g ⊂ C by the total number N of C. The recall is calculated by dividing the number of triples of goal graphs that are covered by the triples of common sense graphs G c ⊂ G by the total number L of G.</p>
<p>We believe that an agent's performance can be improved by giving it a computationally feasible number of commonsense knowledge triples, which cover the common sense needed to solve the problem.Thus, higher both precision and recall are better, but there is a tradeoff between the two.We also use TWC agent-VG+KG-A2C as the most naive method (no.1), and since the performance difference between the proposed method and no.1 is the largest in Tab. 2, we use the hard level games in the OUT set for the score.</p>
<p>We can see that using EbM significantly improves the recall.This reinforces the effectiveness of our idea that common sense should be extracted by meaning independent of linguistic representation.We also see that the precision is also greatly improved by using NbC together.However, the number of triples of common sense graphs extracted is still huge, as indicated by the denominator values of the precision in no.3.No.2 and no.3 could not be executed because the number of triples exceeds the available GPU memory.Since TGR unified the representation of goal graphs and the extracted common sense graphs, multiple triples can be combined into one and the number of the extracted triples can be reduced.Therefore, we believe that the proposed components should not be used individually but as a framework that brings them together.</p>
<p>We also see from the difference in scores that both CE and DE contribute to the performance improvement.Comparing their respective contributions, we can see that DE has a greater impact on performance than the other components (4 → 5).This leads us to believe that the difference graph is relatively critical in DiffG-RL for its impact on the agents' decision-making.Similarity Threshold: We investigate the relationship between the similarity threshold in EbM and the number of the extracted triples of common sense graphs.We compute the precision and the recall between the extracted common sense graphs and the goal graphs in the same way as in Tab. 3 and show the results in Tab. 4. We can observe that the precision and the recall are a tradeoff tendency when the threshold exceeds 0.5.To maximize the performance of DE by providing common sense graphs that correspond to the goal graphs in all games, we focus on the recall in our experiments and set the threshold at 0.3.Note that lowering the threshold increases the noise (the precision decreases), so a method to increase the recall while keeping the threshold high will be required in the future (see Section 7).</p>
<p>Qualitative Results</p>
<p>Figure 5 shows a comparison of the agent's behavior between TWC agent-CN and DiffG-RL.The tasks are to put the dirty singlet in the laundry room (north) into the washing machine and to put the slippers in the suspended shelf in the laundry room into the shoe cabinet in the corridor (south).</p>
<p>TWC agent-CN puts the dirty singlet into the washing machine correctly, but it also puts the slippers into the washing machine wrongly.We assume that this is because by giving the agent a single vector that aggregates common sense knowledge triples about all interactive objects, the agent has been strongly affected by common sense for the dirty singlet.We also observe that for slippers where the goal exists in another room, it repeats the move commands (such as "go south" and "go north") and does not reach the correct location.This can be considered an incorrect understanding of the current state.However, DiffG-RL can put the three interactive objects back where they should be in order, even if it requires moving the room.We believe DiffG-RL is robust to such effects of common sense about other interactive objects because it explicitly encodes the correspondences between the current location and common sense for each interactive object using a difference graph.</p>
<p>Related Work</p>
<p>Common Sense for Text-based Games: Many recent methods have focused on providing common sense to agents to efficiently explore the vast observation and action spaces of text-based games.Murugesan et al. (2021a) proposed a text-based game TextWorld Commonsense (described as the TWC game in this paper) that requires common sense from agents and a baseline TWC agent that utilize common sense obtained from ConceptNet (Speer et al., 2017).Tanaka et al. (2022) proposed using scene graph datasets such as VG (Krishna et al., 2017) as a more grounded common sense source base on the TWC agent.BiKE (Murugesan et al., 2021b) shares information between the state graph and the common sense graph by means of a bidirectional attention mechanism, but focuses only on information that is similar between nodes.Ammanabrolu and Riedl (2019b) transfer common sense trained in other games into the target game strategy.Sahith et al. (2020) utilize common sense obtained in large-scale models such as COMET (Bosselut et al., 2019) and BERT (Devlin et al., 2019) based on KG-A2C (Ammanabrolu and Hausknecht, 2020).Common sense also has a high affinity with logic rules, and some studies (Kimura et al., 2020(Kimura et al., , 2021a,b;,b;Chaudhury et al., 2021) combine them to solve text-based games.Since these methods do not clarify the correspondence between the environment states and common sense, it is difficult to learn how to use common sense according to the situation.</p>
<p>In contrast, our work extracts common sense by meaning and graphically represents the difference between common sense and the environment states.Knowledge Graph Extraction: Some prior works have utilized state representations using knowledge graphs to effectively prune the vast observation and action space of text-based games.KG-DQN (Ammanabrolu and Riedl, 2019a) and KG-A2C use OpenIE (Angeli et al., 2015) to create a knowledge graph of the game's belief state from observation texts.GATA (Adhikari et al., 2020) uses self-supervised learning to train the construction and update the belief graph.Worldfromer (Ammanabrolu and Riedl, 2021) uses the world models to simultaneously tasks agents with a next-step belief graph prediction as well as the usual action generation.These methods utilize knowledge graph representation of the environment states, but do not use prior knowledge such as common sense.</p>
<p>Our work differs in that we utilize a knowledge graph to represent not only the environment states but also the correspondence with common sense in the form of the different graphs.Usage of Common Sense Extraction of suitable commonsense statements is well-studied (Ma et al., 2019;Lin et al., 2019), and recall of commonsense knowledge for a task receives much attention (Lin et al., 2019;Ilievski et al., 2020) in QA.However, as mentioned in 1, we focus on the denominator of precision as well as recall, which we believe is a new perspective.In addition, existing extraction methods (Ma et al., 2019;Lin et al., 2019) use triples with matched words.However, in this study, we use triples that are close in the distance between vectors after transformation by word embedding.</p>
<p>The filtering of common sense statements based on common sense is also studied in works related to the affordance of objects or defeasible reasoning (Qasemi et al., 2022;Rudinger et al., 2020;Do and Pavlick, 2021).They argue that the performance of state-of-the-art language models drops significantly in updating inferences when the context changes from the general situations by using their original tasks and datasets.In contrast, this paper proposes a model that explicitly represents the dynamically changing context of environmental states and common sense differences in RL and argues for its effectiveness through experiments.</p>
<p>Conclusion</p>
<p>In this work, we investigated the difference between the environment state and common sense as a basis for RL agent decision-making in textbased games.We proposed DiffG-RL, a novel agent that constructs a graph that represents the difference, along with a dedicated encoder, also contains a common sense extraction framework to obtain the appropriate amount and representation of common sense to facilitate the comparison between the environment states and the common sense.Our experimental results showed that DiffG-RL outperformed baselines that used only a knowledge graph, only common sense, or a naive combination of the two.These findings demonstrate the effectiveness of the difference graph, which is a representation of the difference between the environment states and common sense, for text-based games.</p>
<p>Limitations</p>
<p>An important aspect of our approach is that it utilizes the difference between the environment states and common sense as the basis for decisionmaking.However, the TWC game does not consider the relationships with other objects, which means the agents were sometimes not provided sufficient context.For example, it is difficult for an agent to determine whether a dirty fork should be placed in a dishwasher or on a dining table based solely on the information that it is holding a dirty fork in its hand.The location depends on further contexts, such as whether there is food left on the plates or whether the person eating is full.Since agents can only repeat their attempts based on scores in the TWC game, we hope to validate agents in games that provide more context, such as ALFWorld (Shridhar et al., 2021) and Science-World (Wang et al., 2022).Our approach should be able to support such high-context situations if we extend the representation of the environment states in the difference graph from node to sub-graph (details are provided in Section 3.3.3).</p>
<p>In addition, although we used GloVe for word embedding, this is slightly outdated considering the recent development of natural language processing.The performance of our approach could be further improved by using more up-to-date word embedding.This may also allow us to obtain a sufficient number of graphs even if we raise the similarity threshold, which was set to 0.3 in our experiments.</p>
<p>Our model does not use sensitive contexts such as legal or medical data.In addition, the dataset and common sense sources used in our experiments do not contain sensitive information.Since the actions taken by agents in the proposed model are based on the difference between the environment states and common sense, we can analyze the difference to reveal the reasons behind the actions.For example, if the model is biased, it can help to identify the cause of the biased behavior.However, when adding new common sense, it is necessary to thoroughly examine the model for bias, including that which has already been added.</p>
<p>the results of the IN and OUT test sets achieved by the baselines and the proposed ap-2 https://radimrehurek.com/gensim/</p>
<p>Figure 4 :
4
Figure 4: Performance evaluation for the medium level in the training.(Smoothing is performed to clarify the differences in the results of a single run.)</p>
<p>Figure 5 :
5
Figure 5: An comparison of agents' behavior.TWC agent-CN (left) and DiffG-RL (right) performed the task of placing a dirty singlet and slippers, which is in the hard difficulty level and uses the OUT set.</p>
<p>Common Sense Extractor Environment States Extractor Difference Graph Node Encoder Difference Graph Encoder Next Action a t+1 Admissible Commands A t Take a bell pepper from the table. Take a dirty fork from the table. Observation o t You've entered a kitchen. You see a dishwasher and a fridge. Command Encoder Scorer Weight Share Difference Encoder Action Selector dirty fork dishwasher bell pepper fridge Common Sense SourceC
Figure 2: An overview of our proposed DiffG-RL for text-based games based on the difference between the environ-ment states and common sense. The Model consists of four components: Environment States Extractor, CommonSense Extractor, Difference Encoder, and Action Selector.</p>
<p>Table 1 :
1
Specifications of TWC game that we use.
LevelInteractive objectsRoomsEasy11Medium31Hard72</p>
<p>Table 2 :
2
General results for two test games: IN within the training distribution of entities and OUT outside the distribution.All experiments were performed with five random seeds.Each value is a pair (average) ± (standard deviation).We highlight the best model in bold.
MethodEasyMediumHardKG-A2C (Ammanabrolu and Hausknecht, 2020)0.89 ± 0.020.76 ± 0.020.33 ± 0.01TWC agent-CN (Murugesan et al., 2021a)0.91 ± 0.020.75 ± 0.020.31 ± 0.01INTWC agent-VG (Tanaka et al., 2022)0.92 ± 0.010.69 ± 0.030.32 ± 0.02TWC agent-VG+KG-A2C0.95 ± 0.010.82 ± 0.020.26 ± 0.01DiffG-RL0.95 ± 0.000.82 ± 0.020.38 ± 0.02KG-A2C (Ammanabrolu and Hausknecht, 2020)0.78 ± 0.030.72 ± 0.020.33 ± 0.01OUTTWC agent-CN (Murugesan et al., 2021a) TWC agent-VG (Tanaka et al., 2022) TWC agent-VG+KG-A2C0.77 ± 0.03 0.78 ± 0.03 0.82 ± 0.030.69 ± 0.02 0.67 ± 0.02 0.72 ± 0.020.29 ± 0.02 0.25 ± 0.02 0.25 ± 0.01DiffG-RL0.91 ± 0.040.76 ± 0.020.35 ± 0.02</p>
<p>Table 3 :
3
Comparison of performance to evaluate the components, EbM, NbC, TGR (Section 3.2), and DE (Section 3.3) in DiffG-RL.We use hard level games in OUT.
No. EbM NbC TGRDEPrecisionRecallScores123 / 896 (2.6%)23 / 257 (6.4%)0.25 ± 0.0222.80M / 11.00M (25.6%)343 / 357 (96.1%)N/A32.80M / 6.05M (37.1%)343 / 357 (96.1%)N/A4343 / 5414 (6.3%)343 / 357 (96.1%)0.26 ± 0.015343 / 5414 (6.3%)343 / 357 (96.1%)0.35 ± 0.02</p>
<p>Table 4 :
4
Ablation results for the relationship between the similarity threshold (TH) in EbM and the extracted common sense graphs.CE denotes the common sense extractor (Section 3.2).We use goal graphs of hard level games in the OUT set.
No. CE THPrecisionRecall1-23 / 896 (2.6%)23 / 357 (6.4%)20.6 268 / 3707 (7.2%)268 / 357 (75.1%)30.5 321 / 2984 (10.8%) 321 / 357 (89.9%)40.4 341 / 5151 (6.6%)341 / 357 (95.5%)50.3 343 / 5414 (6.3%)343 / 357 (96.1%)
We tried doing the extraction with an NVIDIA TITAN X (Pascal) with 12 GB of memory, but the triples extracted from VG overwhelmed the available memory.
A AppendixA.1 Additional ResultsHidden Size: We investigate the optimal hidden size for DiffG-RL by training five agents with different hidden sizes at the medium level and testing them in the OUT set.The results are shown in Fig.5. Since we could not find a consistent trend, we used 512 in this work, which had the best performance throughout the experiments in this work.(Keyulu et al., 2019).Table6shows that the performance is better with the activation function than without.
Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, ( Xingdi, Marc-Alexandre Eric) Yuan, Mikulas Côté, Marc-Antoine Zelinka, Romain Rondeau, Pascal Laroche, Jian Poupart, Adam Tang, William L Trischler, Hamilton, 2020. 2020ACM</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2020</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics2019a1</p>
<p>Transfer in deep reinforcement learning using knowledge graphs. Prithviraj Ammanabrolu, Mark Riedl, 10.18653/v1/D19-5301Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13). the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)Hong Kong2019bAssociation for Computational Linguistics</p>
<p>Learning knowledge graph-based world models of textual environments. Prithviraj Ammanabrolu, Mark Riedl, Advances in Neural Information Processing Systems. 2021</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose , Johnson Premkumar, Christopher D Manning, 10.3115/v1/P15-1034Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Çelikyilmaz, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL). the 57th Annual Meeting of the Association for Computational Linguistics (ACL)2019</p>
<p>Neuro-symbolic approaches for text-based policy learning. Subhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, Asim Munawar, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/D14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Fast and accurate deep network learning by exponential linear units (elus). Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter, ICLR 20164th International Conference on Learning Representations. 2016</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Are rotten apples edible? challenging commonsense inference ability with exceptions. Nam Do, Ellie Pavlick, 10.18653/v1/2021.findings-acl.181Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Filip Ilievski, Pedro Szekely, Jingwei Cheng, Fu Zhang, Ehsan Qasemi, arXiv:2006.06114Consolidating commonsense knowledge. 2020arXiv preprint</p>
<p>How powerful are graph neural networks?. Xu Keyulu, Hu Weihua, Leskovec Jure, Jegelka Stefanie, International Conference on Learning Representations. 2019</p>
<p>LOA: Logical optimal actions for text-based interaction games. Daiki Kimura, Subhajit Chaudhury, Masaki Ono, Michiaki Tatsubori, Don Joven Agravante, Asim Munawar, Akifumi Wachi, Ryosuke Kohita, Alexander Gray, 10.18653/v1/2021.acl-demo.27Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2021a</p>
<p>Reinforcement learning with external knowledge by using logical neural networks. Daiki Kimura, Subhajit Chaudhury, Akifumi Wachi, Ryosuke Kohita, Asim Munawar, Michiaki Tatsubori, Alexander Gray, 2020In IJCAI-PRICAI-W</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agravante, Michiaki Tatsubori, Asim Munawar, Alexander Gray, 10.18653/v1/2021.emnlp-main.283Empirical Methods in Natural Language Processing. 2021bAssociation for Computational Linguistics</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations, ICLR 2015. San Diego, CA, USA2015. May 7-9, 2015Conference Track Proceedings</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 12312017</p>
<p>KagNet: Knowledge-aware graph networks for commonsense reasoning. Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen, Ren, 10.18653/v1/D19-1282Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>Towards generalizable neuro-symbolic systems for commonsense question answering. Kaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, Alessandro Oltramari, 10.18653/v1/D19-6003Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing. the First Workshop on Commonsense Inference in Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell ; Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, 10.18653/v1/2021.acl-short.91Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics2021a. 2021b2Thirty Fifth AAAI Conference on Artificial Intelligence</p>
<p>Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10. the 27th International Conference on International Conference on Machine Learning, ICML'10Madison, WI, USAOmnipress2010</p>
<p>Automatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS-W. 2017</p>
<p>GloVe: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, 10.3115/v1/D14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Paco: Preconditions attributed to commonsense knowledge. Ehsan Qasemi, F Liu, Muhao Chen, Pedro Szekely, 2022EMNLP</p>
<p>Thinking like a skeptic: Defeasible inference in natural language. Rachel Rudinger, Vered Shwartz, Jena D Hwang, Chandra Bhagavatula, Maxwell Forbes, Le Ronan, Noah A Bras, Yejin Smith, Choi, 10.18653/v1/2020.findings-emnlp.418Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Playing textbased games with common sense. Dambekodi Sahith, Frazier Spencer, Ammanabrolu Prithviraj, Mark O Riedl, Advances in Neural Information Processing Systems Workshops. 2020</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17. the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17AAAI Press2017</p>
<p>Commonsense Knowledge from Scene Graphs for Textual Environments. Tsunehiko Tanaka, Daiki Kimura, Michiaki Tatsubori, Thirty Fifth AAAI Conference on Artificial Intelligence Workshop (AAAIW). 2022</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022</p>
<p>Structure-aware fine-tuning of sequence-to-sequence transformers for transitionbased AMR parsing. Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, Young-Suk Lee, Radu Florian, Salim Roukos, 10.18653/v1/2021.emnlp-main.507Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>            </div>
        </div>

    </div>
</body>
</html>
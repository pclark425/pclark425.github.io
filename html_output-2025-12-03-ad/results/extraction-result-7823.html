<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7823 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7823</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7823</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-892dba6e3240433b30c8ac776c3b6dad61ebdfa9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/892dba6e3240433b30c8ac776c3b6dad61ebdfa9" target="_blank">Automated Statistical Model Discovery with Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a method for language model driven automated statistical model discovery within the principled framework of Box's Loop, and identifies models on par with human expert designed models and extends classic models in interpretable ways.</p>
                <p><strong>Paper Abstract:</strong> Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7823.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7823.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoxLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language model driven automated model discovery (BoxLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative framework that uses large language models to propose probabilistic programs (statistical models) from dataset representations and then critiques fitted models to guide further proposals, implemented within Box's Loop. Proposals are written as PyMC probabilistic programs and evaluated via probabilistic inference and posterior predictive checks; a critic LM digests fit statistics and posterior predictive summaries to produce natural-language feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 V (gpt4-11-06-preview), with GPT-3.5 Turbo used in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Few‑shot / in‑context prompting with chain‑of‑thought style instructions; a separate critic LM run deterministically (temperature 0.0); vision-capable prompt variant (multimodal). No fine‑tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not a collection of scholarly papers — the LM conditions on per‑task datasets (tabular data, plots), dataset metadata (natural language descriptions and feature descriptions), exemplar probabilistic programs, and natural‑language critic state. Datasets used include time series (Air, Beer, Heart, Milk, Wine, Wool), four Stan PosteriorDB real datasets (eight schools, dugongs, surgical, peregrine), simulated analogues of those datasets, and synthetic ODE datasets (perturbed Lotka–Volterra and other oscillators). No corpus of papers was ingested or used for automatic extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>The LLM is prompted to propose explicit probabilistic programs (PyMC or Python ODE code) that encode hypotheses/functional forms; these programs are fit with external tools (PyMC MCMC or gradient‑based fitting via JAX/diffrax), posterior predictive means/variances and fit scores (e.g., ELPD LOO or marginal likelihood) are computed, and a critic LM ingests program code plus these statistics to produce natural language feedback which is fed back for further in‑context exemplars (iterative proposal + critique loop). Chain‑of‑thought style reflection and code comments are requested in prompts. Visual-only prompting (plots) and textual dataframe prompting were both used.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Functional/parametric statistical relationships and mechanistic equations: e.g., Gaussian process kernel compositions (functional priors over functions), parametric growth laws (von Bertalanffy, logistic), ODE dynamical systems (Lotka–Volterra and extended forms), polynomial regressions, Beta‑Binomial for counts, and hybrid neural‑physical corrections to ODEs.</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Probabilistic programs (PyMC code), kernel algebraic expressions (compositional GP kernels via +, ×), closed‑form parametric functions (logistic growth, von Bertalanffy), ordinary differential equations (Python/JAX functions), and neural correction terms (MLPs inside ODEs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Held‑out splits of the above datasets; synthetic datasets drawn from expert models (simulated analogues) used to test generalization and to mitigate data‑leakage concerns; also benchmark time series datasets (same collections used by Automatic Statistician) and Stan PosteriorDB examples. ODE extrapolation regions were used to test extrapolative behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Expected log predictive density estimated via leave‑one‑out cross validation (ELPD LOO), mean absolute error (MAE) on held‑out test sets for time series and ODEs, squared error for ODE experiments, marginal likelihood for GP kernel hyperparameter optimization, MCMC diagnostics (Gelman‑Rubin R-hat and effective sample size).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>BoxLM programs matched or exceeded expert and strong baselines in many cases: e.g., for Stan experiments LM matched expert on 9/12 dataset conditions (example ELPD LOO: Eight schools: Expert -30.70 vs LM -30.42; Dugongs: Expert 22.52 vs LM 23.40; Peregrine: Expert -142.19 vs LM -173.11). For GP kernel time series BoxLM MAE examples (per dataset): Air BoxLM MAE 0.07 vs Automatic Statistician 0.19 (BoxLM+ 0.08); Beer BoxLM+ 0.07, BoxLM 0.22, AS 0.06; other datasets reported similarly in Figure 2. For ODE/Lotka–Volterra experiments, BoxLM variations outperformed standard Lotka–Volterra, Neural ODE, and a hybrid neural ODE baseline in test MAE (averaged across 3 runs); exact per‑dataset MAE numbers are reported in figures and tables of the paper. MCMC diagnostics for reported PyMC programs: R-hat ≤ 1.01 and mean bulk ESS ≥ 400 per chain.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against Automatic Statistician (greedy compositional GP search), spectral mixture GP (5 components), single periodic GP, N‑BEATS neural forecaster, human expert Stan programs, Neural ODE and Hybrid Neural ODE baselines. Examples: GP baselines (Periodic, SM) and N‑BEATS MAEs shown in the GP table; Stanfard baselines in ODE tasks (Neural ODE and Hybrid) reported in ODE figures; expert ELPD LOO values shown in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Probabilistic inference with PyMC (HMC/NUTS) and MCMC diagnostics (R-hat, ESS), marginal likelihood optimization (GP hyperparameter multiple restarts for periodic kernels), leave‑one‑out cross validation (ELPD LOO) and held‑out test MAE/squared error; simulated datasets (sampled from generative priors) used to check for dataset‑leakage; qualitative expert‑style evaluation (interpretability of proposed functional forms) and extrapolation checks for ODEs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note the approach operates on datasets and model code rather than extracting laws from collections of scholarly papers; experiments were limited to static, mostly one‑dimensional datasets so model criticism used simple preselected statistics (posterior predictive mean/variance, residuals). Context length and prompt size are concerns (visual‑only prompting used to mitigate long textual contexts). Failure rates for proposed PyMC programs: GPT‑4 textual dataframe 78% of programs could be successfully scored, GPT‑4 vision only 70%, GPT‑3.5 76% (Table 2). LMs can be strongly influenced by provided metadata (sometimes over‑prioritizing domain priors), and in some cases removal of metadata improved predictive performance. No fine‑tuning of the LMs was performed (authors suggest fine‑tuning as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7823.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7823.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP kernel discovery (BoxLM GP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BoxLM application: automated Gaussian process kernel discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A use case where the proposal LM searches a constrained DSL of GP base kernels and composition operators to discover compositional GP kernels (via addition and multiplication) that explain time series data; kernel hyperparameters are optimized via marginal likelihood and model selection uses ELPD/MAE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 V (proposal LM); Critic LM also GPT-4 V (deterministic)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Few‑shot / constrained prompting: the LM is instructed to perform syntactic kernel edits (replace subexpression with subexpression + base kernel or × base kernel, or replace a base kernel family). In-context exemplars (k=3) provided; chain‑of‑thought style dataset reflection requested. Proposal LM temperature: 0.2 (standard) or 0.7 in augmented runs; Critic LM temperature 0.0.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Univariate time series datasets (six canonical datasets used in Automatic Statistician experiments: Air, Beer, Heart, Milk, Wine, Wool). Input to the LM included dataset plots and descriptions; in augmented experiments additional base kernels were provided. No scholarly papers input.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted generation of kernel expressions (symbolic composition) in the prompt DSL; LM operations limited to a small set of syntactic edits; after proposal, kernel hyperparameters optimized by marginal likelihood (multiple initializations for periodic components).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Functional prior over functions — kernel structures capturing trend/periodicity/linear components (i.e., functional relationships governing covariance structure of latent functions).</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Algebraic kernel expressions composed from base kernels (Exponentiated Quadratic, Periodic, Linear, Polynomial; augmented with Matern32/52, Cosine, Rational Quadratic).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Same held‑out splits of the six time series datasets; performance measured on test MAE and plotted extrapolations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mean absolute error (MAE) on held‑out test data; marginal likelihood during hyperparameter search; comparisons averaged across three model runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table/Figure 2 reports per‑dataset MAE. Example numbers: Air — BoxLM MAE 0.07, BoxLM+ 0.08, Automatic Statistician 0.19, Spectral Mixture 0.06, N‑BEATS 0.22. Beer — BoxLM+ 0.07, BoxLM 0.22, AS 0.06, SM 0.05. Overall BoxLM matched Automatic Statistician performance across the six datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Automatic Statistician (greedy compositional GP search), spectral mixture kernel (5 components), periodic kernel, and N‑BEATS were reported in comparison; specific MAE values per dataset are in Figure 2.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>GP hyperparameter optimization with multiple random restarts (and multiple period initializations for periodic kernels); test MAE on held‑out data; average across three runs to account for stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Search confined by a user‑specified DSL in this experiment (authors note this constraint can be useful for interpretability but limits flexibility); marginal likelihood for periodic kernels is multi‑modal so careful initialization is needed; augmented kernel spaces increased expressivity but sometimes gave mixed benefits; LM proposal stochasticity and context limitations (number of exemplars) affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7823.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7823.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODE discovery/correction (BoxLM ODE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BoxLM application: discovering and correcting ordinary differential equation models (Lotka–Volterra and other ODEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BoxLM proposes ODE models and hybrid neural‑physical corrections (implemented in JAX/diffrax) to model dynamical systems; parameters fit by gradient descent (for ODEs) or MCMC (for probabilistic ODE formulations), and proposals can incorporate natural‑language constraints to favor interpretable forms such as Holling type II handling time modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Statistical Model Discovery with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 V (proposal and critic), GPT-3.5 ablations reported</td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>Few‑shot / in‑context prompting with warm‑start seed programs; natural‑language constraint injection (e.g., 'make model interpretable to ecologist') to bias proposals; chain‑of‑thought prompting and code comments requested.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Simulated ODE time series data from a perturbed Lotka–Volterra system (and other nonlinear oscillators in appendix), plus plots and optionally a warm‑start seed program (standard Lotka–Volterra or hybrid neural ODE). No scholarly papers were used.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted code generation of ODE right‑hand‑side functions in Python/JAX; proposals included parametric extensions (e.g., logistic prey growth, predation saturation terms) or MLP correction terms; parameters estimated via gradient descent using diffrax; two‑stage fitting used for hybrid models to avoid neural domination.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Mechanistic dynamical systems (differential equations) and hybrid neural‑mechanistic corrections — functional relationships describing time derivatives of state variables.</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Ordinary differential equations implemented as Python functions (db/dt, dc/dt), with algebraic nonlinear terms (e.g., beta*b*c/(1+psi*b)), fractional exponents, and MLP correction terms; also logistic and Holling type II functional forms and polynomial corrections for other oscillators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>Simulated perturbed Lotka–Volterra dataset (parameters α=0.9, β=1.1, δ=-1.2, γ=2.1, c raised to 0.95), held‑out extrapolation regions; additional oscillator datasets (Duffing, Van der Pol, Cubic Harmonic Damped) in appendix for polynomial correction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Test mean absolute error (MAE) averaged across runs, squared error vs rounds for iterative improvement, qualitative extrapolation accuracy on prediction horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>All BoxLM variations (No warm‑start, Warm‑start constrained, Warm‑start unconstrained) outperformed standard Lotka–Volterra, a Neural ODE baseline, and a Hybrid Neural ODE baseline in test MAE (figures show quantitative comparisons; averaged across three runs). Exact numeric MAE values are plotted in Figure 4 and Figure 10.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines included standard Lotka–Volterra fit by parameter optimization, Neural ODE (MLP parameterized RHS; hyperparameter sweep over widths/depths), and a Hybrid Neural ODE baseline introducing multiplicative or additive MLP corrections. Hybrid baseline used a two‑stage training to avoid MLP dominance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Fit quality assessed on held‑out/extrapolation regions; for hybrid models a two‑stage fitting procedure (fit closed‑form parameters then MLP) used to ensure fair comparison; training and evaluation repeated across multiple runs to report averages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ODE parameter learning used gradient descent rather than full Bayesian inference due to complexity; results limited to low‑dimensional dynamical systems; natural‑language constraints can bias LM proposals (both helpful and harmful depending on dataset); no experimental/real observational ODE datasets beyond simulated examples in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 2)</em></li>
                <li>Automated reverse engineering of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>Structure discovery in nonparametric regression through compositional kernel search <em>(Rating: 2)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 1)</em></li>
                <li>From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought <em>(Rating: 1)</em></li>
                <li>Automated model discovery for human brain using constitutive artificial neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7823",
    "paper_id": "paper-892dba6e3240433b30c8ac776c3b6dad61ebdfa9",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [
        {
            "name_short": "BoxLM",
            "name_full": "Language model driven automated model discovery (BoxLM)",
            "brief_description": "An iterative framework that uses large language models to propose probabilistic programs (statistical models) from dataset representations and then critiques fitted models to guide further proposals, implemented within Box's Loop. Proposals are written as PyMC probabilistic programs and evaluated via probabilistic inference and posterior predictive checks; a critic LM digests fit statistics and posterior predictive summaries to produce natural-language feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Automated Statistical Model Discovery with Language Models",
            "llm_name": "GPT-4 V (gpt4-11-06-preview), with GPT-3.5 Turbo used in ablations",
            "llm_type": "Few‑shot / in‑context prompting with chain‑of‑thought style instructions; a separate critic LM run deterministically (temperature 0.0); vision-capable prompt variant (multimodal). No fine‑tuning reported.",
            "input_corpus_description": "Not a collection of scholarly papers — the LM conditions on per‑task datasets (tabular data, plots), dataset metadata (natural language descriptions and feature descriptions), exemplar probabilistic programs, and natural‑language critic state. Datasets used include time series (Air, Beer, Heart, Milk, Wine, Wool), four Stan PosteriorDB real datasets (eight schools, dugongs, surgical, peregrine), simulated analogues of those datasets, and synthetic ODE datasets (perturbed Lotka–Volterra and other oscillators). No corpus of papers was ingested or used for automatic extraction.",
            "extraction_method": "The LLM is prompted to propose explicit probabilistic programs (PyMC or Python ODE code) that encode hypotheses/functional forms; these programs are fit with external tools (PyMC MCMC or gradient‑based fitting via JAX/diffrax), posterior predictive means/variances and fit scores (e.g., ELPD LOO or marginal likelihood) are computed, and a critic LM ingests program code plus these statistics to produce natural language feedback which is fed back for further in‑context exemplars (iterative proposal + critique loop). Chain‑of‑thought style reflection and code comments are requested in prompts. Visual-only prompting (plots) and textual dataframe prompting were both used.",
            "law_type": "Functional/parametric statistical relationships and mechanistic equations: e.g., Gaussian process kernel compositions (functional priors over functions), parametric growth laws (von Bertalanffy, logistic), ODE dynamical systems (Lotka–Volterra and extended forms), polynomial regressions, Beta‑Binomial for counts, and hybrid neural‑physical corrections to ODEs.",
            "law_representation": "Probabilistic programs (PyMC code), kernel algebraic expressions (compositional GP kernels via +, ×), closed‑form parametric functions (logistic growth, von Bertalanffy), ordinary differential equations (Python/JAX functions), and neural correction terms (MLPs inside ODEs).",
            "evaluation_dataset": "Held‑out splits of the above datasets; synthetic datasets drawn from expert models (simulated analogues) used to test generalization and to mitigate data‑leakage concerns; also benchmark time series datasets (same collections used by Automatic Statistician) and Stan PosteriorDB examples. ODE extrapolation regions were used to test extrapolative behavior.",
            "evaluation_metrics": "Expected log predictive density estimated via leave‑one‑out cross validation (ELPD LOO), mean absolute error (MAE) on held‑out test sets for time series and ODEs, squared error for ODE experiments, marginal likelihood for GP kernel hyperparameter optimization, MCMC diagnostics (Gelman‑Rubin R-hat and effective sample size).",
            "performance_results": "BoxLM programs matched or exceeded expert and strong baselines in many cases: e.g., for Stan experiments LM matched expert on 9/12 dataset conditions (example ELPD LOO: Eight schools: Expert -30.70 vs LM -30.42; Dugongs: Expert 22.52 vs LM 23.40; Peregrine: Expert -142.19 vs LM -173.11). For GP kernel time series BoxLM MAE examples (per dataset): Air BoxLM MAE 0.07 vs Automatic Statistician 0.19 (BoxLM+ 0.08); Beer BoxLM+ 0.07, BoxLM 0.22, AS 0.06; other datasets reported similarly in Figure 2. For ODE/Lotka–Volterra experiments, BoxLM variations outperformed standard Lotka–Volterra, Neural ODE, and a hybrid neural ODE baseline in test MAE (averaged across 3 runs); exact per‑dataset MAE numbers are reported in figures and tables of the paper. MCMC diagnostics for reported PyMC programs: R-hat ≤ 1.01 and mean bulk ESS ≥ 400 per chain.",
            "baseline_comparison": true,
            "baseline_performance": "Compared against Automatic Statistician (greedy compositional GP search), spectral mixture GP (5 components), single periodic GP, N‑BEATS neural forecaster, human expert Stan programs, Neural ODE and Hybrid Neural ODE baselines. Examples: GP baselines (Periodic, SM) and N‑BEATS MAEs shown in the GP table; Stanfard baselines in ODE tasks (Neural ODE and Hybrid) reported in ODE figures; expert ELPD LOO values shown in Table 1.",
            "validation_method": "Probabilistic inference with PyMC (HMC/NUTS) and MCMC diagnostics (R-hat, ESS), marginal likelihood optimization (GP hyperparameter multiple restarts for periodic kernels), leave‑one‑out cross validation (ELPD LOO) and held‑out test MAE/squared error; simulated datasets (sampled from generative priors) used to check for dataset‑leakage; qualitative expert‑style evaluation (interpretability of proposed functional forms) and extrapolation checks for ODEs.",
            "limitations": "Authors note the approach operates on datasets and model code rather than extracting laws from collections of scholarly papers; experiments were limited to static, mostly one‑dimensional datasets so model criticism used simple preselected statistics (posterior predictive mean/variance, residuals). Context length and prompt size are concerns (visual‑only prompting used to mitigate long textual contexts). Failure rates for proposed PyMC programs: GPT‑4 textual dataframe 78% of programs could be successfully scored, GPT‑4 vision only 70%, GPT‑3.5 76% (Table 2). LMs can be strongly influenced by provided metadata (sometimes over‑prioritizing domain priors), and in some cases removal of metadata improved predictive performance. No fine‑tuning of the LMs was performed (authors suggest fine‑tuning as future work).",
            "uuid": "e7823.0",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GP kernel discovery (BoxLM GP)",
            "name_full": "BoxLM application: automated Gaussian process kernel discovery",
            "brief_description": "A use case where the proposal LM searches a constrained DSL of GP base kernels and composition operators to discover compositional GP kernels (via addition and multiplication) that explain time series data; kernel hyperparameters are optimized via marginal likelihood and model selection uses ELPD/MAE.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Automated Statistical Model Discovery with Language Models",
            "llm_name": "GPT-4 V (proposal LM); Critic LM also GPT-4 V (deterministic)",
            "llm_type": "Few‑shot / constrained prompting: the LM is instructed to perform syntactic kernel edits (replace subexpression with subexpression + base kernel or × base kernel, or replace a base kernel family). In-context exemplars (k=3) provided; chain‑of‑thought style dataset reflection requested. Proposal LM temperature: 0.2 (standard) or 0.7 in augmented runs; Critic LM temperature 0.0.",
            "input_corpus_description": "Univariate time series datasets (six canonical datasets used in Automatic Statistician experiments: Air, Beer, Heart, Milk, Wine, Wool). Input to the LM included dataset plots and descriptions; in augmented experiments additional base kernels were provided. No scholarly papers input.",
            "extraction_method": "Prompted generation of kernel expressions (symbolic composition) in the prompt DSL; LM operations limited to a small set of syntactic edits; after proposal, kernel hyperparameters optimized by marginal likelihood (multiple initializations for periodic components).",
            "law_type": "Functional prior over functions — kernel structures capturing trend/periodicity/linear components (i.e., functional relationships governing covariance structure of latent functions).",
            "law_representation": "Algebraic kernel expressions composed from base kernels (Exponentiated Quadratic, Periodic, Linear, Polynomial; augmented with Matern32/52, Cosine, Rational Quadratic).",
            "evaluation_dataset": "Same held‑out splits of the six time series datasets; performance measured on test MAE and plotted extrapolations.",
            "evaluation_metrics": "Mean absolute error (MAE) on held‑out test data; marginal likelihood during hyperparameter search; comparisons averaged across three model runs.",
            "performance_results": "Table/Figure 2 reports per‑dataset MAE. Example numbers: Air — BoxLM MAE 0.07, BoxLM+ 0.08, Automatic Statistician 0.19, Spectral Mixture 0.06, N‑BEATS 0.22. Beer — BoxLM+ 0.07, BoxLM 0.22, AS 0.06, SM 0.05. Overall BoxLM matched Automatic Statistician performance across the six datasets.",
            "baseline_comparison": true,
            "baseline_performance": "Automatic Statistician (greedy compositional GP search), spectral mixture kernel (5 components), periodic kernel, and N‑BEATS were reported in comparison; specific MAE values per dataset are in Figure 2.",
            "validation_method": "GP hyperparameter optimization with multiple random restarts (and multiple period initializations for periodic kernels); test MAE on held‑out data; average across three runs to account for stochasticity.",
            "limitations": "Search confined by a user‑specified DSL in this experiment (authors note this constraint can be useful for interpretability but limits flexibility); marginal likelihood for periodic kernels is multi‑modal so careful initialization is needed; augmented kernel spaces increased expressivity but sometimes gave mixed benefits; LM proposal stochasticity and context limitations (number of exemplars) affect performance.",
            "uuid": "e7823.1",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ODE discovery/correction (BoxLM ODE)",
            "name_full": "BoxLM application: discovering and correcting ordinary differential equation models (Lotka–Volterra and other ODEs)",
            "brief_description": "BoxLM proposes ODE models and hybrid neural‑physical corrections (implemented in JAX/diffrax) to model dynamical systems; parameters fit by gradient descent (for ODEs) or MCMC (for probabilistic ODE formulations), and proposals can incorporate natural‑language constraints to favor interpretable forms such as Holling type II handling time modifications.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Automated Statistical Model Discovery with Language Models",
            "llm_name": "GPT-4 V (proposal and critic), GPT-3.5 ablations reported",
            "llm_type": "Few‑shot / in‑context prompting with warm‑start seed programs; natural‑language constraint injection (e.g., 'make model interpretable to ecologist') to bias proposals; chain‑of‑thought prompting and code comments requested.",
            "input_corpus_description": "Simulated ODE time series data from a perturbed Lotka–Volterra system (and other nonlinear oscillators in appendix), plus plots and optionally a warm‑start seed program (standard Lotka–Volterra or hybrid neural ODE). No scholarly papers were used.",
            "extraction_method": "Prompted code generation of ODE right‑hand‑side functions in Python/JAX; proposals included parametric extensions (e.g., logistic prey growth, predation saturation terms) or MLP correction terms; parameters estimated via gradient descent using diffrax; two‑stage fitting used for hybrid models to avoid neural domination.",
            "law_type": "Mechanistic dynamical systems (differential equations) and hybrid neural‑mechanistic corrections — functional relationships describing time derivatives of state variables.",
            "law_representation": "Ordinary differential equations implemented as Python functions (db/dt, dc/dt), with algebraic nonlinear terms (e.g., beta*b*c/(1+psi*b)), fractional exponents, and MLP correction terms; also logistic and Holling type II functional forms and polynomial corrections for other oscillators.",
            "evaluation_dataset": "Simulated perturbed Lotka–Volterra dataset (parameters α=0.9, β=1.1, δ=-1.2, γ=2.1, c raised to 0.95), held‑out extrapolation regions; additional oscillator datasets (Duffing, Van der Pol, Cubic Harmonic Damped) in appendix for polynomial correction experiments.",
            "evaluation_metrics": "Test mean absolute error (MAE) averaged across runs, squared error vs rounds for iterative improvement, qualitative extrapolation accuracy on prediction horizons.",
            "performance_results": "All BoxLM variations (No warm‑start, Warm‑start constrained, Warm‑start unconstrained) outperformed standard Lotka–Volterra, a Neural ODE baseline, and a Hybrid Neural ODE baseline in test MAE (figures show quantitative comparisons; averaged across three runs). Exact numeric MAE values are plotted in Figure 4 and Figure 10.",
            "baseline_comparison": true,
            "baseline_performance": "Baselines included standard Lotka–Volterra fit by parameter optimization, Neural ODE (MLP parameterized RHS; hyperparameter sweep over widths/depths), and a Hybrid Neural ODE baseline introducing multiplicative or additive MLP corrections. Hybrid baseline used a two‑stage training to avoid MLP dominance.",
            "validation_method": "Fit quality assessed on held‑out/extrapolation regions; for hybrid models a two‑stage fitting procedure (fit closed‑form parameters then MLP) used to ensure fair comparison; training and evaluation repeated across multiple runs to report averages.",
            "limitations": "ODE parameter learning used gradient descent rather than full Bayesian inference due to complexity; results limited to low‑dimensional dynamical systems; natural‑language constraints can bias LM proposals (both helpful and harmful depending on dataset); no experimental/real observational ODE datasets beyond simulated examples in this work.",
            "uuid": "e7823.2",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 2
        },
        {
            "paper_title": "Automated reverse engineering of nonlinear dynamical systems",
            "rating": 2
        },
        {
            "paper_title": "Structure discovery in nonparametric regression through compositional kernel search",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 1
        },
        {
            "paper_title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
            "rating": 1
        },
        {
            "paper_title": "Automated model discovery for human brain using constitutive artificial neural networks",
            "rating": 1
        }
    ],
    "cost": 0.015529999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Statistical Model Discovery with Language Models</h1>
<p>Michael Y. Li ${ }^{1}$ Emily B. Fox ${ }^{123}$ Noah D. Goodman ${ }^{14}$</p>
<h4>Abstract</h4>
<p>Statistical model discovery is a challenging search over a vast space of models subject to domainspecific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.</p>
<h2>1. Introduction</h2>
<p>Modeling, or generating a parsimonious but explanatory representation of a complex system, is at the heart of scientific discovery. Model discovery is challenging because it involves searching over a vast space of candidate models subject to domain-specific constraints (e.g., find the best</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>model that remains interpretable to domain experts). Efficiently searching over the space requires extensive human expertise: modelers need broad knowledge of different modeling approaches and must work closely with domain experts to adapt these approaches to a given problem domain. As a concrete example, consider modeling blood-glucose dynamics in Type 1 diabetes (T1D) patients; accurately modeling these dynamics can enable better insulin regulation and reduce complications from the disease. To model these dynamics, modelers need to understand biomedical models that capture blood-glucose dynamics in idealized, lab settings but they also need to understand techniques for adapting these models to handle real data with noise and missingness (Miller et al., 2020). Domain experts play a crucial role in this process: modelers must work closely with clinicians to ensure that the model is consistent with human physiology. As this example illustrates, model discovery can require significant human expertise. Automating this process could accelerate and democratize scientific discovery.</p>
<p>Automated model discovery is not a new ambition. Previous systems have been successfully deployed for discovering physical laws (Bongard \&amp; Lipson, 2007; McKinney et al., 2006; Linka et al., 2023), reverse-engineering non-linear dynamical systems (Schmidt \&amp; Lipson, 2009), nonparametric regression (Duvenaud et al., 2013) and unsupervised learning (Grosse, 2014). However, in these systems, a human expert had to carefully design a domain specific language (DSL) of models and specify a hand-crafted search procedure for composing models in that DSL. For example, in the Automatic Statistician (Duvenaud et al., 2013; Lloyd et al., 2014), a system for nonparametric regression and time series modeling, human experts defined a DSL of Gaussian process kernels and a search procedure that composes kernels via addition and multiplication. Defining the DSL and the operators for composing models requires significant modeling expertise. These systems also compromised flexibility for automation: rather than choosing a model class bestsuited for a problem, experts chose models that compose conveniently. This breaks the core principle of separation of modeling from inference (van de Meent et al., 2021).</p>
<p>As we saw above, there are two key roles in the model discovery pipeline: the domain expert and the modeler. We hypothesize that LMs can supplement these roles and re-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Language model driven automated model discovery (BoxLM). 1) The prompt for the LM contains the dataset in visual and/or textual form, dataset metadata (e.g., dataset description), the code for previous probabilistic programs, and natural language feedback. 2) Given this, the proposal LM proposes new models expressed as probabilistic programs. 3) To fit these programs in a generic way, we leverage probabilistic programming languages and obtain scores and posterior predictive samples. 4) After we fit models, we compute the posterior predictive mean and variance. We provide these statistics to a critic LM which produces natural language feedback to guide the next round of model building. 5) We propagate the best programs, their posterior predictive means and variances, and natural language feedback forward by updating the prompt.
duce the human expertise required in model discovery. Our hypothesis is motivated by recent work exploring LM capabilities. First, LMs have been successfully applied to domains including law (Bommarito \&amp; Katz, 2022), medicine (Lee et al., 2023), and mathematics (Wu et al., 2023). This suggests that LMs have broad domain knowledge which may enable them to supplement the domain expert. Second, LMs can reliably write code (Rozière et al., 2023; Chen et al., 2021) which means we do not require a human expert to define a DSL. Instead, we can search over a more openended space of models, provided they can be expressed in a generic programming language like Python. Third, LMs have strong inductive reasoning capabilities (e.g., they can generate hypotheses from limited data) (Wang et al., 2024; Qiu et al., 2024). We hypothesize these capabilities may enable them to reason about data (Zhong et al., 2023). These capabilities, in addition to their knowledge of various modeling approaches, may enable the LM to supplement the modeler.</p>
<p>Leveraging LMs for automated model discovery is enticing at a conceptual level. However, in order to deliver on this promising idea, we need to make several important design choices. These design choices should exploit the strengths of LMs but remain grounded in principled statistical modeling. First, we need a generic and flexible representation of a statistical model that can be expressed programmatically and is amenable to automated inference (e.g., model fit-
ting). Second, we need a method for guiding LM proposals through natural language.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Automated Model Discovery with LMs
input dataset \(\mathcal{D}\), number of rounds \(T, k\) number of ex-
    emplars, \(m\) number of proposals per round, (optional)
    warm-start example \(z_{0}\), function for scoring a program
    score, (optional) function for producing natural lan-
    guage feedback criticize
    \(\mathcal{Z} \leftarrow \emptyset\)
    while \(t&lt;T\) do
        \(\left\{z_{i}^{t}\right\}_{i=1}^{m} \sim q_{\mathrm{LM}}\left(\cdot \mid \mathcal{Z}, z_{0}, h^{t}, \mathcal{D}\right)\)
        \(\left\{s_{i}\right\}_{i=1}^{m} \leftarrow\) score-all \(\left(\right.\) score, \(\left.\left\{z_{i}^{t}\right\}_{i=1}^{m}, \mathcal{D}\right)\)
        \(\mathcal{Z} \leftarrow\) select-exemplars \(\left(k,\left\{z_{i}^{t}\right\}_{i=1}^{m},\left\{s_{i}\right\}_{i=1}^{m}\right)\)
        \(h^{t+1} \leftarrow \operatorname{criticize}\left(\left\{z_{i}^{t}\right\}_{i=1}^{m},\left\{s_{i}\right\}_{i=1}^{m}, h^{t}\right)\)
    end while
</code></pre></div>

<p>To fulfill these requirements, we draw on research in probabilistic programming and inductive reasoning with LMs. In particular, we introduce the following method: LMs propose statistical models expressed as probabilistic programs, given a dataset and some metadata (e.g., dataset description). We then fit these models using generic probabilistic inference techniques, compute statistics assessing the model fit, summarize findings from these statistics in natural language, and select exemplar models to guide the next round of proposals (Figure 1). Our approach is connected to recent</p>
<p>work on hypothesis search and inductive reasoning with LMs, but targets a fundamentally different problem (Wang et al., 2024; Qiu et al., 2024). While we also leverage the inductive reasoning capabilities of LMs, our focus is on statistical modeling of noisy real-world data, while previous work focused on learning deterministic input-output rules that can be implemented with standard Python programs.</p>
<p>We evaluate our method in three settings that cover common use cases in probabilistic modeling: searching within a DSL, searching over an open-ended space of probabilistic models, and improving expert models subject to modeling constraints expressed in natural language. In the first use case, we illustrate that our LM system is effective even in a DSL and matches the performance of the Automatic Statistician (Duvenaud et al., 2013). We then consider the more general setting of automatically constructing probabilistic models for real world data; crucially, we do not require a user to define a DSL and this generality is enabled by our choice to use probabilistic programs. Our method identifies probabilistic programs that match the performance of human expert written programs. In the third setting, we use LMs to improve classic models and illustrate a compelling advantage of using LMs for model discovery: given that certain modeling constraints can be difficult to express formally but easy to express in natural language (e.g., this model should be more physical), we use natural language to guide LMs towards models that balance interpretability and flexibility.</p>
<h2>2. Automated Box's Loop with Language Models</h2>
<p>We begin with a brief background on the probabilistic modeling paradigm. We then formally introduce our problem setting and describe our approach. For an overview, see Figure 1 and Algorithm 1.</p>
<h3>2.1. Background</h3>
<p>In probabilistic modeling, our goal is to describe a dataset in terms of unobserved, latent structure. We describe a dataset through a probabilistic model which is a joint distribution $p(x, z \mid \eta)$; here $x=x_{1: N}$ denotes $N$ observed data points, $z=z_{1: M}$ denotes $M$ latent variables, and $\eta$ corresponds to non-random quantities in the model (Blei, 2014). After specifying a probabilistic model, we fit the model to observed data. Fitting a model involves inference, or conditioning on observed data and computing the posterior distribution $p(z \mid x)$. After fitting a model, we perform model criticism. In the model criticism step, we evaluate the model by interrogating the posterior. In this work, our model criticism is inspired by a common model criticism technique known as a posterior predictive check: to identify discrepancies, samples are drawn from the posterior predictive distribution
and statistics of these samples are compared against those of the observed data (Gelman et al., 2013). Model building and model criticism typically take place over multiple iterations in an iterative process known as Box's Loop (Box \&amp; Hunter, 1962).</p>
<p>There are many different representations of a probabilistic model. Since our goal is automated model discovery, we use probabilistic programs. Probabilistic programming languages provide ways to flexibly represent probabilistic models as programs and support generic inference methods for any arbitrary program (Wood et al., 2014; Goodman et al., 2008; van de Meent et al., 2021). In the context of automated model discovery, these are highly desirable properties, since LMs can write code reliably and other representations of probabilistic models can require custom inference methods. Our approach of using LMs to generate probabilistic programs is related to the approach taken by Wong et al. (2023) but is motivated by a different problem. Our emphasis is on using LMs as a tool for developing statistical models of real-world datasets, while their focus was on integrating symbolic methods with LMs.</p>
<h3>2.2. Problem formulation</h3>
<p>Our framework is motivated by recent work in inductive reasoning with LMs (Qiu et al., 2024; Wang et al., 2024), integrating tools with LMs (Gao et al., 2023), and driving LMs via linguistic feedback (Shinn et al., 2023).</p>
<p>At a high level, we consider a method for learning probabilistic models from data that involves two steps: a model building step and a criticism step ${ }^{1}$. Crucially, by learning a model, we mean searching over a space of model structures and not just learning the parameters of some fixed model class. In each step, we leverage LMs. In the proposal step, a proposal $L M$ proposes probabilistic programs for a dataset. We then fit these probabilistic programs and evaluate them. In the criticism step, we provide a critic $L M$ with programs and statistics assessing model fit (e.g., model criticism statistics) and ask the critic LM to provide feedback to guide the next round of proposals.</p>
<p>We start with a dataset $\mathcal{D}=\left{\mathbf{x}<em i="i">{\mathbf{i}}, y</em>\right}<em _mathbf_i="\mathbf{i">{i=1}^{n}$. Here $\mathbf{x}</em>$ (e.g., length of animal). Context informs how human modelers leverage prior knowledge; for example, if a modeler knows their dataset consists of monthly carbon dioxide measurements over a fifty-year}} \in \mathbb{R}^{d}$ are fixed $d$-dimensional input values (e.g., features) and $y_{i} \in \mathbb{R}$ are the observations. Let $\Sigma$ be the vocabulary of the LM. For each dataset, we have an associated metadata set $\mathcal{C} \in \Sigma^{d+1}$, which consists of natural language descriptions of $\mathcal{D}$ (e.g., animal ages vs length) and natural language descriptions of each feature in $\mathcal{D</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>time span, they will choose a model that can capture periodicity and a linear trend. Our goal is to find a probabilistic program $z \sim \Sigma^{*}$ that maximizes some notion of quality, which we take here to be either the log marginal likelihood or expected log predictive density (ELPD) estimated via cross validation (LOO) (Vehtari et al., 2017).</p>
<h3>2.3. Approach</h3>
<p>Model Building Step In the model building step, we automatically generate probabilistic programs for modeling a dataset given information about the dataset and previously proposed programs. In particular, to generate candidates for round $t$, we sample $m$ probabilistic programs $z_{i}^{t}$ from the proposal LM, $q_{\mathrm{LM}}(\cdot)$. In our experiments, we use GPT-4 V (Achiam et al., 2023) (gpt4-11-06-preview), which has multimodal capabilities. We leverage in-context learning, or LM's ability to learn from examples in a prompt, to guide the LM's proposals based on high-scoring programs in the past (Brown et al., 2020). Specifically, $q_{\mathrm{LM}}$ "conditions" on $h^{t} \in \Sigma^{*}$, a natural language instruction synthesizing previous modeling approaches and suggesting new approaches, $k$ exemplars $\left{z_{1}, \ldots z_{k}\right}$, and a visual or textual representation of $\mathcal{D}$. Optionally, $q_{\mathrm{LM}}$ also conditions on a warm-start expert program $z_{0}$ :</p>
<p>$$
z_{i}^{t} \sim q_{\mathrm{LM}}\left(\cdot \mid z_{0}, z_{1}, \ldots, z_{k}, h^{t-1}, \mathcal{D}\right)
$$</p>
<p>We run this at a temperature of 0.7 . Chain-of-thought reasoning, or generating intermediate reasoning steps, improves the performance of LMs (Wei et al., 2022; Kojima et al., 2022). Motivated by this, we instruct $q_{\mathrm{LM}}$ to reflect on the properties of the dataset or plot of the data, sketch a highlevel modeling approach, state the hypotheses that it will address before writing a program, and add comments to code that address specific hypotheses.</p>
<p>To create exemplars $z_{1}, \ldots, z_{k}$ for round $t$ for $q_{\mathrm{LM}}$, we choose the best $k$ programs among the $m$ proposed programs in round $t-1$.</p>
<p>Model Fitting Step In the model fitting step, we fit a probabilistic program to data. This requires us to perform (approximate) inference for generic probabilistic models. To accomplish this, we leverage pymc (Abril-Pla et al., 2023), a Python probabilistic programming library. pymc automatically assigns a Markov Chain Monte Carlo (MCMC) sampler to perform inference; by default pymc uses a Hamiltonian Monte Carlo sampler (Homan \&amp; Gelman, 2014). Crucially, by using a probabilistic program, we decouple modeling from inference: the proposal LM's role is to build a good model, while pymc takes care of inference. Our approach of offloading computations to an external tool is connected to recent work enhancing LMs by giving them tools (e.g., Python interpreter) (Gao et al., 2023; Schick et al., 2023).</p>
<p>Model Criticism Step In the criticism step, we ask the critic LM, $p_{L M}$, to produce natural language criticism of fitted models; we use this criticism to drive model revision. First, we can obtain a scalar score measuring the model fit (e.g., ELPD LOO) for each proposed model. Second, to enable the critic LM to do something akin to a posterior predictive check, we obtain samples from the posterior predictive distribution (e.g., $p\left(x^{\prime} \mid z, x\right)=\int_{z} p\left(x^{\prime} \mid z\right) p(z \mid x)$ ) and then compute summary statistics of these posterior predictive samples (Gelman et al., 2013). For simplicity, we compute the posterior predictive means and variances for each fitted probabilistic program. We then provide $p_{L M}$ with select probabilistic programs, their scores $\mathcal{S}$ (e.g., ELPD LOO), the posterior predictive means and variances $\mathcal{P}$, and the dataset itself $\mathcal{D}$; we explore both visual and textual dataframe based representations of the posterior predictive and the dataset. Finally, we ask $p_{L M}$ to distill this criticism in natural language (e.g., von Bertalanffy growth function with informative priors) which we use to drive the next round of model building (Shinn et al., 2023).</p>
<p>The key design choice is which programs to provide to $p_{L M}$ (e.g., critic exemplars). Naively, we could provide all proposed programs across all rounds to $p_{L M}$. However, this list grows each round and has redundancy. Furthermore, LMs struggle to reason over long contexts. We therefore explore a simple approach to selecting exemplars where we provide $p_{L M}$ with the top $d$ programs $\tilde{z}<em d="d">{1}, \ldots \tilde{z}</em>$, we sample:}$ from the current round $t$. To produce $h^{t+1</p>
<p>$$
h^{t+1} \sim p_{\mathrm{LM}}\left(\cdot \mid \tilde{z}<em d="d">{1}, \ldots, \tilde{z}</em>\right)
$$}, \mathcal{D}, \mathcal{S}, \mathcal{P</p>
<p>In the appendix, we report additional results for an approach inspired by a state-space update (Baum \&amp; Petrie, 1966; Kalman, 1960; Rabiner, 1989). To avoid storing the entire history of proposed programs, we interpret $h^{t+1}$ as a latent state and compute it using the previous state $h^{t}$ and the new fitted programs $\left{z_{i}\right}_{i=1}^{m}$ at round $t$ :</p>
<p>$$
h^{t+1} \sim p_{\mathrm{LM}}\left(\cdot \mid z_{1}, \ldots, z_{m}, h^{t}, \mathcal{D}, \mathcal{S}, \mathcal{P}\right)
$$</p>
<p>In practice, we implement this by asking $p_{\mathrm{LM}}$ to add and delete hypotheses. We find that these two approaches have similar performances. We run this criticism step at a temperature of 0.0 to limit stochasticity in the criticism produced.</p>
<p>We refer to our full LM-driven automated Box's loop as BoxLM.</p>
<h2>3. Experiments</h2>
<h3>3.1. Searching over a DSL: automated Gaussian process kernel discovery</h3>
<p>We first evaluate LM's ability to search over a constrained space of models; in some settings, a domain expert may require the modeler to search over a DSL for reasons such as</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>BoxLM+</th>
<th>BoxLM</th>
<th>Periodic</th>
<th>AS</th>
<th>SM</th>
<th>N-BEATS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Air</td>
<td>0.08</td>
<td>0.07</td>
<td>0.15</td>
<td>0.19</td>
<td>$\mathbf{0 . 0 6}$</td>
<td>0.22</td>
</tr>
<tr>
<td>Beer</td>
<td>0.07</td>
<td>0.22</td>
<td>0.15</td>
<td>0.06</td>
<td>$\mathbf{0 . 0 5}$</td>
<td>0.02</td>
</tr>
<tr>
<td>Heart</td>
<td>0.21</td>
<td>0.21</td>
<td>$\mathbf{0 . 2 0}$</td>
<td>0.21</td>
<td>0.21</td>
<td>0.07</td>
</tr>
<tr>
<td>Milk</td>
<td>0.12</td>
<td>0.10</td>
<td>0.10</td>
<td>0.11</td>
<td>$\mathbf{0 . 0 9}$</td>
<td>0.04</td>
</tr>
<tr>
<td>Wine</td>
<td>0.14</td>
<td>0.16</td>
<td>0.21</td>
<td>$\mathbf{0 . 1 3}$</td>
<td>0.18</td>
<td>0.17</td>
</tr>
<tr>
<td>Wool</td>
<td>0.20</td>
<td>0.19</td>
<td>0.19</td>
<td>0.23</td>
<td>$\mathbf{0 . 1 3}$</td>
<td>0.18</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Test set performance on time series datasets. Our BoxLM system identifies compositional kernels with performance on par with strong baselines. (left) Comparison of BoxLM test mean absolute error (MAE) against Automatic Statistician using greedy search (AS), spectral mixture kernel (SM), periodic kernel (Periodic), and N-BEATS. BoxLM+ searches over an augmented kernel space. We bold the best and underline the second best among the GP methods, treating N-BEATS as a powerful non-GP-constrained baseline. (right) Extrapolations from GP with a BoxLM-discovered kernel.
interpretability. In particular, we consider time-series modeling with Gaussian processes (GPs) as in the Automatic Statistician (Duvenaud et al., 2013). GPs are probabilistic models that specify a distribution over functions. GPs are defined by a mean function and a positive-definite kernel function $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ that gives the covariance between $f(\mathbf{x})$ and $f\left(\mathbf{x}^{\prime}\right)$ as a function of $\mathbf{x}$ and $\mathbf{x}^{\prime}$. The properties of a GP depend significantly on the kernel and therefore choosing a kernel that reflects domain knowledge (e.g., linearity, periodicity) is a key design choice. One common way to produce a more flexible kernel is to compose kernels via addition and multiplication, leveraging the closure properties of kernels. Here, we evaluate LMs' ability to search over a space of kernels to identify an appropriate composition of kernels.</p>
<p>Setup Mirroring Duvenaud et al. (2013), we define a space of base kernels. In the prompt, we ask $q_{L M}$ to perform one of the three operations (addition, multiplication, and replacement) on one of the in-context exemplar programs. For the first round, we ask $q_{L M}$ to produce an initial guess based on the structure of the dataset, which we provide as a plot. Given a kernel expression, we learn the kernel hyperparameters via gradient-based optimization of the marginal likelihood.</p>
<p>Results We evaluate our method on six common univariate time series datasets. We compare against the Automatic Statistician, a greedy algorithm proposed by Duvenaud et al. (2013) (run until a depth of 10), and two GP baselines: a periodic kernel and a spectral mixture kernel, a strong non-compositional baseline. We also compare against N-BEATS, a strong neural baseline (Wilson \&amp; Adams, 2013; Oreshkin et al., 2019). In Figure 2, we compare the mean absolute error (MAE) on held-out test data for all datasets. To account for stochasticity in the Automatic Statistician implementation we used (Saad et al., 2023) and
stochasticity in different repetitions of our pipeline, we average the test MAE across three model runs. Our method, denoted LM in the table, matches the performance of the Automatic Statistician, showing that BoxLM can efficiently search over a constrained space of models. We also experiment with augmenting the base kernel space with additional kernels (denoted BoxLM+ in the table). In some cases, this additional flexibility is beneficial. For example, by using the additional kernels, BoxLM+ can much better capture the Australian beer sales dataset than LM; in other cases, the additional flexibility does not appreciably improve performance. On the right panel, we show the extrapolations and identified kernel for the monthly air passenger dataset; BoxLM+ identifies a kernel with a periodic times linear component to capture the increasing amplitude in the data.</p>
<h3>3.2. Open-ended probabilistic model discovery for real-world datasets</h3>
<p>By integrating LMs into the model discovery process, we can search over a much broader class of models. Here, we explore BoxLMs' ability to automatically construct pymc probabilistic programs (Abril-Pla et al., 2023) for datasets.
Dataset We consider four real world datasets from the Stan PosteriorDB dataset (Magnusson et al., 2023): (1) a dataset consisting of average improvements in SAT scores after an SAT improvement program across eight different high schools, (2) a dataset consisting of ages of twenty-seven dugongs and their lengths, (3) a surgical dataset consisting of mortality rates in twelve hospitals performing cardiac surgery on babies, and (4) a dataset of peregrine population counts in the French Jura from 1964 to 2003. Each dataset has an associated human expert written probabilistic program in Stan, which we translate into pymc. These expert programs are generally open-source contributions from the Stan developer community. The datasets cover</p>
<p>common modeling motifs, such as hierarchical modeling and regression.</p>
<p>Ablations To study how domain knowledge affects the LM's modeling capabilities and to mitigate concerns about dataset leakage, we consider two ablations. In the first ablation, we construct a simulated analog for each dataset: for each dataset, we generate synthetic observations by sampling from the prior of a generative model or by sampling from a model with fixed parameters. We denote these datasets as simulated datasets. For example, to generate a simulated dataset for the eight schools dataset, we sample from the prior of the human expert program. In some cases, such as the dugongs dataset, the prior distributions over parameters are highly unconstrained and we therefore fix values for the parameters instead. For example, to generate a simulated analog of the dugongs dataset, we fix the values of the parameters $\alpha, \beta, \gamma$ used in the expert model $y_{i}=\alpha-\beta \gamma^{\epsilon_{i}}+\epsilon_{i}$ where $\epsilon_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right)$. Since it is unlikely that these simulated datasets appeared in the training data, this ablation helps mitigate concerns about dataset leakage.</p>
<p>In the second ablation, we remove the dataset metadata from the LM prompt. In particular, we remove the dataset description, replace the column names with domain-agnostic column names (e.g., $x_{0}, y$ ), and replace the axis labels with uninformative labels. We denote these datasets as no metadata datasets. By removing this metadata, we can characterize how LMs use domain knowledge.</p>
<p>Quantitative Results In Table 1, we compare the performance of our method across different datasets and ablations relative to the expert programs. We emphasize that the expert programs are strong baselines, especially for the simulated datasets where the expert programs generated the data. We highlight numbers corresponding to significant differences, where significance is defined as an ELPD LOO difference of greater than four times the standard error estimate.</p>
<p>BoxLM reliably identifies programs on par with expert programs; we validate convergence using standard Markov Chain Monte Carlo diagnostics. Interestingly, removing metadata or switching to simulated datasets does not generally reduce performance relative to the expert program. The main exception is the surgical dataset. By replacing the labels with $x_{0}$ and $y$, BoxLM formulates this problem as a regression problem instead of using a hierarchical model like the expert model. For the peregrine dataset, removing metadata improves performance. We discuss this further in the next section.</p>
<p>Qualitative Analysis: Language Models are Domain Experts Earlier, we asked whether LMs can reliably play the role of a domain expert. In our ablations, we study how
metadata influences the model search process by examining how removing metadata changes the programs proposed by BoxLM. In Figure 3, we plot the model fit and list the corresponding BoxLM programs in the figure caption. For the peregrine dataset, when given all the metadata, BoxLM models this dataset using a logistic growth model, which is motivated by the problem domain. However, logistic growth models cannot capture the initial decline in population. Interestingly, when we remove all metadata, BoxLM performs better, because it uses a quartic polynomial to model the data. This closes the gap in performance with the expert program. This highlights how prior knowledge can have a (sometimes overly) strong influence on the modeling choices of BoxLM. We observe a similar trend with the dugongs growth curve dataset. When given metadata, BoxLM uses a von Bertalanffy growth function (von Bertalanffy, 1949), which is commonly used to model animal growth. When we remove all metadata, BoxLM uses a polynomial function with a logarithmically transformed value for the inputs. Here, both modeling approaches fit the data well. Interestingly, BoxLM sets the prior parameters in these models in a data-informed way. For example, BoxLM sets the asymptotic length in the von Bertanlanffy function based on the observed lengths in the dataset. See code snippet in Figure 7 of the Appendix. For the eight schools dataset, BoxLM identifies a hierarchical model even without metadata, illustrating that model criticism can compensate for lack of domain knowledge. We illustrate the improvement round to round in Figure 6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Expert</th>
<th style="text-align: left;">LM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eight schools</td>
<td style="text-align: left;">$\underline{-30.70}$</td>
<td style="text-align: left;">$\underline{-30.42}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-18.31}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim no metadata</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-16.36}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs</td>
<td style="text-align: left;">$\underline{22.52}$</td>
<td style="text-align: left;">$\underline{23.40}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim</td>
<td style="text-align: left;">$\underline{50.04}$</td>
<td style="text-align: left;">$\underline{57.40}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim no metadata</td>
<td style="text-align: left;">$\underline{50.04}$</td>
<td style="text-align: left;">$\underline{55.24}$</td>
</tr>
<tr>
<td style="text-align: left;">Surgical</td>
<td style="text-align: left;">$\underline{-40.29}$</td>
<td style="text-align: left;">-38.03</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim</td>
<td style="text-align: left;">$\underline{-39.80}$</td>
<td style="text-align: left;">$\underline{-38.38}$</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim no metadata</td>
<td style="text-align: left;">$\mathbf{- 3 9 . 8 0}$</td>
<td style="text-align: left;">-63.72</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine</td>
<td style="text-align: left;">$\mathbf{- 1 4 2 . 1 9}$</td>
<td style="text-align: left;">-173.11</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim</td>
<td style="text-align: left;">$\mathbf{- 1 3 0 . 4 8}$</td>
<td style="text-align: left;">-179.06</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim no meta</td>
<td style="text-align: left;">$\underline{-130.48}$</td>
<td style="text-align: left;">$\underline{-136.39}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Comparison of BoxLM programs against expert programs We perform this comparison across four different datasets and two different ablations that replace observations with synthetic observations and remove all metadata. We report the expected predictive log density estimated via leave-one-out cross validation. We bold statistically significant differences and underline non-significant differences. LM programs match the performance of human expert programs on 9/12 datasets.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Domain knowledge shapes BoxLM modeling approaches. In the top row, we keep all metadata (e.g., dataset description). In the bottom row, we remove metadata that reveals information about the domain. This leads to qualitatively different approaches for three datasets; for eight schools, BoxLM discovers a hierarchical model even without metadata. We list the corresponding programs for these different ablations: (top row) $\frac{R}{\left(1+\left(\left(K-P_{0}\right) / P_{0}\right) \exp (-r t)\right)}$, $\operatorname{BetaBin}(n, \alpha, \beta), L_{\text {inf }}\left(1-\exp \left(-k\left(\text { age }-t_{0}\right)\right)\right)$; (bottom row) $a+b x_{0}+c x_{0}^{2}+d x_{0}^{3}+e x_{0}^{4}, \alpha+\beta x_{0}+\gamma x_{0}^{2}, \alpha+\beta_{1} \log x_{0}+\beta_{2} \log x_{0}^{2}+\beta_{3} \log x_{0}^{2}$.</p>
<h3>3.3. Improving classic models under modeling constraints</h3>
<p>In the previous experiment, we explored BoxLM's ability to identify models tabula rasa (e.g., without any initial seed model). However, in many scientific settings, we begin with a well-known model and are tasked with improving it. Here, we explore BoxLM's ability to improve upon a Lotka-Volterra model of predator-prey dynamics. In addition, a crucial component of model discovery is respecting "soft" modeling constraints that are easy to express in natural language but hard to formalize (e.g., ecologists should think this is a plausible model). We therefore illustrate another advantage of BoxLM: we can express these modeling constraints in natural language and use them to drive LM proposals.</p>
<p>Dataset To create our dataset, we simulate data from the following "perturbed" Lotka-Volterra dynamics</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha b-\beta b c \
&amp; \frac{d c}{d t}=-\gamma c+\delta b c^{0.95}
\end{aligned}
$$</p>
<p>Setup BoxLM is tasked with implementing an ODE model in Python using Jax (Bradbury et al., 2018). Estimating the parameters of Lotka-Volterra models via Bayesian inference is challenging. We instead learn the parameters via gradient descent which can be straightforwardly implemented using modern automatic differentiation libraries (Chen et al., 2018; Kidger, 2021); in particular, we use diffrax (Kidger, 2021), a Jax-based ODE library that supports learning ODE parameters via backpropagation. We consider three variations: warm-start with constraints (WS-Constraint), warm-start with no constraints (WS-No Constraint), and no warm-start (No-WS) that differ in their initial seed program and the initial instructions which express modeling
constraints. In all variations, we provide the LM with a scatter plot of training datapoints. In the no warm-start variation, we provide the LM with an implementation of standard Lotka-Volterra dynamics using diffrax and the predictions obtained from fitting standard Lotka-Volterra to the training data. In both warm-start variations, we provide the LM with (1) an implementation of a hybrid neural ODE that introduces an additive correction term to the predator dynamics, parameterized by a multilayer perceptron (MLP), and (2) the predictions obtained from fitting this model to the training data. In the constrained warm-start variation, we ask the LM to produce a hybrid neural ODE model that is interpretable to an ecology expert who suggested a Holling's type II response (Rosenzweig \&amp; MacArthur, 1963). In the unconstrained warm-start variation, we provide the same seed program but do not impose this additional interpretability constraint. For models with both neural and physical components, we employ a two-stage learning procedure so that the neural component does not dominate the dynamics; see Appendix C for details.</p>
<p>Results In Figure 4 (left), we plot the predictions obtained from integrating the learned dynamics for an ODE proposed by BoxLM, the training data points generated from the true dynamics, and the predictions from the standard Lotka-Volterra model. We fit free parameters to the training data via gradient descent. The grey region indicates the extrapolation region.</p>
<p>BoxLM can significantly improve upon the standard LotkaVolterra model by introducing corrections to the dynamics (Figure 4). The standard Lotka-Volterra model cannot capture the decreasing amplitude in the data; furthermore, there is a slight phase shift relative to the training datapoints. In contrast, BoxLM identifies an ODE that captures these properties and extrapolates accurately. In Figure 4 (right), we compare these programs against a neural ODE base-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Correcting misspecified Lotka-Volterra dynamics. BoxLM can introduce corrections to standard Lotka-Volterra dynamics (no warm-start) and a hybrid neural ODE approach (warm-start) that outperform several baselines. (left) LM-proposed model predictions on training data and extrapolations (grey region). (right) Test MAE of LM models (No-WS, WS-Constraint, and WS-No Constraint) compared to the standard Lotka-Volterra model LV, a Neural ODE, and a hybrid Neural ODE model with a multiplicative correction to the prey-predator dynamics (Hybrid).</p>
<div class="codehilite"><pre><span></span><code><span class="n">LM</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">Lotka</span><span class="o">-</span><span class="n">Volterra</span><span class="w"> </span><span class="n">programs</span>
<span class="n">def</span><span class="w"> </span><span class="n">no_warm_start</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">_coeffs</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Hypothesis</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">Logistic</span><span class="w"> </span><span class="n">growth</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">prey</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">carrying</span><span class="w"> </span><span class="n">capacity</span><span class="w"> </span><span class="n">kappa</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Hypothesis</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">Saturation</span><span class="w"> </span><span class="n">effect</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">predation</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="k">parameter</span><span class="w"> </span><span class="n">psi</span>
<span class="w">    </span><span class="n">db_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">kappa</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">psi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="w">    </span><span class="n">dc_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">epsilon</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="o">**</span><span class="mi">2</span>
<span class="n">def</span><span class="w"> </span><span class="n">warm_start_constrained</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">_mlp</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jnp</span><span class="p">.</span><span class="k">array</span><span class="p">(</span><span class="o">[</span><span class="n">b</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">Input</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">prey</span><span class="w"> </span><span class="n">density</span><span class="w"> </span><span class="p">(</span><span class="n">bunny</span><span class="w"> </span><span class="n">population</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Neural</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">learns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">modifier</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">handling</span><span class="w"> </span><span class="nc">time</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">prey</span><span class="w"> </span><span class="n">density</span>
<span class="w">    </span><span class="n">handling_time_modifier</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mlp</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Modulate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predation</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">non</span><span class="o">-</span><span class="n">linear</span><span class="w"> </span><span class="n">manner</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">handling</span>
<span class="w">        </span><span class="nc">time</span>
<span class="w">    </span><span class="n">db_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">handling_time_modifier</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="w">    </span><span class="n">dc_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">handling_time_modifier</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="n">def</span><span class="w"> </span><span class="n">warm_start_unconstrained</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">_mlp</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jfu</span><span class="p">.</span><span class="n">ravel_pytree</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">))</span><span class="o">[</span><span class="n">0</span><span class="o">]</span>
<span class="w">    </span><span class="n">mlp_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mlp</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Fine</span><span class="o">-</span><span class="n">tuned</span><span class="w"> </span><span class="n">scaling</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">MLP</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">match</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">amplitude</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">closely</span>
<span class="w">    </span><span class="n">db_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.02</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mlp_output</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Reduced</span><span class="w"> </span><span class="n">scaling</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="n">bunnies</span>
<span class="w">    </span><span class="n">dc_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.06</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mlp_output</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Increased</span><span class="w"> </span><span class="n">scaling</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="n">cats</span>
</code></pre></div>

<p>Figure 5. BoxLM can propose corrections to ODEs. (top) In the no warm-start (No-WS) variation, BoxLM introduces corrections informed by domain knowledge of predator-prey models (carrying capacity, predation saturation). (middle) When prompted to introduce neural networks in an interpretable way (WS-Constraint), one strategy BoxLM proposes is to make the handling time parameter depend non-linearly on the prey density, extending a traditional approach to modeling predation saturation. (bottom) When prompted to introduce neural networks without constraints (WS-No Constraint), BoxLM introduces additive MLP-parameterized corrections and adjusts the scaling factors.
line, and a hybrid neural ODE baseline that incorporates a multiplicative correction (parameterized by an MLP) to the predator-prey interaction term in the predator equation. See Section C of the Appendix for details on these baselines. For the LM variations, we report the average test MAE across three runs. In Figure 4, we see that all BoxLM variations outperform the baselines.</p>
<p>In Figure 5, we present code snippets corresponding to representative programs proposed in the constrained and unconstrained variations. These snippets show how natural language constraints can guide BoxLM towards more flexible models that retain interpretability. For the variation with no natural language constraints (WS-No Constraint), BoxLM takes a purely empirical approach. In particular, BoxLM adjusts the scaling terms on the additive MLP correction</p>
<p>term. For the WS-Constraint variation, BoxLM proposes a hybrid approach integrating the neural approach in the prompt with classic models in the literature; importantly, even though BoxLM is asked to balance interpretability and flexibility, BoxLM still identifies programs that outperform the neural ODE and standard Lotka-Volterra baselines. One approach BoxLM proposes is an extension of the Rosenzweig and MacArthur model with a Holling's type II functional response (Rosenzweig \&amp; MacArthur, 1963) to allow a static parameter to depend dynamically on the prey density: BoxLM models the handling time, or the time a predator spends "processing" a prey, as a nonlinear function of the prey density via an MLP. These results show how we can use natural language to drive BoxLM towards models that balance flexibility and interpretability.</p>
<h2>4. Conclusion</h2>
<p>We introduced a method for leveraging LMs for automated model discovery. Our method can identify models that perform favorably against strong baselines and improve upon expert models. We also studied how domain knowledge and natural language constraints influence our system. Altogether, our results highlight the compelling advantages of LM-driven statistical model discovery.</p>
<p>Our work has important limitations that motivate future research. First, we focused on modeling static datasets. An interesting direction could be leveraging LMs for active data collection. Second, since our tasks were restricted to onedimensional datasets, simple model criticism statistics were sufficient and therefore decided in advance (residuals, posterior predictive mean). Another interesting future direction could be fully automating the criticism step. Finally, while in-context learning was effective in our tasks, we could explore finetuning techniques for training a language model to produce better probabilistic programs.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Round-to-round improvement. Model revision leads to improvements on average. (Improvement is not necessarily monotonic for a given dataset and run.) (top) ELPD LOO score vs. round for Stan experiments. We normalize the ELPD LOO scores across programs proposed for 3 rounds of Box's loop and average across datasets for the no metadata condition. Larger is better and error bars correspond to standard error. (bottom) Squared error vs round for LV experiment. We report the squared error averaged across three different runs, for the warm-start, no constraint condition; smaller is better.</p>
<h2>Acknowledgements</h2>
<p>This work was supported in part by AFOSR Grant FA9550-21-1-0397, ONR Grant N00014-22-1-2110, and an NSF Expeditions Grant, Award Number (FAIN) 1918771. EBF is a Chan Zuckerberg Biohub - San Francisco Investigator.
We thank Eric Zelikman and Neil Band for detailed feedback on this paper. We also thank Omar Shaikh and Jensen Gao for helpful discussions.</p>
<h2>Impact Statement</h2>
<p>This paper presents work whose goal is to partially automate statistical modeling. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<h2>References</h2>
<p>Abril-Pla, O., Andreani, V., Carroll, C., Dong, L., Fonnesbeck, C. J., Kochurov, M., Kumar, R., Lao, J., Luhmann, C. C., Martin, O. A., Osthege, M., Vieira, R., Wiecki, T., and Zinkov, R. PyMC: a modern, and comprehensive probabilistic programming framework in python. PeerJ Computer Science, 9, 2023.</p>
<p>Achiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kaiser, L., Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, H., Kiros, J. R., Knight, M., Kokotajlo, D., Kondraciuk, L., Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A. A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D. P., Mu, T., Murati, M., Murk, O., M'ely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Long, O., O’Keefe, C., Pachocki, J. W., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Pokorny, M., Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M. D., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D.,</p>
<p>Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B. D., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N. A., Thompson, M., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. GPT-4 Technical Report. 2023.</p>
<p>Baum, L. E. and Petrie, T. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554-1563, 1966.</p>
<p>Blei, D. M. Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application, 1(1):203-232, 2014.</p>
<p>Bommarito, M. J. and Katz, D. M. GPT Takes the Bar Exam. ArXiv, abs/2212.14402, 2022.</p>
<p>Bongard, J. C. and Lipson, H. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 104:9943 - 9948, 2007.</p>
<p>Box, G. E. P. and Hunter, W. G. A useful method for modelbuilding. Technometrics, 4:301-318, 1962.</p>
<p>Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M.,</p>
<p>Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.</p>
<p>Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, pp. 6572-6583, Red Hook, NY, USA, 2018. Curran Associates Inc.</p>
<p>Duvenaud, D., Lloyd, J., Grosse, R., Tenenbaum, J., and Zoubin, G. Structure discovery in nonparametric regression through compositional kernel search. In Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 11661174, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023.</p>
<p>Gelman, A. and Rubin, D. B. Inference from iterative simulation using multiple sequences. Statistical Science, 7(4): 457-472, 1992. ISSN 08834237.</p>
<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. Bayesian data analysis, third edition. 2013.</p>
<p>Goodman, N. D., Mansinghka, V. K., Roy, D. M., Bonawitz, K., and Tenenbaum, J. B. Church: a language for generative models. In Conference on Uncertainty in Artificial Intelligence, 2008.</p>
<p>Grosse, R. B. Model selection in compositional spaces. 2014.</p>
<p>Homan, M. D. and Gelman, A. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593-1623, jan 2014. ISSN $1532-4435$.</p>
<p>Kalman, R. E. A new approach to linear filtering and prediction problems. ASME Journal of Basic Engineering, 82(1):35-45, March 1960.</p>
<p>Kidger, P. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.</p>
<p>Lee, P., Bubeck, S., and Petro, J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. New England Journal of Medicine, 388(13):1233-1239, 2023. doi: 10.1056/NEJMsr2214184. PMID: 36988602.</p>
<p>Linka, K., St. Pierre, S. R., and Kuhl, E. Automated model discovery for human brain using constitutive artificial neural networks. Acta Biomaterialia, 160:134-151, 2023. ISSN 1742-7061. doi: https://doi.org/10.1016/j.actbio. 2023.01.055.</p>
<p>Lloyd, J. R., Duvenaud, D. K., Grosse, R. B., Tenenbaum, J. B., and Ghahramani, Z. Automatic construction and natural-language description of nonparametric regression models. In AAAI Conference on Artificial Intelligence, 2014.</p>
<p>Magnusson, M., Bürkner, P., and Vehtari, A. posteriordb: a set of posteriors for Bayesian inference and probabilistic programming, October 2023.</p>
<p>McKinney, B. A., Crowe, J. E., Voss, H. U., Crooke, P. S., Barney, N., and Moore, J. H. Hybrid grammar-based approach to nonlinear dynamical system identification from biological time series. Phys. Rev. E, 73:021912, Feb 2006. doi: 10.1103/PhysRevE.73.021912.</p>
<p>Miller, A. C., Foti, N. J., and Fox, E. Learning insulinglucose dynamics in the wild. In Proceedings of the 5th Machine Learning for Healthcare Conference, volume 126 of Proceedings of Machine Learning Research, pp. 172-197. PMLR, 07-08 Aug 2020.</p>
<p>Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. ArXiv, abs/1905.10437, 2019.</p>
<p>Qiu, L., Jiang, L., Lu, X., Sclar, M., Pyatkin, V., Bhagavatula, C., Wang, B., Kim, Y., Choi, Y., Dziri, N., and Ren, X. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Rabiner, L. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257-286, 1989. doi: 10.1109/5.18626.</p>
<p>Rosenzweig, M. and MacArthur, R. Graphical representation and stability conditions of predator-prey interaction. American Naturalist, 97:209-223, 1963.</p>
<p>Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M. P., Ferrer, C. C.,</p>
<p>Grattafiori, A., Xiong, W., D’efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code Llama: Open Foundation Models for Code. ArXiv, abs/2308.12950, 2023.</p>
<p>Saad, F. A., Patton, B., Hoffman, M., Saurous, R. A., and Mansinghka, V. K. Sequential monte carlo learning for time series structure discovery. In International Conference on Machine Learning, 2023.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.</p>
<p>Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data. Science, 324(5923):81-85, 2009. doi: 10.1126/science. 1165893.</p>
<p>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
van de Meent, J.-W., Paige, B., Yang, H., and Wood, F. An introduction to probabilistic programming, 2021.</p>
<p>Vehtari, A., Gelman, A., and Gabry, J. Practical Bayesian model evaluation using leave-one-out crossvalidation and WAIC. Statistics and Computing, 27(5): 1413-1432, sep 2017. ISSN 0960-3174. doi: 10.1007/ s11222-016-9696-4.
von Bertalanffy, L. Problems of organic growth. Nature, 163:156-158, 1949.</p>
<p>Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and Goodman, N. D. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.</p>
<p>Wilson, A. G. and Adams, R. P. Gaussian process kernels for pattern discovery and extrapolation. In Proc. ICML, 2013.</p>
<p>Wong, L. S., Grand, G., Lew, A. K., Goodman, N. D., Mansinghka, V. K., Andreas, J., and Tenenbaum, J. B. From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. ArXiv, abs/2306.12672, 2023.</p>
<p>Wood, F., van de Meent, J. W., and Mansinghka, V. A new approach to probabilistic programming inference. In Proceedings of the 17th International conference on Artificial Intelligence and Statistics, pp. 1024-1032, 2014.</p>
<p>Wu, Y., Jia, F., Zhang, S., Li, H., Zhu, E., Wang, Y., Lee, Y. T., Peng, R., Wu, Q., and Wang, C. An Empirical Study on Challenging Math Problem Solving with GPT-4, 2023.</p>
<p>Zhong, R., Zhang, P., Li, S., Ahn, J., Klein, D., and Steinhardt, J. Goal driven discovery of distributional differences via language descriptions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<h1>A. Gaussian Process experiments</h1>
<p>In the prompt, we ask the LM to use the following operations.</p>
<ol>
<li>Replace a subexpression $\mathcal{S}$ with $\mathcal{S}+\mathcal{B}$, where $\mathcal{B}$ is any base kernel.</li>
<li>Replace a subexpression $\mathcal{S}$ with $\mathcal{S} x \mathcal{B}$, where $\mathcal{B}$ is any base kernel.</li>
<li>Any base kernel $\mathcal{B}$ may be replaced with any other base kernel family.</li>
</ol>
<p>LM hyperparameters We provide the LM with the following kernels: Exponentiated Quadratic, Periodic, Linear, and Polynomial. We run our pipeline for two rounds with three proposals each round. We use a temperature of 0.2 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<p>In the augmented variation, we also provide the LM with the following additional kernels: Matern32, Matern52, Cosine, and the Rational Quadratic kernel. In the augumented variation, we run our pipeline for three rounds with eight proposals each round. We use a temperature of 0.7 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<p>The marginal likelihood can be multimodal in the parameters of the periodic kernel. Therefore, following Duvenaud et al. (2013), if the proposed kernel has periodic components, we initialize the period at five different initial values, optimize the marginal likelihood starting from those different initializations, and choose the kernel hyperparameters with the highest marginal likelihood across those initializations.</p>
<p>Spectral Mixture kernel We use a GP with a spectral mixture kernel (Wilson \&amp; Adams, 2013) with 5 mixture components. For each dataset, we randomly initialize the parameters of the mixture and choose the kernel hyperparameters with the highest log marginal likelihood across five random initializations.</p>
<h2>B. Stan Experiments</h2>
<p>Eight Schools Dataset This dataset consists of eight observations: the estimated treatment effect of a SAT coaching program and the standard error of the treatment effect.</p>
<p>Peregrine dataset This dataset consists of peregrine population counts in the French Jura from 1964 to 2003 (40 observations in total).</p>
<p>Dugongs Dataset The ages and lengths of 27 captured dugongs (sea cows).
Surgical Dataset The mortality rates in 12 hospitals performing cardiac surgery on babies.
LM hyperparameters We run our pipeline for three rounds with eight proposals each round. We use a temperature of 0.7 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<p>Markov Chain Monte Carlo diagnostics We evaluated the fidelity of the learned posteriors using the Gelman-Rubin $\hat{R}$ diagnostic (Gelman \&amp; Rubin, 1992) and by examining the Bulk Effective Sample Size (ESS). In particular, the programs reported in the table all had $\hat{R} \leq 1.01$ and mean bulk ESS $&gt;=400$ per chain.</p>
<h2>C. Lotka-Volterra</h2>
<p>Dataset To create our dataset, we simulate data from the following "perturbed" Lotka-Volterra dynamics</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha b-\beta b c \
&amp; \frac{d c}{d t}=-\gamma c+\delta b c^{0.95}
\end{aligned}
$$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7. LM proposes programs informed by domain knowledge LM chooses a model informed by the domain (animal length vs age) and sets the priors based on the dataset (e.g., the largest length in the dataset is smaller than 3).</p>
<p>We set $\alpha=0.9, \beta=1.1, \delta=-1.2, \gamma=2.1$. The parameter $\alpha$ characterizes the prey's maximum growth rate and $\beta$ controls how the predator population modulate the growth rate. The parameter $\gamma$ characterizes the prey's maximum death rate and $\delta$ controls how the predator's growth rate depends on the prey population density. In contrast to the standard Lotka-Volterra dynamics, we raise $c$ to a fractional power.
We now describe the various baselines we compare against in Section 3.3.
Standard Lotka-Volterra We fit the free parameters of the standard Lotkva-Volterra differential equations.</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha b-\beta b c \
&amp; \frac{d c}{d t}=-\gamma c+\delta b c
\end{aligned}
$$</p>
<p>Neural ODE baseline We parameterize the predator and prey equations with an MLP. We run a hyperparameter search over four widths $(4,8,16,32)$ and 3 depths $(1,2,4)$. We use a learning rate of $3 \mathrm{e}-3$ and train using full-batch gradient descent with Adam for 1500 iterations.</p>
<p>Hybrid Neural ODE baseline We implement a Hybrid Neural ODE baseline that introduces a correction to the predatorprey interaction term. Note that, we follow a two-stage "boosting" type procedure to fit the parameters of the MLP. First, we fit the free parameters $\alpha, \beta, \gamma, \delta$ to the data. We then freeze those parameters and fit the MLP parameters. Without this two-staged approach, the MLP term can dominate the dynamics. The MLP term has one layer and four hidden units and we train the MLP with full batch gradient descent with Adam using a learning rate of 3e-3.</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha \cdot b-\beta \cdot b \cdot c \
&amp; \frac{d c}{d t}=-\gamma \cdot c+\delta \cdot b \cdot(c+0.1 \cdot \operatorname{mlp}(b, c))
\end{aligned}
$$</p>
<p>In the warm-start variations, we provide the LM with an initial hybrid Neural ODE baseline that introduces an additive correction to prey equation. The MLP term has one layer and four hidden units.</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha \cdot b-\beta \cdot b \cdot c+0.1 \cdot \operatorname{mlp}(b, c) \
&amp; \frac{d c}{d t}=-\gamma \cdot c+\delta \cdot b \cdot c
\end{aligned}
$$</p>
<p>LM hyperparameters We run our pipeline for four rounds with twelve proposals each round. We use a temperature of 0.7 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<h1>D. Failure rates of GPT-4 V proposed programs</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Percent successfully scored</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 textual dataframe</td>
<td style="text-align: left;">$78 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 vision only</td>
<td style="text-align: left;">$70 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$76 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2. Percentage of pymc programs that we can successfully perform inference in for Stan experiments.</p>
<h2>E. Additional ODE results</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8. Modeling nonlinear ODEs. LM can introduce polynomial corrections to simple harmonic oscillator (SHO) that provide a better fit. Grey region indicates extrapolation region.</p>
<p>We evaluated on three additional ODEs (nonlinear oscillators) from this github repository pysindy. These ODEs include a Duffing ODE, Van Der Pol oscillator, and a Cubic Harmonic Damped oscillator. We give BoxLM a simple harmonic oscillator to start and ask it to introduce polynomial corrections to improve the fit. Consistent with Section 3.3, our approach can generally improve upon a baseline ODE and extrapolate accurately into the test region.</p>
<h2>F. Visual Interface/GPT-3.5 Ablations</h2>
<p>Context length limits are a potential limitation if we provide the dataset in textual format in the prompt. We therefore experiment with a visual-only variation. We remove all textual representations of datasets and model criticism statistics from the prompt and only provide visual plots of these datasets and statistics. We show this visual-only variation does not harm the performance relative to textual variation and should not suffer as much from context length limits as the dataset grows larger. We also show we can obtain similar results with GPT-3.5 Turbo, which is significantly less costly than GPT-4.</p>
<h2>G. State-space model hidden state update experiments</h2>
<p>We present the same results from the main text but take a state-space update inspired approach to computing the natural language criticism $h^{t}$.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>GPT-4 Text</th>
<th>GPT-4 Visual Only</th>
<th>GPT-3.5</th>
<th>Expert</th>
</tr>
</thead>
<tbody>
<tr>
<td>Eight schools</td>
<td>-30.17</td>
<td>-30.40</td>
<td>-30.44</td>
<td>-30.70</td>
</tr>
<tr>
<td>Dugongs</td>
<td>22.61</td>
<td>23.76</td>
<td>21.01</td>
<td>22.52</td>
</tr>
<tr>
<td>Surgical</td>
<td>-37.36</td>
<td>-38.54</td>
<td>-42.2</td>
<td>-40.29</td>
</tr>
<tr>
<td>Peregrine</td>
<td>-164.69</td>
<td>-143.45</td>
<td>-161.14</td>
<td>-142.19</td>
</tr>
</tbody>
</table>
<p>Table 3. Vision interface and model type ablations. Comparison of GPT-4 with textual representation of data and model criticism statistics in prompt, GPT-4 with only visual representations (GPT-4 Visual Only) of data and model criticism statistics (e.g., only plots), and GPT 3.5 Turbo against expert programs. The visual-only variation (GPT-4 Visual-Only) performs comparably to the textual variation (GPT-4 Text )and outperforms the textual variation on the Peregrine dataset. GPT-3.5 performs slightly worse on some datasets but comparably on most. We report the expected predictive log density (LOO).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">BoxLM+ State Space</th>
<th style="text-align: center;">BoxLM State Space</th>
<th style="text-align: center;">Periodic</th>
<th style="text-align: center;">AS</th>
<th style="text-align: center;">SM</th>
<th style="text-align: center;">N-BEATS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Air</td>
<td style="text-align: center;">$\mathbf{0 . 0 4}$</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$\underline{0.06}$</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: left;">Beer</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">$\underline{0.06}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5}$</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: left;">Heart</td>
<td style="text-align: center;">$\mathbf{0 . 2 0}$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$\mathbf{0 . 2 0}$</td>
<td style="text-align: center;">$\underline{0.21}$</td>
<td style="text-align: center;">$\underline{0.21}$</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">Milk</td>
<td style="text-align: center;">$\mathbf{0 . 0 6}$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$\underline{0.10}$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$\underline{0.09}$</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">Wine</td>
<td style="text-align: center;">$\underline{0.17}$</td>
<td style="text-align: center;">$\underline{0.17}$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">Wool</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">$\underline{0.15}$</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">0.18</td>
</tr>
</tbody>
</table>
<p>Figure 9. Test set performance of BoxLM with state-space updated on time series datasets. Performance of BoxLM system with state space update for model criticism. Comparison of BoxLM test mean absolute error (MAE) against Automatic Statistician using greedy search (AS), spectral mixture kernel (SM), periodic kernel (Periodic), and N-BEATS. BoxLM+ searches over an augmented kernel space. We bold the best and underline the second best among the GP methods.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10. Correcting misspecified Lotka-Volterra dynamics (BoxLM with state-space updates). BoxLM can introduce corrections to standard Lotka-Volterra dynamics (no warm-start) and a hybrid neural ODE approach (warm-start) that outperform several baselines. Test MAE of LM models (No-WS, WS-Constraint, and WS-No Constraint) compared to the standard Lotka-Volterra model LV, a Neural ODE, and a hybrid Neural ODE model with a multiplicative correction to the prey-predator dynamics (Hybrid).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Expert</th>
<th style="text-align: left;">LM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eight schools</td>
<td style="text-align: left;">$\underline{-30.70}$</td>
<td style="text-align: left;">$\underline{-30.17}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-18.39}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim no metadata</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-18.90}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs</td>
<td style="text-align: left;">$\underline{22.52}$</td>
<td style="text-align: left;">$\underline{22.61}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim</td>
<td style="text-align: left;">$\underline{50.04}$</td>
<td style="text-align: left;">$\underline{57.4}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim no metadata</td>
<td style="text-align: left;">$\mathbf{5 0 . 0 4}$</td>
<td style="text-align: left;">26.68</td>
</tr>
<tr>
<td style="text-align: left;">Surgical</td>
<td style="text-align: left;">$\mathbf{- 4 0 . 2 9}$</td>
<td style="text-align: left;">-37.36</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim</td>
<td style="text-align: left;">$\underline{-39.80}$</td>
<td style="text-align: left;">$\underline{-38.45}$</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim no metadata</td>
<td style="text-align: left;">$\mathbf{- 3 9 . 8 0}$</td>
<td style="text-align: left;">-58.72</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine</td>
<td style="text-align: left;">$\mathbf{- 1 4 2 . 1 9}$</td>
<td style="text-align: left;">-164.69</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim</td>
<td style="text-align: left;">$\mathbf{- 1 3 0 . 4 8}$</td>
<td style="text-align: left;">-177.15</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim no meta</td>
<td style="text-align: left;">$\underline{-130.48}$</td>
<td style="text-align: left;">$\underline{-127.32}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Comparison of BoxLM with state-space update programs against expert programs We perform this comparison across four different datasets and two different ablations that replace observations with synthetic observations and remove all metadata. We report the expected predictive log density estimated via leave-one-out cross validation. We bold statistically significant differences and underline non-significant differences.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ To avoid overloading the word "model", we will refer to language models as LMs.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
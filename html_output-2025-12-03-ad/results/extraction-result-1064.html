<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1064 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1064</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1064</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-dffaed47ad4c0cde4f38111fc05d56062342db83</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dffaed47ad4c0cde4f38111fc05d56062342db83" target="_blank">Causal Counterfactuals for Improving the Robustness of Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> CausalCF is proposed, the first complete Causal RL solution incorporating ideas from Causal Curiosity and CoPhy, and it is shown that it improves the RL agent's robustness using CausalWorld.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) is used in various robotic applications. RL enables agents to learn tasks autonomously by interacting with the environment. The more critical the tasks are, the higher the demand for the robustness of the RL systems. Causal RL combines RL and causal inference to make RL more robust. Causal RL agents use a causal representation to capture the invariant causal mechanisms that can be transferred from one task to another. Currently, there is limited research in Causal RL, and existing solutions are usually not complete or feasible for real-world applications. In this work, we propose CausalCF, the first complete Causal RL solution incorporating ideas from Causal Curiosity and CoPhy. Causal Curiosity provides an approach for using interventions, and CoPhy is modified to enable the RL agent to perform counterfactuals. Causal Curiosity has been applied to robotic grasping and manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation environment based on the TriFinger robot. We apply CausalCF to complex robotic tasks and show that it improves the RL agent's robustness using CausalWorld.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1064.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1064.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalCF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Counterfactuals (CausalCF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A complete Causal Reinforcement Learning solution that combines interventions and learned counterfactuals (adapted from CoPhy) to produce an abstract causal representation concatenated to agent observations, improving robustness on complex robotic manipulation tasks in CausalWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CausalCF-agent (SAC + causalRep)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An embodied RL agent using Soft Actor-Critic (SAC) whose observations are augmented with a learned causal representation produced by a Counterfactual Model (adapted CoPhy). The CF model is trained to predict counterfactual future states (MSE loss) from interventions; the agent is trained to maximize CausalWorld rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (TriFinger simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (Pushing, Picking tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Realistic PyBullet TriFinger-based robotic manipulation benchmark where tasks involve arranging blocks to goal shapes. The environment exposes structured observations (feature vector per object + robot), allows interventions on causal variables (goal pose, block pose, block mass, block size, floor friction), and defines 12 evaluation protocols that vary subsets of environment variables across two parameter spaces (Space A and Space B). Tasks have long episodes (episode length 834) and multiple objects.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task complexity characterized by: number of objects n (used in fractional success formula), episode length (834 steps), object feature dimensionality (structured obs vector length 56 per object), number of time steps used for CF model (T=30), and overall training horizon (up to 7,000,000 environment steps). Protocol complexity encoded via which variables are varied (bp, bm, bs, gp, ff) in the 12 protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (robotic multi-object manipulation tasks with long episodes and many state variables; training horizon up to 7e6 steps)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Environment variation quantified by: interventions per episode (e.g., goal-shape interventions each episode during training), protocol-specific variable ranges (Space A vs Space B), and the 12 evaluation protocols which specify which variables are randomized (e.g., P6: block pose and goal pose varied in Space B).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high during training for CausalCF (interventions each episode) and high in several evaluation protocols (Space B protocols that randomize block pose/goal/mass/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fractional_success (overlap volume between current and goal block poses) and 'full integrated fractional success' (mean fractional success over 200 evaluation episodes per protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Training: converged to fractional_success >0.9 (same final level as no_Intervene baseline) though required more training time than the repetitive no_Intervene environment; Evaluation: CausalCF(iter) performed equal or better than other solutions in 10/12 evaluation protocols (no per-protocol numeric values given in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: increasing environment variation via repeated interventions per episode increases task difficulty and slows convergence, but produces better generalization than training with no interventions; adding counterfactual learning (CausalCF) mitigates the negative impact of variation by capturing invariant causal mechanisms and thus improves robustness. Iterating updates to the causal representation trades faster convergence for increased robustness (iteration causes slower convergence because the agent must partially re-learn after representation changes).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Example reported: no_Intervene (low variation) converged to fractional_success >0.9 in ~1.5e6 environment steps (used as a comparative point).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-phase training: (1) Counterfactual training (CF model trained end-to-end using data from an RL agent performing interventions) to produce causalRep; (2) Agent training (SAC) with causalRep concatenated to observations; optional iteration loop alternating agent training and counterfactual retraining every 500,000 steps (total agent training up to 7e6 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on 12 CausalWorld protocols that alter different subsets of environment variables and parameter spaces (Space A vs B). CausalCF (iter) was most robust, performing equal or better than baselines in 10/12 protocols; overall, agents trained with interventions and counterfactuals generalized better to out-of-distribution protocols than the no-intervention baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Convergence took substantially more environment interactions than the no-intervention baseline: while no_Intervene reached >0.9 fractional success in ~1.5e6 steps, Intervene needed up to 7e6 steps to reach ~0.8; CausalCF reached >0.9 but required more steps than no_Intervene (within the 7e6-step training budget). CF model training used trajectories of length 30 timesteps and CF training used 40 iterations per epoch for 15 epochs in the described setup.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Interventions (environment variation) increase difficulty and slow learning but improve generalization relative to no interventions; 2) Adding counterfactual learning (CausalCF) allows the agent to capture invariant causal mechanisms and recover high performance (>= no-intervention levels) despite higher variation; 3) Iterating causalRep updates increases robustness across many out-of-distribution protocols (10/12) but slows convergence (trade-off between convergence speed and robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Counterfactuals for Improving the Robustness of Reinforcement Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1064.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1064.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intervene</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervene baseline (interventions, no counterfactuals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline RL agent trained with interventions on environment variables (e.g., goal-shape changes every episode) but without counterfactual learning or a causal representation; used to assess the contribution of interventions alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Intervene-agent (SAC with interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SAC-based simulated TriFinger agent trained with interventions on selected causal variables every episode (e.g., goal pose), but without using the counterfactual CF model or a concatenated causalRep.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (TriFinger simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (Pushing, Picking tasks, training in Space A with goal-pose interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same CausalWorld setup as CausalCF; during training, interventions on goal shape are performed every episode to introduce variation; evaluation uses the 12 protocol pipeline altering subsets of variables and parameter ranges (Space A/B).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity as described above (multi-object manipulation, long episodes); additionally complexity increased by per-episode interventions (goal shape changes).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (robotic manipulation) with higher variation introduced by interventions</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Per-episode interventions on goal shape; evaluation via the 12 protocols and Space B ranges as out-of-distribution variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (training includes interventions each episode)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fractional_success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Training: converged to approx. fractional_success ~0.8 after 7,000,000 steps (slower and lower than no_Intervene baseline). Evaluation: generalized better than no_Intervene baseline but worse than CausalCF in many protocols (specific per-protocol numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that adding interventions (variation) makes tasks more challenging and slows convergence (Intervene slower than no_Intervene) but improves generalization relative to no interventions; lacking counterfactuals limits robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training within Space A with interventions each episode (no counterfactual/causalRep).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Intervene generalized better than no_Intervene across the evaluation protocols (which include Space B variability), but underperformed compared to CausalCF; specific protocol-wise metrics were not reported numerically in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than no_Intervene: required up to 7e6 steps to reach ~0.8 fractional_success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interventions alone produce better generalization than no interventions but increase training difficulty and slow convergence; without counterfactual/causal representations the agent achieves lower final performance under the same training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Counterfactuals for Improving the Robustness of Reinforcement Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1064.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1064.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>no_Intervene</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>no_Intervene baseline (no interventions, SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard SAC baseline trained without interventions (repetitive environment) used to measure how lack of environment variation affects learning speed and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>no_Intervene-agent (SAC baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SAC-based TriFinger agent trained in an environment without interventions (training protocol P0), resulting in a more repetitive, low-variation training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (TriFinger simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (Pushing task, training protocol P0, Space A)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>CausalWorld training with no interventions (P0), so environment variables remain fixed during training and the task distribution is low-variation; still a robotic multi-object manipulation task with episode length 834.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same task complexity measures as above but with low environment variation (no per-episode interventions).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high task complexity but low training variation (repetitive environment)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>No interventions during training (protocol P0); evaluation still uses 12-protocol pipeline to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low during training (no interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fractional_success</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Training: converged to fractional_success >0.9 in ~1.5e6 environment steps (faster convergence due to low variation). Evaluation: generalized worse than Intervene and CausalCF on out-of-distribution protocols (paper notes Intervene generalized better than no_Intervene).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Demonstrates the trade-off: low variation speeds up convergence and reaches high performance on the training distribution, but reduces robustness/generalization to protocols with higher variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Training performance: fractional_success >0.9 (converged) in ~1.5e6 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single environment training without interventions (P0).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Poorer generalization to the 12-protocol evaluation pipeline (especially protocols that intervene on block pose) compared to Intervene and CausalCF.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>More sample-efficient on training distribution: reached >0.9 fractional_success in ~1.5e6 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training in a low-variation, repetitive environment yields rapid convergence and high in-distribution performance but hurts out-of-distribution robustness; interventions during training are beneficial for generalization albeit at cost of slower learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Counterfactuals for Improving the Robustness of Reinforcement Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1064.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1064.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>transfer_Causal_rep + Intervene</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transferred causalRep + Intervene</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that receives a causal representation learned from a CausalCF agent trained on the Pushing task (transfer) and is trained with interventions on the Picking task, demonstrating transferability of causalRep across related robotic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>transfer_Causal_rep + Intervene (SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SAC-based TriFinger agent trained on the Picking task with per-episode interventions; the agent's observations are concatenated with a causal representation transferred from a CausalCF agent trained on the Pushing task (no iterative CF retraining on the new task).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (TriFinger simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (Picking task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Picking task within CausalWorld (same structured observations and protocol machinery); transfer setup keeps the same intervention distribution as Intervene baseline (goal-shape interventions each episode).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Multi-object manipulation complexity as above; transfer complexity arises from changing task (Pushing->Picking) while keeping causal structure similar.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (robotic manipulation) with task transfer complexity</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Interventions each episode on goal shape; evaluation across 12 protocols tests variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (training includes interventions), evaluation includes Space B protocols</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fractional_success and full integrated fractional success across evaluation protocols</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Training: transfer_Causal_rep + Intervene converged to fractional_success >0.9 (better than Intervene which converged to ~0.8). Evaluation: transfer_Causal_rep + Intervene performed equal or better than Intervene in 8/12 evaluation protocols (no per-protocol numeric values given).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that the transferred causalRep provided prior causal knowledge that improved learning under the same intervention-driven variation, demonstrating that causal representations capturing invariant mechanisms can improve performance under varied conditions across related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Transfer learning: causalRep learned on Pushing is transferred to Picking; agent training continues with interventions on the new task (no iterative CF retraining on target task).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Transfer improved both training convergence (reached >0.9 fractional_success) and evaluation robustness (better or equal in 8/12 protocols) compared to the Intervene baseline without transfer, supporting that causalRep captures transferable invariant causal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>More sample-efficient on the Picking task than Intervene (converged to >0.9 vs Intervene's ~0.8 within the same training budget), though exact step counts per convergence not explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Causal representations learned with counterfactuals and interventions on one task transfer to a related task and speed up convergence and improve out-of-distribution robustness compared to agents trained with interventions alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Counterfactuals for Improving the Robustness of Reinforcement Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1064.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1064.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A realistic PyBullet-based simulation environment built around the TriFinger robot that provides structured observations and a 12-protocol evaluation pipeline to test causal representation learning, interventions, and transfer/generalization in robotic manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TriFinger RL agents (SAC variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>CausalWorld is the evaluation environment used by the embodied learning agents in the paper; agents are SAC-based TriFinger controllers receiving structured observations and trained on Pushing and Picking tasks under various protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (TriFinger)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (benchmark environment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Provides structured observations (feature vector per object and robot), controls (joint positions used in experiments), constraints reflecting robot physics, and a 12-protocol evaluation pipeline that defines which environment variables are randomized and whether ranges come from Space A or B. Tasks involve arranging blocks to goal shapes; sim2real transfer is facilitated by TriFinger realism.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity encoded via number of objects, object properties (pose, mass, size), dynamic physics, episode length (834 steps), and long training horizons (millions of steps). The 12 protocols jointly define combinations of variable changes (e.g., P6: block pose and goal pose varied in Space B).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (robotic manipulation with multi-object dynamics and configurable variable perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation is specified by the 12-protocol pipeline and two parameter spaces (Space A and Space B) that define ranges for variables (bp, bm, bs, gp, ff). Agents are evaluated over 200 episodes per protocol to assess robustness to these variations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>ranging from low (P0, Space A, no interventions) to high (protocols that vary many variables and/or use Space B ranges; e.g., P11 varies 'all' variables in Space B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>fractional_success; full integrated fractional success (mean fractional success over 200 episodes for each evaluation protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>CausalWorld explicitly operationalizes variation through protocols and parameter spaces; the paper uses these mechanisms to show that higher variation protocols (Space B and protocols that vary block pose) are harder and reduce performance for all agents, highlighting the trade-off between training variation and learning difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Benchmark supports single-environment training, interventions per episode, and cross-task transfer evaluations; used here with per-episode goal-shape interventions and a 12-protocol evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Used to evaluate generalization across 12 protocols; in this paper, agents trained with interventions and counterfactuals generalized better across the protocol set than agents trained without interventions or without counterfactuals.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CausalWorld's protocol-based variation allows controlled evaluation of how interventions/variation affect learning and generalization; the paper demonstrates that variation via interventions slows in-distribution convergence but improves out-of-distribution robustness, and that causal/counterfactual methods can recover high performance under high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Counterfactuals for Improving the Robustness of Reinforcement Learning', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning <em>(Rating: 2)</em></li>
                <li>COPHY: Counterfactual Learning of Physical Dynamics <em>(Rating: 2)</em></li>
                <li>Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning <em>(Rating: 2)</em></li>
                <li>Causal Reasoning from Meta-reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1064",
    "paper_id": "paper-dffaed47ad4c0cde4f38111fc05d56062342db83",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "CausalCF",
            "name_full": "Causal Counterfactuals (CausalCF)",
            "brief_description": "A complete Causal Reinforcement Learning solution that combines interventions and learned counterfactuals (adapted from CoPhy) to produce an abstract causal representation concatenated to agent observations, improving robustness on complex robotic manipulation tasks in CausalWorld.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CausalCF-agent (SAC + causalRep)",
            "agent_description": "An embodied RL agent using Soft Actor-Critic (SAC) whose observations are augmented with a learned causal representation produced by a Counterfactual Model (adapted CoPhy). The CF model is trained to predict counterfactual future states (MSE loss) from interventions; the agent is trained to maximize CausalWorld rewards.",
            "agent_type": "simulated robotic agent (TriFinger simulation)",
            "environment_name": "CausalWorld (Pushing, Picking tasks)",
            "environment_description": "Realistic PyBullet TriFinger-based robotic manipulation benchmark where tasks involve arranging blocks to goal shapes. The environment exposes structured observations (feature vector per object + robot), allows interventions on causal variables (goal pose, block pose, block mass, block size, floor friction), and defines 12 evaluation protocols that vary subsets of environment variables across two parameter spaces (Space A and Space B). Tasks have long episodes (episode length 834) and multiple objects.",
            "complexity_measure": "Task complexity characterized by: number of objects n (used in fractional success formula), episode length (834 steps), object feature dimensionality (structured obs vector length 56 per object), number of time steps used for CF model (T=30), and overall training horizon (up to 7,000,000 environment steps). Protocol complexity encoded via which variables are varied (bp, bm, bs, gp, ff) in the 12 protocols.",
            "complexity_level": "high (robotic multi-object manipulation tasks with long episodes and many state variables; training horizon up to 7e6 steps)",
            "variation_measure": "Environment variation quantified by: interventions per episode (e.g., goal-shape interventions each episode during training), protocol-specific variable ranges (Space A vs Space B), and the 12 evaluation protocols which specify which variables are randomized (e.g., P6: block pose and goal pose varied in Space B).",
            "variation_level": "medium-to-high during training for CausalCF (interventions each episode) and high in several evaluation protocols (Space B protocols that randomize block pose/goal/mass/etc.)",
            "performance_metric": "fractional_success (overlap volume between current and goal block poses) and 'full integrated fractional success' (mean fractional success over 200 evaluation episodes per protocol)",
            "performance_value": "Training: converged to fractional_success &gt;0.9 (same final level as no_Intervene baseline) though required more training time than the repetitive no_Intervene environment; Evaluation: CausalCF(iter) performed equal or better than other solutions in 10/12 evaluation protocols (no per-protocol numeric values given in paper).",
            "complexity_variation_relationship": "Explicitly discussed: increasing environment variation via repeated interventions per episode increases task difficulty and slows convergence, but produces better generalization than training with no interventions; adding counterfactual learning (CausalCF) mitigates the negative impact of variation by capturing invariant causal mechanisms and thus improves robustness. Iterating updates to the causal representation trades faster convergence for increased robustness (iteration causes slower convergence because the agent must partially re-learn after representation changes).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "Example reported: no_Intervene (low variation) converged to fractional_success &gt;0.9 in ~1.5e6 environment steps (used as a comparative point).",
            "training_strategy": "Two-phase training: (1) Counterfactual training (CF model trained end-to-end using data from an RL agent performing interventions) to produce causalRep; (2) Agent training (SAC) with causalRep concatenated to observations; optional iteration loop alternating agent training and counterfactual retraining every 500,000 steps (total agent training up to 7e6 steps).",
            "generalization_tested": true,
            "generalization_results": "Evaluated on 12 CausalWorld protocols that alter different subsets of environment variables and parameter spaces (Space A vs B). CausalCF (iter) was most robust, performing equal or better than baselines in 10/12 protocols; overall, agents trained with interventions and counterfactuals generalized better to out-of-distribution protocols than the no-intervention baseline.",
            "sample_efficiency": "Convergence took substantially more environment interactions than the no-intervention baseline: while no_Intervene reached &gt;0.9 fractional success in ~1.5e6 steps, Intervene needed up to 7e6 steps to reach ~0.8; CausalCF reached &gt;0.9 but required more steps than no_Intervene (within the 7e6-step training budget). CF model training used trajectories of length 30 timesteps and CF training used 40 iterations per epoch for 15 epochs in the described setup.",
            "key_findings": "1) Interventions (environment variation) increase difficulty and slow learning but improve generalization relative to no interventions; 2) Adding counterfactual learning (CausalCF) allows the agent to capture invariant causal mechanisms and recover high performance (&gt;= no-intervention levels) despite higher variation; 3) Iterating causalRep updates increases robustness across many out-of-distribution protocols (10/12) but slows convergence (trade-off between convergence speed and robustness).",
            "uuid": "e1064.0",
            "source_info": {
                "paper_title": "Causal Counterfactuals for Improving the Robustness of Reinforcement Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Intervene",
            "name_full": "Intervene baseline (interventions, no counterfactuals)",
            "brief_description": "A baseline RL agent trained with interventions on environment variables (e.g., goal-shape changes every episode) but without counterfactual learning or a causal representation; used to assess the contribution of interventions alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Intervene-agent (SAC with interventions)",
            "agent_description": "SAC-based simulated TriFinger agent trained with interventions on selected causal variables every episode (e.g., goal pose), but without using the counterfactual CF model or a concatenated causalRep.",
            "agent_type": "simulated robotic agent (TriFinger simulation)",
            "environment_name": "CausalWorld (Pushing, Picking tasks, training in Space A with goal-pose interventions)",
            "environment_description": "Same CausalWorld setup as CausalCF; during training, interventions on goal shape are performed every episode to introduce variation; evaluation uses the 12 protocol pipeline altering subsets of variables and parameter ranges (Space A/B).",
            "complexity_measure": "Complexity as described above (multi-object manipulation, long episodes); additionally complexity increased by per-episode interventions (goal shape changes).",
            "complexity_level": "high (robotic manipulation) with higher variation introduced by interventions",
            "variation_measure": "Per-episode interventions on goal shape; evaluation via the 12 protocols and Space B ranges as out-of-distribution variation.",
            "variation_level": "high (training includes interventions each episode)",
            "performance_metric": "fractional_success",
            "performance_value": "Training: converged to approx. fractional_success ~0.8 after 7,000,000 steps (slower and lower than no_Intervene baseline). Evaluation: generalized better than no_Intervene baseline but worse than CausalCF in many protocols (specific per-protocol numbers not provided).",
            "complexity_variation_relationship": "Paper reports that adding interventions (variation) makes tasks more challenging and slows convergence (Intervene slower than no_Intervene) but improves generalization relative to no interventions; lacking counterfactuals limits robustness.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-environment training within Space A with interventions each episode (no counterfactual/causalRep).",
            "generalization_tested": true,
            "generalization_results": "Intervene generalized better than no_Intervene across the evaluation protocols (which include Space B variability), but underperformed compared to CausalCF; specific protocol-wise metrics were not reported numerically in the paper.",
            "sample_efficiency": "Less sample-efficient than no_Intervene: required up to 7e6 steps to reach ~0.8 fractional_success.",
            "key_findings": "Interventions alone produce better generalization than no interventions but increase training difficulty and slow convergence; without counterfactual/causal representations the agent achieves lower final performance under the same training budget.",
            "uuid": "e1064.1",
            "source_info": {
                "paper_title": "Causal Counterfactuals for Improving the Robustness of Reinforcement Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "no_Intervene",
            "name_full": "no_Intervene baseline (no interventions, SAC)",
            "brief_description": "A standard SAC baseline trained without interventions (repetitive environment) used to measure how lack of environment variation affects learning speed and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "no_Intervene-agent (SAC baseline)",
            "agent_description": "SAC-based TriFinger agent trained in an environment without interventions (training protocol P0), resulting in a more repetitive, low-variation training distribution.",
            "agent_type": "simulated robotic agent (TriFinger simulation)",
            "environment_name": "CausalWorld (Pushing task, training protocol P0, Space A)",
            "environment_description": "CausalWorld training with no interventions (P0), so environment variables remain fixed during training and the task distribution is low-variation; still a robotic multi-object manipulation task with episode length 834.",
            "complexity_measure": "Same task complexity measures as above but with low environment variation (no per-episode interventions).",
            "complexity_level": "high task complexity but low training variation (repetitive environment)",
            "variation_measure": "No interventions during training (protocol P0); evaluation still uses 12-protocol pipeline to test generalization.",
            "variation_level": "low during training (no interventions)",
            "performance_metric": "fractional_success",
            "performance_value": "Training: converged to fractional_success &gt;0.9 in ~1.5e6 environment steps (faster convergence due to low variation). Evaluation: generalized worse than Intervene and CausalCF on out-of-distribution protocols (paper notes Intervene generalized better than no_Intervene).",
            "complexity_variation_relationship": "Demonstrates the trade-off: low variation speeds up convergence and reaches high performance on the training distribution, but reduces robustness/generalization to protocols with higher variation.",
            "high_complexity_low_variation_performance": "Training performance: fractional_success &gt;0.9 (converged) in ~1.5e6 steps.",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single environment training without interventions (P0).",
            "generalization_tested": true,
            "generalization_results": "Poorer generalization to the 12-protocol evaluation pipeline (especially protocols that intervene on block pose) compared to Intervene and CausalCF.",
            "sample_efficiency": "More sample-efficient on training distribution: reached &gt;0.9 fractional_success in ~1.5e6 steps.",
            "key_findings": "Training in a low-variation, repetitive environment yields rapid convergence and high in-distribution performance but hurts out-of-distribution robustness; interventions during training are beneficial for generalization albeit at cost of slower learning.",
            "uuid": "e1064.2",
            "source_info": {
                "paper_title": "Causal Counterfactuals for Improving the Robustness of Reinforcement Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "transfer_Causal_rep + Intervene",
            "name_full": "Transferred causalRep + Intervene",
            "brief_description": "An agent that receives a causal representation learned from a CausalCF agent trained on the Pushing task (transfer) and is trained with interventions on the Picking task, demonstrating transferability of causalRep across related robotic tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "transfer_Causal_rep + Intervene (SAC)",
            "agent_description": "SAC-based TriFinger agent trained on the Picking task with per-episode interventions; the agent's observations are concatenated with a causal representation transferred from a CausalCF agent trained on the Pushing task (no iterative CF retraining on the new task).",
            "agent_type": "simulated robotic agent (TriFinger simulation)",
            "environment_name": "CausalWorld (Picking task)",
            "environment_description": "Picking task within CausalWorld (same structured observations and protocol machinery); transfer setup keeps the same intervention distribution as Intervene baseline (goal-shape interventions each episode).",
            "complexity_measure": "Multi-object manipulation complexity as above; transfer complexity arises from changing task (Pushing-&gt;Picking) while keeping causal structure similar.",
            "complexity_level": "high (robotic manipulation) with task transfer complexity",
            "variation_measure": "Interventions each episode on goal shape; evaluation across 12 protocols tests variation.",
            "variation_level": "high (training includes interventions), evaluation includes Space B protocols",
            "performance_metric": "fractional_success and full integrated fractional success across evaluation protocols",
            "performance_value": "Training: transfer_Causal_rep + Intervene converged to fractional_success &gt;0.9 (better than Intervene which converged to ~0.8). Evaluation: transfer_Causal_rep + Intervene performed equal or better than Intervene in 8/12 evaluation protocols (no per-protocol numeric values given).",
            "complexity_variation_relationship": "Paper reports that the transferred causalRep provided prior causal knowledge that improved learning under the same intervention-driven variation, demonstrating that causal representations capturing invariant mechanisms can improve performance under varied conditions across related tasks.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Transfer learning: causalRep learned on Pushing is transferred to Picking; agent training continues with interventions on the new task (no iterative CF retraining on target task).",
            "generalization_tested": true,
            "generalization_results": "Transfer improved both training convergence (reached &gt;0.9 fractional_success) and evaluation robustness (better or equal in 8/12 protocols) compared to the Intervene baseline without transfer, supporting that causalRep captures transferable invariant causal mechanisms.",
            "sample_efficiency": "More sample-efficient on the Picking task than Intervene (converged to &gt;0.9 vs Intervene's ~0.8 within the same training budget), though exact step counts per convergence not explicitly reported.",
            "key_findings": "Causal representations learned with counterfactuals and interventions on one task transfer to a related task and speed up convergence and improve out-of-distribution robustness compared to agents trained with interventions alone.",
            "uuid": "e1064.3",
            "source_info": {
                "paper_title": "Causal Counterfactuals for Improving the Robustness of Reinforcement Learning",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "CausalWorld",
            "name_full": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
            "brief_description": "A realistic PyBullet-based simulation environment built around the TriFinger robot that provides structured observations and a 12-protocol evaluation pipeline to test causal representation learning, interventions, and transfer/generalization in robotic manipulation tasks.",
            "citation_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
            "mention_or_use": "use",
            "agent_name": "TriFinger RL agents (SAC variants used in experiments)",
            "agent_description": "CausalWorld is the evaluation environment used by the embodied learning agents in the paper; agents are SAC-based TriFinger controllers receiving structured observations and trained on Pushing and Picking tasks under various protocols.",
            "agent_type": "simulated robotic agent (TriFinger)",
            "environment_name": "CausalWorld (benchmark environment)",
            "environment_description": "Provides structured observations (feature vector per object and robot), controls (joint positions used in experiments), constraints reflecting robot physics, and a 12-protocol evaluation pipeline that defines which environment variables are randomized and whether ranges come from Space A or B. Tasks involve arranging blocks to goal shapes; sim2real transfer is facilitated by TriFinger realism.",
            "complexity_measure": "Complexity encoded via number of objects, object properties (pose, mass, size), dynamic physics, episode length (834 steps), and long training horizons (millions of steps). The 12 protocols jointly define combinations of variable changes (e.g., P6: block pose and goal pose varied in Space B).",
            "complexity_level": "high (robotic manipulation with multi-object dynamics and configurable variable perturbations)",
            "variation_measure": "Variation is specified by the 12-protocol pipeline and two parameter spaces (Space A and Space B) that define ranges for variables (bp, bm, bs, gp, ff). Agents are evaluated over 200 episodes per protocol to assess robustness to these variations.",
            "variation_level": "ranging from low (P0, Space A, no interventions) to high (protocols that vary many variables and/or use Space B ranges; e.g., P11 varies 'all' variables in Space B).",
            "performance_metric": "fractional_success; full integrated fractional success (mean fractional success over 200 episodes for each evaluation protocol).",
            "performance_value": null,
            "complexity_variation_relationship": "CausalWorld explicitly operationalizes variation through protocols and parameter spaces; the paper uses these mechanisms to show that higher variation protocols (Space B and protocols that vary block pose) are harder and reduce performance for all agents, highlighting the trade-off between training variation and learning difficulty.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Benchmark supports single-environment training, interventions per episode, and cross-task transfer evaluations; used here with per-episode goal-shape interventions and a 12-protocol evaluation pipeline.",
            "generalization_tested": true,
            "generalization_results": "Used to evaluate generalization across 12 protocols; in this paper, agents trained with interventions and counterfactuals generalized better across the protocol set than agents trained without interventions or without counterfactuals.",
            "sample_efficiency": null,
            "key_findings": "CausalWorld's protocol-based variation allows controlled evaluation of how interventions/variation affect learning and generalization; the paper demonstrates that variation via interventions slows in-distribution convergence but improves out-of-distribution robustness, and that causal/counterfactual methods can recover high performance under high variation.",
            "uuid": "e1064.4",
            "source_info": {
                "paper_title": "Causal Counterfactuals for Improving the Robustness of Reinforcement Learning",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
            "rating": 2
        },
        {
            "paper_title": "COPHY: Counterfactual Learning of Physical Dynamics",
            "rating": 2
        },
        {
            "paper_title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning",
            "rating": 2
        },
        {
            "paper_title": "Causal Reasoning from Meta-reinforcement Learning",
            "rating": 1
        }
    ],
    "cost": 0.01474025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal Counterfactuals for Improving the Robustness of Reinforcement Learning</h1>
<p>Tom He<br>Trinity College Dublin<br>Dublin, Ireland<br>heto@tcd.ie</p>
<p>Jasmina Gajcin<br>Trinity College Dublin<br>Dublin, Ireland<br>gajcinj@tcd.ie</p>
<h2>Ivana Dusparic</h2>
<p>Trinity College Dublin<br>Dublin, Ireland<br>ivana.dusparic@tcd.ie</p>
<h4>Abstract</h4>
<p>Reinforcement learning (RL) is used in various robotic applications. RL enables agents to learn tasks autonomously by interacting with the environment. The more critical the tasks are, the higher the demand for the robustness of the RL systems. Causal RL combines RL and causal inference to make RL more robust. Causal RL agents use a causal representation to capture the invariant causal mechanisms that can be transferred from one task to another. Currently, there is limited research in Causal RL, and existing solutions are usually not complete or feasible for real-world applications. In this work, we propose CausalCF, the first complete Causal RL solution incorporating ideas from Causal Curiosity and CoPhy. Causal Curiosity provides an approach for using interventions, and CoPhy is modified to enable the RL agent to perform counterfactuals. Causal Curiosity has been applied to robotic grasping and manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation environment based on the TriFinger robot. We apply CausalCF to complex robotic tasks and show that it improves the RL agent's robustness using CausalWorld.</p>
<h2>KEYWORDS</h2>
<p>RL, Robotics, Causality, Counterfactuals, Robustness, Causal RL, Explainability.</p>
<h2>ACM Reference Format:</h2>
<p>Tom He, Jasmina Gajcin, and Ivana Dusparic. 2023. Causal Counterfactuals for Improving the Robustness of Reinforcement Learning. In Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023), London, United Kingdom, May 29 - June 2, 2023, IFAAMAS, 9 pages.</p>
<h2>1 INTRODUCTION</h2>
<p>Intelligent robots are increasingly being considered for use alongside humans in critical applications such as healthcare and transportation [11, 21]. Reinforcement learning (RL) can enable agents or robots to learn tasks unsupervised [20]. RL agents learn tasks from scratch through an iterative exploration and exploitation process. Recent improvements in deep learning (DL) methods enabled deep RL (DRL), which uses deep neural networks (DNN) [8] to represent policies and value functions because DNNs scale efficiently with the amount of data. Applications of RL in the real world, especially in critical applications, require RL agents to be trustworthy. The robustness and explainability of RL need to be improved for RL to be trustworthy in real-world deployments [17].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Causal RL [3] combines RL with causal inference [10, 16] to make RL agents more robust and explainable. Environments are represented as structural causal models (SCM) [16] in Causal RL. SCMs are directed acyclic graphs, where nodes represent causal factors, and edges indicate causal relationships. The causal mechanisms remain invariant across tasks and can improve the robustness of RL [18]. The RL agent learns about the underlying SCM through three different interactions: association (seeing), interventions (doing), and counterfactuals (imagining). All three interactions are needed to learn the complete underlying SCM [3, 4]. The causal information obtained through interacting with the environment is then stored as a causal representation.</p>
<p>Currently, there is limited research on Causal RL. Existing approaches have limitations, such as assuming the SCM is known upfront, or partially known [7, 13, 15], they are usually not complete [1, 12, 19, 23], and they have not been applied to complex tasks like robotics [2, 6]. We define a Causal RL solution to be complete if the solution uses all three of the interactions and a causal representation [3]. In this work, we propose Causal Counterfactuals (CausalCF), the first complete Causal RL solution applied to complex robotic tasks. CausalCF learns about the underlying SCM from scratch using the three different Causal RL interactions. The causal knowledge is stored as an abstract causal representation in the form of a vector and can be concatenated to the states of the agent. The representation is scalable and transferable to different tasks. We implemented and evaluated CausalCF in a realistic robotic simulation environment called CausalWorld [1] which provides a range of complex robotic tasks.</p>
<p>CausalCF incorporates ideas from Causal Curiosity [19] and CoPhy [2]. Causal Curiosity provides an approach for using interventions and a causal representation to train an RL agent, previously successfully applied in CausalWorld [1]. In addition to interventions, CausalCF makes use of counterfactuals. CoPhy is a deep learning solution that can perform counterfactuals. In this work, we adapt the deep learning architecture from CoPhy, originally aimed at supervised learning only, for use in RL. The implementation of our proposed approach, CausalCF, is available on https://github.com/Tom1042roboai/CausalCF. The main contributions of this work are:
(1) We present CausalCF, the first complete Causal RL solution applied to robotic tasks.
(2) We adapt the CoPhy architecture from supervised learning to RL tasks, and incorporate it into CausalCF.
(3) We evaluate all the components of the CausalCF design in CausalWorld and show that they improve the training performance and the robustness of the RL agent.</p>
<p>(4) We evaluate the transferability of the causal representation, and confirm that it captures the invariant causal mechanisms across tasks.</p>
<p>The rest of the paper is structured as follows. Section 2 introduces causal inference, reviews related work, and discusses the limitations of Causal Curiosity and CoPhy. Section 3 presents the evaluation environment, CausalWorld. Section 4 describes the design of our proposed CausalCF. Section 5 presents the experimental designs, results, and analysis. Finally, Section 6 concludes the paper and discusses future work.</p>
<h2>2 BACKGROUND AND RELATED WORK</h2>
<p>Many existing approaches in Causal RL assume that the SCM is known or partially known, making it impractical for many realworld problems. Existing Causal RL solutions are usually incomplete and do not use counterfactuals. Counterfactuals enable the agent to learn the complete structural causal model and improve the robustness of the agent. In this section, we introduce the field of causality, review related work in causal RL, and discuss the approaches: Causal Curiosity and CoPhy.</p>
<h3>2.1 Causality</h3>
<p>The major limitation of current RL and DL methods is their lack of ability to generalize to out-of-distribution data. Combining causality with DRL can help solve the limitation. The SCM M [4] is a model that describes all the causal factors and relationships of a system and can be used to represent a specific RL environment. The agent learns a representation of the SCM M which could be in the form of a causal graph. The aim is that the causal graph captures the invariances of the SCM M that could be transferred to different environments or tasks with similar underlying causal structures. The terms causal modeling, causal reasoning, and causal inference all refer to the study of causality [10, 16]. Different papers use the terms in subtly different ways. In this paper, the term causal inference will refer to the problems of inferring the SCM [16] from data and estimating the causal effects from a learnt representation of the SCM.</p>
<p>The agent can obtain different amounts of information about the SCM M through the three different types of interactions. The three types of interactions are observation or association (seeing), intervention (doing or experimenting), and counterfactual (imagining and introspection). The Pearl causal hierarchy (PCH) [4] categorizes the three types of interactions into different levels. In level 1 (association), the agent can detect regularities from passive observations. In level 2 (interventions), the agent can alter the environment and observe and predict its effects. In level 3 (counterfactuals), the agent can imagine alternate outcomes from unseen interventions or contexts. Higher-level interactions provide more information about the SCM M. The causal hierarchy theorem (CHT) [4] states that the three levels of causality cannot collapse and remain distinct. The CHT implies that agents who learn using lower-level interventions cannot make higher-level inferences like counterfactuals. The agent needs to use level 3 counterfactuals to learn the structural causal model. Counterfactuals require the agent to make inferences on new situations based on the past experiences of the agent. Therefore, counterfactuals prompt the agent to learn the invariant causal
mechanisms across tasks or experiences. The ability of RL agents to perform counterfactuals improves its robustness.</p>
<h3>2.2 Related Work</h3>
<p>Currently, there is limited research on Causal RL. Existing approaches have limitations, such as assuming the SCM is known upfront, or partially known [7, 13, 15], they are usually not complete [1, 12, 19, 23], and they have not been applied to complex tasks like robotics [2, 6].</p>
<p>For instance, [7] presents different approaches for achieving explainable robotics and mentions causal RL as one of the approaches. The paper [7] also highlighted that one of the significant limitations for applying causal RL to real-world problems is that most existing approaches assume a known SCM. For example, [13] is a causal learning solution that uses observational data and interventions to learn the SCM of a Bayesian network. The approach assumes a partially known SCM and does not use counterfactuals. Additionally, [15] assumes a known causal model (causal relationships) and learns the causal factors through RL. In [15], valuable methods for evaluating the explainability of RL were provided. The generation and evaluation of explanations are out of the scope of this paper.</p>
<p>Some methods do not assume a known or partially known SCM but are incomplete Causal RL solutions. CausalWorld [1] provides an environment where RL baselines can train using observations and interventions. However, the RL baselines do not use counterfactuals and causal representations. CausalWorld mainly provides a benchmark for evaluating causal RL solutions. Causal Curiosity [19] uses interventions and an abstract causal representation. However, Causal Curiosity does not use counterfactuals. Another approach [12] makes use of human interventions and learns counterfactual predictions offline for real-world robotic grasping and manipulation tasks. A causal representation is not used in [12]. Additionally, [23] only uses observational data to learn a causal graph, and the solution struggles when graphs reach 50 nodes.</p>
<p>Methods that perform causal structure learning and are complete Causal RL solutions are not applied to more complex tasks like robotics. In [6], all three types of causal interactions were used to learn the causal structure of Bayesian networks, and it used a causal representation. The work demonstrated that all three types of interactions increased the performance of the RL agent. Moreover, CoPhy [2] is a causal learning solution that uses all three types of causal interactions and a causal representation to learn the physical dynamics of different objects in a simulated environment.</p>
<p>Causal Curiosity [19] and Cophy [2] are the most relevant to our chosen application area. We expand on them more.</p>
<h3>2.3 Causal Curiosity</h3>
<p>Causal Curiosity [19] is a Causal RL method that has been applied in CausalWorld. Causal Curiosity learns about the SCM from scratch using interventions and has two main training phases. The recursive training phase aims to learn a Cross Entropy Method optimized Model Predictive Control Planner (CEM MPC) [19] where K actions can be sampled. Each action provides information about one causal factor. This goal is achieved through maximizing the Curiosity reward. In the inference phase, K actions are sampled</p>
<p>from the trained CEM MPC and applied to the RL training environment. The resulting observation is clustered and forms the causal representation (causalRep). The causalRep is then concatenated to all the observations to train the RL agent. The RL agent maximizes the CausalWorld reward.</p>
<p>Causal Curiosity uses interventions and provides an approach for training the RL agent with an abstract causal representation. However, the main drawback is that Causal Curiosity does not use counterfactuals. Counterfactuals are required for the agent to learn the complete underlying SCM and helps to improve the agent's robustness.</p>
<h3>2.4 Counterfactual Learning of Physics Dynamics (CoPhy)</h3>
<p>CoPhy [2] provides a mechanism for performing counterfactuals in a supervised learning application to learn the physical dynamics of different objects in a simulated 3D environment. CoPhy is applied to predict the position, orientation, and stability of a given configuration of blocks for multiple time steps into the future. Figure 1 presents the overall architecture used. Observations A and B for $\tau$ time steps are used for training the entire model from end to end. The latent representation (confounders $U$ ) or the causal representation is the embedding in the last layer of the recurrent neural network (RNN). An intervention is made on observation A (the initial state or $X_{0}$ ), and the result is observation C. Counterfactual predictions are made for $\tau-1$ time steps as the initial intervened observation C is given. The de-rendering blocks in Figure 1 are used to convert the pixel observations into latent representations. The graph convolutional network blocks (GCN) add contextual information to the latent representations. Finally, the RNN temporally integrates the GCN blocks. CoPhy aims to minimize the mean squared error between the predictions and the actual states of the blocks. CoPhy learns about the underlying SCM from scratch using the three interactions in the PCH [4]. As shown in Figure 1, the causal representation (confounders $U$ ) is used to make the counterfactual inferences for every time step. This approach allows us to train confounders $U$ [2] that capture the invariant causal mechanisms across the multiple time steps.</p>
<p>CoPhy is trained using supervised learning and requires many labelled images. DL methods cannot interact with the environment, and the interventions are usually handpicked. On the other hand, RL agents can interact with the environment and perform interventions based on their goals. Our proposed approach, CausalCF, adapts CoPhy for RL and enables RL agents to perform counterfactuals. To the best of our knowledge, this is the first time CoPhy has been adapted for use in RL.</p>
<h2>3 EVALUATION ENVIRONMENT</h2>
<p>CausalWorld [1] is a benchmark environment created to propel causal and transfer learning research for robotics. CausalWorld provides a simulation environment (powered by PyBullet) for robotic manipulation tasks based on the open-source TriFinger robot. Sim2real transfer [22] is easy to perform as CausalWorld is a realistic simulation of TriFinger and real-world physics. All the tasks in CausalWorld aim to arrange the blocks into their desired goal shape.</p>
<p>CausalWorld provides a structured observation type that presents the observation as a feature vector, as shown in Figure 2. We use structured observation to train CausalCF. Structured observation makes experimenting with the feasibility of the solution easier.</p>
<p>The robot can be controlled through three different modes. The modes are joint positions, joint torques, and end-effector positions as indicated with the R1, R2, and R3 parameters in Figure 2. The action is specified for each of the three fingers on the robot. CausalWorld converts the action specified by the vector into motor actions that control the robot. There are constraints set on the values of the action parameters based on the physical properties of the robot. In this work, the RL agent uses joint positions to control the robot. End-effector positions are non-deterministic for the current version of CausalWorld, and joint positions are more intuitive to use than joint torques.</p>
<p>The consecutive actions of the Causal RL agent are chosen to maximize the cumulative rewards in CausalWorld. Rewards are defined differently for different tasks. Rewards are a sum of different objective functions, and each function aims to affect the agent's behavior in a particular way. The user can specify the weights of each function. RL agents would then prioritize to maximize the functions with larger weights. For example, the first objective of the agent in any task is to get its fingers to the blocks, and this objective function would have the largest weight. The weights used for the tasks in this paper are the same as those used in the original CausalWorld paper [1].</p>
<p>CausalWorld will be used for the evaluation of CausalCF. CausalWorld provides an evaluation pipeline that consists of 12 protocols as described in Table 1 [1]. Each of the 12 protocols applied in the evaluation pipeline modifies a specific subset of the CausalWorld variables and defines a range of values that the variables can take [1]. Space A and B [1] in Table 1 define different ranges of values the CausalWorld variables can take. Agents are evaluated over 200 episodes in each of the 12 protocols.</p>
<h2>4 DESIGN OF CAUSALCF</h2>
<p>CausalCF has two main training phases, counterfactual training and agent training, as shown in Figure 3. Counterfactual training produces the causal representation used to train the agent in agent training. An additional component, iteration training, allows the agent to update its causal representation.</p>
<h3>4.1 Counterfactual Training</h3>
<p>The aim of counterfactual training (Figure 3a) is to obtain a useful causal representation that can be concatenated to the observations to train the RL agent. For this purpose, we adapt CoPhy and train it from end to end in counterfactual training.</p>
<p>The original CoPhy solution had a derendering block [2] that converted pixel images into latent representations before passing it to the GCN Block (Figure 1). CoPhy is adaptable to the number of time steps and objects. In contrast to CoPhy, CF model (Figure 3a) receives structured observation (Figure 2) instead of images. Therefore, we replaced the derendering block and dependent processes with new functions. We replaced the derendering block with the convert input shape function (Algorithm 1), which converts the structured observation into a representation adaptable to a</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall architecture of the CoPhy solution [2].
Table 1: Table is adapted from CausalWorld [1]. Variables: bp - block pose, bm - block mass, bs - block size, gp - goal pose and ff - floor friction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">P0</th>
<th style="text-align: left;">P1</th>
<th style="text-align: left;">P2</th>
<th style="text-align: left;">P3</th>
<th style="text-align: left;">P4</th>
<th style="text-align: left;">P5</th>
<th style="text-align: left;">P6</th>
<th style="text-align: left;">P7</th>
<th style="text-align: left;">P8</th>
<th style="text-align: left;">P9</th>
<th style="text-align: left;">P10</th>
<th style="text-align: left;">P11</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Space</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">B</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">B</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">B</td>
<td style="text-align: left;">B</td>
<td style="text-align: left;">A</td>
<td style="text-align: left;">B</td>
</tr>
<tr>
<td style="text-align: left;">Var</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">bm</td>
<td style="text-align: left;">bm</td>
<td style="text-align: left;">bs</td>
<td style="text-align: left;">bp</td>
<td style="text-align: left;">gp</td>
<td style="text-align: left;">bp,</td>
<td style="text-align: left;">bp, gp,</td>
<td style="text-align: left;">bp, gp,</td>
<td style="text-align: left;">bp, gp,</td>
<td style="text-align: left;">all</td>
<td style="text-align: left;">all</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">gp</td>
<td style="text-align: left;">bm</td>
<td style="text-align: left;">bm</td>
<td style="text-align: left;">bm, ff</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Structured observation [1].
varying number of time steps and objects. The representation has the shape (T, K, 56), where T is the number of timesteps, K is the number of objects, and a feature vector of length 56 (Parameters $\mathrm{T}+\mathrm{R} 1+\mathrm{R} 2+\mathrm{R} 3+$ Sn in Figure 2) is enough to describe the state of the robot and any single object. We made modifications throughout the entire architecture to process the structured observations. We used the RL agent to generate the observations and ground truths for training the CF model. Algorithm 2 describes the process of generating the training data and training CF Model. CF Model makes counterfactual predictions about the future states of the environment from the effect of interventions. CF Model aims to minimize the mean squared error between the counterfactual predictions and the actual states of the environment.</p>
<p>The ability to perform counterfactual predictions provides a mechanism for improving the explainability of the RL agent. CausalCF could provide local explanations [17] for a particular action or decision by performing counterfactuals and imagining the future consequences of different actions based on the causal knowledge of the agent. Explanations involve providing the different outcomes with the corresponding rewards as RL agents aim to maximize cumulative rewards. The probability of success [5] could be used instead of rewards because rewards vary from task to task. If the imagined outcomes and probability of success match the knowledge of the human expert, then the decisions or actions of the agent can be trusted. Generation and evaluation of counterfactual explanations are out of the scope of this paper, and we focus on utilizing counterfactuals to improve performance and robustness.
4.1.1 Train CF model with Pretrained agent. CF model was trained with a pretrained RL agent provided by CausalWorld. The pretrained agent can perform useful interventions or actions that allow the causal representation to capture causal factors. The causal representation is concatenated to all the observations for training the RL agent. Using the pretrained agent enables the viability of the CF model to be evaluated in the early stages, and the total training time for CausalCF is reduced. CF model is trained for 15 epochs, and the model is trained for 40 iterations for each epoch. In each epoch, an intervention is made on the goal shape and the block mass. The pretrained agent generates observations for 30 timesteps to train the CF model from end to end in each iteration (Algorithm 2). Approaches without using pretrained agents involve training the RL agent with an initialized causal representation. When the agent converges or reaches a desirable performance, the agent can iterate</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">component</span><span class="w"> </span><span class="nt">Convert</span><span class="w"> </span><span class="nt">Input</span>
<span class="nt">Shape</span>
<span class="nt">1</span><span class="w"> </span><span class="nt">num_timesteps</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">);</span>
<span class="nt">2</span><span class="w"> </span><span class="nt">num_objects</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">K</span><span class="err">\</span><span class="o">);</span>
<span class="nt">3</span><span class="w"> </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="cp">[</span><span class="nx">T</span><span class="p">,</span><span class="w"> </span><span class="nx">K</span><span class="p">,</span><span class="w"> </span><span class="mi">56</span><span class="cp">]</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Empty</span><span class="w"> </span><span class="nt">array</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">storing</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">input</span>
<span class="nt">4</span><span class="w"> </span><span class="nt">lens_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">stack_obs</span><span class="p">.</span><span class="nc">shape</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="mi">1</span><span class="cp">]</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Length</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">the</span>
<span class="w">    </span><span class="nt">Structured</span><span class="w"> </span><span class="nt">observation</span>
<span class="w">    </span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">stack_obs</span><span class="w"> </span><span class="nt">represent</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">Struct_obs_ab</span><span class="w"> </span><span class="nt">and</span>
<span class="w">    </span><span class="nt">Struct_obs_c</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">Figure</span><span class="w"> </span><span class="nt">3a</span>
<span class="nt">5</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">num_timesteps</span><span class="w"> </span><span class="nt">do</span>
<span class="nt">6</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">num_objects</span><span class="w"> </span><span class="nt">do</span>
<span class="nt">7</span><span class="w"> </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">,:</span><span class="w"> </span><span class="mi">28</span><span class="cp">]</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">stack_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="p">,:</span><span class="w"> </span><span class="mi">28</span><span class="cp">]</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="nt">Store</span><span class="w"> </span><span class="nt">T</span><span class="o">,</span><span class="w"> </span><span class="nt">R1</span><span class="o">,</span><span class="w"> </span><span class="nt">R2</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">R3</span>
<span class="w">    </span><span class="nt">obj_index_low</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">28</span><span class="o">+(</span><span class="nt">k</span><span class="o">+</span><span class="nt">17</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">obj_index_up</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">obj_index_low</span><span class="w"> </span><span class="o">+</span><span class="nt">17</span><span class="o">;</span>
<span class="w">    </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="mi">28</span><span class="p">:</span><span class="w"> </span><span class="mi">45</span><span class="cp">]</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">stack_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="o">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obj_index_low</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nx">obj_index_up</span><span class="cp">]</span><span class="o">;</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Store</span><span class="w"> </span><span class="nt">CW</span><span class="w"> </span><span class="nt">Object</span><span class="w"> </span><span class="nt">features</span>
<span class="w">        </span><span class="nt">part_goal_index_low</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">((</span><span class="nt">28</span><span class="o">+(</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">num_objects</span><span class="w"> </span><span class="err">\</span><span class="o">(*</span><span class="w"> </span><span class="nt">17</span><span class="o">))+(</span><span class="nt">k</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nt">11</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">part_goal_index_up</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">part_goal_index_low</span><span class="w"> </span><span class="o">+</span><span class="nt">11</span>
<span class="w">        </span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Store</span><span class="w"> </span><span class="nt">Partial</span><span class="w"> </span><span class="nt">goal</span><span class="w"> </span><span class="nt">features</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="nt">part_goal_index_up</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">geq</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">len_obs</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="mi">45</span><span class="p">:</span><span class="cp">]</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">stack_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="o">\</span><span class="p">),</span><span class="w"> </span><span class="nx">part_goal_index_low</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="cp">]</span><span class="o">;</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">            </span><span class="nt">else</span>
<span class="w">                </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="p">,</span><span class="w"> </span><span class="nx">k</span><span class="p">,</span><span class="w"> </span><span class="mi">45</span><span class="p">:</span><span class="cp">]</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">stack_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="cp">[</span><span class="nx">t</span><span class="o">\</span><span class="p">),</span><span class="w"> </span><span class="nx">part_goal_index_low</span><span class="w"> </span><span class="p">:</span>
<span class="w">                </span><span class="nx">part_goal_index_up</span><span class="cp">]</span><span class="o">;</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="nt">20</span><span class="w"> </span><span class="nt">end</span>
</code></pre></div>

<p>between counterfactual training and agent training to update the causal representation.</p>
<h3>4.2 Agent Training</h3>
<p>The resulting causal representation from counterfactual training is concatenated to all the observations for training the RL agent in agent training. The RL agent aims to maximize the CausalWorld reward in agent training (Figure 3b). The idea of using two training phases and concatenating the causal representation to all the observations used for training the agent is inspired by Causal Curiosity. Interventions on the goal shape (pose) are performed every episode in agent training. CausalCF has an additional component where it iterates between counterfactual and agent training.</p>
<h3>4.3 Iteration Training</h3>
<p>The RL agent iterates between counterfactual and agent training after it has trained for 1.5 million time steps in agent training. Iteration between counterfactual and agent training happens every</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">shows</span><span class="w"> </span><span class="nt">how</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">generated</span><span class="w"> </span><span class="nt">to</span>
<span class="nt">train</span><span class="w"> </span><span class="nt">CF_model</span>
<span class="nt">1</span><span class="w"> </span><span class="nt">max_iter</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">40</span><span class="err">\</span><span class="o">);</span>
<span class="nt">2</span><span class="w"> </span><span class="nt">num_timesteps</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">30</span><span class="err">\</span><span class="o">);</span>
<span class="nt">3</span><span class="w"> </span><span class="nt">initial_obs</span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Stores</span><span class="w"> </span><span class="nt">initial</span><span class="w"> </span><span class="nt">Structured</span>
<span class="w">    </span><span class="nt">observation</span><span class="w"> </span><span class="nt">when</span><span class="w"> </span><span class="nt">environment</span><span class="w"> </span><span class="nt">resets</span>
<span class="nt">4</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">curr_iter</span><span class="w"> </span><span class="err">\</span><span class="o">(=</span><span class="nt">0</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">max_iter</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">RL_agent</span><span class="p">.</span><span class="nc">act</span><span class="o">(</span><span class="nt">initial_obs</span><span class="o">);</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">num_timesteps</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">next_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">env</span><span class="p">.</span><span class="nc">step</span><span class="o">(</span><span class="nt">action</span><span class="o">);</span>
<span class="w">        </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">RL_agent</span><span class="p">.</span><span class="nc">act</span><span class="o">(</span><span class="nt">next_obs</span><span class="o">);</span>
<span class="w">        </span><span class="nt">stack_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">stack_input_obs</span><span class="p">.</span><span class="nc">concatenate</span><span class="o">(</span><span class="nt">next_obs</span><span class="o">);</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Convert_input_shape</span><span class="o">(</span><span class="nt">stack_input_obs</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Calls</span>
<span class="w">        </span><span class="nt">Convert</span><span class="w"> </span><span class="nt">Input</span><span class="w"> </span><span class="nt">Shape</span><span class="w"> </span><span class="nt">component</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">CF_model</span>
<span class="w">    </span><span class="nt">pose_3d_ab</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">process1</span><span class="o">(</span><span class="nt">desired_input_obs</span><span class="o">)</span><span class="w"> </span><span class="o">;</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Turns</span><span class="w"> </span><span class="nt">numpy</span><span class="w"> </span><span class="nt">array</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">torch</span><span class="w"> </span><span class="nt">tensor</span><span class="w"> </span><span class="nt">for</span>
<span class="w">        </span><span class="nt">using</span><span class="w"> </span><span class="nt">GPU</span>
<span class="w">    </span><span class="nt">observation_c</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">goal_intervention</span><span class="o">(</span><span class="nt">env</span><span class="o">);</span>
<span class="w">    </span><span class="nt">desired_obs_c</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Convert_input_shape</span><span class="o">(</span><span class="nt">observation_c</span><span class="o">);</span>
<span class="w">    </span><span class="nt">pose_3d_c</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">process2</span><span class="o">(</span><span class="nt">desired_obs_c</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Turns</span>
<span class="w">        </span><span class="nt">numpy</span><span class="w"> </span><span class="nt">array</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">torch</span><span class="w"> </span><span class="nt">tensor</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">GPU</span>
<span class="w">    </span><span class="nt">pred_D_out</span><span class="o">,</span><span class="nt">pred_stability</span><span class="o">,</span><span class="nt">causal_rep</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">CF_model</span><span class="o">(</span><span class="nt">pose_3d_ab</span><span class="o">,</span><span class="nt">pose_3d_c</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Pass</span><span class="w"> </span><span class="nt">inputs</span>
<span class="w">        </span><span class="nt">through</span><span class="w"> </span><span class="nt">all</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">other</span><span class="w"> </span><span class="nt">components</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">CF_model</span>
<span class="w">    </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">RL_agent</span><span class="p">.</span><span class="nc">act</span><span class="o">(</span><span class="nt">observation_c</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nt">Generate</span>
<span class="w">        </span><span class="nt">ground-truth</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">CF_model</span><span class="w"> </span><span class="nt">predictions</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">num_timesteps</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">next_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">env</span><span class="p">.</span><span class="nc">step</span><span class="o">(</span><span class="nt">action</span><span class="o">);</span>
<span class="w">        </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">RL_agent</span><span class="p">.</span><span class="nc">act</span><span class="o">(</span><span class="nt">next_obs</span><span class="o">);</span>
<span class="w">        </span><span class="nt">stack_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">stack_input_obs</span><span class="p">.</span><span class="nc">concatenate</span><span class="o">(</span><span class="nt">next_obs</span><span class="o">);</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">desired_input_obs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Convert_input_shape</span><span class="o">(</span><span class="nt">stack_input_obs</span><span class="o">);</span>
<span class="w">    </span><span class="nt">actual_D_out</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">process3</span><span class="o">(</span><span class="nt">desired_input_obs</span><span class="o">)</span><span class="w"> </span><span class="o">;</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">Same</span><span class="w"> </span><span class="nt">purpose</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">process1</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">process2</span>
<span class="w">        </span><span class="nt">mse_3d</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Calc_loss</span><span class="o">(</span><span class="nt">pred_D_out</span><span class="o">,</span><span class="nt">actual_D_out</span><span class="o">);</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="nt">CF_model</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">loss</span><span class="w"> </span><span class="nt">calculation</span>
<span class="w">        </span><span class="nt">Reset</span><span class="w"> </span><span class="nt">environment</span>
<span class="w">    </span><span class="nt">end</span>
</code></pre></div>

<p>500,000 time steps. The agent is trained for a total of 7 million time steps. The iteration between the two training phases allows the agent to update the causal representation or its causal knowledge the better the agent gets. The additional iterations for counterfactual training will enable the agent to capture more information about the SCM.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Counterfactual training. This modified version of CoPhy is called Counterfactual model (CF model).
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Agent training</p>
<p>Figure 3: Overview design of the CausalCF solution. CausalCF has two phases of training like in Causal Curiosity. The diagram in the Counterfactual training is adapted from CoPhy [2].</p>
<h2>5 EVALUATION</h2>
<p>In this section, we present the evaluation of CausalCF in CausalWorld. We describe the metrics, training parameters, and stages of evaluation for CausalCF. We also discuss the performance of CausalCF in the stages of evaluation.</p>
<h3>5.1 Metrics and Parameters</h3>
<p>In CausalWorld, fractional success is often used to measure agent performance. Fractional success is calculated as:</p>
<p>$$
\text { fractional_success }=\frac{\sum_{i=1}^{n} \text { intersection }\left(O_{-} \text {vol }<em -="-">{i}, G</em>} \text {vol <em i="1">{i}\right)}{\sum</em>
$$}^{n} G_{-} \text {vol }_{i}</p>
<p>Where n represents the number of objects, $O_{-} \text {vol }<em -="-">{i}$ represents the current pose of the i-th object, and $G</em>} \text {vol <em -="-">{i}$ represents the desired goal pose of the i-th object. The function intersection $\left(O</em>} \text {vol <em -="-">{i}, G</em>\right)$ calculates the overlapping volume between the current block pose and the objects' goal block pose. A higher fractional success will correspond to more precise object manipulations. CausalCF uses fractional success for all the evaluations.} \text {vol }_{i</p>
<p>We use Soft-Actor Critic (SAC) [9] for all the experiments in this work and parameters are given in Table 2. Table 3 shows the training parameters used for all tasks in which CausalCF is evaluated.</p>
<p>Table 2: Parameter configuration for the SAC algorithm.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Parameters</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">gamma</td>
<td style="text-align: left;">0.95</td>
</tr>
<tr>
<td style="text-align: right;">tau</td>
<td style="text-align: left;">$1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: right;">ent_coef</td>
<td style="text-align: left;">$1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: right;">target_entropy</td>
<td style="text-align: left;">auto</td>
</tr>
<tr>
<td style="text-align: right;">learning_rate</td>
<td style="text-align: left;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: right;">buffer_size</td>
<td style="text-align: left;">1000000</td>
</tr>
<tr>
<td style="text-align: right;">learning_starts</td>
<td style="text-align: left;">1000</td>
</tr>
<tr>
<td style="text-align: right;">batch_size</td>
<td style="text-align: left;">256</td>
</tr>
</tbody>
</table>
<p>Table 3: Training parameters for each task used to evaluate CausalCF.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: left;">Pushing \&amp; Picking</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Total time steps</td>
<td style="text-align: left;">7000000</td>
</tr>
<tr>
<td style="text-align: right;">$\quad$ Training time</td>
<td style="text-align: left;">$\approx 22$ hours</td>
</tr>
<tr>
<td style="text-align: right;">Episode length</td>
<td style="text-align: left;">834</td>
</tr>
<tr>
<td style="text-align: right;">Number of episodes</td>
<td style="text-align: left;">8393</td>
</tr>
<tr>
<td style="text-align: right;">$\quad$ Skipframe</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: right;">Space</td>
<td style="text-align: left;">A</td>
</tr>
<tr>
<td style="text-align: right;">Checkpoint frequency</td>
<td style="text-align: left;">500000</td>
</tr>
</tbody>
</table>
<h3>5.2 Evaluation Stages</h3>
<p>There are two different types of evaluation performed on CausalCF: component testing and causalRep transfer.</p>
<p>Component testing evaluates the contributions of the different components of the CausalCF design to the robustness of the RL agent. We compare CausalCF (iter) (CausalCF) with 3 baselines: Counterfactual + Intervene (CausalCF without iterating between Counterfactual and Agent training), Intervene (uses interventions but no counterfactuals), and no_Intervene (uses no interventions and no counterfactuals, the SAC baseline). The different RL models are passed through an evaluation pipeline that CausalWorld provides to measure generalizability.</p>
<p>We demonstrate the transferability of the causal representation across different tasks (and therefore increased robustness) in causalRep transfer. The causal representation learnt by CausalCF in the Pushing task is transferred to train a Causal RL agent in the Picking task. The performance of transfer Causal_rep + Intervene (agent that used the transferred causal representation) is compared against Intervene (agent does not use a causal representation).</p>
<p>The training environment for no_Intervene is the same as P0 (Table 1), where no interventions on the CausalWorld variables are used. The training environment for all the other baselines is the same as P5, where interventions were made on the goal pose (Section 4.2) in Space A (Table 3). All the other protocols will measure the out-of-distribution robustness of the baselines. For example, P6 modifies the objects' block pose, and goal pose with the ranges of values provided in Space B (Section 3). The result of the interventions in P6 is that the agents will have to perform new grasps on the block and encounter new goal poses that are not seen in training.</p>
<h3>5.3 Component Testing</h3>
<p>The training performance of the different RL solutions in the Pushing task is provided in Figure 4a. Some interventions might lead to unsolvable scenarios, and the agent will receive a low fractional success for these rare scenarios. These scenarios can happen in the later stages of training. The mean fractional success is used to produce a smoother plot of the training performance.</p>
<p>No interventions are used to train the no_Intervene agent. Therefore, the environment for the no_Intervene agent is more repetitive, and the agent was able to converge to a good training performance faster than all the other solutions. The no_Intervene agent converged to a fractional success of higher than 0.9 in about 1.5 million time steps (Figure 4a). In contrast to the training environment used for no_Intervene, all the environments used for the other RL solutions intervened on the goal shape at every episode. Interventions make the tasks more challenging. The Intervene solution could only converge to an approximate fractional success of 0.8 in 7 million time steps (Figure 4a). CausalCF outperformed Intervene in the same training environment and converged to the same performance as the no_Intervene solution (Figure 4a). It took CausalCF longer to converge than no_Intervene because the training environment for CausalCF is more challenging. CausalCF (iter) took longer to converge than Counterfactual + Intervene (Figure 4a). The variations to the causal representation during the iteration require the RL agent to partially re-learn and converge more slowly. However, CausalCF (iter) should be more robust than all the other solutions trained. The robustness of the RL solutions is measured through the evaluation pipeline provided by CausalWorld.</p>
<p>The evaluation pipeline provided by CausalWorld consists of 12 protocols as described in Table 1 [1]. The performance of the different RL solutions in the evaluation pipeline is provided in Figure 4b. All the RL solutions performed better for protocols that used space A than B (Figure 4b) because they were all trained in space A. Interventions on different causal variables create different challenges. All the solutions performed worse on the protocols that intervened on the block pose (Figure 4b). The change in block pose requires the agent to consider the initial grasp on the block, which the agent did not consider as much in training. Intervene generalized better than no_Intervene as interventions on one or a subset of causal variables are better than no intervention [3, 14]. CausalCF was more robust than all the other solutions as the agents used counterfactuals to capture more causal information about the underlying SCM. CausalCF (iter) performed equal to or better than the other solutions in 10 out of the 12 protocols and is the most robust (Figure 4b). CausalCF (iter) allows the agent to update its causal representation further as the agent gets better.</p>
<h3>5.4 CausalRep Transfer</h3>
<p>The tasks in CausalWorld have similar causal structures. The learnt causal representation of the CausalCF (iter) solution is transferred from the Pushing task to train a new RL agent in the Picking task. Two solutions are compared, and the training performance is shown in Figure 5a. Both solutions used the same interventions. Intervention is made on the goal shape of the block at every episode. The transfer Causal_rep + Intervene solution converged to a fractional</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Training performance. The x -axis is the training time steps, and the y -axis is the mean of the fractional successes for every 100 episodes.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Evaluation performance. The x-axis shows the 12 protocols (Table 1), and the y-axis is the full integrated fractional success [1] which is the mean fractional success over all 200 episodes for each evaluation protocol.</p>
<p>Figure 4: Results for Component Testing in the Pushing task.
success of higher than 0.9 (Figure 5a). The Intervene solution converged to a fractional success of about 0.8 (Figure 5a). transfer Causal_rep + Intervene converged to a better training performance than Intervene. The causal representation provides additional prior causal knowledge for the RL agent.</p>
<p>Figure 5b shows the performance of the two RL solutions in the evaluation pipeline. transfer Causal_rep + Intervene performed equal to or better than Intervene in 8 out of the 12 evaluation protocols (Figure 5b). The causal representation learnt from the Pushing task helps the agent perform better and become more robust in the Picking task. This result confirms that the causal representation captures causal mechanisms through interventions and counterfactuals. transfer Causal_rep + Intervene could have achieved better evaluation performance if the agent could update the causal representation with new causal information in the Picking task.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Training performance
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Evaluation performance</p>
<p>Figure 5: Results for causalRep transfer in the Picking task. The 12 protocols applied are the same as it is described in Table 1</p>
<h2>6 CONCLUSIONS AND FUTURE WORK</h2>
<p>In this work, we have presented CausalCF, the first complete Causal RL solution that successfully tackles complex RL robotic tasks. We have shown that CausalCF improves the robustness of RL and that none of the design components of CausalCF are redundant in component testing. The results from causalRep transfer show that the causal representation captures the causal mechanisms that remain invariant across tasks. CausalCF improves the robustness of RL and provides mechanisms to help improve the explainability of RL. Future work includes Multitask RL, where the skills obtained from multiple simpler tasks are applied to more challenging tasks. We also hope to evaluate CausalCF in sim2real transfer (mentioned in Section 3).</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was funded in part by the SFI-NSFC Partnership Programme Grant Number 17/NSFC/5224 and Science Foundation Ireland Grant number 18/CRT/6223.</p>
<h2>REFERENCES</h2>
<p>[1] Ossama Ahmed, Frederik Trauble, Anirudh Goyal, Alexander Neitz, Manuel Wuthrich, Yoshua Bengio, Bernhard Schlkopf, and Stefan Bauer. 2021. CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. ArXiv abs/2010.04296 (2021).
[2] Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. 2020. COPHY: Counterfactual Learning of Physical Dynamics. ArXiv abs/1909.12000 (2020).
[3] E Bareinboim. 2020. Towards Causal Reinforcement Learning (CRL). ONLINE. (https://crl.causalai.net/crl-icml20.pdf), Accessed 21 Jan. 2022.
[4] E. Bareinboim, J. Correa, D. Ibeling, and T. Icard. 2020. On Pearl's Hierarchy and the Foundations of Causal Inference. Technical Report R-60. Causal Artificial Intelligence Lab, Columbia University. In: "Probabilistic and Causal Inference: The Works of Judea Pearl", ACM Books, in press.
[5] Francisco Cruz, Richard Dazeley, Peter Vamplew, and Ithan Moreira. 2021. Explainable robotic systems: understanding goal-driven actions in a reinforcement learning scenario. Neural Computing and Applications (2021).
[6] Ishita Dasgupta, Jane X. Wang, Silvia Chiappa, Jovana Mitrovic, Pedro A. Ortega, David Raposo, Edward Hughes, Peter W. Battaglia, Matthew M. Botvinick, and Zeb Kurth-Nelson. 2019. Causal Reasoning from Meta-reinforcement Learning. ArXiv abs/1901.08162 (2019).
[7] Richard Dazeley, Peter Vamplew, and Francisco Cruz. 2021. Explainable reinforcement learning for Broad-XAI: a conceptual framework and survey. arXiv preprint arXiv:2108.09003 (2021).
[8] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org.
[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning. PMLR, 18611870.
[10] Joseph Y Halpern and Judea Pearl. 2020. Causes and explanations: A structuralmodel approach. Part I: Causes. The British journal for the philosophy of science (2020).
[11] Ammar Haydari and Yasin Ylmaz. 2022. Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey. IEEE Transactions on Intelligent</p>
<p>Transportation Systems 23, 1 (2022), 11-32. https://doi.org/10.1109/TITS.2020. 3008612
[12] Jun Jin, Daniel Graves, Cameron Haigh, Jun Luo, and Martin Jgersand. 2020. Offline Learning of Counterfactual Perception as Prediction for Real-World Robotic Reinforcement Learning. ArXiv abs/2011.05857 (2020).
[13] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, H. Larochelle, Chris Pal, and Yoshua Bengio. 2019. Learning Neural Causal Models from Unknown Interventions. ArXiv abs/1910.01075 (2019).
[14] S. Lee and E. Bareinboim. 2018. Structural Causal Bandits: Where to Intervene?. In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Curran Associates, Inc., Montreal, Canada, 2568-2578.
[15] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. 2020. Explainable Reinforcement Learning Through a Causal Lens. ArXiv abs/1905.10958 (2020).
[16] Judea Pearl et al. 2000. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress 19 (2000).
[17] Erika Puiutta and Eric M. S. P. Veith. 2020. Explainable Reinforcement Learning: A Survey. ArXiv abs/2005.06247 (2020).
[18] Bernhard Schlkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. 2021. Toward Causal Representation Learning. Proc. IEEE 109 (2021), 612-634.
[19] Sumedh Anand Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Schlkopf. 2021. Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. In ICML.
[20] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.
[21] Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. 2021. Reinforcement Learning in Healthcare: A Survey. 55, 1 (2021). https://doi.org/10.1145/3477600
[22] Wenshuai Zhao, Jorge Pea Queralta, and Tomi Westerlund. 2020. Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI). 737-744. https://doi.org/ 10.1109/SSCI47803.2020.9308468
[23] Shengyu Zhu and Zhitang Chen. 2020. Causal Discovery with Reinforcement Learning. ArXiv abs/1906.04477 (2020).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023), A. Ricci, W. Yeoh, N. Agmon, B. An (eds.), May 29 - June 2, 2023, London, United Kingdom. (c) 2023 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-b3bba15f000000a6d3b5808f798a9fe7629fa499</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b3bba15f000000a6d3b5808f798a9fe7629fa499" target="_blank">Generalized Planning in PDDL Domains with Pretrained Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work investigates whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain and finds that GPT-4 is a surprisingly powerful generalized planner.</p>
                <p><strong>Paper Abstract:</strong> Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e973.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e973.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-GenPlan (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalized Planning in PDDL Domains with Pretrained Large Language Models (GPT-4 pipeline introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that prompts GPT-4 to (1) summarize a PDDL domain (CoT), (2) propose a simple non-search strategy, and (3) implement that strategy as a Python get_plan(objects, init, goal) program, with automated interactive debugging using four feedback types and VAL plan validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 generalized planner (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline: (a) Domain summarization prompt (CoT) using two training tasks; (b) Strategy proposal prompt explicitly forbidding search; (c) Strategy implementation prompt to synthesize a Python function get_plan(objects, init, goal) that returns a list of ground actions; (d) Automated interactive debugging loop (up to 4 iterations) using four feedback types (Python exceptions, timeouts, plan syntax checks, plan semantics via VAL); (e) Evaluation on held-out PDDL tasks. The synthesized programs may use straightforward procedural algorithms (loops, sorting, greedy heuristics) rather than classical search, and helper functions produced across debugging iterations are appended to a cumulative Python file so fixes build on prior responses.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (STRIPS subset, deterministic)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States: fully-observed sets of ground atoms (conjunction true atoms, rest false). Actions: ground operators corresponding to PDDL operators with preconditions and deterministic add/delete effects. Transitions: deterministic application of operator effects to states; no probabilistic transitions modeled in the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Domain summarization, strategy proposal, program (policy) synthesis (code generation), and automated program debugging/repair based on execution traces and plan validation feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Synthesized domain-specific procedural programs (heuristic / deterministic strategies implemented in Python); explicitly asked to avoid search-based planners, though LLM could still produce search-like code.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fraction of evaluation tasks solved (success rate within 30s) per domain; runtime comparisons vs a classical planner for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 per-domain success rates (median over seeds): Delivery 0.90, Forest 1.00, Gripper 0.90, Miconic 0.01, Ferry 0.80, Spanner 0.10, Heavy 0.60; synthesized programs also tend to run faster than Fast Downward/LAMA on these domains (see Fig.2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to ablations (No CoT, No Debug, No Names, GPT-3.5) and baselines (PG3, Policy Eval, Plan Compare, Random). Example: PG3 often achieves 1.00 success on many domains in Table 1, while GPT-4 pipeline matches or outperforms baselines on several domains (e.g., Delivery, Forest, Gripper, Ferry, Heavy).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablations show automated debugging is critical (No Debug dramatically lowers success), PDDL names strongly help (No Names fails), CoT has mixed effects (helps on some domains, hurts on others), GPT-3.5 performs much worse than GPT-4. No explicit ablation of probabilistic or belief-state uncertainty modeling was reported because the world model is deterministic PDDL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This work uses deterministic PDDL world models and does not explicitly model probabilistic transitions or LLM prediction uncertainty; instead, it handles LLM errors pragmatically via automated debugging and plan validation. The LLM can synthesize compact, efficient generalized planners (Python programs) from very few training tasks, but uncertainty from LLM outputs is handled via iterative repair rather than probabilistic belief-state integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Planning in PDDL Domains with Pretrained Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e973.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e973.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PG3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PG3: Policy-Guided Planning for Generalized Policy Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalized planning approach that synthesizes lifted decision-list, goal-conditioned policies via heuristic search in policy space and uses candidate generalized plans to constrain generation of example plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PG3: Policy-Guided Planning for Generalized Policy Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PG3 (Yang et al., 2022) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Search-based synthesis of generalized policies represented as lifted decision lists (goal-conditioned policies). Uses heuristic-guided search in policy space and relies on planners to generate example plans; policies are symbolic and operate over PDDL representations.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (deterministic, symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Uses PDDL domain descriptions and classical planner infrastructure to generate example plans and evaluate candidate lifted policies; states/actions represented symbolically without probabilistic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Heuristic search in policy (lifted decision list) space; relies on classical planning to generate example plans.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fraction of evaluation tasks solved (success rate) per domain</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 baseline results: PG3 often achieves 1.00 on many domains in this evaluation (e.g., Delivery 1.00, Forest 1.00, Gripper 1.00, Miconic 1.00, Ferry 1.00, Spanner 1.00, Heavy 0.00 in Table 1 averaged over seeds), used as a principal comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against GPT-4 pipeline and other baselines; PG3 typically strong on many domains but fails in Heavy where lifted decision lists are too restrictive.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not part of this paper's ablation but used as a baseline; PG3 does not model probabilistic uncertainty or LLM output uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PG3 is a strong classical generalized planning baseline in these PDDL domains but cannot represent some concepts (e.g., global 'heaviest' aggregation) that GPT-4-synthesized general Python programs can implement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Planning in PDDL Domains with Pretrained Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e973.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e973.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fast Downward / LAMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast Downward planner using the LAMA configuration (landmarks); domain-independent classical planner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art domain-independent PDDL planner (Fast Downward) with the LAMA heuristic/configuration used as a runtime baseline and for planner performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The LAMA planner: Guiding Cost-based Anytime Planning with Landmarks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fast Downward + LAMA baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Classical PDDL planner (Fast Downward) run with LAMA configuration/heuristics; used to produce reference runtimes and plans for comparison with synthesized programs. Stops after the first plan is found.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (deterministic, symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Classical grounding-based representation: states are grounded sets of atoms; actions are grounded operators; transitions deterministic; heavy use of operator grounding, heuristics and search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Classical heuristic search (Fast Downward search algorithms with LAMA heuristics, landmark guidance, anytime behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Wall-clock runtime to first plan, scalability vs number of objects</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used as baseline in Fig.2: synthesized LLM programs often run faster and scale more favorably than Fast Downward/LAMA on the evaluated domains (no single numeric aggregate provided in text; per-domain runtime plots in Fig.2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to the GPT-4 synthesized programs: LLM-generated programs typically outperformed Fast Downward/LAMA in runtime, largely because they avoid grounding overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Classical planners can be outperformed in wall-clock runtime by task-specific synthesized programs because the latter can avoid operator grounding and implement direct procedural strategies; uncertainty is not modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Planning in PDDL Domains with Pretrained Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e973.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e973.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VAL: Automatic plan validation (Howey, Long, and Fox, 2004)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plan validation tool for PDDL that checks plan semantics and can provide plan repair advice (unsatisfied preconditions, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>VAL plan validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used to verify whether plans returned by synthesized get_plan are semantically valid for the PDDL task; when invalid, VAL returns diagnostic 'plan repair advice' which the pipeline sends back to the LLM as feedback for program repair.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL plan semantics checker (deterministic)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Interprets PDDL tasks and checks a proposed plan step-by-step to determine whether action preconditions are met and effects yield the goal; deterministic validation, not probabilistic.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>N/A (validation tool rather than planner)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not applicable; used to produce semantic diagnostics guiding LLM debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VAL is used to provide structured semantic feedback to the LLM so that the synthesized generalized program can be corrected; it is a deterministic validator and is not combined with probabilistic belief-state reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Planning in PDDL Domains with Pretrained Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e973.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e973.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+P (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed idea to give LLMs access to a planner API so the LLM can leverage classical planning capabilities during task solving; cited as a promising direction for combining LLMs with planners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM+P (referenced idea/work, Liu et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an idea where LLMs are given access to external planners/APIs to augment their planning capability; the current paper suggests this could be useful for generalized planning and notes caveats (e.g., example plans might confuse the LLM if they follow irrelevant paths). The paper does not implement LLM+P.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL/classical planner interfaces implied (deterministic); exact representation depends on the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Not implemented in this paper; referenced idea suggests using classical symbolic planners (PDDL) as a tool accessible to LLMs to generate or verify plans. No probabilistic world model integration described here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Not applicable in this paper (mentioned as external work/idea); in referenced work the LLM would orchestrate planner use or incorporate planner outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Classical planning APIs (implied); integration method varies by referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper suggests giving LLMs access to planners is promising but cautions about naive use of example plans; no explicit probabilistic integration of LLM uncertainty is described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Planning in PDDL Domains with Pretrained Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e973.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e973.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corrective Re-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning with Large Language Models via Corrective Re-prompting (Raman et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that iteratively re-prompts an LLM with feedback derived from execution failures to correct planning outputs; cited as inspiration for the automated debugging loop in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning with Large Language Models via Corrective Re-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Corrective Re-prompting (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced method where LLM-produced plans/outputs are executed or checked and then corrective feedback (from environment or validators) is used to re-prompt the LLM to repair outputs; inspired the paper's automated debugging loop with Python exceptions, timeouts, syntax and semantic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>Plan generation and repair via iterative re-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>LLM-based plan generation with iterative repair (no explicit probabilistic planning described here)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper adopts a similar automated debugging paradigm but does not convert this into an explicit probabilistic model of LLM uncertainty or belief states; instead, it uses deterministic diagnostics and iterative repair.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalized Planning in PDDL Domains with Pretrained Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PDDL Planning with Pretrained Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models Still Canâ€™t Plan (A Benchmark for LLMs on Planning and Reasoning about Change) <em>(Rating: 2)</em></li>
                <li>LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency <em>(Rating: 2)</em></li>
                <li>Planning with Large Language Models via Corrective Re-prompting <em>(Rating: 2)</em></li>
                <li>Inner Monologue: Embodied Reasoning through Planning with Language Models <em>(Rating: 1)</em></li>
                <li>Plansformer: Generating Symbolic Plans using Transformers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-973",
    "paper_id": "paper-b3bba15f000000a6d3b5808f798a9fe7629fa499",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "LLM-GenPlan (GPT-4)",
            "name_full": "Generalized Planning in PDDL Domains with Pretrained Large Language Models (GPT-4 pipeline introduced in this paper)",
            "brief_description": "A pipeline that prompts GPT-4 to (1) summarize a PDDL domain (CoT), (2) propose a simple non-search strategy, and (3) implement that strategy as a Python get_plan(objects, init, goal) program, with automated interactive debugging using four feedback types and VAL plan validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPT-4 generalized planner (this paper)",
            "system_description": "Pipeline: (a) Domain summarization prompt (CoT) using two training tasks; (b) Strategy proposal prompt explicitly forbidding search; (c) Strategy implementation prompt to synthesize a Python function get_plan(objects, init, goal) that returns a list of ground actions; (d) Automated interactive debugging loop (up to 4 iterations) using four feedback types (Python exceptions, timeouts, plan syntax checks, plan semantics via VAL); (e) Evaluation on held-out PDDL tasks. The synthesized programs may use straightforward procedural algorithms (loops, sorting, greedy heuristics) rather than classical search, and helper functions produced across debugging iterations are appended to a cumulative Python file so fixes build on prior responses.",
            "world_model_type": "PDDL (STRIPS subset, deterministic)",
            "world_model_description": "States: fully-observed sets of ground atoms (conjunction true atoms, rest false). Actions: ground operators corresponding to PDDL operators with preconditions and deterministic add/delete effects. Transitions: deterministic application of operator effects to states; no probabilistic transitions modeled in the world model.",
            "uses_llm": true,
            "llm_role": "Domain summarization, strategy proposal, program (policy) synthesis (code generation), and automated program debugging/repair based on execution traces and plan validation feedback.",
            "llm_model_name": "GPT-4",
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Synthesized domain-specific procedural programs (heuristic / deterministic strategies implemented in Python); explicitly asked to avoid search-based planners, though LLM could still produce search-like code.",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Fraction of evaluation tasks solved (success rate within 30s) per domain; runtime comparisons vs a classical planner for scalability.",
            "performance_value": "Table 1 per-domain success rates (median over seeds): Delivery 0.90, Forest 1.00, Gripper 0.90, Miconic 0.01, Ferry 0.80, Spanner 0.10, Heavy 0.60; synthesized programs also tend to run faster than Fast Downward/LAMA on these domains (see Fig.2).",
            "baseline_comparison": "Compared to ablations (No CoT, No Debug, No Names, GPT-3.5) and baselines (PG3, Policy Eval, Plan Compare, Random). Example: PG3 often achieves 1.00 success on many domains in Table 1, while GPT-4 pipeline matches or outperforms baselines on several domains (e.g., Delivery, Forest, Gripper, Ferry, Heavy).",
            "has_ablation_uncertainty": false,
            "ablation_results": "Ablations show automated debugging is critical (No Debug dramatically lowers success), PDDL names strongly help (No Names fails), CoT has mixed effects (helps on some domains, hurts on others), GPT-3.5 performs much worse than GPT-4. No explicit ablation of probabilistic or belief-state uncertainty modeling was reported because the world model is deterministic PDDL.",
            "key_findings": "This work uses deterministic PDDL world models and does not explicitly model probabilistic transitions or LLM prediction uncertainty; instead, it handles LLM errors pragmatically via automated debugging and plan validation. The LLM can synthesize compact, efficient generalized planners (Python programs) from very few training tasks, but uncertainty from LLM outputs is handled via iterative repair rather than probabilistic belief-state integration.",
            "uuid": "e973.0",
            "source_info": {
                "paper_title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PG3",
            "name_full": "PG3: Policy-Guided Planning for Generalized Policy Generation",
            "brief_description": "A generalized planning approach that synthesizes lifted decision-list, goal-conditioned policies via heuristic search in policy space and uses candidate generalized plans to constrain generation of example plans.",
            "citation_title": "PG3: Policy-Guided Planning for Generalized Policy Generation",
            "mention_or_use": "use",
            "system_name": "PG3 (Yang et al., 2022) baseline",
            "system_description": "Search-based synthesis of generalized policies represented as lifted decision lists (goal-conditioned policies). Uses heuristic-guided search in policy space and relies on planners to generate example plans; policies are symbolic and operate over PDDL representations.",
            "world_model_type": "PDDL (deterministic, symbolic)",
            "world_model_description": "Uses PDDL domain descriptions and classical planner infrastructure to generate example plans and evaluate candidate lifted policies; states/actions represented symbolically without probabilistic transitions.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Heuristic search in policy (lifted decision list) space; relies on classical planning to generate example plans.",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Fraction of evaluation tasks solved (success rate) per domain",
            "performance_value": "Table 1 baseline results: PG3 often achieves 1.00 on many domains in this evaluation (e.g., Delivery 1.00, Forest 1.00, Gripper 1.00, Miconic 1.00, Ferry 1.00, Spanner 1.00, Heavy 0.00 in Table 1 averaged over seeds), used as a principal comparison.",
            "baseline_comparison": "Compared against GPT-4 pipeline and other baselines; PG3 typically strong on many domains but fails in Heavy where lifted decision lists are too restrictive.",
            "has_ablation_uncertainty": false,
            "ablation_results": "Not part of this paper's ablation but used as a baseline; PG3 does not model probabilistic uncertainty or LLM output uncertainty.",
            "key_findings": "PG3 is a strong classical generalized planning baseline in these PDDL domains but cannot represent some concepts (e.g., global 'heaviest' aggregation) that GPT-4-synthesized general Python programs can implement.",
            "uuid": "e973.1",
            "source_info": {
                "paper_title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Fast Downward / LAMA",
            "name_full": "Fast Downward planner using the LAMA configuration (landmarks); domain-independent classical planner",
            "brief_description": "A state-of-the-art domain-independent PDDL planner (Fast Downward) with the LAMA heuristic/configuration used as a runtime baseline and for planner performance comparisons.",
            "citation_title": "The LAMA planner: Guiding Cost-based Anytime Planning with Landmarks",
            "mention_or_use": "use",
            "system_name": "Fast Downward + LAMA baseline",
            "system_description": "Classical PDDL planner (Fast Downward) run with LAMA configuration/heuristics; used to produce reference runtimes and plans for comparison with synthesized programs. Stops after the first plan is found.",
            "world_model_type": "PDDL (deterministic, symbolic)",
            "world_model_description": "Classical grounding-based representation: states are grounded sets of atoms; actions are grounded operators; transitions deterministic; heavy use of operator grounding, heuristics and search.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Classical heuristic search (Fast Downward search algorithms with LAMA heuristics, landmark guidance, anytime behavior).",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Wall-clock runtime to first plan, scalability vs number of objects",
            "performance_value": "Used as baseline in Fig.2: synthesized LLM programs often run faster and scale more favorably than Fast Downward/LAMA on the evaluated domains (no single numeric aggregate provided in text; per-domain runtime plots in Fig.2).",
            "baseline_comparison": "Compared to the GPT-4 synthesized programs: LLM-generated programs typically outperformed Fast Downward/LAMA in runtime, largely because they avoid grounding overhead.",
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "Classical planners can be outperformed in wall-clock runtime by task-specific synthesized programs because the latter can avoid operator grounding and implement direct procedural strategies; uncertainty is not modeled.",
            "uuid": "e973.2",
            "source_info": {
                "paper_title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "VAL",
            "name_full": "VAL: Automatic plan validation (Howey, Long, and Fox, 2004)",
            "brief_description": "A plan validation tool for PDDL that checks plan semantics and can provide plan repair advice (unsatisfied preconditions, etc.).",
            "citation_title": "VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL",
            "mention_or_use": "use",
            "system_name": "VAL plan validator",
            "system_description": "Used to verify whether plans returned by synthesized get_plan are semantically valid for the PDDL task; when invalid, VAL returns diagnostic 'plan repair advice' which the pipeline sends back to the LLM as feedback for program repair.",
            "world_model_type": "PDDL plan semantics checker (deterministic)",
            "world_model_description": "Interprets PDDL tasks and checks a proposed plan step-by-step to determine whether action preconditions are met and effects yield the goal; deterministic validation, not probabilistic.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "N/A (validation tool rather than planner)",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Not applicable; used to produce semantic diagnostics guiding LLM debugging.",
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": false,
            "ablation_results": null,
            "key_findings": "VAL is used to provide structured semantic feedback to the LLM so that the synthesized generalized program can be corrected; it is a deterministic validator and is not combined with probabilistic belief-state reasoning.",
            "uuid": "e973.3",
            "source_info": {
                "paper_title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM+P (mentioned)",
            "name_full": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
            "brief_description": "A recently proposed idea to give LLMs access to a planner API so the LLM can leverage classical planning capabilities during task solving; cited as a promising direction for combining LLMs with planners.",
            "citation_title": "LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency",
            "mention_or_use": "mention",
            "system_name": "LLM+P (referenced idea/work, Liu et al. 2023)",
            "system_description": "Referenced as an idea where LLMs are given access to external planners/APIs to augment their planning capability; the current paper suggests this could be useful for generalized planning and notes caveats (e.g., example plans might confuse the LLM if they follow irrelevant paths). The paper does not implement LLM+P.",
            "world_model_type": "PDDL/classical planner interfaces implied (deterministic); exact representation depends on the referenced work.",
            "world_model_description": "Not implemented in this paper; referenced idea suggests using classical symbolic planners (PDDL) as a tool accessible to LLMs to generate or verify plans. No probabilistic world model integration described here.",
            "uses_llm": null,
            "llm_role": "Not applicable in this paper (mentioned as external work/idea); in referenced work the LLM would orchestrate planner use or incorporate planner outputs.",
            "llm_model_name": null,
            "uncertainty_modeling": null,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Classical planning APIs (implied); integration method varies by referenced work.",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Paper suggests giving LLMs access to planners is promising but cautions about naive use of example plans; no explicit probabilistic integration of LLM uncertainty is described in this paper.",
            "uuid": "e973.4",
            "source_info": {
                "paper_title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Corrective Re-prompting",
            "name_full": "Planning with Large Language Models via Corrective Re-prompting (Raman et al., 2022)",
            "brief_description": "An approach that iteratively re-prompts an LLM with feedback derived from execution failures to correct planning outputs; cited as inspiration for the automated debugging loop in this paper.",
            "citation_title": "Planning with Large Language Models via Corrective Re-prompting",
            "mention_or_use": "mention",
            "system_name": "Corrective Re-prompting (referenced)",
            "system_description": "Referenced method where LLM-produced plans/outputs are executed or checked and then corrective feedback (from environment or validators) is used to re-prompt the LLM to repair outputs; inspired the paper's automated debugging loop with Python exceptions, timeouts, syntax and semantic checks.",
            "world_model_type": null,
            "world_model_description": null,
            "uses_llm": true,
            "llm_role": "Plan generation and repair via iterative re-prompting",
            "llm_model_name": null,
            "uncertainty_modeling": null,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "LLM-based plan generation with iterative repair (no explicit probabilistic planning described here)",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "The paper adopts a similar automated debugging paradigm but does not convert this into an explicit probabilistic model of LLM uncertainty or belief states; instead, it uses deterministic diagnostics and iterative repair.",
            "uuid": "e973.5",
            "source_info": {
                "paper_title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PDDL Planning with Pretrained Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models Still Canâ€™t Plan (A Benchmark for LLMs on Planning and Reasoning about Change)",
            "rating": 2
        },
        {
            "paper_title": "LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency",
            "rating": 2
        },
        {
            "paper_title": "Planning with Large Language Models via Corrective Re-prompting",
            "rating": 2
        },
        {
            "paper_title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "rating": 1
        },
        {
            "paper_title": "Plansformer: Generating Symbolic Plans using Transformers",
            "rating": 2
        }
    ],
    "cost": 0.015925250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generalized Planning in PDDL Domains with Pretrained Large Language Models</h1>
<p>Tom Silver ${ }^{1}$, Soham Dan ${ }^{2}$, Kavitha Srinivas ${ }^{2}$, Joshua Tenenbaum ${ }^{1}$, Leslie Kaelbling ${ }^{1}$, Michael Katz ${ }^{2}$<br>${ }^{1}$ MIT Computer Science and Artificial Intelligence Laboratory; ${ }^{2}$ IBM Research<br>Correspondence: tslvr@mit.edu, Michael.Katz1@ibm.com</p>
<h4>Abstract</h4>
<p>Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT3.5, and that just two training tasks are often sufficient for strong generalization. ${ }^{1}$</p>
<h2>Introduction</h2>
<p>While some classes of sequential decision-making tasks are provably intractable (Chapman 1987), others can be solved efficiently with a single domain-specific program. In the latter case, there is considerable interest in automatically synthesizing these programs given a small number of training tasks. In AI planning, several approaches to this generalized planning problem have been proposed, with programs expressed as lifted decision lists, as finite state machines, or in domain-specific languages (Srivastava 2011; Bonet and Geffner 2015; JimÃ©nez, Segovia-Aguas, and Jonsson 2019; Rivlin, Hazan, and Karpas 2020). In reinforcement learning, goal-conditioned policies and value functions can be understood as particular kinds of programs learned with the same generalized planning objective (Sutton et al. 2011; Schaul et al. 2015). Despite these efforts, it remains challenging to efficiently synthesize programs from few training tasks that generalize to a wide variety of held-out tasks.</p>
<p>Given the tremendous recent progress in large language models (LLMs) (Brown et al. 2020; Chen et al. 2021;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Chowdhery et al. 2022), especially in code generation (Chen et al. 2021; Nijkamp et al. 2023; Chen et al. 2023a), this work asks a simple question: can pretrained LLMs be used for generalized planning? In particular, we investigate whether GPT-4 (OpenAI 2023) can be used to write a domain-specific Python program that solves a set of tasks in a planning domain. For each domain, we prompt GPT-4 with the domain and a small number of training tasks encoded in the Planning Domain Definition Language (PDDL) (McDermott 2000). We then ask GPT-4 to write a program that consumes a (parsed) task description and outputs a plan. To prevent it from writing domain-general search-based code-a natural inclination given the association between PDDL and search in its pretraining data-we instruct GPT-4 to implement "a simple strategy that does not use search."</p>
<p>Beyond this basic protocol, we consider two extensions. First, inspired by Chain-of-Thought (CoT) (Wei et al. 2022; Jiang et al. 2023), we prompt GPT-4 to write a natural language summary of the PDDL domain. We then ask it to describe a solution strategy before finally implementing the strategy in Python. Second, inspired by Inner Monologue (Huang et al. 2022b) and Corrective Reprompting (Raman et al. 2022), we automatically provide feedback to GPT-4 in the case where it fails to solve training tasks. For example, if executing the Python code results in an exception, we present GPT-4 with that exception and ask it to fix the code. We repeat this automated debugging process up to four times or until all training tasks are solved. See Figure 1 for an overview of this pipeline.</p>
<p>In our experiments, we evaluate this approach on seven PDDL domains: six from recent work in generalized planning (Yang et al. 2022), and a seventh novel domain. We find that the approach is a strong baseline compared to existing generalized planning approaches. This is an important finding that we expect to inform further research in generalized planning. We also present a suite of ablations and additional analyses to unpack the contributions of CoT summarization, automated debugging, names in the PDDL, and GPT-4 vs. GPT-3.5. Our results suggest that automated debugging, PDDL names, and GPT-4 are very important, while the impact of CoT is non-uniform. Finally, we provide qualitative analyses of common failure cases, suggesting directions for future work. We conclude that GPT-4 is a surprisingly powerful generalized planner when properly guided.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: Overview of pipeline for generalized planning with pretrained LLMs. See text for details.</p>
<h2>Related Work</h2>
<p>LLMs for (PDDL) Planning. Generalized planning with LLMs can be seen as an alternative to planning with LLMs <em>Sharma et al. (2022); Ahn et al. (2022); Huang et al. (2022a); Raman et al. (2022); Lin et al. (2022)</em>. Most relevant is work by <em>Valmeekam et al. (2022); Silver et al. (2022)</em> who consider LLM-based planning in PDDL domains. There are several advantages to using LLMs for generalized planning, rather than planning: (1) programs produced by the LLM can be inspected and validated; (2) running a synthesized program can be much faster (and cheaper) than querying the LLM for each new task; (3) synthesized programs can scale to arbitrarily large tasks, whereas current LLMs are limited by context window size. <em>Pallagani et al. (2022)</em> consider fine-tuning an LLM to solve PDDL tasks. Other recent work has considered using LLMs for translating between natural language and PDDL <em>Collins et al. (2022); Lin et al. (2023); Xie et al. (2023); Liu et al. (2023)</em>. These efforts could be combined with our approach.</p>
<p>Generalized Planning. This work contributes to a growing literature on generalized planning <em>Fikes et al. (1972); JimÃ©nez et al. (2023a, b)</em>. Prior work has considered synthesizing generalized plans in several ways: (1) performing a search through a hypothesis class of generalized policies <em>Levine and Humphreys (2003); JimÃ©nez and Jonsson (2015); Segovia-Aguas et al. (2021)</em>; (2) using example plans to construct a generalized plan, often represented with a finite-state machine <em>Levesque (2005); Srivastava et al. (2011); Winner (2008)</em>; and (3) discovering state and action abstractions and then using them in a generalized plan <em>Bonet and Geffner (2018)</em>. One pervasive challenge is that there are often many valid plans for any given task, and only some of these plans are consistent with a simple generalized plan. PG3 addresses this challenge by using candidate generalized plans (represented as lifted decision list goal-conditioned policies) to constrain the generation of example plans <em>Yang et al. (2022)</em>. We use PG3 as the main point of comparison in experiments.</p>
<p>LLMs for Code Generation. Our work builds on recent techniques that use LLMs for code generation <em>Chen et al. (2021); Nijkamp et al. (2023)</em>. CoT summarization is related to several techniques that ask the LLM to outline its â€œthinkingâ€ before arriving at a final implementation <em>Wei et al. (2022); Jiang et al. (2023); Zheng et al. (2023)</em>. A number of recent works also use programs as prompts (i.e., a structured chain of thought) in an attempt to help LLMs perform mathematical reasoning <em>Gao et al. (2022); Imani et al. (2023a)</em>. Related to our automated debugging, <em>Xia and Zhang (2023); Chen et al. (2023b)</em> consider automated program repair by re-prompting the LLM with feedback from failed validation checks. <em>Chen et al. (2023a)</em> consider a related paradigm, but where feedback comes from humans, rather than automated checks. Also relevant are efforts to generate code that can be used for robotic decision-making <em>Liang et al. (2022); Singh et al. (2022)</em>. Beyond LLMs, code generation has been studied extensively in program synthesis <em>Alur et al. (2013); Gulwani et al. (2017)</em> and inductive logic programming <em>Muggleton (1991); Cropper and DumanÄiÄ (2022)</em>.</p>
<h2>Background and Problem Setting</h2>
<p>PDDL Domains and Tasks. We consider deterministic, fully-observed planning tasks represented in PDDL. In experiments, we use the STRIPS subset with types and negative preconditions. We describe PDDL informally and refer the reader to other references for a formal treatment <em>McDermott (2000)</em>. A PDDL domain is characterized by a name, a set of types, a set of predicates, and a set of operators. For example, in the Delivery domain, a robot must pick up newspapers from a home base and then deliver them to certain locations. The domain has two types: loc and paper. One predicate is (at ?1 - loc), where ?1 is a placeholder for a loc object. The domain has three operators: (pick-up ?p - paper ?1 - loc), (move ?from - loc ?to - loc), (deliver ?p - paper ?1 - loc). For example, the pick-up operator in its entirety is:</p>
<div class="codehilite"><pre><span></span><code>:(action pick-up
:parameters (?p - paper ?1 - loc)
:precondition (and (at ?1)
(isHomeBase ?1)
(unpacked ?p))
:effect (and
(not (unpacked ?p))
(carrying ?p)))
</code></pre></div>

<p>A PDDL task is characterized by a domain, a set of objects, an initial state, and a goal. An object has a name and a type, e.g., paper1 - paper. A ground atom is a predicate and a tuple of objects of the appropriate types, e.g., (unpacked paper1). A state consists of a conjunction of ground atoms that are true, assuming all other ground atoms to be false. A goal is a conjunction of ground atoms that must be true in any goal state. (More general goal expressions are also possible in PDDL.) For example, in Delivery, the goal may include (satisfied loc1) and (satisfied loc2).</p>
<p>An action is an operator and a tuple of objects of the appropriate types, e.g., (pick-up paper1 loc4). The operatorâ€™s preconditions determine whether the action is ap-</p>
<p>plicable and the effects define what ground atoms would be added or deleted if the operator is executed. A plan is a finite sequence of actions. The plan is valid for a task if all actions are applicable when executed in succession from the initial state and if the final state is a goal state.</p>
<p>PDDL domains, types, predicates, operators, objects, and types often include human-readable names like the ones shown above. These names are not important for standard AI planners or previous generalized planning approaches. However, the names are very important for humans-and, we expect, for LLMs-trying to make sense of the PDDL.</p>
<p>Generalized Planning in PDDL Domains. A generalized planning instance is characterized by a PDDL domain and a distribution of tasks. A small set of training tasks (10 or fewer in experiments) from the distribution is given at training time. A set of held-out evaluation tasks-typically involving many more objects-are used to measure performance. The objective is to use the training tasks to synthesize a program that will produce valid plans for all of the evaluation tasks. We consider an evaluation task solved if the program returns a valid plan within a fixed wall-clock time budget ( 30 seconds in experiments). In other words, we are interested in satisficing, not optimal, planning, and our primary concern is the efficiency of planning itself.</p>
<h2>Generalized Planning with LLMs</h2>
<p>We are interested in the extent to which pretrained large language models (LLMs) can be used for generalized planning in PDDL domains. We assume familiarity with LLMs (Brown et al. 2020; Chen et al. 2021; Chowdhery et al. 2022; OpenAI 2023). To use LLMs for generalized planning, we need to define a protocol for prompting.</p>
<h2>Prompting Protocol</h2>
<p>Previous work on Chain-of-Thought (CoT) prompting has shown that asking an LLM to "think step by step" can improve performance in reasoning tasks (Wei et al. 2022). With these results in mind, we hypothesized that decomposing generalized planning into three stages-domain summarization, strategy proposal, and strategy implementationwould improve performance.</p>
<p>Domain Summarization. Our first prompt to the LLM is in the following form:</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Domain</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">PDDL</span><span class="w"> </span><span class="n">Domain</span><span class="p">]</span>
<span class="n">Example</span><span class="w"> </span><span class="n">problems</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">PDDL</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Tasks</span><span class="p">]</span>
<span class="n">Write</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="kt">short</span><span class="w"> </span><span class="n">summary</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">domain</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">words</span><span class="p">.</span>
</code></pre></div>

<p>To compensate for the limited context window size of transformer-based LLMs like GPT-4, we abbreviate the encoding of the training tasks in two ways. First, we always use only two training tasks, even when more are given. Second, within each training task, we limit the number of objects and initial state ground atoms shown. For each object type, if the number of objects of that type exceeds 10 , we truncate the object set and add ellipses. Similarly, for each predicate, if the number of ground atoms with that predicate exceeds 10, we truncate and add ellipses. The fact that we only need to
communicate the "gist" of the task distribution, rather than whole tasks, is another advantage of generalized planning with LLMs versus planning with LLMs.</p>
<p>Strategy Proposal. After the LLM responds to the first prompt, we ask for a generalized planning strategy:</p>
<p>There is a simple strategy for solving all problems in this domain without using search. What is that strategy?</p>
<p>In preliminary experiments, omitting the phrase "without using search" would often lead the LLM to propose a searchbased planning strategy.</p>
<p>Strategy Implementation. Finally, we ask the LLM to implement the strategy as a Python program:</p>
<div class="codehilite"><pre><span></span><code>Implement the strategy as a Python function. The code should
be of the form
def get_plan(objects, init, goal):
    # Your code here
    return plan
where
</code></pre></div>

<ul>
<li>object s is a set of (object name, type name) tuples</li>
<li>init is a set of ground atoms represented as tuples of predicate names and arguments (e.g., ('predicate-foo', 'objectbar', ...))</li>
<li>goal is also a set of ground atoms represented in the same way</li>
<li>plan is a list of actions, where each action is a ground operator represented as a string (e.g., 'operator-baz object-qux ...)')</li>
</ul>
<p>In domains without object types, object s is instead just a set of object names.</p>
<h2>Automated Interactive Debugging</h2>
<p>After the LLM has proposed an implementation of get_plan, we use the training tasks to validate the implementation. For each training task, we execute get_plan until it returns an output, throws an exception, or reaches a timeout ( 30 seconds). If the output is a valid plan, we continue onto the next training task. Otherwise, we re-prompt with one of four types of feedback.</p>
<p>Python Exceptions. If executing get_plan results in a Python exception, we capture the traceback and report it to the LLM along with the input. An example is shown below, with the traceback abbreviated for clarity.</p>
<div class="codehilite"><pre><span></span><code>Given this task: [PDDL Training Task]
The code raised the following exception:
File &quot;&lt;file-name-omitted&gt;&quot;, line 86
lift_at = {atom[1]: atom[2] ...}
IndexError: tuple index out of range
</code></pre></div>

<p>Fix the code.</p>
<p>In preliminary experiments, we found that including the full traceback can improve performance.</p>
<p>Timeout. If get_plan does not finish before the timeout, we report to the LLM that the program did not finish and suggest that an infinite loop may be to blame. We also provide a traceback showing where the program was executing when it was interrupted. An example is shown below.</p>
<div class="codehilite"><pre><span></span><code>Given this task: [PDDL Training Task]
The code raised the following exception:
File &quot;&lt;file-name-omitted&gt;&quot;, line 23
while not any(span_loc[1] == ...:
KeyboardInterrupt
</code></pre></div>

<p>The code was interrupted because it timed out (possible infinite loop).
Fix the code.</p>
<p>The traceback is again abbreviated for clarity. Note that the KeyboardInterrupt is automatically thrown after 30 seconds. In practice, nearly all timeouts we observe are due to logic errors in the code, rather than inefficient but correct implementations.</p>
<p>Plan Syntax. If get_plan returns an output, we check its syntax: whether it is a list of strings, whether each string is enclosed in parentheses and space-separated, and whether the action names, object names, and number of objects per action are valid with respect to the domain and task. If any of these checks fail, we report the failure to the LLM. For this type of failure, we also remind the LLM about the valid operators. An example is shown below.</p>
<div class="codehilite"><pre><span></span><code>Given this task: [PDDL Training Task]
The code returned this plan:
[&#39;walk r0_c0 r0_c1&#39;, &#39;walk ...]
However, the action walk r0_c0 r0_c1 is invalid at
step 0. NOTE: the valid operators are: (climb ?from
?to) (walk ?from ?to).
</code></pre></div>

<p>Fix the code.</p>
<p>The full plan is shown to the LLM but abbreviated in the example for clarity. The issue in this example is that the actions are not enclosed in parentheses.</p>
<p>Plan Semantics. If all of the previous checks pass, we use the VAL tool (Howey, Long, and Fox 2004) to check whether the get_plan output is a semantically valid plan. If not, VAL provides "plan repair advice", e.g., if there is an action with invalid preconditions. We extract this plan repair advice and report it to the LLM. Note that we use this advice not to repair the plan, but rather, to repair the generalized plan. An example is shown below.</p>
<div class="codehilite"><pre><span></span><code>Given this task: [PDDL Training Task]
The code failed. It returned the following plan:
[&#39;(pick-up paper-1 loc-0)&#39;, ...].
NOTE: (pick-up paper-0 loc-0) has an unsatis
</code></pre></div>

<p>if precondition at time 3
(Set (at loc-0) to true)
Fix the code.</p>
<p>Additional Details. After re-prompting the LLM, we repeat the process of checking the code and reporting any failures up to four times. To handle rare cases where the LLM implements its own helper functions and then assumes during debugging that the helper functions are still available, we append each new response from the LLM to a growing Python file, rather than overwriting the previous responses. If a failure is still encountered on the last attempt, the final response is used during evaluation.</p>
<h2>Experiments and Results</h2>
<p>Through experiments, we address these questions: 1. Can GPT-4 be used for generalized (PDDL) planning? 2. Are the synthesized programs efficient? 3. Does CoT summarization help? 4. Does automated debugging help? 5. To what extent does GPT-4 rely on names in the PDDL? 6. How does GPT4 compare to GPT-3.5? 7. Do each of the four error types help? 8. How many training tasks are needed?</p>
<h2>Experimental Setup</h2>
<p>We evaluate nine generalized planning approaches on seven PDDL domains over 10 random seeds. Tasks are randomly generated for each seed.</p>
<p>Domains. The first six domains (and tasks) are taken directly from the previous work by Yang et al. (2022). Of these, four (Gripper, Miconic, Ferry, Spanner) are standard planning benchmarks and the other two (Delivery, Forest) were introduced by that work. The last domain (Heavy) is new to this work. The pretraining data for GPT-4 is not publicly available, but it is likely that the domain definitions for at least the four standard domains were included in that data. However, we believe it is unlikely that generalized plans were included, and for the Heavy domain, we can guarantee that neither the domain nor generalized plans were included. We now briefly describe each domain. Unless otherwise specified, there are 10 training tasks and 30 evaluation tasks per domain and seed.</p>
<ul>
<li>Delivery: Newspapers at a home base must be delivered to multiple locations. There are five training tasks with 9-17 objects; evaluation tasks have 70-100 objects.</li>
<li>Forest: A hiker must navigate a 2D grid to reach a goal location while climbing hills and avoiding water. A marked trail leads to the goal, but there are shorter paths through dirt. There are 4 training tasks with 64-100 objects; evaluation tasks have 100-144 objects.</li>
<li>Gripper: Balls must be transported between rooms by a robot with two grippers. Training tasks have 20-30 objects; evaluation tasks have 60-80 objects.</li>
</ul>
<table>
<thead>
<tr>
<th>Domain</th>
<th>GPT-4</th>
<th>No CoT</th>
<th>No Debug</th>
<th>No Names</th>
<th>GPT-3.5</th>
<th>PG3</th>
<th>Policy Eval</th>
<th>Plan Compare</th>
<th>Random</th>
</tr>
</thead>
<tbody>
<tr>
<td>Delivery</td>
<td>0.90</td>
<td>0.70</td>
<td>0.10</td>
<td>0.10</td>
<td>0.00</td>
<td>1.00</td>
<td>0.00</td>
<td>0.10</td>
<td>0.00</td>
</tr>
<tr>
<td>Forest</td>
<td>1.00</td>
<td>1.00</td>
<td>0.62</td>
<td>0.11</td>
<td>0.32</td>
<td>1.00</td>
<td>1.00</td>
<td>0.16</td>
<td>0.03</td>
</tr>
<tr>
<td>Gripper</td>
<td>0.90</td>
<td>0.80</td>
<td>0.50</td>
<td>0.10</td>
<td>0.00</td>
<td>1.00</td>
<td>0.00</td>
<td>0.20</td>
<td>0.00</td>
</tr>
<tr>
<td>Miconic</td>
<td>0.01</td>
<td>0.13</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>1.00</td>
<td>0.00</td>
<td>0.10</td>
<td>0.13</td>
</tr>
<tr>
<td>Ferry</td>
<td>0.80</td>
<td>0.20</td>
<td>0.26</td>
<td>0.00</td>
<td>0.00</td>
<td>1.00</td>
<td>0.00</td>
<td>0.90</td>
<td>0.00</td>
</tr>
<tr>
<td>Spanner</td>
<td>0.10</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>1.00</td>
<td>1.00</td>
<td>0.56</td>
<td>0.06</td>
</tr>
<tr>
<td>Heavy</td>
<td>0.60</td>
<td>1.00</td>
<td>0.20</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<p>Table 1: Fraction of evaluation tasks solved. All results are averaged over 10 random seeds and 30 evaluation tasks per seed.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GPT-4 synthesized program runtime compared to a state-of-the-art planner (Fast Downward). Note the log-log axes. Each point is a median over 10 newly generated tasks, over all seeds where generalized planning solved all evaluation tasks.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Fraction of evaluation tasks solved by GPT-4 versus number of debugging steps allowed, averaged over all domains and seeds. The shaded region is standard error.</p>
<ul>
<li>Miconic: Passengers in multiple buildings, each with an elevator, must be picked up and dropped off on different floors. Training tasks have 6-30 objects; evaluation tasks have 11-150 objects.</li>
<li>Ferry: Cars must be sailed between islands using a ferry that can carry at most one car. Training tasks have 13-20 objects; evaluation tasks have 30-50 objects.</li>
<li>Spanner: Wrenches (spanners) and nuts are distributed along a one-way corridor. An agent must move down the corridor, pick up wrenches, and tighten the nuts, using each wrench at most once. Training tasks have 9-15 objects; evaluation tasks have 30-60 objects.</li>
<li>Heavy: Items must be stacked into an empty box. An item can only be stacked on another item if the latter is heavier. The weight relations are expressed via a (heavier ?x ?y) predicate. One challenge is in</li>
</ul>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>All</th>
<th>Success</th>
<th>Failure</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python Exception</td>
<td>40.0</td>
<td>28.9</td>
<td>42.5</td>
</tr>
<tr>
<td>Plan Semantics</td>
<td>34.0</td>
<td>44.7</td>
<td>31.4</td>
</tr>
<tr>
<td>Plan Syntax</td>
<td>13.0</td>
<td>18.4</td>
<td>11.7</td>
</tr>
<tr>
<td>Timeout</td>
<td>13.0</td>
<td>8.0</td>
<td>14.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentages of error types encountered by GPT-4 in training tasks over all domains and seeds. "All" is the breakdown for all training tasks; "Success" is the breakdown for trials where all evaluation tasks were subsequently solved; "Failure" is the breakdown for the non-Success trials.</p>
<p>determining which item to place into the box first, i.e., which item is the heaviest. Training tasks have 3-10 objects; evaluation tasks have 100-250 objects.</p>
<p>Approaches. We evaluate the main approach, four ablations, and four baselines. The baselines are taken from the work by yang2022automated; see that work for details.</p>
<ul>
<li>GPT-4: Our main approach with CoT summarization and automated debugging.</li>
<li>No CoT: An ablation of the main approach that does not use CoT summarization. The three initial prompts are combined and "Write a short summary of this domain in words." and "What is that strategy?" are removed.</li>
<li>No Debug: An ablation of the main approach that does not use automated debugging. The first implementation of get_plan is used for evaluation.</li>
<li>No Names: An ablation of the main approach where all names in the PDDL domains and tasks are replaced with nondescriptive identifiers. For instance, predicates are</li>
</ul>
<p>renamed to predicate1, predicate2, etc., operators are renamed to operator1, operator2, etc. Altogether, the names of the domain, problem, predicates, operators, variables, types, and objects are ablated.</p>
<ul>
<li>GPT-3.5: GPT-3.5 with CoT summarization and automated debugging.</li>
<li>PG3: The generalized planning approach proposed by Yang et al. (2022). The synthesized programs are goalconditioned policies implemented as lifted decision lists. Synthesis is performed via heuristic search in policy space with their novel heuristic.</li>
<li>Policy Evalulation (PE): An approach from Yang et al. (2022) that is identical to PG3 except that the heuristic used for policy search is sparse: each candidate policy is scored based on the number of training tasks solved.</li>
<li>Plan Compare (PC): Another approach from Yang et al. (2022) that is identical to PG3 except for the policy search heuristic: example plans for each training task are generated offline, and the policy is scored based on its agreement with the example plans.</li>
<li>Random: Valid actions are randomly sampled and executed until a dead-end is encountered, the goal is reached, or a maximum horizon (default 1000, but see the previous work) is exceeded.
Experimental Details. We used a Macbook Pro laptop with an M1 chip and 64 GB RAM. Since an API for GPT4 is not publicly available, we used the ChatGPT browser interface for all experiments (including the GPT-3.5 baseline). The pipeline is fully automated except that prompts and responses are manually copied and pasted between the terminal and browser, with the clipboard programmatically updated. To facilitate reproducibility, we have released all chat logs and code.</li>
</ul>
<h2>Results and Analysis</h2>
<p>Main results are presented in Table 1. Examples of synthesized programs are presented in the appendix. Overall, the performance of GPT-4 with CoT summarization and automated debugging is strong in Delivery, Forest, Gripper, Ferry, and Heavy, and poor in Miconic and Spanner. Note that the reported success rates are averaged over all LLM conversations. In practice, performance could be boosted by restarting the conversation multiple times and using the bestfound program (Chowdhery et al. 2022). The strong performance in Heavy is especially notable. The generalized planning baselines fail in this domain because lifted decision lists are overly restrictive as program representations and cannot discover a concept like "heaviest overall" from pairwise heavier relations. GPT-4's ability to write general Python code is one of its biggest advantages as a generalized planning approach.</p>
<p>We also observe that in nearly all cases, GPT-4 either (1) solves all of the training tasks and then solves all of the evaluation tasks; or (2) fails to solve at least one training task and then fails to solve all of the evaluation tasks. In other words, overfitting to the training tasks is very rare, and evaluation performance is typically all-or-nothing. See Table 3 in the appendix for the maximum fraction of tasks solved.</p>
<p>Miconic failures. GPT-4 has a number of consistent failure modes in Miconic. First, at the strategy proposal level, it often fails to recognize that there can be multiple buildings, each with their own elevator. This is admittedly difficult to recognize given the PDDL encoding: buildings exist only implicitly based on the above relation between floor objects. For example, one would need to see that neither (above f1_b1, f1_b2) nor (above f1_b2, f1_b1) are true and conclude that the floors are in two different buildings. However, especially after automated debugging, GPT-4 can realize that there are multiple buildings, and furthermore, that building names (e.g., b1, b2) can be extracted from the floor names. But then other failures often occur, for example, attempting and failing to create a total ordering of the floors from the above predicate. Overall, we believe that Miconic is just beyond the limit of GPT-4's current capabilities and would likely be solved by the next generation of LLMs, or by GPT-4 with additional guidance.</p>
<p>Spanner failures. GPT-4 consistently fails in Spanner during strategy proposal. In particular, GPT-4 does not appear to realize that locations in Spanner are connected in a one-way chain. The strategy proposed is often "first collect all of the spanners, then tighten all of the nuts" or similar. A correct strategy would instead be to "move to each location in the chain, picking up any spanners and tightening any nuts at each location." Recognizing the existence of the one-way chain requires examining the link atoms in the training problems. Even after automated debugging, GPT-4 often assumes, incorrectly, that links are commutative.</p>
<p>Program efficiency. Although we prompt the LLM to implement a "simple" program that does not use search, it is still possible for the LLM to produce a program that does use search or is slow for other reasons (e.g., poor algorithmic complexity). We therefore measure synthesized program runtime. As a baseline for our comparison we use a state-of-the-art domain-independent PDDL planner LAMA (Richter and Westphal 2010) via Fast Downward (Helmert 2006), stopping after the first plan is found. ${ }^{2}$ In Figure 2, we plot wall-clock runtimes as a function of problem size (number of objects). Overall, we see that the synthesized programs not only scale favorably with respect to the planner, but also consistently beat the planner in absolute runtime by large margins. This is notable given that the LLM synthesizes Python programs, while the PDDL planner uses a highly optimized combination of Python and C++ code. The bottleneck for Fast Downward is often operator grounding. The LLM's programs do not need to ground operators-they can go directly from task to plan.</p>
<p>The role of CoT. Comparing GPT-4 to No CoT, we see that the impact of CoT summarization is mixed: it seems to help in most cases, but hurt in Miconic and Heavy. Miconic is an especially interesting case. When using CoT summarization, GPT-4 nearly always proposes a "sweep" strategy, where the elevator(s) are first moved to the bottom floor; then moved up one floor at a time until the top floor, picking up and dropping off passengers along the way; then moved</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>down one floor at a time, again picking up and dropping off passengers. This strategy would work in theory, but it requires finding a total ordering of floors within buildings. Without CoT, GPT-4 often attempts a different strategy: pick up, move, and drop off each passenger, one at a time. The latter strategy does not require a total ordering over floors and is arguably simpler to implement in Python. This example shows that CoT can influence the strategy proposed by GPT-4. Moreover, strategies that are "simple" to describe in natural language may not be simple to implement in code. In Heavy, there is not a clear difference in strategies with and without CoT. Since a good strategy is evidently discernible from the PDDL alone, it is possible that CoT "distracts" GPT-4 during implementation.</p>
<p>The role of automated debugging. Comparing GPT-4 to No Debug, we see that automated debugging generally improves performance dramatically. Figure 3 shows that even one step of automated debugging helps substantially, and further steps exhibit diminishing marginal improvements. Table 2 reports the fraction of error types encountered during training across. Python exceptions are most common, followed closely by errors in plan semantics, then errors in plan syntax, and finally timeouts. We also see that the error types are well-distributed within successful trials, suggesting that each of the four types of feedback are beneficial. In general, GPT-4 tends to make small, local corrections to code during automated debugging. If the code is structurally flawed and requires a significant rewrite, restarting the dialogue from the beginning may be required.</p>
<p>The role of PDDL names. Examining the results for the No Names ablation, we see performance overall is very poor. This confirms our hypothesis that the terms present in the PDDL domains and tasks are helpful to the LLM, as they would be to a human. Note that planners like Fast Downward and generalized planners like PG3 would be unaffected by name changes. However, there are a few cases where the No Names ablation does succeed, suggesting that the LLM has some capacity for purely syntactic generalized planning.</p>
<p>GPT-3.5 vs. GPT-4. Examining the results for GPT-3.5, we see that it performs much worse than GPT-4. This is consistent with other reports (OpenAI 2023; Bubeck et al. 2023) that GPT-4 is far superior on reasoning and coding tasks. Qualitatively, the programs proposed by GPT-3.5 are flawed in myriad ways and do not usually appear "close". They also do not seem to improve with automated debugging.</p>
<p>Data efficiency. In the appendix, we analyze the number of training tasks used in each successful trial. A training task is used if it appeared in the prompt and/or triggered feedback during automated debugging. Since two training tasks are always used in the prompt, the minimum used is two. Interestingly, in the vast majority of cases, only those two training tasks are used. During automated debugging, these two prompting tasks are always checked first, and most of the time, they are sufficient to identify issues. In a small number of cases, a third task is also used during automated debugging. This result speaks to the strong few-shot learning capabilities of GPT-4. We expect that in many cases, even one training task would suffice, although we did witness a drop in performance in preliminary experiments with one task.</p>
<h2>Discussion and Future Work</h2>
<p>In this work, we showed that GPT-4 with CoT summarization and automated debugging is a surprisingly strong generalized planner in PDDL domains. We conclude with limitations of this work, reflections about the implications of our findings, and opportunities for future work.</p>
<p>Limitations. A major limitation of this work and previous work on generalized planning is that it is easy enough to hand-design generalized plans for all of the domains considered. Nonetheless, we expect this line of work to be practically useful for at least three reasons. (1) In some cases, it may be considerably easier to specify PDDL domain and problem descriptions than it is to directly specify a generalized plan. (2) In a fully autonomous system, where operators and predicates are learned in association to natural language, we would want the system to also synthesize generalized plans autonomously. (3) Beyond PDDL, generalized planning with LLMs would be an even more attractive option, since other approaches rely strongly on formal specifications. Another limitation of this work is our use of training tasks to communicate the task distribution of interest to the LLM. In general, a few example tasks may be insufficient to express the full distribution. Other representations like natural language or procedural generation code may be better, but would require more human input.</p>
<p>Is (generalized) planning now obsolete? No. First, there remains a performance gap between GPT-4 and PG3, and other generalized planners may be even better. However, even if this gap is closed by the next generation of LLMs, we would still say no. Planning remains essential in domains where no simple program exists. An interesting direction for future work would be automatically detecting whether a simple program might exist before attempting to synthesize one. We tried the Sokoban domain and found that GPT4 correctly indicates that no simple program exists. However, this property of Sokoban is well-known, so it is likely parroting pretraining data. We also tried the Slitherlink domain, which was featured in the 2023 International Planning Competition, and found that GPT-4 did not recognize that no simple strategy exists (Takayuki 2000). Generalized planning without LLMs also remains important in cases where domain descriptions are not human-readable, e.g., because the predicates or operators are learned (Silver et al. 2023). Even with natural language descriptions, combining "classical" approaches with LLMs may be best.</p>
<p>What if we gave the LLM access to a planner? Giving an LLM access to APIs is a very powerful idea (Schick et al. 2023) and one such API could be a PDDL planner (Liu et al. 2023). An LLM could potentially use such a planner for generalized planning, especially given that approaches like PG3 rely on access to a planner to generate example plans. In some domains, generating example plans naively would likely confuse the LLM. For example, plans generated in the Forest domain would follow arbitrary paths through the dirt rather than following the slightly longer marked trail. In other cases, though, example plans could be very useful, especially if the LLM generates them in a targeted way. Leveraging diverse plans (Sohrabi et al. 2016; Katz and Sohrabi 2020) could be particularly useful.</p>
<h2>References</h2>
<p>Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes, O.; David, B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.
Alur, R.; Bodik, R.; Juniwal, G.; Martin, M. M.; Raghothaman, M.; Seshia, S. A.; Singh, R.; Solar-Lezama, A.; Torlak, E.; and Udupa, A. 2013. Syntax-guided synthesis. IEEE.
Bonet, B.; and Geffner, H. 2015. Policies that generalize: Solving many planning problems with the same policy. In Twenty-Fourth International Joint Conference on Artificial Intelligence.
Bonet, B.; and Geffner, H. 2018. Features, Projections, and Representation Change for Generalized Planning. CoRR, abs/1801.10055.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901.</p>
<p>Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.
Chapman, D. 1987. Planning for Conjunctive Goals. Artificial Intelligence, 32: 333-377.
Chen, A.; Scheurer, J.; Korbak, T.; Campos, J. A.; Chan, J. S.; Bowman, S. R.; Cho, K.; and Perez, E. 2023a. Improving Code Generation by Training with Natural Language Feedback. arXiv:2303.16749.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.
Chen, X.; Lin, M.; SchÃ¤rli, N.; and Zhou, D. 2023b. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.
Collins, K. M.; Wong, C.; Feng, J.; Wei, M.; and Tenenbaum, J. B. 2022. Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks. arXiv preprint arXiv:2205.05718.
Cropper, A.; and DumanÄiÄ‡, S. 2022. Inductive logic programming at 30: a new introduction. Journal of Artificial Intelligence Research, 74: 765-850.
Fikes, R. E.; Hart, P. E.; and Nilsson, N. J. 1972. Learning and executing generalized robot plans. Artificial intelligence, 3: 251-288.</p>
<p>Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.; Callan, J.; and Neubig, G. 2022. PAL: Program-aided Language Models. arXiv preprint arXiv:2211.10435.
Gulwani, S.; Polozov, O.; Singh, R.; et al. 2017. Program synthesis. Foundations and TrendsÂ® in Programming Languages, 4(1-2): 1-119.
Helmert, M. 2006. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191-246.
Howey, R.; Long, D.; and Fox, M. 2004. VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. In 16th IEEE International Conference on Tools with Artificial Intelligence, 294-301. IEEE.
Huang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. In International Conference on Machine Learning (ICML).
Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y.; et al. 2022b. Inner Monologue: Embodied Reasoning through Planning with Language Models. arXiv preprint arXiv:2207.05608.
Imani, S.; Du, L.; and Shrivastava, H. 2023. MathPrompter: Mathematical Reasoning using Large Language Models. arXiv:2303.05398.
Jiang, X.; Dong, Y.; Wang, L.; Shang, Q.; and Li, G. 2023. Self-planning Code Generation with Large Language Model. arXiv preprint arXiv:2303.06689.
JimÃ©nez, S.; and Jonsson, A. 2015. Computing plans with control flow and procedures using a classical planner. In Proceedings of the Eighth Annual Symposium on Combinatorial Search, SOCS-15, 62-69.
JimÃ©nez, S.; Segovia-Aguas, J.; and Jonsson, A. 2019. A review of generalized planning. The Knowledge Engineering Review, 34.
Katz, M.; and Sohrabi, S. 2020. Reshaping diverse planning. In Proceedings of the AAAI Conference on Artificial Intelligence, 06, 9892-9899.
Levesque, H. 2005. Planning with Loops. In IJCAI.
Levine, J.; and Humphreys, D. 2003. Learning action strategies for planning domains using genetic programming. In Workshops on Applications of Evolutionary Computation, 684-695. Springer.
Liang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter, B.; Florence, P.; and Zeng, A. 2022. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753.
Lin, B. Y.; Huang, C.; Liu, Q.; Gu, W.; Sommerer, S.; and Ren, X. 2022. On Grounded Planning for Embodied Tasks with Language Models. arXiv preprint arXiv:2209.00465.
Lin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J. 2023. Text2motion: From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153.
Liu, B.; Jiang, Y.; Zhang, X.; Liu, Q.; Zhang, S.; Biswas, J.; and Stone, P. 2023. LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency. arXiv preprint arXiv:2304.11477.</p>
<p>McDermott, D. 2000. The 1998 AI Planning Systems Competition. AI Magazine, 21(2): 35-55.
Muggleton, S. 1991. Inductive logic programming. New generation computing, 8: 295-318.
Nijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou, Y.; Savarese, S.; and Xiong, C. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. arXiv:2203.13474.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Pallagani, V.; Muppasani, B.; Murugesan, K.; Rossi, F.; Horesh, L.; Srivastava, B.; Fabiano, F.; and Loreggia, A. 2022. Plansformer: Generating Symbolic Plans using Transformers. arXiv preprint arXiv:2212.08681.
Raman, S. S.; Cohen, V.; Rosen, E.; Idrees, I.; Paulius, D.; and Tellex, S. 2022. Planning with Large Language Models via Corrective Re-prompting. arXiv preprint arXiv:2211.09935.
Richter, S.; and Westphal, M. 2010. The LAMA planner: Guiding Cost-based Anytime Planning with Landmarks. Journal of Artificial Intelligence Research, 39: 127-177.
Rivlin, O.; Hazan, T.; and Karpas, E. 2020. Generalized Planning With Deep Reinforcement Learning. arXiv preprint arXiv:2005.02305.
Schaul, T.; Horgan, D.; Gregor, K.; and Silver, D. 2015. Universal value function approximators. In International conference on machine learning, 1312-1320. PMLR.
Schick, T.; Dwivedi-Yu, J.; DessÃ¬, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.
Segovia-Aguas, J.; JimÃ©nez, S.; and Jonsson, A. 2018. Computing hierarchical finite state controllers with classical planning. Journal of Artificial Intelligence Research, 62: 755-797.
Segovia-Aguas, J.; JimÃ©nez, S.; and Jonsson, A. 2021. Generalized Planning as Heuristic Search. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 31, 569-577.
Sharma, P.; Torralba, A.; and Andreas, J. 2022. Skill Induction and Planning with Latent Language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1713-1726.
Silver, T.; Chitnis, R.; Kumar, N.; McClinton, W.; LozanoPerez, T.; Kaelbling, L. P.; and Tenenbaum, J. 2023. Predicate Invention for Bilevel Planning. In AAAI Conference on Artificial Intelligence (AAAI).
Silver, T.; Hariprasad, V.; Shuttleworth, R. S.; Kumar, N.; Lozano-PÃ©rez, T.; and Kaelbling, L. P. 2022. PDDL Planning with Pretrained Large Language Models. In NeurIPS 2022 Foundation Models for Decision Making Workshop.
Singh, I.; Blukis, V.; Mousavian, A.; Goyal, A.; Xu, D.; Tremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2022. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302.</p>
<p>Sohrabi, S.; Riabov, A. V.; Udrea, O.; and Hassanzadeh, O. 2016. Finding diverse high-quality plans for hypothesis generation. In ECAI 2016, 1581-1582. IOS Press.
Srivastava, S. 2011. Foundations and applications of generalized planning. AI Communications, 24(4): 349-351.
Srivastava, S.; Immerman, N.; Zilberstein, S.; and Zhang, T. 2011. Directed Search for Generalized Plans Using Classical Planners. In ICAPS.
Sutton, R. S.; Modayil, J.; Delp, M.; Degris, T.; Pilarski, P. M.; White, A.; and Precup, D. 2011. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent SystemsVolume 2, 761-768.
Takayuki, Y. 2000. On the NP-completeness of the Slither Link puzzle. IPSJ SIGNotes ALgorithms.
Valmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambhampati, S. 2022. Large Language Models Still Canâ€™t Plan (A Benchmark for LLMs on Planning and Reasoning about Change). arXiv preprint arXiv:2206.10498.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; and Zhou, D. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.
Winner, E. Z. 2008. Learning Domain-Specific Planners from Example Plans. Ph.D. thesis, Carnegie Mellon University, USA.
Xia, C. S.; and Zhang, L. 2023. Conversational Automated Program Repair. arXiv:2301.13246.
Xie, Y.; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023. Translating natural language to planning goals with largelanguage models. arXiv preprint arXiv:2302.05128.
Yang, R.; Silver, T.; Curtis, A.; Lozano-Perez, T.; and Kaelbling, L. P. 2022. PG3: Policy-Guided Planning for Generalized Policy Generation. In IJCAI.
Zheng, W.; Sharan, S.; Jaiswal, A. K.; Wang, K.; Xi, Y.; Xu, D.; and Wang, Z. 2023. Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. arXiv preprint arXiv:2305.00909.</p>
<h1>Additional Results</h1>
<p>Table 3 reports the maximum fraction of evaluation tasks solved over seeds. In most cases, the LLM either solves all or none of the evaluation tasks. This suggests that the LLM does not overfit to the training tasks, even though only a very small number of them are used (see main text).</p>
<p>Figure 4 shows the number of training tasks used in successful trials. See main text for details. In the vast majority of cases, only two tasks are necessary, indicating the strong data-efficiency of GPT-4 as a generalized planner.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">No CoT</th>
<th style="text-align: center;">No Debug</th>
<th style="text-align: center;">No Names</th>
<th style="text-align: center;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Delivery</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">Forest</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Gripper</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">Miconic</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">Ferry</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">Spanner</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">Heavy</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
</tbody>
</table>
<p>Table 3: Max fraction of evaluation tasks solved per seed.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Data efficiency of GPT-4. The reported fractions are over all trials, over all domains.</p>
<h2>Example Programs</h2>
<p>Examples of programs synthesized by GPT-4 with CoT summarization and automated debugging are shown in Listings 1, 2, 3, 4, 5, 6, and 7. Any comments in the code are added by GPT-4 itself.</p>
<p>Listing 1: Delivery domain example synthesized program (success)
def get_plan(objects, init, goal):
locs = [obj[0] for obj in objects if obj[1] == "loc"]
papers = [obj[0] for obj in objects if obj[1] == "paper"]
home_base = next(loc for loc in locs if ("ishomebase", loc) in init)
safe_locs = [loc for loc in locs if ("safe", loc) in init]
wants_paper = [loc for loc in locs if ("wantspaper", loc) in init]
plan $=[]$
for loc in wants_paper:
if loc in safe_locs:
# Pick up a newspaper at the home base
paper = papers.pop()
plan.append(f" (pick-up {paper} {home_base})")
# Move to the safe location that wants a newspaper
plan.append(f" (move {home_base} {loc})")
# Deliver the newspaper to the location
plan.append(f" (deliver {paper} {loc})")
# Move back to the home base
plan.append(f" (move {loc} {home_base})")
return plan</p>
<p>Listing 2: Forest domain example synthesized program (success)</p>
<div class="codehilite"><pre><span></span><code>def get_plan(objects, init, goal):
    def find_location(init):
        for atom in init:
            if atom[0] == &quot;at&quot;:
                return atom[1]
        return None
    def find_adjacent_trail(current_loc, init):
        for atom in init:
            if atom[0] == &quot;ontrail&quot; and atom[1] == current_loc:
                return atom[2]
        return None
    def is_hill(loc, init):
        for atom in init:
            if atom[0] == &quot;ishill&quot; and atom[1] == loc:
                return True
        return False
    current_loc = find_location(init)
    goal_loc = find_location(goal)
    plan = []
    while current_loc != goal_loc:
        next_loc = find_adjacent_trail(current_loc, init)
        if is_hill(next_loc, init):
            action = f&quot;(climb {current_loc} {next_loc})&quot;
        else:
            action = f&quot;(walk {current_loc} {next_loc})&quot;
        plan.append(action)
        current_loc = next_loc
    return plan
</code></pre></div>

<p>Listing 3: Gripper domain example synthesized program (success)
def get_plan(objects, init, goal):
def get_objects_of_type(type_name):
return [obj for obj in objects if obj.startswith(type_name)]
def get_location(state, ball):
for atom in state:
if atom[0] == 'at' and atom[1] == ball:
return atom[2]
return None
balls = get_objects_of_type('ball')
rooms = get_objects_of_type('room')
grippers = get_objects_of_type('gripper')
robby_location = None
for atom in init:
if atom[0] == 'at-robby':
robby_location = atom[1]
break
plan $=[]$
for ball in balls:
initial_location = get_location(init, ball)
goal_location = get_location(goal, ball)
if goal_location is None:
continue
if initial_location != goal_location:
plan.append(f'(move {robby_location} {initial_location}}')
robby_location = initial_location
plan.append(f'(pick {ball} {initial_location} {grippers[0]})')
plan.append(f'(move {robby_location} {goal_location}})')
robby_location = goal_location
plan.append(f'(drop {ball} {goal_location} {grippers[0]})')
return plan</p>
<p>Listing 4: Miconic domain example synthesized program (failure)
def get_plan(objects, init, goal):
floors_by_building $={ }$
passengers_by_building $={ }$
for obj, obj_type in objects:
if obj_type $==$ "floor":
building = obj.split("<em>") [-1]
if building not in floors_by_building:
floors_by_building[building] = []
floors_by_building[building].append(obj)
elif obj_type $==$ "passenger":
building = obj.split("</em>") [-1]
if building not in passengers_by_building:
passengers_by_building[building] = []
passengers_by_building[building].append(obj)
for building in floors_by_building:
floors_by_building[building] = sorted(floors_by_building[building])
origins $={p$ : f for $p, f$ in (atom[1:] for atom in init if atom[0] == "origin") $}$
destins $={p$ : f for $p, f$ in (atom[1:] for atom in init if atom[0] == "destin") $}$
lift_at $={a t o m[1]:$ atom for atom in init if atom[0] == "lift-at")
plan $=[]$
boarded $=$ set ( )
for building, floors in floors_by_building.items():
passengers = passengers_by_building[building]
current_lift_at = lift_at[building]
for floor in floors + list(reversed(floors)):
if current_lift_at != floor:
if current_lift_at &lt; floor:
plan.append(f"(up {current_lift_at} {floor})")
else:
plan.append(f"(down {current_lift_at} {floor}}")
current_lift_at = floor
for passenger in passengers:
if origins.get (passenger) == floor:
plan.append(f"(board {floor} {passenger})")
boarded.add(passenger)
if destins.get (passenger) == floor and passenger in boarded:
plan.append(f"(depart {floor} {passenger})")
boarded.remove (passenger)
return plan</p>
<p>Listing 5: Ferry domain example synthesized program (success)
def get_plan(objects, init, goal):
plan $=[]$
# Helper function to find a car's location
def get_location(car, state):
for atom in state:
if atom[0] == 'at' and atom[1] == car:
return atom[2]
return None
cars = [obj for obj in objects if obj.startswith('c')]
ferry_location = [atom[1] for atom in init if atom[0] == 'at-ferry'][0]
for car in cars:
initial_location = get_location(car, init)
goal_location = get_location(car, goal)
# Sail to the car's initial location
if ferry_location != initial_location:
plan.append(f'(sail {ferry_location} {initial_location})')
ferry_location = initial_location
# Board the car onto the ferry
plan.append(f'(board {car} {initial_location})')
# Sail to the car's goal location
if ferry_location != goal_location:
plan.append(f'(sail {ferry_location} {goal_location})')
ferry_location = goal_location
# Debark the car at the goal location
plan.append(f'(debark {car} {goal_location})')
return plan</p>
<p>Listing 6: Spanner domain example synthesized program (failure)
def get_plan(objects, init, goal):
plan $=[]$
bob_location = get_bob_location(init)
spanner_locations = get_spanner_locations(init)
nuts_to_tighten = {n for pred, *args in goal if pred == 'tightened' for n in args}
while nuts_to_tighten:
closest_spanner = None
min_distance = float('inf')
for loc, spanner in spanner_locations.items():
distance = get_shortest_path(init, bob_location, loc)
if distance &lt; min_distance:
min_distance = distance
closest_spanner = spanner
closest_spanner_loc = loc
# Move Bob to the closest spanner
path_to_spanner = get_path(init, bob_location, closest_spanner_loc)
for next_location in path_to_spanner:
plan.append(f'(walk {bob_location} {next_location} bob)')
bob_location = next_location
# Pick up the spanner
plan.append(f'(pickup_spanner {closest_spanner_loc} {closest_spanner} bob)')
# Move Bob to the gate
gate = 'gate'
path_to_gate = get_path(init, bob_location, gate)
for next_location in path_to_gate:
plan.append(f'{walk {bob_location} {next_location} bob)')
bob_location = next_location
# Tighten a nut using the spanner
nut = nuts_to_tighten.pop()
plan.append(f'(tighten_nut {gate} {closest_spanner} bob {nut})')
# Remove the used spanner from the available spanners
if closest_spanner_loc in spanner_locations:
del spanner_locations[closest_spanner_loc]
return plan
def get_path(init, start, end):
paths = [start]
while paths[-1] != end:
for pred, l1, 12 in init:
if pred == 'link' and paths[-1] == l1 and 12 not in paths:
paths.append(l2)
break
return paths[1:]</p>
<p>Listing 7: Heavy domain example synthesized program (success)
def extract_heavier_relations(init):
heavier_relations = set()
for atom in init:
if atom[0] == 'heavier':
heavier_relations.add(atom)
return heavier_relations
def sort_objects_by_weight(objects, heavier_relations):
sorted_objects = []
while len(objects) &gt; 0:
for obj in objects.copy():
if all(('heavier', other, obj) not in heavier_relations for other in objects):
sorted_objects.append(obj)
objects.remove(obj)
return sorted_objects
def get_plan(objects, init, goal):
heavier_relations = extract_heavier_relations(init)
sorted_objects = sort_objects_by_weight(set(objects), heavier_relations)
plan = []
first_object = sorted_objects.pop(0)
plan.append(f"{pack-first {first_object})")
for obj in sorted_objects:
plan.append(f"(stack {first_object} {obj})")
first_object = obj
return plan</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Our intention is not to compare planners, but rather to provide a frame of reference for runtime.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8539 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8539</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8539</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-024646623aa733df2bb752aa3bb3e76d691cab11</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/024646623aa733df2bb752aa3bb3e76d691cab11" target="_blank">Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels.</p>
                <p><strong>Paper Abstract:</strong> We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts. We extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed, one-shot prompting (rather than simple, zero-shot prompts) with text versions of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4, on zero- and one-shot prompts using image versions of the simplest tasks. Our experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8539.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8539.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (text-only, gpt-4-0613, November 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The text-only version of OpenAI's GPT-4 evaluated on ConceptARC/ARC-style grid-transformation puzzles using detailed one-shot prompts and multi-guess evaluation; shown to improve with more informative prompting but still far below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (text-only, gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large, pre-trained transformer-based language model from OpenAI; in these experiments used as a text-only model receiving grid encodings (numeric color encoding per cell) as text prompts. The authors do not provide architecture details or parameter count in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ConceptARC / ARC grid transformation tasks (including minimal tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based visual analogy / transformation problems requiring spatial/object concepts (e.g., Top/Bottom, Inside/Outside, Center, Same/Different) and basic geometry/topology reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>ConceptARC tasks were converted to text using numeric encodings for colors and rows (format used in prior work). The authors used a one-shot prompt containing detailed instructions and a solved example (see Appendix 7.1). For each task the model was allowed up to three guesses (if first answer incorrect, request another, up to three). Evaluations run with temperature=0 and temperature=0.5. For minimal tasks (48 tasks) the same prompting method was used.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No internal algorithmic modifications; relied on prompt engineering (detailed one-shot prompt + example). Used repeated-query strategy (up to three guesses) standard in ARC evaluations. No chain-of-thought or symbolic program synthesis was applied within GPT-4 beyond what emerges in model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On all 480 ConceptARC tasks (16 concept groups, 30 tasks each) GPT-4 achieved overall accuracy 0.33 with temperature=0 and 0.33 with temperature=0.5 (Table 1). Per-concept accuracies ranged (examples): Top and Bottom 2D: 0.60 (temp=0) / 0.63 (temp=0.5); Extract Objects: 0.13 / 0.13; Center: 0.37 / 0.37; many concept groups between ~0.2–0.5. On 48 minimal tasks: accuracy 0.69 (temp=0) and 0.65 (temp=0.5). Previous (simpler zero-shot) evaluation reported GPT-4 accuracies ~0.19–0.25 (Moskvichev et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors report qualitative and quantitative evidence that GPT-4 does not robustly form human-like abstractions for these spatial concepts: although one-shot prompting improved performance over simpler zero-shot prompts, overall accuracy remains far below human levels (humans 0.91 on ConceptARC) and below special-purpose ARC programs. No internal probing or ablation demonstrating explicit spatial representations is provided; improvement with one-shot prompt suggests sensitivity to prompt format rather than emergence of robust abstract spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to humans: humans achieved 0.91 overall on ConceptARC (and 0.95 on minimal tasks). Compared to special-purpose program(s): Moskvichev et al. reported the first-place Kaggle ARC program had 0.52 on ConceptARC overall (and 0.81 on minimal tasks). Earlier simpler GPT-4 runs reported by Moskvichev et al. had lower accuracy (0.25). The paper also references other works evaluating LLMs on ARC subsets (refs [4,9,14,19]) but does not directly experiment with other LLM architectures here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failures include many concept groups with low accuracy (e.g., Extract Objects ~0.13). GPT-4 often fails to generalize across different instantiations of a concept group despite the one-shot example. Performance gains from prompting indicate reliance on prompt artifacts rather than stable abstract representations. No model size/architecture ablations were done. The authors note that GPT-4's performance, even with one-shot prompting, remains well below humans and special-purpose program solutions, indicating a lack of robust abstraction and spatial generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8539.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8539.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (gpt4-vision-preview, November 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal (vision-capable) version of GPT-4 evaluated on minimal ConceptARC visual grid tasks using image inputs; performed substantially worse than the text-only GPT-4 on these spatial reasoning tasks and exhibited specific visual-to-text mapping failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (gpt4-vision-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal extension of GPT-4 that can process images and text. In these experiments used to receive visual grids as images and to return natural-language descriptions of the output grid (including dimensions). The specific model version used (gpt4-vision-preview) does not allow manual temperature control and documentation did not specify default temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Minimal ConceptARC tasks (48 minimal visual tasks drawn from ConceptARC minimal subset)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based visual analogy / transformation tasks presented as images requiring spatial/object reasoning and mapping to an output grid representation</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Authors experimented with various ways of presenting demonstrations to GPT-4V: (1) all input-output pairs in one image, (2) separate image for each input-output pair, and (3) each input and each output grid as an individual image — only (3) yielded correct solutions in preliminary testing, so it was adopted. Because GPT-4V sometimes failed to translate visual grids to consistent text encodings, the evaluation requested natural-language descriptions of the output grid along with its dimensions. Both one-shot (example included) and zero-shot (interactive web-app style) prompting were used; for zero-shot the model could respond after each demonstration input-output pair before the test input was shown.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Relied on GPT-4V's internal multimodal processing; the authors controlled only prompt format and image presentation. They asked for natural-language grid descriptions (not numeric grid encodings) to avoid transcription errors. The zero-shot setting exploited the web-app's ability to interleave model responses after demonstrations. No external symbolic or program-synthesis modules were used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On 48 minimal visual tasks: GPT-4V one-shot accuracy = 0.23; GPT-4V zero-shot accuracy = 0.25 (Table 2). For comparison, text-only GPT-4 on the same minimal tasks: 0.69 (temp=0) and 0.65 (temp=0.5); humans = 0.95.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors report that GPT-4V sometimes produced descriptions of an abstract transformation rule, indicating some level of rule extraction, but often failed to translate that into a correct output-grid description (or included incorrect details copied from the solved example). There is no systematic probing that demonstrates robust spatial representations; overall low accuracy is taken as evidence against robust spatial abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to text-only GPT-4 on the same minimal tasks: GPT-4V (0.23–0.25) performed substantially worse than text-only GPT-4 (0.65–0.69). Compared to humans (0.95) and Kaggle first-place program on minimal tasks (0.81). The paper also contrasts GPT-4V zero-shot vs one-shot (zero-shot slightly better: 0.25 vs 0.23) and notes overlap of solved tasks between the settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Specific failure modes: (1) inconsistent visual-to-text translation of grids (colors and numeric encodings), (2) tendency to copy details from the solved example into responses (negative transfer from one-shot example), (3) cases where the model correctly states the abstract rule but fails to produce an accurate output-grid description, and (4) occasional correct grid descriptions produced despite an incorrect abstract rule (i.e., brittle or spurious reasoning). Authors expect GPT-4V to perform even worse on the full (non-minimal) ConceptARC corpus. No temperature control and limited debugging visibility of internal multimodal processing were noted as practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The conceptarc benchmark: Evaluating understanding and generalization in the arc domain <em>(Rating: 2)</em></li>
                <li>The Abstraction and Reasoning Corpus (ARC) <em>(Rating: 2)</em></li>
                <li>Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations <em>(Rating: 2)</em></li>
                <li>Large language models are not strong abstract reasoners <em>(Rating: 1)</em></li>
                <li>Large language models as general pattern machines <em>(Rating: 1)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8539",
    "paper_id": "paper-024646623aa733df2bb752aa3bb3e76d691cab11",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4 (text-only)",
            "name_full": "GPT-4 (text-only, gpt-4-0613, November 2023)",
            "brief_description": "The text-only version of OpenAI's GPT-4 evaluated on ConceptARC/ARC-style grid-transformation puzzles using detailed one-shot prompts and multi-guess evaluation; shown to improve with more informative prompting but still far below human performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (text-only, gpt-4-0613)",
            "model_description": "A large, pre-trained transformer-based language model from OpenAI; in these experiments used as a text-only model receiving grid encodings (numeric color encoding per cell) as text prompts. The authors do not provide architecture details or parameter count in this paper.",
            "model_size": null,
            "puzzle_name": "ConceptARC / ARC grid transformation tasks (including minimal tasks)",
            "puzzle_type": "2D grid-based visual analogy / transformation problems requiring spatial/object concepts (e.g., Top/Bottom, Inside/Outside, Center, Same/Different) and basic geometry/topology reasoning",
            "task_setup": "ConceptARC tasks were converted to text using numeric encodings for colors and rows (format used in prior work). The authors used a one-shot prompt containing detailed instructions and a solved example (see Appendix 7.1). For each task the model was allowed up to three guesses (if first answer incorrect, request another, up to three). Evaluations run with temperature=0 and temperature=0.5. For minimal tasks (48 tasks) the same prompting method was used.",
            "mechanisms_or_strategies": "No internal algorithmic modifications; relied on prompt engineering (detailed one-shot prompt + example). Used repeated-query strategy (up to three guesses) standard in ARC evaluations. No chain-of-thought or symbolic program synthesis was applied within GPT-4 beyond what emerges in model outputs.",
            "performance_metrics": "On all 480 ConceptARC tasks (16 concept groups, 30 tasks each) GPT-4 achieved overall accuracy 0.33 with temperature=0 and 0.33 with temperature=0.5 (Table 1). Per-concept accuracies ranged (examples): Top and Bottom 2D: 0.60 (temp=0) / 0.63 (temp=0.5); Extract Objects: 0.13 / 0.13; Center: 0.37 / 0.37; many concept groups between ~0.2–0.5. On 48 minimal tasks: accuracy 0.69 (temp=0) and 0.65 (temp=0.5). Previous (simpler zero-shot) evaluation reported GPT-4 accuracies ~0.19–0.25 (Moskvichev et al.).",
            "evidence_of_spatial_reasoning": "Authors report qualitative and quantitative evidence that GPT-4 does not robustly form human-like abstractions for these spatial concepts: although one-shot prompting improved performance over simpler zero-shot prompts, overall accuracy remains far below human levels (humans 0.91 on ConceptARC) and below special-purpose ARC programs. No internal probing or ablation demonstrating explicit spatial representations is provided; improvement with one-shot prompt suggests sensitivity to prompt format rather than emergence of robust abstract spatial reasoning.",
            "comparisons": "Compared to humans: humans achieved 0.91 overall on ConceptARC (and 0.95 on minimal tasks). Compared to special-purpose program(s): Moskvichev et al. reported the first-place Kaggle ARC program had 0.52 on ConceptARC overall (and 0.81 on minimal tasks). Earlier simpler GPT-4 runs reported by Moskvichev et al. had lower accuracy (0.25). The paper also references other works evaluating LLMs on ARC subsets (refs [4,9,14,19]) but does not directly experiment with other LLM architectures here.",
            "limitations_or_failure_cases": "Failures include many concept groups with low accuracy (e.g., Extract Objects ~0.13). GPT-4 often fails to generalize across different instantiations of a concept group despite the one-shot example. Performance gains from prompting indicate reliance on prompt artifacts rather than stable abstract representations. No model size/architecture ablations were done. The authors note that GPT-4's performance, even with one-shot prompting, remains well below humans and special-purpose program solutions, indicating a lack of robust abstraction and spatial generalization.",
            "uuid": "e8539.0",
            "source_info": {
                "paper_title": "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (gpt4-vision-preview, November 2023)",
            "brief_description": "The multimodal (vision-capable) version of GPT-4 evaluated on minimal ConceptARC visual grid tasks using image inputs; performed substantially worse than the text-only GPT-4 on these spatial reasoning tasks and exhibited specific visual-to-text mapping failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V (gpt4-vision-preview)",
            "model_description": "Multimodal extension of GPT-4 that can process images and text. In these experiments used to receive visual grids as images and to return natural-language descriptions of the output grid (including dimensions). The specific model version used (gpt4-vision-preview) does not allow manual temperature control and documentation did not specify default temperature.",
            "model_size": null,
            "puzzle_name": "Minimal ConceptARC tasks (48 minimal visual tasks drawn from ConceptARC minimal subset)",
            "puzzle_type": "2D grid-based visual analogy / transformation tasks presented as images requiring spatial/object reasoning and mapping to an output grid representation",
            "task_setup": "Authors experimented with various ways of presenting demonstrations to GPT-4V: (1) all input-output pairs in one image, (2) separate image for each input-output pair, and (3) each input and each output grid as an individual image — only (3) yielded correct solutions in preliminary testing, so it was adopted. Because GPT-4V sometimes failed to translate visual grids to consistent text encodings, the evaluation requested natural-language descriptions of the output grid along with its dimensions. Both one-shot (example included) and zero-shot (interactive web-app style) prompting were used; for zero-shot the model could respond after each demonstration input-output pair before the test input was shown.",
            "mechanisms_or_strategies": "Relied on GPT-4V's internal multimodal processing; the authors controlled only prompt format and image presentation. They asked for natural-language grid descriptions (not numeric grid encodings) to avoid transcription errors. The zero-shot setting exploited the web-app's ability to interleave model responses after demonstrations. No external symbolic or program-synthesis modules were used.",
            "performance_metrics": "On 48 minimal visual tasks: GPT-4V one-shot accuracy = 0.23; GPT-4V zero-shot accuracy = 0.25 (Table 2). For comparison, text-only GPT-4 on the same minimal tasks: 0.69 (temp=0) and 0.65 (temp=0.5); humans = 0.95.",
            "evidence_of_spatial_reasoning": "Authors report that GPT-4V sometimes produced descriptions of an abstract transformation rule, indicating some level of rule extraction, but often failed to translate that into a correct output-grid description (or included incorrect details copied from the solved example). There is no systematic probing that demonstrates robust spatial representations; overall low accuracy is taken as evidence against robust spatial abstraction.",
            "comparisons": "Compared directly to text-only GPT-4 on the same minimal tasks: GPT-4V (0.23–0.25) performed substantially worse than text-only GPT-4 (0.65–0.69). Compared to humans (0.95) and Kaggle first-place program on minimal tasks (0.81). The paper also contrasts GPT-4V zero-shot vs one-shot (zero-shot slightly better: 0.25 vs 0.23) and notes overlap of solved tasks between the settings.",
            "limitations_or_failure_cases": "Specific failure modes: (1) inconsistent visual-to-text translation of grids (colors and numeric encodings), (2) tendency to copy details from the solved example into responses (negative transfer from one-shot example), (3) cases where the model correctly states the abstract rule but fails to produce an accurate output-grid description, and (4) occasional correct grid descriptions produced despite an incorrect abstract rule (i.e., brittle or spurious reasoning). Authors expect GPT-4V to perform even worse on the full (non-minimal) ConceptARC corpus. No temperature control and limited debugging visibility of internal multimodal processing were noted as practical limitations.",
            "uuid": "e8539.1",
            "source_info": {
                "paper_title": "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The conceptarc benchmark: Evaluating understanding and generalization in the arc domain",
            "rating": 2
        },
        {
            "paper_title": "The Abstraction and Reasoning Corpus (ARC)",
            "rating": 2
        },
        {
            "paper_title": "Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not strong abstract reasoners",
            "rating": 1
        },
        {
            "paper_title": "Large language models as general pattern machines",
            "rating": 1
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 1
        }
    ],
    "cost": 0.01071925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks</h1>
<p>Melanie Mitchell, Alessandro B. Palmarini, and Arseny Moskvichev<br>Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, NM 87501<br>mm@santafe.edu, abp@santafe.edu, arseny.moskvichev@gmail.com</p>
<h4>Abstract</h4>
<p>We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts. We extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed, one-shot prompting (rather than simple, zero-shot prompts) with text versions of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4, on zero- and one-shot prompts using image versions of the simplest tasks. Our experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels.</p>
<h2>1 Introduction</h2>
<p>To what extent have large pre-trained language models (LLMs) developed "emergent" capabilities for abstract reasoning? The defining characteristic of abstract reasoning is the ability to induce a rule or pattern from limited data or experience and to apply this rule or pattern to new, unseen situations. Such abilities are a key aspect of human intelligence; even very young children are adept at learning abstract rules from just a few examples [13].</p>
<p>Recently, various researchers have claimed that sufficiently large pre-trained language models can develop emergent abilities for reasoning [16], general abstract pattern recognition [9], and analogymaking [15]. However, the internal mechanisms giving rise to these abilities are not well understood, and other researchers have cast doubt on the claims that these systems actually form humanlike abstractions [4], showing in many cases that while LLMs can solve problems involving content similar to that in their training data, they are weak in generalizing outside such problems [8, 11, 18]. Some have interpreted this as evidence that LLMs rely not on generalizable abstract reasoning but on learning complex patterns of associations in their training data and performing "approximate retrieval" of these patterns in new situations [7].</p>
<p>Abilities for creating and reasoning with abstract representations are fundamental to robust generalization, so it is essential to understand the extent to which LLMs have achieved such abilities. In this paper we report on experiments evaluating GPT-4 on tasks in ConceptARC [10], a collection of analogy puzzles that test general abstract reasoning capabilities. We show that by providing prompts with more detailed instructions and a simple solved example, GPT-4's performance on a text version of this corpus improves substantially above that reported in previous work, but remains substantially below that of humans and of special-purpose algorithms for solving tasks in this domain. Because humans are given these tasks in a visual modality, it has been argued that it would only be fair to compare humans with multimodal (rather than text-only) LLMs. We</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of ARC tasks from [2]. Each task has a set of demonstration input-output pairs that illustrate an abstract grid-transformation rule, and a test input. The solver's challenge is to generate a new grid that results from applying the abstract rule to the test input. (Figure is from [10]; best viewed in color.)
perform this comparison using GPT-4V, the multimodal extension of the GPT-4, and show that this particular multimodal LLM performs substantially worse than the text-only version. These results reinforce the conclusion that a large gap in basic abstract reasoning still remains between humans and state-of-the-art AI systems.</p>
<h1>2 The Abstraction and Reasoning Corpus</h1>
<p>Chollet (2019) proposed the Abstraction and Reasoning Corpus (ARC) as a benchmark for fairly evaluating such abilities in both humans and machines. ARC consists of 1,000 manually created analogy puzzles ('tasks"), each of which contains a small number (typically 2-4) demonstrations of transformations on grids, and a "test input" grid. The task for the solver is to induce the abstract rule underlying the demonstrations and to apply that rule to the test input to generate a transformed grid. Figure 1 gives three examples of ARC tasks.</p>
<p>According to Chollet, the prior knowledge needed for solving these tasks is a subset of the core knowledge systems hypothesized to be innate in humans [12]-namely, objectness, numerosity, and basic geometry and topology. Notably, Chollet intentionally omitted knowledge of language or other "learned symbols" from the required prior knowledge, which helps avoid any "approximate retrieval" and pattern matching based on prior training data that might underlie LLMs' apparent</p>
<p>success on language-based reasoning tasks. Instead, ARC is meant to capture the crux of abstract reasoning: inducing general rules or patterns from small numbers of examples and applying these flexibly to new, previously unseen situations.</p>
<p>Chollet published 800 ARC tasks and kept the remaining 200 as a "hidden" test set [2]. 100 of these hidden tasks were used as an test set for a challenge on the Kaggle platform [6]. The firstplace program from the Kaggle challenge solved $21 \%$ of these hidden tasks, and an ensemble of the first- and second-place programs solved $31 \%$. This remains the highest-achieved accuracy on ARC to date. The winning programs on Kaggle used program-synthesis methods that searched over combinations of manually defined, primitive grid operations in order to find combinations that correctly map inputs to outputs in the task demonstrations. The authors of the winning programs acknowledge that such methods are not likely to generalize well $[3,17]$ and ARC remains largely unsolved by any AI methods.</p>
<p>Several groups have tested LLMs on subsets of ARC tasks [4, 9, 14, 19], using different prompting formats, and generally found the best accuracy using straightforward text versions of tasks to be around $10-12 \%$. Limited studies of human performance on subsets of ARC tasks has shown much higher accuracies (e.g., $84 \%$ in [5]).</p>
<h1>3 Previous Evaluation of LLMs using the ConceptARC Benchmark</h1>
<p>Moskvichev et al. (2023) noted two problems with the original ARC corpus. First, they claimed, many of the tasks are quite difficult, even for humans, and this difficulty might be a barrier to progress in developing AI systems that reason in this domain. Second, and most important, ARC does not offer systematic evaluation of understanding of particular core concepts; even if a system can solve an individual ARC task, that does not necessarily mean that the system has a robust understanding of the underlying concepts. To address these issues, Moskvichev et al. created a new benchmark in the ARC domain, ConceptARC, whose tasks are intentionally designed to be easy for humans and, moreover, whose 480 tasks are organized as systematic variations of particular core spatial and semantic concepts, such as Top and Bottom, Inside and Outside, Center, and Same and Different. Each concept group contains 30 tasks, each of which instantiates the concept in a different way, and with differing degrees of abstraction. Moskvichev et al.'s claim was that high performance over these various instantiations of a given concept indicates a robust understanding of, and ability to reason abstractly about, the underlying concept.</p>
<p>Moskvichev et al. gave these tasks to human participants on the Amazon Mechanical Turk and Prolific platforms. They also tested the two winning programs from the Kaggle ARC challenge as well as GPT-4 on all 480 tasks. They found that human performance substantially exceeded that of machines on all concept groups in the corpus; in particular, the overall accuracy of humans was $91 \%$, compared to the first-place Kaggle program's accuracy of $52 \%$, and GPT-4's accuracy of $25 \%$ (using temperature 0.5). The authors concluded that " $[t]$ he generally high accuracies of humans on each concept indicates successful generalization over the different variations in each given concept group. In contrast, the much lower accuracies of programs we tested indicates a lack of ability to generalize over the variations in a concept group, and thus a failure to develop the abstractions that ARC is meant to test."</p>
<p>Moskvichev et al.'s evalation of GPT-4 had two important limitations: (1) the prompt format that they used was overly simple and might not have communicated enough about the task; and (2) it might not be fair to compare the performance of humans, who are presented with a visual version of each task, with LLMs, which are given a text-only version of the task.</p>
<p>Here we address both of these limitations, through two sets of experiments. In the first set of experiments, we evaluate the text-only version of GPT-4 on text versions of ConceptARC tasks using a much more expressive prompt that includes both instructions and an example of a solved task, making this a one-shot rather than zero-shot induction problem. In the second set of experiments, we evaluate GPT-4V, the multimodal version of GPT-4, on the visual version of the simplest ConceptARC tasks, giving it a similar prompt as in the first set of experiments but using images rather than text to represent tasks.</p>
<h1>4 Experiments Evaluating Text-Only GPT-4 on ConceptARC Tasks</h1>
<p>To evaluate the text-only version of GPT-4, we adapted the prompt used in a recent study [14]. The exact prompt is given in the appendix, Section 7.1. This prompt provides detailed instructions about the task as well as an example of a solved task. If GPT-4 responds with an incorrect answer, we repeat a request for it to supply a different answer, for a maximum of three guesses, which is standard for ARC evaluations [6]. If a correct answer is generated within these three guesses, the task is considered to be solved.</p>
<p>We used this prompting method with OpenAI's API to test GPT-4 ${ }^{1}$ on all 480 ConceptARC tasks ( 30 per each of the 16 concept groups $)^{2}$, first with GPT-4's temperature set to zero and then with temperature set to 0.5 , to test the effects of temperature on performance. The accuracies (fraction of solved tasks within each concept group as well as fraction of solved tasks overall) are given in Table 1, along with the human accuracies from [10]. Note that, like GPT-4, humans are given three guesses for each task.</p>
<p>For GPT-4, our more detailed, one-shot prompting method resulted in a higher accuracy overall, 0.33 for both temperature settings, than the simple zero-shot method used in [10], which reported 0.19 for temperature zero and 0.25 for temperature 0.5 . However, GPT-4's performance remains well below the high performance of humans, supporting the conclusion that, even with more informative prompting, the system lacks basic abstract reasoning abilities tested by this corpus.</p>
<h2>5 Experiments Evaluating GPT-4V on Minimal ConceptARC Tasks</h2>
<p>We did a second set of experiments to test the hypothesis that GPT-4V, the multimodal version of GPT-4, would obtain higher performance than the text-only version. Due to the costs of running such an experiment on the 480 tasks in ConceptARC, we decided to establish a baseline for com-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Concept</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;">GPT-4 Temp $=0$</th>
<th style="text-align: center;">GPT-4 Temp $=0.5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Above and Below</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">Center</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">Clean Up</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">Complete Shape</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">Copy</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;">Extend To Boundary</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;">Extract Objects</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: center;">Filled and Not Filled</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">Horizontal and Vertical</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">Inside and Outside</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">Move To Boundary</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;">Order</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">Same and Different</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">Top and Bottom 2D</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: center;">Top and Bottom 3D</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;">All concepts</td>
<td style="text-align: center;">$\mathbf{0 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracies of humans and GPT-4 (with temperature 0 and 0.5) on each concept group (30 tasks) and over all concepts ( 480 tasks) in ConceptARC, using the prompt given in the appendix, Section 7.1. The results on humans are from [10].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;">GPT-4 Temp $=0$</th>
<th style="text-align: center;">GPT-4 Temp $=0.5$</th>
<th style="text-align: center;">GPT-4V Zero-Shot</th>
<th style="text-align: center;">GPT-4V One-Shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.23</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracies of humans, GPT-4 (with Temperature 0 and 0.5), and GPT-4V (zero- and one-shot prompting) on minimal tasks over all concepts ( 48 tasks) in ConceptARC.
paring text-only GPT-4 and GPT-4V using the minimal tasks created by Moskvichev et al. (2023). For each of their 16 concept groups, Moskvichev et al. created three minimal tasks-extremely simple instantiations of the concept. They used these tasks as "attention checks" in their human studies, to make sure that human participants were paying attention and understood the basic idea of the tasks. Participants who failed at solving two or more minimal tasks (out of three given) were excluded from the study. Moskvichev et al. reported that 12 out of 482 participants were excluded on this basis.</p>
<p>Though it was not reported in their paper, we used the data from Moskvichev et al. to evaluate overall human performance on minimal tasks from all 482 participants in their study. We also ran the text-only version of GPT-4 on these tasks, using the same prompting method described in the previous section. As shown in Table 2, the fraction of humans correctly solving the 48 minimal tasks was 0.95 , and GPT-4's accuracies were 0.69 (temperature 0 ) and 0.65 (temperature 0.5 ). The difference between GPT-4's performance on these minimal problems and on the non-minimal tasks (Table 1) underline the simplicity of the minimal tasks compared to those in the regular corpus. ${ }^{3}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We explored various approaches to presenting the minimal tasks to GPT-4V ${ }^{4}$ : displaying all inputoutput pairs of a single task within one image, using a separate image for each input-output pair, and providing each input and each output grid as an individual image. Only the last approach yielded correct solutions during our preliminary investigations, and is thus the approach we adopted. Furthermore, when presented with an image, GPT-4V was unable to consistently translate the visual grid to a text representation, including both color names and numeric encodings. Therefore, to mitigate errors involved in mapping the intended output grid to a text representation, we requested only a natural language description of the grid, along with its dimensions.</p>
<p>We aimed to quantify the influence of visual representations on performance by maintaining consistency with our text-only evaluation. The same prompting method and example of a solved task was used, modified only by substituting text representations of grids with visual counterparts. This prompt, along with the variations from the text-only approach, are given in the appendix, Section 7.2 .</p>
<p>GPT-4V often included descriptions of an abstract transformation rule as part of its solution. Our assessment focused exclusively on the accuracy of the model's test output grid descriptions. In certain cases, the model accurately described the output grid despite identifying an incorrect abstract rule, which we classified as a success. On the other hand, we classified as failures instances in which the model correctly identified the abstract rule but failed to accurately describe the output grid.</p>
<p>The results of our experiments with GPT-4V on minimal tasks are given in the last two columns of Table 2. The visual one-shot prompt resulted in an accuracy of 0.23 over the 48 minimal tasks, compared with 0.69 for the (temperature 0 ) text-only counterpart. Notably, several of GPT-4V's unsuccessful output grid descriptions incorporated details from the solved example, suggesting that including an sample solved task in the prompt may have had a negative impact on performance. Consequently, we extended our evaluation of GPT-4V to include a zero-shot setting.</p>
<p>The zero-shot prompt is also given in the appendix, Section 7.2. Unlike the one-shot setting, the evaluation was done using OpenAI's web application. This required a slightly different prompting method, where the model was able to respond with observations after receiving each demonstration input-output pair, and before receiving the test input grid to provide its solution. The zero-shot prompting method resulted in an accuracy of 0.25 on minimal tasks, solving one additional task compared to the one-shot setting, and with an overlap of seven tasks successfully solved in both settings.</p>
<p>While we tested GPT-4V only on miminal tasks, We expect that GPT-4V's overall performance would be similarly considerably worse than the text-only version on the much-harder non-minimaltask corpus.</p>
<h1>6 Conclusion</h1>
<p>In this paper we extended work described in [10] on evaluating the abstract reasoning capabilities of GPT-4, using the ConceptARC corpus, which systematically tests abstraction abilities using basic</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>core concepts. Moskvichev et al. found that GPT-4 had substantially worse performance than both humans and the first-place program in the Kaggle-ARC challenge on these tasks. However, the prompting method they used was overly simple, and they experimented only with text versions of the tasks. Here, we performed evaluations using a more informative, one-shot prompt for text versions of tasks, and experimented with similar zero- and one-shot prompts for the multimodal case in which task-grids were given as images. We found that our more informative one-shot prompt improved GPT-4's performance in the text case, but its performance remained well below that of humans and the special-purpose Kaggle-ARC program. We also found that giving minimal tasks as images to the multimodal GPT-4 resulted in substantially worse performance than in the textonly case. Our results support the hypothesis that GPT-4, perhaps the most capable "general" LLM currenly available, is still not able to robustly form abstractions and reason about basic core concepts in contexts not previously seen in its training data. It is possible that other methods of prompting or task representation would increase the performance of GPT-4 and GPT-4V; this is a topic for future research.</p>
<h1>Acknowledgments</h1>
<p>This material is based in part upon work supported by the National Science Foundation under Grant No. 2139983. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. This work has also been supported by the Templeton World Charity Foundation, Inc. (funder DOI 501100011730) under the grant https://doi.org/10.54224/20650.</p>
<h2>References</h2>
<p>[1] F. Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
[2] F. Chollet. The Abstraction and Reasoning Corpus (ARC). https://github.com/fchollet/ ARC, 2023. Accessed 2023-11-09.
[3] A. de Miquel Bleier. Finishing 2nd in Kaggle's Abstraction and Reasoning Challenge. https://blog.jovian.com/ finishing-2nd-in-kaggles-abstraction-and-reasoning-challenge-24e59c07b50a, 2020. Accessed 2023-11-09.
[4] G. Gendron, Q. Bao, M. Witbrock, and G. Dobbie. Large language models are not strong abstract reasoners. arXiv preprint arXiv:2305.19555, 2023.
[5] A. Johnson, W. K. Vong, B. M. Lake, and T. M. Gureckis. Fast and flexible: Human program induction in abstract reasoning tasks. arXiv preprint arXiv:2103.05823, 2021.
[6] Kaggle.com. Kaggle Abstraction and Reasoning Challenge. https://www.kaggle.com/c/ abstraction-and-reasoning-challenge, 2020. Accessed 2023-11-09.
[7] S. Kambhampati. Can llms really reason and plan? Communications of the ACM, 2023. September 12, 2023.</p>
<p>[8] R. T. McCoy, S. Yao, D. Friedman, M. Hardy, and T. L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023.
[9] S. Mirchandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. G. Arenas, K. Rao, D. Sadigh, and A. Zeng. Large language models as general pattern machines. In Seventh Conference on Robot Learning (CoRL 2023), 2023.
[10] A. Moskvichev, V. V. Odouard, and M. Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. Transactions On Machine Learning Research, 2023.
[11] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840-854, 2022.
[12] E. S. Spelke and K. D. Kinzler. Core knowledge. Developmental Science, 10(1):89-96, 2007.
[13] C. M. Walker and A. Gopnik. Toddlers infer higher-order relational principles in causal learning. Psychological Science, 25(1):161-169, 2014.
[14] R. Wang, E. Zelikman, G. Poesia, Y. Pu, N. Haber, and N. D. Goodman. Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660, 2023.
[15] T. Webb, K. J. Holyoak, and H. Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):1526-1541, 2023.
[16] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022 .
[17] J. S. Wind. 1st place solution + code and official documentation. https://www.kaggle.com/ competitions/abstraction-and-reasoning-challenge/discussion/154597, 2020. Accessed 2023-11-09.
[18] Z. Wu, L. Qiu, A. Ross, E. Akyürek, B. Chen, B. Wang, N. Kim, J. Andreas, and Y. Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477, 2023.
[19] Y. Xu, W. Li, P. Vaezipoor, S. Sanner, and E. B. Khalil. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. arXiv preprint arXiv:2305.18354, 2023.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) A task from the ConceptARC corpus. (b) The corresponding prompt used in [10] to give to GPT-4. (Image is from [10]; best viewed in color.)</p>
<h1>7 Appendix</h1>
<h3>7.1 Prompts for Text-only GPT-4</h3>
<p>In their evaluation of GPT-4, Moskvichev et al. (2023) translated ConceptARC tasks into text representations used in prompts like the one shown in Figure 2. Here the 10 possible colors are encoded as integers, and each row of a grid is encoded as a list of integers inside square brackets.</p>
<p>Figure 3 shows an example of the prompt we used (adapted from[14]) in testing text-only GPT-4. We use the format required by the OpenAI API. The symbol "#" indicates comments not given in the actual prompt. We used the same encoding for colors and grid rows as in [10].</p>
<h3>7.2 Prompts for GPT-4V</h3>
<p>Figure 4 shows the adapted prompt used to test GPT-4V in the one shot setting, for the same example used in Figure 3. Differences from the text-only prompt are highlighted in red text.</p>
<p>Figure 5 shows an example of the prompts used to test GPT-4V in the zero-shot setting. This evaluation was conducted using OpenAI's web application and thus all messages are sent as the 'user' role, , interspersed with the model providing a response after each message.</p>
<h1>GENERAL INSTRUCTIONS</h1>
<p>[System]
You will be given a list of input-output pairs, labeled "Case 0", "Case 1", and so on. Each input and output is a grid of
numbers representing a visual grid. There is a SINGLE rule that transforms each input grid to the corresponding output grid.</p>
<p>The pattern may involve counting or sorting objects (e.g. sorting by size), comparing numbers (e.g. which shape or symbol
appears the most? Which is the largest object? Which objects are the same size?), or repeating a pattern for a fixed number
of time.</p>
<p>There are other concepts that may be relevant.
- Lines, rectangular shapes
- Symmetries rotations, translations.
- Shape upscaling or downscaling, elastic distortions.
- Containing / being contained / being inside or outside of a perimeter.
- Drawing lines, connecting points, orthogonal projections.
- Copying, repeating objects.</p>
<p>You should treat cells with 0 as empty cells (backgrounds).</p>
<p>Please generate the Output grid that corresponds to the last given Input grid, using the transformation rule you induced
from the previous input-output pairs.</p>
<h1>EXAMPLE SOLVED TASK</h1>
<p>[User]
Case 0:
Input:
[0 0 0 0 0 0] [0 2 2 0 0 0] [0 2 2 0 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0]
Output:
[0 0 0 0 0 0] [0 0 2 2 0 0] [0 0 2 2 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0]</p>
<p>Case 1:
Input:
[0 0 0 0 0 0] [0 0 4 4 0 0] [0 0 4 4 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0]
Output:
[0 0 0 0 0 0] [0 0 0 4 4 0] [0 0 0 4 4 0] [0 0 0 0 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0]</p>
<p>Case 2:
Input:
[0 0 0 0 0 0] [0 3 0 0 0 0] [0 3 3 3 0 0] [0 0 0 3 0 0] [0 0 0 0 0 0] [0 0 0 0 0 0]
Output:
[0 0 0 0 0 0] [0 0 3 0 0 0] [0 0 3 3 3 0] [0 0 0 0 3 0] [0 0 0 0 0 0] [0 0 0 0 0 0]</p>
<p>Case 3:
Input:
[0 0 0 0 0 0] [0 0 0 0 0 0] [0 0 6 0 0 0] [0 0 6 0 0 0] [0 0 6 6 0 0] [0 0 0 0 0 0]</p>
<p>[Assistant]
Output:
[0 0 0 0 0 0] [0 0 0 0 0 0] [0 0 0 6 0 0] [0 0 0 6 0 0] [0 0 0 6 6 0] [0 0 0 0 0 0]</p>
<h1>QUANT TASK</h1>
<p>[User]
Case 0:
Input:
[0 0 0 0 0 0] [0 3 0 3 0 0] [0 3 3 0 0 0] [0 0 3 3 3 0] [0 3 3 0 3 0] [0 0 0 0 0 0]
Output:
[0 0 0 0 0 0] [0 3 3 3 3 0] [0 3 3 3 3 0] [0 3 3 3 3 0] [0 3 3 3 3 0] [0 0 0 0 0 0]</p>
<p>Case 1:
Input:
[0 0 0 0 0 0] [0 4 0 0 4 0] [0 4 4 4 0 0] [0 4 0 4 4 0] [0 0 4 0 4 0] [0 0 0 0 0 0]
Output:
[0 0 0 0 0 0] [0 4 4 4 4 0] [0 4 4 4 4 4 0] [0 4 4 4 4 0] [0 4 4 4 4 0] [0 0 0 0 0 0]</p>
<p>Case 2:
Input:
[0 0 0 0 0 0] [0 6 0 0 6 6] [0 6 0 6 0 0] [0 6 0 6 0 0] [0 0 6 0 6 6] [0 0 6 6 6 0]</p>
<p>[Assistant]
Output:</p>
<h1>IF THE SYSTEM RETURNS WRONG ANSWER (REPEAT UP TO TWO TIMES)</h1>
<p>[User]
Your answer does not solve the puzzle. Try again.</p>
<p>[Assistant]
I apologize for my mistake. Here is a better answer:
Output:</p>
<p>Figure 3: Example of the prompt used to test text-only GPT-4 on ConceptARC tasks. The symbol
"#" indicates comments not given in the actual prompt.</p>
<h1>GENERAL INSTRUCTIONS</h1>
<p>[System]
You will be given a list of input-output pairs, labeled "Case 0", "Case 1", and so on. Each input and output is a 2D visual grid containing colored cells. There is a SINGLE rule that transforms each input grid to the corresponding output grid.</p>
<p>The pattern may involve counting or sorting objects (e.g. sorting by size), comparing numbers (e.g. which shape or symbol appears the most? Which is the largest object? Which objects are the same size?), or repeating a pattern for a fixed number of time.</p>
<p>There are other concepts that may be relevant.
- Lines, rectangular shapes
- Symmetries rotations, translations.
- Shape upscaling or downscaling, elastic distortions.
- Containing / being contained / being inside or outside of a perimeter.
- Drawing lines, connecting points, orthogonal projections.
- Copying, repeating objects.</p>
<p>You should treat black cells as empty cells (backgrounds).
Please provide a language description of the Output grid that corresponds to the last given Input grid, using the transformation rule you induced from the previous input-output pairs. Please include the dimensions of the Output grid as part of your description.
# EXAMPLE SOLVED TASK
[User]
Case 0:
Input:
[Image of input grid]
Output:
[Image of output grid]
Case 1:
Input:
[Image of input grid]
Output:
[Image of output grid]
Case 2:
Input:
[Image of input grid]
Output:
[Image of output grid]
Case 3:
Input:
[Image of input grid]
[Assistant]
Output:
The output should contain the same pink object found in the input, shifted one cell to the right. The dimensions of the output grid should be 6x6.
# QUERY TASK
[User]
Case 0:
Input:
[Image of input grid]
Output:
[Image of output grid]
Case 1:
Input:
[Image of input grid]
Output:
[Image of output grid]
Case 2:
Input:
[Image of input grid]
[Assistant]
Output:
# IF THE SYSTEM RETURNS WRONG ANSWER (REPEAT UP TO TWO TIMES)
[User]
Your answer does not solve the puzzle. Try again.
[Assistant]
I apologize for my mistake. Here is a better answer:
Output:</p>
<p>Figure 4: Example of the prompt used to test GPT-4V in the one-shot setting. The symbol "#" indicates comments not given in the actual prompt. Red text signifies differences from the prompt used to test text-only GPT-4, as provided in Figure 3.</p>
<h1>GENERAL INSTRUCTIONS</h1>
<p>[Dear]
You will be given a sequence of images containing several cases of input-output pairs. Each input and output is a 2D visual grid containing colored cells. There is a SINGLE pattern that transforms each input grid to the corresponding output grid.</p>
<p>The pattern may involve counting or sorting objects (e.g. sorting by size), comparing numbers (e.g. which shape or symbol appears the most? Which is the largest object? Which objects are the same size?), or repeating a pattern for a fixed number of time.</p>
<p>There are other concepts that may be relevant.
- Lines, rectangular shapes
- Symmetries rotations, translations.
- Shape upscaling or downscaling, elastic distortions.
- Containing / being contained / being inside or outside of a perimeter.
- Drawing lines, connecting points, orthogonal projections.
- Copying, repeating objects.</p>
<p>You should treat black cells as empty cells (backgrounds).</p>
<p>I will present each example input-output pair one at a time. The input will always be the first attached image and the output will always be the second attached image. You can note down any observations about the possible transformation after seeing each input-output example. Once I have shown you all the input-output examples, I will present you a test input grid and ask you to provide a language description the single pattern that transformed each example input and a language description of the test output grid when using the pattern on the test input grid. Please include the dimensions of the output grid as part of your description.</p>
<h1>QUESY</h1>
<p>[Dear]
[Image of input grid] [Image of output grid]
[Dear]
[Image of input grid] [Image of output grid]
[Dear]
[Image of input grid]
Here is the test input grid. Provide a language description the single pattern that transformed each example input and a language description of the test output grid when using the pattern on the test input grid. Please include the dimensions of the output grid as part of your description.</p>
<h1>IF THE SYSTEM RETURNS WRONG ANSWER (REPEAT UP TO TWO TIMES)</h1>
<p>[Dear]
Incorrect. Try again.</p>
<p>Figure 5: Example of the prompts used to test GPT-4V in the zero-shot setting. The symbol "#" indicates comments not given in the actual prompt.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We used the version of GPT-4V available in November, 2023 (gpt4-vision-preview). This version does not allow a manual temperature setting and the documentation does not specify the default temperature.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
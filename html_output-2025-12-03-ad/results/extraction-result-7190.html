<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7190 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7190</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7190</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-a638f4e1d36c7cae05d950fce38fe6fffed90044</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a638f4e1d36c7cae05d950fce38fe6fffed90044" target="_blank">Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Natural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> A trainable neural planning component is introduced that can generate effective plans several orders of magnitude faster than the original planner and a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts is introduced.</p>
                <p><strong>Paper Abstract:</strong> We follow the step-by-step approach to neural data-to-text generation proposed by Moryossef et al (2019), in which the generation process is divided into a text planning stage followed by a plan realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner; (2) we incorporate typing hints that improve the model’s ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7190.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7190.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan linearization (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-plan tree linearization via DFS traversals</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text representation that maps the input directed labeled graph into a sequence of sentence plans: each sentence plan is a tree that is linearized into a string using depth-first-search (DFS) traversals; these linearized plans are fed to an NMT realizer with a copy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Step-by-step: Separating planning from realization in neural data-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sentence-plan tree linearization (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each input graph is split into sentence subgraphs; each sentence plan is a tree whose nodes/edges are linearized into a string by performing a DFS traversal over the sentence subgraph. Linearization uses structural tokens (opening/closing brackets) and indexed entity symbols; the resulting sequence is a plain token sequence consumed by a seq2seq realizer (OpenNMT) with a copy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; hierarchical (tree linearization); described as lossy w.r.t. type information</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Undirected/unordered depth-first-search traversals (DFS) over sentence subgraphs; original exhaustive system enumerated all DFS traversals; training uses a plan-to-DFS mapping to recover traversal sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>data-to-text / graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenNMT seq2seq with copy mechanism (realizer); exhaustive planner (original) used enumerative DFS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural sequence-to-sequence NMT decoder (OpenNMT) with copy mechanism used to translate linearized sentence-plan strings into natural language; in the original system plans were produced by an exhaustive enumerator over splits and DFS traversals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; manual correctness counts (expressed/omitted/wrong/over-generation); entity-order coverage; beam-based entity-sequence match rates; planning time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU: reported 46.882 (Exhaustive Planner baseline in this paper's comparisons). Manual correctness (Exhaustive+Verify): Expressed=426, Omitted=0, Wrong=14, Over-generation=0 (from manual sample). Entity-order coverage and exact entity-sequence match rates reported in related experiments (see other entries). Planning time for original exhaustive planner: ~250s for 7-edge graphs (from earlier system).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a structured, verifiable intermediate representation that guarantees coverage of input facts (plans themselves are faithful to the graph), enabling a two-stage training/separate realizer which improves controllability and correctness relative to monolithic end-to-end models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linearized strings are lossy with respect to symbol typing (they do not explicitly mark which tokens are entities vs relations), which makes it harder for the decoder to learn to copy entities vs verbalize relations; exhaustive enumeration of all plans is computationally infeasible for larger graphs (exponential growth).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to end-to-end neural generation, the plan-linearization approach improves faithfulness and controllability and reduces hallucinations. Compared to the neural/truncated-DFS planner introduced in this paper, the exhaustive enumerator gives slightly higher BLEU in some settings but is orders of magnitude slower and does not scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7190.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7190.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Typed linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Typed sentence-plan linearization with explicit symbol-type embeddings (S/E/R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the linearized sentence-plan representation in which each token receives an explicit type embedding indicating structural, entity, or relation token (S, E, or R) concatenated to the token embedding to help the decoder differentiate copying vs verbalization behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Typed linearized sentence-plan (S/E/R annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The same tree-to-string linearization as above, but each input symbol's embedding is concatenated with one of three learned type vectors: S for structural tokens (brackets), E for entity symbols, and R for relation symbols; this extra typed signal is provided to the seq2seq realizer.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; hierarchical (tree linearization) with additional token-type annotations; reduces information loss (less lossy)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>DFS-based linearization of sentence-plan trees, with appended type embeddings per symbol (S/E/R).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>data-to-text / graph-to-text generation (improved generalization to unseen entities/relations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenNMT seq2seq realizer with copy mechanism; same realizer but inputs augmented with type embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq NMT decoder (OpenNMT) augmented by input tokens whose embeddings are concatenated with one of three learned type vectors (S, E, R).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; coverage of entities in outputs; entity-order adherence; unseen / seen split analyses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Typing caused a negligible drop in overall BLEU (small, not precisely enumerated here), but reduced the number of texts not containing all entities by 18% (seen) and 16% (unseen); for texts that contained all entities, order-violations were reduced by 46% (seen) and 35% (unseen). Exact BLEU change described as small/negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Explicit typing improved generalization to unseen relations and entities (substantially better coverage and order adherence on unseen split) by making it easier for the decoder to learn different behaviors for entities (copy) vs relations (verbalize).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Slight overall BLEU drop attributed to freer but correct verbalizations of relations; typing requires extraction/assignment of types (here via DBPedia for pronoun restrictions), and does not fully eliminate all errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to untyped linearization, typed linearization yields substantially better behavior on unseen entities/relations at little cost to BLEU; compared to end-to-end learned typing via position alone, explicit type vectors make the distinction clearer and help generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7190.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7190.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Truncated-DFS plan encoding (neural planner)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random truncated DFS traversal plan encoding using a trainable controller</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear-time, verifiable planner that produces sentence-plan linearizations by performing a sequence of truncated DFS traversals guided by a neural controller; each truncated DFS traversal yields one sentence plan which is linearized incrementally during traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Truncated-DFS traversal sequence (neural planner output)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is converted to a sequence of truncated DFS traversals: pick a non-zero-degree start node, perform a (possibly stochastic) DFS that may 'POP' before visiting all children (truncation), remove traversed edges, repeat until no edges remain. Each traversal is converted incrementally into a linearized sentence-plan string (same linearization used by the realizer). Actions (choose-node, traverse-via-edge, POP) are represented and scored by a neural controller.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (ordered sequence of traversal actions producing sentence-plan strings); verifiable w.r.t. content coverage</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Transition-based truncated DFS traversals guided by a neural classifier/controller (inspired by transition-based parsing). Action set is dynamic and includes traverse-via-edge, choose-node-i, and POP; decisions scored by dot-products between action representations and an LSTM state over plan symbols generated so far.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>data-to-text / graph-to-text generation (fast plan generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural planner (LSTM-based controller) + OpenNMT seq2seq realizer; action representations use learned embeddings for nodes, edges and a POP vector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Planner: a transition-based neural classifier operating over graph-structured embeddings (node embeddings x_n, relation embeddings r_i, projected edge vectors E([x_i; r; x_j]), node vectors V([...])) and an LSTM state over generated plan symbols; actions scored with dot-product. Realizer: OpenNMT seq2seq with copy mechanism. REG uses BERT LM for scoring referring expression candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU; planning time (seconds per graph); manual correctness (expressed/omitted/wrong/over-generated); beam-based entity-sequence match rates</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU: small drop from exhaustive planner (46.882 -> 46.506). Planning time: ~0.0025s for a 7-edge graph (5 orders of magnitude faster than exhaustive planner's ~250s). Manual correctness (Neural+Verify): Expressed=405, Omitted=2, Wrong=30, Over-generation=4 (from manual sample). Beam entity-sequence exact-match improvements: with beam=5, for seen entities 99.82% have exact match vs 98.48% at 1-best; for unseen entities 72.3% with beam=5 vs 58.06% at 1-best.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables linear-time plan generation that scales to larger graphs and enables sampling/generative plan diversity; combined with verification and REG yields fast end-to-end generation with high faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Slight drop in BLEU compared to exhaustive planner; planner decisions are neural and can be stochastic (requires greedy or sampling strategies); in rare cases failing to find matching realization candidates requires re-sampling plans. Action scoring relies on learned embeddings and LSTM state which may propagate errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to exhaustive enumeration, the truncated-DFS neural planner is orders of magnitude faster with only a small BLEU penalty; compared to end-to-end models it preserves verifiability of plans and allows fast, scalable plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Step-by-step: Separating planning from realization in neural data-to-text generation <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from dbpedia data <em>(Rating: 2)</em></li>
                <li>Char2char generation with reranking for the e2e nlg challenge <em>(Rating: 1)</em></li>
                <li>Globally coherent text generation with neural checklist models <em>(Rating: 1)</em></li>
                <li>Neuralreg: An end-to-end approach to referring expression generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7190",
    "paper_id": "paper-a638f4e1d36c7cae05d950fce38fe6fffed90044",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Plan linearization (DFS)",
            "name_full": "Sentence-plan tree linearization via DFS traversals",
            "brief_description": "A graph-to-text representation that maps the input directed labeled graph into a sequence of sentence plans: each sentence plan is a tree that is linearized into a string using depth-first-search (DFS) traversals; these linearized plans are fed to an NMT realizer with a copy mechanism.",
            "citation_title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
            "mention_or_use": "use",
            "representation_name": "Sentence-plan tree linearization (DFS)",
            "representation_description": "Each input graph is split into sentence subgraphs; each sentence plan is a tree whose nodes/edges are linearized into a string by performing a DFS traversal over the sentence subgraph. Linearization uses structural tokens (opening/closing brackets) and indexed entity symbols; the resulting sequence is a plain token sequence consumed by a seq2seq realizer (OpenNMT) with a copy mechanism.",
            "representation_type": "sequential; hierarchical (tree linearization); described as lossy w.r.t. type information",
            "encoding_method": "Undirected/unordered depth-first-search traversals (DFS) over sentence subgraphs; original exhaustive system enumerated all DFS traversals; training uses a plan-to-DFS mapping to recover traversal sequence.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "data-to-text / graph-to-text generation",
            "model_name": "OpenNMT seq2seq with copy mechanism (realizer); exhaustive planner (original) used enumerative DFS",
            "model_description": "Neural sequence-to-sequence NMT decoder (OpenNMT) with copy mechanism used to translate linearized sentence-plan strings into natural language; in the original system plans were produced by an exhaustive enumerator over splits and DFS traversals.",
            "performance_metric": "BLEU; manual correctness counts (expressed/omitted/wrong/over-generation); entity-order coverage; beam-based entity-sequence match rates; planning time",
            "performance_value": "BLEU: reported 46.882 (Exhaustive Planner baseline in this paper's comparisons). Manual correctness (Exhaustive+Verify): Expressed=426, Omitted=0, Wrong=14, Over-generation=0 (from manual sample). Entity-order coverage and exact entity-sequence match rates reported in related experiments (see other entries). Planning time for original exhaustive planner: ~250s for 7-edge graphs (from earlier system).",
            "impact_on_training": "Provides a structured, verifiable intermediate representation that guarantees coverage of input facts (plans themselves are faithful to the graph), enabling a two-stage training/separate realizer which improves controllability and correctness relative to monolithic end-to-end models.",
            "limitations": "Linearized strings are lossy with respect to symbol typing (they do not explicitly mark which tokens are entities vs relations), which makes it harder for the decoder to learn to copy entities vs verbalize relations; exhaustive enumeration of all plans is computationally infeasible for larger graphs (exponential growth).",
            "comparison_with_other": "Compared to end-to-end neural generation, the plan-linearization approach improves faithfulness and controllability and reduces hallucinations. Compared to the neural/truncated-DFS planner introduced in this paper, the exhaustive enumerator gives slightly higher BLEU in some settings but is orders of magnitude slower and does not scale.",
            "uuid": "e7190.0",
            "source_info": {
                "paper_title": "Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Typed linearization",
            "name_full": "Typed sentence-plan linearization with explicit symbol-type embeddings (S/E/R)",
            "brief_description": "An extension of the linearized sentence-plan representation in which each token receives an explicit type embedding indicating structural, entity, or relation token (S, E, or R) concatenated to the token embedding to help the decoder differentiate copying vs verbalization behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Typed linearized sentence-plan (S/E/R annotations)",
            "representation_description": "The same tree-to-string linearization as above, but each input symbol's embedding is concatenated with one of three learned type vectors: S for structural tokens (brackets), E for entity symbols, and R for relation symbols; this extra typed signal is provided to the seq2seq realizer.",
            "representation_type": "sequential; hierarchical (tree linearization) with additional token-type annotations; reduces information loss (less lossy)",
            "encoding_method": "DFS-based linearization of sentence-plan trees, with appended type embeddings per symbol (S/E/R).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "data-to-text / graph-to-text generation (improved generalization to unseen entities/relations)",
            "model_name": "OpenNMT seq2seq realizer with copy mechanism; same realizer but inputs augmented with type embeddings",
            "model_description": "Seq2seq NMT decoder (OpenNMT) augmented by input tokens whose embeddings are concatenated with one of three learned type vectors (S, E, R).",
            "performance_metric": "BLEU; coverage of entities in outputs; entity-order adherence; unseen / seen split analyses",
            "performance_value": "Typing caused a negligible drop in overall BLEU (small, not precisely enumerated here), but reduced the number of texts not containing all entities by 18% (seen) and 16% (unseen); for texts that contained all entities, order-violations were reduced by 46% (seen) and 35% (unseen). Exact BLEU change described as small/negligible.",
            "impact_on_training": "Explicit typing improved generalization to unseen relations and entities (substantially better coverage and order adherence on unseen split) by making it easier for the decoder to learn different behaviors for entities (copy) vs relations (verbalize).",
            "limitations": "Slight overall BLEU drop attributed to freer but correct verbalizations of relations; typing requires extraction/assignment of types (here via DBPedia for pronoun restrictions), and does not fully eliminate all errors.",
            "comparison_with_other": "Compared to untyped linearization, typed linearization yields substantially better behavior on unseen entities/relations at little cost to BLEU; compared to end-to-end learned typing via position alone, explicit type vectors make the distinction clearer and help generalization.",
            "uuid": "e7190.1",
            "source_info": {
                "paper_title": "Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Truncated-DFS plan encoding (neural planner)",
            "name_full": "Random truncated DFS traversal plan encoding using a trainable controller",
            "brief_description": "A linear-time, verifiable planner that produces sentence-plan linearizations by performing a sequence of truncated DFS traversals guided by a neural controller; each truncated DFS traversal yields one sentence plan which is linearized incrementally during traversal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Truncated-DFS traversal sequence (neural planner output)",
            "representation_description": "The graph is converted to a sequence of truncated DFS traversals: pick a non-zero-degree start node, perform a (possibly stochastic) DFS that may 'POP' before visiting all children (truncation), remove traversed edges, repeat until no edges remain. Each traversal is converted incrementally into a linearized sentence-plan string (same linearization used by the realizer). Actions (choose-node, traverse-via-edge, POP) are represented and scored by a neural controller.",
            "representation_type": "sequential (ordered sequence of traversal actions producing sentence-plan strings); verifiable w.r.t. content coverage",
            "encoding_method": "Transition-based truncated DFS traversals guided by a neural classifier/controller (inspired by transition-based parsing). Action set is dynamic and includes traverse-via-edge, choose-node-i, and POP; decisions scored by dot-products between action representations and an LSTM state over plan symbols generated so far.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "data-to-text / graph-to-text generation (fast plan generation)",
            "model_name": "Neural planner (LSTM-based controller) + OpenNMT seq2seq realizer; action representations use learned embeddings for nodes, edges and a POP vector",
            "model_description": "Planner: a transition-based neural classifier operating over graph-structured embeddings (node embeddings x_n, relation embeddings r_i, projected edge vectors E([x_i; r; x_j]), node vectors V([...])) and an LSTM state over generated plan symbols; actions scored with dot-product. Realizer: OpenNMT seq2seq with copy mechanism. REG uses BERT LM for scoring referring expression candidates.",
            "performance_metric": "BLEU; planning time (seconds per graph); manual correctness (expressed/omitted/wrong/over-generated); beam-based entity-sequence match rates",
            "performance_value": "BLEU: small drop from exhaustive planner (46.882 -&gt; 46.506). Planning time: ~0.0025s for a 7-edge graph (5 orders of magnitude faster than exhaustive planner's ~250s). Manual correctness (Neural+Verify): Expressed=405, Omitted=2, Wrong=30, Over-generation=4 (from manual sample). Beam entity-sequence exact-match improvements: with beam=5, for seen entities 99.82% have exact match vs 98.48% at 1-best; for unseen entities 72.3% with beam=5 vs 58.06% at 1-best.",
            "impact_on_training": "Enables linear-time plan generation that scales to larger graphs and enables sampling/generative plan diversity; combined with verification and REG yields fast end-to-end generation with high faithfulness.",
            "limitations": "Slight drop in BLEU compared to exhaustive planner; planner decisions are neural and can be stochastic (requires greedy or sampling strategies); in rare cases failing to find matching realization candidates requires re-sampling plans. Action scoring relies on learned embeddings and LSTM state which may propagate errors.",
            "comparison_with_other": "Compared to exhaustive enumeration, the truncated-DFS neural planner is orders of magnitude faster with only a small BLEU penalty; compared to end-to-end models it preserves verifiability of plans and allows fast, scalable plan generation.",
            "uuid": "e7190.2",
            "source_info": {
                "paper_title": "Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "The webnlg challenge: Generating text from dbpedia data",
            "rating": 2
        },
        {
            "paper_title": "Char2char generation with reranking for the e2e nlg challenge",
            "rating": 1
        },
        {
            "paper_title": "Globally coherent text generation with neural checklist models",
            "rating": 1
        },
        {
            "paper_title": "Neuralreg: An end-to-end approach to referring expression generation",
            "rating": 1
        }
    ],
    "cost": 0.01034825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation</h1>
<p>Amit Moryossef ${ }^{\dagger}$ Ido Dagan ${ }^{\dagger}$ Yoav Goldberg ${ }^{\ddagger \ddagger}$<br>amitmoryossef@gmail.com, {dagan, yogo}@cs.biu.ac.il<br>${ }^{\dagger}$ Bar Ilan University, Ramat Gan, Israel<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence</p>
<h4>Abstract</h4>
<p>We follow the step-by-step approach to neural data-to-text generation we proposed in Moryossef et al. (2019), in which the generation process is divided into a text-planning stage followed by a plan-realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner; (2) we incorporate typing hints that improve the model's ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate.</p>
<h2>1 Introduction</h2>
<p>In the data-to-text generation task (D2T), the input is data encoding facts (e.g., a table, a set of tuples, or a small knowledge graph), and the output is a natural language text representing those facts. ${ }^{1}$ In neural D2T, the common approaches train a neural end-to-end encoder-decoder system that encodes the input data and decodes an output text. In recent work (Moryossef et al., 2019) we proposed to adopt ideas from "traditional" language generation approaches (i.e. Reiter and Dale (2000); Walker et al. (2007); Gatt and Krahmer (2017)) that separate the generation into a planning stage that determines the order and structure of the expressed facts, and a realization stage that maps the plan to natural language text. We show that by breaking the task this way, one can achieve the same fluency</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of neural generation systems while being able to better control the form of the generated text and to improve its correctness by reducing missing facts and "hallucinations", common in neural systems.</p>
<p>In this work we adopt the step-by-step framework of Moryossef et al. (2019) and propose four independent extensions that improve aspects of our original system: we suggest a new plan generation mechanism, based on a trainable-yetverifiable neural decoder, that is orders of magnitude faster than the original one (§3); we use knowledge of the plan structure to add typing information to plan elements. This improves the system's performance on unseen relations and entities (§4); the separation of planning from realizations allows the incorporation of a simple output verification heuristic that drastically improves the correctness of the output (§5); and finally we incorporate a post-processing referring expression generation (REG) component, as proposed but not implemented in our previous work, to improve the naturalness of the resulting output (§6).</p>
<h2>2 Step-by-step Generation</h2>
<p>We provide a brief overview of the step-by-step system. See Moryossef et al. (2019) for further details. The system works in two stages. The first stage (planning) maps the input facts (encoded as a directed, labeled graph, where nodes represent entities and edges represent relations) to text plans, while the second stage (realization) maps the text plans to natural language text.</p>
<p>The text plans are a sequence of sentence plans-each of which is a tree- representing the ordering of facts and entities within the sentence. In other words, the plans determine the separation of facts into sentences, the ordering of sentences, and the ordering of facts and entities within each sentence. This stage is completely verifiable:</p>
<p>the text plans are guaranteed to faithfully encode all and only the facts from the input. The realization stage then translates the plans into natural language sentences, using a neural sequence-to-sequence system, resulting in fluent output.</p>
<h2>3 Fast and Verifiable Planner</h2>
<p>The data-to-plan component in Moryossef et al. (2019) exhaustively generates all possible plans, scores them using a heuristic, and chooses the highest scoring one for realization. While this is feasible with the small input graphs in the WebNLG challenge (Colin et al., 2016), it is also very computationally intensive, growing exponentially with the input size. We propose an alternative planner which works in linear time in the size of the graph and remains verifiable: generated plans are guaranteed to represent the input faithfully.</p>
<p>The original planner works by first enumerating over all possible splits into sentences (subgraphs), and for each sub-graph enumerating over all possible undirected, unordered, Depth First Search (DFS) traversals, where each traversal corresponds to a sentence plan. Our planner combines these into a single process. It works by performing a series of what we call random truncated DFS traversals. In a DFS traversal, a node is visited, then its children are visited recursively in order. Once all children are visited, the node "pops" back to the parent. In a random truncated traversal, the choice of which children to visit next, as well as whether to go to the next children or to "pop", is non-deterministic (in practice, our planner decides by using a neural-network controller). Popping at a node before visiting all its children truncates the DFS: further descendants of that node will not be visited in this traversal. It behaves as a DFS on a graph where edges to these descendants do not exist. Popping the starting node terminates the traversal.</p>
<p>Our planner works by choosing a node with a non-zero degree and performing a truncated DFS traversal from that node. Then, all edges visited in the traversal are removed from the input graph, and the process repeats (performing another truncated DFS) until no more edges remain. Each truncated DFS traversal corresponds to a sentence plan, following the DFS-to-plan procedure of Moryossef et al. (2019): the linearized plan is generated incrementally at each step of the
traversal. This process is linear in the number of edges in the graph.</p>
<p>At training time, we use the plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller, choosing which action to perform at each step. At test time, we use the controller to guide the truncated DFS process. This mechanism is inspired by transition based parsing (Nivre and McDonald, 2008). The action set at each stage is dynamic. During traversal, it includes the available children at each stage and POP. Before traversals, it includes a choose-i action for each available node $n_{i}$. We assign a score to each action, normalize with softmax, and train to choose the desired one using cross-entropy loss. At test time, we either greedily choose the best action, or we can sample plans by sampling actions according to their assigned probabilities.
Feature Representation and action scoring. Each graph node $n_{i}$ corresponds to an entity $x_{n_{i}}$, and has an associated embedding vector $\mathbf{x}<em _mathbf_i="\mathbf{i">{\mathbf{n}</em>}}}$. Each relation $r_{i}$ is associated with an embedding vector $\mathbf{r<em k="k">{\mathbf{i}}$. Each labeled input graph edge $e</em>}=\left(n_{i}, r_{\ell}, n_{j}\right)$ is represented as a projected concatenated vector $\mathbf{e<em _mathbf_n="\mathbf{n">{\mathbf{k}}=$ $\mathbf{E}\left(\mathbf{x}</em><em _ell="\ell">{\mathbf{i}}} ; \mathbf{r}</em>} ; \mathbf{x<em _mathbf_j="\mathbf{j">{\mathbf{n}</em>}}}\right)$, where $\mathbf{E}$ is a projection matrix. Finally, each node $n_{i}$ is then represented as a vector $\mathbf{n<em _mathbf_n="\mathbf{n">{\mathbf{i}}=\mathbf{V}\left[\mathbf{x}</em><em e__j="e_{j">{\mathbf{i}}} ; \sum</em>} \in \pi(i)} \mathbf{e<em e__j="e_{j">{\mathbf{j}} ; \sum</em>} \in \pi^{-1}(i)} \mathbf{e<em i="i">{\mathbf{j}}\right]$, where $\pi(i)$ and $\pi^{-1}(i)$ are the incoming and outgoing edges from node $n</em>}$. The traverse-to-child-via-edge- $e_{j}$ action is represented as $\mathbf{e<em _mathbf_i="\mathbf{i">{\mathbf{j}}$, choose-node-i is represented as $\mathbf{n}</em>$ is a learned vector. The score for an action $a$ at time $t$ is calculated as a dot-product between the action representation and the LSTM state over the symbols generated in the plan so far. Thus, each decision takes into account the immediate surrounding of the node in the graph, and the plan structure generated so far.
Speed On a 7 edges graph, the planner of Moryossef et al. (2019) takes an average of 250 seconds to generate a plan, while our planner takes 0.0025 seconds, 5 orders of magnitude faster.}}$ and pop-to-node-i is represented as $\mathbf{n}_{\mathbf{i}}+\mathbf{p}$ where $\mathbf{p</p>
<h2>4 Incorporating typing information for unseen entities and relations</h2>
<p>In Moryossef et al. (2019), the sentence plan trees were linearized into strings that were then fed to a neural machine translation decoder (OpenNMT) (Klein et al., 2017) with a copy mecha-</p>
<p>nism. This linearization process is lossy, in the sense that the linearized strings do not explicitly distinguish between symbols that represent entities (e.g., BARACK_OBAMA) and symbols that represent relations (e.g., works-for). While this information can be deduced from the position of the symbol within the structure, there is a benefit in making it more explicit. In particular, the decoder needs to act differently when decoding relations and entities: entities are copied, while relations need to be verbalized. By making the typing information explicit to the decoder, we make it easier for it to generalize this behavior distinction and apply it also for unseen entities and relations. We thus expect the typing information to be especially useful for the unseen part of the evaluation set.</p>
<p>We incorporate typing information by concatenating to the embedding vector of each input symbol one of three embedding vectors, $\mathbf{S}, \mathbf{E}$ or $\mathbf{R}$, where $\mathbf{S}$ is concatenated to structural elements (opening and closing brackets), $\mathbf{E}$ to entity symbols and $\mathbf{R}$ to relation symbols.</p>
<h2>5 Output verification</h2>
<p>While the plan generation stage is guaranteed to be faithful to the input, the translation process from plans to text is based on a neural seq2seq model and may suffer from known issues with such models: hallucinating facts that do not exist in the input, repeating facts, or dropping facts. While the clear mapping between plans and text helps to reduce these issues greatly, the system in Moryossef et al. (2019) still has $2 \%$ errors of these kinds.</p>
<p>Existing approaches: soft encouragement via neural modules. Recent work in neural text generation and summarization attempt to address these issues by trying to map the textual outputs back to structured predicates, and comparing these predicates to the input data. Kiddon et al. (2016) uses a neural checklist model to avoid the repetition of facts and improve coverage. Agarwal et al. (2018) generate $k$-best output candidates with beam search, and then try to map each candidate output back to the input structure using a reverse seq2seq model trained on the same data. They then select the highest scoring output candidate that best translates back to the input. Mohiuddin and Joty (2019) reconstructs the input in training time, by jointly learning a back-translation model and enforcing the back-translation to re-
construct the input. Both of these approaches are "soft" in the sense that they crucially rely on the internal dynamics or on the output of a neural network module that may or may not be correct.</p>
<p>Our proposal: explicit verification. The separation between planning and realization provided by the step-by-step framework allows incorporating a robust and straightforward verification step, that does not rely on brittle information extraction procedures or trust neural network models.</p>
<p>The plan-to-text generation handles each sentence individually and translates entities as copy operations. We thus have complete knowledge of the generated entities and their locations. We can then assess the correctness of an output sentence by comparing ${ }^{2}$ its sequence of entities to the entity sequence in the corresponding sentence plan, which is guaranteed to be complete.</p>
<p>We then decode $k$-best outputs and rerank them based on their correctness scores, tie-breaking using model scores. We found empirically that, with a beam of size 5 we find at least one candidate with an exact match to the plan's entity sequence in $99.82 \%$ of the cases for seen entities and relations compared to $98.48 \%$ at 1-best, and $72.3 \%$ for cases of unseen entities and relations compared to $58.06 \%$ at 1-best. In the remaining cases, we set the system to continue searching by trying other plans, by going down the list of plans (when using the exhaustive planner of Moryossef et al. (2019)) or by sampling a new plan (when using the linear time planner suggested in this paper).</p>
<h2>6 Referring Expressions</h2>
<p>The step-by-step system generates entities by first generating an indexed entity symbols, and then lexicalizing each symbol to the string associated with this entity in the input structure (i.e., all occurrences of the entity 11TH MISSISSIPPI INFANTRY MONUMENT will be lexicalized with the full name rather than "it" or "the monument"). This results in correct but somewhat unnatural structures. In contrast, end-to-end neural generation systems are trained on text that includes referring expressions, and generate them naturally as part of the decoding process, resulting in natural looking text. However, the generated referring expressions are sometimes incorrect. Moryossef et al. (2019) suggests the possibility of handling</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>this with a post-processing referring-expression generation step (REG). Here, we propose a concrete REG module and demonstrate its effectiveness. One option is to use a supervised REG module (Ferreira et al., 2018), that is trained to lexicalize in-context mentions. Such an approach is suboptimal for our setup as it is restricted to the entities and contexts it seen in training, and is prone to error on unseen entities and contexts.</p>
<p>Our REG solution lexicalizes the first mention of each entity as its associated string and attempts to generate referring expressions to subsequent mentions. The generated referring expressions can take the form "PRON", "X" or "THE X" where Pron is a pronoun ${ }^{3}$, and X is a word appearing in the entity's string (allowing, e.g., John, or the monument). We also allow referring to its entity with its entire associated string. We restrict the set of allowed pronouns for each entity according to its type (male, female, plural-animate, unknown-animate, inanimate). ${ }^{4}$ We then take, for each entity mention individually, the referring expression that receives the best language model score in context, using a strong unsupervised neural LM (BERT (Devlin et al., 2018)). The system is guaranteed to be correct in the sense that it will not generate wrong pronouns. It also has failure modes: it is possible for the system to generate ambiguous referring expressions (e.g., John is Bob's father. He works as a nurse.), and may lexicalize Boston University as Boston. We find that the second kind of mistake is rare as it is handled well by the language model. It can also be controlled by manually restricting the set of possible referring expression to each entity. Similarly, it is easy to extend the system to support other lexicalizations of entities by extending the sets of allowed lexicalizations (for example, supporting abbreviations, initials or nicknames) either as user-supplied inputs or using heuristics.</p>
<h2>7 Evaluation and Results</h2>
<p>We evaluate each of the introduced components separately. Tables listing their interactions are available in the appendix. The appendix also lists some qualitative outputs. The main trends that we observe are:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- The new planner causes a small drop in BLEU, but is orders of magnitude faster ( $\S 7.1$ ).
- Typing information causes a negligible drop in BLEU overall, but improves results substantially for the unseen portion of the dataset (§7.2).
- The verification step is effective at improving the faithfulness of the output, practically eliminating omitted and overgenerated facts, reducing the number of wrong facts, and increasing the number of correctly expressed facts. This is based on both manual and automatic evaluations. (§7.3).
- The referring expression module is effective, with an intrinsic correctness of $92.2 \%$. It substantially improves BLEU scores. (§7.4).</p>
<p>Setup We evaluate on the WebNLG dataset (Colin et al., 2016), comparing to the step-bystep systems described in Moryossef et al. (2019), which are state of the art. Due to randomness inherent in neural training, our reported automatic evaluation measures are based on an average of 5 training runs of each system (neural planner and neural realizer), each run with a different random seed.</p>
<h3>7.1 Neural Planner vs Exhaustive Planner</h3>
<p>We compare the exhaustive planner from Moryossef et al. (2019) to our neural planner, by replacing the planner component in the Moryossef et al. (2019) system. Moving to the neural planner exhibits a small drop in BLEU ( 46.882 dropped to 46.506). However, figure 1 indicates 5 orders of magnitude ( $100,000 \mathrm{x}$ ) speedup for graphs with 7 edges, and a linear growth in time for number of edges compared to exponential time for the exhaustive planner.</p>
<h3>7.2 Effect of Type Information</h3>
<p>We repeat the coverage experiment in (Moryossef et al., 2019), counting the number of output texts that contain all the entities in the input graph, and, of these text, counting the ones in which the entities appear in the exact same order as the plan. Incorporating typing information reduced the number of texts not containing all entities by $18 \%$ for the seen part of the test set, and $16 \%$ for the unseen part. Moreover, for the text containing all</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Average (+std) planning time (seconds) for different graph sizes, for exhaustive vs neural planner.</p>
<p>|  | Moryosef et al
StrongNeural | Moryosef et al
BestPlan | Exhaustive
+Verify | Neural
+Verify |
| :-- | :-- | :-- | :-- | :-- |
| Expressed | 360 | 417 | $\mathbf{4 2 6}$ | 405 |
| Omitted | 41 | 6 | $\mathbf{0}$ | 2 |
| Wrong | 39 | 17 | $\mathbf{1 4}$ | 30 |
| Over-generation | 29 | 3 | $\mathbf{0}$ | 4 |
| Wrong REG | - | - | $\mathbf{0}$ | 3 |</p>
<p>Table 1: Manual correctness analysis comparing our systems with the ones from Moryossef et al. (2019).
entities, the number of texts that did not follow the plan's entity order is reduced by $46 \%$ for the seen part of the test set, and by $35 \%$ for the unseen part. We also observe a small drop in BLEU scores, which we attribute to some relations being verbalized more freely (though correctly).</p>
<h3>7.3 Effect of Output Verification</h3>
<p>The addition of output verification resulted in negligible changes in BLEU, reinforcing that automatic metrics are not sensitive enough to output accuracy. We thus performed manual analysis, following the procedure in Moryossef et al. (2019). We manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts. ${ }^{5}$ We compare to the StrongNeural and BestPlan systems from Moryossef et al. (2019). Results in Table 1 indicate that the effectiveness of the verification process in ensuring correct output, reducing the already small number of ommited and overgenerated facts to 0 (with the exhaustive planner) and keeping it small (with the fast neural planner).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>7.4 Referring Expression Module</h3>
<p>Intrinsic evaluation of the REG module. We manually reviewed 1,177 pairs of entities and referring expressions generated by the system. We find that $92.2 \%$ of the generated referring expressions refer to the correct entity.</p>
<p>From the generated expressions, 325 (27.6\%) were pronouns, 192 (16.3\%) are repeating a onetoken entity as is, and 505 ( $42.9 \%$ ) are generating correct shortening of a long entity. In 63 (5.6\%) of the cases the system did not find a good substitute and kept the entire entity intact. Finally, 92 $(7.82 \%)$ are wrong referrals. Overall, $73.3 \%$ of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions.</p>
<p>Effect on BLEU scores. As can be seen in Table 2, using the REG module increases BLEU scores for both the exhaustive and the neural planner.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">-</th>
<th style="text-align: left;">REG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Exhaustive Planner</td>
<td style="text-align: left;">46.882</td>
<td style="text-align: left;">47.338</td>
</tr>
<tr>
<td style="text-align: left;">Neural Planner</td>
<td style="text-align: left;">46.506</td>
<td style="text-align: left;">47.124</td>
</tr>
</tbody>
</table>
<p>Table 2: Effect of the REG component on BLEU score</p>
<h2>8 Conclusions</h2>
<p>We adopt the planning-based neural generation framework of Moryossef et al. (2019) and extend it to be orders of magnitude faster and produce more correct and more fluent text. We conclude that these extensions not only improve the system of Moryossef et al. (2019) but also highlight the flexibility and advantages of the step-by-step framework for text generation.</p>
<h2>Acknowledgements</h2>
<p>This work was supported in part by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1) and by a grant from Reverso and Theo Hoffenberg.</p>
<h2>References</h2>
<p>Shubham Agarwal, Marc Dymetman, and Eric Gaussier. 2018. Char2char generation with reranking for the e2e nlg challenge. arXiv preprint arXiv:1811.05826.</p>
<p>Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th International Natural Language Generation conference, pages 163167.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Thiago Castro Ferreira, Diego Moussallem, Ákos Kádár, Sander Wubben, and Emiel Krahmer. 2018. Neuralreg: An end-to-end approach to referring expression generation. arXiv preprint arXiv:1805.08093.</p>
<p>Albert Gatt and Emiel Krahmer. 2017. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. CoRR, abs/1703.09902.</p>
<p>Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi. 2016. Globally coherent text generation with neural checklist models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 329-339.</p>
<p>Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810.</p>
<p>Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707-710.</p>
<p>Tasnim Mohiuddin and Shafiq Joty. 2019. Revisiting adversarial autoencoder for unsupervised word translation with cycle consistency and improved training. arXiv preprint arXiv:1904.04116.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. arXiv preprint arXiv:1904.03396.</p>
<p>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.</p>
<p>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.</p>
<p>Marilyn A Walker, Amanda Stent, François Mairesse, and Rashmi Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research, 30:413-456.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} \mathrm{~A}$ wrong fact is one in which a fact exists between the two entities, but the text implies a different fact from the graph, while over-generated is either repeating facts or inventing new facts.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
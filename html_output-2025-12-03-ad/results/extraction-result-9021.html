<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9021 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9021</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9021</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-271038844</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.03859v2.pdf" target="_blank">Anthropocentric bias in language model evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence ("auxiliary oversight"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent ("mechanistic chauvinism"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9021.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9021.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hu&Frank 2024 (subject-verb agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auxiliary task demands mask the capabilities of smaller language models (Hu & Frank, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study comparing two evaluation methods for syntactic sensitivity (subject–verb agreement): (1) metalinguistic grammaticality judgments elicited via prompting and (2) direct probability estimation on minimal pairs; finds the direct-probability approach often yields better measurements of underlying syntactic competence in LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Auxiliary task demands mask the capabilities of smaller language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (multiple sizes, unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic family of pre-trained language models evaluated across sizes; exact architectures not specified in this paper's summary of Hu & Frank.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Subject–verb agreement sensitivity (syntactic judgment)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary grammaticality task using minimal pairs (e.g., "every child has studied" vs "every child have studied") to measure sensitivity to syntactic features such as agreement; two evaluation modes compared: metalinguistic judgment prompts vs direct probability comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Direct probability estimation typically yields better performance than metalinguistic prompting across model sizes (qualitative statement; no numeric accuracy reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>When evaluated via direct probability estimation, LLMs perform better (relative to their metalinguistic-prompted performance); paper argues metalinguistic prompts impose auxiliary demands that underestimate competence.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Two evaluation paradigms contrasted: (a) metalinguistic prompting asking the model to explicitly indicate which of two sentences is better (and analyzing outputs and probabilities over labels), and (b) direct probability estimation comparing model-assigned log-probabilities for grammatical vs ungrammatical minimal pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No numeric human baselines reported in the discussed summary; primary caveat is that metalinguistic prompting introduces an auxiliary task demand (the ability to produce explicit grammaticality judgments) that is conceptually independent from syntactic competence and can depress measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anthropocentric bias in language model evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9021.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9021.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lampinen 2023 (nested grammar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans (Lampinen, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Case study comparing human subjects and language models on recursively nested grammatical constructions; highlights the role of mismatched experimental context (humans given training/feedback vs models evaluated zero-shot) and shows that providing a few examples to models can close or reverse the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLMs (evaluated zero-shot and with few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models evaluated under different prompting conditions; architectures and sizes not specified in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Recursively nested grammatical structure comprehension (long-distance subject–verb agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Sentences with long-distance subject–verb dependencies in embedded clauses intended to stress hierarchical syntactic capabilities (recursive nesting).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Zero-shot evaluation: models performed worse than humans (per Lakretz et al.); with modest few-shot/example-based prompts meant to match human orienting context, LLMs perform as well as or better than humans (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human subjects (in Lakretz et al. 2022) outperformed models under original experimental comparisons where humans received more instructions/training/feedback; exact numeric values not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance gap attributed to mismatched auxiliary task demands; when models are given similar orienting context/examples as humans, they match or exceed human performance on deep nesting.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Key manipulation was matching the orienting context: humans received instructions, training, and feedback, while initial model evaluations were zero-shot; Lampinen provided few-shot prompts (examples) to models to replicate human task context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Highlights confound of mismatched experimental conditions (auxiliary oversight). Paper summary does not provide numeric accuracies; interpretation emphasizes the need for species-fair / condition-matched comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anthropocentric bias in language model evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9021.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9021.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Webb et al. 2023 (analogy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent analogical reasoning in large language models (Webb et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study reporting that LLMs can match or surpass average human performance on a variety of novel analogical reasoning tasks (including letter-string analogies), suggesting emergent analogical reasoning capabilities in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent analogical reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (unspecified in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Group of language models evaluated on novel analogical tasks; specific models and sizes are not detailed in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Novel analogical reasoning tasks (including letter-string analogies)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks requiring mapping relations across pairs (e.g., [ABC]→[ABE], [MNO]→[?]) designed to probe relational/analogical reasoning rather than rote pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to match or surpass average human performance on various tasks (qualitative claim; no numeric accuracy provided in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Average human performance used as a baseline in Webb et al.; specific numeric baselines not provided in the summary here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs can match or exceed average human performance on many analogical tasks, indicating emergent abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Analogy tasks presented as novel problems (e.g., letter-string analogies). Later discussion notes counterfactual variants (permuted alphabet) that change task demands.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Lewis & Mitchell (2024) and follow-up debates highlight counterfactual task variants (permuted alphabet) where LLM performance deteriorates, suggesting auxiliary task demands (e.g., counting indices) can mask underlying analogical competence; whether deterioration indicates lack of general analogical ability or an auxiliary counting limitation is disputed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anthropocentric bias in language model evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9021.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9021.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lewis&Mitchell 2024 (permuted alphabet)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models (Lewis & Mitchell, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper showing that LLM performance on analogical reasoning tasks can deteriorate under counterfactual variants (permuted alphabet), arguing this as evidence that LLMs may lack general analogical competence; counters argue that the permuted task imposes counting/indexing demands that are auxiliary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models evaluated on both standard and counterfactual analogical tasks; precise architectures not listed in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Counterfactual/permute-alphabet letter-string analogy task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Variant of letter-string analogy tasks in which the alphabet is permuted, requiring models to track positions/indices (a counting-like auxiliary demand) rather than rely on familiar symbolic orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performance deteriorates on the permuted-alphabet variant relative to the standard task (qualitative; no numeric scores given here).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans can typically perform the task (especially when visually referencing the permuted alphabet); exact numeric baselines not provided in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs show degraded performance on the permuted variant whereas humans are less affected given visual aids; suggests asymmetry due to auxiliary counting demands for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Permuted alphabet requires tracking indices/counting; authors interpret the drop as indicating lack of general analogical competence, while Webb et al. (2024) counter that the permuted task imposes an auxiliary counting demand that LLMs are poorly equipped to handle without tools (e.g., Python interpreter) or sophisticated prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Debate centers on whether the permuted variant tests the same underlying competence or simply introduces an auxiliary counting task that masks latent analogical skill; no definitive numeric comparisons included in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anthropocentric bias in language model evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9021.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9021.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Huginn-0125 (Winogrande example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Huginn-0125 (example model used in this paper's Figure 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model example demonstrated in this paper: increasing test-time latent/recurrent iterations shifts log-likelihoods from an incorrect completion to the correct one on a Winogrande commonsense reasoning item, illustrating test-time computational bottlenecks revealing latent competence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Huginn-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrently evaluated language model (or an LLM with test-time latent iteration capability) used in the paper to illustrate latent-reasoning/test-time-scaling effects; specific architecture and training details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Winogrande (commonsense pronoun resolution / fill-in-the-blank)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Commonsense reasoning benchmark (Winogrande) requiring contextual/world-knowledge inference to resolve anaphora or select the correct completion for a sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>With few latent/recurrent steps: model assigns higher log-likelihood to incorrect completion "jars"; as number of latent steps increases, log-likelihood for the correct completion "glow sticks" increases and becomes dominant (qualitative shift in ranked probabilities; no numeric accuracy provided).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not directly compared to humans in the figure; demonstrates that apparent model failure can be reversed by allocating more test-time compute, revealing latent competence.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Manipulation: vary number of latent/recurrent inference iterations (test-time scaling) and observe changes in token-level log-likelihoods for candidate completions on a Winogrande example.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Single illustrative example (Figure 1) rather than broad benchmark statistics; no numeric human baselines or aggregate model accuracies reported here—serves to exemplify the computational-bottleneck auxiliary factor rather than to provide a full quantitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anthropocentric bias in language model evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9021.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9021.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Muennighoff et al. 2025 (LRMs / test-time scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S1: Simple test-time scaling (Muennighoff et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work describing 'large reasoning models' (LRMs) trained to exploit test-time scaling (e.g., varying number of chain-of-thought or 'thinking' tokens); reports that increasing the thinking-token budget improves performance on difficult math and science benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>S1: Simple test-time scaling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1 (examples given)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models that undergo an additional post-training stage (supervised fine-tuning on chain-of-thought examples and RL on verifiable rewards) and are designed to benefit from dynamic allocation of compute at inference time by varying the number of thinking tokens generated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Difficult math and science benchmarks (unspecified in summary)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Benchmarks testing formal-domain reasoning (math, science) where extended chain-of-thought or increased test-time compute can improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported improvement in performance as the 'thinking' token budget is increased on difficult math and science benchmarks (qualitative; no numeric figures provided in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not directly compared to human baselines in this paper's discussion; main finding is that performance can switch from failure to success within the same model by allocating more test-time compute.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>LRMs are fine-tuned on chain-of-thought style examples and RL; at inference time, the number of intermediate 'thinking' tokens (test-time compute) is modulated to measure performance changes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Shows that test-time compute budget is an auxiliary factor; poor performance under low thinking-token budgets should not be taken as evidence of lack of competence. Exact benchmarks, numeric scores, and human comparisons are not reported in the summary here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Anthropocentric bias in language model evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Auxiliary task demands mask the capabilities of smaller language models <em>(Rating: 2)</em></li>
                <li>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans <em>(Rating: 2)</em></li>
                <li>Can Transformers Process Recursive Nested Constructions, Like Humans? <em>(Rating: 1)</em></li>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 1)</em></li>
                <li>S1: Simple test-time scaling <em>(Rating: 2)</em></li>
                <li>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 1)</em></li>
                <li>The Expressive Power of Transformers with Chain of Thought <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9021",
    "paper_id": "paper-271038844",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Hu&Frank 2024 (subject-verb agreement)",
            "name_full": "Auxiliary task demands mask the capabilities of smaller language models (Hu & Frank, 2024)",
            "brief_description": "Study comparing two evaluation methods for syntactic sensitivity (subject–verb agreement): (1) metalinguistic grammaticality judgments elicited via prompting and (2) direct probability estimation on minimal pairs; finds the direct-probability approach often yields better measurements of underlying syntactic competence in LMs.",
            "citation_title": "Auxiliary task demands mask the capabilities of smaller language models",
            "mention_or_use": "mention",
            "model_name": "various LLMs (multiple sizes, unspecified)",
            "model_description": "Generic family of pre-trained language models evaluated across sizes; exact architectures not specified in this paper's summary of Hu & Frank.",
            "model_size": null,
            "test_battery_name": "Subject–verb agreement sensitivity (syntactic judgment)",
            "test_description": "Binary grammaticality task using minimal pairs (e.g., \"every child has studied\" vs \"every child have studied\") to measure sensitivity to syntactic features such as agreement; two evaluation modes compared: metalinguistic judgment prompts vs direct probability comparison.",
            "llm_performance": "Direct probability estimation typically yields better performance than metalinguistic prompting across model sizes (qualitative statement; no numeric accuracy reported here).",
            "human_baseline_performance": null,
            "performance_comparison": "When evaluated via direct probability estimation, LLMs perform better (relative to their metalinguistic-prompted performance); paper argues metalinguistic prompts impose auxiliary demands that underestimate competence.",
            "experimental_details": "Two evaluation paradigms contrasted: (a) metalinguistic prompting asking the model to explicitly indicate which of two sentences is better (and analyzing outputs and probabilities over labels), and (b) direct probability estimation comparing model-assigned log-probabilities for grammatical vs ungrammatical minimal pairs.",
            "limitations_or_caveats": "No numeric human baselines reported in the discussed summary; primary caveat is that metalinguistic prompting introduces an auxiliary task demand (the ability to produce explicit grammaticality judgments) that is conceptually independent from syntactic competence and can depress measured performance.",
            "uuid": "e9021.0",
            "source_info": {
                "paper_title": "Anthropocentric bias in language model evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Lampinen 2023 (nested grammar)",
            "name_full": "Can language models handle recursively nested grammatical structures? A case study on comparing models and humans (Lampinen, 2023)",
            "brief_description": "Case study comparing human subjects and language models on recursively nested grammatical constructions; highlights the role of mismatched experimental context (humans given training/feedback vs models evaluated zero-shot) and shows that providing a few examples to models can close or reverse the gap.",
            "citation_title": "Can language models handle recursively nested grammatical structures? A case study on comparing models and humans",
            "mention_or_use": "mention",
            "model_name": "unspecified LLMs (evaluated zero-shot and with few-shot prompting)",
            "model_description": "Language models evaluated under different prompting conditions; architectures and sizes not specified in this paper's summary.",
            "model_size": null,
            "test_battery_name": "Recursively nested grammatical structure comprehension (long-distance subject–verb agreement)",
            "test_description": "Sentences with long-distance subject–verb dependencies in embedded clauses intended to stress hierarchical syntactic capabilities (recursive nesting).",
            "llm_performance": "Zero-shot evaluation: models performed worse than humans (per Lakretz et al.); with modest few-shot/example-based prompts meant to match human orienting context, LLMs perform as well as or better than humans (qualitative).",
            "human_baseline_performance": "Human subjects (in Lakretz et al. 2022) outperformed models under original experimental comparisons where humans received more instructions/training/feedback; exact numeric values not reported here.",
            "performance_comparison": "Performance gap attributed to mismatched auxiliary task demands; when models are given similar orienting context/examples as humans, they match or exceed human performance on deep nesting.",
            "experimental_details": "Key manipulation was matching the orienting context: humans received instructions, training, and feedback, while initial model evaluations were zero-shot; Lampinen provided few-shot prompts (examples) to models to replicate human task context.",
            "limitations_or_caveats": "Highlights confound of mismatched experimental conditions (auxiliary oversight). Paper summary does not provide numeric accuracies; interpretation emphasizes the need for species-fair / condition-matched comparisons.",
            "uuid": "e9021.1",
            "source_info": {
                "paper_title": "Anthropocentric bias in language model evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Webb et al. 2023 (analogy)",
            "name_full": "Emergent analogical reasoning in large language models (Webb et al., 2023)",
            "brief_description": "Study reporting that LLMs can match or surpass average human performance on a variety of novel analogical reasoning tasks (including letter-string analogies), suggesting emergent analogical reasoning capabilities in LLMs.",
            "citation_title": "Emergent analogical reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "various LLMs (unspecified in this paper's summary)",
            "model_description": "Group of language models evaluated on novel analogical tasks; specific models and sizes are not detailed in this paper's summary.",
            "model_size": null,
            "test_battery_name": "Novel analogical reasoning tasks (including letter-string analogies)",
            "test_description": "Tasks requiring mapping relations across pairs (e.g., [ABC]→[ABE], [MNO]→[?]) designed to probe relational/analogical reasoning rather than rote pattern matching.",
            "llm_performance": "Reported to match or surpass average human performance on various tasks (qualitative claim; no numeric accuracy provided in this paper's summary).",
            "human_baseline_performance": "Average human performance used as a baseline in Webb et al.; specific numeric baselines not provided in the summary here.",
            "performance_comparison": "LLMs can match or exceed average human performance on many analogical tasks, indicating emergent abilities.",
            "experimental_details": "Analogy tasks presented as novel problems (e.g., letter-string analogies). Later discussion notes counterfactual variants (permuted alphabet) that change task demands.",
            "limitations_or_caveats": "Lewis & Mitchell (2024) and follow-up debates highlight counterfactual task variants (permuted alphabet) where LLM performance deteriorates, suggesting auxiliary task demands (e.g., counting indices) can mask underlying analogical competence; whether deterioration indicates lack of general analogical ability or an auxiliary counting limitation is disputed.",
            "uuid": "e9021.2",
            "source_info": {
                "paper_title": "Anthropocentric bias in language model evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Lewis&Mitchell 2024 (permuted alphabet)",
            "name_full": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models (Lewis & Mitchell, 2024)",
            "brief_description": "Paper showing that LLM performance on analogical reasoning tasks can deteriorate under counterfactual variants (permuted alphabet), arguing this as evidence that LLMs may lack general analogical competence; counters argue that the permuted task imposes counting/indexing demands that are auxiliary.",
            "citation_title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
            "mention_or_use": "mention",
            "model_name": "various LLMs (unspecified)",
            "model_description": "Language models evaluated on both standard and counterfactual analogical tasks; precise architectures not listed in this paper's summary.",
            "model_size": null,
            "test_battery_name": "Counterfactual/permute-alphabet letter-string analogy task",
            "test_description": "Variant of letter-string analogy tasks in which the alphabet is permuted, requiring models to track positions/indices (a counting-like auxiliary demand) rather than rely on familiar symbolic orderings.",
            "llm_performance": "Performance deteriorates on the permuted-alphabet variant relative to the standard task (qualitative; no numeric scores given here).",
            "human_baseline_performance": "Humans can typically perform the task (especially when visually referencing the permuted alphabet); exact numeric baselines not provided in this summary.",
            "performance_comparison": "LLMs show degraded performance on the permuted variant whereas humans are less affected given visual aids; suggests asymmetry due to auxiliary counting demands for LLMs.",
            "experimental_details": "Permuted alphabet requires tracking indices/counting; authors interpret the drop as indicating lack of general analogical competence, while Webb et al. (2024) counter that the permuted task imposes an auxiliary counting demand that LLMs are poorly equipped to handle without tools (e.g., Python interpreter) or sophisticated prompting.",
            "limitations_or_caveats": "Debate centers on whether the permuted variant tests the same underlying competence or simply introduces an auxiliary counting task that masks latent analogical skill; no definitive numeric comparisons included in this paper's summary.",
            "uuid": "e9021.3",
            "source_info": {
                "paper_title": "Anthropocentric bias in language model evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Huginn-0125 (Winogrande example)",
            "name_full": "Huginn-0125 (example model used in this paper's Figure 1)",
            "brief_description": "Model example demonstrated in this paper: increasing test-time latent/recurrent iterations shifts log-likelihoods from an incorrect completion to the correct one on a Winogrande commonsense reasoning item, illustrating test-time computational bottlenecks revealing latent competence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Huginn-0125",
            "model_description": "Recurrently evaluated language model (or an LLM with test-time latent iteration capability) used in the paper to illustrate latent-reasoning/test-time-scaling effects; specific architecture and training details not provided.",
            "model_size": null,
            "test_battery_name": "Winogrande (commonsense pronoun resolution / fill-in-the-blank)",
            "test_description": "Commonsense reasoning benchmark (Winogrande) requiring contextual/world-knowledge inference to resolve anaphora or select the correct completion for a sentence.",
            "llm_performance": "With few latent/recurrent steps: model assigns higher log-likelihood to incorrect completion \"jars\"; as number of latent steps increases, log-likelihood for the correct completion \"glow sticks\" increases and becomes dominant (qualitative shift in ranked probabilities; no numeric accuracy provided).",
            "human_baseline_performance": null,
            "performance_comparison": "Not directly compared to humans in the figure; demonstrates that apparent model failure can be reversed by allocating more test-time compute, revealing latent competence.",
            "experimental_details": "Manipulation: vary number of latent/recurrent inference iterations (test-time scaling) and observe changes in token-level log-likelihoods for candidate completions on a Winogrande example.",
            "limitations_or_caveats": "Single illustrative example (Figure 1) rather than broad benchmark statistics; no numeric human baselines or aggregate model accuracies reported here—serves to exemplify the computational-bottleneck auxiliary factor rather than to provide a full quantitative evaluation.",
            "uuid": "e9021.4",
            "source_info": {
                "paper_title": "Anthropocentric bias in language model evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Muennighoff et al. 2025 (LRMs / test-time scaling)",
            "name_full": "S1: Simple test-time scaling (Muennighoff et al., 2025)",
            "brief_description": "Work describing 'large reasoning models' (LRMs) trained to exploit test-time scaling (e.g., varying number of chain-of-thought or 'thinking' tokens); reports that increasing the thinking-token budget improves performance on difficult math and science benchmarks.",
            "citation_title": "S1: Simple test-time scaling",
            "mention_or_use": "mention",
            "model_name": "Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1 (examples given)",
            "model_description": "Language models that undergo an additional post-training stage (supervised fine-tuning on chain-of-thought examples and RL on verifiable rewards) and are designed to benefit from dynamic allocation of compute at inference time by varying the number of thinking tokens generated.",
            "model_size": null,
            "test_battery_name": "Difficult math and science benchmarks (unspecified in summary)",
            "test_description": "Benchmarks testing formal-domain reasoning (math, science) where extended chain-of-thought or increased test-time compute can improve performance.",
            "llm_performance": "Reported improvement in performance as the 'thinking' token budget is increased on difficult math and science benchmarks (qualitative; no numeric figures provided in this paper's summary).",
            "human_baseline_performance": null,
            "performance_comparison": "Not directly compared to human baselines in this paper's discussion; main finding is that performance can switch from failure to success within the same model by allocating more test-time compute.",
            "experimental_details": "LRMs are fine-tuned on chain-of-thought style examples and RL; at inference time, the number of intermediate 'thinking' tokens (test-time compute) is modulated to measure performance changes.",
            "limitations_or_caveats": "Shows that test-time compute budget is an auxiliary factor; poor performance under low thinking-token budgets should not be taken as evidence of lack of competence. Exact benchmarks, numeric scores, and human comparisons are not reported in the summary here.",
            "uuid": "e9021.5",
            "source_info": {
                "paper_title": "Anthropocentric bias in language model evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Auxiliary task demands mask the capabilities of smaller language models",
            "rating": 2,
            "sanitized_title": "auxiliary_task_demands_mask_the_capabilities_of_smaller_language_models"
        },
        {
            "paper_title": "Can language models handle recursively nested grammatical structures? A case study on comparing models and humans",
            "rating": 2,
            "sanitized_title": "can_language_models_handle_recursively_nested_grammatical_structures_a_case_study_on_comparing_models_and_humans"
        },
        {
            "paper_title": "Can Transformers Process Recursive Nested Constructions, Like Humans?",
            "rating": 1,
            "sanitized_title": "can_transformers_process_recursive_nested_constructions_like_humans"
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "using_counterfactual_tasks_to_evaluate_the_generality_of_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "S1: Simple test-time scaling",
            "rating": 2,
            "sanitized_title": "s1_simple_testtime_scaling"
        },
        {
            "paper_title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
            "rating": 2,
            "sanitized_title": "scaling_up_testtime_compute_with_latent_reasoning_a_recurrent_depth_approach"
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 1,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "The Expressive Power of Transformers with Chain of Thought",
            "rating": 1,
            "sanitized_title": "the_expressive_power_of_transformers_with_chain_of_thought"
        }
    ],
    "cost": 0.014363749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A n t h ropoc e n t ric b i as i n l ang uage mode l e val uat ion
21 Jun 2025</p>
<p>Raphaël Millière raphael.milliere@mq.edu.au 
Macquarie University</p>
<p>Charles Rathkopf c.rathkopf@fz-juelich.de 
Macquarie University</p>
<p>A n t h ropoc e n t ric b i as i n l ang uage mode l e val uat ion
21 Jun 2025A2A59F4E8D80F60CA9C74918D14FF870arXiv:2407.03859v2[cs.CL]
Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases.This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (auxiliary oversight), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (mechanistic chauvinism).Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.</p>
<p>Introduction</p>
<p>What cognitive competencies do large language models (LLMs) have, if any?That is a question of immense theoretical and practical importance.It is also formidable, since we currently lack an appropriate methodological framework with which to answer it.Among existing scientific disciplines, experimental psychology likely offers the most suitable approach.Yet, this methodological framework was designed with human subjects in mind, and LLMs are decidedly non-human.While the rigorous experimental methods of psychology provide an excellent foundation for investigating the cognitive capacities of LLMs, we cannot simply transfer them whole cloth.Indeed, the inferences enabled by these methods in human studies are buttressed by a rich set of background assumptions about how humans typically operate, and there is no a priori reason to think that those assumptions also hold true of LLMs.As a result, naively applying the methods of experimental psychology to LLMs risks succumbing to both anthropomorphic and anthropocentric biases.Anthropomorphic bias involves attributing human qualities to LLMs without justification -being too eager to recognize the capacities of an LLM as instances of our own.Anthropocentric bias, though more challenging to articulate, roughly entails evaluating LLMs according to human standards without adequate justification for applying those standards, and refusing to acknowledge the possibility of genuine cognitive competence that differs in substantive ways from our own.This article aims to elucidate and expand upon this challenge, offering a constructive path forward for resolving current disputes and advancing our understanding of the capacities of LLMs.1</p>
<p>The performance/competence distinction</p>
<p>The distinction between competence and performance (Chomsky 1965) plays a crucial role in cognitive science.The distinction is often introduced by defining competence as the system's internal knowledge underlying a particular capacity, and performance as the observable behavior of a system exercising that capacity (Firestone 2020).For a competent system, then, performance is the external manifestation of competence.This formulation allows us to draw upon examples illustrating how performance and competence can diverge.For instance, a student cheating on a test demonstrates performance success without competence, while a knowledgeable student failing due to anxiety shows performance failure despite competence.Such examples might invite criticism that applying the performance/competence distinction to LLMs is inherently anthropomorphic, since it relies on a concept of knowledge that properly applies only to humans.We can avoid that concern by providing an alternative formulation of the distinction.In an experimental context, performance refers to how closely a system's behavior aligns with some normative standard of success on a task, while competence refers to system's computational capacity to meet that normative standard under ideal conditions.</p>
<p>It is widely recognized that there is a double dissociation between performance and competence, and that this distinction gives us reason to be wary not only of naive inferences from good performance to competence, but also from bad performance to lack of competence.Although the performance/competence distinction is applied bidirectionally in human experimental psychology, it is almost exclusively applied in the former direction when studying LLMs.Without adequate justification, this asymmetry is suggestive of anthropocentric bias.We suspect that, within the ML research community, anthropocentric bias is motivated by the reasonable desire to temper the opposite and perhaps more irresponsible form of bias -anthropomorphism, the tendency to ascribe human-like capacities to LLMs without sufficient evidence.2However, fighting one bias by entrenching another won't foster impartiality (Buckner 2021).Instead, as the philosopher Elliot Sober once remarked in a discussion of anthropocentric reasoning in comparative psychology, "the only prophylactic we need is empiricism" (Sober 2005, p. 97).</p>
<p>Taxonomy of Anthropocentric Bias</p>
<p>To call a pattern of reasoning biased is to say it lacks adequate justification.But justification can fail in different ways.Each failure corresponds to a distinct variety of anthropocentric bias, with its own methodological challenges.We outline a taxonomy of these biases and strategies for mitigating them.</p>
<p>Auxiliary oversight</p>
<p>The first kind of bias, which we call auxiliary oversight, is the tendency to assume that an LLM's performance failures on a task designed to measure competence  always indicate that the system lacks .This assumption is flawed because it overlooks the possibility that auxiliary factors caused the performance failure.In human psychology, auxiliary factors are often illustrated by cases such as the excessively nervous student described above: the student's nervousness negatively impacts her performance on the test, even though she Preprint actually has the relevant knowledge and would have otherwise done well.One might question whether applying the concept of auxiliary factor to LLMs introduces a subtle form of anthropomorphism, as it relies in the human context on the existence of a complex, semimodular faculty psychology, where limitations in one faculty might bottleneck performance in another.Since LLMs presumably do not have psychological faculties in that sense, citing auxiliary factors as causes of LLM performance failure may seem like a nonstarter.</p>
<p>However, auxiliary factors can and do influence LLM performance.To understand their role, it is helpful to recall that every mechanistic explanation involves a degree of idealization: it picks out a subset of the many causal influences on behavior.In doing so, it implicitly assumes that the remaining factors will either remain stable or exert only minor influence.When that assumption fails, performance may degrade in ways that do not reflect a lack of underlying competence.</p>
<p>Sometimes, failure results from a missing enabler -a subsystem or context the subject relies on but does not receive.For example, in the mirror test for self-recognition, an animal may fail not because it lacks the capacity for self-representation, but because it lacks motivation to remove a visible mark.In other cases, failure stems from interference by competing mechanisms, as when a bilingual child struggles with a vocabulary test in one language due to lexical interference from the other.Finally, performance may be limited by how much processing the subject is permitted or able to perform -as when a student asked to solve a math problem mentally fails, but succeeds immediately when given pen and paper.</p>
<p>What unites these scenarios is that they violate the idealized assumptions built into our explanatory models.Auxiliary factors are not part of the competence being tested, yet their influence can mask or suppress it.Ignoring them risks conflating failure of performance with failure of cognition.</p>
<p>In LLMs, we can distinguish at least three kinds of auxiliary factors.The first and most familiar kind are auxiliary task demands.Hu &amp; Frank (2024) provide a helpful illustration of such demands in evaluating whether language models are sensitive to syntactic features like subject-verb agreement.They compare two approaches: (1) prompting the model to make explicit grammaticality judgments, and (2) directly comparing the probabilities the model assigns to minimal pairs that vary the target feature.For the metalinguistic approach, they use prompts such as: "Here are two English sentences: 1) Every child has studied.</p>
<p>2) Every child have studied.Which sentence is a better English sentence?Respond with either 1 or 2 as your answer."They then analyze both the output and the probabilities assigned to '1' and '2'.For the direct estimation approach, they prompt LMs with minimal pairs such as "every child has studied" and "every child have studied", and compare the log probabilities assigned to each string.A model is considered successful if it assigns a higher probability to the grammatical sentence.Across model sizes and datasets, Hu and Frank find that direct probability estimation yields results that differ from and are often better than the metalinguistic approach.They conclude that metalinguistic prompting introduces an auxiliary task demand -the ability to generate explicit grammaticality judgments -that is irrelevant to the underlying syntactic competence of interest.In contrast, direct probability estimation more validly measures the target capacity.We concur with their assessment.What makes it a genuine demand is the fact it degrades performance.What makes it genuinely auxiliary is the fact that metalinguistic judgment is conceptually independent of the psychological construct of interest, which is the capacity to track grammaticality.</p>
<p>Neglecting the effect of auxiliary task demands on model performance can lead inferences about competence astray.Such negligence is compounded in comparative studies with mismatched experimental conditions, resulting in divergent auxiliary task demands for LLMs and human subjects.This concern is highlighted by Lampinen (2023)'s case study comparing human and model performance on recursively nested grammatical structures, in response to prior work by Lakretz et al. (2022).Lakretz et al. found that humans outperformed language models on challenging long-distance subject-verb agreement dependencies in embedded clauses.However, Lampinen notes that human subjects were given substantial instructions, training and feedback to orient them to the experimental task, while models were evaluated "zero-shot" without any task-specific context.The discrepancy in experimental conditions confounds the comparison: the additional context provided to humans but not models can be interpreted as imposing weaker auxiliary task demands on humans than models.To level the playing field, Lampinen tested LLMs on the same task by providing it with prompts containing a few examples, intended to match the orienting context given to human subjects.With this modest task-specific context, LLMs perform as well as or better than humans, even on challenging sentences with deeper nesting than those tested in humans.This cautionary tale illustrates how mismatched experimental conditions across humans and models -with respect to instructions, examples, motivation, and other factors -can distort comparisons of their capacities.Meaningful comparative evaluation requires that humans and models are subject to similar auxiliary task demands, just as comparative psychology strives for "species-fair comparisons" across humans and animals.</p>
<p>Auxiliary task demands present a particularly challenging issue in LLM evaluation because tasks that are considered trivial for humans may not be trivial for an LLM.The classification of a task as trivial depends on the causal structure of the system being tested, specifically whether the cause of performance failure is associated with the parts and operations that explain its success when things go well.If the cause of performance failure is related to other parts of the system that do not contribute to its success in normal circumstances, then it is a non-trivial task demand relative to that system.This problem is exemplified by a recent debate about whether LLMs possess competence in analogical reasoning.Webb et al. (2023) showed that LLMs can match or surpass average human performance on various novel analogical reasoning tasks, including letter-string analogies such as Lewis &amp; Mitchell (2024) found that performance deteriorates when using a variant with a permuted alphabet.They interpret this drop as evidence that LLMs lack general competence in analogical reasoning.In response, Webb et al. (2024) argue that solving the permuted task requires counting letter indices, which LLMs cannot do effectively without tools like a Python interpreter (see Chang &amp; Bisk 2024).Counting, they contend, imposes an auxiliary task demand that undermines the inference from poor performance to a lack of analogical competence.While humans may not assign numerical indices explicitly, they still must track how many times to apply successor or predecessor operations.This too is an auxiliary demand, though it may not impede human performance-especially when subjects can visually refer back to the permuted alphabet.For LLMs, by contrast, the same demand is more disruptive: without a Python interpreter or sophisticated chain-of-thought prompts, they struggle to count reliably.This asymmetry in the impact of counting-related demands could explain the performance gap on counterfactual letter-string tasks.
[ 𝐴𝐵𝐶] → [ 𝐴𝐵𝐸], [𝑀𝑁𝑂] → [?]. However,
Another kind of auxiliary factor is what we call test-time computational bottlenecks-variable constraints on the computations a model can perform at inference time, regardless of task demands.These are most apparent in the context of test-time scaling, a Preprint family of techniques for dynamically allocating compute during inference without retraining the model.The simplest example involves prompting a language model to generate a "chain of thought" (CoT) before answering (Wei et al. 2023).When asked for a direct answer, Transformer models are fundamentally limited; theoretical work shows they cannot solve many sequential reasoning problems in a single forward pass (Merrill &amp; Sabharwal 2024).Prompting them to "think step by step" triggers the generation of intermediate tokens that scaffold reasoning and substantially improve performance (Kojima et al. 2023).In theory, each additional decoding step expands the model's expressive power (Merrill &amp; Sabharwal 2024).In practice, mechanistic analyses confirm that models use CoT tokens actively, drawing on multiple circuits to extract information from intermediate steps and assemble a final answer (Dutta et al. 2024).The stark difference in performance with and without CoT shows that latent capabilities may go unexpressed when models are prompted to answer directly.</p>
<p>The influence of test-time computational bottlenecks is demonstrated more starkly by "large reasoning models" (LRMs), such as OpenAI o1 or DeepSeek R1, which are specifically trained to benefit from test-time scaling.RLMs are language models that undergo an additional post-training stage, which typically includes supervised fine-tuning on curated examples of CoT reasoning, as well as reinforcement learning on verifiable rewards in formal domains.At inference time, the computational budget for these models can be dynamically controlled by modulating the number of "thinking" tokens they generate before providing an answer.For example, Muennighoff et al. (2025) found that increasing the "thinking" token budget would increase performance on difficult math and science benchmarks.In this case, it would be wrong to conclude prematurely that performance failures with a low "thinking" token budget provides reliable evidence that the model lacks competence in the task domain.For the very same model and task, performance can switch from failure to success simply by letting the model generate a longer CoT.</p>
<p>The notion of computational bottlenecks extends beyond the generation of external intermediate tokens to scaffold computations.Experimental language models designed with an intrinsic capacity for test-time scaling in latent space provide an even clearer illustration of the same phenomenon.For example, Geiping et al. (2025) introduce a recurrent architecture that allows the model to "think" by iteratively refining its internal state in a continuous latent space, without ever needing to verbalize intermediate steps.They found that their model's performance on reasoning benchmarks improved when they increased the number of latent iterations at test time.For the very same problem, the model might fail with a few latent iterations but succeed with dozens.This demonstrates a purely computational bottleneck, disentangled from the auxiliary demand of articulating a chain of thought in natural language.Figure 1 illustrates this with a specific example from the Winogrande commonsense reasoning benchmark (Sakaguchi et al. 2021).The task in this example involves filling in the blank in the sentence "I tried to make mini lamps by using glow sticks in mason jars, but had to get larger jars because the ___ were too big."Initially, with only a few latent steps, the model assigns a higher log-likelihood to the incorrect completion, "jars".However, as the number of latent steps increases, the log-likelihood for the correct completion, "glow sticks", increases to become the dominant prediction.The model's initial failure is not evidence of its inability to solve the problem, but rather the consequence of a task-independent computational bottleneck.Its eventual success reveals a latent competence that is only expressed when sufficient computational resources are allocated at test time.</p>
<p>Finally, a third kind of auxiliary factor is what we might call mechanistic interference, according to which a network learns a mechanism for solving a certain kind of problem, Preprint Figure 1 | Test-time scaling improves Huginn-0125's performance on a Winogrande task.The model initially assigns higher log-likelihood to the incorrect completion, "jars".As the number of recurrent steps increases, it favors the correct completion, "glow sticks".This shift illustrates how allocating more compute at test time can reveal latent competence otherwise suppressed by computational bottlenecks.</p>
<p>but the activity of that mechanism is interrupted by another, conceptually distinct process.Recent work on Transformer models trained on modular addition tasks provides insights into how a model's performance can be impacted by such auxiliary factors.Nanda et al. (2022) show that even after a Transformer has formed a generalizable circuit that implements an algorithm to solve modular addition, its test set performance can still be impeded by previously memorized input-output mappings until a later "cleanup" phase in which this rote memorization is removed.This shows that the presence of a general solution algorithm does not necessarily translate to strong test set performance when there is interference from auxiliary computations.Zhong et al. (2023) further show that even in fully trained networks that have "grokked" the task, there can be multiple concurrent algorithms or circuits that interact to solve the task and potentially interfere with each other.For instance, a Transformer might contain one circuit implementing a highly general algorithm alongside another implementing a more limited or approximate algorithm for the same task.The model's overall performance could then be negatively impacted by the causal interference of the lesser algorithm, even though the latent competence to fully solve the task is present.Taken together, these findings illustrate that a model's performance on a task can be meaningfully influenced by auxiliary computations that are conceptually distinct from the core competence required for the task.</p>
<p>Mechanistic chauvinism</p>
<p>Mechanistic chauvinism is the tendency to assume that even when LLMs match or exceed average human performance, any substantive difference in strategy counts as evidence that the model's solution lacks generality.In slogan form: all cognitive kinds are human cognitive kinds.3Whenever an LLM arrives at a solution through a different computational process than humans use, the mechanistic chauvinist concludes that the LLM lacks genuine competence, regardless of how well it performs.However, this line of thinking risks obscuring the real capabilities of LLMs.Consider a model that learns a general algorithm for addition, rather than simply memorizing a large set of specific addition problems and their solutions.Now suppose that humans use a somewhat different algorithm for addition.The mere fact that the LLM's approach differs from the human approach does not mean that the LLM lacks real competence at addition.What matters is whether the LLM has learned a robust, generalizable strategy --not whether this strategy mirrors human cognition.Indeed, we can easily imagine an LLM outperforming humans at addition while using a distinctly unhuman-like computational process.</p>
<p>The key point is that competence, once abstracted from a narrowly human-centric understanding of the term, should be related to the generality and flexibility of the system's computations rather than superficial resemblance to the human cognitive architecture.An LLM that relies on a giant lookup table of memorized addition problems would not be competent at addition, because it could not flexibly apply its memorized content-specific transitions to novel cases.But a neural network that learns a general addition algorithm could be seen as competent in the domain, even if the solution it has converged upon is distinctly non-human.</p>
<p>An objection</p>
<p>We now anticipate an objection to our discussion of mechanistic chauvinism: since the only indisputable examples of language-driven cognition we have are human, and since the capacities of LLMs are acquired through training on human-produced data, isn't it appropriate to treat human cognition as the only appropriate yardstick for studying LLMs?Our response is that it depends on how the yardstick is used.While we can -and presumably must -begin our investigation of LLM competencies by comparing them to our own, the role of human cognition as a benchmark can be overstated in two ways.First, the human competencies we use as reference points should be regarded as what philosopher Ali Boyle calls "investigative kinds" (Boyle 2024) -they serve as an initial search template, but not as necessary conditions for cognitive status.Second, questions of the form "Do LLMs have cognitive competence ?" should be treated as empirical questions.Though this may seem self-evident, it is a principle that is easy to violate, particularly when we assume that facts about implementation are among the distinguishing features between competence  and competence .If facts about the physical implementation of a competence are included in its definition, then it will be impossible for LLMs, which are implemented in silicon computers, to acquire any of the competencies that humans enjoy.Moreover, this impossibility will be logical, rather than empirical.In order to preserve the empirical character of debates about LLM capacities, therefore, we must focus on the algorithmic level of description, where facts about implementation are explicitly set aside.</p>
<p>The investigation of cognitive capacities in LLMs is best viewed as an iterative, cyclic process in which our conception of the relevant competencies and our understanding of the Preprint mechanisms that implement them in LLMs mutually inform and revise each other.Solving the challenge of mapping cognitive tasks to capacities often involves consideration of the underlying mechanisms (Francken et al. 2022).But to home in on the mechanisms responsible for a particular competence, we must make principled decisions about the level of abstraction at which to characterize the mechanism and about how to delineate the boundaries of the mechanism itself.These decisions, in turn, depend on how we conceptualize the cognitive phenomenon we are trying to explain.What results is a investigative process in which our characterization of cognitive tasks, our ontology of capacities, and our understanding of mechanisms evolve in tandem as we search for maximally predictive and explanatory mappings between them.In the case of LLMs, this process may lead us quite far from our initial starting point.Though we inevitably begin by searching for human-like competencies in LLMs, the gradual discovery and refinement of LLM-specific mechanisms may ultimately produce a novel ontology of cognitive kinds, one that is optimized for explaining the distinctive strengths and weaknesses of machine intelligence rather than human intelligence.In this way, the iterative nature of the investigative process allows it to drift away from its anthropocentric origins.</p>
<p>Conclusion</p>
<p>Like humans, LLMs can be right for the wrong reasons; we should not take good performance on various benchmarks, particularly those designed without attention to construct validity and potential confounds, at face value.However, they can also be wrong for the wrong reasons.Auxiliary factors can suppress performance in ways that obscure latent competence, so both successes and failures offer only defeasible evidence.While anthropomorphism has received substantial critical attention, anthropocentric bias has gone largely unexamined.We have argued that it poses a serious obstacle to the objective assessment of LLM capacities, and we offered a taxonomy of its forms to help guide future research.</p>
<p>The most direct way to work out whether auxiliary factors actually cause performance degradation on a task is to look inside the model.As such, mechanistic interpretability techniques are essential for counteracting auxiliary oversight.However, the role of mechanistic interpretability in addressing mechanistic chauvinism is more nuanced.It is possible to decode a feature or circuit of interest from a large neural network even if that feature or circuit does not significantly contribute to the network's functionality (Makelov et al. 2024, Huang et al. 2023).Consequently, there is a risk of anthropomorphic projection in mechanistic interpretability research.As noted above, we should not combat anthropocentrism by resorting to anthropomorphism.Nevertheless, there exist mechanisms that process information in ways that differ from typical human strategies but are nonetheless robust and general.Mechanistic interpretability research can help identify these mechanisms.When they are discovered, we should not dismiss them simply because they deviate from the human case.Doing so would foreclose the possibility of understanding artificial cognition on its own terms.</p>
<p>Our aim is not to debate whether LLMs should be designed to have human-like capacities, but to discuss how we can fairly assess and compare their capacities to human cognition.
See, for example, Emily Bender's "On the NYT Magazine on AI: Resist the Urge to be Impressed," a response to a New York Times Magazine article about OpenAI(Bender 2022).
This bias resembles whatBuckner (2013) calls "anthropofabulation"-a combination of (i) human-centric definitions of psychological terms with (ii) an exaggeration of human cognitive abilities. Mechanistic chauvinism differs from anthropofabulation in that it focuses on process rather than semantics or performance. Nevertheless, both forms of bias risk setting standards that no non-human system could meet.</p>
<p>E M Bender, On NYT Magazine on AI: Resist the Urge to be Impressed. 2022</p>
<p>Disagreement &amp; classification in comparative cognitive science. A Boyle, 2024</p>
<p>Morgan's Canon, meet Hume's Dictum: Avoiding anthropofabulation in cross-species comparisons. C Buckner, Biology &amp; Philosophy. 2852013</p>
<p>Black Boxes or Unflattering Mirrors? Comparative Bias in the Science of Machine Behaviour. C Buckner, The British Journal for the Philosophy of Science. 2021</p>
<p>Language Models Need Inductive Biases to Count Inductively. Y Chang, Y Bisk, 2024</p>
<p>N Chomsky, Aspects of the Theory of Syntax. Cambridge, MA, USAMIT Press1965</p>
<p>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning. S Dutta, J Singh, S Chakrabarti, T Chakraborty, 2024</p>
<p>Performance vs. competence in human-machine comparisons', Proceedings of the National Academy of. C Firestone, Sciences. 117432020</p>
<p>Cognitive ontology and the search for neural mechanisms: Three foundational problems. J C Francken, M Slors, C F Craver, Synthese. 20053782022</p>
<p>J Geiping, S Mcleish, N Jain, J Kirchenbauer, S Singh, B R Bartoldson, B Kailkhura, A Bhatele, T Goldstein, Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach. 2025</p>
<p>J Hu, M C Frank, Auxiliary task demands mask the capabilities of smaller language models. 2024</p>
<p>J Huang, A Geiger, K D'oosterlinck, Z Wu, C Potts, Rigorously Assessing Natural Language Explanations of Neurons. 2023</p>
<p>. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Large Language Models are Zero-Shot Reasoners. 2023</p>
<p>Can Transformers Process Recursive Nested Constructions, Like Humans?. Y Lakretz, T Desbordes, D Hupkes, S Dehaene, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022</p>
<p>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. A K Lampinen, 2023</p>
<p>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models. M Lewis, M Mitchell, 2024</p>
<p>Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching. A Makelov, G Lange, A Geiger, N Nanda, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>The Expressive Power of Transformers with Chain of Thought. W Merrill, A Sabharwal, 2024</p>
<p>. N Muennighoff, Z Yang, W Shi, X L Li, L Fei-Fei, H Hajishirzi, L Zettlemoyer, P Liang, E Candès, T Hashimoto, 2025S1: Simple test-time scaling</p>
<p>Progress measures for grokking via mechanistic interpretability. N Nanda, L Chan, T Lieberum, J Smith, J Steinhardt, The Eleventh International Conference on Learning Representations. 2022</p>
<p>WinoGrande: An adversarial winograd schema challenge at scale. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Commun. ACM. 6492021</p>
<p>Comparative psychology meets evolutionary biology. E Sober, Thinking with Animals: New Perspectives on Anthropomorphism. L Datson, G Mitman, Columbia University Press2005</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 2023</p>
<p>Evidence from counterfactual tasks supports emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, 2024</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023</p>
<p>Z Zhong, Z Liu, M Tegmark, J Andreas, The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
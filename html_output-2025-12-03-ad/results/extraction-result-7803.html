<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7803 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7803</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7803</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fineâ€‘tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-3f413dca2607d68301143770e599b59d461a569e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3f413dca2607d68301143770e599b59d461a569e" target="_blank">Table-GPT: Table-tuned GPT for Diverse Table Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new "\emph{table-tuning}" paradigm, where language models like GPT-3.5 and ChatGPT are trained/fine-tune using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks.</p>
                <p><strong>Paper Abstract:</strong> Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \emph{one-dimensional} natural-language texts, whereas relational tables are \emph{two-dimensional} objects. In this work, we propose a new"\emph{table-tuning}"paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong \emph{generalizability}, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7803",
    "paper_id": "paper-3f413dca2607d68301143770e599b59d461a569e",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0073409999999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Table-GPT: Table-tuned GPT for Diverse Table Tasks</h1>
<p>Peng $\mathrm{Li}^{\dagger}$, Yeye $\mathrm{He}^{\ddagger}$, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri<br>Microsoft Corporation</p>
<h4>Abstract</h4>
<p>Language models, such as GPT-3 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on onedimensional natural-language texts, whereas relational tables are two-dimensional objects.</p>
<p>In this work, we propose a new "table-tuning" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better table-understanding capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong generalizability, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models, such as GPT and LLaMa, have recently demonstrated impressive abilities in performing diverse naturallanguage tasks [5, 8, 12, 50]. In the database literature, a number of pioneering work, such as [20, 29, 39, 43], have also shown that by using "prompt engineering", to careful select the best instructions and few-shot examples for a particular task at hand, language models can be prompted to perform well on a number of table-tasks such as entity matching and data-imputation.</p>
<p>While prompt-engineering is a promising direction to enhance model performance, it requires task-specific tuning (e.g., task-specific labeled-data to test the performance of different instruction/example combinations) [6, 8, 61]. We in this work propose an orthogonal paradigm called "table-tuning", where instead of modifying prompts, we modify the weights of the underlying language models for once (i.e., not task-specific), by continuing to train them using</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two simple tests to probe language-models' basic ability to read and understand tables. (Left) T-1: Missing cells identification, which is to identify the column-header/rowid of a missing cell. (Right) T-2: Column-Finding, which is to identify the column-name of a given value. Even large models (e.g. 175B GPT-3.5) can frequently fail on such tests, with only 0.26 accuracy in one variant of the tests.
diverse table-tasks as training data, to improve their ability to understand tables. We show that table-tuned Table-GPT consistently outperform the vanilla GPT-3.5 and ChatGPT on a wide-range of table tasks, including new and unseen table-tasks. We note that our model-tuning approach is complementary to prompt-engineering, because carefully engineered prompts can continue to benefit both vanilla language-models and our table-tuned models.</p>
<p>Today's language models cannot "read tables" reliably. While today's language models excel in natural-language tasks, we start by asking the question of whether these models are optimal for table-tasks, because after all, they are pre-trained predominantly on natural language texts, which are different from tables.</p>
<p>More specifically, natural language texts are (1) one-directional, (2) read left-to-right, where (3) swapping two tokens will generally change the meaning of a sentence. In contrast, relational tables are (1) two-dimensional in nature with both rows and columns, (2) where reading top-to-bottom in the vertical direction for values in the same column, is crucial in many table-tasks. Furthermore, unlike text, (3) tables are largely "invariant" to row and column permutations, where swapping two rows or columns do not generally change the semantic meaning of the table.</p>
<p>With this question in mind, we perform two simple tests to probe language models' ability to "read" tables and then answer basic questions, which we call (T-1) Missing-value-identification, and (T-2) Column-finding, as shown in Figure 1.</p>
<p>In (T-1) Missing-value-identification, we show language models with a real table, presented in a markdown or alternative format, where we make sure that there is exactly one empty cell in the table.</p>
<p>Markdown table is a common format used by prior work to feed tables into languagemodels, and also a format that models like GPT will use when it needs to respond with a table, presumably because GPT-like models use GitHub data in its pre-training, where markdown-format tables are abundant.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example table-tasks, where the ability of language models to "read" tables vertically is important. (Left) T-3: Table Question-Answering. (Right) T-8: Data Imputation. More tasks like these are shown in Table 2.</p>
<p>We then ask the model to identify the empty cell, by responding with the column-name and row-id of the empty cell, repeating for 1000 randomly sampled real tables. Despite the impressive ability of language-models like GPT-3.5 to perform diverse tasks, we find that they fail on a surprisingly large fraction (up to $74 \%$ ) of such tests, often responding with incorrect column-headers or row-ids - for instance, in the example shown in Figure 1, the model may answer that the column "music" has a missing cell, when the correct answer should be "art".</p>
<p>In order to ensure that there is no ambiguity in what "missing value" or "empty cell" could mean to language models, we design a second and even simpler test, which we refer to as: (T-2) Columnfinding, shown on the right of Figure 1. In this test, we present a language model with a real table, and ask it to find a specific cell-value that appears exactly once in the entire table (e.g., "93" in this example), and then respond with the column-name of the that value. We find that language models such as GPT-3.5 are prone to fail on such tests again (e.g., answering that " 93 " is in column "art" when the correct answer is "music"), on over half of such tests.</p>
<p>We believe these simple probes show that today's large language models, when pre-trained on large amounts of one-directional natural-language texts, are not best-suited to "read" two-dimensional tables, especially in the vertical direction, which however is crucial in performing many table-tasks.</p>
<p>Consider, for example, the popular NLP task of (T-3) TableQA $[11,42,49]$, where the task is to answer a natural-language question, based on the content of the table. The left side of Figure 2 shows such an example. Given the question "How many second-graders scored over 90 in art, in the table below?" Imagine that a model is not able to "read" tables correctly, it may reason that both "Jennifer" and "James" satisfy the condition (because it believes "93" is in the column "art", like shown in Figure 1 (Right)), and may answer "2" instead of the correct " 1 ". We emphasize that the ability to read in the vertical direction (top-to-bottom for values in the same column) is similarly important in many other tabletasks, such as data-imputation (shown on the right of Figure 2), data-transformation, error-detection, NL-to-SQL, etc., like the list in Table 2 would show, which includes a diverse set of table-tasks considered in this work.</p>
<p>In addition, we find that large language models are sensitive to the order in which columns are presented in a table - e.g., when we swap the order of two columns in a table, a model can change its response for a table-task, even when such a swap should not
change the semantic meaning of the table, at least to humans. This is presumably because language-models are pre-trained on text where the order of tokens matters (e.g., "Jennifer called you" vs. "you called Jennifer"), leading to sub-optimal behaviors on tables.</p>
<p>We believe observations like these point to opportunities for us to improve the underlying language model, by enhancing their ability to understand tables and perform table-tasks.</p>
<p>Instruction-tuning in NLP: train language-models to follow diverse human instructions. To change the behaviour of language models, successful attempts have been made in the NLP community, using a technique known as "instruction-tuning" in the literature [40, 47, 57-59].</p>
<p>It was observed in the NLP community [8, 40, 59], that earlier versions of pre-trained language models, such as GPT-3, is able to complete a sentence with the next likely token (e.g., "write a bed-time" $\rightarrow$ "story"), but cannot reliable follow higher-level instructions from humans (e.g., "write a bed-time story for a 3 years-old, in 100 words"), a behavior that is only demonstrated in later models such as ChatGPT.</p>
<p>Instruction-tuning was the key technique invented that continues to train GPT-like models into ChatGPT-like models, in a process shown on the left of Figure 3. Diverse training data in the form of "(instruction, completion)" pairs are constructed, often manually annotated by human labellers [40], e.g. ("write a bed-time story" $\rightarrow$ an-actual-story), to continue train language-models on these explicit demonstrations of how to follow high-level human instructions, leading to well-known models such as ChatGPT/InstructGPT [2, 40], as well as their open-source counterparts like Stanford-Alpaca [4] and LLaMa-chat [50].</p>
<p>Table-tuning: train language-models to understand tables. We believe that the research on instruction-tuning in NLP, which successfully enhances language-models ability to follow human instructions, holds lessons for us when we aim to enhance languagemodels ability to understand tables and perform table-tasks.</p>
<p>In this work, we propose a "table-tuning" paradigm analogous to instruction-tuning, where we continue to train language-models, using diverse training data in the form of (instruction, table, completion), which we synthesize using large amounts of real tables. This process is illustrated on the right of Figure 3.</p>
<p>Through extensive experiments, we show that "table-tuning" is a promising new direction, as our resulting Table-GPT models are:
(1) Strong table models, which substantially outperform 175B GPT3.5 and ChatGPT, on a wide range of seen and unseen table-tasks, as we summarize in Table 2 and Figure 9;
(2) Generalizable to new tasks, as they can respond well to novel and unseen table-tasks, similar to how Chat-GPT could generalize and respond to new and unseen NLP tasks, like shown in Figure 4.
Contributions. We make the following contributions:</p>
<ul>
<li>We propose a new "table-tuning" paradigm to continue to train language models, specifically designed to enhance languagemodels' ability to perform table-tasks, using diverse table-tasks synthesized from large amounts of real tables, in a "synthesis-then-augment" process.</li>
<li>We develop task-level, table-level, instruction-level, and completionlevel data augmentation techniques for table-tuning, which we</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Instruction-tuning vs. Table-tuning. (Left) Instruction-tuning is a technique developed in the NLP community that continues to train language-models (e.g., GPT) for instruction-following capabilities (e.g., in ChatGPT). (Right) Table-tuning is an analogous approach we propose to train language-models to better understand table and perform table-tasks.
show are crucial to avoid over-fitting and ensure the generality of Table-GPT.</p>
<ul>
<li>We show that Table-GPT not only excels on table-tasks in both zero-shot and few-shot settings out of box, but can also serve as a "table foundation model" and used as a better starting point than vanilla GPT, for down-stream single-task optimizations such as task-specific fine-tuning and prompt-engineering.</li>
</ul>
<h2>2 PRELIMINARIES</h2>
<p>We will start with a review of language models, and then the use of language models in table-tasks.</p>
<h3>2.1 Language models</h3>
<p>There are two popular styles of language models today, known as the decoder and encoder-style, both derived from the original transformer architecture [53].</p>
<p>Encoder-style language models. One class of popular language models, including the well-known BERT [17] and RoBERTa [35], use only encoders from the transformer, and are pre-trained on large amounts of texts to effectively represent the semantics of texts using embedding vectors.</p>
<p>Down-stream tasks: Task-specific fine-tuning. To use encoderstyle models like BERT for downstream tasks, task-specific finetuning is generally employed [21, 34], which continues to fine-tune (or train) BERT-like models for a given task, using task-specific labeled data. For example, suppose the downstream task is sentiment analysis of Yelp restaurant reviews, then labels in the form of ("The food is amazing", "positive"), ("The service is slow", "negative"), are needed to fine-tune BERT-like models for the desired outcome $[17,46]$.</p>
<p>Crucially, when the target input data or the desired output changes, the labeling effort often needs to repeat for the best performance. For example, if the input data for sentiment analysis changes to IMDB reviews, or if the output needs to include a classification of "cuisine-type" for restaurant reviews. While encoderstyle language-models are strong models, the need to fine-tune with task-specific labelled data limits its ability to generalize to new unseen tasks [17, 22, 35, 46].</p>
<p>Decoder-style "generative" language models. Another class of decoder-only language models, such as GPT [8] and LLaMa [50], are generative in nature, and are shown to excel in generalizing to new downstream tasks without task-specific fine-tuning [8].</p>
<p>Generalize to new tasks: zero-shot and few-shot learning. It was shown in the NLP literature that the decoder-style models (e.g., GPT and LLaMa), especially after instruction-tuning [31, 40, 47, 5659, 67] (e.g., ChatGPT/InstructGPT [2, 40] and Stanford Alpaca [4]), can adapt to new tasks easily, using just natural-language instructions (e.g., "classify the sentiments in the following reviews"), and optionally a few examples. Such an approach can adapt to new datasets (e.g., IMDB vs. Yelp reviews) and new tasks (sentimentanalysis vs. machine-translation), without fine-tuning on labelled data for each specific task, making the decoder-style models more general and versatile. Figure 5 shows the benefit of "instructiontuning" in model generalizability, depicted pictorially on the y-axis.</p>
<h3>2.2 Language models for table tasks</h3>
<p>Pioneering work in the database literature have employed language models in various ways to perform table-related tasks.</p>
<p>Encoder-style language models for table tasks. There is a long and fruitful line of research (e.g., TURL [16], TaBERT [64], Ditto [32] and Doduo [48]), where table-models are trained based</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Table-models should ideally "generalize" to new datasets and new tasks. (Left) Column type annotation (CTA): while this is a common table-task, the list of target-types to choose from can vary from dataset to dataset (e.g., 78 types in [25], and 107 in [16]). Making table-models to "generalize" to new CTA dataset without needing to retrain, is useful. (Right) Text-to-Table: a general table-model should be as general-purpose as models like ChatGPT, in following instructions to perform novel unseen table-tasks, such as "extracting tables from text" in the example.
on encoder-style BERT-like models, which are shown to perform well on various table tasks.</p>
<p>However, like their BERT-like base models, to generalize to a new dataset or a new task, these encoder-style table-models generally require fine-tuning with labeled data. As a concrete example, for the table-task of "column-type-annotation" [16, 48], in order to move from one dataset with 78 semantic types [25], to another dataset with 107 semantic types [16], new labeled data have to be obtained, so that the models can be fine-tuned to generate the new output with 107 classes [16]. In contrast, being able to adapt to new datasets and tasks without task-specific fine-tuning, is a key goal that we want to achieve in this work, like illustrated in Figure 4.</p>
<p>Decoder-style language models for table tasks. With the success of decoder-style language models such as GPT-3 and ChatGPT, which are shown to perform tasks out-of-the-box with instructions only, pioneering research in the database field develop "promptengineering" techniques for table-tasks [29, 39, 43], which carefully selects instructions and examples in the prompt, such that vanilla language models can perform well on table-related tasks.</p>
<p>Table-tuning for table-tasks. In contrast to prompt-engineering that optimizes prompts, our proposed "table-tuning" explores the orthogonal direction, where we continue to train the underlying language models, for once only (not task-specific), so that the resulting model perform better on a range of table-tasks. This is complementary to prompt-engineering, because carefully-engineered instructions and examples can continue to benefit both the vanilla GPT as well as our Table-GPT, as we will show in our experiments.</p>
<p>Figure 5 shows the process of table-tuning, which is analogous to instruction-tuning, but unlike instruction-tuning that improves model generalizability to follow human instructions (y-axis), we focus on improving underlying models ability to understand tables and perform table-tasks (x-axis). Crucially, as we will show, our table-tuned models remain to be general and capable of following human-instructions to perform table-tasks (without task-specific</p>
<p>Peng Li ${ }^{1}$, Yeye $\mathrm{He}^{2}$, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Instruction-tuning vs. Table-tuning. Instructiontuning improves model "generalizability", to follow diverse human-instructions to perform new and unseen tasks (xaxis), whereas our proposed table-tuning is analogous in spirit but aims to improve model ability to understand tables and perform table-tasks ( $y$-axis).
fine-tuning), just like the underlying GPT-3 and ChatGPT models. In other words, in Table-GPT we aim to get the "best of both worlds", with both generalizability, and good table-task performance.</p>
<h2>3 CAN LANGUAGE MODELS "READ" TABLES?</h2>
<p>Since language models like GPT are pre-trained predominantly on natural language text, we start by asking a basic question of whether language models can reliable read and understand relational tables, which are different from text in many ways, as we discuss below.</p>
<p>One-dimensional (text) vs. two-dimensional (tables). Language models trained mostly on natural language text (e.g, books and web pages) and programming code (e.g., GitHub), both of which that are one-directional that is meant to be read left-to-right, toke-by-token, in a sequential manner.</p>
<p>In contrast, relational tables are two-dimensional with rows and columns, where reading top-to-bottom vertically, for columnheaders and other values in the same column (which may be far away when a table is serialized), is crucial for many table-tasks.</p>
<p>Consider the task of Data-Imputation [7, 37] (T-8 in Table 2), which is to infer a missing value in a table cell, like shown in the example of Figure 2 (Right). At least for humans, it is natural to look vertically in the horizontal direction, to see the column-header ("continent" in this case), as well as other values in the same column (e.g., "Americas"), before one can make a guess for the missing value.</p>
<p>Similarly, for the task of Error-Detection [14] (T-9 in Table 2) it is also necessary to look at the column-header and other values in the same column, to understand the semantics of the column, before one can determine if a cell is erroneous.</p>
<p>Even for table-tasks that may be a bit removed, such as Table Question-Answering [42, 49] (T-3 in Table 2), which is traditionally an NLP problem - examples like in Figure 2 (Left) would show that, in order to answer a question correctly on a table, reading vertically in a column (e.g., for values in the art) is similarly important.</p>
<p>To test language models' ability to read tables in the columnar direction, we design simple tests. In the first test, referred to as</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Two variants of the task (T-1) Missing-cellidentification. (Left) T-1(a): We remove a random cell from a real table, but keep its column-separator. The presence of "| |" indicates a missing cell, which should be easy to identify. (Right) T-1(b): We remove a random cell, as well as its column-separator, which is a common but challenging CSV parsing issue $[18,52,54]$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(T-1): Missing cell</th>
<th style="text-align: center;">Find col-header texts:</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Find row-id texts:</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(a) no col-sep</td>
<td style="text-align: center;">(b) has col-sep</td>
<td style="text-align: center;">(a) no col-sep</td>
<td style="text-align: center;">(b) has col-sep</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 (zero-shot)</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 (few-shot)</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.91</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy numbers of vanilla GPT-3.5 (using Text-Davinci-002), on the task of (T-1) Missing-valueidentification as shown in 6.
"Missing-value-identification" (T-1 in Table 2), we sample a real table $T$ with no missing cells, and remove a random cell from $T$. We then produce two variants of the test, like shown in Figure 6:
T-1(a): we keep the column separator of the missing cell and ask language-models to identify the row-id/column-header of the missing cell, like in Figure 6 (Left), which seems simple;
T-1(b): We remove the column separator of the missing cell and then ask the same question, like in Figure 6 (Right). This is a common situation in CSV parsing that can be challenging [18, 52, 54], as one needs to align values vertically to see the missing value is in which column. (In the case, humans can see that the countries "USA" and "China" should align, the GPD numbers should align, so there must be a missing cell in "row-2", in between "China" and "19, 373, 586", for the column "Continent").</p>
<p>We repeat these two tests 1000 times, using 1000 randomly sampled real tables. Table 1 shows the result of this test. We can see that it is clearly challenging for language models to read tables in the column direction, where the accuracy with and without columnseparator is 0.38 and 0.26 , respectively. Even with column-separator and explicit few-shot demonstrations, the model is only able to get half of the tests right $(0.51)$.</p>
<p>In the row-direction, the model's ability to identify a missing cell is clearly better, though still not great, especially in the "no col-separator" setting.</p>
<p>To ensure that the language models are not confused by what we mean in "missing cell", we create a second, even simpler test, called Column-Finding (T-2 in Table 2), shown with an example in Figure 1 (Right), where we ask the model to find the column-header of a specific value, which appears exactly once in a given table $T$, for 1000 randomly sampled real tables. Our result show that the accuracy of GPT-3 is similarly low ( 0.46 ), confirming the hypothesis that language models ability to read two dimensional tables is likely insufficient.</p>
<p>Order-sensitive (text) vs. permutation-invariant (tables). In addition, we observe that natural-language texts tend to be ordersensitive, where swapping two tokens will generally lead to different meanings (e.g., "Jennifer called you" vs. "you called Jennifer"). In comparison, tables tend to be permutation-invariant, where swapping two rows or two columns, should generally not change the semantic meaning of the resulting table.</p>
<p>As a result, when applying language-models to table-tasks, e.g., Entity-matching, Error-Detection, Schema-Matching, we find that the predictions can be rather sensitive to the order in which columns are presented in the input tables, even when we only slightly reorder the columns.</p>
<p>We believe it shows that language models understanding of tables is still unstable and sub-optimal (likely influenced by the overwhelming text data used in its pre-training), because the decisions for tasks like Entity-matching and Error-Detection should really not depend on the order of columns.</p>
<p>Other differences. There are a number of additional aspects that make tables different from text. For example, table-cells tend to be short-form entity-names or phrases, which when serialized in a row, will typically be different from natural-language sentences found in text documents. Furthermore, values in the same column generally have homogeneous values, with pairs of columns encode regular relationships, which is another property not found in texts. All of these make tables different from texts, likely rendering languagemodels sub-optimal for table use cases, which motivates our tabletuning approach described next.</p>
<h2>4 TABLE-TUNING FOR TABLE-GPT</h2>
<p>We propose a new table-tuning paradigm, to enhance language models ability to understand tables and perform table-tasks,</p>
<h3>4.1 Overall approach: Synthesis-then-Augment</h3>
<p>Like discussed earlier, our table-tuning is inspired by the success of "instruction-tuning" from the NLP literature [40, 57, 59], illustrated in Figure 3 (Left), where diverse training data in the form of "(instruction, completion)" pairs are used to continue to train language-models, and has led to popular models like ChatGPT and LLaMa-chat that can understand and follow human instructions.</p>
<p>Our proposed table-tuning, as illustrated in Figure 3 (Right), is similar in spirit - instead of improving language-model ability to follow instructions using diverse "(instruction, completion)" pairs, we aim to improve language-model ability to perform table tasks using diverse "(instruction, table, completion)" triples, where each such triple defines an instance of a table-task:</p>
<p>Definition 1. An instance of a table-task, denoted by $t$, is defined as a triple $t=($ Ins, $T, C)$, where Ins is the natural-language instruction that specifies the table-task, $T$ is the input table on which the task is to be performed, and $C$ is the expected completion from following the instruction Ins and performing the task on table $T$.</p>
<p>Example 1. The examples in Figure 1, Figure 2, and Figure 3, show simple examples of table-tasks, defined by the (Ins, $T, C$ ) triples, which correspond to (instruction, table, completion), respectively. Note that the completion $C$ can be natural-language texts (with JSON or other alternatives for answer parsing), tables, or a combination of both.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task-name</th>
<th style="text-align: center;">Task description (related work)</th>
<th style="text-align: center;">Task category</th>
<th style="text-align: center;">Table data</th>
<th style="text-align: center;">Train/Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T-1: Missing-value identification (MV)</td>
<td style="text-align: center;">Identify the row and column position of the only missing cell in a given table</td>
<td style="text-align: center;">Table understanding</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Test only</td>
</tr>
<tr>
<td style="text-align: center;">T-2: Column-finding (CF)</td>
<td style="text-align: center;">Identify the column-name of a specific value that appears only once in a given table</td>
<td style="text-align: center;">Table Understanding</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Test only</td>
</tr>
<tr>
<td style="text-align: center;">T-3: Table-QA (TQA)</td>
<td style="text-align: center;">Answer a natural-language question based on the content of a table ( $[11,42,49])$</td>
<td style="text-align: center;">Table QA</td>
<td style="text-align: center;">[42]</td>
<td style="text-align: center;">Test only</td>
</tr>
<tr>
<td style="text-align: center;">T-4: Column type annotation (CTA)</td>
<td style="text-align: center;">Find the semantic type of a column, from a given list of choices ( $[16,25,63])$</td>
<td style="text-align: center;">Table understanding</td>
<td style="text-align: center;">$[16,25]$</td>
<td style="text-align: center;">Test only</td>
</tr>
<tr>
<td style="text-align: center;">T-5: Row-to-row transform (R2R)</td>
<td style="text-align: center;">Transform table data based on input/output examples ( $[23,24,27])$</td>
<td style="text-align: center;">Data transformation</td>
<td style="text-align: center;">synthesized (test: [24])</td>
<td style="text-align: center;">Train/Test</td>
</tr>
<tr>
<td style="text-align: center;">T-6: Entity matching (EM)</td>
<td style="text-align: center;">Match rows from two tables that refer to the same real-world entity ( $[32,38,41,66])$</td>
<td style="text-align: center;">Table matching</td>
<td style="text-align: center;">[1]</td>
<td style="text-align: center;">Train/Test</td>
</tr>
<tr>
<td style="text-align: center;">T-7: Schema matching (SM)</td>
<td style="text-align: center;">Match columns from two tables that refer to the same meaning ( $[30,36,44])$</td>
<td style="text-align: center;">Table matching</td>
<td style="text-align: center;">synthesized (test: [30])</td>
<td style="text-align: center;">Train/Test</td>
</tr>
<tr>
<td style="text-align: center;">T-8: Data imputation (DI)</td>
<td style="text-align: center;">Predict the missing values in a cell based on the table context ( $[7,37])$</td>
<td style="text-align: center;">Data cleaning</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train/Test</td>
</tr>
<tr>
<td style="text-align: center;">T-9: Error detection (ED)</td>
<td style="text-align: center;">Detect data values in a table that is a likely error from misspelling ( $[14,45])$</td>
<td style="text-align: center;">Data cleaning</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train/Test</td>
</tr>
<tr>
<td style="text-align: center;">T-10: List extraction (LE)</td>
<td style="text-align: center;">Extract a structured table, from a list that lacks explicit column delimiters [9, 13, 19]</td>
<td style="text-align: center;">Data transformation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-11: Head value matching (HVM)</td>
<td style="text-align: center;">Match column-headers with its data values drawn from the same table</td>
<td style="text-align: center;">Table matching</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-12: Natural-language to SQL (NS)</td>
<td style="text-align: center;">Translate a natural-language question on a table into a SQL query ( $[62,65])$</td>
<td style="text-align: center;">NL-to-SQL</td>
<td style="text-align: center;">[65]</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-13: Table summarization (TS)</td>
<td style="text-align: center;">Produce a natural-language summary for the content in a table</td>
<td style="text-align: center;">Data augmentation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-14: Column augmentation (CA)</td>
<td style="text-align: center;">Augment a table with additional columns compatible with a given table</td>
<td style="text-align: center;">Data augmentation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-15: Row augmentation (RA)</td>
<td style="text-align: center;">Augment a table with additional rows compatible with a given table</td>
<td style="text-align: center;">Data augmentation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-16: Row/column swapping (RCSW)</td>
<td style="text-align: center;">Manipulate a given table, by swapping the position of two rows or columns</td>
<td style="text-align: center;">Table manipulation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-17: Row/column filtering (RCF)</td>
<td style="text-align: center;">Manipulate a given table, by filtering on given rows or columns</td>
<td style="text-align: center;">Table manipulation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
<tr>
<td style="text-align: center;">T-18: Row/column sorting (RCS)</td>
<td style="text-align: center;">Manipulate a given table, by performing sorting on given rows or columns</td>
<td style="text-align: center;">Table manipulation</td>
<td style="text-align: center;">synthesized</td>
<td style="text-align: center;">Train only</td>
</tr>
</tbody>
</table>
<p>Table 2: A summary of 18 table-related tasks, which we collect and synthesize, in order to "table-tune" GPT into Table-GPT. [Task categories]: These tasks cover diverse areas such as: table understanding, table-QA, table matching, table cleaning, table transformation, etc. Some of these tasks (T-1 to T-4) are used as unseen hold-out tasks, to evaluate Table-GPT ability to generalize to completely new and unseen tasks. [Table Data]: we choose to "synthesize" table tasks from diverse real tables when possible (e.g., when ground-truth can be produced automatically), to ensure the diversity of the training data and avoids over-fitting. When the ground-truth cannot be automatically produced (e.g., entity-matching, table-QA, NL-to-SQL, etc.), we use existing benchmark data from the literature.</p>
<p>The challenge, however, is that prior work on instruction-tuning have shown that the quality of the "(instruction, completion)" pairs is crucial [40, 50], to the extent that companies hired armies of human labelers to manually label such data, (e.g., instruction: "write a bed-time story with a bear goes to beach", completion: an-actual-story-with-bears) [40], to ensure the quality and diverse of the training data.</p>
<p>We would like to replicate the success of instruction-tuning in the table domain, but ideally without the expensive human labeling.</p>
<p>Reusing existing benchmark data: insufficient diversity. One approach to generate table-tasks, is to use existing benchmark data
published in the database literature (similar efforts were made in the NLP literature for instruction-tuning [59]).</p>
<p>However, we found that the existing benchmark data to have: (1) limited task-diversity: as the literature tends to focus on a few select table-tasks that are hard and challenging (e.g., entitymatching and data-transformation); and
(2) limited data-diversity: as benchmark data are typically labeled manually by researchers, only on a few specific datasets, which is sufficient for benchmark evaluation purposes, but insufficient when we want to use them as "training data" for language models. Our attempt to use only existing benchmark data for table-tuning leads to over-fitting, due to the lack of task and data diversity.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Synthesize</span><span class="w"> </span><span class="nx">table</span><span class="o">-</span><span class="nx">tasks</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">table</span><span class="o">-</span><span class="nx">tuning</span>
<span class="w">    </span><span class="nx">input</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">corpus</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">diverse</span><span class="w"> </span><span class="nx">real</span><span class="w"> </span><span class="nx">tables</span><span class="w"> </span><span class="nx">C</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">table</span><span class="o">-</span><span class="nx">task</span><span class="w"> </span><span class="nx">types</span><span class="w"> </span><span class="nx">S</span>
<span class="w">    </span><span class="nx">output</span><span class="w"> </span><span class="p">:</span><span class="nx">Diverse</span><span class="w"> </span><span class="nx">synthesized</span><span class="w"> </span><span class="nx">table</span><span class="o">-</span><span class="nx">tasks</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">A</span><span class="p">=</span><span class="err">\</span><span class="p">{(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Ins</span><span class="p">},</span><span class="w"> </span><span class="nx">T</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="p">)</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="err">\</span><span class="p">},</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">foreach</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">C</span><span class="p">},</span><span class="w"> </span><span class="nx">S</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">S</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="p">(</span><span class="nx">Ins</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Synthesize</span><span class="o">-</span><span class="nx">Table</span><span class="o">-</span><span class="nx">Task</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">S</span><span class="p">,</span><span class="w"> </span><span class="nx">T</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">Section</span><span class="w"> </span><span class="m m-Double">4.2</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">D</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Ins</span><span class="p">},</span><span class="w"> </span><span class="nx">T</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">foreach</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">I</span><span class="w"> </span><span class="nx">n</span><span class="w"> </span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">T</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">Ins</span><span class="err">&#39;</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Augment</span><span class="o">-</span><span class="nx">Instruction</span><span class="p">(</span><span class="nx">Ins</span><span class="p">)</span><span class="w"> </span><span class="c1">// (Section 4.3)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Augment</span><span class="o">-</span><span class="nx">Table</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">T</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">Section</span><span class="w"> </span><span class="m m-Double">4.3</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">C</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Augment</span><span class="o">-</span><span class="nx">Completion</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">C</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">Section</span><span class="w"> </span><span class="m m-Double">4.3</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">A</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Ins</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">T</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="nx">C</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">A</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Our approach: Synthesis-then-Augment. We therefore propose a "synthesize-then-augment" approach to create diverse table-tasks using real tables, which can be used as training-data to table-tune language-models.</p>
<p>We show the main steps of our synthesize-then-augment approach in Algorithm 1. First, we sample a table $T \in \mathbf{C}$ from a large corpus of real tables $\mathbf{C}$, and a type of table-task $S \in \mathbf{S}$. From the $(T, S)$ pair, we synthesize an instance of a table-task $t=(I n s, T, C)$ (line 3), which is the task-synthesis step we will discuss in detail in Section 4.2. From the set of diverse instances of table-tasks created (Ins, $T, C$ ), we then proceed to "augment" the tasks, at instruction/table/completion levels (line 6-8), which is a step that we will describe in Section 4.3. The resulting table-tasks $A=\left{\left(I n s^{\prime}, T^{\prime}, C^{\prime}\right)\right}$ become the training data we use to table-tune language-models.</p>
<h3>4.2 Synthesize diverse table-tasks</h3>
<p>We now describe how we synthesize diverse instances of table-tasks $t=(I n s, T, C)$ (Line 3 of Algorithm 1), so as to exercise languagemodels ability to understand two-dimensional table structures.</p>
<p>We propose two complementary approaches that (1) synthesize new table-tasks for task-diversity, and (2) synthesize new table test-cases of existing table-tasks for data-diversity. We will discuss each below in turn.</p>
<p>Synthesize new table-tasks for task-diversity. Since our goal is to enhance language models' ability to understand tables, we believe it is not necessary to focus exclusively on challenging tabletasks that have been the focus of the literature [45]. Instead, we propose a number of table-understanding/augmentation/manipulation tasks that are easy to synthesize, leveraging large amounts of real tables that already exist. Specifically, we crawled 2.9 M high-quality web-tables (e.g., Wikipedia) [10], referred to as $\mathbf{C}^{\text {wt }}$, and 188 K database-tables (extracted from BI data models) [33], referred to as $\mathbf{C}^{d b}$, and synthesize table-tasks based on real tables sampled from the corpus.</p>
<p>We will go over the list of synthesized table-tasks below:
(T-13) Table summarization (TS). Since web-tables often have descriptive titles, we synthesize a table-summarization task, where we ask the model to summarize the content in a table. Specifically, we sample $T \in \mathbf{C}^{\text {wt }}$ whose extracted table-title title $(T)$ are neither too long nor too short, and create a table-summarization task as:</p>
<p>$$
T S(T)=\left(I n s^{T S}, T, \text { title }(T)\right)
$$</p>
<p>where $I n s^{T S}$ is the canonical human-instruction to describe the TS task (e.g., "Please provide a succinct summary for the table below"), which we will further augment for diversity (Section 4.3), $T$ is the input table we sampled from $\mathbf{C}^{\text {wt }}$, and title $(T)$ is its expected completion.</p>
<p>This task is designed to use real tables, with real human annotated titles, to enhance models ability to read tables and understand the highlights from the table. Note that although we use title $(T)$ as the expected completion/answer, it does not overconstrain language-models to over-fit on such answers - it only nudges language-models in that general direction, just like training data in the form of ("write a bed-time story with a bear goes to beach" $\rightarrow$ an-actual-human-written-story) used in instructiontuning does not over-constrain/over-fit the underlying models.
(T-14) Column augmentation. Since we have lots of real tables in $\mathbf{C}^{\text {wt }}$ and $\mathbf{C}^{d b}$, we create a task where we take the first $k$ columns in a table $T$, denoted as $C_{[1, k]}(T)$, and ask the language-models to generate the $(k+1)$-th column $C_{k+1}(T)$, written as follows:</p>
<p>$$
C A(T, k)=\left(I n s^{C A}, C_{[1, k]}(T), C_{k+1}(T)\right)
$$</p>
<p>where $I n s^{C A}$ is again the natural-language instruction that describes the row-augmentation task. This task exercises a model's ability to generate realistic columns given a table context that need to be semantically compatible.
(T-15) Row augmentation (RA). Similar to Column-augmentation, we synthesize a Row-augmentation task where we sample a table $T$ and ask the model to generate the $(k+1)$-th row, given the first $k$ rows, written as:</p>
<p>$$
R A(T, k)=\left(I n s^{R A}, R_{[1, k]}(T), R_{k+1}(T)\right)
$$</p>
<p>This task exercises a model's ability to synthesize realistic rows given a table context, which need to align vertically with existing rows.
(T-16) Row/column swapping (RS/CS). In this task, we ask the models to perform a table-manipulation step, where given a sampled table $T$, we provide an instruction to swap the $i$-th and $j$-th row. We programmatically generate the resulting output table from the swap operation, denoted as $\operatorname{Swap}\left(T, R_{i}, R_{j}\right)$, which is the target "completion". The Row-swapping task $R S_{i, j}(T)$ is written as:</p>
<p>$$
R S_{i, j}(T)=\left(I n s^{R S}, T, S \operatorname{wap}\left(T, R_{i}, R_{j}\right)\right)
$$</p>
<p>We similarly synthesize the Column-swapping task $C S_{i, j}(T)$ as:</p>
<p>$$
C S_{i, j}(T)=\left(I n s^{C S}, T, S \operatorname{wap}\left(T, C_{i}, C_{j}\right)\right)
$$</p>
<p>We note that tasks like Row/Column-swapping would seem simple to perform, both programmatically or through UI interactions (e.g., inside spreadsheets using menu options), and are therefore not tasks studied in the literature (unlike more challenging tasks like entitymatching or data-transformation). We are similarly not intending to use table tasks as "tests", but because "tables serialized as naturallanguage texts" are ultimately the only way to feed input into language models (regardless of whether we want to output to be text/code/table/etc.), these table-tasks are still useful as "training data" for models to better read and understand tables.
(T-17) Row/column filtering. In this table-manipulation task, we ask models to filter down to specific rows/columns on a sampled</p>
<p>table $T$, based on a specified set of row/column indexes $S$ :</p>
<p>$$
\begin{aligned}
&amp; R F_{S}(T)=\left(\text { Ins }^{R F}, T, R_{S}(T)\right) \
&amp; C F_{S}(T)=\left(\text { Ins }^{C F}, T, C_{S}(T)\right)
\end{aligned}
$$</p>
<p>These tests are again meant to exercise model ability to manipulate tables, where cells in both vertical and horizontal directions need to be aligned.
(T-18) Row/column sorting (RS/CS). In the sorting tasks, we ask models to sort rows in a table $T$, based on values in a column $C$, where the expected output table can be programmatically generated, which we write as $\operatorname{Sort}<em C="C">{C}(T)$, so that the task $R S</em>(T)$ is:</p>
<p>$$
R S_{C}(T)=\left(\text { Ins }^{R S}, T, \operatorname{Sort}_{C}(T)\right)
$$</p>
<p>Similarly, we have a task to sort columns in a table $T$, based on column-headers $H$, written as $C S s(T)$ :</p>
<p>$$
C S(T)=\left(\text { Ins }^{C S}, T, \text { Sort }_{H}(T)\right)
$$</p>
<p>We note that the sorting tasks are fairly challenging for languagemodels - while we do not expect models to be perfect on such tasks, they exercises model ability to manipulate tables nevertheless.
(T-11) Head-value matching (HVM). In this task, we sample a table $T$, remove all its column headers $H$ to produce the corresponding table without headers, $\bar{T}$. We then shuffle these headers $H$, and ask models to fill $H$ into $T^{\prime}$, to produce the $\operatorname{HVM}(T)$ task:</p>
<p>$$
\operatorname{HVM}(T)=\left(\text { Ins }^{H V M}, \bar{T}, T\right)
$$</p>
<p>Like other tasks above, HVM is another task that we can synthesize in large quantities, using real tables, and without labeling. It is intended to be a task that helps models to better understand and correlate the semantics of column-headers and values.</p>
<p>Discussions. We show in our experiments, that using synthesized table-tasks on diverse tables improves the task- and data-diversity, which lead to better model generalizability (our ablation study shows that without these synthesized tasks there is a substantial drop in model quality).</p>
<p>Our list of synthesized table-tasks, however, is obviously not meant to be exhaustive, and is only a starting point. We believe that with some creativity, many more tasks can be synthesized to further improve the table-tuning process. For comparison, the NLP community has amassed over 1000 tasks for instruction-tuning, in a community effort [15], where they show that having more and diverse tasks always helps instruction-tuning.</p>
<p>Synthesize new table test-cases for data-diversity. There are a number of existing and important table-tasks, such as datatransformation, entity-matching, etc. that are extensively studied in the database literature. We want to use these established tasks in table-tuning too, also in the "(instruction, table, completion)" format. However, like mentioned earlier, the existing benchmarks for these tasks are typically manually labeled on a few datasets, which can be used to evaluation, but are unfit as training data for table-tuning, due to their limited quantities and diversity.</p>
<p>Instead, we synthesize new table test-cases for these established table-tasks, using real tables sampled from $\mathbf{C}^{\text {set }}$ and $\mathbf{C}^{\text {db }}$.
(T-5) Row-to-row Data Transformation (R2R) [23, 24]. To synthesize diverse test tables with data-transformations, we run a production-quality program-synthesizer [24], on web-tables sampled from $\mathbf{C}^{\text {web }}$, to identify tables $T \in \mathbf{C}^{\text {web }}$ where some columns
$C_{\text {in }} \subset T$ can be transformed into $C_{\text {out }} \subset T$, using an inferred program $P$, such that $P\left(C_{\text {in }}\right)=C_{\text {out }}$ hold on all rows in $T$ (e.g., (first-name, last-name) $\rightarrow$ (full-name) in the same table [26]). We then remove one random value $v \in C_{\text {out }}$ from $T$, to produce a test table $T_{-v}$ where $v$ is missing. We then synthesize a task $R 2 R(T)$ :</p>
<p>$$
R 2 R(T)=\left(\text { Ins }^{R 2 R}, T_{-v}, T\right)
$$</p>
<p>where given $T_{-v}$ as the input, we want to the model to infer the transformation and fill in the missing $v$ to produce $T$.
(T-7) Schema Matching (SM) [44]. To synthesize new table test cases for schema matching, we sample a real table $T$, and take the first $k$ rows of $T$ to produce $T_{1}=R_{[1, k]}(T)$. We then take the next $k$ rows from $T$ to produce $T_{2}=R_{[k+1,2 k]}(T)$, where we additionally "paraphrase" the column-headers of the original $T$, into new columnheaders in $T_{2}$, using a mapping of semantically-similar columnnames generated by GPT, denoted as $M$ (e.g., "company names" $\rightarrow$ "enterprises", "emp-id" $\rightarrow$ "employee identifier", etc.). Finally, we shuffle the columns in $T_{1}$ and $T_{2}$, and make the two a test case for schema matching, where the ground-truth is in $M$. The resulting task is written as $S M(T)$ :</p>
<p>$$
S M(T)=\left(\text { Ins }^{S M},\left(T_{1}, T_{2}\right), M\right)
$$</p>
<p>This again can systematically generate large numbers of schemamatching test tables, as training data for table-tuning.
(T-8) Data Imputation (DI) [7, 37]. For data imputation, we randomly sample a real table $T$, and then remove a random value $v \in T$, to produce $T_{-v}$. The task $D I(T)$ is then to predict the missing $v$ from its table context:</p>
<p>$$
D I(T)=\left(\text { Ins }^{D I}, T_{-v}, v\right)
$$</p>
<p>Note that while not all missing values $v$ in DI tasks so generated can be reliably predicted, it nevertheless exercises models' ability to leverage correlations that exist between values in the row and column contexts.
(T-9) Error Detection (ED) [45]. To synthesize error-detection tasks, we sample a real table $T \in \mathbf{C}^{\text {set }}$, and generate a modified $\hat{T}$, where we replace a value $v \in T$ with $v^{\prime}$, using an existing package [3] that injects one likely typographic error into $v$. The task $E D(T)$ is then:</p>
<p>$$
E D(T)=\left(\text { Ins }^{E D}, \hat{T}, v^{\prime}\right)
$$</p>
<p>where we aim to identify the misspelled $v^{\prime} \in \hat{T}$ based on surrounding table context.
(T-10) List extraction (LE) [13, 19]. To synthesize the task of extracting tables from list data without explicit column-delimiters, we sample a table $T$, and replace all column separators with white spaces to generate its unsegmented list-form $L(T)$. The task $L E(T)$ is then:</p>
<p>$$
L E(T)=\left(\text { Ins }^{L E}, L(T), T\right)
$$</p>
<p>which is to produce the correct column-segmentation of $L(T)$, and generate the corresponding table $T$, based on value alignment in the vertical direction.</p>
<p>Since we have large numbers of diverse tables, in Line 3 of Algorithm 1 we make sure that each table $T$ is used by one task-type above, to synthesize one instance of table-task, to ensure the diversity of data we generate.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Example table-tasks we generate for (T-8) Entitymatching, and (T-9) Error-detection, using "augmentedcompletions" that contain reasoning steps similar to chain-of-thought, which when used as training-data in table-tuning, can ground model responses and improve result quality.</p>
<h3>4.3 Augment synthesized table-tasks</h3>
<p>From synthesized instances of table-tasks $t=(I n s, T, C)$, we then perform additional augmentations at multiple levels, corresponding to steps in Line 6-Line 8 of Algorithm 1, where the goal is to create even more task/data diversity and avoid over-fitting in table-tuning.</p>
<p>We will go over different levels of augmentations below in turn.
Instruction-level augmentations. At the instruction level, because using the same instruction repeatedly across training-data instances can lead to over-fitting [57], we augment the canonical instruction using generative models like GPT to paraphrase the canonical human-written instruction into many different variants.</p>
<p>For example, for the task-type (T-13): Table-Summarization (Section 4.2), the canonical human-written instruction is: "Please look at the table below and provide a title that can summarize the table". We generate alternative instructions for the task using language-models, to produce variations such as "Please examine the table below and give it a descriptive title", in a manner similar to [57], which we can then use to populate instances of table-tasks as alternative instructions (Line 6).</p>
<p>Table-level augmentations. At the table-level, we know that two-dimensional tables should largely be "permutation-invariant", where permutating rows and columns should generally lead to a table with similar semantic meanings (Section 3), at the tablelevel we can perform operations such as column-permutation, rowpermutation, column-sampling, and row-sampling, to increase the diversity of tables used in our table tasks.</p>
<p>When the training data has an original instance of the table-task, $t=(I n s, T, C)$, and its augmented version $t^{\prime}=\left(I n s, T^{\prime}, C\right)$, where $T^{\prime}$ is an augmented version of $T$, which has the same semantic meaning and thus the same completion $C$, the hope is that by continuing to train language-models on such training-data, we can increase model stability on tables and make them less sensitive to "semanticpreserving table-operations" (e.g., column-reordering like discussed in Section 3).</p>
<p>Completion-level augmentations. At the completion-level, for a synthesized instance of table-task $t=(I n s, T, C)$, we augment our original completion $C$ by generating $C^{\prime}$ that adds reasoning steps into the original $C$, after we observe that performing reasoning-steps (similar to [60]) on more complex table-tasks (e.g., entity-matching and error-detection) can lead to better task performance.</p>
<p>Language-model assisted completion-augmentation. We show the completion-level augmentation using examples in Figure 7. The left of the figure is the task of (T-8) Entity-matching, where models need to identify whether two table-rows may refer to the same real-world entities. We tried two modes of operations on vanilla language-models (before table-tuning):
(1) Direct-answer: where models are asked to produce a "yes/no" answer directly, without additional reasoning;
(2) Reason-then-answer: where we require the models to "explain step-by-step" in the prompt, before producing a "yes/no" answer. We observe that for complex tasks like Entity-matching, models in the "direct-answer" mode are more prone to make mistakes (e.g., answering with only a "no" in the response), leading to lower accuracy; whereas with "reason-then-answer", even vanilla language-models perform better, likely because the step-by-step reasoning chain makes it likely to generate the correct answer, similar to what was observed in the NLP literature (e.g., chain-of-thought reasoning [60] and self-consistency [55], etc.).</p>
<p>Recall that our goal is to table-tune language models, so that the resulting model can perform better on complex table-tasks, by hopefully developing a habit of performing step-by-step reasoning. In order to produce training-data for table-tuning, so that languagemodels can learn-to-reason on complex tasks, we augment $t=$ $(I n s, T, C)$ by replacing the direct yes/no answer $C$, with a more detailed answer using step-by-step reasoning (Figure 7 shows a simplified example), which we generate by providing GPT with the yes/no answer, and then ask the model to produce reasoning-steps as our new $C^{\prime}$. We find this to encourage table-tuned models to perform correct reasoning-steps, on other complex table-tasks that are similar in nature.</p>
<p>Ground-truth assisted completion-augmentation. The right of Figure 7 shows another example of completion-augmentation, where we use ground-truth to produce step-by-step reasoning, for the example task of (T-9): Error-detection. The task here is to identify misspelled cells from a given table. We find that vanilla languagemodels to produce lots of false-positives on the error-detection task, where the models confidently predict a cell-value (e.g., an abbreviation or uncommon company/person/location names) to be misspelled, when no such misspellings exist upon close inspection. The tendency to produce false-positives persists, with or without "step-by-step" chain-of-thought style prompting.</p>
<p>Because we cannot rely on chain-of-thought style prompts to generate correct reasoning steps as augmented-completions $C^{\prime}$, we instead use the ground-truth available to us in the table-task synthesis step (Section 4.2), to generate augmented completions that embeds the reasoning step. Specifically, like shown in Figure 7 (right), we augment the completion to contain not only the prediction (a value $v$ is misspelled), but also generate the explanation that points out the correct version of the predicted misspelling</p>
<p>(e.g., "Missisipi" should be "Mississippi"). We find this grounds the language-models predictions with an actual explanation, which substantially reduces false-positives and improve result quality.</p>
<p>Additional augmentations. Along the lines of augmentations, there are additional types of augmentations we perform, including "template-level augmentation", where we mix zero-shot task template and few-shot task template (which appends multiple input-table/output-completion examples after the instruction Ins), as well as "task-level augmentation" (by synthesizing new types of tabletasks), which all improve training-data diversity and help tabletuning.</p>
<h3>4.4 Table-GPT as "table foundation models"</h3>
<p>Using the synthesis-then-augment approach in Algorithm 1, describe in previous sections, we now generate large numbers of diverse table-tasks $A={(I n s, T, C)}$. We then continue to train language models such as GPT, using serialized (Ins, $T$ ) as the "prompt", and $C$ as the "completion", where we minimize the language-modeling loss of completion given the prompt, subject to regularization. We refer to this process as table-tuning.</p>
<p>Let $M$ be a decoder-style language model, such as GPT and ChatGPT, let TableTune $(M)$ be the table-tuned version of $M$. We argue that TableTune $(M)$ could serve as a better "table foundation model", if it performs better than $M$ on table-tasks, in all of the following scenarios:
(1) Out of the box zero-shot: when we use only instructions for $M$ or TableTune $(M)$ to perform table-tasks;
(2) Out of the box few-shot: when we use instructions and randomly selected few-shot examples to perform table-tasks;
(3) Task-specific prompt-tuning: when we have a small amount of labeled data for a downstream task, and perform prompt-tuning to select the best instruction/example combinations;
(4) Task-specific fine-tuning: when we have sufficient amounts of labeled data, and perform task-specific fine-tuning for a task. If table-tuning is effective for language models to learn to better understand and manipulate tables, we expect that TableTune $(M)$ can perform better on most if not all of the scenarios described above, which is the goal of our experimental evaluation next.</p>
<h2>5 EXPERIMENTS</h2>
<p>We perform extensive experiments to evaluate table-tuned GPT relative to vanilla GPT on diverse table tasks. We plan to release our code and data after internal reviews ${ }^{1}$.</p>
<h3>5.1 Experiment Setup</h3>
<p>Models Compared. We test the following models.</p>
<ul>
<li>GPT-3.5 (text-davinci-002). This 175B model is available from OpenAI, and is one of the vanilla GPT models that we compare with.</li>
<li>Table-GPT-3.5 (text-davinci-002 +table-tune). This is the model we obtain by performing table-tuning on GPT-3.5 (text-davinci-002). We compare the performance of Table-GPT-3.5 with GPT-3.5.</li>
<li>ChatGPT (text-chat-davinci-002). This is a version of the ChatGPT model available internally [28], which we use as a second vanilla base model, from which we perform table-tuning.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- Table-ChatGPT (text-chat-davinci-002 +table-tune). This is the model we obtain by performing table-tuning on ChatGPT (text-chat-davinci-002), which we compare with the vanilla ChatGPT.</p>
<p>Training tasks and data. In our default settings, we use a total of 14 types of table-tasks, listed as T-5 to T-18 in Table 2, as training data for table-tuning.</p>
<p>In all but two task-types (T-6: Entity Matching and T-12: NL-toSQL), we use synthesized instances of table-tasks. For each task type, we generate 1000 instances of table-tasks using a 50:50 mix of zero-shot and few-shot templates, following a synthesis-thenaugment approach described in Section 4. During task-synthesis, we sample from 125 k real web-tables $\mathrm{C}^{\text {wt }}$ and database-tables $\mathrm{C}^{d b}$ (aggressively deduped from over 2 M original tables). For Entity Matching and NL-to-SQL where realistic labels/completions cannot be automatically synthesized, we use existing manually-labeled benchmark data, from [1] and [65], respectively.
Test tasks and data. To evaluate the benefit of table-tuning, we test the performance of paired models that are table-tuned vs. vanilla un-tuned, namely, we compare (GPT-3.5 vs. Table-GPT-3.5) and (ChatGPT vs. Table-ChatGPT), as two groups.</p>
<p>We test on 4 unseen tasks (T-1 to T-4 in Table 2), which are completely unseen during table-tuning, to evaluate whether our table-tuned models can continue to follow to human-instructions and perform novel unseen tasks (like illustrated in Figure 4). In addition, we make sure that the test-data used in unseen tasks, are completely separate from the tables used in synthesizing table-tasks as training-data for table-tuning. Specifically, our training data for table-tuning are always drawn from web-tables $\mathrm{C}^{\text {wt }}$ and databasetables $\mathrm{C}^{d b}$, whereas test-data used in our synthesized table-tasks (T1: Missing-value identification and T2: Column-finding) are always drawn from a corpus of real spreadsheet tables $\mathrm{C}^{s p}$, completely separate from $\mathrm{C}^{\text {wt }}$ and $\mathrm{C}^{d b}$ and with very different characteristics. For the remaining two unseen tests (T-3: Table Question and T-4: Column Type Annotation), we use established benchmark data [42] and $[16,25,51]$ respectively, which are unseen during table-tuning.</p>
<p>We also evaluate 5 seen tasks (T-5 to T-9 in Table 2), which are important table-tasks extensively studied in the literature, which we want table-tuned models to be exposed of to understand these table-related concepts. While these task-types are seen during tabletuning, we make sure that the test datasets are completely separate from the training data used in table-tuning. For synthesized tabletasks (T-8 Data Imputation), similar to discussed above, our test cases are always drawn from a corpus of real spreadsheet tables $\mathrm{C}^{s p}$, separate from the corpus of web-tables $\mathrm{C}^{\text {wt }}$ and database-tables $\mathrm{C}^{d b}$ used in synthesizing training table-tasks, in order to test tabletuned models' ability to generalize to new tables. For other tasks, we use existing benchmark data, completely unseen when training table-tuned models (e.g., [24] for T-5: Row-to-row transformation, [1] for T-6: Entity-matching, using the same setup as [39], [30] for T-7: Schema-matching). The task of (T-9) Error-detection is of high value for our business, where we manually labeled a benchmark using real spreadsheet-tables and web-tables for this evaluation.</p>
<p>Details of test data and their statistics can be found in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Few-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Zero-Shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Few-Shot</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">+table-tune</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">+table-tune</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">+table-tune</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">+table-tune</td>
</tr>
<tr>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Column Finding</td>
<td style="text-align: center;">Spreadsheets-CF</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Column Type Annotation</td>
<td style="text-align: center;">Efthymiou</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.861</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Linuye</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.853</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sherlock</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.553</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2D</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.912</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Missing Value Identification</td>
<td style="text-align: center;">Column (no separator)</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.474</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Column (with separator)</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.665</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Row (no separator)</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.894</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Row (with separator)</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.968</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Table Question</td>
<td style="text-align: center;">Wiki</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.527</td>
</tr>
<tr>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Data Imputation</td>
<td style="text-align: center;">Spreadsheets-DI</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.609</td>
<td style="text-align: center;">0.649</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entity Matching</td>
<td style="text-align: center;">Amazon-Google</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.701</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.963</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-ACM</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.938</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-GoogleScholar</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fodors-Zagats</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.977</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walmart-Amazon</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.824</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">iTunes-Amazon</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.929</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Error Detection</td>
<td style="text-align: center;">Spreadsheets-Real</td>
<td style="text-align: center;">0.058</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.058</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.551</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebTables-Real</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.684</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Schema Matching</td>
<td style="text-align: center;">DeepM</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Row-to-Row Transformation</td>
<td style="text-align: center;">BingQL-Unit</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">N.A.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.446</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BingQL-other</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.607</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FF-GR-Trifacta</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.825</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Headcase</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.795</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stackoverflow</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.758</td>
</tr>
</tbody>
</table>
<p>Table 3: Detailed results of Table-tuning, on both GPT-3.5 and ChatGPT, for individual datasets. Zero-shot is not applicable to row-to-row by-example transformations (marked as "N.A."), which requires examples. For all "Unseen" tasks, the tasks are held-out and unseen during table-tuning. For all "Seen" tasks, the task is seen during table-tuning, but the test datasets are held-out and unseen.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Overall quality improvement, between vanilla ChatGPT and Table-ChatGPT.</p>
<p>Table 4: Details of test data and evaluation metrics</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Evaluation Metrics</th>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T-1: Missing Value Identification</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Column (no Separator)</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Column (with Separator)</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Row (no Separator)</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Row (with Separator)</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">T-2: Column Finding</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Spreadsheets-CF</td>
<td style="text-align: center;">841</td>
</tr>
<tr>
<td style="text-align: center;">T-3: Table Question</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Wiki</td>
<td style="text-align: center;">4344</td>
</tr>
<tr>
<td style="text-align: center;">T-4: Column Type Annotation</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Efthymiou</td>
<td style="text-align: center;">594</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Limaye</td>
<td style="text-align: center;">174</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sherlock</td>
<td style="text-align: center;">971</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T2D</td>
<td style="text-align: center;">367</td>
</tr>
<tr>
<td style="text-align: center;">T-5: Row-to-Row Transformation</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">BingQL-Unit</td>
<td style="text-align: center;">103</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BingQL-other</td>
<td style="text-align: center;">1102</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FF-GR-Trifacts</td>
<td style="text-align: center;">132</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Headcase</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stackoverflow</td>
<td style="text-align: center;">145</td>
</tr>
<tr>
<td style="text-align: center;">T-6: Entity Matching</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Amazon-Google</td>
<td style="text-align: center;">2293</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-ACM</td>
<td style="text-align: center;">2473</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-GoogleScholar</td>
<td style="text-align: center;">5742</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fodoro-Zugats</td>
<td style="text-align: center;">189</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walmart-Amazon</td>
<td style="text-align: center;">2049</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">iTunes-Amazon</td>
<td style="text-align: center;">109</td>
</tr>
<tr>
<td style="text-align: center;">T-7: Schema Matching</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">DeepM</td>
<td style="text-align: center;">41</td>
</tr>
<tr>
<td style="text-align: center;">T-8: Data Impuation</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Spreadsheets-DI</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">T-9: Error Detection</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Spreadsheets-Real</td>
<td style="text-align: center;">870</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebTables-Real</td>
<td style="text-align: center;">432</td>
</tr>
</tbody>
</table>
<h3>5.2 Quality Comparisons: Unseen + Seen tasks</h3>
<p>In Figure 8, we compare the performance between (GPT-3.5 vs. Table-GPT-3.5), and in Figure 9, we compare the performance between (ChatGPT vs. Table-ChatGPT), which are table-tuned vs. untuned vanilla models, using GPT-3.5 and ChatGPT as base-models, respectively. Within each task-group in the figures, we show 4 bars, where the first two correspond to zero-shot settings, and the last two correspond to few-shot settings. We can see that across the board, table-tuned models show strong performance benefits on diverse table-tasks.</p>
<p>It is interesting to note that the benefit of table-tuning is observed when both GPT-3.5 and ChatGPT are used as base-models, showing the generality of our proposed table-tuning approach, on top of underlying language-models of different styles.</p>
<p>Table 4 shows a detailed breakdown of the results, at the individual data-set level. We can see that across 26 test datasets, on 2 base-models (GPT-3.5 and ChatGPT), in 2 settings (zero-shot and few-shot), for a total of 104 tests, table-tuned models outperform their vanilla un-tuned counterparts in 98/104 tests (with the remaining being 3 ties and 3 losses), showing the strong performance benefits of table-tuning.</p>
<h3>5.3 Benefits on task-specific optimizations</h3>
<p>In addition to performing well out-of-the-box in zero-shot and (random) few-shot settings, as shown above, table-tuned GPT models could potentially be used as "table foundation models", if they continue to show quality benefits on downstream tasks, when taskspecific optimizations are applied.</p>
<p>Like we discussed in Section 4.4, these include (1) single-task prompt-engineering, where we select the best instructions and fewshot examples for a single task, using a small number of labeled examples; and (2) single-task fine-tuning, where we continue to fine-tune models for a specific task, with a sufficient number of
labeled examples. We will study the benefit of table-tuning in these two settings below.</p>
<p>Single-task prompt-engineering: We perform prompt-engineering for Table-GPT-3.5 and GPT-3.5, on the column-type-annotation (CTA) task (using the Efthymiou [16] dataset), by selecting the best few-shot examples using 200 labeled examples (randomly sampled from the ground-truth), where the goodness of a prompt is evaluated on the labeled examples. Figure 10 shows the top-5 prompts selected, for Table-GPT-3.5 and GPT-3.5, respectively. We can see that Table-GPT-3.5 consistently outperforms GPT-3.5, on the 5 best prompts produced from prompt-engineering.
Single-task fine-tuning: We perform task-specific fine-tuning, on Table-GPT-3.5 and GPT-3.5, using labeled data for that specific task. Table 11(a) and Table 11(b) show the comparison, on the CTA task (using Efthymiou [16]) and Table-Question-Answering or TQA (using WikiTableQuestions [42]), respectively. In both cases, we vary the amount of training data on the x -axis. As expected, the performance of both Table-GPT-3.5 and GPT-3.5 benefit from continued task-specific fine-tuning, but with the same amount of training data, Table-GPT-3.5 continues to dominate GPT-3.5. Looking at the graph from a different way, to achieve the same performance (y-axis), fine-tuning Table-GPT-3.5 would require a smaller number of labeled data than fine-tuning the vanilla GPT-3.5.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Comparison of quality, when using promptengineering. Results shown are for 5 best prompt-templates on the Efthymiou dataset.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Single Task Fine-tuning</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Vary number of training tasks</p>
<h3>5.4 Sensitivity Analysis</h3>
<p>We perform sensitivity analysis to better understand table-tuning.
Varying the number of training tasks. To see whether using more training tasks brings a general benefit, we sample 1/5/10 tasks from all of our training table-tasks for 4 times each, perform finetuning on each subset of tasks selected, and compute the average from these runs.</p>
<p>The average quality results are shown in Figure 13. As we can see, on the left of the figure with a small number of tasks (e.g., 1), table-tuning degenerates to single-task tuning, which actually hurts the performance of other tasks in general (notice that the performance corresponding to 1-task is lower than the dotted green line, which corresponds to GPT-3.5). As we have more training-tasks, the performance goes up consistently, for all tasks as well as for the average across all tasks, showing the benefit that is analogous to multi-task training.
Vary the amount of training data. Figure 13 shows the average performance on seen/unseen test tasks with different amounts of training data. As we can see, the performance on both seen and unseen tasks improves with training data, which plateaus as more data is used.
Vary base-model Size. To understand how the size of the basemodels affects the performance of table-tuned models, we tabletune four variants of GPT, namely, Text-Ada-001 (350M parameters), Text-Babbage-001 (3B parameters), Text-Curie-001 (13B parameters), Text-Davinci-002 (175B parameters).</p>
<p>Figure 14 shows the average performance of base-models vs. table-tuned models, on seen/unseen tasks. We can see that for the unseen tasks (important to check for model generalizability), tabletuned models produce little benefit on smaller models (Ada/Babbage/Curie), but the benefit becomes suddenly significant on larger models (GPT-3.5 and ChatGPT), which appear to be an emerging ability consistent with what is reported in other contexts (e.g., [8, 59]).</p>
<p>Vary prompt templates. To test the robustness of our table-tuned models, we generate 5 different prompt templates (task descriptions and special markers), which are paraphrased automatically using GPT, from a canonical prompt template written by humans.</p>
<p>Figure 15 shows the average model performance over all unseen test tasks for each prompt template. As we can see, different prompt templates introduce variations in performance, for both</p>
<p>Table-GPT-3.5 and GPT-3.5, but the former consistently outperforms the latter by more than 10 percentage points on all 5 prompt templates, showing the robustness of Table-GPT to different kinds of prompts.
Vary table formats. There are multiple options when serializing a table $T$ into text, such as Markdown, CSV, JSON, etc. We use the Markdown table format, because it is succinct, and furthermore vanilla GPT tends to generate tables in the Markdown format in responding to human questions, suggesting that it is the table format of its choice, likely because GPT is pre-trained on lots of GitHub code, where Markdown tables are abundant. To understand the effect of using different table formats in representing tables in prompts, we test two different table formats, namely CSV and JSON.</p>
<p>Table 5 shows the average performance when using different table formats. As we can see, the Markdown format on average performs better than other formats, although the gap is not too significant.</p>
<p>Table 5: Performance of Table-GPT-3.5, when different table formats are used to serialize tables</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">Markdown</th>
<th style="text-align: center;">CSV</th>
<th style="text-align: center;">JSON</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.713</td>
</tr>
<tr>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.621</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.672</td>
</tr>
</tbody>
</table>
<h3>5.5 Ablation Studies</h3>
<p>We perform ablation analysis to understand the benefit of different augmentation strategies (Section 4.3). The results are summarized in Table 6.</p>
<p>Table 6: Ablation Studies of table-tuning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;">Table-GPT-3.5</th>
<th style="text-align: center;">NoSyn</th>
<th style="text-align: center;">NoColPer.</th>
<th style="text-align: center;">NoPromptVar.</th>
<th style="text-align: center;">NoCOT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.728</td>
</tr>
<tr>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.666</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.701</td>
</tr>
</tbody>
</table>
<p>No task-level augmentation (no synthesized tasks). Because we synthesized diverse table-tasks for table-tuning (Section 4.2), our first ablation is to remove all such tasks from the training data. The result is shown in Table 6 as "NoSyn". As we can see, the average performance on seen and unseen tasks drops significantly, showing the contribution of our diverse synthesized table-tasks.
No table-level augmentation (no column permutations). We disable the table-level augmentation by turning off the column permutation. The result is shown in Table 6 as "NoColPer". We can see that the average performance on seen and unseen tasks is lowered without column permutations.
No instruction-level augmentation (no prompt variations). We then disable the instruction-level augmentation, by using only one canonical prompt template for each task (without paraphrasing). The result is shown in Table 6 as "NoPromptVar". As we can see, the average performance of seen and unseen tasks drops slightly, likely because diverse types of table-tasks we use can somewhat mitigate the negative effect of using repeated instruction templates.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Vary Training Size
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Vary Model Size
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Vary Templates</p>
<p>No completion-level augmentation (no chain-of-thought). We drop the augmentation at completion level by removing the chain-of-thought (COT) reasoning from the completion in the finetuning data. The result is shown in Table 6 as "NoCOT". The average performance on seen tasks becomes lower with no COT, which is expected.</p>
<h2>6 CONCLUSIONS AND FUTURE WORK</h2>
<p>In this work, we propose a new paradigm called table-tuning, that can continue to fine-tune the model weights of pre-trained large language-models like GPT-3.5 and ChatGPT, such that the resulting models are better in understanding tables and performing table tasks, while still being versatile in following diverse human instructions for unseen tasks. Just like how instruction-tuning has turned into a rich and fruitful line of research in the NLP literature, we hope our initial steps in table-tuning can serve as a springboard for others to continue in this path to develop more optimized models for tables and table-related tasks.</p>
<h2>REFERENCES</h2>
<p>[1] [n.d.]. Magellan data repository. https://sites.google.com/site/anhaidgroup/ useful-stuff/the-magellan-data-repository?authuser=0.
[2] [n.d.]. OpenAI: ChatGPT. https://openai.com/blog/chatgpt.
[3] [n.d.]. Python typo generator. https://pypi.org/project/typo/.
[4] [n.d.]. Stanford Alpaca. https://github.com/tatsu-lab/stanford_alpaca.
[5] Rohan Arall, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).
[6] Simrun Arora, Avanika Narayan, Mayee P Chen, Laurel J Orr, Neel Guba, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher RÃ©. 2022. Ask me anything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441 (2022).
[7] Felix Biessmann, Tammo Rukat, Philipp Schmidt, Prathik Naidu, Sebastian Schelter, Audrey Taptunov, Dustin Lange, and David Salinas. 2019. DataWig: Missing Value Imputation for Tables. J. Mach. Learn. Res. 20, 175 (2019), 1-6.
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[9] Michael J Cafarella, Alon Y Halevy, Yang Zhang, Daisy Zhe Wang, and Eugene Wu. 2008. Uncovering the Relational Web.. In WebDB. Citeseer, 1-6.
[10] Kaushik Chakrabarti, Surajit Chaudhuri, Zhimin Chen, Kris Ganjam, Yeye He, and W Redmond. 2016. Data services leveraging Bing's data assets. IEEE Data Eng. Bull. 39, 3 (2016), 15-28.
[11] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2019. Tabfact: A large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164 (2019).
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02511 (2022).
[13] Xu Chu, Yeye He, Kaushik Chakrabarti, and Kris Ganjam. 2015. Tegra: Table extraction by global record alignment. In Proceedings of the 2015 ACM SIGMOD international conference on management of data. 1713-1728.
[14] Xu Chu, Ihab F Ilyas, Sanjay Krishnan, and Jiannan Wang. 2016. Data cleaning: Overview and emerging challenges. In Proceedings of the 2016 international conference on management of data. 2201-2206.
[15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).
[16] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2022. Turl: Table understanding through representation learning. ACM SIGMOD Record 51, 1 (2022), 33-40.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[18] Till DÃ¶hmen, Hannes MÃ¼hleisen, and Peter Boncz. 2017. Multi-hypothesis CSV parsing. In Proceedings of the 29th International Conference on Scientific and Statistical Database Management. 1-12.
[19] Hazem Elmeleegy, Jayant Madhavan, and Alon Halevy. 2009. Harvesting relational tables from lists on the web. Proceedings of the VLDB Endowment 2, 1 (2009), 1078-1089.
[20] Raul Castro Fernandes, Aaron J Elmore, Michael J Franklin, Sanjay Krishnan, and Chenhao Tan. 2023. How Large Language Models Will Disrupt Data Management. Proceedings of the VLDB Endowment 16, 11 (2023), 3302-3309.
[21] Tiaoyu Gao, Adam Fisch, and Dansj Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).
[22] Suchin Gururangan, Ana MarasoviÄ‡, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964 (2020).
[23] William R Harris and Sumit Gulwani. 2011. Spreadsheet table transformations from examples. ACM SIGPLAN Notices 46, 6 (2011), 317-328.
[24] Yeye He, Xu Chu, Kris Ganjam, Yudian Zheng, Vivek Narasayya, and Surajit Chaudhuri. 2018. Transform-data-by-example (TDE) an extensible search engine for data transformations. Proceedings of the VLDB Endowment 11, 10 (2018), $1165-1177$.
[25] Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind Satyanarayan, Tim Kraska, Ã‡iÄŸattay DemirdÇ«Ì†, and CÃ©sar Hidalgo. 2019. Sherlock: A deep learning approach to semantic data type detection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining. $1500-1508$.
[26] Zhongjun Jin, Yeye He, and Surajit Chaudhuri. 2020. Auto-transform: learning-to-transform by patterns. Proceedings of the VLDB Endowment 15, 12 (2020), $2368-2381$.
[27] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wrangler: Interactive visual specification of data transformation scripts. In Proceedings of the sigcbi conference on human factors in computing systems. 3363-3372.
[28] Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520 (2023).
[29] Keti Korini and Christian Bizer. 2023. Column Type Annotation using ChatGPT. arXiv preprint arXiv:2306.00745 (2023).
[30] Christos Koutras, George Stachanin, Andra Ionescu, Kyriakos Psarakis, Jerry Brons, Marios Fragkoulis, Christoph Lofi, Angela Bonifati, and Asterios Katsifodimos. 2021. Valentine: Evaluating matching techniques for dataset discovery. In 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEE, $468-479$.
[31] Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023. Self-Alignment with Instruction Backtranslation. arXiv preprint arXiv:2308.06259 (2023).
[32] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan. 2020. Deep entity matching with pre-trained language models. arXiv preprint arXiv:2004.00584 (2020).
[33] Yiming Lin, Yeye He, and Surajit Chaudhuri. 2023. Auto-BI: Automatically Build BI-Models Leveraging Local Join Prediction and Global Schema Graph. arXiv preprint arXiv:2306.12515 (2023).</p>
<p>[34] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2025. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023), $1-35$.
[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[36] Jayant Madhavan, Philip A Bernstein, and Erhard Rahm. 2001. Generic schema matching with cupid. In vldb. Vol. 1, 49-58.
[37] Chris Mayfield, Jennifer Neville, and Sunil Prabhakar. 2010. ERACER: a database approach for statistical inference and data cleaning. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. 75-86.
[38] Sulharth Madgal, Han Li, Theodoros Rekatsinas, AetKai Doan, Youngchoon Park, Ganesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra. 2018. Deep learning for entity matching: A design space exploration. In Proceedings of the 2018 International Conference on Management of Data. 19-34.
[39] Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, and Christopher RÃ©. 2022. Can foundation models wrangle your data? arXiv preprint arXiv:2205.09911 (2022).
[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730-27744.
[41] George Papadakis, Ekaterini Ioannou, Emanouil Thanos, and Themis Palpanas. 2021. The four generations of entity resolution. Springer.
[42] Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305 (2015).
[43] Ralph Peeters and Christian Bizer. 2023. Using ChatGPT for Entity Matching. arXiv preprint arXiv:2305.03423 (2023).
[44] Erhard Rahm and Philip A Bernstein. 2001. A survey of approaches to automatic schema matching. the VLDB Journal 10 (2001), 334-350.
[45] Erhard Rahm, Hong Hai Do, et al. 2000. Data cleaning: Problems and current approaches. IEEE Data Eng. Bull. 23, 4 (2000), 3-13.
[46] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics 8 (2021), 842-866.
[47] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafrai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021).
[48] Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, Ã‡aÄŸatay Demiralp, Chen Chen, and Wang-Chiew Tan. 2022. Annotating columns with pre-trained language models. In Proceedings of the 2022 International Conference on Management of Data. 1493-1503.
[49] Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. 2016. Table cell search for question answering. In Proceedings of the 25th International Conference on World Wide Web. 771-782.
[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[51] Jianhong Tu, Ju Fan, Nan Tang, Peng Wang, Guoliang Li, Xiaoyong Du, Xiaofeng Jia, and Song Gao. 2023. Unicorn: A unified multi-tasking model for supporting matching tasks in data integration. Proceedings of the ACM on Management of Data 1, 1 (2023), 1-26.
[52] Gerrit JJ van den Burg, Alfredo NazÃ¡bal, and Charles Sutton. 2019. Wrangling messy CSV files by detecting row and type patterns. Data Mining and Knowledge Discovery 33, 6 (2019), 1799-1820.
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[54] Gerardo Vitagliano, Mazhar Hameed, Lan Jiang, Lucas Reisener, Eugene Wu, and Felix Naumann. 2023. Pollock: A Data Loading Benchmark. Proceedings of the VLDB Endowment 16, 8 (2023), 1870-1882.
[55] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).
[56] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Is Beltagy, et al. 2023. How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. arXiv preprint arXiv:2306.04751 (2023).
[57] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alina Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022).
[58] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, AmiÑ‚reza Mirzaei, Anjana Arunkumar, Arjan Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Map, et al. 2022. Super-naturalinstructions: Generalization
via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705 (2022).
[59] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824-24837.
[61] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382 (2023).
[62] Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet: Generating structured queries from natural language without reinforcement learning. arXiv preprint arXiv:1711.04436 (2017).
[63] Cong Yan and Yeye He. 2018. Synthesizing type-detection logic for rich semantic data types using open-source code. In Proceedings of the 2018 International Conference on Management of Data. 35-50.
[64] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for joint understanding of textual and tabular data. arXiv preprint arXiv:2005.08514 (2020).
[65] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium.
[66] Chen Zhao and Yeye He. 2019. Auto-em: End-to-end fuzzy entity-matching using pre-trained deep models and transfer learning. In The World Wide Web Conference. 2413-2424.
[67] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 (2023).</p>
<h1>A TASK DETAILS</h1>
<h2>Missing Value Identification (Column No Sep) Zero-Shot</h2>
<h2>Prompt:</h2>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using its column name. Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Input:
|Project|Team|Req|
|---|---|---|
|A|5|3|
|I|3|2|
|U|2|3|
|2|1|
|I|2|2|
Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Output:</p>
<h2>Completion:</h2>
<p>{"missing_col": "Project"}</p>
<h2>Missing Value Identification (Column No Sep) Few-Shot</h2>
<h2>Prompt:</h2>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using its column name. Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Input:
|Project|Team|Req|
|---|---|---|
|A|4|1|
|I|2|1|
|0|3|3|
|A|1|
|E|4|2|
## Output:
{"missing_col": "Req"}
## Input:
|Project|Team|Req|
|---|---|---|
|I|2|1|
|E|1|3|
|A|1|3|
|1|2|
|E|4|2|
## Output:
{"missing_col": "Project"}
## Input:
|Project|Team|Req|
|---|---|---|</p>
<p>|E|4|2|
|O|2|
|U|5|2|
|I|4|2|
|A|4|2|
## Output:
{"missing_col": "Team"}
## Input:
|Project|Team|Req|
|---|---|---|
|A|5|3|
|I|3|2|
|U|2|3|
|2|1|
|I|2|2|
Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Output:
Completion:
{"missing_col": "Project"}</p>
<h1>Missing Value Identification (Row Sep) Zero-Shot</h1>
<h2>Prompt:</h2>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using the row id shown in the first column. Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|A|5|3|
|2|I|3|2|
|3|U|2|3|
|4||2|1|
|5|I|2|2|
Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Output:
Completion:
{"row_id": "4"}</p>
<h2>Missing Value Identification (Row Sep) Few-Shot</h2>
<h2>Prompt:</h2>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using the row id shown in the first column. Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|A|4|1|</p>
<p>|2|I|2|1|
|3|0|3|3|
|4|A|1||
|5|E|4|2|
## Output:
("row_id": "4")
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|I|2|1|
|2|E|1|3|
|3|A|1|3|
|4||1|2|
|5|E|4|2|
## Output:
("row_id": "4")
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|E|4|2|
|2|0||2|
|3|U|5|2|
|4|I|4|2|
|5|A|4|2|
## Output:
("row_id": "2")
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|A|5|3|
|2|I|3|2|
|3|U|2|3|
|4||2|1|
|5|I|2|2|</p>
<p _4_="&quot;4&quot;" _row_id_:="&quot;row_id&quot;:">Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}).
## Output:
Completion:</p>
<p>Missing Value Identification (Row No Sep) Zero-Shot</p>
<h1>Prompt:</h1>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using the row id shown in the first column. Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|</p>
<p>|1|A|5|3|
|2|I|3|2|
|3|U|2|3|
|4|2|1|
$15|I| 2 \mid 2 \mid$
Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Output:</p>
<p>Completion:
{"row_id": "4"}</p>
<p>Missing Value Identification (Row No Sep) Few-Shot</p>
<h1>Prompt:</h1>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using the row id shown in the first column. Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|A|4|1|
|2|I|2|1|
|3|0|3|3|
|4|A|1|
$15|E| 4 \mid 2|$
## Output:
{"row_id": "4"}
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|I|2|1|
|2|E|1|3|
|3|A|1|3|
|4|1|2|
$15|E| 4 \mid 2|$
## Output:
{"row_id": "4"}
## Input:
|row_id|Project|Team|Req|
|---|---|---|---|
|1|E|4|2|
|2|0|2|
|3|U|5|2|
|4|I|4|2|
$15|A| 4 \mid 2|$
## Output:
{"row_id": "2"}
## Input:
|row_id|Project|Team|Req|</p>
<p>|---|---|---|---|
|1|A|5|3|
|2|I|3|2|
|3|U|2|3|
|4|2|1|
|5|I|2|2|
Return the final result as JSON in the format {"row_id": "<row_id of the row with missing cell>"}.
## Output:
Completion:
{"row_id": "4"}</p>
<p>Missing Value Identification (Column Sep) Zero-Shot</p>
<h1>Prompt:</h1>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using its column name. Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Input:
|Project|Team|Req|
|---|---|---|
|A|5|3|
|I|3|2|
|U|2|3|
||2|1|
|I|2|2|
Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Output:
Completion:
{"missing_col": "Project"}</p>
<h2>Missing Value Identification (Column Sep) Few-Shot</h2>
<h2>Prompt:</h2>
<p># Task Description: Please check the following table, there is one and exactly one cell in the table that is missing. When you find this missing cell, please point it out using its column name. Return the final result as JSON in the format {"missing_col": "<missing column name>"}.
## Input:
|Project|Team|Req|
|---|---|---|
|A|4|1|
|I|2|1|
|0|3|3|
|A|1||
|E|4|2|
## Output:
{"missing_col": "Req"}
## Input:
|Project|Team|Req|
|---|---|---|</p>
<p>|I|2|1|
|E|1|3|
|A|1|3|
||1|2|
|E|4|2|
# Output:
("missing_col": "Project")
# Input:
|Project|Team|Req|
|---|---|---|
|E|4|2|
|O||2|
|U|5|2|
|I|4|2|
|A|4|2|
# Output:
("missing_col": "Team")
# Input:
|Project|Team|Req|
|---|---|---|
|A|5|3|
|I|3|2|
|U|2|3|
||2|1|
|I|2|2|
Return the final result as JSON in the format {"missing_col": "<missing column name>"}.# Output:</p>
<h1>Completion:</h1>
<p>("missing_col": "Project")</p>
<h2>Column Finding Zero-Shot</h2>
<h2>Prompt:</h2>
<p># Task Description: Please look at the table below and find the column that contains the given cell value. Return the final result as JSON in the format {"result": "<name of the column containing the given cell value>"}.
# Input:
<strong>Input Table:</strong>
|1|12|13|14|15|16|17|18|
|---|---|---|---|---|---|---|---|
|2|2|2|2|2|2|2|
|3|3|3|3|3|3|3|3|
|4|4|4|4|4|4|4|4|
|5|5|5|5|5|5|5|5|
|6|6|6|6|6|6|6|6|
|7|7|7|7|7|7|7|7|
|8|8|8|8|8|8|8|8|
|9|9|9|9|9|9|9|9|
|10|10|10|10|10|10|10|10|
|11|11|11|11|11|11|11|11|11|</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ : https://aka.ms/table-gpt&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7353 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7353</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7353</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-274436066</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.00821v1.pdf" target="_blank">Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding. When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors. While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously. To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs. Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents. We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA). MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7353.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7353.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Refinement Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic iterative refinement framework that uses an external verifier to identify problem-comprehension, conceptual, and computational errors in LLM-generated physics Chain-of-Thought solutions and routes specialized refinement agents (miscomprehension, concept, computation) to correct them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixture of Refinement Agents (MoRA) applied with evaluated LLMs (LLaMA-3-70B, Gemma-2-27B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>framework (uses models of varying sizes, e.g., 70B, 27B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>agentic refinement framework that orchestrates open-source and proprietary instruction-tuned LLMs plus retrieval and code-execution tools</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (high-school / intermediate physics problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of physics problem solving: generate step-by-step (CoT) solutions to multi-hop physics problems, detect and iteratively correct miscomprehension, incorrect concept/formula application, and computational errors to improve final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Initial Chain-of-Thought (CoT) or few-shot prompting to generate a base solution; GPT-4o used as an external verifier to assign flags/scores; prioritized instruction prompting for miscomprehension; retrieval-augmented prompting (GraphRAG) for concept refinement; code-generation + execution (Python) for computational refinement; iterative rerunning until flags/scores resolved.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final-answer accuracy (exact-match / multiple-choice accuracy); Score_concept (0–1) indicating earliest step of conceptual error; Score_comp (0–1) for computation correctness; agent 'refinement rate' (% of erroneous solutions corrected by an agent).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>MoRA improved PhysicsQA accuracy over CoT by +13.38% (LLaMA-3-70B) and +16.03% (Gemma-2-27B); example final accuracies: LLaMA-3-70B PhysicsQA 70.14% (MoRA) vs 56.76% (CoT) / 59.29% (3-shot); Gemma-2-27B PhysicsQA 70.62% (MoRA) vs 54.59% (CoT) / 59.45% (3-shot). (Also improvements reported across SciEval-Static and MMLU subsets.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines: Answer-Only (AO), Chain-of-Thought (CoT), Few-shot (3-shot). Example baselines: LLaMA-3-70B SciEval-Static CoT 82.23% (AO 70.07%, 3-shot 63.41%); Gemma-2-27B SciEval-Static CoT 79.26% (AO 60.36%, 3-shot 53.04%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['use of a stronger external verifier (GPT-4o) for error identification', 'prioritized iterative routing of specialized agents (miscomprehension → concept → computation)', 'external retrieval (GraphRAG) to a physics knowledge base for concept correction', 'code-generation + execution for computational correction (OpenAI Code Interpreter)', 'model capacity/type (open-source medium-parameter models vs GPT-4o)', 'dataset difficulty / domain specificity (PhysicsQA more conceptually and computationally demanding)', 'quality of initial CoT solution and correctness-location scores (Score_concept, Score_comp)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same prompts used across datasets and LLMs; GPT-4o used for error identification and score assignment; computation verification uses GPT-4o with OpenAI Code Interpreter and an error tolerance of 0.1; prioritized routing and iterative loop with a maximum iteration limit N and threshold epsilon (ϵ) to stop refinements; GraphRAG used to query physics knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Relies on a strong proprietary verifier (GPT-4o) for error detection; open-source LLMs sometimes fail to generate correct retrieval thoughts for concept refinement and produce variable refinement rates; occasional inability to fully correct miscomprehension in complex cases; dependency on external knowledge-base coverage and on correct code-generation for computation refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7353.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7353.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, instruction-tuned large language model of approximately 70 billion parameters used in the paper as a primary base LLM for generating Chain-of-Thought physics solutions and for applying MoRA refinement agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source, instruction-tuned LLM (used via API prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (high-school/intermediate physics problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate step-by-step solutions (CoT) for multi-hop physics problems and serve as the base model that refinement agents modify to produce corrected final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought prompting (CoT) and few-shot (3-shot) used for initial solution generation; when refined, same model receives targeted instruction prompts for miscomprehension, retrieval thoughts for concept refinement, or code-driven computational refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final-answer accuracy (exact-match), Score_concept and Score_comp (used for identifying error location), and agent refinement rates (%) per error type.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>SciEval-Static: CoT 82.23%, MoRA 86.58%; PhysicsQA: CoT 56.76%, 3-shot 59.29%, MoRA 70.14%; MMLU High School: CoT 72.88%, MoRA 78.81%; MMLU College: CoT 71.76%, MoRA 78.82%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Answer-Only (AO) SciEval-Static 70.07%; AO PhysicsQA 38.37%; AO MMLU-College 59.41%; baseline CoT and 3-shot values as above.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model capacity (70B) relative to proprietary models', 'quality of initial CoT reasoning', 'ability to follow instruction prompts for miscomprehension correction', 'effectiveness of code-driven computation refinement', 'availability of external knowledge via GraphRAG for concept fixes']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same prompt templates across datasets and LLMs; MoRA pipeline used GPT-4o for error identification; computation refinement used generated Python code executed (Code Interpreter) to correct arithmetic/algebraic errors; iterative loop with thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Moderate rates of conceptual error (~18.11% average on PhysicsQA across open-source models) and computational mistakes (~21.62% average on PhysicsQA), difficulty generating accurate retrieval thoughts for concept refinement (variable refinement rates), and incomplete correction of miscomprehension in some complex physics questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7353.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7353.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-27B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 27B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medium-parameter open-source LLM (~27B) evaluated as a base solver for physics reasoning tasks and improved via the MoRA refinement pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-27B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source / research LLM (instruction-tuned variant used via API)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (high-school/intermediate physics problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of solving physics multiple-choice and open problems using CoT; serves as base model for MoRA refinements (concept retrieval and code-driven computation fixes).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Chain-of-Thought prompting (CoT), few-shot prompting (3-shot) for initial solutions; targeted instruction prompts, GraphRAG retrieval for concept fixes, and code-generation for computation refinements under MoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final-answer accuracy (exact-match on multiple-choice), Score_concept and Score_comp, and refinement rates for each refinement agent.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>SciEval-Static: CoT 79.26%, MoRA 88.76%; PhysicsQA: CoT 54.59%, 3-shot 59.45%, MoRA 70.62%; MMLU College: CoT 73.52%, MoRA 82.20%; MMLU High School: CoT 77.11%, MoRA 75.88% (notable decrease).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>AO and CoT baselines: AO SciEval-Static 60.36%; AO PhysicsQA 39.18%; other baselines listed in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['model parameter count (27B) relative to larger models', 'effectiveness of retrieval-augmented concept correction', 'code-generation ability for computational refinement (variable across datasets)', 'dataset type and difficulty (PhysicsQA posed particular challenge)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same prompt templates and MoRA orchestration as other models; GraphRAG used for retrieval; code-execution used for computational checks; agent ablations measured refinement rates per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Variable and sometimes unstable refinement rates (e.g., low concept-refinement performance on some MMLU splits); struggles with retrieval-thought generation for concept errors in some datasets; inconsistent gains in MMLU High School where MoRA decreased accuracy slightly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7353.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7353.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as external error identifier/verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary advanced GPT-4-class model used in this paper specifically to identify and localize problem miscomprehension, concept/formula errors, and computational mistakes in LLM-generated physics solutions and to produce Score_concept and Score_comp.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used as error-identification/verifier model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary / not specified</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary instruction-tuned LLM used as an external verifier and code-execution interface (with Code Interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (used to verify physics problem solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based verification / simulation of the reasoning chain: identify objective alignment and variable-application flags, compute Score_concept (0–1) for conceptual error location, compute Score_comp (0–1) for computation correctness, and generate Python code via Code Interpreter for computation verification.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Prompted to check solutions and set flags (Objective Alignment Flag F_obj, Variables Application Flag F_val), assign Score_concept and Score_comp; used with OpenAI Code Interpreter to generate and execute Python for computation checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to compute Score_concept and Score_comp; paper reports verifier performance (accuracy) describing GPT-4o's high performance in these identification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Paper reports GPT-4o achieves near-perfect accuracy on SciEval-Static, MMLU College and High School for problem comprehension and concept application; average concept-identification accuracy ≈ 96.4% and average computational-verification accuracy ≈ 95.64% across the four datasets (reported in analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Not used as a baseline; treated as a stronger oracle/verifier relative to open-source models, which perform worse on concept and computation verification.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['superior problem comprehension and concept-application performance compared to open-source LLMs', 'ability to generate and execute code via the Code Interpreter for robust computational verification', "used as a guide/oracle for routing refinements (thus its accuracy strongly influences MoRA's effectiveness)"]</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>GPT-4o used solely for error identification and scoring; uses OpenAI Code Interpreter for Python generation and execution with an error tolerance of 0.1; verifier outputs flags and scalar scores used to route refinement agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Dependency on a proprietary strong verifier which may not be available in all deployment contexts; potential propagation of verifier errors (rare, per paper) would misroute refinements; paper does not provide internal ablation replacing GPT-4o with weaker verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7353.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7353.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Code Interpreter (used for computation verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-generation and execution tool (Python) used in conjunction with GPT-4o to evaluate and correct arithmetic and algebraic computations in LLM-generated solutions, producing concrete numeric corrections that the LLM integrates back into the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Code Interpreter (invoked by GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>tool / service (not applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>code-execution tool integrated with GPT-4o for programmatic verification</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (numerical / algebraic computation verification)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Execute Python code generated from the LLM's identified faulty computation steps to produce numerically verified intermediate and final results for physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>LLM (the same model as the solution generator) is instructed, based on Score_comp, to generate Python code Cp that implements the computation for the identified failing step; Code Interpreter executes Cp to obtain R_c which is fed back to refine the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used to compute Score_comp (0–1) for computation correctness; refinement rate (%) of computational errors corrected after code-driven refinement is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Not an accuracy for the tool itself, but paper reports substantial computational-refinement rates when using code execution: LLaMA-3-70B computation refinement rates e.g., PhysicsQA 72.6%, SciEval-Static 60%, MMLU College 81.8%; Gemma-2-27B computational refinement rates e.g., PhysicsQA 73.3%, MMLU College 100% (per ablation table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baseline is LLM-only computation without code execution (higher computational error rates); exact baseline computational-error rates for LLM-only are reported elsewhere in the paper (e.g., open-source models show ~21.62% computational-error rate on PhysicsQA).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>["code-generation correctness and the model's ability to produce correct Python", 'error tolerance used in numerical checks (paper used 0.1)', 'propagation of corrected numeric results back into the textual solution']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Python code generation by LLMs guided by Score_comp and executed by OpenAI Code Interpreter; error tolerance 0.1; iterative integration into solution; code-driven refinement inspired by PAL/PoT/MathCoder paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Effectiveness depends on the LLM's ability to generate correct code and the coverage of arithmetic/algebraic cases; some datasets and models exhibit variable improvement (Gemma-2-27B shows variability across splits), and code-based fixes cannot resolve upstream conceptual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7353.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7353.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRAG (graph-based retrieval-augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval augmentation technique that queries a physics knowledge base using a generated 'retrieval thought' to retrieve correct conceptual context and formulae which the LLM then uses to refine concept-level errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From local to global: A graph rag approach to query-focused summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphRAG (used as retrieval component in MoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>method / system (not a single LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>retrieval-augmented generation method leveraging a knowledge-graph style retrieval backend</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics (retrieval of domain concepts/formulae for corrections)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Textual retrieval of the correct conceptual context and formulae for the step at which a concept error is identified, enabling the LLM to re-apply correct physics concepts in the refined solution.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>LLM generates a succinct sequential 'retrieval thought' T_R that queries the physics knowledge base K_P via GraphRAG to return an observation O_T; O_T is then used to guide concept-level refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Effectiveness measured indirectly via concept-refinement 'refinement rate' (%) and final-answer accuracy improvements after concept refinement; Score_concept used to locate where retrieval is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>No direct retrieval accuracy metric provided; effectiveness observed via improved concept refinement rates (example: concept-refinement rates for LLaMA-3-70B on PhysicsQA 46.9%, SciEval-Static 57.1%; Gemma-2-27B PhysicsQA 48.7%, SciEval-Static 62.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baseline is concept refinement without retrieval augmentation (not explicitly quantified), and open-source LLMs often failed to retrieve correct formulae without external K_P.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['quality of LLM-generated retrieval thoughts', 'coverage and correctness of the physics knowledge base K_P', "integration between retrieved context O_T and LLM's local reasoning"]</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>GraphRAG queries performed using retrieval thoughts generated by the same LLM that produced the solution; retrieved observation O_T is used to re-run/refine reasoning from the failure stage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Open-source LLMs sometimes fail to generate effective retrieval thoughts leading to modest concept-refinement rates; success depends on knowledge-base coverage and the precision of the retrieval query.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving challenging math word problems using gpt-4 code interpreter with codebased self-verification <em>(Rating: 2)</em></li>
                <li>MathCoder: Seamless code integration in llms for enhanced mathematical reasoning <em>(Rating: 2)</em></li>
                <li>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research <em>(Rating: 2)</em></li>
                <li>From local to global: A graph rag approach to query-focused summarization <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7353",
    "paper_id": "paper-274436066",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "MoRA",
            "name_full": "Mixture of Refinement Agents",
            "brief_description": "An agentic iterative refinement framework that uses an external verifier to identify problem-comprehension, conceptual, and computational errors in LLM-generated physics Chain-of-Thought solutions and routes specialized refinement agents (miscomprehension, concept, computation) to correct them.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mixture of Refinement Agents (MoRA) applied with evaluated LLMs (LLaMA-3-70B, Gemma-2-27B, etc.)",
            "model_size": "framework (uses models of varying sizes, e.g., 70B, 27B)",
            "model_type": "agentic refinement framework that orchestrates open-source and proprietary instruction-tuned LLMs plus retrieval and code-execution tools",
            "scientific_domain": "physics (high-school / intermediate physics problem solving)",
            "simulation_task_description": "Text-based simulation of physics problem solving: generate step-by-step (CoT) solutions to multi-hop physics problems, detect and iteratively correct miscomprehension, incorrect concept/formula application, and computational errors to improve final-answer accuracy.",
            "prompting_strategy": "Initial Chain-of-Thought (CoT) or few-shot prompting to generate a base solution; GPT-4o used as an external verifier to assign flags/scores; prioritized instruction prompting for miscomprehension; retrieval-augmented prompting (GraphRAG) for concept refinement; code-generation + execution (Python) for computational refinement; iterative rerunning until flags/scores resolved.",
            "evaluation_metric": "Final-answer accuracy (exact-match / multiple-choice accuracy); Score_concept (0–1) indicating earliest step of conceptual error; Score_comp (0–1) for computation correctness; agent 'refinement rate' (% of erroneous solutions corrected by an agent).",
            "reported_accuracy": "MoRA improved PhysicsQA accuracy over CoT by +13.38% (LLaMA-3-70B) and +16.03% (Gemma-2-27B); example final accuracies: LLaMA-3-70B PhysicsQA 70.14% (MoRA) vs 56.76% (CoT) / 59.29% (3-shot); Gemma-2-27B PhysicsQA 70.62% (MoRA) vs 54.59% (CoT) / 59.45% (3-shot). (Also improvements reported across SciEval-Static and MMLU subsets.)",
            "baseline_accuracy": "Baselines: Answer-Only (AO), Chain-of-Thought (CoT), Few-shot (3-shot). Example baselines: LLaMA-3-70B SciEval-Static CoT 82.23% (AO 70.07%, 3-shot 63.41%); Gemma-2-27B SciEval-Static CoT 79.26% (AO 60.36%, 3-shot 53.04%).",
            "factors_reported": [
                "use of a stronger external verifier (GPT-4o) for error identification",
                "prioritized iterative routing of specialized agents (miscomprehension → concept → computation)",
                "external retrieval (GraphRAG) to a physics knowledge base for concept correction",
                "code-generation + execution for computational correction (OpenAI Code Interpreter)",
                "model capacity/type (open-source medium-parameter models vs GPT-4o)",
                "dataset difficulty / domain specificity (PhysicsQA more conceptually and computationally demanding)",
                "quality of initial CoT solution and correctness-location scores (Score_concept, Score_comp)"
            ],
            "experimental_conditions": "Same prompts used across datasets and LLMs; GPT-4o used for error identification and score assignment; computation verification uses GPT-4o with OpenAI Code Interpreter and an error tolerance of 0.1; prioritized routing and iterative loop with a maximum iteration limit N and threshold epsilon (ϵ) to stop refinements; GraphRAG used to query physics knowledge base.",
            "limitations_or_failure_modes": "Relies on a strong proprietary verifier (GPT-4o) for error detection; open-source LLMs sometimes fail to generate correct retrieval thoughts for concept refinement and produce variable refinement rates; occasional inability to fully correct miscomprehension in complex cases; dependency on external knowledge-base coverage and on correct code-generation for computation refinement.",
            "uuid": "e7353.0",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLaMA-3-70B",
            "name_full": "LLaMA-3 70B",
            "brief_description": "An open-source, instruction-tuned large language model of approximately 70 billion parameters used in the paper as a primary base LLM for generating Chain-of-Thought physics solutions and for applying MoRA refinement agents.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-70B",
            "model_size": "70B",
            "model_type": "open-source, instruction-tuned LLM (used via API prompts)",
            "scientific_domain": "physics (high-school/intermediate physics problem solving)",
            "simulation_task_description": "Generate step-by-step solutions (CoT) for multi-hop physics problems and serve as the base model that refinement agents modify to produce corrected final answers.",
            "prompting_strategy": "Chain-of-Thought prompting (CoT) and few-shot (3-shot) used for initial solution generation; when refined, same model receives targeted instruction prompts for miscomprehension, retrieval thoughts for concept refinement, or code-driven computational refinements.",
            "evaluation_metric": "Final-answer accuracy (exact-match), Score_concept and Score_comp (used for identifying error location), and agent refinement rates (%) per error type.",
            "reported_accuracy": "SciEval-Static: CoT 82.23%, MoRA 86.58%; PhysicsQA: CoT 56.76%, 3-shot 59.29%, MoRA 70.14%; MMLU High School: CoT 72.88%, MoRA 78.81%; MMLU College: CoT 71.76%, MoRA 78.82%.",
            "baseline_accuracy": "Answer-Only (AO) SciEval-Static 70.07%; AO PhysicsQA 38.37%; AO MMLU-College 59.41%; baseline CoT and 3-shot values as above.",
            "factors_reported": [
                "model capacity (70B) relative to proprietary models",
                "quality of initial CoT reasoning",
                "ability to follow instruction prompts for miscomprehension correction",
                "effectiveness of code-driven computation refinement",
                "availability of external knowledge via GraphRAG for concept fixes"
            ],
            "experimental_conditions": "Same prompt templates across datasets and LLMs; MoRA pipeline used GPT-4o for error identification; computation refinement used generated Python code executed (Code Interpreter) to correct arithmetic/algebraic errors; iterative loop with thresholds.",
            "limitations_or_failure_modes": "Moderate rates of conceptual error (~18.11% average on PhysicsQA across open-source models) and computational mistakes (~21.62% average on PhysicsQA), difficulty generating accurate retrieval thoughts for concept refinement (variable refinement rates), and incomplete correction of miscomprehension in some complex physics questions.",
            "uuid": "e7353.1",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gemma-2-27B",
            "name_full": "Gemma-2 27B",
            "brief_description": "A medium-parameter open-source LLM (~27B) evaluated as a base solver for physics reasoning tasks and improved via the MoRA refinement pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2-27B",
            "model_size": "27B",
            "model_type": "open-source / research LLM (instruction-tuned variant used via API)",
            "scientific_domain": "physics (high-school/intermediate physics problem solving)",
            "simulation_task_description": "Text-based simulation of solving physics multiple-choice and open problems using CoT; serves as base model for MoRA refinements (concept retrieval and code-driven computation fixes).",
            "prompting_strategy": "Chain-of-Thought prompting (CoT), few-shot prompting (3-shot) for initial solutions; targeted instruction prompts, GraphRAG retrieval for concept fixes, and code-generation for computation refinements under MoRA.",
            "evaluation_metric": "Final-answer accuracy (exact-match on multiple-choice), Score_concept and Score_comp, and refinement rates for each refinement agent.",
            "reported_accuracy": "SciEval-Static: CoT 79.26%, MoRA 88.76%; PhysicsQA: CoT 54.59%, 3-shot 59.45%, MoRA 70.62%; MMLU College: CoT 73.52%, MoRA 82.20%; MMLU High School: CoT 77.11%, MoRA 75.88% (notable decrease).",
            "baseline_accuracy": "AO and CoT baselines: AO SciEval-Static 60.36%; AO PhysicsQA 39.18%; other baselines listed in paper tables.",
            "factors_reported": [
                "model parameter count (27B) relative to larger models",
                "effectiveness of retrieval-augmented concept correction",
                "code-generation ability for computational refinement (variable across datasets)",
                "dataset type and difficulty (PhysicsQA posed particular challenge)"
            ],
            "experimental_conditions": "Same prompt templates and MoRA orchestration as other models; GraphRAG used for retrieval; code-execution used for computational checks; agent ablations measured refinement rates per dataset.",
            "limitations_or_failure_modes": "Variable and sometimes unstable refinement rates (e.g., low concept-refinement performance on some MMLU splits); struggles with retrieval-thought generation for concept errors in some datasets; inconsistent gains in MMLU High School where MoRA decreased accuracy slightly.",
            "uuid": "e7353.2",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o (verifier)",
            "name_full": "GPT-4o (used as external error identifier/verifier)",
            "brief_description": "A proprietary advanced GPT-4-class model used in this paper specifically to identify and localize problem miscomprehension, concept/formula errors, and computational mistakes in LLM-generated physics solutions and to produce Score_concept and Score_comp.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used as error-identification/verifier model)",
            "model_size": "proprietary / not specified",
            "model_type": "proprietary instruction-tuned LLM used as an external verifier and code-execution interface (with Code Interpreter)",
            "scientific_domain": "physics (used to verify physics problem solutions)",
            "simulation_task_description": "Text-based verification / simulation of the reasoning chain: identify objective alignment and variable-application flags, compute Score_concept (0–1) for conceptual error location, compute Score_comp (0–1) for computation correctness, and generate Python code via Code Interpreter for computation verification.",
            "prompting_strategy": "Prompted to check solutions and set flags (Objective Alignment Flag F_obj, Variables Application Flag F_val), assign Score_concept and Score_comp; used with OpenAI Code Interpreter to generate and execute Python for computation checks.",
            "evaluation_metric": "Used to compute Score_concept and Score_comp; paper reports verifier performance (accuracy) describing GPT-4o's high performance in these identification tasks.",
            "reported_accuracy": "Paper reports GPT-4o achieves near-perfect accuracy on SciEval-Static, MMLU College and High School for problem comprehension and concept application; average concept-identification accuracy ≈ 96.4% and average computational-verification accuracy ≈ 95.64% across the four datasets (reported in analysis).",
            "baseline_accuracy": "Not used as a baseline; treated as a stronger oracle/verifier relative to open-source models, which perform worse on concept and computation verification.",
            "factors_reported": [
                "superior problem comprehension and concept-application performance compared to open-source LLMs",
                "ability to generate and execute code via the Code Interpreter for robust computational verification",
                "used as a guide/oracle for routing refinements (thus its accuracy strongly influences MoRA's effectiveness)"
            ],
            "experimental_conditions": "GPT-4o used solely for error identification and scoring; uses OpenAI Code Interpreter for Python generation and execution with an error tolerance of 0.1; verifier outputs flags and scalar scores used to route refinement agents.",
            "limitations_or_failure_modes": "Dependency on a proprietary strong verifier which may not be available in all deployment contexts; potential propagation of verifier errors (rare, per paper) would misroute refinements; paper does not provide internal ablation replacing GPT-4o with weaker verifiers.",
            "uuid": "e7353.3",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OpenAI Code Interpreter",
            "name_full": "OpenAI Code Interpreter (used for computation verification)",
            "brief_description": "A code-generation and execution tool (Python) used in conjunction with GPT-4o to evaluate and correct arithmetic and algebraic computations in LLM-generated solutions, producing concrete numeric corrections that the LLM integrates back into the solution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI Code Interpreter (invoked by GPT-4o)",
            "model_size": "tool / service (not applicable)",
            "model_type": "code-execution tool integrated with GPT-4o for programmatic verification",
            "scientific_domain": "physics (numerical / algebraic computation verification)",
            "simulation_task_description": "Execute Python code generated from the LLM's identified faulty computation steps to produce numerically verified intermediate and final results for physics problems.",
            "prompting_strategy": "LLM (the same model as the solution generator) is instructed, based on Score_comp, to generate Python code Cp that implements the computation for the identified failing step; Code Interpreter executes Cp to obtain R_c which is fed back to refine the solution.",
            "evaluation_metric": "Used to compute Score_comp (0–1) for computation correctness; refinement rate (%) of computational errors corrected after code-driven refinement is reported.",
            "reported_accuracy": "Not an accuracy for the tool itself, but paper reports substantial computational-refinement rates when using code execution: LLaMA-3-70B computation refinement rates e.g., PhysicsQA 72.6%, SciEval-Static 60%, MMLU College 81.8%; Gemma-2-27B computational refinement rates e.g., PhysicsQA 73.3%, MMLU College 100% (per ablation table).",
            "baseline_accuracy": "Baseline is LLM-only computation without code execution (higher computational error rates); exact baseline computational-error rates for LLM-only are reported elsewhere in the paper (e.g., open-source models show ~21.62% computational-error rate on PhysicsQA).",
            "factors_reported": [
                "code-generation correctness and the model's ability to produce correct Python",
                "error tolerance used in numerical checks (paper used 0.1)",
                "propagation of corrected numeric results back into the textual solution"
            ],
            "experimental_conditions": "Python code generation by LLMs guided by Score_comp and executed by OpenAI Code Interpreter; error tolerance 0.1; iterative integration into solution; code-driven refinement inspired by PAL/PoT/MathCoder paradigms.",
            "limitations_or_failure_modes": "Effectiveness depends on the LLM's ability to generate correct code and the coverage of arithmetic/algebraic cases; some datasets and models exhibit variable improvement (Gemma-2-27B shows variability across splits), and code-based fixes cannot resolve upstream conceptual errors.",
            "uuid": "e7353.4",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphRAG",
            "name_full": "GraphRAG (graph-based retrieval-augmented generation)",
            "brief_description": "A retrieval augmentation technique that queries a physics knowledge base using a generated 'retrieval thought' to retrieve correct conceptual context and formulae which the LLM then uses to refine concept-level errors.",
            "citation_title": "From local to global: A graph rag approach to query-focused summarization",
            "mention_or_use": "use",
            "model_name": "GraphRAG (used as retrieval component in MoRA)",
            "model_size": "method / system (not a single LLM)",
            "model_type": "retrieval-augmented generation method leveraging a knowledge-graph style retrieval backend",
            "scientific_domain": "physics (retrieval of domain concepts/formulae for corrections)",
            "simulation_task_description": "Textual retrieval of the correct conceptual context and formulae for the step at which a concept error is identified, enabling the LLM to re-apply correct physics concepts in the refined solution.",
            "prompting_strategy": "LLM generates a succinct sequential 'retrieval thought' T_R that queries the physics knowledge base K_P via GraphRAG to return an observation O_T; O_T is then used to guide concept-level refinement.",
            "evaluation_metric": "Effectiveness measured indirectly via concept-refinement 'refinement rate' (%) and final-answer accuracy improvements after concept refinement; Score_concept used to locate where retrieval is needed.",
            "reported_accuracy": "No direct retrieval accuracy metric provided; effectiveness observed via improved concept refinement rates (example: concept-refinement rates for LLaMA-3-70B on PhysicsQA 46.9%, SciEval-Static 57.1%; Gemma-2-27B PhysicsQA 48.7%, SciEval-Static 62.5%).",
            "baseline_accuracy": "Baseline is concept refinement without retrieval augmentation (not explicitly quantified), and open-source LLMs often failed to retrieve correct formulae without external K_P.",
            "factors_reported": [
                "quality of LLM-generated retrieval thoughts",
                "coverage and correctness of the physics knowledge base K_P",
                "integration between retrieved context O_T and LLM's local reasoning"
            ],
            "experimental_conditions": "GraphRAG queries performed using retrieval thoughts generated by the same LLM that produced the solution; retrieved observation O_T is used to re-run/refine reasoning from the failure stage.",
            "limitations_or_failure_modes": "Open-source LLMs sometimes fail to generate effective retrieval thoughts leading to modest concept-refinement rates; success depends on knowledge-base coverage and the precision of the retrieval query.",
            "uuid": "e7353.5",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving challenging math word problems using gpt-4 code interpreter with codebased self-verification",
            "rating": 2,
            "sanitized_title": "solving_challenging_math_word_problems_using_gpt4_code_interpreter_with_codebased_selfverification"
        },
        {
            "paper_title": "MathCoder: Seamless code integration in llms for enhanced mathematical reasoning",
            "rating": 2,
            "sanitized_title": "mathcoder_seamless_code_integration_in_llms_for_enhanced_mathematical_reasoning"
        },
        {
            "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
            "rating": 2,
            "sanitized_title": "scieval_a_multilevel_large_language_model_evaluation_benchmark_for_scientific_research"
        },
        {
            "paper_title": "From local to global: A graph rag approach to query-focused summarization",
            "rating": 2,
            "sanitized_title": "from_local_to_global_a_graph_rag_approach_to_queryfocused_summarization"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 1,
            "sanitized_title": "pal_programaided_language_models"
        }
    ],
    "cost": 0.015702749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</p>
<p>Raj Jaiswal 
HarshDhruv Jain dhruv.jain.ece21@itbhu.ac.in 
Parimal Popat 
Avinash Anand 
Abhishek Dharmadhikari abhishekdharmadhikari25@gmail.com 
Atharva Marathe atharvamarathe8@gmail.com 
Ratn Rajiv rajivratn@iiitd.ac.in 
Shah 
Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents
BCF50812D5A08AE799E9F64C19BF68DF
Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks.However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding.When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors.While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously.To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs.Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents.We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA).MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.</p>
<p>Introduction</p>
<p>Scientific reasoning, particularly in field of physics, requires a deep understanding that spans multiple disciplines.It demands not only domain-specific knowledge but also the integration of mathematical reasoning with theoretical concepts, applying abstract principles and formulae across various contexts and scenario.Successfully solving these challenges is a fundamental aspect of human intelligence, as it entails not just recalling information but adapting knowledge to solve diverse complex problems.</p>
<p>Solving complex physics problems still remains a challenge for open source LLMs.The difficulty stems from the need to integrate both mathematical and domain-specific knowledge while engaging in multi-hop, step-by-step reasoning.One approach to address this challenge can be collecting question and solution trajectory annotations and finetune LLMs to enhance these capabilities, similar to recent mathematical reasoning works (Luo et al. 2023;Yuan et al. 2024).However, the process of such annotations and finetuning is time-consuming and costly.On the other hand, so-lutions generated by LLMs for physics problems using CoT prompting (Wei et al. 2022) often contain errors, such as objective misalignment, incorrect formula application, and computational mistakes, as illustrated in Figure 1.Moreover, solutions to multihop physics problems contain multiple such errors together.</p>
<p>Open source LLMs struggles to accurately directly identify reasoning mistakes in their own solutions (Li et al. 2024;Tyen et al. 2024;Anand et al. 2023e), making them unreliable for error detection and self-refinement.While objective alignment errors can be corrected once identified, refining computational and conceptual errors requires strong mathematical reasoning and contextual understanding of the specific question.Addressing all these different errors simultaneously remains a significant challenge for open-source LLMs.</p>
<p>This motivated us to develop the Mixture of Refinement Agents (MoRA) framework.MoRA iteratively refines LLM responses through a two-step process in each iteration.First, the framework leverages a advanced model to identify various errors within the solution using appropriate flags and scores.In the next step, based on the identified errors, prioritized agent routing is conducted, in which the appropriate agents are activated to address and mitigate the specific errors.This process results in a progressively refined solution.</p>
<p>In the domain of physics, evaluation benchmarks are essential for assessing the conceptual and mathematical reasoning of LLMs.Benchmarks like MMLU, SciEval (Sun et al. 2024), and ScienceQA (Lu et al. 2022) focus on foundational knowledge and general reasoning, while more challenging ones like OlympiadBench (He et al. 2024) and JEEBench (Arora, Singh et al. 2023) require advanced reasoning skills.To bridge the gap, we curated our own dataset PhysicsQA, containing set of diverse, intermediatelevel high school physics problems that provide a balanced challenge, allowing a exhaustive evaluation and analysis of open-source LLMs on physics problems.</p>
<p>We perform exhaustive experimentation of MoRA across four datasets including PhysicsQA as shown in Table 3. MoRA improves accuracy on the PhysicsQA benchmark over CoT-generated solutions by 13.38% for Llama-3-70B and by 16.03% for Gemma-2-27B.This significant enhancement highlights MoRA's effectiveness in refining solutions, particularly in complex and diverse physics problems as in arXiv:2412.00821v1[cs.AI] 1 Dec 2024 PhysicsQA.Our further analysis offers insights into the error distribution across different models and evaluates the effectiveness of individual refinement agents based on their refinement rates.</p>
<p>Related Works</p>
<p>LLM Reasoning LLMs have been successfully applied to address multi-step reasoning tasks by generating intermediate reasoning steps, referred to as Chain-of-Thought (CoT) (Wei et al. 2022), Auto-CoT (Zhang et al. 2022), and Complex-CoT (Fu et al. 2022), among others.Advanced techniques like Iter-CoT (Sun et al. 2023a) and ToT (Yao et al. 2024) extend these capabilities but remain constrained by the knowledge in training data and the specific structures they were designed with.While In-Context Learning (ICL) (Brown et al. 2020) has significantly improved LLM performance, challenges like hallucinations and limitations in reasoning flexibility persist.</p>
<p>LLMs for Scientific Reasoning LLMs face significant limitations in complex knowledge reasoning tasks (Petroni et al. 2020).(Ouyang et al. 2023) introduced a structured reasoning strategy to guide LLMs in solving complex chemistry problems, enabling them to generate high-quality reasoning.Solving these problems requires not only domain knowledge, like formulae and calculations, but also a stepby-step reasoning process.(Ma et al. 2024) proposed a method where agents generate a high-level plan based on the question, retrieve relevant functions from a toolset, and execute low-level actions by integrating natural language and Python code.</p>
<p>Self Verification with LLMs Recent works (Cobbe et al. 2021;Ling et al. 2024) have attempted to address the challenge of error detection in step-by-step reasoning.However, these methods often require additional training data or domain-specific exemplars, making them less practical.(Miao, Teh, and Rainforth 2023) proposes using the LLM itself to verify the conditional correctness of each step in the reasoning chain, similar to how a human reviews their work.(Anand et al. 2023c) Accurate error recognition and correction are crucial for enhancing problem-solving capabilities, as demonstrated by (Li et al. 2024), which defines tasks to assess LLMs' mathematical reasoning abilities in error identification and correction.</p>
<p>LLMs for Mathematical Reasoning LLMs tends to struggle with arithmetic calculations when solving math problems (Cobbe et al. 2021;Gao et al. 2023).However, incorporating code generation and execution has shown promise in enhancing the accuracy of mathematical reasoning.Leveraging these strengths, the GPT-4 Code Interpreter (Zhou et al. 2023) has been integral to frameworks like MathCoder (Wang et al. 2023), which is designed to improve the mathematical reasoning capabilities of opensource models.Findings from (Zhou et al. 2023) indicate that GPT-4 Code's impressive proficiency in solving mathematical problems is largely due to its step-by-step code generation and the dynamic refinement of solutions based on code execution outcomes.</p>
<p>LLM Reasoning with external database (Lewis et al. 2020) proposed RAG framework, which incorporates (Anand et al. 2023b) a retrieval component to fetch relevant information from a given knowledge base.Integrating LLMs with knowledge representation tools, such as knowledge graphs (KGs) (Mruthyunjaya et al. 2023), has further enhanced reasoning capabilities.(Yao et al. 2024) demonstrated that augmenting LLMs with comprehensive external knowledge from KGs can significantly improve their performance and facilitate more robust reasoning processes.A notable example is GraphRAG (Edge et al. 2024), a retrieval enhancement technique that leverages knowledge graphs to map relationships between entities, thereby enhancing the retrieval process using large language models (LLMs).</p>
<p>Dataset: PhysicsQA</p>
<p>Our dataset comprises 370 carefully selected high school physics questions sourced from online resources.These questions are notably complex, often requiring the application of multiple concepts, intricate computations, (Anand et al. 2023a) and multihop reasoning.Each question is paired with a comprehensive, step-by-step solution, to support the evaluation (Anand et al. 2024b) and fine-tuning of LLMs for physics reasoning.Table 1 illustrates the topic-wise distribution of the questions, providing a clear overview of the areas covered.PhysicsQA offers a more robust evaluation and analysis of LLM performance by encompassing a diverse range of questions, both in terms of complexity and the topics covered.</p>
<p>Chapter</p>
<p>Mixture of Refinement Agents</p>
<p>This section introduces our mixture of refinement agents (MoRA) framework.We first discuss our motivation behind MoRA; then, we introduce the error identification stage and refinement agents.Finally, we discuss how these agents are routed iteratively to correct different errors in the solutions generated by the LLM.</p>
<p>Motivation</p>
<p>While analyzing physics problems and their CoT solutions generated with LLMs (Llama-3-70B &amp; Gemma-2-27B), we observed three key errors made by them: Observation 1: LLMs in few cases struggle to fully grasp the objective of the question, along with misinterpreting the values of variables and constants provided in the question.</p>
<p>Although this issue has been identified in only a few cases, it is significant one because it leads to solutions that Observation 2: LLMs struggle to apply the correct concepts or formulae with respect to the context of the given problem.</p>
<p>This issue is a more recurring one in LLMs, especially for problems requiring considering a specific case rather than relying on a generic formula.(Anand et al. 2024c) For instance, the formula for calculating the moment of inertia varies depending on the distribution of mass.</p>
<p>Observation 3: Many physics problems involve mathematical reasoning and algebraic computation, areas where LLMs tend to struggle.</p>
<p>Computational errors account for the majority of errors in solutions generated by LLMs.LLMs struggles with accurate algebraic and arithmetic computations resulting in errors within the reasoning and final answer.</p>
<p>While these three issues can be addressed individually, solutions often exhibit multiple errors together.Therefore, a single framework is required to rectify all three issues effectively, which motivated us to develop the MoRA.We first perform error identification on a given solution; then these errors are mitigated iteratively using specialized refinement agents, resulting in accurate solutions.</p>
<p>Error Identification</p>
<p>The errors in the solutions are classified into three categories: 1) problem miscomprehension, 2) incorrect concept application, and 3) computational errors as showed in Figure 1.</p>
<p>For error identification, we choose to rely on GPT-4o.Our experiments and analysis shows that GPT-4o showcases superior performance compared to other models, particularly in problem comprehension and correct physics concept application required.Thus, it is adequate for locating errors within solutions generated by other models.Given a question and it's LLM response, we prompt GPT-4o to identify and locate different errors in the solution using the combination of following flags and scores:</p>
<p>Problem Comprehension Flags: We prompt GPT-4o to check for the problem miscomprehension using the following two flags: (i) Objective Alignment Flag, F obj , verifies whether the solution is focused on solving the correct objective of the given question.(Anand et al. 2023d) (ii) Variables Application Flag, F val , verifies whether the solution uses the correct values for all variables and constants provided in the question, ensuring their correct values are applied in formulae and reasoning.</p>
<p>Concept Verification Score: We instruct GPT-4o to check the given solution against the relevant concept and formulae required to solve the given problem, based on its own understanding of the question.A score (Score concept ) is assigned to each solution to quantify the correctness of the applied physics concepts and formulae.The score is designed to identify the stage at which any conceptual or formulae error first occurs, if at all.Score concept ranges from 0 to 1, where a lower score indicates an earlier-stage error and a higher score indicates a later-stage error in the solution process.The score is calculated as follows:
Score concept =    n N if 1 ≤ n &lt; N (error at step n) n N +1 if n = N (error at last step) 1 if no errors occur
where n is the step at which the first error occurs, and N is the total number of steps in the solution process.</p>
<p>Computation Verification Score: We employ GPT-4o with OpenAI Code Interpreter for generation and execution of python code to evaluate the correctness of all arithmetic and algebraic operations in the given solution.Similar to the Score concept , we assign Score comp to each solution.This score quantifies the accuracy of the mathematical computation performed, ranges from 0 to 1, and is calculated similar to Score concept .All the computations are evaluated with an Figure 2: The illustration of thought and concept retrieval for conceptual error refinement in LLM response.Given the response and concept verification score, LLM generates a retrieval thought, which acts as a query to retrieve the correct conceptual context from an physics knowledge base using GraphRAG.error tolerance of 0.1.Using Code Interpreter enables us to leverage the code generation capabilities of GPT-4o rather than solely relying on its mathematical reasoning, which sometimes can lead to erroneous scores.Recent works, such as (Zhou et al. 2023) and (Wang et al. 2023), highlights the remarkable capability of OpenAI Code Interpreter in solving challenging math problems and self-verification.(Anand et al. 2024d) We utilize GPT-4o solely for error identification, which guides the routing to the appropriate refinement agent.The scores are used as the feedback to help refinement agents understand the first stage of the mistake from where the refinement needs to be initiated.</p>
<p>Refinement Agents</p>
<p>To address the three key errors in LLM generated solutions, we introduce a set of specialized refinement agents.Each agent is designed to rectify a specific type of error within the solution, ensuring targeted and effective corrections.The refinement agents use the same LLM with which the original solutions are generated.</p>
<p>Miscomprehension Refinement Although there are very few cases of problem miscomprehension in LLM generated solutions, once identified these mistakes can easily be corrected with simple instruction prompting:</p>
<p>You are tasked with solving a physics problem.Here is the question: [question], The following is your generated solution: [solution], In the generated solution, the correct objective of the question is not being addressed.The solutions contains mistakes which leads to misalignment with the objective of the question.Please carefully review the question &amp; understand the objective in detail and regenerate the solution accordingly.</p>
<p>The above prompt assists in refining the solution to align with the correct objective of the given question.This may involve regenerating the entire solution or correcting an intermediate mistake to ensure the solution addressed the correct objective.Similarly, any incorrect variable values used within the solution is corrected using instruction prompting.</p>
<p>Concept Refinement To address the incorrect concepts and formulae applied in the LLM's solutions, we utilize an external physics knowledge base.This is necessary because LLMs may not always have access to or accurately retrieve the correct formulae, as this information may not be embedded in their internal knowledge.The conceptual refinement occurs in two steps:</p>
<p>Error Identification &amp; Thought Generation: Given a solution S orig and a concept score Score concept , the LLM systematically reviews the solution to identify the earliest stage where an incorrect concept or formula has been applied.Score concept pinpoints this stage of error within the solution.LLM then generates a retrieval thought T R for the the concept or formula required at the failure stage.The thought is structured to be simple and sequential query.</p>
<ol>
<li>Concept Retrieval &amp; Solution Refinement: Given the retrieval thought T R and physics knowledge base K P , we use GraphRAG (Edge et al., 2024) to query the K P to retrieve an observation O T , based on T R as demonstrated in Figure 2. O T contains the correct context for concept and formulae required at the stage of failure.The LLM then initiates the refinement from this stage using the information present in O T , resulting in the refined solution S refined with corrected physics concepts and reasoning.</li>
</ol>
<p>Computational Refinement Inspired by recent works such as PAL (Gao et al. 2023), PoT (Chen et al. 2022), CSV (Zhou et al. 2023), MathCoder (Wang et al. 2023), we use code generation for the refinement of computational and mathematical errors within a solution.The computation score Score comp allows the LLM to locate the first step of error and then initiate the refinement of the failure stage and subsequent computations.The process occurs in two steps:</p>
<ol>
<li>
<p>Code Generation &amp; Execution: Given the original solution S orig and computation score Score comp , the LLM first locates the error step and then generates a Python code C p designed to accurately perform the necessary computation at the identified failure stage and produce the correct result.The generated code C p is then executed to obtain the response R c as shown in Figure 3.</p>
</li>
<li>
<p>Solution Refinement: The LLM is then instructed to refine S orig using the correct response R c generated by the code.This involves pinpointing the exact step where the error occurred, guided by the computation score Score comp , and integrating the correct computation from R c into the solution.The refined solution S refined is then presented with the corrected computations.</p>
</li>
</ol>
<p>Agent Routing and Iterative Refinement</p>
<p>After the error identification, the respective refinement agents are activated to mitigate these errors.The agent routing follows a prioritized sequence: 1.) miscomprehension refinement, 2.) concept refinement, 3.) computational refinement.This prioritization mirrors the human approach to solving physics problems: first, understanding the objectives and variables; next, identifying relevant concepts and formulae; and finally, applying them to perform the necessary computations.</p>
<p>The activated refinement agent then acts upon the solution to mitigate the error.The solution undergoes iterative cycles of error identification and refinement until all flags and scores are resolved or a maximum iteration limit is reached.This process ensures that all errors are corrected without introducing new ones in final refined solution.The complete process is illustrated in Algorithm 1.</p>
<p>Algorithm 1: Error Identification and Iterative Refinement</p>
<p>Require: Question Q, Initial Solution S0, GPT-4o L, Refinement Agents R, Maximum Iterations N , Threshold ϵ Ensure: Final refined solution to Q 1: i = 0, Si = S0 2: while i &lt; N do 3:  (Sun et al. 2023b), consisting 164 questions from physics divided into multiple sub-topics.MMLU (Hendrycks et al. 2021), consists of a 118 College level and 173 high school multiplechoice questions from various disciplines.
(F i obj , F i val , Score i concept , Score i comp ) ← L(Q, Si) 4: if F i obj == −1 or F i val == −1 then 5: Si+1 ← Rmiscomprehension(Q, Si) 6: else if Score i concept &lt; 1 − ϵ then 7: Si+1 ← Rconcept(Q, Si) 8: else if Score i comp &lt; 1 − ϵ then 9: Si+1 ← Rcomputation(Q, Si
LLMs We utilize the API of a range of models with varying parameters and capabilities including LLaMa-3-70B, LLaMa 3.1-405B, Gemma-2-27B, Gemini-1.5-Flash,GPT-3.5 Turbo and GPT-4 as our LLMs for the evaluation.We use same prompts for all the datasets and LLMs during our evaluation.</p>
<p>Baselines We employ an Answer-only approach (AO), where the model is given a question with four options and asked to select the correct answer without any explanation relying solely on its pre-existing knowledge .In contrast, few-shot prompting (Xu et al. 2023;Yasunaga et al. 2023) uses a few examples to help the model learn and apply that knowledge to similar tasks.Chain-of-Thought (CoT) prompting (Wei et al. 2022) guides the model to generate intermediate reasoning steps, improving its performance on complex tasks by breaking them down into smaller, more manageable parts.These three approaches form our primary baselines.</p>
<p>Evaluation Most of the existing works (Luo et al. 2023) , (Chern et al. 2023) , (Yu et al. 2023) measure the mathematical reasoning quality of LLMs by directly comparing the final answer and calculating the overall accuracy on a given dataset.(Anand et al. 2024a) We choose to follow the same evaluation for physics reasoning as well.</p>
<p>Results</p>
<p>In Table 2 we present results from our experiments reveal compelling insights into the strengths and challenges of vari-   ous models across diverse benchmarks.In the SciEval-Static benchmark, LLaMa-3-70B and Gemini 1.5 Flash stand out, with LLaMa-3-70B achieving an accuracy of 82.23% using (CoT), and Gemini 1.5 Flash not far behind at 85.97%.In the PhysicsQA domain, which demands intricate reasoning skills, the models face more significant challenges.LLaMa-3-70B and Gemma 2-27B both show improved performance with CoT, reaching 56.76% and 54.59%, respectively.On the MMLU-High benchmark, LLaMa-3-70B continues to perform solidly, achieving 72.88% with CoT, while Gemini 1.5 Flash pushes ahead to 79.66%.Interestingly, in MMLU-College, a benchmark with a mix of academic and reasoning tasks, Gemma 2-27B shows a significant leap in performance with CoT, reaching 73.52%, which surpasses its base score by over 22%, indicating the effectiveness of CoT in enhancing reasoning in academic settings.</p>
<p>As shown in Table 3, MoRA framework delivers marked improvements across all benchmarks for both LLaMA-3-70B and Gemma-2-27B models.In SciEval-Static, the introduction of MoRA enhances LLaMA-3-70B's accuracy from 82.23% (CoT) to 86.58%, and Gemma-2-27B sees a boost from 79.26% (CoT) to 88.76%.In the PhysicsQA dataset, MoRA significantly improves LLaMA-3-70B's performance from 59.29% (3-shot) to 70.14%, and Gemma-2-27B's from 59.45% (3-shot)to 70.62%.In MMLU High School, LLaMA-3-70B accuracy reaches from 73.66% (3-Shot) to 78.81% and in Gemma-2-27B, accuracy reaches from 77.11% (CoT) to 75.88%.In MMLU College, LLaMA-3-70B accuracy reaches from 71.76% (3-Shot) to 78.82% and in Gemma-2-27B, accuracy reaches from 73.52% (CoT) to 82.20%.The results show that even without extensive fine-tuning, these models can achieve competitive performance.These improvements demonstrate MoRA's ability to elevate smaller models to compete effectively with much larger ones across a range of complex tasks.</p>
<p>Analysis</p>
<p>In this section, we first conduct an in-depth error analysis of physics CoT solutions across various models and datasets, highlighting the error distribution that inspired the development of MoRA.We then present ablation studies, analyzing the effectiveness of each refinement agent in our framework.</p>
<p>Error Analysis</p>
<p>We perform manual analysis of the incorrect CoT solutions of GPT-4o, Llama-3-70B, and Gemma-2-27B on the following datasets: SciEval-Static, PhysicsQA, MMLU High School and College as shown in Table 4. Based on this analysis here are our observations:</p>
<p>(i) LLMs demonstrate good problem comprehension ability for physics question.All models demonstrate strong problem comprehension across the datasets.GPT-4o excels, achieving near-perfect accuracy on SciEval-Static, MMLU College, and High School, with only minor errors in Physic-sQA.This suggests a deep understanding of physics problem structure.Llama-3-70B and Gemma-2-27B also perform well but show slightly higher error rates, particularly in PhysicsQA and SciEval-Static, indicating occasional missed details that need attention.</p>
<p>(ii) Open source LLMs sometimes struggles to retrieve correct physics concept and formulae while reasoning.On average, 18.11% of questions in the PhysicsQA dataset are answered with conceptual errors by Gemma-2-27B and Llama-3-70B, highlighting the difficulty opensource LLMs face in applying correct concepts to physics problems.In contrast, GPT-4o excels with an average accuracy of 96.4% across all four datasets.Notably, Gemma-2-27B outperforms Llama-3-70B on SciEval-Static, Physic-sQA, and MMLU High School.The high error rates of both Llama-3-70B and Gemma-2-27B on PhysicsQA suggest that medium-parameter open-source LLMs may still need external knowledge bases for complex physics problemsolving.</p>
<p>(iii) Open-source LLMs struggles with algebraic and arithmetic computation required while solving physics On average, 21.62% of questions in the Physic-sQA dataset are answered with computational mistakes by Gemma-2-27B and Llama-3-70B, highlighting challenges in executing correct calculations.GPT-4o excels with an average accuracy of 95.64% across four datasets.Gemma-2-27B outperforms Llama-3-70B on SciEval Static and MMLU (High School and College), with both performing similarly on PhysicsQA.The accuracy gap between GPT-4o and Llama-3-70B (12.16%) and GPT-4o and Gemma-2-27B (13.24%) on PhysicsQA suggests that open-source LLMs could benefit from further refinement in handling complex calculations.</p>
<p>Ablation</p>
<p>To understand the effectiveness of each refinement agent, we conduct ablation of each of the refinement agents with Llama-3-70B and Gemma-2-70B in terms of their refinement rate across different datasets as shown in Table 5.Here are our observations:</p>
<p>(i) Problem miscomprhension errors are mitigated with simple instruction prompting and error feedback.Llama-3-70B and Gemma-2-27B demonstrate good miscomprehension error refinement with instruction prompting, particularly in the MMLU datasets (High School and College), where both models achieve a perfect 100% refinement rate.Llama-3-70B outperforms Gemma-2-27B slightly on PhysicsQA, with a refinement rate of 66.7% compared to Gemma-2-27B's 62.5%.However, Gemma-2-27B excels in SciEval Static and MMLU (College and High School).The decent accuracy on PhysicsQA suggests that open-source LLMs sometimes fail to rectify their misinterpretations in complex physics problems.</p>
<p>(ii) Open-source LLM performers moderately in identifying the conceptual mistake and retrieval thought generation.Llama-3-70B shows a 46.9% refinement rate in PhysicsQA and slightly improves in SciEval Static at 57.1%, but struggles with MMLU datasets, achieving 37.5% and 33.3% refinement in High School and College, respectively.Gemma-2-27B has a similar 48.7% refinement rate in PhysicsQA and performs better in SciEval Static at 62.5%, but underperforms significantly on MMLU High School with a 16.7% refinement rate, improving modestly to 37.5% on MMLU College.These results suggest that open-source LLMs have difficulty generating relevant retrieval thoughts at the initial stage of failure.</p>
<p>(iii) Using code-driven refinement significantly corrects the computational errors.Llama-3-70B and Gemma-2-27B excel in refining computational errors, demonstrating the effectiveness of code generation and execution.Llama-3-70B shows consistent performance with a 72.6% refinement rate on PhysicsQA and strong results across SciEval Static (60%), MMLU High School (75%), and MMLU College (81.8%).Gemma-2-27B slightly outperforms in PhysicsQA at 73.3% and achieves a 100% refinement rate in MMLU College.However, Gemma-2-27B's performance is more variable, particularly in MMLU High School (33.3%), indicating potential challenges in specific code generation scenarios.</p>
<p>Conclusion</p>
<p>In this work, we introduce MoRA, a novel agentic refinement framework designed to mitigate three critical errors commonly made by LLMs when solving complex physics problems.MoRA first leverages GPT-4o for error identification and score assignment, which are then subsequently used to guide the refinement agents.This process is done iteratively until all the errors in the solution are mitigated successfully.To ensure a comprehensive evaluation, we also curate our own dataset, PhysicsQA, which includes a diverse set of high school-level physics problems.Our experiments and in-depth analysis across multiple datasets demonstrate that MoRA significantly enhances the performance of Llama-3-70B and Gemma-2-27B across multiple datasets.</p>
<p>Figure 1 :
1
Figure 1: The illustration of three key error observations in the CoT solution of open source LLMs for physics problems.(a)showcases problem miscomprehension, where the LLM response uses the incorrect value of variables given in the question here, M instead of 9M, (b) showcases incorrect concept application in the LLM response, here incorrect moment of inertia formula for uniform cylinder, (c) demonstrate computational error within LLM response here, incorrect calculation of time period.</p>
<p>Figure 3 :
3
Figure 3: The illustration of code generation and execution for computation error refinement in LLM response.Given the response and computation verification score, LLM generates a code to perform the correct required computation; the code is then executed to obtain the response.</p>
<p>Table 1 :
1
Topic-wise Distribution in PhysicsQA
NamePercentageElectromagnetism29.8%Mechanics and Kinematics21.8%Thermodynamics and Heat15.7%Waves and Optics15.4%Nuclear and Modern Physics8.9%Material Properties and Elasticity8.3%</p>
<p>Table 2 :
2
Experimentation of Answer-Only (AO) , COT and Few-Shot (3-shot) on different Datasets
ModelDatasetAOCOT 3-Shot MORAGemma 2 MMLU College51.11% 73.52% 67.64% 82.20%27BMMLU High School 55.93% 77.11% 74.45% 75.88%PhysicsQA39.18% 54.59% 59.45% 70.62%SciEval-Static60.36% 79.26% 53.04% 88.76%LLaMa 3 MMLU College59.41% 71.76% 71.76% 78.82%70BMMLU High School 60.16% 72.88% 73.66% 78.81%PhysicsQA38.37% 56.76% 59.29% 70.14%SciEval-Static70.07% 82.23% 63.41% 86.58%</p>
<p>Table 3 :
3
Comparison of baseline approaches with MoRA across four datasets: SciEval-Static, PhysicsQA, MMLU High School and College based on final answer accuracy.</p>
<p>Table 4 :
4
Error Analysis of incorrect physics CoT solutions of different models across four datasets.
Error TypeDatasetGemma LLaMa2-27B3-70BComputationalMMLU College100%81.8%RefinementMMLU High School 33.3% 75.0%PhysicsQA73.3% 72.6%SciEval-Static57.1% 60.0%Miscomprehension MMLU College37.5% 33.3%RefinementMMLU High School 16.7% 37.5%PhysicsQA48.7% 46.9%SciEval-Static62.5% 57.1%ConceptMMLU College100%100%RefinementMMLU High School 100%100%PhysicsQA62.5% 66.7%SciEval-Static100%66.7%</p>
<p>Table 5 :
5
Ablation studies for different refinement agent in MoRA using Gemma-2-27B and Llama-3-70B across four datasets, evaluated by refinement rate.</p>
<p>Revolutionizing High School Physics Education: A Novel Dataset. A Anand, K Addala, K Baghel, A Goel, M Hira, R Gupta, R R Shah, Big Data and Artificial Intelligence. V Goyal, N Kumar, S S Bhowmick, P Goyal, N Goyal, D Kumar, Cham; Nature SwitzerlandSpringer2023a</p>
<p>Sciphyrag-retrieval augmentation to improve llms on physics q &amp;a. A Anand, A Goel, M Hira, S Buldeo, J Kumar, A Verma, R Gupta, R R Shah, International Conference on Big Data Analytics. Springer2023b</p>
<p>KG-CTG: citation generation through knowledge graph-guided large language models. A Anand, M Gupta, K Prasad, U Goel, N Lal, A Verma, R R Shah, International Conference on Big Data Analytics. Springer2023c</p>
<p>A Anand, M Gupta, K Prasad, N Singla, S Sanjeev, J Kumar, A R Shivam, R R Shah, arXiv:2404.13099Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks. 2024aarXiv preprint</p>
<p>GEC-DCL: Grammatical Error Correction Model with Dynamic Context Learning for Paragraphs and Scholarly Papers. A Anand, A Jairath, N Lal, S Bangar, J Sikka, A Verma, R R Shah, S Satoh, International Conference on Big Data Analytics. Springer2023d</p>
<p>GeoVQA: A Comprehensive Multimodal Geometry Dataset for Secondary Education. A Anand, R Jaiswal, A Dharmadhikari, A Marathe, H Popat, H Mital, A R Nair, K Prasad, S Kumar, A Verma, 2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE2024b</p>
<p>A Anand, J Kapuriya, C Kirtani, A Singh, J Saraf, N Lal, J Kumar, A R Shivam, A Verma, R R Shah, arXiv:2404.12926MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering. 2024carXiv preprint</p>
<p>MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting. A Anand, J Kapuriya, A Singh, J Saraf, N Lal, A Verma, R Gupta, R Shah, Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer2024d</p>
<p>Context-enhanced language models for generating multi-paper citations. A Anand, K Prasad, U Goel, M Gupta, N Lal, A Verma, R R Shah, International Conference on Big Data Analytics. Springer2023e</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. D Arora, H G Singh, arXiv:2305.150742023arXiv preprint</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 202033</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, E Chern, H Zou, X Li, J Hu, K Feng, J Li, P Liu, arXiv:2211.12588Generative ai for math. 2022. 2023arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>From local to global: A graph rag approach to query-focused summarization. D Edge, H Trinh, N Cheng, J Bradley, A Chao, A Mody, S Truitt, J Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Complexity-based prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. C He, R Luo, Y Bai, S Hu, Z L Thai, J Shen, J Hu, X Han, Y Huang, Y Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>X Li, W Wang, M Li, J Guo, Y Zhang, F Feng, arXiv:2406.00755Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction. 2024arXiv preprint</p>
<p>Deductive verification of chain-ofthought reasoning. Z Ling, Y Fang, X Li, Z Huang, M Lee, R Memisevic, H Su, Advances in Neural Information Processing Systems. 202436</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Y Ma, Z Gou, J Hao, R Xu, S Wang, L Pan, Y Yang, Y Cao, A Sun, arXiv:2402.11451SciAgent: Tool-augmented Language Models for Scientific Reasoning. 2024arXiv preprint</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. N Teh, Y W Rainforth, T , arXiv:2308.004362023arXiv preprint</p>
<p>Rethinking language models as symbolic knowledge graphs. V Mruthyunjaya, P Pezeshkpour, E Hruschka, N Bhutani, arXiv:2308.136762023arXiv preprint</p>
<p>S Ouyang, Z Zhang, B Yan, X Liu, J Han, L Qin, arXiv:2311.09656Structured chemistry reasoning with large language models. 2023arXiv preprint</p>
<p>F Petroni, A Piktus, A Fan, P Lewis, M Yazdani, N De Cao, J Thorne, Y Jernite, V Karpukhin, J Maillard, arXiv:2009.02252KILT: a benchmark for knowledge intensive language tasks. 2020arXiv preprint</p>
<p>Enhancing chain-of-thoughts prompting with iterative bootstrapping in large language models. J Sun, Y Luo, Y Gong, C Lin, Y Shen, J Guo, N Duan, arXiv:2304.116572023aarXiv preprint</p>
<p>L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, arXiv:2308.13149SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research. 2023barXiv preprint</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>LLMs cannot find reasoning errors, but can correct them given the error location. G Tyen, H Mansoor, V Carbune, P Chen, T Mak, Findings of the Association for Computational Linguistics ACL 2024. L.-W Ku, A Martins, V Srikumar, BangkokAssociation for Computational Linguistics2024</p>
<p>K Wang, H Ren, A Zhou, Z Lu, S Luo, W Shi, R Zhang, L Song, M Zhan, H Li, arXiv:2310.03731Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. 2023arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>B Xu, A Yang, J Lin, Q Wang, C Zhou, Y Zhang, Z Mao, arXiv:2305.14688Expertprompting: Instructing large language models to be distinguished experts. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Large language models as analogical reasoners. M Yasunaga, X Chen, Y Li, P Pasupat, J Leskovec, P Liang, E H Chi, D Zhou, L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, arXiv:2310.01714arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023. 2023arXiv preprint</p>
<p>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. Z Yuan, H Yuan, C Li, G Dong, K Lu, C Tan, C Zhou, J Zhou, Z Zhang, A Zhang, M Li, A Smola, arXiv:2210.034932024. 2022arXiv preprintAutomatic chain of thought prompting in large language models</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with codebased self-verification. A Zhou, K Wang, Z Lu, W Shi, S Luo, Z Qin, S Lu, A Jia, L Song, M Zhan, arXiv:2308.079212023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
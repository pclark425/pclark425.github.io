<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5458 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5458</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5458</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1ca3b6ff250b4f73486a89f6954edcc4ae21834e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1ca3b6ff250b4f73486a89f6954edcc4ae21834e" target="_blank">When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> The influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher.</p>
                <p><strong>Paper Abstract:</strong> Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5458.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5458.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-SR2V-TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-16k-0613) using Single-Round Self-Reflection Verification on TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiment applying a single-round generate-then-reflect prompting pipeline (SR^2V) to ChatGPT (gpt-3.5-turbo-16k-0613) on the TruthfulQA benchmark, comparing standard prompting, an exploration-only baseline, and generate-then-reflect revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-16k-0613 (ChatGPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI ChatGPT (gpt-3.5-turbo-16k-0613) chat model used as a single snapshot for experiments; large transformer-based conversational model accessible via OpenAI API (exact parameter count not stated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-stage single-round pipeline: (1) Exploration — sample K candidate responses independently from the model (default K=4; sometimes K=10 in artificial-response experiments); (2) Reflection — for each candidate r_j, prompt the model with [input; r_j] to generate a self-critique c_j; (3) Revision — concatenate all response+critique pairs and prompt the same model to produce a final revised answer. No external feedback or iterative multi-round prompting is allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark assessing truthfulness of model responses across diverse questions where models often mimic human falsehoods; evaluated with ROUGE-1 and BLEURT similarity to preferred references.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported qualitatively as 'significantly better' than both exploration-only and standard prompting for ChatGPT under the SR^2V single-round setting (paper states a statistically significant improvement but does not provide the main-table numeric breakdown for ChatGPT SR^2V in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Exploration-only baseline and standard prompting (no numeric breakdown given for main SR^2V ChatGPT results in main text); exploration-only serves as concatenation-of-responses aggregation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Authors report statistically significant improvement on TruthfulQA for ChatGPT when using SR^2V (generate-then-reflect) versus exploration-only and standard prompting; consistent with prior observations that model self-evaluation can increase factuality. The paper provides evaluation using ROUGE-1 and BLEURT metrics (TruthfulQA automatic evaluation), and reports consistent improvement trends across models (see appendices for other models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvement is dataset-dependent: although TruthfulQA improved, HotpotQA performance declined (see separate entry). Effectiveness depends on initial response accuracy (Response Accuracy, RA) and question difficulty: self-reflection benefits when RA is low or questions are hard; harms when RA is reliably high. The paper also notes limitations from using a single model snapshot, difficulty predicting RA without ground truth, and potential sensitivity to prompt phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5458.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5458.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-SR2V-HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-16k-0613) using Single-Round Self-Reflection Verification on HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SR^2V generate-then-reflect prompting on ChatGPT for the HotpotQA multi-hop reasoning dataset, comparing outcomes to exploration-only and standard prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-16k-0613 (ChatGPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI ChatGPT (gpt-3.5-turbo-16k-0613) chat model used for single-round multi-response experiments; token budget extended to 16k to accommodate concatenated responses and critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same three-stage SR^2V pipeline: sample K independent candidate answers (K=4 default), produce individual self-critiques per candidate, and prompt the model with all candidate+critique pairs to generate a final revised answer. Iterative multi-round prompting and any external feedback are disallowed.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering dataset requiring complex multi-document reasoning (Wikipedia-based); evaluation commonly uses exact-match style accuracy, but authors manually assessed 1,000 HotpotQA answers due to semantic variability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Approximately 4% lower accuracy compared to exploration-only and standard prompting for ChatGPT under SR^2V (paper states 'about 4% worse' for HotpotQA with self-reflection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Exploration-only baseline and standard prompting both outperform self-reflection by around 4% on HotpotQA (exact numeric breakdown for ChatGPT SR^2V not presented in main text, but the relative decline is reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No improvement observed for HotpotQA with SR^2V for ChatGPT: empirical results show a decrease (~4%) in accuracy when using self-reflection versus baselines. The authors provide manual evaluation for HotpotQA and further decompose performance by question difficulty and Response Accuracy (RA).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-reflection harms performance on multi-hop reasoning (HotpotQA) when the model's initial candidate answers are already often correct (high RA). The paper identifies an interaction: reflection is harmful when the model is reliably correct and can reduce accuracy by deviating from majority/consistent correct candidate responses. Also, SR^2V disallows iterative hints but still can cause the model to change a correct initial consensus to an incorrect revision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5458.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5458.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-SR2V-TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-7b-chat using Single-Round Self-Reflection Verification on TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiment applying SR^2V generate-then-reflect prompting to LLaMA-2-7b-chat evaluated on TruthfulQA, with automatic metrics ROUGE-1 and BLEURT reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 chat model (7B parameters) used in experiments; transformer-based chat model with standard 4096 token context limit.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Parallel sampling of K independent candidate responses, model-self critique for each candidate, and final revision prompt concatenating candidate+critique pairs; K=4 in primary experiments (context limit constrained to 4096 for LLaMA-2).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmark measuring truthfulness; evaluated here using ROUGE-1 and BLEURT similarity to preferred references.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>ROUGE-1 = 53.8 ± 0.4; BLEURT = 63.0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting ROUGE-1 = 53.8 ± 0.4; BLEURT = 60.9 ± 0.6; Exploration-only ROUGE-1 = 51.7, BLEURT = 58.2</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>BLEURT improved from 60.9 (standard) and 58.2 (exploration-only) to 63.0 with self-reflection; ROUGE-1 with self-reflection matches standard prompting and exceeds exploration-only. This suggests SR^2V improved semantic/factual alignment (BLEURT) though ROUGE-1 change vs standard was neutral.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mixed metric signal (ROUGE unchanged vs standard, BLEURT up), indicating that improvements may be metric-dependent. LLaMA-2 context limit prevented replication of K=10 artificial-response experiments. As with ChatGPT, gains depend on RA and question difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5458.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5458.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-SR2V-HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-7b-chat using Single-Round Self-Reflection Verification on HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SR^2V to LLaMA-2-7b-chat on HotpotQA with manual accuracy evaluation and decomposition by question difficulty and Response Accuracy (RA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-2 family chat model (7B parameters) with 4096-token context limit; used with manual evaluation on HotpotQA.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Parallel sampling of K candidate answers (K=4), per-candidate self-critiques prompted with the original context+candidate, and final revision using concatenated candidate+critique pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA dataset requiring reasoning across multiple supporting facts; manual evaluation used to judge answer correctness due to semantic variability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Accuracy (manual) = 57.5 ± (reported as value in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting Accuracy = 61.0 ± 1.0; Exploration-only Accuracy = 62.9</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No improvement: self-reflection reduced accuracy versus both standard prompting and exploration-only for HotpotQA on LLaMA-2-7b-chat, consistent with ChatGPT pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reflection harmed performance when RA was high; highlights dependence on initial candidate accuracy and question difficulty. Context length limited ability to run larger K artificial-response experiments for LLaMA-2 in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5458.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5458.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-SR2V-TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7B-v0.1 using Single-Round Self-Reflection Verification on TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SR^2V to Mixtral-8x7B-v0.1 on TruthfulQA with automatic ROUGE-1 and BLEURT metrics reported, showing mixed metric outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral of experts model (8x7B configuration) evaluated in the paper; larger token limits allowed replication of artificial-response K=10 experiments for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same SR^2V pipeline: sample K candidate answers (K=4 default; artificial experiments used K=10), generate per-candidate self-critiques, concatenate pairs and prompt model for revised answer.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Truthfulness evaluation over diverse questions; evaluated with ROUGE-1 and BLEURT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>ROUGE-1 = 63.3 ±  (reported in Table 4); BLEURT = 71.7</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting ROUGE-1 = 59.1 ± 1.0, BLEURT = 71.5 ± 0.4; Exploration-only ROUGE-1 = 61.3, BLEURT = 73.9</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>ROUGE-1 increased from 59.1 (standard) and 61.3 (exploration-only) to 63.3 with self-reflection, indicating improvement in surface overlap; BLEURT however decreased slightly from exploration-only (73.9) to 71.7 with self-reflection, showing mixed signals depending on metric.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-reflection effects vary by metric (ROUGE improved, BLEURT decreased relative to exploration-only). Authors hypothesize Mixtral may be less sensitive to reflection instructions; in many HotpotQA conditions self-reflection failed to improve and in most cases was harmful (see Mixtral HotpotQA entry).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5458.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5458.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-SR2V-HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7B-v0.1 using Single-Round Self-Reflection Verification on HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of SR^2V on Mixtral-8x7B-v0.1 for HotpotQA; results show self-reflection does not improve and often harms performance for this model/dataset configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral mixture-of-experts model (8x7B) with larger context capacity; used to run K=10 artificial-response experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Parallel sampling of candidate responses (K=4 for primary; K=10 artificial responses for simulation), per-candidate critiques, and final revision prompt concatenating all response-reflection pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA requiring reasoning over supporting facts; manual accuracy evaluation used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Accuracy (manual) = 89.2 (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting Accuracy = 89.8 ± 0.3; Exploration-only Accuracy = 90.9</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No improvement: self-reflection reduced or did not increase HotpotQA accuracy for Mixtral in most settings; artificial-response experiments showed self-reflection sometimes helps at 0% RA for easy questions but is harmful in most other configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mixtral appears less sensitive/competent at following reflection instructions; reflection often acted as a distractor. Majority-voting tendencies changed little with reflection for Mixtral compared to ChatGPT, suggesting limited ability to use reflective critiques effectively. Overall, self-reflection can harm performance, especially when initial RA is high or when the model cannot utilize critique instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Constitutional ai: Harmlessness from ai feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5458",
    "paper_id": "paper-1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "ChatGPT-SR2V-TruthfulQA",
            "name_full": "ChatGPT (gpt-3.5-turbo-16k-0613) using Single-Round Self-Reflection Verification on TruthfulQA",
            "brief_description": "Experiment applying a single-round generate-then-reflect prompting pipeline (SR^2V) to ChatGPT (gpt-3.5-turbo-16k-0613) on the TruthfulQA benchmark, comparing standard prompting, an exploration-only baseline, and generate-then-reflect revisions.",
            "citation_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-16k-0613 (ChatGPT-3.5)",
            "model_description": "OpenAI ChatGPT (gpt-3.5-turbo-16k-0613) chat model used as a single snapshot for experiments; large transformer-based conversational model accessible via OpenAI API (exact parameter count not stated in paper).",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect",
            "reflection_method_description": "Three-stage single-round pipeline: (1) Exploration — sample K candidate responses independently from the model (default K=4; sometimes K=10 in artificial-response experiments); (2) Reflection — for each candidate r_j, prompt the model with [input; r_j] to generate a self-critique c_j; (3) Revision — concatenate all response+critique pairs and prompt the same model to produce a final revised answer. No external feedback or iterative multi-round prompting is allowed.",
            "num_iterations": 1,
            "task_name": "TruthfulQA",
            "task_description": "Benchmark assessing truthfulness of model responses across diverse questions where models often mimic human falsehoods; evaluated with ROUGE-1 and BLEURT similarity to preferred references.",
            "performance_with_reflection": "Reported qualitatively as 'significantly better' than both exploration-only and standard prompting for ChatGPT under the SR^2V single-round setting (paper states a statistically significant improvement but does not provide the main-table numeric breakdown for ChatGPT SR^2V in the main text).",
            "performance_without_reflection": "Exploration-only baseline and standard prompting (no numeric breakdown given for main SR^2V ChatGPT results in main text); exploration-only serves as concatenation-of-responses aggregation baseline.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Authors report statistically significant improvement on TruthfulQA for ChatGPT when using SR^2V (generate-then-reflect) versus exploration-only and standard prompting; consistent with prior observations that model self-evaluation can increase factuality. The paper provides evaluation using ROUGE-1 and BLEURT metrics (TruthfulQA automatic evaluation), and reports consistent improvement trends across models (see appendices for other models).",
            "limitations_or_failure_cases": "Improvement is dataset-dependent: although TruthfulQA improved, HotpotQA performance declined (see separate entry). Effectiveness depends on initial response accuracy (Response Accuracy, RA) and question difficulty: self-reflection benefits when RA is low or questions are hard; harms when RA is reliably high. The paper also notes limitations from using a single model snapshot, difficulty predicting RA without ground truth, and potential sensitivity to prompt phrasing.",
            "uuid": "e5458.0",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatGPT-SR2V-HotpotQA",
            "name_full": "ChatGPT (gpt-3.5-turbo-16k-0613) using Single-Round Self-Reflection Verification on HotpotQA",
            "brief_description": "Application of SR^2V generate-then-reflect prompting on ChatGPT for the HotpotQA multi-hop reasoning dataset, comparing outcomes to exploration-only and standard prompting.",
            "citation_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-16k-0613 (ChatGPT-3.5)",
            "model_description": "OpenAI ChatGPT (gpt-3.5-turbo-16k-0613) chat model used for single-round multi-response experiments; token budget extended to 16k to accommodate concatenated responses and critiques.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect",
            "reflection_method_description": "Same three-stage SR^2V pipeline: sample K independent candidate answers (K=4 default), produce individual self-critiques per candidate, and prompt the model with all candidate+critique pairs to generate a final revised answer. Iterative multi-round prompting and any external feedback are disallowed.",
            "num_iterations": 1,
            "task_name": "HotpotQA",
            "task_description": "Multi-hop question answering dataset requiring complex multi-document reasoning (Wikipedia-based); evaluation commonly uses exact-match style accuracy, but authors manually assessed 1,000 HotpotQA answers due to semantic variability.",
            "performance_with_reflection": "Approximately 4% lower accuracy compared to exploration-only and standard prompting for ChatGPT under SR^2V (paper states 'about 4% worse' for HotpotQA with self-reflection).",
            "performance_without_reflection": "Exploration-only baseline and standard prompting both outperform self-reflection by around 4% on HotpotQA (exact numeric breakdown for ChatGPT SR^2V not presented in main text, but the relative decline is reported).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No improvement observed for HotpotQA with SR^2V for ChatGPT: empirical results show a decrease (~4%) in accuracy when using self-reflection versus baselines. The authors provide manual evaluation for HotpotQA and further decompose performance by question difficulty and Response Accuracy (RA).",
            "limitations_or_failure_cases": "Self-reflection harms performance on multi-hop reasoning (HotpotQA) when the model's initial candidate answers are already often correct (high RA). The paper identifies an interaction: reflection is harmful when the model is reliably correct and can reduce accuracy by deviating from majority/consistent correct candidate responses. Also, SR^2V disallows iterative hints but still can cause the model to change a correct initial consensus to an incorrect revision.",
            "uuid": "e5458.1",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA2-SR2V-TruthfulQA",
            "name_full": "LLaMA-2-7b-chat using Single-Round Self-Reflection Verification on TruthfulQA",
            "brief_description": "Experiment applying SR^2V generate-then-reflect prompting to LLaMA-2-7b-chat evaluated on TruthfulQA, with automatic metrics ROUGE-1 and BLEURT reported.",
            "citation_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7b-chat",
            "model_description": "Open-source LLaMA-2 chat model (7B parameters) used in experiments; transformer-based chat model with standard 4096 token context limit.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect",
            "reflection_method_description": "Parallel sampling of K independent candidate responses, model-self critique for each candidate, and final revision prompt concatenating candidate+critique pairs; K=4 in primary experiments (context limit constrained to 4096 for LLaMA-2).",
            "num_iterations": 1,
            "task_name": "TruthfulQA",
            "task_description": "Benchmark measuring truthfulness; evaluated here using ROUGE-1 and BLEURT similarity to preferred references.",
            "performance_with_reflection": "ROUGE-1 = 53.8 ± 0.4; BLEURT = 63.0",
            "performance_without_reflection": "Standard prompting ROUGE-1 = 53.8 ± 0.4; BLEURT = 60.9 ± 0.6; Exploration-only ROUGE-1 = 51.7, BLEURT = 58.2",
            "has_performance_comparison": true,
            "evidence_of_improvement": "BLEURT improved from 60.9 (standard) and 58.2 (exploration-only) to 63.0 with self-reflection; ROUGE-1 with self-reflection matches standard prompting and exceeds exploration-only. This suggests SR^2V improved semantic/factual alignment (BLEURT) though ROUGE-1 change vs standard was neutral.",
            "limitations_or_failure_cases": "Mixed metric signal (ROUGE unchanged vs standard, BLEURT up), indicating that improvements may be metric-dependent. LLaMA-2 context limit prevented replication of K=10 artificial-response experiments. As with ChatGPT, gains depend on RA and question difficulty.",
            "uuid": "e5458.2",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaMA2-SR2V-HotpotQA",
            "name_full": "LLaMA-2-7b-chat using Single-Round Self-Reflection Verification on HotpotQA",
            "brief_description": "Application of SR^2V to LLaMA-2-7b-chat on HotpotQA with manual accuracy evaluation and decomposition by question difficulty and Response Accuracy (RA).",
            "citation_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7b-chat",
            "model_description": "Open-source LLaMA-2 family chat model (7B parameters) with 4096-token context limit; used with manual evaluation on HotpotQA.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect",
            "reflection_method_description": "Parallel sampling of K candidate answers (K=4), per-candidate self-critiques prompted with the original context+candidate, and final revision using concatenated candidate+critique pairs.",
            "num_iterations": 1,
            "task_name": "HotpotQA",
            "task_description": "Multi-hop QA dataset requiring reasoning across multiple supporting facts; manual evaluation used to judge answer correctness due to semantic variability.",
            "performance_with_reflection": "Accuracy (manual) = 57.5 ± (reported as value in Table 3)",
            "performance_without_reflection": "Standard prompting Accuracy = 61.0 ± 1.0; Exploration-only Accuracy = 62.9",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No improvement: self-reflection reduced accuracy versus both standard prompting and exploration-only for HotpotQA on LLaMA-2-7b-chat, consistent with ChatGPT pattern.",
            "limitations_or_failure_cases": "Reflection harmed performance when RA was high; highlights dependence on initial candidate accuracy and question difficulty. Context length limited ability to run larger K artificial-response experiments for LLaMA-2 in this study.",
            "uuid": "e5458.3",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mixtral-SR2V-TruthfulQA",
            "name_full": "Mixtral-8x7B-v0.1 using Single-Round Self-Reflection Verification on TruthfulQA",
            "brief_description": "Application of SR^2V to Mixtral-8x7B-v0.1 on TruthfulQA with automatic ROUGE-1 and BLEURT metrics reported, showing mixed metric outcomes.",
            "citation_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-v0.1",
            "model_description": "Mixtral of experts model (8x7B configuration) evaluated in the paper; larger token limits allowed replication of artificial-response K=10 experiments for this model.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect",
            "reflection_method_description": "Same SR^2V pipeline: sample K candidate answers (K=4 default; artificial experiments used K=10), generate per-candidate self-critiques, concatenate pairs and prompt model for revised answer.",
            "num_iterations": 1,
            "task_name": "TruthfulQA",
            "task_description": "Truthfulness evaluation over diverse questions; evaluated with ROUGE-1 and BLEURT.",
            "performance_with_reflection": "ROUGE-1 = 63.3 ±  (reported in Table 4); BLEURT = 71.7",
            "performance_without_reflection": "Standard prompting ROUGE-1 = 59.1 ± 1.0, BLEURT = 71.5 ± 0.4; Exploration-only ROUGE-1 = 61.3, BLEURT = 73.9",
            "has_performance_comparison": true,
            "evidence_of_improvement": "ROUGE-1 increased from 59.1 (standard) and 61.3 (exploration-only) to 63.3 with self-reflection, indicating improvement in surface overlap; BLEURT however decreased slightly from exploration-only (73.9) to 71.7 with self-reflection, showing mixed signals depending on metric.",
            "limitations_or_failure_cases": "Self-reflection effects vary by metric (ROUGE improved, BLEURT decreased relative to exploration-only). Authors hypothesize Mixtral may be less sensitive to reflection instructions; in many HotpotQA conditions self-reflection failed to improve and in most cases was harmful (see Mixtral HotpotQA entry).",
            "uuid": "e5458.4",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mixtral-SR2V-HotpotQA",
            "name_full": "Mixtral-8x7B-v0.1 using Single-Round Self-Reflection Verification on HotpotQA",
            "brief_description": "Evaluation of SR^2V on Mixtral-8x7B-v0.1 for HotpotQA; results show self-reflection does not improve and often harms performance for this model/dataset configuration.",
            "citation_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-v0.1",
            "model_description": "Mixtral mixture-of-experts model (8x7B) with larger context capacity; used to run K=10 artificial-response experiments in the paper.",
            "reflection_method_name": "Single-Round Self-Reflection Verification (SR^2V) / generate-then-reflect",
            "reflection_method_description": "Parallel sampling of candidate responses (K=4 for primary; K=10 artificial responses for simulation), per-candidate critiques, and final revision prompt concatenating all response-reflection pairs.",
            "num_iterations": 1,
            "task_name": "HotpotQA",
            "task_description": "Multi-hop QA requiring reasoning over supporting facts; manual accuracy evaluation used.",
            "performance_with_reflection": "Accuracy (manual) = 89.2 (Table 4)",
            "performance_without_reflection": "Standard prompting Accuracy = 89.8 ± 0.3; Exploration-only Accuracy = 90.9",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No improvement: self-reflection reduced or did not increase HotpotQA accuracy for Mixtral in most settings; artificial-response experiments showed self-reflection sometimes helps at 0% RA for easy questions but is harmful in most other configurations.",
            "limitations_or_failure_cases": "Mixtral appears less sensitive/competent at following reflection instructions; reflection often acted as a distractor. Majority-voting tendencies changed little with reflection for Mixtral compared to ChatGPT, suggesting limited ability to use reflective critiques effectively. Overall, self-reflection can harm performance, especially when initial RA is high or when the model cannot utilize critique instructions.",
            "uuid": "e5458.5",
            "source_info": {
                "paper_title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Constitutional ai: Harmlessness from ai feedback",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        }
    ],
    "cost": 0.016725749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</h1>
<p>Yanhong $\mathbf{L i}^{1, <em>}$, Chenghao Yang ${ }^{1, </em>}$, Allyson Ettinger ${ }^{2}$<br>${ }^{1}$ University of Chicago ${ }^{2}$ Allen Institute for AI<br>{yanhongli, chenghao}@uchicago.edu<br>allysone@allenai.org</p>
<h4>Abstract</h4>
<p>Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https: //github.com/yanhong-lbh/ LLM-SelfReflection-Eval.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have shown impressive performance in generating human-like text (e.g., ChatGPT (OpenAI, 2021)), and recent works demonstrate that we can further prompt LLMs to reflect on their own outputs to improve their capabilities on complicated reasoning, programming and planning tasks (Huang et al., 2022; Kim et al., 2023; Madaan et al., 2023; Shinn et al., 2023; Chen et al., 2023b; Wang et al., 2023b) and also improve their alignment with human values (e.g., less harmful</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example of Self-Reflection Prompting
and more helpful) (Bai et al., 2022; Ganguli et al., 2023). ${ }^{1}$ However, Huang et al. (2023) find that performance gains associated with self-reflection may be due to implicit usage of external feedback as a stop criterion, as well as overly-engineered prompts that bias the model outputs, casting doubt on the true effectiveness of self-reflection.</p>
<p>To verify the extent to which LLMs can truly reflect on their outputs, we take a more stringent evaluation approach: in addition to excluding external feedback (Huang et al., 2023), we also disallow multi-round iterative prompting, which can hint to the model that its prior response is incorrect. Instead, we sample multiple model responses given a prompt, and ask the model to self-reflect on these candidate outputs. With this single-round testing, we can zero in on the model's ability to use selfreflection without implicit hints about whether a given response candidate is correct or incorrect.</p>
<p>Our experiments show that, in a case study with ChatGPT on different QA datasets, self-reflection in our setting yields mixed results. Specifically, self-reflection improves performance on TruthfulQA (Lin et al., 2022), but decreases model performance in HotpotQA (Yang et al., 2018). Through follow-up analyses, we identify that the effectiveness of self-reflection strongly depends on the confidence in accuracy of the model's initial responses, as well as overall question difficulty as</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>judged by humans: when the model is reliably giving correct answers from the start, self-reflection is more often harmful—however, on questions of greater difficulty, self-reflection is beneficial even when a decent percent of initial model responses are correct. We also find that self-reflection reduces model tendency toward majority voting, suggesting more sophisticated decision-making (albeit sometimes resulting in lower accuracy). Based on our findings, we propose a practical guideline for users to decide when to use self-reflection.</p>
<h2>2 Self-Reflection Prompting</h2>
<p>To focus on evaluating intrinsic reflective thinking capability, we adopt the following evaluation setting: in addition to the Huang et al. (2023) protocol of excluding external feedback and prompt optimization, we additionally disallow iterative prompting, which samples new responses based on previous responses, creating an implicit hint to bias the model behavior (Huang et al., 2023). ${ }^{2}$ We call our approach Single-Round Self-Reflection Verification ( $S R^{2} V$ ). We evaluate LLMs' reflective thinking capability using the following simple threestage format: 1) Exploration: Given an input $X$, we prompt LLM $M$ to generate $K$ candidate responses $r_{j} \sim P_{M}\left(r_{j} \mid X, I_{\text {Exploration }}\right), 1 \leq j \leq K$ with instruction $I_{\text {Exploration }}$. Note that this generation of candidate responses differs from iterative prompting because each response is sampled without conditioning on any other candidate responses. 2) Reflection: For each response $r_{j}$, we prompt $M$ with the concatenated input $\left[X ; r_{j}\right]$ to generate a self-critique $c_{j} \sim P_{M}\left(c_{j} \mid\left[X ; r_{j}\right], I_{\text {Reflection }}\right)$ with another instruction $I_{\text {Reflection }}$. 3) Revision: We concatenate the $K$ response-reflection pairs into a new input and prompt $M$ to generate an improved output. An illustration of this procedure is shown in Figure 1.</p>
<h2>3 Preliminary Study: Does Self-Reflection Prompting Work Under SR ${ }^{2} \mathrm{~V}$ ?</h2>
<p>We follow previous works (Bai et al., 2022; Shinn et al., 2023; Huang et al., 2023) in using two representative datasets, TruthfulQA and HotpotQA, to verify the effectiveness of self-reflection under $\mathrm{SR}^{2} \mathrm{~V}$. TruthfulQA is designed to evaluate the truthfulness of LMs' responses, while HotpotQA fo-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>cuses on multi-hop reasoning tasks, aimed at requiring complex reasoning capabilities.</p>
<p>Experiment Setup For these experiments we set $K=4$, and we prompt ChatGPT-3.5 ("gpt-3.5-turbo-16k-0613") with the questions from each dataset. ${ }^{3}$ Our full process for making these API calls is presented in Appendix F, and all prompt templates used can be found in Appendix E. We also extend our experiments to LLaMA-2 (Touvron et al., 2023) and Mixtral (Jiang et al., 2024), finding similar results to ChatGPT-3.5-we present results and discussion for LLaMA-2 and Mixtral in Appendix I and Appendix J.</p>
<p>For TruthfulQA we evaluate automatically (see details in Appendix D). For HotpotQA, we find that traditional exact match often unfairly assigns a score of 0 for semantically correct model responses; therefore, we manually assess 1,000 randomly chosen HotpotQA instances to check the model's answers against references.</p>
<p>To isolate the specific effect of the generated reflections, we also include an exploration-only baseline, in which we retain the Exploration stage but remove the Reflection component, and only concatenate the candidate model responses in the Revision prompt. ${ }^{4}$</p>
<p>Observations Results are shown in Table 1. In TruthfulQA, we see that using self-reflection achieves significantly better performance than either the exploration-only baseline or standard</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance Decomposition on Question Difficulty and Response Accuracy.</p>
<p>prompting. This finding is consistent with the observation of Bai et al. (2022) that LLMs' self-evaluation (in the form of reflection) can help to produce more factual outputs. However, we see that on HotpotQA, accuracy when using self-reflection is about 4% worse compared to both the exploration-only baseline and standard prompting. These results suggest that self-reflection may in fact harm performance in multi-hop reasoning tasks. This aligns with the self-reflection limitations found in Huang et al. (2023), and verifies that these limitations also extend to our more stringent evaluation setting, but presents a more complicated picture with the continued effectiveness of self-reflection on TruthfulQA under this setting.</p>
<h2>4 Why Self-Reflection May Not Work?</h2>
<p>To better understand these patterns, we conduct an error analysis drawing inspiration from the reflection conceptual model in psychology (Hommel et al., 2023). We hypothesize that two key factors influence self-reflection's efficacy: 1) the objective <strong>question difficulty</strong> (quantifiable based on human annotations), and 2) the <strong>model's comprehension quality</strong> (quantifiable based on the proportion of correct responses). Following this framework, we can predict that if a question is above average in human-annotated difficulty, self-reflection may be of greater benefit. Similarly, if the model already has a strong grasp of the question, it may not benefit as much from self-reflection.</p>
<p>To test these hypotheses, we break down model performance based on levels of question difficulty and model comprehension. We focus our analysis on HotpotQA, as this is the dataset on which we</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance Decomposition on Question Difficulty and Response Accuracy (Artificial Responses). Dotted lines show "turning points" at which reflection loses effectiveness, for Easy/Medium/Hard questions.</p>
<p>observe significant detrimental effects of applying self-reflection prompting. Additionally, this dataset contains annotated human judgments of question difficulty, and enables a clearly-defined notion of accuracy. We use these human difficulty annotations for our measure of question difficulty, and for model comprehension we use Response Accuracy (RA): the proportion of correct answers among the K candidate model responses sampled during Exploration.</p>
<p>The broken-down results are shown in Figure 2. The results show an interaction between our two variables. For questions judged by humans as Easy, self-reflection shows a benefit only when the model's candidate responses are mostly—but not all—incorrect, with self-reflection otherwise having negligible or negative effects on performance. For questions judged as Medium, there is a more even split: when most or all of the model's candidate responses are wrong, self-reflection is beneficial, but when half or more of the responses are correct, self-reflection is often harmful—with the notable exception of the 75% RA bin. A similar pattern is seen for questions judged as Hard, though for this category self-reflection is more consistently beneficial through the 75% RA bin, showing harm to performance only when all candidate model responses are already correct.</p>
<h2>5 Error Analysis via Artificial Response</h2>
<p>The above analysis suggests an interaction between difficulty and comprehension variables in effectiveness of self-reflection—however, our ability to disentangle these effects is limited by imbalanced distribution of model comprehension relative to question difficulty. To assess the interaction more thoroughly, we simulate model "mis-comprehension"</p>
<p>across a wider range of question difficulties, by sampling model responses to minimally edited versions of the prompts, and then pairing these responses with the original prompts when eliciting self-reflection. This allows us to increase the number of incorrect candidate responses, and thus to more evenly distribute RA levels across human difficulty levels. More details on this simulation process can be found in Appendix B.</p>
<p>For this experiment, we generate $\mathrm{K}=10$ candidate responses per question, with a mix of synthetic pairings and real pairings. ${ }^{5}$ Results are shown in Figure 3. We see that the benefits of self-reflection are now limited to the lowest RA levels, and there is also now a clearer shift from beneficial to harmful effects of self-reflection as RA increases. We also see that the interaction with question difficulty remains: the turning point from beneficial to harmful falls around 50\% RA for Hard questions, 30\% for Medium questions, and 20\% for Easy questions. Overall, this indicates that a major contributor to the effectiveness of self-reflection is the confidence of model accuracy on the question-if the model is reliably correct on initial responses, self-reflection tends to be harmful. However, this effect is further modulated by overall question difficulty: the benefits of self-reflection persist to higher levels of response accuracy if the questions are more difficult based on human judgment.</p>
<p>Though TruthfulQA is not as conducive to exact quantification of our variables, based on these results we can now speculate that the effectiveness of self-reflection on that dataset may be attributable to lower rate of good initial model responses, and potentially also higher overall question difficulty.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Majority Voting Analysis</p>
<h2>6 Effects on majority voting</h2>
<p>A natural question to ask at this point is to what extent the effect of RA is due to the model em-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Proposed guide for using Self-Reflection.
ploying majority voting on the candidate responses. In Figure 4 we plot the percentage of items in which the model's output is consistent with majority voting, at different RA levels (computed at $K=10$ including artificially generated responses), both with and without self-reflection. The plot shows that without self-reflection, the tendency to give answers consistent with majority voting is strong and closely correlated with the strength of the accuracy trend (i.e., more majority voting when most candidate responses are either correct or incorrect, and less majority voting when candidates are more mixed). However, with self-reflection the tendency to align with majority voting is significantly reduced across RA levels, suggesting that self-reflection does encourage more sophisticated decision strategies (even if in the case of higher RA levels, this in fact has a harmful effect on accuracy).</p>
<h2>7 Discussion</h2>
<p>Our analyses above have found that self-reflection benefits are limited to cases in which model accuracy is unreliable on initial responses, though benefits are more persistent for harder questions. Based on these findings, we propose a set of guidelines for determining when to implement self-reflection in practical applications, for a given request or prompt. The core principle involves basing decisions on estimated RA and question difficulty, and these guidelines can be applied by simply sampling responses for the target question or prompt. First, if external tools or certain access to ground truth answers are available such that RA can be reliably estimated, then self-reflection should be used when RA levels are low. Next, if difficulty annotations/subjective difficulty judgements are available, self-reflection can also be promising when RA levels are interme-</p>
<p>diate and question difficulty is high. If RA cannot be estimated, response consistency can be used as a proxy: if responses are highly consistent, selfreflection may be unlikely to provide benefit. If consistency is low, then self-reflection may be beneficial, especially for questions of higher difficulty. An illustration of these guidelines is in Figure 5.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we evaluate ChatGPT's self-reflective capabilities under a stringent single-round multiresponse evaluation setting. We find mixed results, and further analysis shows that the effectiveness of self-reflection is impacted both by question difficulty and by model response accuracy level: benefits of self-reflection are mostly limited to cases in which the model's initial responses are unreliable in accuracy, but with more persistent benefits for harder questions. Additionally, we find that self-reflection reduces the model's tendency for majority voting. We propose guidelines for when to use self-reflection, and we look forward to work further exploring impacts on self-reflection, and further refining these guidelines.</p>
<h2>Acknowledgements</h2>
<p>We are grateful for the insightful discussion with Xinyun Chen (Google) and Jie Huang (UIUC) at the early stage of this work (names are not listed in particular order). We also thank the anonymous NAACL reviewers and chairs for providing insightful and constructive feedback to make this work more solid.</p>
<h2>Limitations</h2>
<p>In this work, we adopt a stringent evaluation strategy to test the effectiveness of self-reflective abilities of LLMs. One limitation is that our experiments reported in the main text are based on a single snapshot of the ChatGPT model (gpt-3.5-turbo-16k-0613). We focus on ChatGPT because it is a state-of-the-art chat model, allowing us to make our results directly comparable with previous work-and we limit to this particular version of ChatGPT to ensure that results will not be affected by model updates. However, the assessment of selfreflection may vary between different versions of ChatGPT, as well as between ChatGPT and other LLMs. We do verify our experimental results on other open language models including LLaMA2 (Touvron et al., 2023) and Mixtral (Jiang et al.,
2024) in Appendix I and Appendix J, respectively. While we find that our conclusions can be extended to these models, due to budget limitations we leave more extensive evaluation over other popular proprietary and open models for future works.</p>
<p>Our experiments also use only two datasets for evaluating reflective ability. We chose these two datasets for a focused study covering two very different QA domains, but we look forward to future work further extending these types of analyses to a broader collection of datasets.</p>
<p>We conducted an artificial response experiment in Section 5 to simulate the real output distribution of the language model. This is a rough estimate of ChatGPT's actual output distribution. As we sampled ten fake responses from the language model, it is impossible to cover all possible cases of outputs, and there might be bias in the sample distribution. Future work could try generating a higher number of fake responses to obtain a more accurate distribution of the model.</p>
<p>Finally, although RA proves a valuable metric for determining the utility of self-reflection, its reliance on access to ground truth undermines its practical use. An initial attempt to use GPT-4 to produce an estimate of RA yielded unsatisfactory results (detailed in Appendix G). Further examination of this topic is reserved for future research.</p>
<h2>References</h2>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023a. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023b. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. 2023. The capacity for moral selfcorrection in large language models. arXiv preprint arXiv:2302.07459.</p>
<p>Mandy Hommel, Bärbel Fürstenau, and Regina H Mulder. 2023. Reflection at work-a conceptual model</p>
<p>and the meaning of its components in the domain of VET teachers. Frontiers in Psychology, 13:923888.</p>
<ul>
<li>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</li>
<li>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</li>
<li>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.</li>
<li>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491.</li>
<li>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.</li>
<li>Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods.</li>
<li>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</li>
<li>OpenAI. 2021. Chatgpt. https://openai.com/api/models/gpt-chat/.</li>
<li>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics.</li>
<li>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.</li>
<li>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</li>
<li>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models.</li>
<li>Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, and Heng Ji. 2023b. Enable language models to implicitly learn self-improvement from data. arXiv preprint arXiv:2310.00898.</li>
<li>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380.</li>
</ul>
<h2>Appendix A Accuracy Decomposition over 4 responses</h2>
<p>See Figure 6 for accuracy decomposition over 4 responses using ChatGPT.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy vs. Correctness Margin for each artificial response</p>
<h2>Appendix B Artificial Response Generation</h2>
<p>We do artificial response generation by prompting ChatGPT to edit the context used in HotpotQA. Specifically, the following steps were adopted: 1) For chosen questions, perform a simple perturbation on the context (e.g., entity replacement). An example is shown in Figure 7. 2) Manually inspect some samples to ensure minimal edits and answerability. 3) Prompt the model to regenerate responses and reflections based on the altered context. In this way, we are simulating scenarios where the model doesn't comprehend the context perfectly. ^{6}</p>
<p>Here is an example for how we modify the context:</p>
<p>^{6}While directly editing outputs to create correct or incorrect answers is an option, we avoid this to ensure the results reflect the model's natural response distribution.</p>
<p>Original question: What nationality was James Henry Miller's wife?</p>
<p>Original context: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was an English folk singer, songwriter, communist, labour activist, actor, poet, playwright and record producer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is an American folksinger. She is also well known in Britain, where she has lived for more than 30 years, and was married to the singer and songwriter Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 1: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was a Scottish folk singer, songwriter, capitalist, labour activist, actor, poet, playwright and record producer.. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is an American country singer. She is also well known in France, where she has lived for more than 30 years, and was married to the actor and playwright Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 2: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was an Australian folk singer, songwriter, conservative, labour activist, actor, poet, playwright and record producer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is a British pop singer. She is also well known in Germany, where she has lived for more than 30 years, and was married to the musician and producer Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 3: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was a Canadian folk singer, songwriter, anarchist, labour activist, actor, poet, playwright and record producer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is an American rapper. She is also well known in Spain, where she has lived for more than 30 years, and was married to the actor and politician Ewan MacColl until his death in 1989. ...</p>
<p>Fake context 4: ... Ewan MacColl: James Henry Miller (25 January 1915 - 22 October 1989), better known by his stage name Ewan MacColl, was an Irish folk singer, songwriter, monarchist, labour activist, actor, poet, playwright and record pro- ducer. Peggy Seeger: Margaret "Peggy" Seeger (born June 17, 1935) is a French jazz singer. She is also well known in Italy, where she has lived for more than 30 years, and was married to the artist and filmmaker Ewan MacColl until his death in 1989. ...</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Original Context: Alice is Irish; her best friend is Bob, Bob is British Question: What's the nationality of Alice's best friend?</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original Answer: British</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fake context (many versions):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alice is Irish; her ... is Bob, Bob is Chinese Alice is Irish; her ... is Bob, Bob is American Alice is Irish; her ... is Bob, Bob is Australian</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10 different versions w/ minimum edits</td>
</tr>
</tbody>
</table>
<p>Figure 7: Synthesized Artificial Contexts Example</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Standard Prompting</th>
<th style="text-align: center;">Exploration-Only</th>
<th style="text-align: center;">Self-Reflection</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TruthfulQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rouge-1</td>
<td style="text-align: center;">$57.5 \pm 1.1$</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">$66.8 \pm 1.9$</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">$80.2 \pm 0.4$</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">71.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Self-Reflection experiment results using iterative prompting. Bold-faced numbers at each row indicate the best-performing method under each metric.</p>
<h2>C Conditional Prompting Results</h2>
<p>We demonstrate the conditional prompting results in Table 2. Comparing the results in Table 1 and Table 2, we can see that there is no significant difference between these parallel prompting and conditional prompting. To avoid the implicit bias introduced by conditional prompting, as Huang et al. (2023) point out, we stick to parallel prompting to conduct our evaluation on self-reflective thinking capability.</p>
<h2>D Evaluation details for TruthfulQA</h2>
<p>We use the generation setting of TruthfulQA, which evaluates by comparing how closely the model's responses match a preferred reference versus an undesired one We follow (Lin et al., 2022) to use Rouge-1 (Lin, 2004) and BLEURT (Sellam et al., 2020) for similarity computation.</p>
<h2>E Prompts used in Experiment</h2>
<h2>E. 1 TruthfulQA: Standard Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages={
    {&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question}
}
</code></pre></div>

<h2>E. 2 TruthfulQA: Response Critique Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Could you critique
    your last response?&quot;\}
\}
</code></pre></div>

<h2>E. 3 TruthfulQA: Response Without Reflection</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_1\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_2\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_3\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\}
\}
</code></pre></div>

<h2>E. 4 TruthfulQA: Response With Reflection</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_1\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Please critique your
    responses&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: critique_1\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_2\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Please critique your
    responses&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: critique_2\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: response_3\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Please critique your
    responses&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: critique_3\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: question\}
\}
</code></pre></div>

<h2>E. 5 HotpotQA: Standard Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question
    based on the context provided.
    Provide extremely concise answers
    with no explanation.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Context: Earth: The
    Earth is the third planet from
    the Sun. Question: Which planet
    is Earth from the Sun? Answer:
    Third&quot; \},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Context:
    \{formatted_context\}\n
    Question: \{question\}\nProvide a
    short answer without
    explanation.&quot;\}
\}
</code></pre></div>

<h2>E. 6 HotpotQA: Response Critique Prompt</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question
    based on the context provided.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Context:
    \{formatted_context\}\n
    Question: \{question\}&quot; \},
    \{&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: f&quot;\{response\}&quot; \},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Please review and
    critique your previous response,
    and keep in mind not to add any
    unnecessary apologies. You can
    refer back to the original
    context if needed.&quot;\}
\}
</code></pre></div>

<h2>E. 7 HotpotQA: Response Without Reflection</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\{\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question
    based on the context provided.
    Provide extremely concise answers
    with no explanation.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;Context: Earth: The
    Earth is the third planet from
    the Sun. Question: Which planet
    is Earth from the Sun?
    Answer: Third&quot; \},
    \{&quot;role&quot;: &quot;user&quot;,
</code></pre></div>

<p>"content": f"Context: {formatted_context}\n
Question: {question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_1}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_2}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_3}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_4}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
$}$
E. 8 HotpotQA: Response With Reflection
messages $=$ [
${"$ role": "system",
"content": "You are a helpful assistant. Answer the question based on the context provided. Provide extremely concise answers with no explanation."},
${"$ role": "user",
"content": "Context: Earth: The Earth is the third planet from the Sun. Question: Which planet is Earth from the Sun? Answer: Third" },
${"$ role": "user",
"content": f"Context:
{formatted_context}\n
Question: {question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_1}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_1}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_2}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_2}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_3}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_3}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."},
${"$ role": "assistant",
"content": f"{response_4}"},
${"$ role": "user",
"content": f"Please review and critique your previous response, and keep in mind not to add any unnecessary apologies. You can refer back to the original context if needed."},
${"$ role": "assistant",
"content": f"{critique_4}"},
${"$ role": "user",
"content": f"{question}\n
Provide a short answer without explanation."}
]</p>
<h2>E. 9 HotpotQA: Fake Evidence Generation</h2>
<div class="codehilite"><pre><span></span><code>messages \(=\)
    \{&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Here is a question:
    \{question\}. Please create 10
    different versions of &#39;fake
    supporting facts&#39; based on the
    following real supporting facts.
    Modify only one sentence in each
    version, making sure the modified
    sentence is still relevant but
    contains false information. Keep
    the other sentences unmodified.
    Each version of fake supporting
    facts should have the same number
    of sentences as the real
    supporting facts.&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Real Supporting
    Facts:\{real_sf\}&quot;\},
    \{&quot;role&quot;: &quot;user&quot;,
</code></pre></div>

<p>"content": "Please generate the fake supporting facts versions. Remember to index all the sentences. You must generate 10 versions before you stop."), {"role": "user", "content": f"Fake Supporting Facts Version 1:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 2:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 3:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 4:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 5:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 6:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 7:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 8:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 9:\n [Insert manipulated sentences here]} $\hookrightarrow$ n Fake Supporting Facts Version 10:\n [Insert manipulated sentences here $\hookrightarrow$ ]"), ]</p>
<h2>F Illustration of API Calling Processes</h2>
<p>In this section, we provide a simple example to illustrate the API calling process under our $\mathrm{SR}^{2} \mathrm{~V}$, conditional prompting and the Exploration-Only Baseline.</p>
<h2>F. 1 SR $^{2} \mathrm{~V}$ API Calling Process</h2>
<p>(Splitters and other special tokens are $\hookrightarrow$ omitted)
<em>First API Call</em>:
[Instructions and Context]
Question: [question]
Response: (Sample response_1,
$\hookrightarrow$ response_2 here.)
<em>Second API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: $\qquad$ (Sample reflection_1
$\hookrightarrow$ here.)
<em>Third API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_2
[Instruction for Reflection]
Reflection: $\qquad$ (Sample reflection_2
$\hookrightarrow$ here.)
<em>Final API Call (to get the final
$\hookrightarrow$ revised answer)</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: reflection_1
Question: [question]
Response: response_2
[Instruction for Reflection]
Reflection: reflection_2
Question: [question]
Response: $\qquad$ (Sample final_response $\hookrightarrow$ here)</p>
<h2>F. 2 Conditional Prompting Baseline API Calling Process</h2>
<p><em>First API Call</em>:
[Instructions and Context]
Question: [question]
Response: $\qquad$ (sample response_1 here)
<em>Second API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: $\qquad$ (sample reflection_1
$\hookrightarrow$ here)
<em>Third API Call</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: reflection_1
Question: [question]
Response: $\qquad$ (sample response_2 here)
・.
<em>Final API Call (to get the final
$\hookrightarrow$ revised answer)</em>:
[Instructions and Context]
Question: [question]
Response: response_1
[Instruction for Reflection]
Reflection: reflection_1
Question: [question]
Response: response_2
[Instruction for Reflection]
Reflection: reflection_2
Question: [question]
Response: final_reponse</p>
<h2>F. 3 Exploration-Only Baseline API Calling Process</h2>
<p><em>First API Call</em>:</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Instructions and Context]</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">(sample response_1,</span>
<span class="w">    </span><span class="na">~s response_2 here)</span>
<span class="na">*Final API Call*</span><span class="o">:</span>

<span class="k">[Instructions and Context]</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">response_1</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">response_2</span>
<span class="na">Question</span><span class="o">:</span><span class="w"> </span><span class="s">[question]</span>
<span class="na">Response</span><span class="o">:</span><span class="w"> </span><span class="s">(sample final_response</span>
<span class="w">    </span><span class="na">~s here)</span>
</code></pre></div>

<h2>G Challenges in Predicting the Correctness Margin for Model Comprehension</h2>
<p>The effectiveness of a model's self-reflection largely hinges on its "correctness margin," a metric quantifying its understanding of both the question and its context. Ideally, we would like to predict this margin through user prompts, thereby allowing the user to make an informed decision on whether to enable the model's self-reflection capability.</p>
<p>Nevertheless, our experiments indicate that current models struggle to self-assess their understanding reliably. Below, we outline our prompt design used for this experiment:</p>
<div class="codehilite"><pre><span></span><code>messages=[
    {&quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: &quot;You are a helpful
    assistant. Answer the question based
    on the context provided. Provide
    extremely concise answers with no
    explanation.&quot;},
    {&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: f&quot;Context:
    {formatted_context}\n
    Question: {question}&quot;},
    {&quot;role&quot;: &quot;assistant&quot;,
    &quot;content&quot;: f&quot;{response}&quot;},
    {&quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;\nYou have just answered
    a question. Now, please evaluate
        ~s your
    own comprehension of the question
        ~s and
    answer provided. Rate your level of
    understanding on a scale from -5 to
        ~s 5.
    A rating of 5 signifies extreme
    certainty that you understand the
    question, while a rating of -5
    indicates extreme uncertainty or
        ~s lack
    of understanding.&quot;},
]
</code></pre></div>

<p>We tested this prompt structure on two sets of questions: one where all 10 model responses were incorrect, and another where all 10 were correct. If the model were capable of accurately evaluating its own comprehension, it should consistently rate its understanding at -5 for questions in the all-wrong dataset and 5 for those in the all-right dataset. However, after experimenting with 20 examples from each dataset, we found that the model consistently assigned high scores (typically 4 or 5 ) regardless of the dataset origin. Thus, reliable self-assessment remains an open challenge for current models.</p>
<h2>H Scientific Artifacts</h2>
<p>In this paper, we use the following artifacts:</p>
<ul>
<li>TruthfulQA (Lin et al., 2022) is a benchmark assessing a language model's ability to generate truthful answers for 817 diverse questions in 38 categories, requiring models to avoid false answers commonly found in human texts due to misconceptions or false beliefs. We use it for the preliminary studies on reflective thinking in LLMs. It is licensed under the Apache License, Version 2.0.</li>
<li>HotpotQA (Yang et al., 2018) is a 113k question-answer dataset based on Wikipedia that requires multi-document reasoning, features diverse questions unconstrained by knowledge bases or schemas, provides sentence-level supporting facts for strong supervision and explanation, and introduces a new factoid comparison question type to evaluate QA systems' extraction and comparison abilities. We use it for evaluating reflective thinking in LLMs. It is distributed under a CC BY-SA 4.0 License.</li>
<li>openai-python ${ }^{7}$ (v0.27.8) provides convenient access to the OpenAI REST API from any Python 3.7+ application. We use it to access ChatGPT models. It is licensed under the Apache License, Version 2.0.</li>
</ul>
<h2>I Results on LLaMA-2-7b-chat</h2>
<p>We extend our experiments to the open-sourced model LLaMA-2-7b-chat (Touvron et al., 2023), and the results support the conclusions that we draw from our experiments on ChatGPT, indicating that our findings can be generalized to different models.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Standard</th>
<th>Exploration-</th>
<th>Self-</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Prompting</td>
<td>Only</td>
<td>Reflection</td>
</tr>
<tr>
<td>TruthfulQA</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rouge-1</td>
<td>$53.8 \pm 0.4$</td>
<td>51.7</td>
<td>53.8</td>
</tr>
<tr>
<td>BLEURT</td>
<td>$60.9 \pm 0.6$</td>
<td>58.2</td>
<td>63.0</td>
</tr>
<tr>
<td>HotpotQA</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Accuracy*</td>
<td>$61.0 \pm 1.0$</td>
<td>62.9</td>
<td>57.5</td>
</tr>
</tbody>
</table>
<p>Table 3: Self-reflection SR ${ }^{2} \mathrm{~V}$ experiment results on QA datasets using LLaMA-2-chat. Bold-facing indicates the best-performing method under each metric. *Evaluated manually.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Performance Decomposition on Question Difficulty and Response Accuracy (LLaMA-2-chat).</p>
<p>More specifically, for the preliminary study on performance across TruthfulQA and HotpotQA, the results on LLaMA-2-7b-chat (see Table 3) are consistent with the results obtained from ChatGPT (see Table 1): self-reflection prompts improve performance on TruthfulQA while worsening performance on HotpotQA. Additionally, we conduct our error analysis on the results of HotpotQA, breaking down model performance based on levels of question difficulty and model comprehension. The LLaMA-2-7b-chat results for this analysis (Figure 8) also closely follow the trend observed in the results from ChatGPT (Figure 2): self-reflection is more beneficial when the model's initial responses are incorrect and when the question difficulty is higher.</p>
<p>We do not replicate the 10-response artificial experiments on LLaMA-2-7b-chat due to the context length limit. The context length for LLaMA-2 is 4096, which is shorter than our context length for the task. We replicate this experiment in Appendix J as Mixtral models have larger token limits.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Standard <br> Prompting</th>
<th style="text-align: center;">Exploration- <br> Only</th>
<th style="text-align: center;">Self- <br> Reflection</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TruthfulQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rouge-1</td>
<td style="text-align: center;">$59.1 \pm 1.0$</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">$71.5 \pm 0.4$</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Accuracy*</td>
<td style="text-align: center;">$89.8 \pm 0.3$</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">89.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Self-reflection $\mathrm{SR}^{2} \mathrm{~V}$ experiment results on QA datasets using Mixtral-8x7B-v0.1. Bold-facing indicates the best-performing method under each metric. *Evaluated manually.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Performance Decomposition on Question Difficulty and Response Accuracy (Mixtral-8x7B-v0.1).</p>
<h2>J Results on Mixtral-8x7B-v0.1</h2>
<p>We repeat our experiments on Mixtral-8x7B-v0.1. For the preliminary study on performance across TruthfulQA and HotpotQA (Table 4, we again observe a similar trend to ChatGPT: while selfreflection may help improve the performance on TruthfulQA, it harms the performance on HotpotQA. Then, we again break down model performance on HotpotQA based on question difficulty levels and model comprehension in Figure 9. Here we observe a somewhat different pattern: under all question difficulty levels and model comprehension, self-reflection prompting fails to improve the performance. To further verify this finding, we also conduct the artificial response experiments, with results in Figure 10. Here we see that self-reflection prompting is not always harmful to performance (e.g., under $0 \%$ RA, self-reflection helps improve the performance of easy questions.), but in most cases it is harmful.</p>
<p>We hypothesize that these divergent patterns arise because this particular model may be less sen-</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Performance Decomposition on Question Difficulty and Response Accuracy (Artificial Responses) for Mixtral-8x7B-v0.1.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Majority Voting Analysis (Mixtral-8x7Bv0.1).
sitive in general to instructions for reflection, or less well-equipped to understand them, such that the reflection part serves mostly as a distractor in the input. We examine this hypothesis in the majority voting experiments (Figure 11) and find that compared with ChatGPT, the addition of self-reflection exerts minimal impact on majority voting trends, suggesting that it is comparatively difficult to use self-reflection prompting to change the default behaviors in the case of this model. This is consistent with our hypothesis that Mixtral-8x7B-v0.1 lacks sensitivity or competence in self-reflection, so we speculate that additional training may be needed to help this model to unlock self-reflection prompting potential.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/openai/openai-python&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ The 16 k variant is chosen to accommodate responses and reflection pairs that exceed the standard 4096 token limit, particularly in detailed experiments of Section 5.
${ }^{4}$ The exploration-only baseline can be viewed as one implementation of (universal) self-consistency prompting (Wang et al., 2023a; Chen et al., 2023a). Rather than applying majority voting directly to the outputs, this method involves inputting these outputs back into the model for aggregation. As we'll explore in Section 6, we also find the model predominantly engages in a form of majority voting in this process.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6738 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6738</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6738</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-856fe866bcce5e7a540655bea6ecc7406bdcfcba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/856fe866bcce5e7a540655bea6ecc7406bdcfcba" target="_blank">Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences, and tests the zero-shot generalization capabilities of a variety of recurrent neural networks trained on SCAN with sequence-to-sequence methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6738.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6738.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCAN seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence recurrent networks on the SCAN benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-decoder recurrent neural network models (SRN, LSTM, GRU), with and without attention, are trained to translate simplified natural-language navigation commands (SCAN) into action sequences to probe compositional generalization and systematicity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>seq2seq encoder-decoder RNN (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>sequence-to-sequence translation / compositional decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate simplified navigation commands into action sequences (tests zero-shot compositional generalization and systematicity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact match of output action sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>99.7</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Seq2seq RNNs can achieve very high zero-shot accuracy on SCAN when test commands are compositionally similar to training items (mix-and-match style generalization), but fail when generalization requires systematic algebraic compositionality (e.g., applying modifiers to a verb seen only in isolation).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6738.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-best-Exp1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2-layer LSTM (200 units/layer, no attention) evaluated on SCAN random-split</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Top-performing LSTM encoder-decoder architecture on the random 80/20 SCAN split, showing near-perfect recompositional generalization when test items are similar in distribution to training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (2-layer, 200 hidden units, no attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>seq2seq with LSTM encoder-decoder</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN (random 80/20 split)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot generalization to novel compositions built from familiar parts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>99.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Simple RNN (SRN)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>98.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>LSTM gated recurrence enabled near-perfect recombinational generalization on held-out commands that reuse familiar subparts; classic SRNs performed extremely poorly on the same split, indicating architecture-dependent inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6738.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SRN-att-Exp1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple Recurrent Network (SRN) with Bahdanau attention on SCAN random-split</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vanilla SRN augmented with attention substantially improved learning on SCAN compared to SRN without attention but still underperformed gated architectures on average.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Simple RNN (SRN) with attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>seq2seq with SRN encoder-decoder + attention</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN (random 80/20 split)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot generalization to novel compositions built from familiar parts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (mean over runs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>59.7</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SRN without attention</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>58.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Adding attention to SRNs improved test accuracy dramatically compared to SRNs without attention (which achieved <1.5%), suggesting attention can partially compensate for weak recurrence, but gated RNNs (LSTM/GRU) were still more reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6738.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU-top-Exp2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRU with attention (1 layer, 50 hidden units) evaluated on SCAN long-sequence split</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Best-performing model for generalizing to longer-than-trained output action sequences; still has very low absolute performance, indicating difficulty extrapolating output length and systematic composition for longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU with attention (1-layer, 50 hidden units, dropout=0.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>seq2seq with GRU encoder-decoder + attention</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN (train up to 22-action sequences; test: 24–48 actions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot generalization to commands demanding longer action sequences than ever seen during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (mean over runs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>20.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>overall-best model (different architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>7.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Models failed to generalize to substantially longer output sequences; partial success concentrated on the shortest unseen lengths, indicating reliance on similarity to training distribution rather than productive systematic rules. Providing an oracle target length improved but did not eliminate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6738.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU-turnleft-Exp3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRU with attention (1 layer, 100 units) generalizing 'turn left' compositions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GRU+attention model generalized composed commands containing 'turn left' (which was seen only as a primitive during training) to most composed contexts, achieving high accuracy despite the primitive being presented in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU with attention (1-layer, 100 hidden units, dropout=0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>bootstrapped compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN (primitive-only exposure for 'turn left', compositional test)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply modifiers and conjunctions to a primitive ('turn left') seen only in isolation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>90.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>overall-best model on same split</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>High accuracy suggests that if the action (LTURN) appears in many contexts during training (via other sentences), the model can generalize application of modifiers to the primitive; however, the pattern of the few systematic errors indicates non-human-like, counter-intuitive generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6738.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-jump-fail-Exp3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM with attention (1 layer, 100 units) failing to generalize 'jump' compositions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LSTM-based seq2seq models failed to generalize when a primitive ('jump') was seen only in isolation at training time; near-zero accuracy shows inability to apply previously learned compositional operators to a novel filler seen sparsely.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM with attention (1-layer, 100 hidden units, dropout=0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>bootstrapped compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN (primitive-only exposure for 'jump', compositional test)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply modifiers and conjunctions to a primitive ('jump') seen only in isolation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>1.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>GRU-turnleft (same experimental paradigm but different primitive)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-89.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The model could not generalize modifier application to 'jump' when that primitive's action (JUMP) was not observed compositional contexts in training; representation analysis shows 'jump' token remained isolated in embedding/hidden-state space, preventing transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6738.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>jump-shot-varying</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of adding varying numbers of composed 'jump' examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retraining with a small number of composed examples for the previously-isolated primitive ('jump') shows a graded improvement in generalization: weak at 8 examples, strong by 16–32 examples, indicating gradual statistical learning instead of immediate rule-like generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM/overall-best retrained with composed 'jump' examples</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>few-shot inclusion of composed exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / data-driven</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCAN (controlled additions of composed 'jump' training items)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how many composed examples of a primitive are needed for generalization to unseen composed forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on held-out composed 'jump' commands</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>38.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>baseline 'jump' primitive-only training (0.08% overall-best baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>38.22</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>With 8 composed 'jump' training items the model reached 38.3% correctness; with 16 and 32 items performance rose to 77.8% and 88.4% respectively, showing gradual statistical learning rather than immediate systematic rule-application.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6738.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT-daxy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Small-scale English→French MT 'daxy' compositionality probe</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proof-of-concept machine translation test where a new adjective 'daxy' is introduced many times in one sentence form, and the model is probed on its ability to translate novel constructions with that new word.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM with attention (2-layer, 400 hidden units, dropout=0.05)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>seq2seq neural machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>small, controlled English-French dataset (short sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate sentences embedding a newly-seen adjective 'daxy' into French across 8 constructions not seen with 'daxy' during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU on original test set; accuracy on 8 'daxy' constructions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>12.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>adjective 'tired' baseline (8/8 correct, 100% on same 8 constructions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-87.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Despite seeing 'I am daxy' many times in training, the model could translate only 1 of 8 novel constructions with 'daxy' (12.5%), whereas a frequent adjective 'tired' achieved 8/8; this mirrors SCAN failures and suggests lack of human-like systematic compositionality in MT models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6738.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-Bahdanau</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bahdanau-style soft attention mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention mechanism that computes a learned alignment between decoder state and all encoder hidden states to produce a context vector used by the decoder at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation by jointly learning to align and translate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bahdanau attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>soft attentional alignment</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / alignment-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>used across SCAN experiments as an architectural variant</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve decoder access to encoder hidden states to aid sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>varied (used as architectural component)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>no-attention variants</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Attention sometimes improved weak architectures (SRN) substantially and marginally helped GRUs to generalize to longer sequences, but attention did not solve systematic compositionality failures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6738.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6738.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory/modular proposals</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-augmented and modular neural architectures (proposed remedies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors mention several potential architectural remedies — differentiable memory structures, neural program induction, modular networks, and meta-learning — as plausible ways to encourage abstract rule extraction and systematic compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>memory-augmented networks / modular program-induction models / meta-learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>memory-augmentation, modular function composition, learning-to-learn</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented / modular / meta-learning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (proposed ensemble of approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Architectural and meta-learning approaches intended to enable extraction of systematic rules from data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors hypothesize these structured or meta-learning approaches could encourage abstraction into rule-like representations and mitigate the sample-inefficiency and lack of systematicity seen in vanilla seq2seq RNNs; no experiments in this paper test them.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 2)</em></li>
                <li>Sequence to sequence learning with neural networks <em>(Rating: 2)</em></li>
                <li>Long short-term memory <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Model-agnostic meta-learning for fast adaptation of deep networks <em>(Rating: 2)</em></li>
                <li>Neural programmer-interpreters <em>(Rating: 2)</em></li>
                <li>Convolutional sequence to sequence learning <em>(Rating: 1)</em></li>
                <li>Google's neural machine translation system: Bridging the gap between human and machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6738",
    "paper_id": "paper-856fe866bcce5e7a540655bea6ecc7406bdcfcba",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "SCAN seq2seq",
            "name_full": "Sequence-to-sequence recurrent networks on the SCAN benchmark",
            "brief_description": "Encoder-decoder recurrent neural network models (SRN, LSTM, GRU), with and without attention, are trained to translate simplified natural-language navigation commands (SCAN) into action sequences to probe compositional generalization and systematicity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "seq2seq encoder-decoder RNN (general)",
            "model_size": null,
            "reasoning_method_name": "sequence-to-sequence translation / compositional decoding",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "SCAN",
            "task_description": "Translate simplified navigation commands into action sequences (tests zero-shot compositional generalization and systematicity).",
            "performance_metric": "accuracy (exact match of output action sequence)",
            "performance_value": 99.7,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Seq2seq RNNs can achieve very high zero-shot accuracy on SCAN when test commands are compositionally similar to training items (mix-and-match style generalization), but fail when generalization requires systematic algebraic compositionality (e.g., applying modifiers to a verb seen only in isolation).",
            "ablation_study_present": false,
            "uuid": "e6738.0",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "LSTM-best-Exp1",
            "name_full": "2-layer LSTM (200 units/layer, no attention) evaluated on SCAN random-split",
            "brief_description": "Top-performing LSTM encoder-decoder architecture on the random 80/20 SCAN split, showing near-perfect recompositional generalization when test items are similar in distribution to training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM (2-layer, 200 hidden units, no attention)",
            "model_size": null,
            "reasoning_method_name": "seq2seq with LSTM encoder-decoder",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "SCAN (random 80/20 split)",
            "task_description": "Zero-shot generalization to novel compositions built from familiar parts.",
            "performance_metric": "accuracy",
            "performance_value": 99.8,
            "comparison_target_method": "Simple RNN (SRN)",
            "performance_difference": 98.3,
            "statistical_significance": null,
            "analysis_notes": "LSTM gated recurrence enabled near-perfect recombinational generalization on held-out commands that reuse familiar subparts; classic SRNs performed extremely poorly on the same split, indicating architecture-dependent inductive biases.",
            "ablation_study_present": false,
            "uuid": "e6738.1",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "SRN-att-Exp1",
            "name_full": "Simple Recurrent Network (SRN) with Bahdanau attention on SCAN random-split",
            "brief_description": "A vanilla SRN augmented with attention substantially improved learning on SCAN compared to SRN without attention but still underperformed gated architectures on average.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Simple RNN (SRN) with attention",
            "model_size": null,
            "reasoning_method_name": "seq2seq with SRN encoder-decoder + attention",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "SCAN (random 80/20 split)",
            "task_description": "Zero-shot generalization to novel compositions built from familiar parts.",
            "performance_metric": "accuracy (mean over runs)",
            "performance_value": 59.7,
            "comparison_target_method": "SRN without attention",
            "performance_difference": 58.2,
            "statistical_significance": null,
            "analysis_notes": "Adding attention to SRNs improved test accuracy dramatically compared to SRNs without attention (which achieved &lt;1.5%), suggesting attention can partially compensate for weak recurrence, but gated RNNs (LSTM/GRU) were still more reliable.",
            "ablation_study_present": false,
            "uuid": "e6738.2",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "GRU-top-Exp2",
            "name_full": "GRU with attention (1 layer, 50 hidden units) evaluated on SCAN long-sequence split",
            "brief_description": "Best-performing model for generalizing to longer-than-trained output action sequences; still has very low absolute performance, indicating difficulty extrapolating output length and systematic composition for longer sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GRU with attention (1-layer, 50 hidden units, dropout=0.5)",
            "model_size": null,
            "reasoning_method_name": "seq2seq with GRU encoder-decoder + attention",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "SCAN (train up to 22-action sequences; test: 24–48 actions)",
            "task_description": "Zero-shot generalization to commands demanding longer action sequences than ever seen during training.",
            "performance_metric": "accuracy (mean over runs)",
            "performance_value": 20.8,
            "comparison_target_method": "overall-best model (different architecture)",
            "performance_difference": 7.0,
            "statistical_significance": null,
            "analysis_notes": "Models failed to generalize to substantially longer output sequences; partial success concentrated on the shortest unseen lengths, indicating reliance on similarity to training distribution rather than productive systematic rules. Providing an oracle target length improved but did not eliminate failures.",
            "ablation_study_present": false,
            "uuid": "e6738.3",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "GRU-turnleft-Exp3",
            "name_full": "GRU with attention (1 layer, 100 units) generalizing 'turn left' compositions",
            "brief_description": "A GRU+attention model generalized composed commands containing 'turn left' (which was seen only as a primitive during training) to most composed contexts, achieving high accuracy despite the primitive being presented in isolation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GRU with attention (1-layer, 100 hidden units, dropout=0.1)",
            "model_size": null,
            "reasoning_method_name": "bootstrapped compositional generalization",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "SCAN (primitive-only exposure for 'turn left', compositional test)",
            "task_description": "Apply modifiers and conjunctions to a primitive ('turn left') seen only in isolation during training.",
            "performance_metric": "accuracy",
            "performance_value": 90.3,
            "comparison_target_method": "overall-best model on same split",
            "performance_difference": 0.3,
            "statistical_significance": null,
            "analysis_notes": "High accuracy suggests that if the action (LTURN) appears in many contexts during training (via other sentences), the model can generalize application of modifiers to the primitive; however, the pattern of the few systematic errors indicates non-human-like, counter-intuitive generalization.",
            "ablation_study_present": false,
            "uuid": "e6738.4",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "LSTM-jump-fail-Exp3",
            "name_full": "LSTM with attention (1 layer, 100 units) failing to generalize 'jump' compositions",
            "brief_description": "LSTM-based seq2seq models failed to generalize when a primitive ('jump') was seen only in isolation at training time; near-zero accuracy shows inability to apply previously learned compositional operators to a novel filler seen sparsely.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM with attention (1-layer, 100 hidden units, dropout=0.1)",
            "model_size": null,
            "reasoning_method_name": "bootstrapped compositional generalization",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "SCAN (primitive-only exposure for 'jump', compositional test)",
            "task_description": "Apply modifiers and conjunctions to a primitive ('jump') seen only in isolation during training.",
            "performance_metric": "accuracy",
            "performance_value": 1.2,
            "comparison_target_method": "GRU-turnleft (same experimental paradigm but different primitive)",
            "performance_difference": -89.1,
            "statistical_significance": null,
            "analysis_notes": "The model could not generalize modifier application to 'jump' when that primitive's action (JUMP) was not observed compositional contexts in training; representation analysis shows 'jump' token remained isolated in embedding/hidden-state space, preventing transfer.",
            "ablation_study_present": false,
            "uuid": "e6738.5",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "jump-shot-varying",
            "name_full": "Effect of adding varying numbers of composed 'jump' examples",
            "brief_description": "Retraining with a small number of composed examples for the previously-isolated primitive ('jump') shows a graded improvement in generalization: weak at 8 examples, strong by 16–32 examples, indicating gradual statistical learning instead of immediate rule-like generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM/overall-best retrained with composed 'jump' examples",
            "model_size": null,
            "reasoning_method_name": "few-shot inclusion of composed exemplars",
            "reasoning_method_type": "sequential / data-driven",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "SCAN (controlled additions of composed 'jump' training items)",
            "task_description": "Measure how many composed examples of a primitive are needed for generalization to unseen composed forms.",
            "performance_metric": "accuracy on held-out composed 'jump' commands",
            "performance_value": 38.3,
            "comparison_target_method": "baseline 'jump' primitive-only training (0.08% overall-best baseline)",
            "performance_difference": 38.22,
            "statistical_significance": null,
            "analysis_notes": "With 8 composed 'jump' training items the model reached 38.3% correctness; with 16 and 32 items performance rose to 77.8% and 88.4% respectively, showing gradual statistical learning rather than immediate systematic rule-application.",
            "ablation_study_present": false,
            "uuid": "e6738.6",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "MT-daxy",
            "name_full": "Small-scale English→French MT 'daxy' compositionality probe",
            "brief_description": "A proof-of-concept machine translation test where a new adjective 'daxy' is introduced many times in one sentence form, and the model is probed on its ability to translate novel constructions with that new word.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM with attention (2-layer, 400 hidden units, dropout=0.05)",
            "model_size": null,
            "reasoning_method_name": "seq2seq neural machine translation",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "small, controlled English-French dataset (short sentences)",
            "task_description": "Translate sentences embedding a newly-seen adjective 'daxy' into French across 8 constructions not seen with 'daxy' during training.",
            "performance_metric": "BLEU on original test set; accuracy on 8 'daxy' constructions",
            "performance_value": 12.5,
            "comparison_target_method": "adjective 'tired' baseline (8/8 correct, 100% on same 8 constructions)",
            "performance_difference": -87.5,
            "statistical_significance": null,
            "analysis_notes": "Despite seeing 'I am daxy' many times in training, the model could translate only 1 of 8 novel constructions with 'daxy' (12.5%), whereas a frequent adjective 'tired' achieved 8/8; this mirrors SCAN failures and suggests lack of human-like systematic compositionality in MT models.",
            "ablation_study_present": false,
            "uuid": "e6738.7",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "Attention-Bahdanau",
            "name_full": "Bahdanau-style soft attention mechanism",
            "brief_description": "An attention mechanism that computes a learned alignment between decoder state and all encoder hidden states to produce a context vector used by the decoder at each step.",
            "citation_title": "Neural machine translation by jointly learning to align and translate",
            "mention_or_use": "use",
            "model_name": "Bahdanau attention",
            "model_size": null,
            "reasoning_method_name": "soft attentional alignment",
            "reasoning_method_type": "sequential / alignment-based",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "used across SCAN experiments as an architectural variant",
            "task_description": "Improve decoder access to encoder hidden states to aid sequence generation.",
            "performance_metric": "varied (used as architectural component)",
            "performance_value": null,
            "comparison_target_method": "no-attention variants",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Attention sometimes improved weak architectures (SRN) substantially and marginally helped GRUs to generalize to longer sequences, but attention did not solve systematic compositionality failures.",
            "ablation_study_present": false,
            "uuid": "e6738.8",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "Memory/modular proposals",
            "name_full": "Memory-augmented and modular neural architectures (proposed remedies)",
            "brief_description": "The authors mention several potential architectural remedies — differentiable memory structures, neural program induction, modular networks, and meta-learning — as plausible ways to encourage abstract rule extraction and systematic compositionality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "memory-augmented networks / modular program-induction models / meta-learning",
            "model_size": null,
            "reasoning_method_name": "memory-augmentation, modular function composition, learning-to-learn",
            "reasoning_method_type": "tool-augmented / modular / meta-learning",
            "reasoning_style_diversity": "diverse (proposed ensemble of approaches)",
            "benchmark_name": null,
            "task_description": "Architectural and meta-learning approaches intended to enable extraction of systematic rules from data.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors hypothesize these structured or meta-learning approaches could encourage abstraction into rule-like representations and mitigate the sample-inefficiency and lack of systematicity seen in vanilla seq2seq RNNs; no experiments in this paper test them.",
            "ablation_study_present": null,
            "uuid": "e6738.9",
            "source_info": {
                "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "publication_date_yy_mm": "2017-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 2
        },
        {
            "paper_title": "Sequence to sequence learning with neural networks",
            "rating": 2
        },
        {
            "paper_title": "Long short-term memory",
            "rating": 2
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2
        },
        {
            "paper_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "rating": 2
        },
        {
            "paper_title": "Neural programmer-interpreters",
            "rating": 2
        },
        {
            "paper_title": "Convolutional sequence to sequence learning",
            "rating": 1
        },
        {
            "paper_title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
            "rating": 1
        }
    ],
    "cost": 0.017166749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks</h1>
<p>Brenden Lake ${ }^{12}$ Marco Baroni ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.</p>
<h2>1. Introduction</h2>
<p>Human language and thought are characterized by systematic compositionality, the algebraic capacity to understand and produce a potentially infinite number of novel combinations from known components (Chomsky, 1957; Montague, 1970). For example, if a person knows the meaning and usage of words such as "twice," "and," and "again," once she learns a new verb such as "to dax" she can immediately understand or produce instructions such as "dax twice and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>then dax again." This type of compositionality is central to the human ability to make strong generalizations from very limited data (Lake et al., 2017). In a set of influential and controversial papers, Jerry Fodor and other researchers have argued that neural networks are not plausible models of the mind because they are associative devices that cannot capture systematic compositionality (Fodor \&amp; Pylyshyn, 1988; Marcus, 1998; Fodor \&amp; Lepore, 2002; Marcus, 2003; Calvo \&amp; Symons, 2014, a.o.).</p>
<p>In the last few years, neural network research has made astounding progress in practical domains where success depends on generalization. Perhaps most strikingly, end-toend recurrent neural networks currently dominate the state-of-the-art in machine translation (Bojar et al., 2016; Wu et al., 2016). Since the overwhelming majority of sentences or even word sequences in a language only occur once, even in a large corpus (Baroni, 2009), this points to strong generalization abilities. Still, it is commonly observed that neural networks are extremely sample inefficient, requiring very large training sets, which suggests they may lack the same algebraic compositionality that humans exploit, and they might only be sensitive to broad patterns over lots of accumulated statistics (Lake et al., 2017).</p>
<p>In this paper, we introduce a grounded navigation environment where the learner must translate commands given in a limited form of natural language into a sequence of actions. This problem is naturally framed as a sequence-to-sequence task, and, due to its simplicity, it is ideal to study systematic generalization to novel examples in a controlled setup. We thus use it to test a wide range of modern recurrent network architectures in terms of their compositional abilities. Our results suggest that standard recurrent seq2seq architectures generalize very well when novel examples feature a mixture of constructions that have been observed in training. However, the models are catastrophically affected by systematic differences between training and test sentences, of the sort that would be trivial for an agent equipped with an "algebraic mind" (Marcus, 2003).</p>
<h2>2. The SCAN tasks</h2>
<p>We call our data set SCAN because it is a Simplified version of the CommAI Navigation tasks (Mikolov et al., 2016). ${ }^{1}$ For a learner, the goal is to translate commands presented in simplified natural language into a sequence of actions. Since each command is unambiguously associated to a single action sequence, SCAN (unlike the original CommAI tasks) can be straightforwardly treated as a supervised sequence-to-sequence semantic parsing task (Dong \&amp; Lapata, 2016; Jia \&amp; Liang, 2016; Herzig \&amp; Berant, 2017), where the input vocabulary is given by the set of words used in the commands, and the output by the set of actions available to the learner.</p>
<p>Several examples from SCAN are presented in Fig. 1. Formally, SCAN consists of all the commands generated by a phrase-structure grammar (presented in Supplementary) and the corresponding sequence of actions, produced according to a semantic interpretation function (see Supplementary). Intuitively, the SCAN grammar licenses commands denoting primitive actions such as JUMP (denoted by "jump"; Fig. 1), WALK (denoted by "walk") and LTURN (denoted by "turn left"). We will refer to these as primitive commands. ${ }^{2}$ It also accepts a set of modifiers and conjunctions that compositionally build expressions referring to action sequences. The "left" and "right" modifiers take commands denoting undirected primitive actions as input and return commands denoting their directed counterparts ("jump left"; Fig. 1). The "opposite" modifier produces an action sequence that turns the agent backward in the specified direction before executing a target action ("jump opposite left"), while "around" makes the agent execute the action at each step while turning around in the specified direction ("jump around right"; Fig. 1). The "twice/thrice" modifiers trigger repetition of the command they take scope over, and "and/after" combine two action sequences. Although the SCAN examples in Fig. 1 focus on the "jump"/JUMP primitive, each instance of JUMP can be replaced with either WALK, RUN, or LOOK to generate yet more commands. Many more combinations are possible as licensed by the grammar. The input vocabulary includes 13 words, the output 6 actions.</p>
<p>The SCAN grammar, lacking recursion, generates a finite but large set of unambiguous commands (20,910, to be precise). Commands can be decoded compositionally by applying the corresponding interpretation function. This means that, if it discovers the right interpretation function,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>a learner can understand commands it has not seen during training. For example, the learner might have only observed the primitive "jump" command during training, but if it has learned the meaning of "after", "twice" and "around left" from other verbs, it should be able to decode, zero-shot, the complex command: "jump around left after jump twice".</p>
<h2>3. Models and setup</h2>
<p>We approach SCAN through the successful sequence-tosequence (seq2seq) framework, in which two recurrent networks work together to learn a mapping between input sequences and output sequences (e.g., Sutskever et al., 2014). ${ }^{3}$ Fig. 2 illustrates the application of the seq2seq approach to a SCAN example. First, a recurrent network encoder receives the input sequence word-by-word, forming a lowdimensional representation of the entire command. Second, the low-dimensional representation is passed to a recurrent network decoder, which then generates the output sequence action-by-action. The decoder's output is compared with the ground truth, and the backpropagation algorithm is used to update the parameters of both the encoder and decoder. Note that although the encoder and decoder share the same network structure (e.g., number of layers and hidden units), they do not otherwise share weights/parameters with each other. More details regarding the encoder-decoder RNN are provided in Supplementary.</p>
<p>Using the seq2seq framework, we tested a range of standard recurrent neural network models from the literature: simple recurrent networks (SRNs; Elman, 1990), long short-term memory networks (LSTMs; Hochreiter \&amp; Schmidhuber, 1997), and gated recurrent units (GRUs; Chung et al., 2014). Recurrent networks with attention have become increasingly popular in the last few years, and thus we also tested each network with and without an attentional mechanism, using the model from Bahdanau et al. (2015) (see Supplementary for more details). Finally, to make the evaluations as systematic as possible, a large-scale hyperparameter search was conducted that varied the number of layers ( 1 or 2 ), the number of hidden units per layer ( $25,50,100,200$, or 400), and the amount of dropout ( $0,0.1,0.5$; applied to recurrent layers and word embeddings). Varying these hyperparameters leads to 180 different network architectures, all of which were run on each experiment and replicated 5 times each with different random initializations. ${ }^{4}$</p>
<p>In reporting the results, we focus on the overall-best architecture as determined by the extensive hyperparameter search. The winning architecture was a 2-layer LSTM with</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">jump</th>
<th style="text-align: left;">$\Rightarrow$</th>
<th style="text-align: left;">JUMP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">jump left</td>
<td style="text-align: left;">$\Rightarrow$</td>
<td style="text-align: left;">LTURN JUMP</td>
</tr>
<tr>
<td style="text-align: left;">jump around right</td>
<td style="text-align: left;">$\Rightarrow$</td>
<td style="text-align: left;">RTURN JUMP RTURN JUMP RTURN JUMP RTURN JUMP</td>
</tr>
<tr>
<td style="text-align: left;">turn left twice</td>
<td style="text-align: left;">$\Rightarrow$</td>
<td style="text-align: left;">LTURN LTURN</td>
</tr>
<tr>
<td style="text-align: left;">jump thrice</td>
<td style="text-align: left;">$\Rightarrow$</td>
<td style="text-align: left;">JUMP JUMP JUMP</td>
</tr>
<tr>
<td style="text-align: left;">jump opposite left and walk thrice</td>
<td style="text-align: left;">$\Rightarrow$</td>
<td style="text-align: left;">LTURN LTURN JUMP WALK WALK WALK</td>
</tr>
<tr>
<td style="text-align: left;">jump opposite left after walk around left</td>
<td style="text-align: left;">$\Rightarrow$</td>
<td style="text-align: left;">LTURN WALK LTURN WALK LTURN WALK LTURN WALK</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">LTURN LTURN JUMP</td>
</tr>
</tbody>
</table>
<p>Figure 1. Examples of SCAN commands (left) and the corresponding action sequences (right).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2. The seq2seq framework is applied to SCAN. The symbols $&lt;$ EOS $&gt;$ and $&lt;$ SOS $&gt;$ denote end-of-sentence and start-ofsentence, respectively. The encoder (left) ends with the first $&lt;$ EOS $&gt;$ symbol, and the decoder (right) begins with $&lt;$ SOS $&gt;$.</p>
<p>200 hidden units per layer, no attention, and dropout applied at the $\mathbf{0 . 5}$ level. Although the detailed analyses to follow focus on this particular model, the top-performing architecture for each experiment individually is also reported and analyzed.</p>
<p>Networks were trained with the following specifications. Training consisted of 100,000 trials, each presenting an input/output sequence and then updating the networks weights. ${ }^{5}$ The ADAM optimization algorithm was used with default parameters, including a learning rate of 0.001 (Kingma \&amp; Welling, 2014). Gradients with a norm larger than 5.0 were clipped. Finally, the decoder requires the previous step's output as the next step's input, which was computed in two different ways. During training, for half the time, the network's self-produced outputs were passed back to the next step, and for the other half of the time, the groundtruth outputs were passed back to the next step (teacher forcing; Williams \&amp; Zipser, 1989). The networks were implemented in PyTorch and based on a standard seq2seq implementation. ${ }^{6}$</p>
<p>Training accuracy was above $99.5 \%$ for the overall-best network in each of the key experiments, and it was at least $95 \%$ for the top-performers in each experiment specifically.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>4. Experiments</h2>
<p>In each of the following experiments, the recurrent networks are trained on a large set of commands from the SCAN tasks to establish background knowledge as outlined above. After training, the networks are then evaluated on new commands designed to test generalization beyond the background set in systematic, compositional ways. In evaluating these new commands, the networks must make zero-shot generalizations and produce the appropriate action sequence based solely on extrapolation from the background training.</p>
<h2>Experiment 1: Generalizing to a random subset of commands</h2>
<p>In this experiment, the SCAN tasks were randomly split into a training set ( $80 \%$ ) and a test set ( $20 \%$ ). The training set provides broad coverage of the task space, and the test set examines how networks can decompose and recombine commands from the training set. For instance, the network is asked to perform the new command, "jump opposite right after walk around right thrice," as a zero-shot generalization in the test set. Although the conjunction as a whole is novel, the parts are not: The training set features many examples of the parts in other contexts, e.g., "jump opposite right after turn opposite right" and "jump right twice after walk around right thrice" (both bold sub-strings appear 83 times in the training set). To succeed, the network needs to generalize by recombining pieces of existing commands to interpret new ones.</p>
<p>Overall, the networks were highly successful at generalization. The top-performing network for this experiment achieved $99.8 \%$ correct on the test set (accuracy values here and below are averaged over the five training runs). The topperforming architecture was a LSTM with no attention, 2 layers of 200 hidden units, and no dropout. The best-overall network achieved $99.7 \%$ correct. Interestingly, not every architecture was successful: Classic SRNs performed very poorly, and the best SRN achieved less than $1.5 \%$ correct at test time (performance on the training set was equally low). However, attention-augmented SRNs learned the commands much better, achieving $59.7 \%$ correct on average for the test set (with a range between $18.4 \%$ and $94.0 \%$ across SRN</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3. Zero-shot generalization after training on a random subset of the SCAN tasks. The overall-best network was trained on varying proportions of the set of distinct tasks (x-axis) and generalization was measured on new tasks (y-axis). Each bar shows the mean over 5 training runs with corresponding $\pm 1$ SEM.
architectures). For LSTMs and GRUs, attention was instead not essential. Since the SCAN commands are never longer than 9 words, attention is probably superfluous, at least in this simple setup, for gated architectures that generally exhibit a more robust long-distance behaviour than SRNs.</p>
<p>As indicated above, the main split was quite generous, providing $80 \%$ of the commands at training time for a total of over 16,700 distinct examples (with strong combinatorial coverage). We next re-trained the best-overall network with varying numbers of distinct examples (the actual number of training presentations was kept constant at 100 K ). The results are shown in Fig. 3. With $1 \%$ of the commands shown during training (about 210 examples), the network performs poorly at about $5 \%$ correct. With $2 \%$ coverage, performance improves to about $54 \%$ correct on the test set. By $4 \%$ coverage, performance is about $93 \%$ correct. Our results show that not only can networks generalize to random subsets of the tasks, they can do so from relatively sparse coverage of the compositional command space. This is well in line with the success of seq2seq architectures in machine translation, where most test sentences are likely never encountered in training. Still, even with this sparser coverage, differences between training and test instances are not dramatic. Let us for example consider the set of all commands without a conjunction (e.g., "walk around thrice", "run", "jump opposite left twice"). All the commands of this sort that occur in the test set of the $2 \%$ training coverage split (either as components of a conjunction or by themselves) also occur in the corresponding training set, with an average of 8 occurrences. Even for the $1 \%$ split, there is only one
conjunction-less test command that does not also occur in the training split, and the frequency of occurrence of such commands in the training set is at a non-negligible average value of 4 times.</p>
<h2>Experiment 2: Generalizing to commands demanding longer action sequences</h2>
<p>We study next a more systematic form of generalization, where models must bootstrap to commands requiring longer action sequences than those seen in training. ${ }^{7}$ Now the training set contains all 16,990 commands requiring sequences of up to 22 actions, whereas the test set includes all remaining commands ( 3,920 , requiring action sequences of lengths from 24 to 48). Under this split, for example, at test time the network must execute the command "jump around left twice and walk opposite right thrice", requiring a sequence of 25 actions. Although all the elements used in the command have been observed during training, the network has never been asked to produce a sequence of this length, nor it has ever seen an "around * twice" command conjoined with an "opposite * thrice" command (although it did observe both components conjoined with others). Thus, it must productively generalize familiar verbs, modifiers and conjunctions to generate longer action sequences. This is a fair task for a system that is correctly translating the input commands. If you know how to "walk around," how to "jump," and the function of the "and" conjunction, you will be immediately able to "walk around and jump," even if you have never performed an action sequence of that length.</p>
<p>This test turns out to be very challenging for all models. The best result ( $20.8 \%$ on average, again over 5 runs) is achieved by a GRU with attention, one 50-dimensional hidden layer, and dropout 0.5 . Interestingly, this is a model with considerably less capacity than the best for the random-split setup, but it uses attention, which might help, to a limited degree, to generalize to longer action sequences. The overall-best model achieves $13.8 \%$ accuracy.</p>
<p>Fig. 4 (top) shows partial success is almost entirely explained by generalization to the shortest action sequence lengths in the test set. Although we might not expect even humans to be able to generalize to very long action sequences, the sharp drop between extrapolating to 25 and 26 actions is striking. The bottom panel of Fig. 4 shows accuracy in the test set organized by command length (in word tokens). The model only gets right some of the longest commands ( 8 or 9 tokens). In the training set, the longest ac-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Zero-shot generalization to commands with action sequence lengths not seen in training. Top: accuracy distribution by action sequence length. Bottom: accuracy distribution by command length (only lengths attested in the test set shown, in both cases). Bars show means over 5 runs of overall-best model with $\pm 1$ SEM.
tion sequences $(\geq 20)$ are invariably associated to commands containing 8 or 9 tokens. Thus, the model is correctly generalizing only in those cases that are most similar to training instances.</p>
<p>Finally, we performed two additional analyses to better understand the source of the errors. First, we examined the greedy decoder for search-related errors. We confirmed that, for almost every error, the network preferred its selfgenerated output sequence to the target output sequence (as measured by log-likelihood). Thus, the errors were not due to search failures in the decoder. ${ }^{8}$ Second, we studied whether the difficulty with long sequences can be mitigated if the proper length was provided by an oracle at evaluation</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>time. ${ }^{9}$ If this difficulty is a relatively straightforward issue of the decoder terminating too early, then this should provide an (unrealistic) fix. If this difficulty is symptomatic of deeper problems with generalization, then this change will have only a small effect. With the oracle, the overallbest network performance improved from $13.8 \%$ to $23.6 \%$ correct, which was notable but insufficient to master the long sequences. The top-performing model showed a more substantial improvement ( $20.8 \%$ to $60.2 \%$ ). Although improved, the networks were far from perfect and still exhibited difficulties with long sequences of output actions (again, even for the top model, there was a strong effect of action sequence length, with average accuracy ranging from $95.76 \%$ for commands requiring 24 actions to $22.8 \%$ for commands requiring 48 actions).</p>
<h2>Experiment 3: Generalizing composition across primitive commands</h2>
<p>Our next test is closest to the "dax" thought experiment presented in the introduction. In the training phase, the model is exposed to the primitive command only denoting a certain basic action (e.g., "jump"). The model is also exposed to all primitive and composed commands for all other actions (e.g., "run", "run twice", "walk", "walk opposite left and run twice", etc.). At test time, the model has to execute all composed commands for the action that it only saw in the primitive context (e.g., "jump twice", "jump opposite left and run twice", etc.). According to the classic thought experiments of Fodor and colleagues, this should be easy: if you know the meaning of "run", "jump" and "run twice", you should also understand what "jump twice" means.</p>
<p>We run two variants of the experiment generalizing from "turn left" and "jump", respectively. Since "turn right" is distributionally identical to "turn left" (in the sense that it occurs in exactly the same composed commands) and "walk", "run" and "look" are distributionally identical to "jump", it is redundant to test all commands. Moreover, to ensure the networks were highly familiar with the target primitive command ("jump" or "turn left"), the latter was over-represented in training such that roughly $10 \%$ of all training presentations were of the command. ${ }^{10}$</p>
<p>We obtain strikingly different results for "turn left" and "jump". For "turn left", many models generalize very well to composed commands. The best performance is achieved by a GRU network with attention, one layer with 100 hidden units, and dropout of 0.1 ( $90.3 \%$ accuracy). The overall-</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>best model achieved $90.0 \%$ accuracy. On the other hand, for "jump," models are almost completely incapable to generalize to composed commands. The best performance was $1.2 \%$ accuracy (LSTM, attention, one layer, 100 hidden units, dropout 0.1 ). The overall-best model reached $0.08 \%$ accuracy. As in Experiment 2, the errors were not due to search failures in the decoder.</p>
<p>In the case of "turn left", although models are only exposed to the primitive command during training, they will see the action it denotes (LTURN) many times, as it is used to accomplish many directed actions. For example, a training item is: "walk left and jump left", with ground-truth interpretation: LTURN WALK LTURN JUMP. Apparently, seeing action sequences containing LTURN suffices for the model to understand composed commands with "turn left", probably because the model receives direct evidence about how LTURN is used in context. On the other hand, the action denoted by "jump" (JUMP) only occurs with this primitive command in training, and the model does not generalize from this minimal context to new composed ones.</p>
<p>We now take a closer look at the results, focusing on the median-performance run of the overall-best model (as the most representative run of this model). We observe that even in the successful "turn left" case model errors are surprising. One would expect such errors to be randomly distributed, or perhaps to pertain to the longest commands or action sequences. Instead, all 45 errors made by the model are conjunctions where one of the components is simple "turn left" (22 cases) or "turn left thrice" (23 cases). This is particularly striking because the network produced the correct mapping for "turn left" during training, as well as for "turn left thrice" at test time, and it gets many more conjunctions right (ironically, including "turn left thrice and turn left", "turn left thrice after turn left" etc.). We conclude that, even when the network has apparently learned systematic composition almost perfectly, it got at it in a very counter-intuitive way. It's hard to conceive of someone who understood the meaning of "turn left", and "jump right and turn left twice" (which the network gets right), but not that of "jump right and turn left" (one of the examples the network missed). In the "jump" experiment, the network could only correctly decode two composite cases, both starting with the execution of primitive "jump", conjoined with a different action: "jump and run opposite right", "jump and walk around left thrice".</p>
<p>It is instructive to look at the representations that the network induced for various commands in the latter experiment. Table 1 reports the 5 nearest neighbours for a sample of commands. Command similarity is measured by the cosine between the final encoder hidden state vectors, and computed with respect to all commands present in the training set. "Run" is provided as an example primitive command
for which the model has been exposed to the full composed paradigm in training. As one would expect, "run" is close to the other primitive commands ("look", "walk"), as well as to short conjoined commands that contain primitive "run" as one of the conjuncts (we observe a similar pattern for the non-degenerate "jump" representation induced in Experiment 1). Instead, since "jump" had a different training distribution than the other primitive commands, the model does not capture its similarity to them, as shown by the very low cosines of its nearest commands. Since it fails to establish a link to other basic commands, the model does not generalize modifier application from them to "jump". Although "run twice" is similar to (conjunctions of) other primitive tasks composed with "twice", "jump twice" is isolated in representational space, and its (far) nearest neighbours look arbitrary.</p>
<p>We tested here systematicity in its purest form: the model was only exposed to "jump" in isolation, and asked to bootstrap to its compositional paradigm based on the behaviour of other primitive commands such as "walk", "look" and "run". Although we suspect humans would not have problems with this setup, it arguably is too opaque for a computational model, which could lack evidence for "jumping" being the same sort of action as "walking". Suppose we give the network some evidence that "jumping" composes like "walking" by showing a few composed "jump" command during training. Is the network then able to generalize to the full composed paradigm?</p>
<p>This question is answered in Figure 5. We present here results for the best model in the "jump"-generalization task, which was noticeably better in the present setup than the overall-best model. Again, the new primitive command (and its compositions) were over-sampled during training to make up $10 \%$ of all presentations. Here, even when shown 8 different composed commands with "jump" at training time, the network only weakly generalizes to other composed commands ( $38.3 \%$ correct). Significant generalization (still far from systematic) shows up when the training set contains 16 and especially 32 distinct composed commands ( $77.8 \%$ and $88.4 \%$, respectively). We conclude that the network is not failing to generalize simply because, in the original setup, it had little evidence that "jump" should behave like the other commands. On the other hand, the runs with more composed examples confirm that, as we found in Experiment 1, the network does display powerful generalization abilities. Simply, they do not conform to the "all-or-nothing" rule-based behaviour we would expect from a systematically compositional device-and, as a consequence, they require more positive examples to emerge.</p>
<p>Table 1. Nearest training commands for representative commands, with the respective cosines. Here, "jump" was trained in isolation while "run" was trained compositionally. Italics mark low similarities (cosine $&lt;0.2$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">run</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">jump</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">run twice</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">jump twice</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">look</td>
<td style="text-align: center;">.73</td>
<td style="text-align: center;">run</td>
<td style="text-align: center;">. 15</td>
<td style="text-align: center;">look twice</td>
<td style="text-align: center;">.72</td>
<td style="text-align: center;">walk and walk</td>
<td style="text-align: center;">. 19</td>
</tr>
<tr>
<td style="text-align: center;">walk</td>
<td style="text-align: center;">.65</td>
<td style="text-align: center;">walk</td>
<td style="text-align: center;">. 13</td>
<td style="text-align: center;">run twice and <br> look opposite right thrice</td>
<td style="text-align: center;">.65</td>
<td style="text-align: center;">run and walk</td>
<td style="text-align: center;">. 16</td>
</tr>
<tr>
<td style="text-align: center;">walk after run</td>
<td style="text-align: center;">.55</td>
<td style="text-align: center;">turn right</td>
<td style="text-align: center;">. 12</td>
<td style="text-align: center;">run twice and <br> run right twice</td>
<td style="text-align: center;">. 64</td>
<td style="text-align: center;">walk opposite right <br> and walk</td>
<td style="text-align: center;">. 12</td>
</tr>
<tr>
<td style="text-align: center;">run thrice <br> after run</td>
<td style="text-align: center;">.50</td>
<td style="text-align: center;">look right twice <br> after walk twice</td>
<td style="text-align: center;">.09</td>
<td style="text-align: center;">run twice and <br> look opposite right twice</td>
<td style="text-align: center;">.63</td>
<td style="text-align: center;">look right and walk</td>
<td style="text-align: center;">. 12</td>
</tr>
<tr>
<td style="text-align: center;">run twice <br> after run</td>
<td style="text-align: center;">.49</td>
<td style="text-align: center;">turn right <br> after turn right</td>
<td style="text-align: center;">.09</td>
<td style="text-align: center;">walk twice and run twice</td>
<td style="text-align: center;">.63</td>
<td style="text-align: center;">walk right and walk</td>
<td style="text-align: center;">. 11</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Zero-shot generalization after adding the primitive "jump" and some compositional "jump" commands. The model that performed best in generalizing from primitive "jump" only was retrained with different numbers of composed "jump" commands (x-axis) in the training set, and generalization was measured on new composed "jump" commands (y-axis). Each bar shows the mean over 5 runs with varying training commands along with the corresponding $\pm 1$ SEM.</p>
<h2>Experiment 4: Compositionality in machine translation</h2>
<p>Our final experiment is a proof-of-concept that our findings are more broadly applicable; that is, the limitations of recurrent networks with regards to systematic compositionality extend beyond SCAN to other sequence-to-sequence problems such as machine translation. First, we trained our standard seq2seq code on short ( $\leq 9$ words) English-French sentence pairs that begin with English phrases such as "I am," "he is," "they are," and their contractions (randomly split with 10,000 for training and 1180 for testing). ${ }^{6}$ An informal hyperparameter search led us to pick a LSTM with attention, 2 layers of 400 hidden units, and 0.05 dropout.</p>
<p>With these hyperparameters and the same training procedure used for the SCAN tasks (Section 3), the network reached a respectable 28.6 BLEU test score after 100,000 steps.</p>
<p>Second, to examine compositionality with the introduction of a new word, we trained a fresh network after adding 1,000 repetitions of the sentence "I am daxy" (fr. "je suis daxiste") to the training data (the BLEU score on the original test set dropped less than 1 point). ${ }^{11}$ We tested this network by embedding "daxy" into the following constructions: "you are daxy" ("tu es daxiste"), "he is daxy" ("il est daxiste"), "I am not daxy" ("je ne suis pas daxiste"), "you are not daxy" ("tu n'es pas daxiste"), "he is not daxy" ("il n'est pas daxiste"), "I am very daxy" ("je suis très daxiste"), "you are very daxy" ("tu es très daxiste"), "he is very daxy" ("il est très daxiste"). During training, the model saw these constructions occurring with 22 distinct predicates on average (limiting the counts to perfect matches, excluding, e.g., "you are not very X"). Still, the model could only get one of the 8 translations right (that of "he is daxy"). For comparison, for the adjective "tired", which occurred in 80 different constructions in the training corpus, our network had $8 / 8$ accuracy when testing on the same constructions as for "daxy" (only one of which also occurred with "tired" in the training set). Although this is a small-scale machine translation problem, our preliminary result suggests that models will similarly struggle with systematic compositionality in larger data sets, when adding a new word to their vocabulary, in ways that people clearly do not.</p>
<h2>5. Discussion</h2>
<p>In the thirty years since the inception of the systematicity debate, many have tested the ability of neural networks to solve tasks requiring compositional generalization, with mixed results (e.g., Christiansen \&amp; Chater, 1994; Marcus, 1998; Phillips, 1998; Chang, 2002; van der Velde et al.,</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2004; Botvinick \&amp; Plaut, 2006; Wong \&amp; Wang, 2007; Bowers et al., 2009; Botvinick \&amp; Plaut, 2009; Brakel \&amp; Frank, 2009; Frank et al., 2009; Frank, 2014; Bowman et al., 2016). However, to the best of our knowledge, ours is the first study testing systematicity in modern seq2seq models, and our results confirm the mixed picture. On the one hand, Experiment 1 and the "turn left" results in Experiment 3 show how standard recurrent models can reach very high zeroshot accuracy from relatively few training examples. We would like to stress that this is an important positive result, showing in controlled experiments that seq2seq models can make powerful zero-shot generalizations. Indeed, an interesting direction for future work is to understand what are, precisely, the generalization mechanisms that subtend the networks' success in these experiments. After all, human language does have plenty of generalization patterns that are not easily accounted for by algebraic compositionality (see, e.g., Goldberg, 2005).</p>
<p>On the other hand, the same networks fail spectacularly when the link between training and testing data is dependent on the ability to extract systematic rules. This can be seen as a trivial confirmation of the basic principle of statistical machine learning that your training and test data should come from the same distribution. But our results also point to an important difference in how humans and current seq2seq models generalize, since there is no doubt that human learners can generalize to unseen data when such data are governed by rules that they have learned before. Importantly, the training data of experiments 2 and 3 provide enough evidence to learn composition rules affording the correct generalizations. In Experiment 2, the training data contain examples of all modifiers and connectives that are needed at test time for producing longer action sequences. In Experiment 3, the usage of modifiers and connectives is illustrated at training time by their application to many combinations of different primitive commands, and, at test time, the network should apply them to a new command it encountered in isolation during training.</p>
<p>We thus believe that the fundamental component that current models are missing is the ability to extract systematic rules from the training data. A model that can abstract away from surface statistical patterns and operate in "rule space" should extract rules such as: translate $(x$ and $y)=$ translate $(x)$ translate $(y)$; translate $(x$ twice $)=$ translate $(x)$ translate $(x)$. Then, if the meaning of a new command (translate("jump")) is learned at training time, and acts as a variable that rules can be applied to, no further learning is needed at test time. When represented in this more abstract way, the training and test distributions are quite similar, even if they differ in terms of shallower statistics such as word frequency.</p>
<p>How can we encourage seq2seq models to extract rules from data rather than exploiting shallower pattern recognition
mechanisms? We think there are several, non-mutually exclusive avenues to be explored.</p>
<p>First, in a "learning-to-learn" approach (Thrun \&amp; Pratt, 1997; Risi et al., 2009; Finn et al., 2017, a.o.), a network can be exposed to a number of different learning environments regulated by similar rules. An objective function requiring successful generalization to new environments might encourage learners to discover the shared general rules.</p>
<p>Another promising approach is to add more structure to the neural networks. Taking inspiration from recent neural program induction and modular network models (e.g., Reed \&amp; de Freitas, 2016; Hu et al., 2017; Johnson et al., 2017), we could endow RNNs with a set of manually-encoded or (ideally) learned functions for interpreting individual modifiers, connectives, and primitives. The job of the RNN would be to learn how to apply and compose these functions as appropriate for interpreting a command. Similarly, differentiable stacks, tapes, or random-access memory (e.g., Joulin \&amp; Mikolov, 2015; Graves et al., 2016) could equip seq2seq models with quasi-discrete memory structures, enabling separate storage of variables, which in turn might encourage abstract rule learning (see Feng et al., 2017, for a memory-augmented seq2seq model).</p>
<p>Other solutions, such as ad-hoc copying mechanisms or special ways to initialize the embeddings of novel words, might help to solve the SCAN tasks specifically. But they are unlikely to help with more general seq2seq problems. It remains to be seen, of course, if any of our proposed approaches offer a truly general solution. Nonetheless, we see all of the suggestions as directions worth pursuing, perhaps simultaneously and in complementary ways, with the goal of achieving human-like systematicity on SCAN and beyond.</p>
<p>Given the astounding successes of seq2seq models in challenging tasks such as machine translation, one might argue that failure to generalize by systematic composition indicates that neural networks are poor models of some aspects of human cognition, but it is of little practical import. However, systematicity is an extremely efficient way to generalize. Once a person learns the new English adjective "daxy", he or she can immediately produce and understand an infinity of sentences containing it. The SCAN experiments and a proof-of-concept machine translation experiment (Experiment 4) suggest that this ability is still beyond the grasp of state-of-the-art neural networks, likely contributing to their striking need for very large training sets. These results give us hope that neural networks capable of systematic compositionality could greatly benefit machine translation, language modeling, and other applications.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank the reviewers, Joost Bastings, Kyunghyun Cho, Douwe Kiela, Germán Kruszewski, Adam Liska, Tomas Mikolov, Kristina Gulordava, Gemma Boleda, Michael Auli, Matt Botvinick, Sam Bowman, Jeff Dean, Jonas Gehring, David Grangier, Angeliki Lazaridou, Gary Marcus, Jason Weston, the CommAI team and the audiences at the Facebook Dialogue Summit, 2017 Paris Syntax and Semantics Colloquium and CLIC-it 2017 for feedback and advice. The SCAN tasks are based on the navigation tasks available at: https://github.com/ facebookresearch/CommAI-env</p>
<h2>References</h2>
<p>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR Conference Track, San Diego, CA, 2015. Published online: http://www.iclr.cc/ doku.php?id=iclr2015:main.</p>
<p>Baroni, M. Distributions in text. In Lüdeling, A. and Kytö, M. (eds.), Corpus Linguistics: An International Handbook, volume 2, pp. 803-821. Mouton de Gruyter, Berlin, Germany, 2009.</p>
<p>Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., Jimeno Yepes, A., Koehn, P., Logacheva, V., Monz, C., Negri, M., Neveol, A., Neves, M., Popel, M., Post, M., Rubino, R., Scarton, C., Specia, L., Turchi, M., Verspoor, K., and Zampieri, M. Findings of the 2016 Conference on Machine Translation. In Proceedings of the First Conference on Machine Translation, pp. 131-198, Berlin, Germany, 2016.</p>
<p>Botvinick, M. and Plaut, D. Short-term memory for serial order: A recurrent neural network model. Psychological Review, 113(2):201-233, 2006.</p>
<p>Botvinick, M. and Plaut, D. Empirical and computational support for context-dependent representations of serial order: Reply to Bowers, Damian, and Davis (2009). Psychological Review, 116(4):998-1002, 2009.</p>
<p>Bowers, J., Damian, M., and David, C. A fundamental limitation of the conjunctive codes learned in PDP models of cognition: Comment on Botvinick and Plaut (2006). Psychological Review, 116(4):986-997, 2009.</p>
<p>Bowman, S. R., Manning, C. D., and Potts, C. TreeStructured Composition in Neural Networks without TreeStructured Architectures. arXiv preprint, 2016.</p>
<p>Brakel, P. and Frank, S. Strong systematicity in sentence processing by simple recurrent networks. In Proceedings of CogSci, pp. 1599-1604, Amsterdam, the Netherlands, 2009.</p>
<p>Calvo, P. and Symons, J. (eds.). The architecture of cognition: Rethinking Fodor and Pylyshyn's systematicity challenge. MIT Press, Cambridge, MA, 2014.</p>
<p>Chang, F. Symbolically speaking: a connectionist model of sentence production. Cognitive Science, 26:609-651, 2002.</p>
<p>Chomsky, N. Syntactic Structures. Mouton, Berlin, Germany, 1957.</p>
<p>Christiansen, M. and Chater, N. Generalization and connectionist language learning. Mind \&amp; Language, 9(3): 273-287, 1994.</p>
<p>Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. In Proceedings of the NIPS Deep Learning and Representation Learning Workshop, Montreal, Canada, 2014. Published online: http://www.dlworkshop. org/accepted-papers.</p>
<p>Dong, L. and Lapata, M. Language to logical form with neural attention. In Proceedings of ACL, pp. 33-43, Berlin, Germany, 2016.</p>
<p>Elman, J. Finding structure in time. Cognitive Science, 14: 179-211, 1990.</p>
<p>Feng, Y., Zhang, S., Zhang, A., Wang, D., and Abel, A. Memory-augmented neural machine translation. In Proceedings of EMNLP, pp. 1390-1399, 2017.</p>
<p>Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In Proceedings of ICML, pp. 1126-1135, Sydney, Australia, 2017.</p>
<p>Fodor, J. and Lepore, E. The Compositionality Papers. Oxford University Press, Oxford, UK, 2002.</p>
<p>Fodor, J. and Pylyshyn, Z. Connectionism and cognitive architecture: A critical analysis. Cognition, 28:3-71, 1988.</p>
<p>Frank, S. Getting real about systematicity. In Calvo, P. and Symons, J. (eds.), The architecture of cognition: Rethinking Fodor and Pylyshyn's systematicity challenge, pp. 147-164. MIT Press, Cambridge, MA, 2014.</p>
<p>Frank, S., Haselager, W., and van Rooij, I. Connectionist semantic systematicity. Cognition, 110(3):358-379, 2009.</p>
<p>Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y. Convolutional sequence to sequence learning. https : //arxiv.org/abs/1705.03122, 2017.</p>
<p>Goldberg, A. Constructions at work: The nature of generalization in language. Oxford University Press, Oxford, UK, 2005.</p>
<p>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwinska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., Badia, A. P., Hermann, K. M., Zwols, Y., Ostrovski, G., Cain, A., King, H., Summerfield, C., Blunsom, P., Kavukcuoglu, K., and Hassabis, D. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471-476, 2016.</p>
<p>Herzig, J. and Berant, J. Neural semantic parsing over multiple knowledge-bases. In Proceedings of ACL (Volume 2: Short Papers), pp. 623-628, Vancouver, Canada, 2017.</p>
<p>Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9:1735-1780, 1997.</p>
<p>Hu, R., Andreas, J., Rohrbach, M., Darrell, T., and Saenko, K. Learning to reason: End-to-end module networks for visual question answering. In Proceedings of ICCV, pp. 804-813, Venice, Italy, 2017.</p>
<p>Jia, R. and Liang, P. Data recombination for neural semantic parsing. In Proceedings of ACL, pp. 12-22, Berlin, Germany, 2016.</p>
<p>Johnson, J., Hariharan, B., van der Maaten, L., Hoffman, J., Fei-fei, L., Zitnick, C. L., and Girshick, R. Inferring and Executing Programs for Visual Reasoning. In International Conference on Computer Vision, 2017.</p>
<p>Joulin, A. and Mikolov, T. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of NIPS, Montreal, Canada, 2015.</p>
<p>Kingma, D. P. and Welling, M. Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets. In International Conference on Machine Learning (ICML 2014), 2014.</p>
<p>Lake, B., Ullman, T., Tenenbaum, J., and Gershman, S. Building machines that learn and think like people. Behavorial and Brain Sciences, 2017. In press.</p>
<p>Marcus, G. F. Rethinking Eliminative Connectionism. Cognitive Psychology, 282(37):243-282, 1998.</p>
<p>Marcus, G. F. The Algebraic Mind: Integrating Connectionism and Cognitive Science. MIT Press, Cambridge, MA, 2003.</p>
<p>Mikolov, T., Joulin, A., and Baroni, M. A Roadmap towards Machine Intelligence. arXiv preprint, 2016. URL http: //arxiv.org/abs/1511.08130.</p>
<p>Montague, R. Universal Grammar. Theoria, 36:373-398, 1970.</p>
<p>Phillips, S. Are feedforward and recurrent networks systematic? analysis and implications for a connectionist cognitive architecture. Connection Science, 10:137-160, 1998.</p>
<p>Reed, S. and de Freitas, N. Neural programmer-interpreters. In Proceedings of ICLR, San Juan, Puerto Rico, 2016. Published online: http://www.iclr.cc/doku. php?id=iclr2016:main.</p>
<p>Risi, S., Vanderbleek, S., Hughes, C., and Stanley, K. How novelty search escapes the deceptive trap of learning to learn. In Proceedings of GECCO, pp. 153-160, Montreal, Canada, 2009.</p>
<p>Sutskever, I., Vinyals, O., and Le, Q. Sequence to sequence learning with neural networks. In Proceedings of NIPS, pp. 3104-3112, Montreal, Canada, 2014.</p>
<p>Thrun, S. and Pratt, L. (eds.). Learning to Learn. Kluwer, Dordrecht, 1997.
van der Velde, F., , van der Voort van der Kleij, G., and de Kamps, M. Lack of combinatorial productivity in language processing with simple recurrent networks. Connection Science, 16(1):21-46, 2004.</p>
<p>Williams, R. J. and Zipser, D. A Learning Algorithm for Continually Running Fully Recurrent Neural Networks. Neural Computation, 1:270-280, 1989.</p>
<p>Wong, F. and Wang, W. Generalisation towards combinatorial productivity in language acquisition by simple recurrent networks. In Proceedings of KIMAS, pp. 139144, Waltham, MA, 2007.</p>
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. Google's neural machine translation system: Bridging the gap between human and machine translation. http://arxiv.org/abs/1609.08144, 2016.</p>
<h2>Supplementary materials</h2>
<h2>SCAN grammar and interpretation function</h2>
<p>The phrase-structure grammar generating all SCAN commands is presented in Figure 6. The corresponding interpretation functions is in Figure 7.</p>
<h2>Standard Encoder-Decoder RNN</h2>
<p>We describe the encoder-decoder framework, borrowing from the description in Bahdanau et al. (2015). The encoder receives a natural language command as a sequence of $T$ words. The words are transformed into a sequence of vectors, $\left{w_{1}, \ldots, w_{T}\right}$, which are learned embeddings with the same number of dimensions as the hidden layer. A recurrent neural network (RNN) processes each word</p>
<p>$$
h_{t}=f_{E}\left(h_{t-1}, w_{t}\right)
$$</p>
<p>where $h_{t}$ is the encoder hidden state. The final hidden state $h_{T}$ (which may include multiple layers for multilayer RNNs) is passed to the RNN decoder as hidden state $g_{0}$ (see seq2seq diagram in the main article). Then, the RNN decoder must generate a sequence of output actions $a_{1}, \ldots, a_{R}$. To do so, it computes</p>
<p>$$
g_{t}=f_{D}\left(g_{t-1}, a_{t-1}\right)
$$</p>
<p>where $g_{t}$ is the decoder hidden state and $a_{t-1}$ is the (embedded) output action from the previous time step. Last, the hidden state $g_{t}$ is mapped to a softmax to select the next action $a_{t}$ from all possible actions.</p>
<h2>Attention Encoder-Decoder RNN</h2>
<p>For the encoder-decoder with attention, the encoder is identical to the one described above. Unlike the standard decoder that can only see $h_{T}$, the attention decoder can access all of the encoder hidden states, $h_{1}, \ldots, h_{T}$ (in this case, only the last layer if multi-layer). At each step $i$, a context vector $c_{i}$ is computed as a weighted sum of the encoder hidden states</p>
<p>$$
c_{i}=\sum_{t=1}^{T} \alpha_{i t} h_{t}
$$</p>
<p>The weights $\alpha_{i t}$ are computed using a softmax function</p>
<p>$$
\alpha_{i t}=\exp \left(e_{i t}\right) / \sum_{j=1}^{T} \exp \left(e_{i j}\right)
$$</p>
<p>where $e_{i t}=v_{a}^{\top} \tanh \left(W_{a} g_{i-1}+U_{a} h_{t}\right)$ is an alignment model that computes the similarity between the previous decoder hidden state $g_{i-1}$ and an encoder hidden state $h_{t}$ (for the other variables, $v_{a}, W_{a}$, and $U_{a}$ are learnable parameters) (Bahdanau et al., 2015). This context vector $c_{i}$ is
then passed as input to the decoder RNN at each step with the function</p>
<p>$$
g_{i}=f_{D}\left(g_{i-1}, a_{i-1}, c_{i}\right)
$$</p>
<p>which also starts with hidden state $g_{0}=h_{T}$, as in the standard decoder. Last, the hidden state $g_{i}$ is concatenated with $c_{i}$ and mapped to a softmax to select new action $a_{i}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathrm{C} \rightarrow \mathrm{S}$ and S</th>
<th style="text-align: left;">$\mathrm{V} \rightarrow \mathrm{D}[1]$ opposite $\mathrm{D}[2]$</th>
<th style="text-align: left;">$\mathrm{D} \rightarrow$ turn left</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{C} \rightarrow \mathrm{S}$ after S</td>
<td style="text-align: left;">$\mathrm{V} \rightarrow \mathrm{D}[1]$ around $\mathrm{D}[2]$</td>
<td style="text-align: left;">$\mathrm{D} \rightarrow$ turn right</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{C} \rightarrow \mathrm{S}$</td>
<td style="text-align: left;">$\mathrm{V} \rightarrow \mathrm{D}$</td>
<td style="text-align: left;">$\mathrm{U} \rightarrow$ walk</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{S} \rightarrow \mathrm{V}$ twice</td>
<td style="text-align: left;">$\mathrm{V} \rightarrow \mathrm{U}$</td>
<td style="text-align: left;">$\mathrm{U} \rightarrow$ look</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{S} \rightarrow \mathrm{V}$ thrice</td>
<td style="text-align: left;">$\mathrm{D} \rightarrow \mathrm{U}$ left</td>
<td style="text-align: left;">$\mathrm{U} \rightarrow$ run</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{S} \rightarrow \mathrm{V}$</td>
<td style="text-align: left;">$\mathrm{D} \rightarrow \mathrm{U}$ right</td>
<td style="text-align: left;">$\mathrm{U} \rightarrow$ jump</td>
</tr>
</tbody>
</table>
<p>Figure 6. Phrase-structure grammar generating SCAN commands. We use indexing notation to allow infixing: D[i] is to be read as the i-th element directly dominated by category D .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">[walk ] = WALK</th>
<th style="text-align: center;">[u opposite left] = [turn opposite left] [u]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[look] = LOOK</td>
<td style="text-align: center;">[u opposite right] = [turn opposite right] [u]</td>
</tr>
<tr>
<td style="text-align: center;">[run] = RUN</td>
<td style="text-align: center;">[turn around left] = LTURN LTURN LTURN LTURN</td>
</tr>
<tr>
<td style="text-align: center;">[jump] = JUMP</td>
<td style="text-align: center;">[turn around right] = RTURN RTURN RTURN RTURN</td>
</tr>
<tr>
<td style="text-align: center;">[turn left] = LTURN</td>
<td style="text-align: center;">[u around left] = LTURN [u] LTURN [u] LTURN [u]</td>
</tr>
<tr>
<td style="text-align: center;">[turn right] = RTURN</td>
<td style="text-align: center;">[u around right] = RTURN [u] RTURN [u] RTURN [u]</td>
</tr>
<tr>
<td style="text-align: center;">[u left] = LTURN [u]</td>
<td style="text-align: center;">[x twice] = [x] [x]</td>
</tr>
<tr>
<td style="text-align: center;">[u right] = RTURN [u]</td>
<td style="text-align: center;">[x thrice] = [x] [x] [x]</td>
</tr>
<tr>
<td style="text-align: center;">[turn opposite left] = LTURN LTURN</td>
<td style="text-align: center;">[x and $x_{2}]=\llbracket x_{1} \rrbracket x_{2} \rrbracket$</td>
</tr>
<tr>
<td style="text-align: center;">[turn opposite right] = RTURN RTURN</td>
<td style="text-align: center;">[x after $x_{2} \rrbracket=\llbracket x_{2} \rrbracket \llbracket x_{1} \rrbracket$</td>
</tr>
</tbody>
</table>
<p>Figure 7. Double brackets ( $\llbracket \rrbracket$ ) denote the interpretation function translating SCAN's linguistic commands into sequences of actions (denoted by uppercase strings). Symbols $x$ and $u$ denote variables, the latter limited to words in the set {walk, look, run, jump}. The linear order of actions denotes their temporal sequence.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ Results do not change if, instead of repeating "I am daxy" 1,000 times, we insert it 100 times; with just 1 or 10 occurrences of this sentence in the training data, we get $0 / 8$ translations right.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Any attempt from the decoder to terminate the action sequence with an $&lt;\mathrm{EOS}&gt;$ was ignored (and the second strongest action was chosen) until a sequence with proper length was produced.
${ }^{10} \mathrm{~B}$ Without over-sampling, performance was consistently worse than what we report.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
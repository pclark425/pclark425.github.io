<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6564 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6564</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6564</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-277272275</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.17726v1.pdf" target="_blank">A Survey on Mathematical Reasoning and Optimization with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning and optimization are fundamental to artificial intelligence and computational problem-solving. Recent advancements in Large Language Models (LLMs) have significantly improved AI-driven mathematical reasoning, theorem proving, and optimization techniques. This survey explores the evolution of mathematical problem-solving in AI, from early statistical learning approaches to modern deep learning and transformer-based methodologies. We review the capabilities of pretrained language models and LLMs in performing arithmetic operations, complex reasoning, theorem proving, and structured symbolic computation. A key focus is on how LLMs integrate with optimization and control frameworks, including mixed-integer programming, linear quadratic control, and multi-agent optimization strategies. We examine how LLMs assist in problem formulation, constraint generation, and heuristic search, bridging theoretical reasoning with practical applications. We also discuss enhancement techniques such as Chain-of-Thought reasoning, instruction tuning, and tool-augmented methods that improve LLM's problem-solving performance. Despite their progress, LLMs face challenges in numerical precision, logical consistency, and proof verification. Emerging trends such as hybrid neural-symbolic reasoning, structured prompt engineering, and multi-step self-correction aim to overcome these limitations. Future research should focus on interpretability, integration with domain-specific solvers, and improving the robustness of AI-driven decision-making. This survey offers a comprehensive review of the current landscape and future directions of mathematical reasoning and optimization with LLMs, with applications across engineering, finance, and scientific research.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6564.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6564.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solving olympiad geometry (Trinh et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solving olympiad geometry without human demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work (Nature 2024) reporting language-model-based approaches that solve high-level olympiad geometry problems without human demonstrations; cited in this survey but not experimentally analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving olympiad geometry without human demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Olympiad geometry problems (high‑level geometric theorem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / geometric theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Only cited by title in the survey as evidence that LLMs have been applied to complex geometry; the survey does not report methods, metrics, or internal analyses from that work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No details in this survey — the citation is mentioned as a notable result but the survey does not report failure modes or limitations from that paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Mathematical Reasoning and Optimization with Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6564.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6564.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AtomThink</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AtomThink: A slow thinking framework for multimodal mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model (fine‑tuned from LLaVA/EMOVA per the survey) designed for diagram/visual mathematical reasoning; the survey notes it attains the highest reported Math‑V (visual math) score among compared multimodal models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AtomThink: A slow thinking framework for multimodal mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AtomThink</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model built on LLaVA/EMOVA, fine‑tuned for mathematical/diagram reasoning; survey table associates it with Long Chain‑of‑Thought prompting and PRM planning variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Diagram‑based visual math problems (visual/diagram reasoning; includes geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / visual diagram interpretation / geometric problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Math‑V (visual math benchmark) / MathVista (diagram benchmark) as referenced in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Long Chain‑of‑Thought (Long CoT) with PRM (planning) as listed in the survey table</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Planning‑based reasoning (PRM) / long CoT; survey does not provide implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Math‑V score (visual math benchmark score) as reported in the survey</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>40.5% Math‑V score (survey states AtomThink achieved the highest Math‑V score among compared multimodal models)</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Survey highlights AtomThink's relative lead on the Math‑V metric but does not provide internal analyses (e.g., attention patterns or representation studies) or detailed error breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes that despite AtomThink's top Math‑V score the absolute performance remains modest and that multimodal/diagram reasoning remains a major challenge across models; no model‑specific failure modes are detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Mathematical Reasoning and Optimization with Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6564.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6564.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math‑LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Math‑LLaVA (LLaVA finetuned for mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal LLaVA variant fine‑tuned on mathematical datasets to tackle diagram and visual math problems; cited as extending multimodal mathematical reasoning in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Math‑LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Math‑LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaVA‑based multimodal model fine‑tuned on math‑specific multimodal corpora; associated in the survey with Foundational Chain‑of‑Thought prompting (F‑CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Diagram‑based visual math problems / geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / visual diagram interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MathVista / visual math benchmarks (survey groups Math‑LLaVA with diagram benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Foundational Chain‑of‑Thought (F‑CoT) prompting (as listed in survey tables)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Survey remarks that Math‑LLaVA extends multimodal reasoning by finetuning on specialized mathematical datasets but provides no per‑task internal analyses or per‑model error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey indicates multimodal models (including Math‑LLaVA) still face substantial challenges in diagram/geometry understanding; no Math‑LLaVA‑specific limitations are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Mathematical Reasoning and Optimization with Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6564.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6564.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoQA (geometric question answering benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset/benchmark for geometric question answering and diagram/geometry reasoning, listed in the survey's dataset taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GeoQA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geometric question answering (diagram‑based geometry problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / geometry</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GeoQA</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Survey includes GeoQA in its dataset list for evaluating geometry/visual math but does not report model performances on GeoQA in the body text.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes general gaps in multimodal and spatial reasoning benchmarks — GeoQA represents a challenging dataset in this space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Mathematical Reasoning and Optimization with Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6564.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6564.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathVista</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MathVista (diagram‑based visual math benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal benchmark for diagram/visual mathematics problems (used to evaluate models' ability to reason about diagrams and geometry), cited in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MathVista</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Diagram‑based visual math problems / geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / visual diagram interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MathVista</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Survey uses MathVista as an example of visual/diagram benchmarks and notes that multimodal models tend to outperform text‑only models on such benchmarks, though absolute performance remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey emphasizes that visual/diagram benchmarks (like MathVista) expose weaknesses in current LLM multimodal reasoning and that more work is needed on internal verification and multimodal integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Mathematical Reasoning and Optimization with Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving olympiad geometry without human demonstrations <em>(Rating: 2)</em></li>
                <li>AtomThink: A slow thinking framework for multimodal mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Math‑LLaVA: Bootstrapping mathematical reasoning for multimodal large language models <em>(Rating: 2)</em></li>
                <li>MathVista <em>(Rating: 2)</em></li>
                <li>GeoQA <em>(Rating: 2)</em></li>
                <li>MathVerse: Does your multi‑modal LLM truly see the diagrams in visual math problems? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6564",
    "paper_id": "paper-277272275",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "Solving olympiad geometry (Trinh et al. 2024)",
            "name_full": "Solving olympiad geometry without human demonstrations",
            "brief_description": "A referenced work (Nature 2024) reporting language-model-based approaches that solve high-level olympiad geometry problems without human demonstrations; cited in this survey but not experimentally analyzed here.",
            "citation_title": "Solving olympiad geometry without human demonstrations",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "Olympiad geometry problems (high‑level geometric theorem solving)",
            "puzzle_type": "spatial reasoning / geometric theorem proving",
            "dataset_name": null,
            "prompting_method": null,
            "reasoning_technique": null,
            "internal_representation": null,
            "use_of_external_tool": null,
            "external_tool_description": null,
            "evaluation_metric": null,
            "performance": null,
            "analysis_findings": "Only cited by title in the survey as evidence that LLMs have been applied to complex geometry; the survey does not report methods, metrics, or internal analyses from that work.",
            "ablation_comparison": null,
            "limitations": "No details in this survey — the citation is mentioned as a notable result but the survey does not report failure modes or limitations from that paper.",
            "uuid": "e6564.0",
            "source_info": {
                "paper_title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "AtomThink",
            "name_full": "AtomThink: A slow thinking framework for multimodal mathematical reasoning",
            "brief_description": "A multimodal model (fine‑tuned from LLaVA/EMOVA per the survey) designed for diagram/visual mathematical reasoning; the survey notes it attains the highest reported Math‑V (visual math) score among compared multimodal models.",
            "citation_title": "AtomThink: A slow thinking framework for multimodal mathematical reasoning",
            "mention_or_use": "mention",
            "model_name": "AtomThink",
            "model_description": "Multimodal model built on LLaVA/EMOVA, fine‑tuned for mathematical/diagram reasoning; survey table associates it with Long Chain‑of‑Thought prompting and PRM planning variants.",
            "model_size": null,
            "puzzle_name": "Diagram‑based visual math problems (visual/diagram reasoning; includes geometry)",
            "puzzle_type": "spatial reasoning / visual diagram interpretation / geometric problem solving",
            "dataset_name": "Math‑V (visual math benchmark) / MathVista (diagram benchmark) as referenced in the survey",
            "prompting_method": "Long Chain‑of‑Thought (Long CoT) with PRM (planning) as listed in the survey table",
            "reasoning_technique": "Planning‑based reasoning (PRM) / long CoT; survey does not provide implementation details",
            "internal_representation": null,
            "use_of_external_tool": null,
            "external_tool_description": null,
            "evaluation_metric": "Math‑V score (visual math benchmark score) as reported in the survey",
            "performance": "40.5% Math‑V score (survey states AtomThink achieved the highest Math‑V score among compared multimodal models)",
            "analysis_findings": "Survey highlights AtomThink's relative lead on the Math‑V metric but does not provide internal analyses (e.g., attention patterns or representation studies) or detailed error breakdowns.",
            "ablation_comparison": null,
            "limitations": "Survey notes that despite AtomThink's top Math‑V score the absolute performance remains modest and that multimodal/diagram reasoning remains a major challenge across models; no model‑specific failure modes are detailed.",
            "uuid": "e6564.1",
            "source_info": {
                "paper_title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Math‑LLaVA",
            "name_full": "Math‑LLaVA (LLaVA finetuned for mathematical reasoning)",
            "brief_description": "A multimodal LLaVA variant fine‑tuned on mathematical datasets to tackle diagram and visual math problems; cited as extending multimodal mathematical reasoning in the survey.",
            "citation_title": "Math‑LLaVA",
            "mention_or_use": "mention",
            "model_name": "Math‑LLaVA",
            "model_description": "LLaVA‑based multimodal model fine‑tuned on math‑specific multimodal corpora; associated in the survey with Foundational Chain‑of‑Thought prompting (F‑CoT).",
            "model_size": null,
            "puzzle_name": "Diagram‑based visual math problems / geometry",
            "puzzle_type": "spatial reasoning / visual diagram interpretation",
            "dataset_name": "MathVista / visual math benchmarks (survey groups Math‑LLaVA with diagram benchmarks)",
            "prompting_method": "Foundational Chain‑of‑Thought (F‑CoT) prompting (as listed in survey tables)",
            "reasoning_technique": null,
            "internal_representation": null,
            "use_of_external_tool": null,
            "external_tool_description": null,
            "evaluation_metric": null,
            "performance": null,
            "analysis_findings": "Survey remarks that Math‑LLaVA extends multimodal reasoning by finetuning on specialized mathematical datasets but provides no per‑task internal analyses or per‑model error modes.",
            "ablation_comparison": null,
            "limitations": "Survey indicates multimodal models (including Math‑LLaVA) still face substantial challenges in diagram/geometry understanding; no Math‑LLaVA‑specific limitations are reported.",
            "uuid": "e6564.2",
            "source_info": {
                "paper_title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GeoQA",
            "name_full": "GeoQA (geometric question answering benchmark)",
            "brief_description": "A dataset/benchmark for geometric question answering and diagram/geometry reasoning, listed in the survey's dataset taxonomy.",
            "citation_title": "GeoQA",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "Geometric question answering (diagram‑based geometry problems)",
            "puzzle_type": "spatial reasoning / geometry",
            "dataset_name": "GeoQA",
            "prompting_method": null,
            "reasoning_technique": null,
            "internal_representation": null,
            "use_of_external_tool": null,
            "external_tool_description": null,
            "evaluation_metric": null,
            "performance": null,
            "analysis_findings": "Survey includes GeoQA in its dataset list for evaluating geometry/visual math but does not report model performances on GeoQA in the body text.",
            "ablation_comparison": null,
            "limitations": "Survey notes general gaps in multimodal and spatial reasoning benchmarks — GeoQA represents a challenging dataset in this space.",
            "uuid": "e6564.3",
            "source_info": {
                "paper_title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MathVista",
            "name_full": "MathVista (diagram‑based visual math benchmark)",
            "brief_description": "A multimodal benchmark for diagram/visual mathematics problems (used to evaluate models' ability to reason about diagrams and geometry), cited in the survey.",
            "citation_title": "MathVista",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "Diagram‑based visual math problems / geometry",
            "puzzle_type": "spatial reasoning / visual diagram interpretation",
            "dataset_name": "MathVista",
            "prompting_method": null,
            "reasoning_technique": null,
            "internal_representation": null,
            "use_of_external_tool": null,
            "external_tool_description": null,
            "evaluation_metric": null,
            "performance": null,
            "analysis_findings": "Survey uses MathVista as an example of visual/diagram benchmarks and notes that multimodal models tend to outperform text‑only models on such benchmarks, though absolute performance remains limited.",
            "ablation_comparison": null,
            "limitations": "Survey emphasizes that visual/diagram benchmarks (like MathVista) expose weaknesses in current LLM multimodal reasoning and that more work is needed on internal verification and multimodal integration.",
            "uuid": "e6564.4",
            "source_info": {
                "paper_title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving olympiad geometry without human demonstrations",
            "rating": 2,
            "sanitized_title": "solving_olympiad_geometry_without_human_demonstrations"
        },
        {
            "paper_title": "AtomThink: A slow thinking framework for multimodal mathematical reasoning",
            "rating": 2,
            "sanitized_title": "atomthink_a_slow_thinking_framework_for_multimodal_mathematical_reasoning"
        },
        {
            "paper_title": "Math‑LLaVA: Bootstrapping mathematical reasoning for multimodal large language models",
            "rating": 2,
            "sanitized_title": "mathllava_bootstrapping_mathematical_reasoning_for_multimodal_large_language_models"
        },
        {
            "paper_title": "MathVista",
            "rating": 2
        },
        {
            "paper_title": "GeoQA",
            "rating": 2
        },
        {
            "paper_title": "MathVerse: Does your multi‑modal LLM truly see the diagrams in visual math problems?",
            "rating": 2,
            "sanitized_title": "mathverse_does_your_multimodal_llm_truly_see_the_diagrams_in_visual_math_problems"
        }
    ],
    "cost": 0.022362,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Mathematical Reasoning and Optimization with Large Language Models
22 Mar 2025</p>
<p>Ali Forootani 
A Survey on Mathematical Reasoning and Optimization with Large Language Models
22 Mar 2025FC0FDF7DB4DB31032C57E73C0D09EF8FarXiv:2503.17726v1[cs.AI]Mathematical ReasoningOptimizationLarge Language Models (LLMs)Chain-of-Thought (CoT) ReasoningTheorem ProvingSymbolic ComputationHybrid Neural-Symbolic MethodsAI-Driven Decision-MakingControl and OptimizationLinear Programming
Mathematical reasoning and optimization are fundamental to artificial intelligence and computational problem-solving.Recent advancements in large language models have significantly enhanced AI-driven mathematical reasoning, theorem proving, and optimization techniques.This survey explores the evolution of mathematical problem-solving in AI, from early statistical learning approaches to the latest deep learningbased and transformer-driven methodologies.We review the capabilities of pre-trained language models and Large Language Models in handling arithmetic operations, complex mathematical reasoning, theorem proving, and structured symbolic computation.A key focus is on how Large Language Models integrate with optimization and control frameworks, including mixed-integer programming, linear quadratic control, and multi-agent optimization strategies.We discuss how Large Language Models assist in problem formulation, constraint generation, and heuristic search, thereby bridging the gap between theoretical mathematical reasoning and practical optimization applications.Additionally, we analyze various enhancement techniques, including Chain-of-Thought reasoning, instruction tuning, and tool-augmented approaches that improve Large Language Model's problem-solving capabilities.Despite these advancements, Large Language Models face challenges in numerical precision, logical consistency, and verification of mathematical proofs.We highlight emerging trends in hybrid neural-symbolic reasoning, structured prompt engineering, and multistep self-correction mechanisms aimed at improving Large Language Model performance in mathematical and optimization tasks.Future research should focus on refining interpretability, integrating domainspecific solvers, and ensuring the robustness of AIdriven decision-making frameworks.This survey provides a comprehensive review of the current landscape and future directions of mathematical reasoning and optimization with Large Language Models, offering insights into their potential applications across engineering, finance, and scientific research.</p>
<p>I. Introduction</p>
<p>Mathematics serves as a cornerstone of human cognition, underpinning a vast array of disciplines, including the natural sciences, engineering, medicine, finance, computer Ali Forootani is with Helmholtz Centre for Environmental Research -UFZ, Permoserstraße 15, 04318 Leipzig, Germany, email: ali.forootani@ufz.de,aliforootani@ieee.org.</p>
<p>science, and the social sciences.In the field of natural language processing (NLP), the development of computational models capable of autonomously solving mathematical word problems has been an area of significant research interest since the early 1960s [1]- [4].This pursuit is not merely about solving arithmetic and algebraic expressions but also about advancing general reasoning mechanisms, which are fundamental to achieving artificial general intelligence (AGI) [5].By tackling the complexities of mathematical reasoning, researchers aim to push the boundaries of AI systems' logical deduction capabilities and their ability to understand and manipulate symbolic knowledge.</p>
<p>Early approaches to solving mathematical word problems primarily relied on statistical learning techniques [6]- [9].Over time, more sophisticated methodologies emerged, incorporating semantic parsing and structured representations [7], [10]- [13].These innovations laid the foundation for modern deep learning-based mathematical problemsolving, where neural networks have become a powerful tool for automating complex mathematical reasoning tasks [5], [14].</p>
<p>In recent years, the field has witnessed unprecedented advancements, largely driven by the rise of powerful pretrained language models (PLMs) and large language models (LLMs) [15], [16].These models, including BERT [17], RoBERTa [18], BART [19], GPT-1 [20], and GPT-2 [21], have demonstrated remarkable capabilities in learning from large-scale textual corpora, thereby acquiring general linguistic and numerical reasoning skills.</p>
<p>To enhance the mathematical reasoning abilities of PLMs, researchers have explored various techniques, including pre-training and fine-tuning on specialized mathematical datasets [22]- [24].Several models, such as Gen-BERT [23], NF-NSM [24], MathBERT [25], and LISA [26], integrate numerical data and mathematical expressions into their training objectives, enhancing their ability to perform symbolic and numerical reasoning.Moreover, architectural innovations such as specialized loss functions and task-specific modules have been proposed to further refine PLM-based mathematical comprehension [27]- [30].</p>
<p>The emergence of large-scale LLMs, exemplified by GPT-4 [31], has significantly transformed mathematical problem-solving.These models have demonstrated remarkable generalization across various NLP tasks, benefiting from key techniques such as in-context learning [32]- [34] and instruction learning [35], [36].Notably, research by Wang et al. [37] suggests that LLMs with over 100 billion parameters-such as GPT-3 (175 billion) [33] and PaLM (540 billion) [38]-can effectively solve complex mathematical problems through chain-of-thought (CoT) reasoning when provided with well-structured demonstrations [15].</p>
<p>Advancements in CoT methodologies have further improved the reasoning capabilities of LLMs, leading to the development of structured reasoning techniques, programmatic reasoning frameworks, and dynamic CoT models [39]- [42].Additionally, tool-augmented models that integrate external symbolic manipulation systems have been proposed to enhance mathematical problem-solving performance [43]- [45].Such approaches highlight the potential of combining neural and symbolic reasoning to achieve more robust AI systems capable of higher-order mathematical cognition.</p>
<p>Several surveys have reviewed different aspects of mathematical reasoning in LLMs.Notably:</p>
<p>• [46] provides a comprehensive survey of deep learning methodologies for mathematical reasoning (DL4Math).</p>
<p>• [47] explores general reasoning mechanisms in language models, covering arithmetic, commonsense, logical, symbolic, and multi-modal reasoning (LM4Reasoning).</p>
<p>• [48] presents an analytical review of chain-of-thought (CoT) reasoning methodologies.• [49] and [50] offer surveys on pre-trained language models (PLMs) and LLMs, respectively.</p>
<p>• [51] reviews multi-modal LLMs for mathematical reasoning (MMLM4Math).Overall, the fusion of deep learning techniques, structured reasoning approaches, and tool-augmented models continues to drive rapid progress in AI-driven mathematical problem-solving.While existing methods have demonstrated impressive performance, fundamental challenges remain, particularly in areas such as formal proof generation, theorem proving, and real-world mathematical applications.As research continues to push the boundaries of AI's mathematical reasoning capabilities, future advancements will likely be characterized by improved interpretability, robustness, and the seamless integration of symbolic and neural reasoning paradigms.</p>
<p>A. Bridging Mathematical Reasoning and Optimization with LLMs</p>
<p>Mathematical reasoning forms the foundation of logical inference, problem-solving, and numerical computation in artificial intelligence [52].Large language models (LLMs) have demonstrated remarkable capabilities in handling mathematical reasoning tasks, including theorem proving, algebraic manipulation, and structured numerical problem-solving.Through advancements such as Chainof-Thought (CoT) prompting, programmatic reasoning, and tool-augmented inference, LLMs have shown substantial progress in solving complex mathematical problems [53].However, the transition from theoretical reasoning to practical applications, such as optimization and control, requires an additional layer of adaptability where models can not only deduce but also generate and refine solutions iteratively.</p>
<p>Optimization and control problems often demand a structured approach where solutions must satisfy strict mathematical constraints while achieving predefined objectives.In scenarios such as mixed-integer programming (MIP) and linear programming (LP), traditional solvers rely on well-established algorithms, including branch-andbound methods and gradient-based optimization.While LLMs may not replace these solvers outright, they can significantly enhance the modeling process by formulating problem statements, generating initial constraints, and suggesting heuristic improvements [54].By leveraging their extensive pretraining in symbolic mathematics and structured problem-solving, LLMs can aid domain experts in defining well-posed optimization problems and verifying computational solutions.</p>
<p>A crucial advantage of LLMs in this domain lies in their ability to process natural language descriptions of complex optimization scenarios and convert them into structured mathematical formulations [55].For instance, in the context of battery energy storage system (BESS) optimization, an LLM can help translate high-level policy constraints-such as cost minimization, peak shaving, or energy arbitrage-into a solvable mathematical model.Similarly, in control applications like linear quadratic regulation (LQR), LLMs can assist in deriving optimal control laws by structuring the necessary state-space representations and computing feedback gains iteratively.</p>
<p>By integrating mathematical reasoning with optimization and control strategies, LLMs present an opportunity to bridge the gap between abstract theoretical models and real-world engineering applications [56].Their ability to automate certain aspects of model formulation, constraint generation, and heuristic search makes them valuable tools for decision-making in complex systems.Future research should focus on refining their numerical precision, enhancing symbolic computation capabilities, and improving their ability to interface seamlessly with existing optimization solvers.This synergy between mathematical reasoning and optimization will pave the way for more intelligent, interpretable, and efficient decision-support systems in engineering and scientific domains.</p>
<p>This paper is organized as follows: In Section II, we introduce the types of mathematical tasks that motivate the use of language models.Section III presents pre-trained language model (PLM)-based approaches for mathematical reasoning, with a particular focus on Non-Autoregressive Language Models discussed in Section III-B.Section IV explores large language model (LLM)-based methods tailored for mathematical reasoning.In Section V, we compare various mathematical LLMs, followed by a performance analysis in Section VI.Section VIII investigates the application of LLMs to time series analysis, outlining the associated challenges, methodologies, and future directions.A technical comparison of commonly used LLMs is provided in Section IX.</p>
<p>Section X discusses the role of LLMs in optimization and control.Section XI highlights open challenges and outlines potential research directions.Finally, conclusions are drawn in Section XII.</p>
<p>II. Mathematical Tasks</p>
<p>In this section, we categorize mathematical tasks into two primary domains: mathematical calculation and mathematical reasoning.</p>
<p>A. Mathematical Calculation</p>
<p>The emergence of LMs has significantly influenced computational research, particularly in performing arithmetic operations.Early LMs exhibited only rudimentary arithmetic capabilities, encoding numbers as mere textual representations.However, as these models evolved, they demonstrated the ability to develop arithmetic skills through fine-tuning on specialized datasets, even without explicitly designed numerical representations [57].</p>
<p>1) Arithmetic Representation: Initially, numerical values were often omitted, oversimplified, or treated as ordinary text tokens, leading to suboptimal performance on mathematical tasks.For instance, BERT performs five times worse when the expected answer is a numerical value rather than a textual span in the DROP benchmark [58].This limitation highlighted the necessity of refining how LMs process and represent numbers.</p>
<p>Recent research has proposed several techniques to enhance numerical representation within LMs:</p>
<p>• GenBERT [23] tokenizes numbers at the digit level and fine-tunes on arithmetic word problems.[60] and [61] integrate digit embeddings into a single embedding that represents entire numbers more effectively.</p>
<p>• In [62] a method is proposed to leverage digit-RNN and exponent embeddings, emphasizing the exponent while disregarding the mantissa for improved numerical processing.</p>
<p>• GOAT [36] introduced Consistent Tokenization to enhance the numerical representation consistency within LMs.These techniques have collectively improved the way LMs interpret, store, and manipulate numerical values, significantly boosting their effectiveness in arithmetic reasoning.</p>
<p>2) Arithmetic Calculation: The ability of LMs to perform arithmetic computations has been a widely studied topic.Early research primarily focused on fundamental operations such as addition and subtraction [63], [64], whereas more recent studies have expanded into complex operations like multi-digit multiplication [65].</p>
<p>Key findings in arithmetic calculation include:</p>
<p>• In [66] arithmetic capabilities across various LMs, including GPT-4 [31], Galactica [67], and LLaMA [68] hase been evaluated.• In [69] has been demonstrated that specialized prompt engineering can significantly improve addition accuracy, though multiplication remains challenging for numbers exceeding seven digits.</p>
<p>• In [70] length generalization has been investigated in arithmetic using relative position embeddings and training set priming.</p>
<p>• ScratchpadGPT [71] leveraged Chain-of-Thought (CoT) prompting to enhance addition accuracy.• In GOAT [36] supervised instruction tuning has been applied to improve performance on large-integer arithmetic tasks.</p>
<p>• MathGLM [35] decomposed complex arithmetic expressions into simpler subproblems, enhancing model precision.These advancements illustrate the growing sophistication of LMs in arithmetic computations and their potential applications in computational mathematics.</p>
<p>B. Mathematical Reasoning</p>
<p>Mathematical reasoning is a critical capability in artificial intelligence, enabling models to understand, explain, and solve complex mathematical problems.Research in this domain has focused on two primary areas: math problem-solving and theorem proving.</p>
<p>1) Math Problem Solving: Math problem-solving requires LMs to apply logical reasoning and computational models to solve mathematical problems at various levels of complexity.Generally, mathematical problems fall into two categories:</p>
<p>• Contextualized problems: These involve understanding a given scenario, translating it into a mathematical problem, and generating a step-by-step solution.</p>
<p>• Direct mathematical problems: These require direct computation of a solution without the need for additional contextual interpretation.Several notable advancements in this domain include:</p>
<p>• MathPrompter [53] utilizes GPT-3 DaVinci for solving mathematical word problems (MWPs).</p>
<p>• In [72] Rejection Sampling Fine-Tuning (RFT) has been proposed to enhance mathematical reasoning capabilities in LMs.</p>
<p>• MetaMath [73] generates synthetic mathematical problems to facilitate self-improvement.</p>
<p>• WizardMath [74] applies evolutionary instruction tuning to refine LM's mathematical reasoning skills.</p>
<p>• MathAttack [75] evaluates the robustness of LMs against adversarial examples in mathematical problem-solving.</p>
<p>• LLEMMA [76] is an open-source LM designed specifically for mathematical reasoning.These models illustrate the ongoing effort to refine LM's ability to reason mathematically and solve increasingly complex problems.</p>
<p>2) Theorem Proving: Theorem proving (TP) is a challenging AI task that involves verifying mathematical statements using formal logic and structured reasoning.Recent research has investigated the ability of LMs to engage in theorem proving by leveraging deep learning techniques and formal proof verification systems.</p>
<p>Key advancements in this area include:</p>
<p>• GPT-f [77] generates proof strategies using deep learning-based methods.</p>
<p>• In [78] incorporating informal proofs into TP models to enhance theorem reasoning has been proposed.• NaturalProofs [79] compiled a comprehensive corpus for evaluating theorem-proving models.</p>
<p>• DeepMath [80] and INT [81] assess the generalization capabilities of LMs in theorem proving.</p>
<p>• CoqGym [82] provides a dataset of human-written proofs to aid LM training in formal theorem proving.</p>
<p>• ASTactic [82] autonomously generates proof strategies using deep learning.</p>
<p>• LLEMMA [76] explores auto-formalization and tactic prediction for mathematical proof generation.Despite promising advancements, a significant challenge remains: hallucination in neural text generation [83], [84].Errors in logical reasoning can accumulate across proof steps, leading to incorrect conclusions.This highlights the urgent need for robust verification mechanisms to ensure mathematical rigor in AI-generated proofs.</p>
<p>Mathematical reasoning and arithmetic problem-solving represent crucial steps in improving LMs' structured reasoning capabilities.While current research continues to push the boundaries of AI-driven mathematical understanding, future advancements will likely focus on interpretability, error reduction, and the seamless integration of symbolic and neural reasoning approaches.</p>
<p>III. Pre-trained Language Models (PLMs)-Based Approaches for Mathematical</p>
<p>Reasoning PLMs, built on the Transformer architecture [85], have profoundly influenced Natural Language Processing (NLP) through the pre-training and fine-tuning paradigm.PLMs have been extensively utilized in text generation [86] and can be broadly categorized into two major types: Autoregressive LMs (ALMs), such as GPT-1 [20] and T5 [87], and Non-Autoregressive LMs (NALMs), including BERT [88] and RoBERTa [18].These architectures have laid the groundwork for advanced mathematical reasoning models, offering new capabilities in symbolic manipulation, theorem proving, and arithmetic problem-solving.</p>
<p>A. Autoregressive Language Models</p>
<p>Autoregressive Language Models (ALMs) generate text in a sequential manner, where each token is predicted based on the preceding tokens.These models are primarily categorized into two types: causal decoders, such as GPT-1 [20] and GPT-2 [21], and encoder-decoder architectures, exemplified by T5 [87].These architectures have significantly contributed to advancements in mathematical reasoning within Pretrained Language Models (PLMs).</p>
<p>Several research efforts have focused on adapting autoregressive models to improve mathematical reasoning capabilities:</p>
<p>• GPT-f [77] explores automated theorem proving, demonstrating that large-scale language models can generate formal proofs, thereby showcasing their potential for logical deduction.• The Expression-Pointer Transformer (EPT) [89] addresses expression fragmentation issues, significantly enhancing equation-solving abilities in symbolic mathematical tasks.• Generate &amp; Rank [90] introduces a ranking-based post-processing mechanism that refines the quality of generated solutions for mathematical word problems (MWPs).</p>
<p>• THOR [91] integrates theorem proving within language models, leveraging logical reasoning to improve deductive capabilities.• HyperTree Proof Search (HTPS) [92] optimizes proof search strategies by incorporating prior results, surpassing GPT-f in theorem-proving efficiency and accuracy.</p>
<p>• Galactica [67] enhances performance on mathematical reasoning benchmarks, outperforming conventional PLMs on datasets such as MMLU [93] and MATH [94] through improved pretraining and fine-tuning techniques.The performance of autoregressive PLMs in mathematical reasoning has been further refined through the development of specialized datasets and fine-tuning methodologies:</p>
<p>• MATH-PLM [94] underscores the challenge of scaling PLMs for mathematical reasoning and provides benchmark datasets that serve as rigorous evaluation tools.</p>
<p>• LISA [26] constructs theorem-proving models by leveraging formal proof structures, thereby strengthening structured logical reasoning within PLMs.</p>
<p>• PACT [95] introduces self-supervised learning techniques to enhance proof-term generation, improving the interpretability and accuracy of mathematical proofs.</p>
<p>• Minerva [96] fine-tunes PLMs on high-quality technical mathematical content, achieving state-of-theart (SOTA) performance on various mathematical reasoning tasks.</p>
<p>• LIME [97] proposes a novel pretraining strategy that induces mathematical reasoning biases, leading to superior performance compared to conventional transformer-based models.The success of autoregressive LMs in solving mathematical problems can largely be attributed to their capacity to model complex sequential dependencies.This makes them particularly well-suited for theorem proving, symbolic manipulation, and structured logical inference tasks.</p>
<p>B. Non-Autoregressive Language Models</p>
<p>Non-Autoregressive LMs (NALMs) differ from their autoregressive counterparts by generating multiple tokens simultaneously rather than sequentially.This parallelism significantly reduces dependency on previously generated tokens, leading to improved efficiency.Models such as BERT [88] and RoBERTa [18] employ masked token prediction, allowing them to learn contextual representations more effectively, making them well-suited for NLP tasks that require robust comprehension rather than generative capabilities.</p>
<p>In mathematical reasoning, fine-tuned variants of BERT have demonstrated remarkable performance:</p>
<p>• Aristo [22] adapts BERT for solving scientific exam questions, demonstrating its ability to process structured reasoning problems.</p>
<p>• GenBERT [23] and NF-NSM [24] enhance numerical reasoning by incorporating numerical data into pretraining objectives.• MWP-BERT [98] improves numerical representation in mathematical word problems through numeracyaugmented pretraining.</p>
<p>• MathBERT [25] jointly trains on both textual and mathematical formula representations, improving mathematical comprehension.</p>
<p>• TAGOP [27], MT2Net [28], and DeductReasoner [29] leverage PLMs for advanced mathematical operations and logical reasoning.• BERT-TD [30] applies contrastive learning to improve mathematical word problem pattern recognition.Although NALMs are not inherently designed for generative tasks, they excel in mathematical comprehension due to their bidirectional encoding capabilities.By leveraging masked language modeling and contextual token embeddings, these models can capture intricate mathematical structures and improve equation parsing accuracy.</p>
<p>C. Discussion and Future Directions</p>
<p>The adoption of PLM-based approaches in mathematical reasoning has yielded significant progress, particularly in theorem proving, arithmetic calculations, and mathematical problem-solving.However, several challenges remain:</p>
<p>• Numerical Precision: PLMs often struggle with exact arithmetic computations due to floating-point representation errors.structured reasoning modules to ensure mathematical rigor.As PLMs continue to evolve, their role in automating theorem proving and mathematical modeling is expected to expand, pushing the boundaries of artificial intelligence in formal reasoning.</p>
<p>IV. LLM-Based Approaches for Mathematical Reasoning</p>
<p>LLMs have revolutionized text processing and generation, extending their capabilities to mathematical reasoning [31], [68].Since mathematics follows structured linguistic and symbolic rules, LLMs capable of understanding these rules serve as powerful tools for problemsolving in various mathematical domains [74], [76].Existing research in LLM-based mathematical reasoning can be broadly categorized into four key areas: instruction learning, tool-based methods, fundamental Chain-of-Thought (CoT) techniques, and advanced CoT methodologies.</p>
<p>A. Instruction Learning</p>
<p>Instruction learning improves mathematical reasoning in LLMs through structured task formulation, supervised fine-tuning, and in-context learning.</p>
<p>Instruction Building involves generating structured explanations and guidance to enhance model capabilities.Auto-explanation [99] enables LLMs to construct selfexplanatory mathematical datasets, improving problem comprehension.RefAug [100] leverages GPT-4o to augment mathematical reasoning data, enriching training samples.Evol-Instruct [101] introduces an iterative refinement mechanism where LLMs generate and refine their own instructional prompts, further enhanced by Wizard-Math's [74] RLEIF optimization method.EURUS [102] integrates CoT [15] with external tools, optimizing reasoning paths using Direct Preference Optimization (DPO) [103] and Noise Contrastive Alignment (NCA) [104].</p>
<p>Instruction Tuning fine-tunes LLMs to align outputs with human intent [105].MathGLM [35] and GOAT [36] improve arithmetic operations through structured tokenization, while Calculon [65] employs digit decomposition for numerical precision.PaLM 2-L-Math [106] demonstrates that combining solution re-ranking with majority voting enhances performance in mathematical reasoning.LLEMMA [76] pre-trains on mathematical corpora and integrates a Python interpreter to execute symbolic computations.More recent LLMs such as Qwen2.5-MATH[107] and InternLM-Math [108] employ iterative fine-tuning and reward modeling to refine mathematical understanding and improve accuracy.</p>
<p>In-Context Learning (ICL) enhances mathematical reasoning by presenting task examples during inference without modifying model parameters [33].ScratchpadGPT [71] improves multi-step calculations by generating intermediate reasoning steps.Codex-math [44] fine-tunes LLMs on code-based mathematical tasks for automated problem-solving.LAMBADA [109] introduces backward reasoning, guiding LLMs in reverse problem-solving, while LogicSolver [110] embeds algebraic logic into in-context prompts to enhance interpretability.</p>
<p>B. Tool-Based Methods</p>
<p>LLMs can augment their mathematical reasoning abilities by integrating symbolic solvers and programming tools [111], [112].</p>
<p>Single-tool methods focus on integrating LMs with a specific symbolic reasoning framework.SymbLLM [45] enhances mathematical word problem (MWP) solving by leveraging symbolic solvers, while LeanReasoner [113] incorporates Lean theorem proving to ensure logical consistency.PAL [43] improves problem-solving by generating Python programs as intermediate reasoning steps.</p>
<p>Multi-tool approaches extend LLM's capabilities by enabling interactions with multiple external tools.Toolformer [111] equips LLMs with APIs for calculators, search engines, and symbolic computation systems.ART [114] expands on this by integrating automated retrieval-based tool selection, dynamically choosing the most appropriate tool for each problem.</p>
<p>C. Fundamental Chain-of-Thought (CoT) Methods</p>
<p>Chain-of-Thought (CoT) reasoning enhances LLM's performance in mathematical tasks by decomposing problems into step-by-step logical sequences [15].</p>
<p>Auto-CoT [41] automatically clusters representative problems to generate effective reasoning examples, reducing reliance on manual prompt engineering.Rejection Sampling Fine-Tuning (RFT) [72] refines solution paths by discarding incorrect reasoning chains through rejection sampling.PromptPG-CoT [40] optimizes CoT prompts using reinforcement learning (RL), improving generalization across diverse mathematical tasks.</p>
<p>D. Advanced Chain-of-Thought (CoT) Methods</p>
<p>Verify-based methods enhance mathematical reasoning by incorporating self-verification mechanisms.VerifyCoT [115] ensures deductive consistency, while SSC-CoT [116] validates intermediate steps to prevent cascading reasoning errors.</p>
<p>Ensemble-based approaches improve accuracy by aggregating multiple reasoning trajectories.Self-Consistency [37] selects the most frequent solution among multiple sampled responses, while MCR [117] uses majority voting across distinct reasoning paths.</p>
<p>Planning-based methods organize reasoning hierarchically for structured problem-solving.Tree-of-Thought (ToT) [118] structures problem-solving as a tree-like process, exploring multiple reasoning pathways.Graph-of-Thought (GoT) [119] extends this concept to graph-based reasoning, capturing complex interdependencies between logical steps.</p>
<p>Self-correction methods introduce iterative feedback loops to refine responses.STaR [120] and CoRe [121] employ Monte Carlo Tree Search (MCTS) to iteratively improve generated solutions.OmegaPRM [122] applies automated supervision to enhance Gemini's mathematical reasoning capabilities.</p>
<p>Socratic Teaching Methods [123] refine mathematical understanding through structured questioning.Socratic Models (SMs) [124] integrate dialogue-based learning, enabling LLMs to explore mathematical concepts interactively.</p>
<p>E. Future Directions</p>
<p>While LLM-based mathematical reasoning has made remarkable progress, several challenges remain:</p>
<p>• Robust Verification: Ensuring LLMs generate provably correct mathematical solutions remains an open problem [125].• Handling Complex Proofs: Current models struggle with multi-step logical deductions in formal theorem proving [126].• Hybrid Symbolic-Neural Methods: Combining symbolic solvers with LLMs could enhance mathematical accuracy and reliability [127].Future research should focus on improving reasoning consistency [128], integrating interactive problem-solving frameworks [129], and enhancing mathematical comprehension through hybrid neural-symbolic approaches [127].</p>
<p>V. Comparison of Mathematical Large Language Models</p>
<p>Mathematical LLMs have evolved significantly as seen in benchmarks like MATH [213] and geometry reasoning [214], leveraging various architectural designs, training methodologies, reasoning strategies, and tool integrations to enhance mathematical problem-solving capabilities [215].</p>
<p>However, open-source models lag behind proprietary ones like GPT-4 [216] and Gemini-Ultra [217].To bridge this gap, Deepseek [218] introduced a domain-specific model (spmath) trained on the DeepSeekMath Corpus, a high-quality dataset of 120B math tokens curated from Common Crawl using a fastText-based classifier [219].</p>
<p>This model, initialized with DeepSeek-Coder-Base-v1.5 7B [220], outperforms Minerva 540B [221] on MATH [213] and exhibits strong multilingual performance on CMATH [222] and AGIEval [223].Pre-training on code enhances mathematical reasoning, supporting the idea that coding skills improve logic-based problem-solving.spmath-Instruct 7B is fine-tuned using chain-of-thought [224], program-of-thought [225], [226], and tool-integrated reasoning [227], achieving results comparable to 70B open-source instruction-tuned models.Additionally, the Deepseek introduced Group Relative Policy Optimization (GRPO), a RL method that eliminates the critic model, significantly reducing training costs while improving performance on GSM8K (82.9% → 88.2%) and MATH (46.8% → 51.7%).A unified paradigm is presented in [218] to connect various RL-based fine-tuning methods, such as  Rejection Sampling Fine-Tuning (RFT) [72], Direct Preference Optimization (DPO) [228], and Proximal Policy Optimization (PPO) [229], with extensive experiments exploring online vs. offline training, outcome vs. process supervision, and single-turn vs. iterative RL.Deepseek also constructs the DeepSeekMath Corpus using an iterative fastText-based classifier, filtering 35.5M math-related web pages while preventing contamination from evaluation datasets like GSM8K [230] and MATH [213].Evaluations show spmath-Base 7B surpasses Mistral 7B [231] and Llemma 34B [232] in mathematical reasoning, excelling in theorem proving on miniF2F [233] and performing well in general reasoning (MMLU [234], BBH [235]) and coding (HumanEval [236], MBPP [237]).After supervised fine-tuning with 776K mathematical instructions, spmath-Instruct 7B outperforms all 7B models and rivals 70B instruction-tuned models such as InternLM2-Math 20B, WizardMath 7B [238], MetaMath 70B [239], and MAm-moTH 70B [240].
GPT-f [77] - Pre-T Symbolic-S ✗ ✗ ✗ ✗ LISA [26] - Pre-T Symbolic-S ✗ ✗ ✗ ✗ MATH-PLM [94] GPT-2, GPT-3 Pre-T ✗ F-CoT ✗ ✗ ✗ Minerva [96] PaLM Pre-T ✗ ✗ ✗ ✗ ✗ MWP-BERT [98] BERT Pre-T ✗ ✗ ✗ ✗ ✗ ControlMath [130] LLaMA-2, Mistral FT ✗ F-CoT ✗ ✗ ✗ EURUS [102] Mistral, CodeLLaMA FT ✗ F-CoT DPO, KTO, NCA ✗ ✓ MathGLM [35] GLM FT ✗ ✗ ✗ ✗ ✓ WizardLM [101] LLaMA FT ✗ ✗ ✗ ✗ ✓ Wizardmath [74] LLaMA-2 FT ✗ ✗ PPO ✗ ✓ PaLM 2-L-Math [106] PaLM-2-L FT ✗ F-CoT ✗ ✗ ✗ Codex-math [44] Codex Prompt Program ✗ ✗ ✗ ✗ Toolformer [111] GPT-J FT Calculator ✗ ✗ ✗ ✗ LeanReasoner [113] CodeLLaMA FT Symbolic-S ✗ ✗ ✗ ✓ LINC [131] StarCoder+ Prompt Symbolic-S F-CoT ✗ ✗ ✓ MuMath-Code [132] Llama-2 FT Program ✗ ✗ ✗ ✓ MAmmoTH [133] - FT Program F-CoT ✗ ✗ ✗ InternLM-Math [108] InternLM2 Pre-T, FT Program F-CoT PPO ✗ ✓ Qwen-Math [107] - Pre-T, FT Program F-CoT PPO ✗ ✓ PAL [43] - Prompt Program F-CoT ✗ ✗ ✓ CoT [15] - Prompt ✗ F-CoT ✗ ✗ ✓ PromptPG-CoT [40] - Prompt ✗ ✗ ✗ ✗ ✓ BoostedPrompt [134] - Prompt ✗ F-CoT ✗ ✗ ✗ ToT [118] - Prompt ✗ A-CoT ✗ ✗ ✓ PoT [39] - Prompt Program F-CoT ✗ ✗ ✓ Self-check [135] - Prompt ✗ A-CoT ✗ ✗ ✓ Self-Consistency [37] - Prompt ✗ A-CoT ✗ ✗ ✓ Diversity-of-Thought [136] - Prompt ✗ A-CoT ✗ ✗ ✗ RAP [137] - Prompt ✗ A-CoT ✗ ✗ ✓ LATS [138] - Prompt ✗ A-CoT ✗ ✗ ✗ LLM+P [139] - Prompt Symbolic-S A-CoT ✗ ✗ ✓ LLM+DP [140] - Prompt Symbolic-S A-CoT ✗ ✗ ✓ ISR-LLM [141] - Prompt ✗ A-CoT ✗ ✗ ✓ MCR [117] - Prompt ✗ A-CoT ✗ ✗ ✓ Rank-verifier [142] - Prompt ✗ A-CoT ✗ ✗ ✗ GRACE [143] - Prompt ✗ A-CoT ✗ ✗ ✓ Reflexion [144] - Prompt ✗ A-CoT ✗ ✗ ✓ CR [145] - Prompt ✗ A-CoT ✗ ✗ ✓ Step-Plan [146] - Prompt ✗ A-CoT ✗ ✗ ✓ STaR [120] GPT-J FT ✗ A-CoT ✗ ✗ ✓ V-STaR [147] LLaMA2, CodeLLaMA FT ✗ A-CoT DPO ✗ ✗ Quiet-STaR [148] Mistral Pre-T ✗ A-CoT ✗ ✗ ✗ REFT [149] Galactica, CodeLLaMA FT ✗ A-CoT PPO ✗ ✓ SCoRe [150] Gemini FT ✗ A-CoT SCoRe ✗ ✗ HGS-PRM [151] LLaMA2, WizardMath FT ✗ A-CoT PPO ✗ ✗ MCTSr [152] LLaMa-3 FT ✗ A-CoT ✗ ✗ ✓ CoRe [121] GPT-J, DeBERTa-large FT ✗ A-CoT ✗ ✗ ✓ Marco-o1 [153] Qwen2 FT ✗ A-CoT ✗ ✗ ✓ Step-DPO [154] Qwen2 FT ✗ A-CoT DPO ✗ ✓ Flow-DPO [155] LLaMA3, Phi-3 FT ✗ A-CoT DPO ✗ ✗ OmegaPRM [122] Gemini Pro, Gemma2 FT ✗ A-CoT PRM ✗ ✗ SocraticLLM [156] Qwen2 FT ✗ A-CoT ✗ ✗ ✓ MATHDIAL [157] T5 FT ✗ A-CoT ✗ ✗ ✓ QwQ [158] Qwen2.5 Pre-T ✗ Long CoT ✓ ✗ ✓ o1 [159] - Pre-T ✗ Long CoT ✓ ✓ ✗ GPT-4o [160] - Pre-T ✓ A-CoT ✓ ✓ ✗ GPT-4V [31] - Pre-T ✓ A-CoT ✓ ✓ ✗ Qwen2-VL [161] Qwen2-VL Pre-T ✗ F-CoT ✗ ✓ ✓ LLaVA-o1 [162] LLaVA FT ✗ Long CoT ✗ ✓ ✓ Gemini [163] Gemini Pre-T ✓ A-CoT ✓ ✓ ✗ GLM-4V [164] ChatGLM Pre-T ✓ A-CoT ✓ ✓ ✗ AtomThink [165] LLaVA, EMOVA FT ✗ Long CoT PRM ✓ ✓ Math-LLaVA [166] LLaVA FT ✗ F-CoT ✗ ✓ ✓ M-STAR [167] MiniCPM-V FT ✗ A-CoT PRM ✓ ✗ UnAC [168] GPT-4V, Gemini, LLaVA Prompt ✗ F-CoT ✗ ✓ ✗
Table I provides a comparative overview of multiple mathematical LLMs, highlighting key aspects such as training methods, reasoning techniques, RL applications, multi-modal capabilities, and accessibility.This section explores the observed trends, major advancements, and existing limitations in the domain of mathematical LLMs.</p>
<p>A. Chain-of-Thought (CoT) Reasoning</p>
<p>One of the most critical advancements in mathematical reasoning for LLMs is the incorporation of Chainof-Thought (CoT) prompting [15].CoT enables models to break down complex problems into intermediate steps, improving their ability to solve mathematical and logical problems.Foundational CoT (F-CoT) is a common feature among models such as MATH-PLM [94], ControlMath [130], and EURUS [102], which helps them generate structured multi-step solutions.More advanced forms of CoT reasoning (A-CoT) are implemented in models like ToT [118], Self-check [135], and Reflexion [144], allowing them to refine their solutions iteratively.Some recent models, such as QwQ [158], o1 [159], and LLaVA-o1 [162], employ Long CoT, which extends reasoning over a longer context for more complex mathematical problems.</p>
<p>B. Integration of External Tools</p>
<p>Tool integration is another key differentiator among mathematical LLMs.Some models, such as Toolformer  [111] and LeanReasoner [113], incorporate external symbolic solvers and calculators to enhance their accuracy.Other models, such as Codex-math [44] and MuMath-Code [132], rely on program-based reasoning, allowing them to execute mathematical scripts and verify calculations dynamically.These tool-enhanced approaches generally outperform models that rely purely on neural network inference without external computation.</p>
<p>C. Training Paradigms: Pre-Training, Fine-Tuning, and Prompt Engineering</p>
<p>Mathematical LLMs adopt different training strategies, primarily categorized into Pre-Training (Pre-T), Fine-Tuning (FT), and Prompt Engineering.Pre-trained models, such as Minerva [96], GPT-4o [160], and Gemini [163], are trained on large mathematical datasets before being adapted for specific problem-solving tasks.Finetuned models, such as WizardMath [74] and InternLM-Math [108], undergo domain-specific adaptations to improve their mathematical reasoning skills.Prompt-based models, such as PAL [43] and PoT [39], rely on structured prompt design to guide their reasoning, making them more flexible for zero-shot and few-shot learning.</p>
<p>D. RL for Mathematical Reasoning</p>
<p>RL techniques, particularly Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), have been applied to mathematical LLMs to refine their solution generation process.Models like WizardMath [74] and InternLM-Math [108] leverage PPO to enhance their performance through iterative self-improvement.Additionally, DPO-based models, such as EURUS [102] and V-STaR [147], optimize their outputs based on user preferences and ranking mechanisms.Recent advances, including Step-DPO [154] and Flow-DPO [155], incorporate structured planning into reinforcement learning to refine problem-solving pathways.</p>
<p>E. Multi-Modal Capabilities</p>
<p>While most mathematical LLMs focus on text-based problem-solving, a growing number of models support multi-modal reasoning, integrating text and image inputs to solve visual mathematics problems.Notable multimodal models include GPT-4V [31], Gemini [163], and GLM-4V [164], which incorporate vision-language capabilities for tasks involving diagrams, plots, and mathematical notation recognition.Similarly, Math-LLaVA [166] and AtomThink [165] extend multi-modal reasoning by finetuning on specialized mathematical datasets.However, many models still lack this capability, indicating an area for further improvement.</p>
<p>F. Open vs. Closed Access Models</p>
<p>The accessibility of mathematical LLMs varies significantly.Open-source models such as EURUS [102], Qwen-Math [107], and LINC [131] provide transparency and adaptability, allowing researchers to fine-tune them for specific applications.In contrast, proprietary models such as GPT-4o [160], Gemini [163], and o1 [159] are closedsource, limiting external modifications.The trade-off between openness and performance remains a key consideration in model selection.</p>
<p>Mathematical LLMs continue to advance in reasoning capabilities, tool integration, and training methodologies.While Chain-of-Thought reasoning has become a standard technique for improving accuracy, models differ in their use of external tools, reinforcement learning, and multi-modal support.Additionally, open-source models provide flexibility for customization, whereas closed-source models typically offer superior performance at the cost of accessibility.Future advancements are likely to focus on enhancing multi-modal mathematical reasoning, better tool integrations, and more robust self-improving learning mechanisms to achieve greater accuracy and efficiency in solving complex mathematical problems.</p>
<p>VI. Performance Analysis of Mathematical</p>
<p>Language Models Mathematical LMs have demonstrated varying degrees of success across different mathematical benchmarks.To assess the arithmetic and mathematical capabilities of language models, various datasets and evaluation methods have been proposed.Commonly used benchmarks include MATH [94], GSM8K [142], and AGIEval [192], which test models across different difficulty levels and problem types.Traditional NLP evaluation metrics, such as accuracy and perplexity, are insufficient for mathematical reasoning.Instead, numeric predictions are evaluated using regression metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) [241].Adjusted Perplexity [60] has also been proposed to account for numeric token distributions.</p>
<p>Some benchmarks adopt specialized evaluation strategies.The MATH dataset [94] enforces strict answer formatting for consistency, while AGIEval [192] employs Exact Match and F1 scores for fill-in-the-blank questions.Recent research explores automated assessment methods, including large language model-based scoring [242], to provide more nuanced evaluations of reasoning quality.</p>
<p>Given the complexity of mathematical problem-solving, developing comprehensive evaluation metrics remains an open challenge.Future directions should focus on refining benchmarks to assess multi-step reasoning, logical consistency, and theorem-proving capabilities more effectively.</p>
<p>Table II presents the performance of various text-based and multi-modal LMs on multiple datasets, including MATH, GSM8K, MMLU, ASDiv, AIME, and others.</p>
<p>Among the text-based methods, Codex-math achieves the highest accuracy on the MATH dataset (81.8%), while QwQ leads with a remarkable 90.6%.For GSM8K, MCTSr outperforms other models with an accuracy of 96.6%, followed closely by Diversity-of-Thought (96.3%) and Step-DPO (94.0%).In the AIME dataset, Self-check achieves the highest performance with 81.2%.Similarly, in ASDiv, EURUS attains the best result (93.0%), demonstrating its effectiveness in arithmetic reasoning tasks.</p>
<p>For dataset-specific performance, PoT leads in MathQA (89.1%), and CR achieves the highest accuracy on the FOLIO dataset (98.0%).These results highlight the strengths of specialized models in certain tasks.Meanwhile, InternLM-Math and Qwen-Math show strong overall performance across multiple datasets, particularly with Qwen-Math achieving 91.6% in GSM8K and 82.8% in MMLU.</p>
<p>Multi-modal models, which leverage both textual and visual input, generally perform better than their textbased counterparts.o1 outperforms all models on the MATH dataset with an accuracy of 94.8%, and it also leads in MMLU (92.3%) and AIME (83.3%).GPT-4o also exhibits strong performance in MMLU (88.0%), indicating the advantage of integrating vision and language for mathematical reasoning.Qwen2-VL and AtomThink show competitive results on multimodal datasets, with the latter achieving the highest Math-V score (40.5%).</p>
<p>Overall, the results suggest that newer models incorporating diverse reasoning strategies, self-consistency, and program-of-thought approaches achieve higher accuracy.Future advancements in multi-modal LMs are likely to further improve mathematical reasoning capabilities by integrating structured symbolic processing with deep learning architectures.</p>
<p>VII. Datasets</p>
<p>To train and evaluate the arithmetic and mathematical reasoning abilities of language models, various math word problem (MWP) datasets [7], [94], [142], [191] have been developed.We categorize these datasets into training, benchmark, and augmented datasets (Table III).</p>
<p>A. Training Datasets</p>
<p>Mathematical Calculation datasets extract numerical information from text.Clinical Data [170] contains patient reports with structured attributes, while Scientific Data [60] includes research papers.DoQ [171] captures scalar attribute values for over 350K nouns and verbs.VERBPHYSICS [169] encodes action-object relations, and DROP [58] tests discrete reasoning over adversarial questions.</p>
<p>Math Word Problems datasets include AddSub [6], focusing on addition and subtraction, and SingleOp [10], covering multiplication and division.GSM8K [142] comprises high-quality grade school problems requiring multistep reasoning.Math23k [5] and MATHPILE [178] provide structured equations.To improve semantic parsing, Dol-phin1878 [12] and SingleEq [13] enable equation mapping.Recent datasets like HMWP [175] and MathQA [180] expand problem diversity with multilingual and multiplechoice questions.</p>
<p>Theorem Proving datasets such as Feit-Thompson [243] and IsarStep [185] provide structured theorem proofs.CoqGym [82] and LeanStep [95] integrate proof tactics, while NaturalProofs [79] covers formal and informal mathematics.</p>
<p>B. Benchmark Datasets</p>
<p>Benchmark datasets evaluate mathematical reasoning.MAWPS [188] and Dolphin18K [187] contain diverse MWPs.MATH [94] provides competition-level problems with step-by-step solutions.MGSM [190] extends GSM8K into 10 languages.miniF2F [204] benchmarks neural theorem proving across Lean, Isabelle, and HOL Light.FOLIO [205] offers first-order logic reasoning annotations.</p>
<p>Recent multimodal benchmarks assess vision-language models.MathVista [194] evaluates diagram-based problems, while MMMU-Math [201] and U-Math [203] focus on university-level problem solving.</p>
<p>C. Augmented Datasets</p>
<p>Augmented datasets enrich existing collections.Aggregate [206] extends AllArith with additional perturbed problems.MetaMathQA [73] reformulates mathematical questions using GPT-4.Math5K [208] generates 50,000 new problems.</p>
<p>To enhance reasoning transparency, MathQA-Python [207] converts text to code, and Lila [210] provides Pythonbased solutions.PEN [211] annotates algebraic explanations, while PRM800K [209] labels step-wise mathematical reasoning correctness.MiniF2F+informal [78] and NaturalProofs-Gen [212] introduce informal theorem statements to theorem-proving datasets.</p>
<p>VIII. LLMs for Time Series Analysis:</p>
<p>Challenges, Methods, and Future Directions LLMs, such as GPT-4o and LLaMA [244], have achieved remarkable success in natural language processing.Inspired by this, researchers have started exploring their application in time series analysis for tasks such as forecasting, classification, and anomaly detection [245], [246].By leveraging their strong pattern recognition capabilities, LLMs have been integrated into various domains where time series data play a crucial role.These applications span across finance [247], [248], healthcare [249], [250], traffic forecasting [251], and mobility prediction [252].In finance, LLMs assist in stock market forecasting by analyzing textual and numerical data.In healthcare, LLMs are leveraged for Electrocardiogram classification and electronic health record analysis [249].</p>
<p>However, applying LLMs to time series data presents unique challenges: (1) diverse formats and domains make model training complex, and (2) real-world time series data often exhibit non-stationary properties, leading to concept drift that impedes continuous adaptation [253].</p>
<p>To address these challenges, recent works exploit pretrained LLMs to analyze time series data without training new models from scratch.This section systematically categorizes these efforts into five key strategies: (1) direct querying of LLMs, (2) tokenization techniques for numerical data, (3) prompt engineering, (4) fine-tuning methods, and (5) integration of LLMs with existing time series models.Additionally, we discuss domain-specific applications and propose future research directions.</p>
<p>A. Taxonomy of LLMs in Time Series Analysis</p>
<p>General Pipeline.LLM-based time series analysis can be performed through three main approaches: direct querying, fine-tuning, or integrating LLMs into models.Fine-tuning involves tokenizing time series data, enhancing representation with prompts, and adapting LLMs for downstream tasks.</p>
<p>Direct Query.Works like PromptCast [254] and LLM-Time [255] use zero-shot or few-shot learning by querying LLMs with structured prompts.</p>
<p>Tokenization.Patching techniques [256], [257] and STL decomposition [258] help convert numerical time series data into a format interpretable by LLMs.</p>
<p>Prompt Design.Context-enhanced and soft prompts [259], [260] improve the ability of LLMs to process time series data effectively.</p>
<p>Fine-tuning.Techniques such as LoRA [261] help optimize specific layers of LLMs to adapt them to time series tasks [256], [257].</p>
<p>Model Integration.Some methods use LLMs as feature enhancement tools in multimodal frameworks [249], [262], [263].</p>
<p>Open challenges include:</p>
<p>• Improved tokenization and prompts to better represent time series patterns [264].</p>
<p>IX. Comparison of commonly used LLMs based on their technical specifications</p>
<p>LLMs have evolved significantly, with various models offering different strengths and weaknesses.In this section we compare some famous LLMs including GPT-4 Turbo, LLaMA3, DeepSeek-R1, Mixtral, Claude3, and Gemini1.5.These models differ in terms of architecture, training data, computational efficiency, response accuracy, and suitability for specific applications.Some models prioritize speed and cost-effectiveness, while others focus on reasoning capabilities, multilingual support, or specialized domain knowledge.</p>
<p>A. GPT-4 Turbo (OpenAI)</p>
<p>Pros:</p>
<p>• Best overall reasoning and language understanding.• Supports multi-modal input (text and images).</p>
<p>• Handles up to 128K tokens 3 [268].</p>
<p>• Strong API ecosystem and enterprise adoption.</p>
<p>Cons:</p>
<p>• Closed-source, making fine-tuning difficult.</p>
<p>• Expensive compared to open-source alternatives.</p>
<p>• Some guardrails limit responses to certain queries.</p>
<p>B. LLaMA 2 &amp; 3 (Meta)</p>
<p>Pros:</p>
<p>• Open-source, allowing fine-tuning and self-hosting.</p>
<p>• Efficient performance and competitive results.</p>
<p>• Strong research community support.</p>
<p>Cons:</p>
<p>• Weaker reasoning than GPT-4 Turbo.</p>
<p>• Shorter context windows (4K-32K tokens) [269].</p>
<p>• Requires high-end GPUs for full deployment.</p>
<p>C. DeepSeek-R1 (DeepSeek)</p>
<p>Pros:</p>
<p>• Cost-Effective Development: Achieved performance comparable to GPT-4 at a fraction of the cost.</p>
<p>D. Mixtral (Mistral AI)</p>
<p>Pros:</p>
<p>• Mixture of Experts (MoE) architecture for efficiency.</p>
<p>• Open-source and highly optimized.</p>
<p>• Compact and fast inference speed.</p>
<p>Cons:</p>
<p>• Shorter context length than GPT-4 (4K-32K tokens) [271].• Less adoption in enterprise applications. 3100 tokens are about 60-80 English words E. Claude 3 (Anthropic) Pros:</p>
<p>• Best alignment and ethical AI practices.</p>
<p>• Handles very long documents (up to 200K tokens [272]).• More human-like responses and high creativity.</p>
<p>Cons:</p>
<p>• Slower response times compared to GPT-4 Turbo.</p>
<p>• Closed-source, requiring API access.</p>
<p>F. Gemini 1.5 Pro (Google)</p>
<p>Pros:</p>
<p>• Strong multi-modal capabilities (text, images, and audio).• Deep integration with Google services.</p>
<p>• Very long context windows (up to 2M tokens [273]).</p>
<p>Cons:</p>
<p>• Still catching up to OpenAI in benchmarks.</p>
<p>• Limited open-source versions.Table V summarizes the technical specifications of commonly used LLMs based on context length, strength, and weaknesses.</p>
<p>X. Application of LLms on optimization and control</p>
<p>LLMs in solving control and optimization problems is essential for understanding their strengths, limitations, and potential applications.As these models continue to advance, their capabilities in multi-step reasoning, broad knowledge retrieval, and intuitive code generation make them promising tools for tackling complex computational tasks.However, their limitations-such as logical inconsistencies, sensitivity to prompt variations, and susceptibility to errors-necessitate a rigorous assessment of their reliability in control and optimization scenarios.</p>
<p>A systematic evaluation of LLMs in these domains allows us to determine the extent to which they can autonomously handle problem-solving, adapt to dynamically changing parameters, and refine solutions iteratively.</p>
<p>Moreover, this investigation provides critical insights into how LLMs can complement traditional solvers, optimizing human-AI workflows and enhancing the safety, robustness, and efficiency of solutions in real-world applications.Additionally, by testing these models in structured optimization problems, we may uncover novel solution strategies that lead to improved computational efficiency and innovation in control systems.</p>
<p>To address this objective, we systematically test LLMs across a range of control and optimization tasks, including:</p>
<p>(i) Mixed-Integer Linear Programming (MILP) for Battery Energy Storage Systems (BESS): Optimizing charge/discharge scheduling while minimizing operational costs and adhering to system constraints.</p>
<p>(ii) Linear Quadratic Control (LQC) for water level regulation in interconnected tanks: Evaluating the ability of LLMs to design and analyze optimal control strategies for fluid management.</p>
<p>(iii) Climate dataset analysis: Utilizing LLMs for pattern recognition, forecasting, and decision-making based on historical climate data.</p>
<p>(iv) Multi-Agent Linear Programming Optimization: Exploring how LLMs can assist in coordinating multiple decision-making agents to achieve optimal solutions.</p>
<p>By rigorously testing LLMs in these diverse applications, we aim to identify their strengths and weaknesses in solving complex mathematical optimization and control problems.This research will inform best practices for integrating LLMs into existing computational workflows, ultimately advancing the field of AI-driven optimization and control.</p>
<p>a) Code availability statement: The Python scripts that we use for each example leverage multiple libraries to perform data processing, optimization, and AI-driven analysis.They utilize NumPy [274] and Pandas [275] for numerical and tabular data manipulation, while Matplotlib [276] is used for visualization.Scipy [275] provides numerical optimization functions, particularly for linear programming, and Scikit-learn [277] is employed for preprocessing tasks like scaling.For geospatial analysis, Cartopy [278] is used to map wind data.The asyncio [279]  library enables asynchronous execution, ensuring smooth parallel processing, especially in federated optimization scenarios.The script also incorporates OpenAI's API to generate insights from wind and battery storage data, assisting in decision-making.Furthermore, PuLP [280] is used for mixed-integer linear programming (MILP) to optimize battery charge/discharge schedules dynamically.Finally, AutoGen [281] is used to create multi-agent frameworks for executing linear programming problems asynchronously.In Appendix A we provide the link to the repository containing the codes that are used for the simulation.</p>
<p>A. An example of the LLm application on the BESS</p>
<p>This section details an optimization problem for the operation of a BESS with LLMs.Our objective is to minimize the net operational cost (or maximize net revenue) by scheduling the charging and discharging of a battery over a discrete 10-hour planning horizon.We incorporate fuzzy logic rules to capture realistic decision-making heuristics concerning electricity prices, battery state of charge (SOC), and operational switching considerations.</p>
<p>1) System Overview: We consider a BESS with the following characteristics:</p>
<p>• Battery capacity: SOC max = 100 kWh • Initial SOC: SOC 0 = 50 kWh • Charging efficiency: η = 0.90 • Discharging efficiency: 1 η = 0.90 • Max charge/discharge power:
P max charge = P max discharge = 20 kW
We operate over time steps t = 1, . . ., 10, each representing a one-hour interval.</p>
<p>2) Decision Variables:
• P charge (t0 ≤ SOC(t) ≤ SOC max , ∀t.
b) Power Limits:
0 ≤ P charge (t) ≤ P max charge = 20, ∀t, 0 ≤ P discharge (t) ≤ P max discharge = 20, ∀t.
c) Energy Balance:
SOC(t + 1) = SOC(t) + η • P charge (t) − 1 η • P discharge (t), t = 1, . . . , 9.(1)
We take SOC(1) = SOC 0 = 50 kWh as the initial condition.</p>
<p>d) No Simultaneous Charge/Discharge:
P charge (t) • P discharge (t) = 0, ∀t.
This ensures the battery cannot charge and discharge in the same hour.</p>
<p>C. Objective Function</p>
<p>We wish to minimize the total net cost Z over the 10hour horizon:
min Z = 10 t=1 C t • P charge (t) − R t • P discharge (t) .
The term C t P charge (t) represents the cost of charging, while − R t P discharge (t) represents revenue gained from discharging.</p>
<p>1) Fuzzy Logic Integration: We enrich the classical optimization with fuzzy logic rules to guide or penalize decisions that deviate from human-like strategies.The fuzzy rules help shape how aggressively the system charges or discharges based on electricity prices, battery SOC, and the desire to avoid frequent switching.</p>
<p>1) If the electricity price C t is high, prioritize discharging.2) If the battery SOC is below 30% of SOC max , limit discharging and prioritize charging.3) If C t is low, delay or reduce charging unless SOC is critically low.4) Avoid frequent switching between charging and discharging.</p>
<p>In Table B (Appendix) we have shown an example of how an hour-by-hour schedule might look based on the output of the LLM.Actual solutions may vary depending on specific fuzzy membership functions, penalty weights, and solver nuances.In addition, the LLM model is also plays an important role for the response to the prompt.In this example we used GPT4O LLM model.a) Cost Computation Example: The total cost Z is computed by
Z = 10 t=1 C t • P charge (t) − R t • P discharge (t) .</p>
<p>D. LLM-Driven Fuzzy Logic Control for Water Level Regulation in Interconnected Tanks</p>
<p>Water level regulation in interconnected tank systems is a fundamental problem in control engineering, frequently addressed using classical and modern control techniques such as Proportional-Integral-Derivative (PID) controllers, Model Predictive Control (MPC), or fuzzy logic controllers.</p>
<p>In this section, we integrate a LLM as a decisionmaking component in a fuzzy logic-based control system to optimize the regulation of water levels while minimizing a quadratic cost function.This novel approach leverages natural language processing (NLP)-based reasoning within an LLM to generate real-time control decisions based on predefined fuzzy logic rules.</p>
<p>1) System Model: The dynamics of the interconnected tanks are represented using a state-space formulation, where the water levels in the two tanks form the system state:
x(t) = x 1 (t) x 2 (t) ,(2)
The system evolves according to the linear state-space model:
ẋ(t) = Ax(t) + Bu(t),(3)
where:
A = −0.1 0.05 0.05 −0.1 , B = 0.1 0 ,(4)
The control input u(t) represents the pump flow rate, which is constrained within the range [−u max , u max ], with u max = 0.5.The cost function to be minimized is a quadratic penalty function:
J = Q 1 (x 1 − x 1,desired ) 2 + Q 2 (x 2 − x 2,desired ) 2 + Ru 2 , (5)
where:</p>
<p>• Q 1 = 10, Q 2 = 8 are state penalty weights,</p>
<p>• R = 2 penalizes excessive control effort,</p>
<p>• x 1,desired = 2.0, x 2,desired = 1.5 are the target levels.</p>
<p>2) Fuzzy Logic-Based Control Using LLM: In this scenario the LLM is integrated as an inference engine that follows predefined fuzzy logic rules to determine the control action u(t) at each time step.The control strategy involves:</p>
<p>1) Assessing Deviation: The LLM compares the actual water levels x 1 , x 2 with their desired levels.2) Applying Fuzzy Rules:
• If x 1 &gt; x 1,desired , decrease u. • If x 2 &lt; x 2,desired , increase u.
• Balance the adjustments to avoid oscillations.</p>
<p>3) Minimizing the Cost Function: The LLM aims to select u(t) that optimally reduces deviations while considering energy efficiency.</p>
<p>At each time step t, a prompt (see Algorithm X-D2) is dynamically generated and sent to the LLM (see appendix C).In Algorithm X-D2 we present a general procedure that is used for tank level control.</p>
<p>The LLM generates a response, which is parsed to extract the control input.If parsing fails, a fallback u = 0 is used.</p>
<p>Algorithm 1 LLM-Based Fuzzy Control for Interconnected Tanks</p>
<p>1: Input:
Initial state x 0 = [x 1 , x 2 ] ⊤ , desired state x d = [x 1,desired , x 2,desired ] ⊤ 2: Parameters: 3: System matrices A, B 4: Quadratic cost weights Q 1 , Q 2 , R 5:
Maximum pump flow rate u max 6: Time step ∆t, total time T 7: Initialize: Set t = 0, state x = x 0 , control input u = 0 8: while t &lt; T do 9:</p>
<p>Construct prompt with current state and system parameters 10:</p>
<p>Send prompt to LLM to obtain fuzzy logic-based control input u(t)</p>
<p>11:</p>
<p>Parse the LLM response to extract u(t)</p>
<p>12:</p>
<p>Ensure control input is within bounds:
u(t) ← clip(u(t), −u max , u max ) 13:
Compute state derivative: ẋ = Ax + Bu 14:</p>
<p>Update system state:
x ← x + ẋ • ∆t 15:
(t, x 1 , x 2 , u) for analysis Increment time: t ← t + ∆t 17: end while 18: Output: Time trajectory of states and control inputs The model is simulated over a 10-second horizon, with the LLM making decisions at each step.The summary of the simulation results produced by LLM is shown in Table C.</p>
<p>E. Application of Large Language Models in Time Series dataset analysis</p>
<p>In this section we consider the application of LLMs for Wind and Pressure Data Analysis for renewable energies.We describe the implementation of a climate data analysis framework integrating machine learning techniques for wind speed and pressure assessment in Germany [282].The system processes CMIP6 climate datasets, extracts and interpolates meteorological variables, computes statistical features, and formulates queries for a LLM to analyze patterns and trends.Here we detail the algorithmic structure and highlights how LLMs assist in interpreting climate data for renewable energy applications.Wind energy forecasting and optimal wind turbine placement require an extensive analysis of historical meteorological data.Traditional statistical methods provide limited insights due to their inability to capture complex interactions among different environmental factors.The integration of advanced machine learning techniques, specifically LLMs, offers a promising approach to identifying patterns, anomalies, and correlations in meteorological datasets.The objective here is to: (i) Extract and preprocess wind speed and pressure data from climate datasets.(ii) Compute and analyze statistical features across multiple geographic locations.(iii) Utilize LLMs to generate domain-specific insights for optimal wind turbine placement and energy forecasting.(iv) Assess the effectiveness of LLMs in climate data interpretation and decision-making.</p>
<p>Algorithm 2 outlines the step-by-step procedure for climate data processing and LLM-assisted analysis.</p>
<p>In this scenario the LLM serves as an intelligent assistant, performing the following tasks: (i) Pattern Recognition: Identifies anomalies, trends, and correlations in the data.(ii) Data Interpretation: Converts numerical statistics into actionable insights.(iii) Domain-Specific Suggestions: Offers recommendations for wind farm placement and data augmentation strategies.(iv) Exploratory Analysis: Proposes hypotheses for further study based on statistical distributions.</p>
<p>The advantages are: (i) Automates statistical analysis and interpretation.(ii) Reduces manual effort in data exploration.(iii) Provides contextual recommendations for renewable energy planning.Moreover, challenges are: (i) LLM-generated insights must be validated against domain expertise.(ii) Computational overhead due to large-scale data processing.(iii) Dependency on high-quality input prompts for effective responses.</p>
<p>F. Application of Multi-Agent LLms to solve Linear Programming</p>
<p>Optimization problems, particularly Linear Programming (LP), play a crucial role in logistics, resource allocation, and decision-making.Traditional LP solvers are effective but operate in a static manner without realtime constraint validation or iterative execution based on external feedback.This section presents an AutoGen Multi-Agent LP Optimization framework, which utilizes Algorithm 2 Climate dataset Analysis using LLMs Input: Time series dataset, e.g.CMIP6 climate including wind and pressure datasets, real wind power production Output: LLM-generated insights for energy forecasting</p>
<p>Step 1: Data Acquisition Extract pressure data and wind speed from CMIP6 dataset.</p>
<p>Step 2: Data Preprocessing and Interpolation Interpolate wind speed Interpolate pressure Normalize data using</p>
<p>Step 3: Statistical Feature Extraction Compute location-wise statistics using such as mean, variance, skewness, kurtosis, etc.</p>
<p>Step 4: LLM Integration for Data Interpretation Formulate prompt incorporating computed statistics.</p>
<p>Send prompt to LLM model.Retrieve and process LLM-generated insights.</p>
<p>Step</p>
<p>5: Interpretation and Decision Support</p>
<p>Analyze LLM response for wind energy site selection.Identify temporal patterns for optimal energy capture.Suggest additional data transformations for improved accuracy.</p>
<p>Output: LLM response based on the Prompt</p>
<p>LLMs to autonomously verify constraints and execute optimization procedures.</p>
<p>AutoGen is an open-source Python framework designed to streamline the development of applications utilizing LLMs through the collaboration of multiple agents.These agents can communicate with each other, incorporate human inputs, and employ external tools to collectively tackle complex tasks [283].Developed by Microsoft Research, AutoGen offers customizable agents capable of conversing, executing code, and integrating various functionalities, thereby simplifying the orchestration and automation of sophisticated LLM workflows.Its flexible architecture supports diverse applications, including code generation, debugging, and problem-solving across various domains [281].</p>
<p>The system consists of an EvaluatorAgent for constraint validation and an LPExecutorAgent for executing the solver iteratively.The proposed approach introduces an intelligent layer of decision-making, improving the reliability and adaptability of LP-based optimization.</p>
<p>G. Application to a Single Linear Programming Problem 1) Problem Statement:</p>
<p>The optimization task is a transportation cost minimization problem.Given two warehouses supplying goods to three stores, the objective is to determine the optimal transportation allocation that minimizes costs while satisfying supply and demand constraints.</p>
<p>2) Mathematical Formulation: The LP model is defined as:</p>
<p>Minimize Z = 4x 1 + 6x 2 + 9x 3 + 5x 4 + 3x 5 + 8x 6 , (6) Subject to:</p>
<p>x 1 + x 2 + x 3 ≤ 50 (Supply limit at Warehouse 1), (7) x 4 + x 5 + x 6 ≤ 60 (Supply limit at Warehouse 2), (8) x 1 + x 4 ≥ 30 (Demand at Store 1), ( 9)
x 2 + x 5 ≥ 40 (Demand at Store 2),(10)
x 3 + x 6 ≥ 20 (Demand at Store 3), ( 11)
x i ≥ 0 ∀i ∈ {1, 2, 3, 4, 5, 6}.(12)
H. Application to a Multi-Stage LP Optimization Problem 1) Problem Statement: In a more complex scenario, the optimization problem involves three separate but interconnected LP problems where solutions from the first stage influence constraints in the subsequent stages.This is relevant for dynamic supply chain optimization, multitime-period scheduling, and adaptive cost minimization.</p>
<p>I. Mathematical Formulation</p>
<p>Each LP problem is formulated similarly to the single problem but with additional constraints:</p>
<p>Minimize Z 1 = 3x 1 + 7x 2 + 2x 3 + 6x 4 + 4x 5 + 5x 6 , (13) Subject to:
x 1 + x 2 + x 3 ≤ 40,(14)
x 4 + x 5 + x 6 ≤ 70, (15)
x 1 + x 4 ≥ 20,(16)
x 2 + x 5 ≥ 30, (17)
x 3 + x 6 ≥ 25, (18)x i ≥ 0,(19)
For the second and third stage problems:
Minimize Z 2 = 5x 1 + 4x 2 + 6x 3 + 3x 4 + 7x 5 + 8x 6 , (20)
Subject to:
x 1 + x 2 + x 3 ≤ 60,(21)x 4 + x 5 + x 6 ≤ 80,(22)x 1 + x 4 ≥ 35,(23)
x 2 + x 5 ≥ 25, (24)
x 3 + x 6 ≥ 30, (25)x i ≥ 0,(26)
Minimize
Z 3 = 6x 1 + 8x 2 + 7x 3 + 5x 4 + 9x 5 + 4x 6 , (27)
Subject to:
x 1 + x 2 + x 3 ≤ 50,(28)
x 4 + x 5 + x 6 ≤ 75, (29)
x 1 + x 4 ≥ 28,(30)
x 2 + x 5 ≥ 38, (31)
x 3 + x 6 ≥ 22, (32)x i ≥ 0,(33)
1) Role of LLMs: To integrate intelligent decisionmaking in LP optimization, we employ LLM-based agents using the AutoGen framework [281].The system consists of two key agents: (i) EvaluatorAgent, (ii) LPExecutorAgent.</p>
<p>J. EvaluatorAgent: Constraint Validation</p>
<p>Before executing the LP solver, an agent verifies whether constraints are correctly defined.This prevents unnecessary computation and ensures the solver is executed under valid conditions.</p>
<p>1) System Message for EvaluatorAgent:</p>
<p>$system_message$: "You verify if the LP constraints are satisfied before execution."</p>
<p>2) Prompt used by EvaluatorAgent:</p>
<p>"validate constraints."</p>
<p>The agent responds with:</p>
<p>• ✓Constraints are valid.Proceed with LP execution.</p>
<p>• ✗Invalid constraints detected.Execution halted.</p>
<p>3) LPExecutorAgent: Optimization Execution: Upon receiving confirmation that constraints are valid, the execution agent runs the LP solver three times for consistency.The agent iteratively solves the LP and returns solutions for analysis.</p>
<p>4) Implementation and Execution:</p>
<p>The process follows these steps:</p>
<p>1) The EvaluatorAgent checks the validity of constraints.2) If constraints are valid, the LPExecutorAgent executes the LP solver three times.</p>
<p>3) The results are compiled and analyzed for consistency.</p>
<p>5) Results and Discussion: By leveraging LLMs for constraint validation and execution, the system prevents errors due to incorrect LP formulations and ensures a structured execution process.The use of multiple runs improves solution robustness and reliability.Future extensions of this framework may include:</p>
<p>• Adaptive learning mechanisms to refine constraints dynamically.• Multi-objective optimization for real-world applications.</p>
<p>• Integration with reinforcement learning for decisionmaking in dynamic environments.</p>
<p>XI. Challenges and Further Directions</p>
<p>Despite the remarkable advancements in mathematical language models, several critical challenges remain unresolved.These challenges include faithfulness, multi-modal integration, uncertainty, evaluation, creative mathematical reasoning, application in education, and data scarcity.</p>
<p>a) Faithfulness: Mathematical LLMs suffer from hallucinations, generating incorrect or misleading results that undermine reliability [84], [284].Several approaches attempt to mitigate this issue by incorporating additional knowledge [285], reinforcement learning from human feedback [209], tool usage [111], [112], and verification mechanisms [115], [285]- [289].However, these improvements remain limited, necessitating further research to enhance trustworthiness and practical applicability.</p>
<p>b) Multi-Modal Integration: Mathematical problems often require multi-modal reasoning, involving not just text but also diagrams, graphs, and equations [290], [291].While existing LLMs perform well in text-based tasks, they struggle with processing and integrating multimodal information [28], [291].Compared to general visionlanguage tasks like VQA [292] and image captioning [293], mathematical multi-modal reasoning is significantly more complex.Advancements in this area are essential for improving problem-solving capabilities in geometry and symbolic reasoning.c) Uncertainty: LLMs inherently produce ambiguous outputs, particularly in probabilistic reasoning and problems with incomplete information [294], [295].While deterministic calculations are well-handled, tasks requiring fuzzy logic, approximate solutions, or uncertain constraints remain challenging.Some studies address this through controlled generation [296], but a more robust approach is needed to ensure mathematical models accurately navigate uncertainty.d) Evaluation: Existing evaluation methods struggle to comprehensively assess mathematical reasoning capabilities.Traditional NLP metrics fail to capture the logical depth required for complex problem-solving.Several benchmarks have been proposed [7], [94], [142], [191], alongside evaluation metrics [66], [297]- [299].However, ensuring a fair assessment across arithmetic, theorem proving, and applied mathematics remains an ongoing challenge.</p>
<p>e) Creative Mathematical Reasoning: While LLMs excel at solving existing problems, their ability to formulate and prove new mathematical theorems remains limited.Mathematical creativity requires deep conceptual understanding beyond memorized patterns.Some studies have explored machine-learning-driven theorem discovery [300], and FunSearch [301] demonstrated the potential for LLMs to contribute to open problems.However, bridging the gap between automated reasoning and genuine mathematical discovery remains a major research frontier.</p>
<p>f) Application in Education: Mathematical LLMs have significant potential as tutors and educational tools.However, effective deployment in classrooms requires addressing pedagogical challenges such as adaptive explanations and interactive engagement.Socratic questioning methods [124], [302] have been proposed to improve student understanding.Future research should focus on customizing LLMs to different learning styles and integrating them into structured educational frameworks.</p>
<p>g) Data Scarcity: Training data plays a crucial role in the performance of mathematical LLMs [303].While instruction-building methods have explored dataset construction using LLMs, current efforts rely heavily on limited datasets such as GSM8K [142] and MATH [94].Expanding high-quality datasets and constructing multimodal mathematical training data will be key to further improvements.</p>
<p>XII. Conclusion</p>
<p>This survey has explored the landscape of mathematical reasoning and optimization using Large Language Models (LLMs), highlighting their rapid advancements, core methodologies, and diverse applications.LLMs have demonstrated significant progress in solving mathematical word problems, theorem proving, and symbolic computation, primarily due to techniques such as Chain-of-Thought (CoT) reasoning, instruction tuning, reinforcement learning, and tool-augmented inference.While autoregressive and non-autoregressive language models have improved mathematical comprehension, the integration of structured reasoning techniques and hybrid neuralsymbolic approaches remains a crucial research direction.</p>
<p>Beyond theoretical mathematical reasoning, we examined the role of LLMs in optimization and control, including their applications in mixed-integer programming (MIP), linear quadratic control (LQC), climate dataset analysis, and multi-agent optimization frameworks.The case studies on Battery Energy Storage Systems (BESS) optimization and interconnected tank control illustrate how LLMs can assist in formulating optimization problems, generating constraints, and enhancing decisionmaking through fuzzy logic and reinforcement learning.Similarly, multi-agent systems, such as AutoGen-based LP solvers, provide a promising avenue for intelligent and adaptive optimization workflows.</p>
<p>Despite these advancements, several challenges persist, including faithfulness in reasoning, multi-modal integration, handling uncertainty, evaluation consistency, and fostering creative mathematical discovery.Additionally, the gap between automated theorem proving and genuine mathematical insight remains a major research frontier.While LLMs have shown potential in assisting with optimization tasks, their reliability and interpretability must be further refined to ensure robustness in real-world applications.</p>
<p>Future research should focus on improving the interpretability of LLMs, enhancing their ability to perform rigorous mathematical proofs, and developing seamless integrations with existing solvers and symbolic computation frameworks.Moreover, expanding high-quality datasets, refining training paradigms, and adopting hybrid neurosymbolic architectures will be essential in pushing the boundaries of AI-driven mathematical reasoning and optimization.As LLMs continue to evolve, their synergy with formal mathematical methods, scientific computing, and engineering applications will play a pivotal role in advancing AI's capabilities in structured problem-solving.</p>
<p>C. Prompt for Tank Level Control</p>
<p>prompt = f""" We aim to control the water levels in two interconnected tanks to achieve desired levels while minimizing a quadratic cost function.The system is represented as a second-order dynamical system using state-space representation.</p>
<p>The current time step is {t}.The system state is:</p>
<p>-Tank 1 water level: x1(t) = {x[0]:.2f}-Tank 2 water level: x2(t) = {x [1]:.2f}</p>
<p>The desired water levels are:</p>
<ul>
<li>
<p>Based on these observations, it appears that the wind speed data exhibits significant variability across different locations and time periods, with some locations experiencing stronger winds than others.The pressure data is relatively stable, but some locations experience larger pressure fluctuations than others.The kurtosis values suggest that the wind speed and pressure data are slightly platykurtic, which may indicate that the data is influenced by extreme values or outliers.<em> The observed patterns in wind speed and pressure can inform optimal turbine placement by identifying locations with stronger wind speeds and more stable pressure conditions.</em> The data suggests that certain seasons or time periods might yield higher energy output, as wind speed and pressure conditions vary throughout the year.For example, locations with stronger winds during the winter months might be more suitable for wind turbine installation.3. <strong>Data Quality &amp; Transformations</strong> * Based on the skewness and kurtosis values, it appears that no data transformations are necessary for wind speed data.However, for pressure data, a log-transform or normalization might be useful to reduce the impact of extreme values and improve the distribution of the data.* Additional data such as temperature, humidity, and topographic information could refine site-selection or energy forecasting models by providing more context about the local environment and its impact on wind and pressure conditions.</p>
</li>
<li>
<p><strong>Further Analysis</strong> * The coefficient of variation and range metrics can reveal anomalies or outliers in the data that necessitate further investigation.For example, locations with high coefficient of variation values may indicate unstable wind conditions or outliers in the data.* These statistics could guide more advanced modeling approaches, such as using machine learning algorithms to identify patterns and relationships in the data that are not apparent through simple descriptive statistics.--------------------------------------------------------------------------------lp_executor (to lp_executor):</p>
</li>
</ul>
<p>Proceed with LP execution.</p>
<p>--------------------------------------------------------------------------------[DEBUG] LPExecutorAgent received message: Proceed with LP execution.</p>
<p>If, for instance, at hour 2 we have P charge (2) = 0 and P discharge (2) = 10, then the hour-2 cost contribution is Cost 2 = 25 × 0 − 35 × 10 = −350 $, indicating a net profit of $350 at hour 2 (negative cost).</p>
<p>a) System Message for LPExecutorAgent: $system_message$: "You execute the LP solver only if constraints are valid."b) Prompt used by LPExecutorAgent:"Proceed with LP execution."</p>
<p>4 .
4
Tank 1: x1_desired = {x_desired[0]:.2f}-Tank 2: x2_desired = {x_desired[1]:.2f}System parameters: -State transition matrix A: {A} -Control input matrix B: {B} -Quadratic cost function weights: Q1 = {Q[0]}, Q2 = {Q[1]}, R = {R} -Maximum pump flow rate: u_max = {u_max} Based on the improved fuzzy logic rules: 1.If x1 &gt; x1_desired, decrease the pump flow rate u(t) proportionally to the error (x1 -x1_desired).2. If x2 &lt; x2_desired, increase the pump flow rate u(t) proportionally to the error (x2_desired -x2).3.If x1 and x2 are close to the desired levels, reduce the rate of change in u(t) to avoid oscillations.Balance the control action between x1 and x2 deviations to prioritize the larger deviation.5. Minimize the quadratic cost function: J = Q1 * (x1 -x1_desired)^2 + Q2 * (x2 -x2_desired)^2 + R * u(t)^2.6. Ensure the control input u(t) stays within the range [-u_max, u_max].Provide the control input u(t) for this time step.Ensure u(t) is within [-u_max, u_max]."""</p>
<p>2 .
2
<strong>Optimal Placement &amp; Time Periods</strong></p>
<p>E.</p>
<p>Output of the LLM to Single LP problem lp_evaluator (to lp_evaluator): validate constraints.--------------------------------------------------------------------------------[DEBUG]EvaluatorAgent received message: validate constraints.lp_evaluator (to lp_evaluator): ✓Constraints are valid.Proceed with LP execution.</p>
<p>Execution 1 :
1
Running LP solver... ✓Result 1: {'status': 'Success', 'optimal_solution': [30.0, 0.0, 0.0, 0.0, 40.0, 20.0], 'objective_value': 400.0} ==== Final Results ==== Execution 1: {'status': 'Success', 'optimal_solution': [30.0, 0.0, 0.0, 0.0, 40.0, 20.0], 'objective_value': 400.0} ======================= lp_executor (to lp_executor): Execution 1: {'status': 'Success', 'optimal_solution': [30.0, 0.0, 0.0, 0.0, 40.0, 20.0], 'objective_value': 400.0} --------------------------------------------------------------------------------[INFO] LP Execution Finished.F.Output of the LLM to multi-stage LP problemlp_evaluator (to lp_evaluator):validate constraints.--------------------------------------------------------------------------------[DEBUG] EvaluatorAgentreceived message: validate constraints.lp_evaluator (to lp_evaluator): ✓Problem 0 constraints are valid.✓Problem 1 constraints are valid.✓Problem 2 constraints are valid.--------------------------------------------------------------------------------[DEBUG]LPExecutorAgent received message: Proceed with LP execution.==== Final Results ==== Problem 0 Execution: {'problem_id': 0, 'status': 'Success', 'optimal_solution': [30.0, 0.0, 0.0, 0.0, 40.0, 20.0], 'objective_value': 400.0}Problem 1 Execution: {'problem_id': 1, 'status': 'Success', 'optimal_solution': [0.0, 0.0, 20.0, 0.0], 'objective_value': 40.0}Problem 2 Execution: {'problem_id': 2, 'status': 'Success', 'optimal_solution': [0.0, -0.0, 15.0], 'objective_value': 75.0} ======================= [INFO] LP Execution Finished.</p>
<p>TABLE I
I
Comparison of mathematical Language Models.Pre-T: Pre-Training, FT: Finetuning, MM: Multimodal, Symbolic-S: Symbolic solvers, F-CoT: Foundational CoT, A-CoT: Advanced CoT, RL: Reinforce Learning
MethodsBase ModelTrainingToolCoTRLMMAccess</p>
<p>TABLE II
II
Performance of various mathematical LMs in terms of accuracy.
Text-based MethodsMATHGSM8KMMLUASDivAIMEMathQASVAMPFOLIOAQUAMATH-PLM [94]6.9--------Minerva [96]50.378.575.0------MWP-BERT [98]-----76.6---ControlMath [130]9.549.3----51.6--EURUS [102]41.762.8-93.0--90.4--Wizardmath [74]22.781.6------Codex-math [44]81.8--------Toolformer [111]---40.4--29.4--LeanReasoner [113]-------82.6-LINC [131]-------72.5-MuMath-Code [132]55.190.7-------MAmmoTH [133]44.276.7------61.4InternLM-Math [108]37.782.6-------Qwen-Math [107]66.891.682.8-63.386.3---PAL [43]-72.0-79.6--79.4--BoostedPrompt [134]-87.1------63.8PoT [39]-80.0---89.1--58.6Self-check [135]51.388.1--81.2----Self-Consistency [37]-78.0-87.8--86.8-52.0Diversity-of-Thought [136]-96.3------81.7GRACE [143]-36.3---84.468.6--CR [145]72.2------98.0-REFT [149]-75.3---71.879.2--SCoRe [150]64.4--------HGS-PRM [151]13.765.4-------MCTSr [152]58.296.6--11.8----CoRe [121]-63.2-90.5-----Step-DPO [154]70.894.0-------Flow-DPO [155]38.671.3-------OmegapRM [122]69.493.6-------QwQ [158]90.6---50.0----GLM-4-9B-Chat [164]50.679.672.4------Multi-modal MethodsMATHGSM8KMMLUASDivAIMEMMMUMathVistaCMM-MathMath-Vo1 [159]94.8-92.3-83.378.173.9--GPT-4o [160]60.3-88.0-13.469.263.829.0230.4GPT-4V [31]-----56.849.9-22.8Qwen2-VL [161]--46.2--64.570.543.025.9LLaVA-o1 [162]64.0-----54.8--Gemini [163]67.7-85.9--62.263.941.917.7GLM-4V [164]-----47.2---AtomThink [165]------53.3-40.5Math-LLaVA [166]----38.338.346.6-15.7M-STAR [167]------59.5--UnAC [168]-----59.756.6--</p>
<p>TABLE III
III
The statistics information of mathematical datasets.Level: E = Elementary, M = Middle School, H = High School, U = University, C = Competition, H = Hybrid, O = Others.Modality: T = Text, M = Multimodal
4 Dataset#Train #Val #Test#TotalLanguageTaskTypeSolutionLevelModalityVERBPHYSICS [169]7331,096 1,8283,657ENCalculation Training Formula-TClinical [60], [170]11,170 1,625 3,22016,015ENCalculation Training FormulaOTScientific [60]14,694 2,037 4,23120,962ENCalculation Training FormulaOTDoQ [171]5875,418 6,00712,012ENCalculation Training Formula-TDROP [58]77,409 9,536 9,62296,567ENCalculation TrainingTextETAddSub [6]---395ENMWPTraining FormulaETSingleOp [10]265107159531ENMWPTraining FormulaETSingleEq [13]---508ENMWPTraining FormulaETMultiArith [11]420-180600ENMWPTraining Formula-TAlg514 [7]---514ENMWPTraining Formula-TMath23k [5]22,162-100023,162CHMWPTraining FormulaETAQuA [172]97,46725425497,975ENMWPTrainingTextUTGSM8K [142]7,4731,3198,792ENMWPTrainingTextETSVAMP [173]7003001,000ENMWPTraining FormulaETDRAW [174]---1,000ENMWPTraining FormulaOTDolphin1878 [12]-3741,5041,878ENMWPTraining Formula-THMWP [175]---5,470CHMWPTraining Formula-TArMATH [176]---6,000ArabicMWPTraining FormulaETTabMWP [40]---38,431ENMWPTrainingTextE MTTAL-SCQ5K 13,000-2,0005,000CH/ENMWPTrainingTextCTREALFP [177]185185558928ENMWPTraining FormulaCTSYNTHFP [177]10,000 1,000 1,00012,000ENMWPTraining FormulaOTMultiHiertt [28]7,8301,044 1,56610,440ENMWPTraining FormulaOMMATHPILE [178]---903,180ENMWPTrainingTextHTOpenWebMath [179]----ENMWPTraining FormulaHTMathQA [180]29,837 4,475 28,98537,297ENMQATraining Formula-TFinQA [181]6,2518831,1478,281ENMQATraining FormulaOTTAT-QA [27]---16,552ENMQATrainingTextOMMML [182]---57,882ENTPTraining Formula-THolStep [183]2,013,046-196,030 2,209,076ENTPTraining Formula-TCoqGym [82]---71,000ENTPTraining Formula-THOList [184]---29,462ENTPTraining Formula-TIsarStep [185]820,000 5,000 5,000830,000ENTPTraining FormulaUTLISA [26]---183,000ENTPTraining Formula-TNaturalProofs [79]32,000ENTPTrainingTextOTLeanStep [95]---21,606,000ENTPTraining Formula-TNumGLUE [186]---101,835ENCalculation Benchmark TextHTDophin18k [187]---18,460ENMWPBenchmark TextETMAWPS [188]---3,320ENMWPBenchmark FormulaHTASDiv [189]---2,305ENMWPBenchmark FormulaETMATH [94]7,5005,00012,500ENMWPBenchmark TextH OTMGSM [190]----MultilingualMWPBenchmark TextETMathematics [191]2,000,000100,000 2,100,000ENMWPBenchmark Formula-TMMLU-Math [93]---906ENWMPBenchmark FormulaE H CTAGIEval [192]---469/220CH/ENMWPBenchmark Formula H C OTAIME 2---933ENMWPBenchmark FormulaCTMATHTRAP [193]---105/155ENMWPBenchmark FormulaHTMathVista [194]---6,141ENMWPBenchmark Formula E M H UMMath-V [195]---3,040ENMWPBenchmark Formula E H OMCMM-Math [196]22,248-5,82128,069CHMWPBenchmark TextE M HMMathVerse [197]---2,612ENMWPBenchmark FormulaHMGeoQA [198]3,4997457544,998ENMWPBenchmark FormulaMMM3CoT [199]---11,459ENMWPBenchmark FormulaOMMR-MATH [200]----ENMWPBenchmark FormulaETMMMU-Math [201]---505ENMWPBenchmark TextUMWe-Math [202]---1700ENMWPBenchmark Formula E H UMU-MATH [203]---1,080ENMWPBenchmark FormulaUMINT [81]----ENTPBenchmark Formula-TminiF2F [204]-244244488ENTPBenchmark Formula E H OTFOLIO [205]---1,430ENTPBenchmark FormulaHTAggregate [206]---1,492ENMWPAugmented FormulaHTMathQA-Python [207]19,209 2,822 2,82223,914ENMWPAugmented Code-TMath50k [208]---50,000ENWMPAugmented Text-TPRM800K [209]---2,868ENWMPAugmented TextH OTMetaMathQA [73]---395,000ENMWPAugmented TextHTLila [210]---134,000ENMWPAugmented CodeHTPEN [211]---3,581ENMWPAugmented FormulaHTminiF2F+informal [78]-244244488ENTPAugmented Formula E H OTNaturalProofs-Gen [212] 12,500 1,000 1,00014,500ENTPAugmented TextOT4</p>
<p>TABLE IV
IV
Taxonomy of LLM-based time series methods.The data type TS denotes standard time series, STS represents spatiotemporal time series, and the prefix MM-indicates multimodal inputs.The task entry Multiple includes forecasting, classification, imputation, and anomaly detection.Query refers to direct LLM-based querying, Token indicates time series tokenization design, Prompt denotes the use of textual or parameterized prompts, Tune represents fine-tuning of LLM parameters, and Fusion specifies whether LLMs are integrated into external models for downstream tasks.
MethodData TypeDomainTaskQueryTokenPromptTuneFusionLLMCodeTime-LLM [260]MM-TSGeneralForecasting✗✓✓✓✗LLaMA, GPT-2Yes [1]OFA [256]TSGeneralMultiple✗✓✗✓✗GPT-2Yes [2]TEMPO [258]TSGeneralForecasting✗✓✓✓✗GPT-2NoTEST [259]MM-TSGeneralForecasting, Classification✗✓✓✗✓BERT, GPT-2, ChatGLM, LLaMA2Yes [3]LLM4TS [257]TSGeneralForecasting✗✓✗✓✗GPT-2NoPromptCast [254]TSGeneralForecasting✓✗✓✗✗Bart, BERTYes [4]LAMP [262]TSGeneralEvent Prediction✓✗✓✗✓GPT-3&amp;3.5, LLaMA-2Yes [6]Yu et al. [247]MM-TSFinanceForecasting✓✗✓✓✗GPT-4, Open-LLaMANoMETS [249]MM-TSHealthcareClassification✓✗✓✗✓ClinicalBERTNoJiang et al. [250]MM-TSHealthcareClassification✗✗✗✓✗NYUTron (BERT)Yes [8]AuxMobLCast [266]STSMobilityForecasting✗✗✓✓✓BERT, RoBERTa, GPT-2, XLNetYes [9]ST-LLM [251]STSTrafficForecasting✗✓✗✓✗LLaMA, GPT-2NoGATGPT [267]STSTrafficImputation✗✓✗✓✗GPT-2NoLA-GCN [263]MM-STSVisionClassification✗✓✗✗✓BERTYes [11]</p>
<p>TABLE V
V
Comparison of Commonly used LLMs based on their technical specifications
ModelOpen-SourceContext LengthStrengthsWeaknessesGPT-4 Turbo✗128KBest reasoning, multi-modalExpensive, closed-sourceLLaMA 2/3✓4K-32KOpen-source, efficientWeaker reasoningDeepSeek-R1✓12KGood multilingual supportLess adoptionMixtral✓4K-32KEfficient, compactShorter contextClaude 3✗200KEthical AI, document processingSlower responsesGemini 1.5 Pro✗2MMulti-modal, long contextNot fully open-source</p>
<p>TABLE VII Simulation
VII
Results for LLM-Driven Fuzzy Control Time (s) x 1 (</p>
<p>Tank 1 Level) x 2 (Tank 2 Level) u (Control Input)</p>
<p>. Prompt for time series dataset and the LLM Response prompt = f""" Below are location-wise <strong>extended</strong> statistical features for wind speeds and pressure data collected over a 1-year period (2928 time steps at 3-hour intervals, across 232 locations).The following metrics are provided for each location: • Mean • Median • Standard Deviation • Variance • Min, Max • Range • 25th Percentile • 75th Percentile • Interquartile Range (IQR) • Skewness • Kurtosis • Coefficient of Variation Overall (all locations, all time steps) aggregates include: • Overall Mean Wind Speed: {overall_mean_ws:.3f}• Overall Mean Pressure: {overall_mean_p:.3f}Sample of per-location extended stats (first {num_locations_to_show} locations): {stats_string} Using these extended statistics, please address the following: 1. <strong>Distribution &amp; Patterns</strong> -Comment on any notable patterns in mean, standard deviation, range, or skewness for wind speed and pressure.-Identify whether certain locations exhibit heavy tails or extreme values (based on kurtosis) and any implications for site selection or forecasting.2. <strong>Optimal Placement &amp; Time Periods</strong> -Explain how the observed patterns could inform optimal turbine placement.-Highlight which time periods or seasons might yield the highest energy output.3. <strong>Data Quality &amp; Transformations</strong> -Suggest if any data transformations (e.g., log-transform, outlier removal, normalization) might be warranted based on skewness/kurtosis.-Propose additional data (e.g., temperature, humidity, topographic) that could refine site-selection or energy forecasting models.4. <strong>Further Analysis</strong> -Discuss whether metrics like coefficient of variation or range reveal anomalies/outliers needing investigation.-If relevant, suggest how these statistics could guide more advanced modeling approaches.
0.000.930.500.001.000.860.500.002.000.800.490.003.000.790.480.504.000.740.470.005.000.690.460.006.000.640.450.007.000.600.440.008.000.560.420.009.000.530.410.00"""Location 231:Wind Speed Stats:Mean = 4.006Median = 3.684StdDev = 1.998Variance= 3.993Range = 15.42825% = 2.51075% = 5.173IQR = 2.663
D</p>
<p>AcknowledgmentTo be written in the future.AppendixA. Code Availability StatementThe simulation codes are publicly available in the following links:[Github or https://github.com/Ali-Forootani/A-Survey-on-Mathematical-Reasoning-and-Optimization-with-Large-Language-Models]-[Gitlab or https://git.ufz.de/forootan/a-survey-on-mathematical-reasoning-and-optimization-with-large-language-models]-[Zenodo or https://zenodo.org/records/15037375].B. Prompt for BESS to GPT4Oprompt = """ We want to optimize the operation of a Battery Energy Storage System (BESS).The goal is to minimize the operational cost by determining the charge/discharge schedule for the battery over a 10-hour period.### Problem Definition:The problem involves scheduling the charging and discharging of a battery to minimize the operational cost while adhering to constraints.
LAGCN ### Fuzzy Logic Rules: 1. If the electricity price (C_t) is high, prioritize discharging the battery</p>
<p>If the battery SOC is below 30%, limit discharging and prioritize charging. </p>
<p>If (C_t) is low, delay charging unless the SOC is critically low. </p>
<p>Avoid frequent switching between charge and discharge states. ### Requirements: 1. Use fuzzy logic rules to guide decisions dynamically during optimization. </p>
<p>Reformulate constraints or priorities based on the fuzzy logic rules. </p>
<p>Solve the optimization problem and provide: -A step-by-step explanation of how fuzzy logic influenced the solution. -The final optimal charge/discharge schedule ((P_{charge}(t)), (P_{discharge}(t))) and state of charge ((SOC(t))) for each hour. The total minimized operational cost</p>
<p>. ## Expected Output, </p>
<p>An explanation of how fuzzy logic rules were applied in each time step. </p>
<p>The optimal charge/discharge schedule over 10 hours. </p>
<p>The total minimized operational cost of the system. References</p>
<p>. E A Feigenbaum, J Feldman, Computers and thought. 71963</p>
<p>Natural language input for a computer problem solving system. D Bobrow, 1964</p>
<p>An integrated model of skill in solving elementary word problems. D J Briars, J H Larkin, Cognition and instruction. 131984</p>
<p>Understanding and solving arithmetic word problems: A computer simulation. C R Fletcher, Behavior Research Methods, Instruments, &amp; Computers. 198517</p>
<p>Deep neural solver for math word problems. Y Wang, X Liu, S Shi, EMNLP. 2017</p>
<p>Learning to solve arithmetic word problems with verb categorization. M J Hosseini, H Hajishirzi, O Etzioni, N Kushman, EMNLP. 2014</p>
<p>Learning to automatically solve algebra word problems. N Kushman, Y Artzi, L Zettlemoyer, R Barzilay, ACL. June 2014</p>
<p>Learn to solve algebra word problems using quadratic programming. L Zhou, S Dai, L Chen, EMNLP. 2015</p>
<p>Learning to use formulas to solve simple arithmetic problems. A Mitra, C Baral, ACL. 2016</p>
<p>Reasoning about quantities in natural language. S Roy, T Vieira, D Roth, TACL. 32015</p>
<p>Solving general arithmetic word problems. S Roy, D Roth, EMNLP. 2015</p>
<p>Automatically solving number word problems by semantic parsing and reasoning. S Shi, Y Wang, C.-Y Lin, X Liu, Y Rui, EMNLP. 2015</p>
<p>Parsing algebraic word problems into equations. R Koncel-Kedziorski, H Hajishirzi, A Sabharwal, O Etzioni, S D Ang, TACL. 32015</p>
<p>Large language models and mathematical understanding. J Couperus, 2023Master's thesis</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, NeurIPS. 352022</p>
<p>Learning mathematics with large language models: A comparative study with computer algebra systems and other tools. N Matzakos, S Doukakis, M Moundridou, International Journal of Emerging Technologies in Learning (Online). 1820512023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 2019</p>
<p>RoBERTa: A robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, 2019</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, ACL. 2020</p>
<p>Improving Language Understanding by Generative Pre-Training. A Radford, K Narasimhan, T Salimans, I Sutskever, June 201812</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Feb. 201924</p>
<p>. P Clark, O Etzioni, D Khashabi, T Khot, B D Mishra, K Richardson, A Sabharwal, C Schoenick, O Tafjord, N Tandon, S Bhakthavatsalam, D Groeneveld, M Guerquin, M Schmitz, 2021From 'F' to 'a' on the N.Y. regents science exams: An overview of the aristo project</p>
<p>Injecting numerical reasoning skills into language models. M Geva, A Gupta, J Berant, ACL (D. Jurafsky, J. Chai, N. Schluter, and J. TetreaultJuly 2020</p>
<p>Injecting numerical reasoning skills into knowledge base question answering models. Y Feng, J Zhang, X Zhang, L Liu, C Li, H Chen, 2022</p>
<p>MathBERT: A pretrained model for mathematical formula understanding. S Peng, K Yuan, L Gao, Z Tang, 2021</p>
<p>Lisa: Language models of isabelle proofs. A Q Jiang, W Li, J M Han, Y Wu, AITP. 2021</p>
<p>Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. F Zhu, W Lei, Y Huang, C Wang, S Zhang, J Lv, F Feng, T.-S Chua, ACL. 2021</p>
<p>Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data. Y Zhao, Y Li, C Li, R Zhang, ACL. 2022</p>
<p>Learning to reason deductively: Math word problem solving as complex relation extraction. Z Jie, J Li, W Lu, 2022</p>
<p>Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems. Z Li, W Zhang, C Yan, Q Zhou, C Li, H Liu, Y Cao, 2022</p>
<p>Gpt-4 technical report. Openai, 2023</p>
<p>Metaicl: Learning to learn in context. S Min, M Lewis, L Zettlemoyer, H Hajishirzi, NAACL. 2022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, NeurIPS. 332020</p>
<p>Metalearning via language model in-context tuning. Y Chen, R Zhong, S Zha, G Karypis, H He, ACL. 2022</p>
<p>Gpt can solve mathematical problems without a calculator. Z Yang, M Ding, Q Lv, Z Jiang, Z He, Y Guo, J Bai, J Tang, 2023arXiv</p>
<p>Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. T Liu, B K H Low, 2023arXiv</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, ICLR2023</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, 2022arXiv</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, 2022arXiv</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. P Lu, L Qiu, K.-W Chang, Y N Wu, S.-C Zhu, T Rajpurohit, P Clark, A Kalyan, ICLR. 2023</p>
<p>Automatic chain of thought prompting in large language models. Z Zhang, A Zhang, M Li, A Smola, ICLR2023</p>
<p>Complexity-based prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, ICLR2023</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, ICML. 2023</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. I Drori, S Zhang, R Shuttleworth, L Tang, A Lu, E Ke, K Liu, L Chen, S Tran, N Cheng, PNAS. 11932e21234331192022</p>
<p>Solving math word problems by combining language models with symbolic solvers. J He-Yueya, G Poesia, R E Wang, N D Goodman, 2023arXiv</p>
<p>A survey of deep learning for mathematical reasoning. P Lu, L Qiu, W Yu, S Welleck, K.-W Chang, ACL (A. Rogers, J. Boyd-Graber, and N. Okazaki2023</p>
<p>Reasoning with language model prompting: A survey. S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, C Tan, F Huang, H Chen, ACL. 2023</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, 2023</p>
<p>Pretrained models for natural language processing: A survey. X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang, Science China Technological Sciences. 63102020</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, A survey of large language models. 2023arXiv</p>
<p>Y Yan, J Su, J He, F Fu, X Zheng, Y Lyu, K Wang, S Wang, Q Wen, X Hu, A survey of mathematical reasoning in the era of multimodal large language model: Benchmark, method &amp; challenges. 2024arXiv</p>
<p>Mathematical language models: A survey. W Liu, H Hu, J Zhou, Y Ding, J Li, J Zeng, M He, Q Chen, B Jiang, A Zhou, arXiv:2312.076222023arXiv preprint</p>
<p>S Imani, L Du, H Shrivastava, Mathprompter: Mathematical reasoning using large language models. 2023arXiv</p>
<p>Leveraging large language models for solving rare MIP challenges. T Wang, W.-Y Yu, R She, W Yang, T Chen, J Zhang, arXiv:2409.044642024arXiv preprint</p>
<p>Lm4opt: Unveiling the potential of large language models in formulating mathematical optimization problems. T Ahmed, S Choudhury, 2024</p>
<p>Llms for mathematical modeling. X Huang, Q Shen, Y Hu, A Gao, B Wang, arXiv:2405.13144v3Towards bridging the gap between natural and mathematical languages. 2025</p>
<p>Executing arithmetic: Fine-tuning large language models as turing machines. J Lai, J Xu, Y Yang, Y Huang, C Cao, J Xu, arXiv:2410.078962024arXiv preprint</p>
<p>Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, NAACL. 2019</p>
<p>Do language embeddings capture scales?. X Zhang, D Ramachandran, I Tenney, Y Elazar, D Roth, Black-boxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. 2020</p>
<p>Numeracy for language models: Evaluating and improving their ability to predict numbers. G Spithourakis, S Riedel, ACL. 201856</p>
<p>Do nlp models know numbers? probing numeracy in embeddings. E Wallace, Y Wang, S Li, S Singh, M Gardner, EMNLP. 2019</p>
<p>An empirical investigation of contextualized number prediction. T Berg-Kirkpatrick, D Spokoyny, EMNLP. Nov. 2020</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, J Lin, 2021arXiv</p>
<p>Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. C Wang, B Zheng, Y Niu, Y Zhang, NLPCC. 2021</p>
<p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition. M Muffo, A Cocco, E Bertino, </p>
<p>How well do large language models perform in arithmetic tasks?. Z Yuan, H Yuan, C Tan, W Wang, S Huang, 2023arXiv</p>
<p>Galactica: A Large Language Model for Science. R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, 2022</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, 2023arXiv</p>
<p>Teaching algorithmic reasoning via in-context learning. H Zhou, A Nova, H Larochelle, A Courville, B Neyshabur, H Sedghi, 2022</p>
<p>S Jelassi, S Ascoli, C Domingo-Enrich, Y Wu, Y Li, F Charton, Length generalization in arithmetic transformers. 2023arXiv</p>
<p>Show Your Work: Scratchpads for Intermediate Computation with Language Models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, 2021</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Z Yuan, H Yuan, C Li, G Dong, C Tan, C Zhou, 2023arXiv</p>
<p>L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, Metamath: Bootstrap your own mathematical questions for large language models. 2023arXiv</p>
<p>H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv</p>
<p>Mathattack: Attacking large language models towards math solving ability. Z Zhou, Q Wang, M Jin, J Yao, J Ye, W Liu, W Wang, X Huang, K Huang, 2023</p>
<p>Llemma: An open language model for mathematics. Z Azerbayev, H Schoelkopf, K Paster, M Santos, S Mcaleer, A Q Jiang, J Deng, S Biderman, S Welleck, Minerva. 8164B</p>
<p>Generative language modeling for automated theorem proving. S Polu, I Sutskever, 2020</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. A Q Jiang, S Welleck, J P Zhou, T Lacroix, J Liu, W Li, M Jamnik, G Lample, Y Wu, ICLR. 2023</p>
<p>Naturalproofs: Mathematical theorem proving in natural language. S Welleck, J Liu, R Le Bras, H Hajishirzi, Y Choi, K Cho, NeurIPS Datasets and Benchmarks Track. 2021Round 1</p>
<p>Deepmath-deep sequence models for premise selection. G Irving, C Szegedy, A A Alemi, N Eén, F Chollet, J Urban, NeurIPS. 292016</p>
<p>Int: An inequality benchmark for evaluating generalization in theorem proving. Y Wu, A Jiang, J Ba, R B Grosse, ICLR2021</p>
<p>Learning to prove theorems via interacting with proof assistants. K Yang, J Deng, ICML. 2019</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, ACL. July 2020</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, NeurIPS. 302017</p>
<p>Pretrained language models are symbolic mathematics solvers too!. K Noorbakhsh, M Sulaiman, M Sharifi, K Roy, P Jamshidi, 2021arXiv</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, JMLR. 211402020</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, NAACL. 2019</p>
<p>Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model. B Kim, K S Ki, D Lee, G Gweon, EMNLP. 2020</p>
<p>Generate &amp; Rank: A Multi-task Framework for Math Word Problems. J Shen, Y Yin, L Li, L Shang, X Jiang, M Zhang, Q Liu, 2021</p>
<p>Thor: Wielding hammers to integrate language models and automated theorem provers. A Q Jiang, W Li, S Tworkowski, K Czechowski, T Odrzygóźdź, P Miłoś, Y Wu, M Jamnik, 2022</p>
<p>G Lample, M.-A Lachaux, T Lavril, X Martinet, A Hayat, G Ebner, A Rodriguez, T Lacroix, HyperTree Proof Search for Neural Theorem Proving. </p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021ICLR</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, NeurIPS Datasets and Benchmarks Track. 2021</p>
<p>Proof artifact co-training for theorem proving with language models. J M Han, J Rute, Y Wu, E Ayers, S Polu, ICLR2022</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, NeurIPS. 352022</p>
<p>Y Wu, M Rabe, W Li, LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning. </p>
<p>MWP-BERT: Numeracy-augmented pre-training for math word problem solving. Z Liang, J Zhang, L Wang, W Qin, Y Lan, J Shao, X Zhang, 2022</p>
<p>Enhancing automated scoring of math self-explanation quality using llm-generated datasets: A semi-supervised approach. R Nakamoto, B Flanagan, T Yamauchi, D Yilling, K Takami, H Ogata, 2023</p>
<p>Learn beyond the answer: Training language models with reflection for mathematical reasoning. Z Zhang, T Ge, Z Liang, W Yu, D Yu, M Jia, D Yu, M Jiang, 2024arXiv</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, 2023arXiv</p>
<p>Advancing llm reasoning generalists with preference trees. L Yuan, G Cui, H Wang, N Ding, X Wang, J Deng, B Shan, H Chen, R Xie, Y Lin, 2024arXiv</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, NeurIPS. 362024</p>
<p>Noise contrastive alignment of language models with explicit rewards. H Chen, G He, L Yuan, G Cui, H Su, J Zhu, 2024arXiv</p>
<p>Instruction tuning for large language models: A survey. S Zhang, L Dong, X Li, S Zhang, X Sun, S Wang, J Li, R Hu, T Zhang, F Wu, 2023arXiv</p>
<p>Improving large language model fine-tuning for solving math problems. Y Liu, A Singh, C D Freeman, J D Co-Reyes, P J Liu, 2023arXiv</p>
<p>A Yang, B Zhang, B Hui, B Gao, B Yu, C Li, D Liu, J Tu, J Zhou, J Lin, Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. 2024arXiv</p>
<p>Internlm-math: Open math large language models toward verifiable reasoning. H Ying, S Zhang, L Li, Z Zhou, Y Shao, Z Fei, Y Ma, J Hong, K Liu, Z Wang, Y Wang, Z Wu, S Li, F Zhou, H Liu, S Zhang, W Zhang, H Yan, X Qiu, J Wang, K Chen, D Lin, 2024</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. M Kazemi, N Kim, D Bhatia, X Xu, D Ramachandran, 2022arXiv</p>
<p>Logicsolver: Towards interpretable math word problem solving with logical prompt-enhanced learning. Z Yang, J Qin, J Chen, L Lin, X Liang, 2022arXiv</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, 2023</p>
<p>Talm: Tool augmented language models. A Parisi, Y Zhao, N Fiedel, 2022</p>
<p>Leanreasoner: Boosting complex logical reasoning with lean. D Jiang, M Fonseca, S B Cohen, 2024arXiv</p>
<p>Art: Automatic multi-step reasoning and tool-use for large language models. B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, 2023</p>
<p>Deductive verification of chain-of-thought reasoning. Z Ling, Y Fang, X Li, Z Huang, M Lee, R Memisevic, H Su, 2023arXiv</p>
<p>Stepwise self-consistent mathematical reasoning with large language models. Z Zhao, Y Rong, D Guo, E Gözlüklü, E Gülboy, E Kasneci, 2024arXiv</p>
<p>Answering questions by meta-reasoning over multiple chains of thought. O Yoran, T Wolfson, B Bogin, U Katz, D Deutch, J Berant, 2023arXiv</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 2023arXiv</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Y Yao, Z Li, H Zhao, 2023arXiv</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, NeurIPS. 352022</p>
<p>Solving math word problems via cooperative reasoning induced language models. X Zhu, J Wang, L Zhang, Y Zhang, R Gan, J Zhang, Y Yang, 2022arXiv</p>
<p>Improve mathematical reasoning in language models by automated process supervision. L Luo, Y Liu, R Liu, S Phatale, H Lara, Y Li, L Shu, Y Zhu, L Meng, J Sun, 2024arXiv</p>
<p>The socratic method. L Nelson, 19802</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, M Attarian, B Ichter, K Choromanski, A Wong, S Welker, F Tombari, A Purohit, M Ryoo, V Sindhwani, 2022arXiv</p>
<p>Don't trust: Verify-grounding llm quantitative reasoning with autoformalization. J P Zhou, C Staats, W Li, C Szegedy, K Q Weinberger, Y Wu, arXiv:2403.181202024arXiv preprint</p>
<p>Formal theorem proving by rewarding llms to decompose proofs hierarchically. K Dong, A Mahankali, T Ma, arXiv:2411.018292024arXiv preprint</p>
<p>Neuro-symbolic data generation for math reasoning. Z Li, Z Zhou, Y Yao, X Zhang, Y.-F Li, C Cao, F Yang, X Ma, Advances in Neural Information Processing Systems. 202537</p>
<p>Enhancing mathematical reasoning in llms by stepwise correction. Z Wu, Q Zeng, Z Zhang, Z Tan, C Shen, M Jiang, arXiv:2410.129342024arXiv preprint</p>
<p>Evaluating language models for mathematics through interactions. K M Collins, A Q Jiang, S Frieder, L Wong, M Zilka, U Bhatt, T Lukasiewicz, Y Wu, J B Tenenbaum, W Hart, Proceedings of the National Academy of Sciences. 12124e23181241212024</p>
<p>Controlmath: Controllable data generation promotes math generalist models. N Chen, N Wu, J Chang, J Li, 2024arXiv</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. T X Olausson, A Gu, B Lipkin, C E Zhang, A Solar-Lezama, J B Tenenbaum, R Levy, 2023arXiv</p>
<p>Mumathcode: Combining tool-use large language models with multiperspective data augmentation for mathematical reasoning. S Yin, W You, Z Ji, G Zhong, J Bai, 2024arXiv</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. X Yue, X Qu, G Zhang, Y Fu, W Huang, H Sun, Y Su, W Chen, 2023arXiv</p>
<p>Boosted prompt ensembles for large language models. S Pitis, M R Zhang, A Wang, J Ba, 2023arXiv</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. N Miao, Y W Teh, T Rainforth, 2023arXiv</p>
<p>R Naik, V Chandrasekaran, M Yuksekgonul, H Palangi, B Nushi, Diversity of thought improves reasoning abilities of large language models. 2023arXiv</p>
<p>Reasoning with language model is planning with world model. S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, 2023arXiv</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. A Zhou, K Yan, M Shlapentokh-Rothman, H Wang, Y.-X Wang, 2023arXiv</p>
<p>Llm+ p: Empowering large language models with optimal planning proficiency. B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, 2023arXiv</p>
<p>Dynamic planning with a llm. G Dagan, F Keller, A Lascarides, 2023arXiv</p>
<p>Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning. Z Zhou, J Song, K Yao, Z Shu, L Ma, 2023arXiv</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>Discriminator-guided multi-step reasoning with language models. M Khalifa, L Logeswaran, M Lee, H Lee, L Wang, 2023arXiv</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K R Narasimhan, S Yao, NeurIPS2023</p>
<p>Y Zhang, J Yang, Y Yuan, A C , -C Yao, Cumulative reasoning with large language models. 2023arXiv</p>
<p>Interpretable math word problem solution generation via step-bystep planning. M Zhang, Z Wang, Z Yang, W Feng, A Lan, 2023arXiv</p>
<p>V-star: Training verifiers for self-taught reasoners. A Hosseini, X Yuan, N Malkin, A Courville, A Sordoni, R Agarwal, 2024arXiv</p>
<p>Quiet-star: Language models can teach themselves to think before speaking. E Zelikman, G Harik, Y Shao, V Jayasiri, N Haber, N D Goodman, 2024arXiv</p>
<p>Reft: Reasoning with reinforced fine-tuning. T Q Luong, X Zhang, Z Jie, P Sun, X Jin, H Li, 2024arXiv</p>
<p>Training language models to self-correct via reinforcement learning. A Kumar, V Zhuang, R Agarwal, Y Su, J D Co-Reyes, A Singh, K Baumli, S Iqbal, C Bishop, R Roelofs, 2024arXiv</p>
<p>Q Ma, H Zhou, T Liu, J Yuan, P Liu, Y You, H Yang, Let's reward step by step: Step-level reward model as the navigators for reasoning. 2023arXiv</p>
<p>Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. D Zhang, X Huang, D Zhou, Y Li, W Ouyang, 2024arXiv</p>
<p>Marco-o1: Towards open reasoning models for open-ended solutions. Y Zhao, H Yin, B Zeng, H Wang, T Shi, C Lyu, L Wang, W Luo, K Zhang, 2024arXiv</p>
<p>X Lai, Z Tian, Y Chen, S Yang, X Peng, J Jia, Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. 2024arXiv</p>
<p>Flow-dpo: Improving llm mathematical reasoning through online multi-agent learning. Y Deng, P Mineiro, 2024arXiv</p>
<p>Boosting large language models with socratic method for conversational mathematics teaching. Y Ding, H Hu, J Zhou, Q Chen, B Jiang, L He, CIKM. 2024</p>
<p>Mathdial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems. J Macina, N Daheim, S P Chowdhury, T Sinha, M Kapur, I Gurevych, M Sachan, 2023arXiv</p>
<p>Qwq: Reflect deeply on the boundaries of the unknown. Q Team, 2024</p>
<p>OpenAI O1 System Card. Openai, 2024</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, Gpt-4o system card. 2024arXiv</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. P Wang, S Bai, S Tan, S Wang, Z Fan, J Bai, K Chen, X Liu, J Wang, W Ge, Y Fan, K Dang, M Du, X Ren, R Men, D Liu, C Zhou, J Zhou, J Lin, 2024arXiv</p>
<p>Llava-o1: Let vision language models reason step-by-step. G Xu, P Jin, L Hao, Y Song, L Sun, L Yuan, 2024arXiv</p>
<p>Gemini: a family of highly capable multimodal models. G Team, R Anil, S Borgeaud, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, 2023arXiv</p>
<p>T Glm, A Zeng, B Xu, B Wang, C Zhang, D Yin, D Zhang, D Rojas, G Feng, H Zhao, Chatglm: A family of large language models from glm-130b to glm-4 all tools. 2024arXiv</p>
<p>Atomthink: A slow thinking framework for multimodal mathematical reasoning. K Xiang, Z Liu, Z Jiang, Y Nie, R Huang, H Fan, H Li, W Huang, Y Zeng, J Han, 2024arXiv</p>
<p>Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. W Shi, Z Hu, Y Bin, J Liu, Y Yang, S.-K Ng, L Bing, R K , - W Lee, 2024</p>
<p>Diving into self-evolve training for multimodal reasoning. Anonymous, Submitted to The Thirteenth International Conference on Learning Representations. 2024</p>
<p>Understanding, abstracting and checking: Evoking complicated multimodal reasoning in LMMs. Y Wang, Y Fu, 2024</p>
<p>Verb physics: Relative physical knowledge of actions and objects. M Forbes, Y Choi, ACL. 2017</p>
<p>Numerically grounded language models for semantic error correction. G Spithourakis, I Augenstein, S Riedel, EMNLP. 2016</p>
<p>How large are lions? inducing distributions over quantitative attributes. Y Elazar, A Mahabal, D Ramachandran, T Bedrax-Weiss, D Roth, ACL. 2019</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. W Ling, D Yogatama, C Dyer, P Blunsom, ACL. 2017</p>
<p>Are nlp models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, NAACL. 2021</p>
<p>Draw: A challenging and diverse algebra word problem set. S Upadhyay, M.-W Chang, tech. rep. 2015Citeseer</p>
<p>Semanticallyaligned universal tree-structured solver for math word problems. J Qin, L Lin, X Liang, R Zhang, L Lin, EMNLP. 2020</p>
<p>Armath: a dataset for solving arabic math word problems. R Alghamdi, Z Liang, X Zhang, LREC. 2022</p>
<p>How much coffee was consumed during emnlp 2019? fermi problems: A new reasoning challenge for ai. A Kalyan, A Kumar, A Chandrasekaran, A Sabharwal, P Clark, EMNLP. 2021</p>
<p>Generative ai for math: Part imathpile: A billion-token-scale pretraining corpus for math. Z Wang, R Xia, P Liu, 2023arXiv</p>
<p>Openwebmath: An open dataset of high-quality mathematical web text. K Paster, M D Santos, Z Azerbayev, J Ba, 2023</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. A Amini, S Gabriel, S Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, NAACL. 2019</p>
<p>Finqa: A dataset of numerical reasoning over financial data. Z Chen, W Chen, C Smiley, S Shah, I Borova, D Langdon, R Moussa, M Beane, T.-H Huang, B R Routledge, EMNLP. 2021</p>
<p>Four decades of mizar: Foreword. A Grabowski, A Korniłowicz, A Naumowicz, Journal of Automated Reasoning. 552015</p>
<p>Holstep: A machine learning dataset for higher-order logic theorem proving. C Kaliszyk, F Chollet, C Szegedy, ICLR2017</p>
<p>Holist: An environment for machine learning of higher order logic theorem proving. K Bansal, S Loos, M Rabe, C Szegedy, S Wilcox, ICML. 2019</p>
<p>Isarstep: a benchmark for high-level mathematical reasoning. W Li, L Yu, Y Wu, L C Paulson, ICLR2021</p>
<p>Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. S Mishra, A Mitra, N Varshney, B Sachdeva, P Clark, C Baral, A Kalyan, ACL. 2022</p>
<p>How well do computers solve math word problems? large-scale dataset construction and evaluation. D Huang, S Shi, C.-Y Lin, J Yin, W.-Y Ma, ACL. 2016</p>
<p>Mawps: A math word problem repository. R Koncel-Kedziorski, S Roy, A Amini, N Kushman, H Hajishirzi, NAACL. 2016</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. S.-Y Miao, C.-C Liang, K.-Y Su, ACL. 2020</p>
<p>Language models are multilingual chain-of-thought reasoners. F Shi, M Suzgun, M Freitag, X Wang, S Srivats, S Vosoughi, H W Chung, Y Tay, S Ruder, D Zhou, ICLR. 2022</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, ICLR2019</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, 2023</p>
<p>Exploring the compositional deficiency of large language models in mathematical reasoning through trap problems. J Zhao, J Tong, Y Mou, M Zhang, Q Zhang, X.-J Huang, EMNLP. 2024</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, 2023arXiv</p>
<p>Measuring multimodal mathematical reasoning with math-vision dataset. K Wang, J Pan, W Shi, Z Lu, M Zhan, H Li, 2024arXiv</p>
<p>Cmm-math: A chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large multimodal models. W Liu, Q Pan, Y Zhang, Z Liu, J Wu, J Zhou, A Zhou, Q Chen, B Jiang, L He, 2024arXiv</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. R Zhang, D Jiang, Y Zhang, H Lin, Z Guo, P Qiu, A Zhou, P Lu, K.-W Chang, Y Qiao, ECCV. 2025</p>
<p>Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. J Chen, J Tang, J Qin, X Liang, L Liu, E P Xing, L Lin, 2021arXiv</p>
<p>M 3 CoT: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought. Q Chen, L Qin, J Zhang, Z Chen, X Xu, W Che, ACL. 2024</p>
<p>S Xia, X Li, Y Liu, T Wu, P Liu, Evaluating mathematical reasoning beyond accuracy. 2024arXiv</p>
<p>Mmmu: A massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. X Yue, Y Ni, K Zhang, T Zheng, R Liu, G Zhang, S Stevens, D Jiang, W Ren, Y Sun, C Wei, B Yu, R Yuan, R Sun, M Yin, B Zheng, Z Yang, Y Liu, W Huang, H Sun, Y Su, W Chen, 2024</p>
<p>Wemath: Does your large multimodal model achieve human-like mathematical reasoning?. R Qiao, Q Tan, G Dong, M Wu, C Sun, X Song, Z Gongque, S Lei, Z Wei, M Zhang, R Qiao, Y Zhang, X Zong, Y Xu, M Diao, Z Bao, C Li, H Zhang, 2024</p>
<p>U-math: A university-level benchmark for evaluating mathematical skills in llms. K Chernyshev, V Polshkov, E Artemova, A Myasnikov, V Stepanov, A Miasnikov, S Tilga, 2024</p>
<p>minif2f: a cross-system benchmark for formal olympiad-level mathematics. K Zheng, J M Han, S Polu, ICLR2023</p>
<p>FOLIO: natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, W Zhou, J Coady, D Peng, Y Qiao, L Benson, L Sun, A Wardle-Solano, H Szabó, E Zubova, M Burtell, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, A R Fabbri, W Kryscinski, S Yavuz, Y Liu, X V Lin, S Joty, Y Zhou, C Xiong, R Ying, A Cohan, D Radev, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. November 12-16, 2024. 2024</p>
<p>Mapping to declarative knowledge for word problem solving. S Roy, D Roth, TACL. 62018</p>
<p>J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, Program synthesis with large language models. 2021arXiv</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, NeurIPS2023</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, ICLR. 2024</p>
<p>Lila: A unified benchmark for mathematical reasoning. S Mishra, M Finlayson, P Lu, L Tang, S Welleck, C Baral, T Rajpurohit, O Tafjord, A Sabharwal, P Clark, EMNLP. 2022</p>
<p>Ept-x: An expression-pointer transformer model that generates explanations for numbers. B Kim, K S Ki, S Rhim, G Gweon, ACL. 2022</p>
<p>Naturalprover: Grounded mathematical proof generation with language models. S Welleck, J Liu, X Lu, H Hajishirzi, Y Choi, NeurIPS. 352022</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, Nature. 62579952024</p>
<p>A Patil, arXiv:2502.03671Advancing reasoning in large language models: Promising methods and approaches. 2025arXiv preprint</p>
<p>Openai, arXiv:2303.08774GPT4 technical report. 2023arXiv preprint</p>
<p>Gemini: A family of highly capable multimodal models. R Anil, S Borgeaud, Y Wu, J Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, D Silver, S Petrov, M Johnson, I Antonoglou, J Schrittwieser, A Glaese, J Chen, E Pitler, T P Lillicrap, A Lazaridou, O Firat, J Molloy, M Isard, P R Barham, T Hennigan, B Lee, F Viola, M Reynolds, Y Xu, R Doherty, E Collins, C Meyer, E Rutherford, E Moreira, K Ayoub, M Goel, G Tucker, E Piqueras, M Krikun, I Barr, N Savinov, I Danihelka, B Roelofs, A White, A Andreassen, T Von Glehn, L Yagati, M Kazemi, L Gonzalez, M Khalman, J Sygnowski, 2023CoRR</p>
<p>Deepseek LLM: scaling open-source language models with longtermism. Deepseek-Ai , abs/2401.02954CoRR. 2024</p>
<p>A Joulin, E Grave, P Bojanowski, M Douze, H Jégou, T Mikolov, arXiv:1612.03651Fasttext. zip: Compressing text classification models. 2016arXiv preprint</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. D Guo, Q Zhu, D Yang, Z Xie, K Dong, W Zhang, G Chen, X Bi, Y Wu, Y K Li, F Luo, Y Xiong, W Liang, 2024</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Y Wu, B Neyshabur, G Gur-Ari, V Misra, arXiv:2206.148582022arXiv preprint</p>
<p>Cmath: Can your language model pass chinese elementary school math test?. T Wei, J Luan, W Liu, S Dong, B Wang, 2023</p>
<p>AGIEval: A humancentric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, abs/2304.06364CoRR. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, NeurIPS2022</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, abs/2211.12588CoRR. 2022</p>
<p>PAL: program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning, ICML 2023. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, Honolulu, Hawaii, USAPMLRJuly 2023. 2023202of Proceedings of Machine Learning Research</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Z Gou, Z Shao, Y Gong, Y Shen, Y Yang, M Huang, N Duan, W Chen, abs/2309.17452CoRR. 2023</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, S Ermon, C D Manning, C Finn, 2023</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, Proximal policy optimization algorithms. 2017arXiv</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Z Azerbayev, H Schoelkopf, K Paster, M D Santos, S Mcaleer, A Q Jiang, J Deng, S Biderman, S Welleck, arXiv:2310.10631Llemma: An open language model for mathematics. 2023arXiv preprint</p>
<p>Minif2f: a cross-system benchmark for formal olympiad-level mathematics. K Zheng, J M Han, S Polu, arXiv:2109.001102021arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.033002020arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, arXiv:2107.033742021arXiv preprintEvaluating large language models trained on code</p>
<p>Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.095832023arXiv preprint</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, abs/2309.12284CoRR. 2023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. X Yue, X Qu, G Zhang, Y Fu, W Huang, H Sun, Y Su, W Chen, abs/2309.05653CoRR. 2023</p>
<p>An empirical investigation of contextualized number prediction. T Berg-Kirkpatrick, D Spokoyny, EMNLP. Nov. 2020</p>
<p>Socreval: Large language models with the socratic method for reference-free reasoning evaluation. H He, H Zhang, D Roth, 2023arXiv</p>
<p>Gamepad: A learning environment for theorem proving. D Huang, P Dhariwal, D Song, I Sutskever, ICLR2019</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, 2023arXiv</p>
<p>Times-Net: Temporal 2D-Variation Modeling for General Time Series Analysis. H Wu, T Hu, Y Liu, H Zhou, J Wang, M Long, ICLR2022</p>
<p>TimeGPT-1. A Garza, M Mergenthaler-Canseco, arXiv:2310.035892023arXiv preprint</p>
<p>X Yu, Z Chen, Y Ling, S Dong, Z Liu, Y Lu, arXiv:2306.11025Temporal Data Meets LLM-Explainable Financial Time Series Forecasting. 2023arXiv preprint</p>
<p>Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. A Lopez-Lira, Y Tang, arXiv:2304.076192023arXiv preprint</p>
<p>Frozen Language Model Helps ECG Zero-Shot Learning. J Li, C Liu, S Cheng, R Arcucci, S Hong, MIDL2023</p>
<p>Health system-scale language models are allpurpose prediction engines. L Y Jiang, X C Liu, N P Nejatian, M Nasir-Moin, D Wang, A Abidin, Nature. 2023</p>
<p>Spatial-Temporal Large Language Model for Traffic Prediction. C Liu, S Yang, Q Xu, Z Li, C Long, Z Li, R Zhao, 2024</p>
<p>Where Would I Go Next? Large Language Models as Human Mobility Predictors. X Wang, M Fang, Z Zeng, T Cheng, 2023</p>
<p>Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift. T Kim, J Kim, Y Tae, C Park, J.-H Choi, J Choo, ICLR2021</p>
<p>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. H Xue, F D Salim, IEEE TKDE. 2023</p>
<p>Large Language Models Are Zero Shot Time Series Forecasters. N Gruver, M Finzi, S Qiu, A G Wilson, NeurIPS2023</p>
<p>One Fits All: Power General Time Series Analysis by Pretrained LM. T Zhou, P Niu, X Wang, L Sun, R Jin, NeurIPS2023</p>
<p>C Chang, W.-C Peng, T.-F Chen, arXiv:2308.08469LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters. 2023arXiv preprint</p>
<p>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. D Cao, F Jia, S O Arik, T Pfister, Y Zheng, W Ye, Y Liu, ICLR2024</p>
<p>TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. C Sun, Y Li, H Li, S Hong, ICLR2024</p>
<p>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. M Jin, S Wang, L Ma, Z Chu, J Y Zhang, X Shi, P.-Y Chen, Y Liang, Y.-F Li, S Pan, ICLR. 2024</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. E J Hu, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, ICLR2021</p>
<p>Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. X Shi, S Xue, K Wang, F Zhou, J Y Zhang, J Zhou, C Tan, H Mei, NeurIPS2023</p>
<p>Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition. H Xu, Y Gao, Z Hui, J Li, X Gao, arXiv:2305.123982023arXiv preprint</p>
<p>Lag-Llama: Towards Foundation Models for Time Series Forecasting. K Rasul, A Ashok, A R Williams, A Khorasani, G Adamopoulos, R Bhagwatkar, 2023</p>
<p>Enhanced Scalable Graph Neural Network via Knowledge Distillation. C Mai, Y Chang, C Chen, Z Zheng, IEEE TNNLS. 2023</p>
<p>Leveraging Language Foundation Models for Human Mobility Forecasting. H Xue, B P Voutharoja, F D Salim, SIGSPATIAL2022</p>
<p>GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation. Y Chen, X Wang, G Xu, arXiv:2311.143322023arXiv preprint</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Introducing the next generation of claude. A Anthropic, 2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. G Team, P Georgiev, V I Lei, R Burnell, L Bai, A Gulati, G Tanzer, D Vincent, Z Pan, S Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Array programming with numpy. C R Harris, K J Millman, S J Van Der Walt, R Gommers, P Virtanen, D Cournapeau, E Wieser, J Taylor, S Berg, N J Smith, Nature. 58578252020</p>
<p>Data structures for statistical computing in python. W Mckinney, SciPy. 44512010</p>
<p>Matplotlib: A 2d graphics environment. J D Hunter, Computing in science &amp; engineering. 9032007</p>
<p>Scikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, the Journal of machine Learning research. 122011</p>
<p>Cartopy: A cartographic python library with a matplotlib interface. U Met Office, 2010Exeter, Devon</p>
<p>Diseño e implementación de un simulador basado en agentes estilo JGOMAS en Python. S Alemany Ibor, 2018Universitat Politècnica de ValènciaPhD thesis</p>
<p>Pulp: a linear programming toolkit for python. S Mitchell, M Osullivan, I Dunning, 20116525Auckland, New ZealandThe University of Auckland</p>
<p>Autogen: Enabling nextgen llm applications via multi-agent conversation framework. Q Wu, G Bansal, J Zhang, Y Wu, S Zhang, E Zhu, B Li, L Jiang, X Zhang, C Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Climate aware deep neural networks (cadnn) for wind power simulation. A Forootani, D E Aliabadi, D Thraen, arXiv:2412.121602024arXiv preprint</p>
<p>Offline training of language model agents with functions as learnable weights. S Zhang, J Zhang, J Liu, L Song, C Wang, R Krishna, Q Wu, Forty-first International Conference on Machine Learning. </p>
<p>A survey of hallucination in large foundation models. V Rawte, A Sheth, A Das, 2023arXiv</p>
<p>Rethinking with retrieval: Faithful large language model inference. H He, H Zhang, D Roth, 2022arXiv</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. A Zhou, K Wang, Z Lu, W Shi, S Luo, Z Qin, S Lu, A Jia, L Song, M Zhan, 2023arXiv</p>
<p>Making language models better reasoners with stepaware verifier. Y Li, Z Lin, S Zhang, Q Fu, B Chen, J.-G Lou, W Chen, ACL. 2023</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. R Zhao, X Li, S Joty, C Qin, L Bing, 2023arXiv</p>
<p>Screws: A modular framework for reasoning with revisions. K Shridhar, H Jhamtani, H Fang, B Van Durme, J Eisner, P Xia, 2023arXiv</p>
<p>Empirical explorations of the geometry theorem machine. H Gelernter, J R Hansen, D W Loveland, western joint IRE-AIEE-ACM computer conference. 1960</p>
<p>Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. P Lu, L Qiu, J Chen, T Xia, Y Zhao, W Zhang, Z Yu, X Liang, S.-C Zhu, NeurIPS Datasets and Benchmarks Track. 2021</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, ICCV. 2015</p>
<p>A comprehensive survey of deep learning for image captioning. M Z Hossain, F Sohel, M F Shiratuddin, H Laga, ACM Computing Surveys (CsUR). 5162019</p>
<p>A survey of uncertainty in deep neural networks. J Gawlikowski, C R N Tassi, M Ali, J Lee, M Humt, J Feng, A Kruspe, R Triebel, P Jung, R Roscher, Artificial Intelligence Review. 5612023Suppl</p>
<p>Shifting attention to relevance: Towards the uncertainty estimation of large language models. J Duan, H Cheng, S Wang, C Wang, A Zavalny, R Xu, B Kailkhura, K Xu, 2023arXiv</p>
<p>Controlled text generation with natural language instructions. W Zhou, Y E Jiang, E Wilcox, R Cotterell, M Sachan, ICML. 2023202</p>
<p>Gpt-j-6b: A 6 billion parameter autoregressive language model. B Wang, A Komatsuzaki, 2021</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, 2022arXiv</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, 2022arXiv</p>
<p>Advancing mathematics by guiding human intuition with ai. A Davies, P Veličković, L Buesing, S Blackwell, D Zheng, N Tomašev, R Tanburn, P Battaglia, C Blundell, A Juhász, Nature. 60078872021</p>
<p>Mathematical discoveries from program search with large language models. A N Bernardino Romera-Paredes, Mohammadamin Barekatain, Nature. 2023</p>
<p>Automatic generation of socratic subquestions for teaching math word problems. K Shridhar, J Macina, M El-Assady, T Sinha, M Kapur, M Sachan, 2022arXiv</p>
<p>Enhancing chat language models by scaling highquality instructional conversations. N Ding, Y Chen, B Xu, Y Qin, S Hu, Z Liu, M Sun, B Zhou, EMNLP. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5352 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5352</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5352</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-72ccc300dd13d180ac8328e40b0f24e8df8a2722</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/72ccc300dd13d180ac8328e40b0f24e8df8a2722" target="_blank">GPT-too: A Language-Model-First Approach for AMR-to-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> An alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring is proposed that outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures.</p>
                <p><strong>Paper Abstract:</strong> Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5352.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5352.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PENMAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN AMR linearization/notation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Full PENMAN bracketed serialization of AMR graphs (node/concept labels plus edge relations and parentheses) used verbatim as the sequential input to a pretrained GPT-2 language model for AMR-to-text fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Abstract meaning representation for sembanking.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The AMR graph is represented using the standard PENMAN bracketed notation (nested parentheses with concept labels and labelled relations, e.g. (r / recommend-01 :ARG1 (a / advocate-01 ...))). That full textual serialization (including relation labels and parentheses) is tokenized (mapping special AMR symbols to unused GPT-2 tokens) and concatenated with a separator token before fine-tuning GPT-2 to generate target sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR (Abstract Meaning Representation) graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves both concept nodes and labelled relations (edge information) and hierarchical structure; relatively high fidelity to original graph semantics; directly interpretable and human-readable; sequential rather than explicit graph structure so some graph locality constraints become implicit; compatible with pretrained LMs without explicit graph encoder but not an optimal graph-native encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation on LDC2017T10 (train/dev/test splits)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with BLEU, METEOR, chrF++; best systems using PENMAN + GPT-2: GPT-2M Rec. (test) BLEU 32.10, METEOR 35.86, chrF++ 61.81; GPT-2L Rec. BLEU 32.47, METEOR 36.80, chrF++ 62.88; with cycle-consistency re-scoring GPT-2L Rec. re-scoring BLEU 33.02, METEOR 37.68, chrF++ 63.89. Also measured with BERTScore (SemSim) and human evaluation where GPT-2L Rec. improved SemSim F1 to 94.63 and human P(4/5) to 41.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Direct PENMAN serialization produced the best results among the tested sequentializations; it outperformed a DFS traversal linearization and a DFS ablation that removed edges (nodes-only). Compared to previous graph-aware models (e.g., Zhu et al. 2019), PENMAN+GPT-2 with reconstruction and re-scoring outperformed prior state-of-the-art on BLEU and METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although PENMAN keeps full relation labels, it remains a sequential encoding and may not capture graph structural inductive biases as explicitly as graph encoders; PENMAN tokens require mapping into the LM vocab and some AMR-specific tokens are non-linguistic; sequential ordering is arbitrary and may hide global graph topology; standard surface-based metrics may underrepresent semantic adequacy variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5352.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5352.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-First Search AMR linearization (Konstas et al. 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization obtained by following a depth-first traversal of the AMR graph and emitting nodes and relation tokens in traversal order, used as a sequential input for language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural amr: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DFS traversal linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Perform a depth-first search through the AMR graph and output the path sequence including concept nodes and relation labels encountered along the traversal, yielding a flat token sequence (linearized graph) suitable as input context to a seq-to-seq or LM encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encodes node order and relations in traversal order; compact sequential form; retains relation labels but linearization order is arbitrary and may conflate distant graph structure; easier to feed into sequence models but risks losing some structural context compared to graph-native encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (dev/test comparisons against PENMAN and nodes-only ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports DFS-based linearization performs worse than PENMAN when fine-tuning GPT-2; exact numeric deltas vs. PENMAN are not tabulated in the main text, but qualitative/dev results indicate PENMAN > DFS and removal of edges degrades performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly in experiments with PENMAN and a nodes-only ablation: DFS (with edges) outperforms nodes-only, but is outperformed by full PENMAN serialization when fine-tuned with GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Traversal imposes an arbitrary sequence order that may be suboptimal; relation ordering may bias the LM; still a sequentialization so lacks explicit graph inductive bias; reported lower performance than PENMAN in this LM-first approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5352.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5352.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nodes-only DFS (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DFS linearization without edge/relation tokens (nodes-only ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where the DFS linearization is stripped of all edge labels, leaving only concept node tokens, used to test importance of relation information for LM-based AMR-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>nodes-only DFS linearization (edge-removed)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Take the depth-first traversal ordering but remove all relation (edge) tokens so the input sequence contains only the concept node labels in traversal order, thereby eliminating explicit relation information from the sequential input fed to GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graph (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Greatly reduced relational expressivity; more compact and simpler sequence; loses subject/object and role information; tests how much pretrained LM can infer relations from nodes alone.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (ablation study on dev set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively to strongly decrease performance compared to DFS with edges and PENMAN; no specific numeric values provided for this ablation in the paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Performed worse than both DFS with edges and PENMAN; demonstrates edge information (relations) is fundamental for AMR-to-text performance in LM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Removes critical relation information causing major drop in generation adequacy; shows pretrained LM cannot reliably reconstruct relational structure from nodes-only sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5352.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5352.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reconstruction loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR reconstruction (joint) training objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training procedure that maximizes the joint probability of the AMR sequentialization and the target sentence, adding an AMR reconstruction term so the LM also models AMR token sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Joint AMR+text reconstruction objective</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Fine-tune GPT-2 to maximize the joint probability p(w, a) = product over text tokens conditioned on prior generated text and the entire AMR sequence, times product over AMR tokens conditioned on previous AMR tokens; effectively the model learns to reconstruct the AMR sequence as well as to generate the text, implemented by concatenating AMR and text and optimizing the joint likelihood (with separator token).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs represented sequentially (PENMAN/DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encourages the LM to model the structure of the AMR sequentialization explicitly, acts as an auxiliary task improving alignment between graph tokens and generated text; regularizes fine-tuning and improves use of AMR signal while leveraging pretrained linguistic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation on LDC2017T10; compared conditional-only fine-tuning vs joint reconstruction objective</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reconstruction term improved dev/test metrics substantially: example given for GPT-2M Greedy decoding: Conditional BLEU 25.73 vs Rec. BLEU 30.41 on dev; overall best models use reconstruction. Test numbers (with PENMAN and reconstruction) include GPT-2M Rec. BLEU 32.10 and GPT-2L Rec. BLEU 32.47.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to conditional-only fine-tuning (no reconstruction term) the joint reconstruction objective yields large gains (e.g., +4.68 BLEU for GPT-2M in dev greedy setting).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Adds training complexity and requires tokenization/vocabulary extension for AMR symbols; it's a heuristic objective (reconstructing sequential AMR) rather than directly enforcing graph structure; its effectiveness may depend on quality of sequentialization and vocabulary handling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5352.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5352.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cycle-rescoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cycle-consistency re-scoring via AMR parsing and Smatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-generation re-scoring method that parses candidate sentences back to AMR graphs and scores them by Smatch agreement with the gold AMR, selecting candidates that better reconstruct the source graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Cycle-consistency re-scoring (Smatch-based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate top-k candidate sentences for a given gold AMR using the LM; parse each candidate using an off-the-shelf AMR parser to produce a reconstructed AMR; compute Smatch between reconstructed AMR and gold AMR; re-rank or select the candidate with highest Smatch score as final output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (used as gold input and reconstructed graphs from generated text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Operates as an external semantic-consistency filter leveraging an AMR parser and Smatch metric; enforces semantic fidelity to the original graph rather than surface similarity to a reference; computationally heavier at inference (parsing k candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Re-ranking generated outputs for AMR-to-text to improve semantic fidelity and standard metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cycle-consistency re-ranking improved performance over beam-best: reported dev numbers include increase up to 33.57 BLEU and 64.86 chrF++ (compared to beam-best lower values). On test set GPT-2M Rec. re-scoring BLEU 32.98 and GPT-2L Rec. re-scoring BLEU 33.02 (improvements over non re-scored variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Used as an orthogonal improvement on top of PENMAN+GPT-2+reconstruction; consistently boosted BLEU, METEOR and chrF++ compared to selecting top beam output.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on accuracy of the external AMR parser (errors there can mis-rank good candidates); adds inference-time cost (parse k candidates); Smatch measures graph overlap which may not capture all desirable linguistic variations; requires available AMR parser trained for same AMR formalism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5352.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5352.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenization & embedding freezing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR token mapping, vocabulary extension and frozen input embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical preprocessing/training techniques: map AMR special symbols to unused GPT-2 tokens and extend vocabulary with arc labels and :root, while freezing GPT-2 input embeddings during fine-tuning to stabilize training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR-special token mapping and frozen embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Special AMR symbols are assigned to GPT-2's unused token slots and arc labels plus the root marker (:root) are added to the GPT-2 vocabulary; during fine-tuning the input embedding layer is frozen (not updated) which empirically improved generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graph serialized tokens (PENMAN/DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables pretrained LM to ingest non-linguistic AMR symbols without catastrophic changes to embeddings; freezing embeddings preserves pretrained distributional knowledge and prevents overfitting to small AMR datasets; pragmatic approach to bridge formal graph tokens and LM vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used across AMR-to-text fine-tuning experiments; part of implementation details that impacted dev/test performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Freezing input embeddings reported to have a positive impact on performance (no precise numeric ablation values provided), used in best reported models (metrics reported elsewhere in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Technique is orthogonal to representation choice (PENMAN/DFS) and was applied when fine-tuning GPT-2; not directly compared to alternative vocabulary strategies in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mapping introduces arbitrary token assignments for AMR symbols which may not reflect semantic similarity; frozen embeddings prevent the model from learning embeddings specialized to AMR tokens, which could limit potential gains if more AMR data were available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-too: A Language-Model-First Approach for AMR-to-Text Generation', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural amr: Sequence-to-sequence models for parsing and generation. <em>(Rating: 2)</em></li>
                <li>Modeling graph structure in transformer for better amr-to-text generation. <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning. <em>(Rating: 2)</em></li>
                <li>Enhancing amr-to-text generation with dual graph representations. <em>(Rating: 2)</em></li>
                <li>Structural neural encoders for amr-to-text generation. <em>(Rating: 2)</em></li>
                <li>Abstract meaning representation for sembanking. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5352",
    "paper_id": "paper-72ccc300dd13d180ac8328e40b0f24e8df8a2722",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "PENMAN",
            "name_full": "PENMAN AMR linearization/notation",
            "brief_description": "Full PENMAN bracketed serialization of AMR graphs (node/concept labels plus edge relations and parentheses) used verbatim as the sequential input to a pretrained GPT-2 language model for AMR-to-text fine-tuning.",
            "citation_title": "Abstract meaning representation for sembanking.",
            "mention_or_use": "use",
            "representation_name": "PENMAN serialization",
            "representation_description": "The AMR graph is represented using the standard PENMAN bracketed notation (nested parentheses with concept labels and labelled relations, e.g. (r / recommend-01 :ARG1 (a / advocate-01 ...))). That full textual serialization (including relation labels and parentheses) is tokenized (mapping special AMR symbols to unused GPT-2 tokens) and concatenated with a separator token before fine-tuning GPT-2 to generate target sentences.",
            "graph_type": "AMR (Abstract Meaning Representation) graph",
            "representation_properties": "Preserves both concept nodes and labelled relations (edge information) and hierarchical structure; relatively high fidelity to original graph semantics; directly interpretable and human-readable; sequential rather than explicit graph structure so some graph locality constraints become implicit; compatible with pretrained LMs without explicit graph encoder but not an optimal graph-native encoding.",
            "evaluation_task": "AMR-to-text generation on LDC2017T10 (train/dev/test splits)",
            "performance_metrics": "Evaluated with BLEU, METEOR, chrF++; best systems using PENMAN + GPT-2: GPT-2M Rec. (test) BLEU 32.10, METEOR 35.86, chrF++ 61.81; GPT-2L Rec. BLEU 32.47, METEOR 36.80, chrF++ 62.88; with cycle-consistency re-scoring GPT-2L Rec. re-scoring BLEU 33.02, METEOR 37.68, chrF++ 63.89. Also measured with BERTScore (SemSim) and human evaluation where GPT-2L Rec. improved SemSim F1 to 94.63 and human P(4/5) to 41.83%.",
            "comparison_to_other_representations": "Direct PENMAN serialization produced the best results among the tested sequentializations; it outperformed a DFS traversal linearization and a DFS ablation that removed edges (nodes-only). Compared to previous graph-aware models (e.g., Zhu et al. 2019), PENMAN+GPT-2 with reconstruction and re-scoring outperformed prior state-of-the-art on BLEU and METEOR.",
            "limitations_or_challenges": "Although PENMAN keeps full relation labels, it remains a sequential encoding and may not capture graph structural inductive biases as explicitly as graph encoders; PENMAN tokens require mapping into the LM vocab and some AMR-specific tokens are non-linguistic; sequential ordering is arbitrary and may hide global graph topology; standard surface-based metrics may underrepresent semantic adequacy variability.",
            "uuid": "e5352.0",
            "source_info": {
                "paper_title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "DFS linearization",
            "name_full": "Depth-First Search AMR linearization (Konstas et al. 2017)",
            "brief_description": "A linearization obtained by following a depth-first traversal of the AMR graph and emitting nodes and relation tokens in traversal order, used as a sequential input for language models.",
            "citation_title": "Neural amr: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "use",
            "representation_name": "DFS traversal linearization",
            "representation_description": "Perform a depth-first search through the AMR graph and output the path sequence including concept nodes and relation labels encountered along the traversal, yielding a flat token sequence (linearized graph) suitable as input context to a seq-to-seq or LM encoder.",
            "graph_type": "AMR graph",
            "representation_properties": "Encodes node order and relations in traversal order; compact sequential form; retains relation labels but linearization order is arbitrary and may conflate distant graph structure; easier to feed into sequence models but risks losing some structural context compared to graph-native encoders.",
            "evaluation_task": "AMR-to-text generation (dev/test comparisons against PENMAN and nodes-only ablation)",
            "performance_metrics": "Paper reports DFS-based linearization performs worse than PENMAN when fine-tuning GPT-2; exact numeric deltas vs. PENMAN are not tabulated in the main text, but qualitative/dev results indicate PENMAN &gt; DFS and removal of edges degrades performance substantially.",
            "comparison_to_other_representations": "Compared directly in experiments with PENMAN and a nodes-only ablation: DFS (with edges) outperforms nodes-only, but is outperformed by full PENMAN serialization when fine-tuned with GPT-2.",
            "limitations_or_challenges": "Traversal imposes an arbitrary sequence order that may be suboptimal; relation ordering may bias the LM; still a sequentialization so lacks explicit graph inductive bias; reported lower performance than PENMAN in this LM-first approach.",
            "uuid": "e5352.1",
            "source_info": {
                "paper_title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Nodes-only DFS (ablation)",
            "name_full": "DFS linearization without edge/relation tokens (nodes-only ablation)",
            "brief_description": "An ablation where the DFS linearization is stripped of all edge labels, leaving only concept node tokens, used to test importance of relation information for LM-based AMR-to-text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "nodes-only DFS linearization (edge-removed)",
            "representation_description": "Take the depth-first traversal ordering but remove all relation (edge) tokens so the input sequence contains only the concept node labels in traversal order, thereby eliminating explicit relation information from the sequential input fed to GPT-2.",
            "graph_type": "AMR graph (ablation)",
            "representation_properties": "Greatly reduced relational expressivity; more compact and simpler sequence; loses subject/object and role information; tests how much pretrained LM can infer relations from nodes alone.",
            "evaluation_task": "AMR-to-text generation (ablation study on dev set)",
            "performance_metrics": "Reported qualitatively to strongly decrease performance compared to DFS with edges and PENMAN; no specific numeric values provided for this ablation in the paper's text.",
            "comparison_to_other_representations": "Performed worse than both DFS with edges and PENMAN; demonstrates edge information (relations) is fundamental for AMR-to-text performance in LM fine-tuning.",
            "limitations_or_challenges": "Removes critical relation information causing major drop in generation adequacy; shows pretrained LM cannot reliably reconstruct relational structure from nodes-only sequences.",
            "uuid": "e5352.2",
            "source_info": {
                "paper_title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Reconstruction loss",
            "name_full": "AMR reconstruction (joint) training objective",
            "brief_description": "A training procedure that maximizes the joint probability of the AMR sequentialization and the target sentence, adding an AMR reconstruction term so the LM also models AMR token sequence generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Joint AMR+text reconstruction objective",
            "representation_description": "Fine-tune GPT-2 to maximize the joint probability p(w, a) = product over text tokens conditioned on prior generated text and the entire AMR sequence, times product over AMR tokens conditioned on previous AMR tokens; effectively the model learns to reconstruct the AMR sequence as well as to generate the text, implemented by concatenating AMR and text and optimizing the joint likelihood (with separator token).",
            "graph_type": "AMR graphs represented sequentially (PENMAN/DFS)",
            "representation_properties": "Encourages the LM to model the structure of the AMR sequentialization explicitly, acts as an auxiliary task improving alignment between graph tokens and generated text; regularizes fine-tuning and improves use of AMR signal while leveraging pretrained linguistic knowledge.",
            "evaluation_task": "AMR-to-text generation on LDC2017T10; compared conditional-only fine-tuning vs joint reconstruction objective",
            "performance_metrics": "Reconstruction term improved dev/test metrics substantially: example given for GPT-2M Greedy decoding: Conditional BLEU 25.73 vs Rec. BLEU 30.41 on dev; overall best models use reconstruction. Test numbers (with PENMAN and reconstruction) include GPT-2M Rec. BLEU 32.10 and GPT-2L Rec. BLEU 32.47.",
            "comparison_to_other_representations": "Compared to conditional-only fine-tuning (no reconstruction term) the joint reconstruction objective yields large gains (e.g., +4.68 BLEU for GPT-2M in dev greedy setting).",
            "limitations_or_challenges": "Adds training complexity and requires tokenization/vocabulary extension for AMR symbols; it's a heuristic objective (reconstructing sequential AMR) rather than directly enforcing graph structure; its effectiveness may depend on quality of sequentialization and vocabulary handling.",
            "uuid": "e5352.3",
            "source_info": {
                "paper_title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Cycle-rescoring",
            "name_full": "Cycle-consistency re-scoring via AMR parsing and Smatch",
            "brief_description": "A post-generation re-scoring method that parses candidate sentences back to AMR graphs and scores them by Smatch agreement with the gold AMR, selecting candidates that better reconstruct the source graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Cycle-consistency re-scoring (Smatch-based)",
            "representation_description": "Generate top-k candidate sentences for a given gold AMR using the LM; parse each candidate using an off-the-shelf AMR parser to produce a reconstructed AMR; compute Smatch between reconstructed AMR and gold AMR; re-rank or select the candidate with highest Smatch score as final output.",
            "graph_type": "AMR graphs (used as gold input and reconstructed graphs from generated text)",
            "representation_properties": "Operates as an external semantic-consistency filter leveraging an AMR parser and Smatch metric; enforces semantic fidelity to the original graph rather than surface similarity to a reference; computationally heavier at inference (parsing k candidates).",
            "evaluation_task": "Re-ranking generated outputs for AMR-to-text to improve semantic fidelity and standard metrics",
            "performance_metrics": "Cycle-consistency re-ranking improved performance over beam-best: reported dev numbers include increase up to 33.57 BLEU and 64.86 chrF++ (compared to beam-best lower values). On test set GPT-2M Rec. re-scoring BLEU 32.98 and GPT-2L Rec. re-scoring BLEU 33.02 (improvements over non re-scored variants).",
            "comparison_to_other_representations": "Used as an orthogonal improvement on top of PENMAN+GPT-2+reconstruction; consistently boosted BLEU, METEOR and chrF++ compared to selecting top beam output.",
            "limitations_or_challenges": "Depends on accuracy of the external AMR parser (errors there can mis-rank good candidates); adds inference-time cost (parse k candidates); Smatch measures graph overlap which may not capture all desirable linguistic variations; requires available AMR parser trained for same AMR formalism.",
            "uuid": "e5352.4",
            "source_info": {
                "paper_title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Tokenization & embedding freezing",
            "name_full": "AMR token mapping, vocabulary extension and frozen input embeddings",
            "brief_description": "Practical preprocessing/training techniques: map AMR special symbols to unused GPT-2 tokens and extend vocabulary with arc labels and :root, while freezing GPT-2 input embeddings during fine-tuning to stabilize training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "AMR-special token mapping and frozen embeddings",
            "representation_description": "Special AMR symbols are assigned to GPT-2's unused token slots and arc labels plus the root marker (:root) are added to the GPT-2 vocabulary; during fine-tuning the input embedding layer is frozen (not updated) which empirically improved generation performance.",
            "graph_type": "AMR graph serialized tokens (PENMAN/DFS)",
            "representation_properties": "Enables pretrained LM to ingest non-linguistic AMR symbols without catastrophic changes to embeddings; freezing embeddings preserves pretrained distributional knowledge and prevents overfitting to small AMR datasets; pragmatic approach to bridge formal graph tokens and LM vocabulary.",
            "evaluation_task": "Used across AMR-to-text fine-tuning experiments; part of implementation details that impacted dev/test performance",
            "performance_metrics": "Freezing input embeddings reported to have a positive impact on performance (no precise numeric ablation values provided), used in best reported models (metrics reported elsewhere in paper).",
            "comparison_to_other_representations": "Technique is orthogonal to representation choice (PENMAN/DFS) and was applied when fine-tuning GPT-2; not directly compared to alternative vocabulary strategies in the paper.",
            "limitations_or_challenges": "Mapping introduces arbitrary token assignments for AMR symbols which may not reflect semantic similarity; frozen embeddings prevent the model from learning embeddings specialized to AMR tokens, which could limit potential gains if more AMR data were available.",
            "uuid": "e5352.5",
            "source_info": {
                "paper_title": "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural amr: Sequence-to-sequence models for parsing and generation.",
            "rating": 2
        },
        {
            "paper_title": "Modeling graph structure in transformer for better amr-to-text generation.",
            "rating": 2
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning.",
            "rating": 2
        },
        {
            "paper_title": "Enhancing amr-to-text generation with dual graph representations.",
            "rating": 2
        },
        {
            "paper_title": "Structural neural encoders for amr-to-text generation.",
            "rating": 2
        },
        {
            "paper_title": "Abstract meaning representation for sembanking.",
            "rating": 2
        }
    ],
    "cost": 0.01170175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPT-too: A Language-Model-First Approach for AMR-to-Text Generation</h1>
<p>Manuel Mager ${ }^{1 *}$ Ramn Fernandez Astudillo ${ }^{2}$ Tahira Naseem ${ }^{2}$ Md Arafat Sultan ${ }^{2}$ Young-Suk Lee ${ }^{2}$ Radu Florian ${ }^{2}$ Salim Roukos ${ }^{2}$<br>${ }^{1}$ Institute for Natural Language Processing, University of Stuttgart, Germany<br>${ }^{2}$ IBM Research AI, Yorktown Heights, NY 10598, USA<br>manuel.mager@ims.uni-stuttgart.de<br>{ramon.astudillo,arafat.sultan}@ibm.com<br>{tnaseem, ysuklee}@us.ibm.com</p>
<h4>Abstract</h4>
<p>Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing "who is doing what to whom". AMR-to-text generates sentences representing the semantics underlying an AMR graph.</p>
<p>Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019)</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism.</p>
<p>Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option. Feeding the network with some sequential representation of the graph, such as a topological sorting, looses some of the graphs representational power. Complex graph annotations, such as AMR, also contain many special symbols and special constructs that departure from natural language and may by not interpretable by a pretrained language model.</p>
<p>In this paper we explore the possibility of directly fine-tuning a pre-trained transformer language model on a sequential representation of AMR graphs, despite the expected difficulties listed above. For this we re-purpose a GPT-2 language model (Radford et al., 2019) to yield an AMR-to-text system. We show that it is surprisingly easy to fine-tune GPT-2 to learn AMR graph to text mapping that outperforms the previous state-of-the-art on automatic evaluation metrics. Since a single graph AMR, graph corresponds to multiple sentences with the same meaning, we</p>
<p>also provide human evaluation and semantic similarity metric results (Zhang et al., 2020) which are less dependent on reference text. Human evaluation and semantic similarity results highlight the positive impact of a strong language model strategy. Finally we also introduce a simple re-scoring technique based on cycle-consistency that further improves performance.</p>
<h2>2 Fine-tuning GPT-2 for conditional language generation</h2>
<p>In order to fine-tune a generative model (GPT-2; Radford et al. (2019)) for conditional text generation, prior works fine-tune the language model to predict target text starting from the additional source text as context. In our experiments, we found it beneficial to fine-tune on the joint distribution of AMR and text instead i.e. also reconstruct the source. Given a tokenized sentence $w_{1} \cdots w_{N}$ and the sequential AMR representation $a_{1} \cdots a_{M}$ we maximized the joint probability</p>
<p>$$
\begin{aligned}
p_{\mathrm{GPT}-2}(\mathbf{w}, \mathbf{a})= &amp; \prod_{j=1}^{N} p_{\mathrm{GPT}-2}\left(w_{j} \mid w_{1: j-1}, a_{1: M}\right) \
&amp; \cdot \prod_{i=1}^{M} p_{\mathrm{GPT}-2}\left(a_{i} \mid a_{1: i-1}\right)
\end{aligned}
$$</p>
<p>A special separator token is added to mark the end of the sequential AMR representation. Special AMR symbols that should not be interpreted literally are assigned tokens from the GPT-2 unused token list. In addition to this, we also observed that freezing the input embeddings when fine-tuning had positive impact in performance.</p>
<p>At test time, we provide the AMR as context as in conventional conditional text generation:</p>
<p>$$
\hat{w}<em w__j="w_{j">{j}=\arg \max </em>\right)\right}
$$}}\left{p_{\mathrm{GPT}-2}\left(w_{j} \mid w_{1: j-1}, a_{1: M</p>
<h2>3 Re-scoring via Cycle Consistency</h2>
<p>The general idea of cycle consistency is to assess the quality of a system's output based on how well an external 'reverse' system can reconstruct the input from it. In previous works, cycle-consistency based losses have been used as part of the training objective in machine translation (He et al., 2016) and speech recognition (Hori et al., 2019). It has
also been used for filtering synthetic training data for question answering (Alberti et al., 2019). Here we propose the use of a cycle consistency measure to re-score the system outputs.</p>
<p>In particular, we take the top $k$ sentences generated by our system from each gold AMR graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs.</p>
<h2>4 Experimental setup</h2>
<p>Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in PENMAN notation and the corresponding texts. It also includes 1368 and 1371 development and test instances, respectively. We tokenize each input text using The JAMR toolkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all arc labels seen in the training set and the root node : root to the vocabulary of the GPT-2model, but we freeze the embedding layer for training. We use the Hugging Face implementation of (Wolf et al., 2019) for GPT-2 small (GPT-2S), medium (GPT-2M) and large (GPT-2L). Fine-tuning converges after 6 epochs, which takes just a few hours on a V100 $\mathrm{GPU}^{1}$. For cycle-consistency re-scoring we use an implementation of Naseem et al. (2019) in PyTorch. For re-scoring experiments, we use a beam size of 15 .</p>
<p>AMR input representation. we test three variants of AMR representation. First, a depth-first search (DFS) through the graph following Konstas et al. (2017), where the input sequence is the path followed in the graph. Second, to see if GPT-2 is in fact learning from the graph structure, we remove all the edges from the DFS, keeping only the concept nodes. This has the effect of removing the relation information between concepts, such as subject/object relations. As a third option, we use the PENMAN representation without any modification. The three input representations are illustrated below:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Nodes</th>
<th style="text-align: left;">recommend advocate-01 it <br> vigorous</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DFS</td>
<td style="text-align: left;">recommend :ARG1 advocate-01 <br> :ARG1 it :manner vigorous</td>
</tr>
<tr>
<td style="text-align: left;">Penman</td>
<td style="text-align: left;">(r / recommend-01 :ARG1 (a / <br> advocate-01 :ARG1 (i / it) <br> :manner (v / vigorous)))</td>
</tr>
</tbody>
</table>
<p>Decoding. For generation, we experiment with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5,10 and 15 . As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences.</p>
<p>Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation. ${ }^{2}$</p>
<p>In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one single AMR graph corresponds to multiple sentences with the same semantic meaning. Conventional metrics for AMR-to-text are are strongly influenced by surface symbols and thus do not capture well the ability of the system to produce a diverse sentences with same underlying semantics.</p>
<p>Human evaluations are carried out by three professional annotators on 51 randomly selected sentences from the 1371 test sentences, on a 6 point scale, ranging from 0 to 5 .</p>
<ul>
<li>0=Exceptionally poor (No useful information is conveyed at all.)</li>
<li>1=Poor (Fundamental errors in grammar and vocabulary make it difficult to understand the meaning.)</li>
<li>2=Not good enough (Errors in grammar, vocabulary and style make it difficult to understand the meaning.)</li>
<li>3=Good enough (There are errors in the text, but I am reasonably confident that I understand the meaning.)</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Results on the LDC2017T10 development set using GPT-2 S(mall) and M(edium) with Rec(onstruction) loss (see $\S 2$ ) for different AMR representations (see $\S 4$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">Decoding</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-2M Conditional</td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">25.73</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec.</td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">30.41</td>
<td style="text-align: center;">61.36</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec.</td>
<td style="text-align: center;">BEAM</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">62.56</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec.</td>
<td style="text-align: center;">BEAM 10</td>
<td style="text-align: center;">32.32</td>
<td style="text-align: center;">62.79</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec.</td>
<td style="text-align: center;">Sampling</td>
<td style="text-align: center;">28.75</td>
<td style="text-align: center;">61.19</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on the LDC2017T10 development set. Rec(onstruction) uses the AMR reconstruction term (see $\S 2$ ) whereas Conditional does not.</p>
<ul>
<li>4=Very good (There may be minor errors in the text, but I am very confident that I understand the meaning.)</li>
<li>5=Excellent (The information is presented clearly and with appropriate grammar, vocabulary and style.)</li>
</ul>
<p>For each system, scores from all annotators are averaged to compute a single score. Inter-annotator agreement was 0.7 when measured by Pearson correlation coefficient.</p>
<p>Our system produces de-tokenized cased output after BPE decoding, whereas previous systems produce traditional tokenized lower-cased output. Therefore, we lowercase and tokenize our system outputs to have fair comparisons with previous systems.</p>
<h3>4.1 Results</h3>
<p>Regarding the type of AMR representation, as shown in Table 1, using directly the PENMAN notation for AMR representation leads to the best results outperforming DFS. Edge information, indicating relations between concepts, seems also to play a fundamental role since its absence strongly decreases performance in both DFS and PENMAN representations. Penman notation was chosen for the rest of the experiments.</p>
<p>The impact of the use of a reconstruction term explained in $\S 2$ is shown in Table 2. The model trained using this additional term achieves 30.41 BLEU and $61.36 \mathrm{chrF}++$, as opposed to 25.73</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Meteor</td>
<td style="text-align: center;">chrF++</td>
</tr>
<tr>
<td style="text-align: center;">Beck et al. (2018)</td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.40</td>
</tr>
<tr>
<td style="text-align: center;">Damonte and Cohen (2019)</td>
<td style="text-align: center;">24.54</td>
<td style="text-align: center;">24.07</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2019)</td>
<td style="text-align: center;">27.60</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.30</td>
</tr>
<tr>
<td style="text-align: center;">Cao and Clark (2019)</td>
<td style="text-align: center;">26.80</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Sinh and Le Minh (2019)</td>
<td style="text-align: center;">18.36</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ribeiro et al. (2019)</td>
<td style="text-align: center;">27.87</td>
<td style="text-align: center;">33.21</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Cai and Lam (2020)</td>
<td style="text-align: center;">29.80</td>
<td style="text-align: center;">35.10</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: center;">Zhu et al. (2019)</td>
<td style="text-align: center;">31.82</td>
<td style="text-align: center;">36.38</td>
<td style="text-align: center;">64.05</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec.</td>
<td style="text-align: center;">$32.10^{\text {a }}$</td>
<td style="text-align: center;">$35.86^{\circ}$</td>
<td style="text-align: center;">$61.81^{\text {* }}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2L Rec.</td>
<td style="text-align: center;">$32.47^{\text {a }}$</td>
<td style="text-align: center;">$36.80^{\circ}$</td>
<td style="text-align: center;">$62.88^{\text {* }}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec. re-scoring</td>
<td style="text-align: center;">$32.98^{\text {a }}$</td>
<td style="text-align: center;">$37.33^{\circ}$</td>
<td style="text-align: center;">$63.09^{\text {* }}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2L Rec. re-scoring</td>
<td style="text-align: center;">$33.02^{\text {a }}$</td>
<td style="text-align: center;">$37.68^{\circ}$</td>
<td style="text-align: center;">$63.89^{\square}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on the LDC2017T10 test set for best performing models compared to other results reported in the literature. ${ }^{\text {a }}$ indicates statistical significance at $(P&lt;.01),{ }^{\circ}$ at $(P&lt;0.05)$ and ${ }^{\square}$, not significant. All significance tests are with respect to (Zhu et al., 2019).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">LDC2017T10</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human Eval.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SemSim</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">P45</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2019)</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">15.69\%</td>
<td style="text-align: center;">92.68</td>
</tr>
<tr>
<td style="text-align: center;">Ribeiro et al. (2019)</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">16.37\%</td>
<td style="text-align: center;">92.63</td>
</tr>
<tr>
<td style="text-align: center;">Zhu et al. (2019)</td>
<td style="text-align: center;">2.61</td>
<td style="text-align: center;">20.26\%</td>
<td style="text-align: center;">93.31</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2M Rec.</td>
<td style="text-align: center;">3.03</td>
<td style="text-align: center;">37.91\%</td>
<td style="text-align: center;">94.55</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2L Rec.</td>
<td style="text-align: center;">3.04</td>
<td style="text-align: center;">41.83\%</td>
<td style="text-align: center;">94.63</td>
</tr>
</tbody>
</table>
<p>Table 4: Human evaluation and semantic similarity (SemSim) results on the LDC2017T10 test set. Human evaluations (Human Eval.) show the average (Avg.) of scores ( 0 to 5 ) and the ratio of sentence evaluated between 4 and 5 (P45). All results for human evaluation are on 51 randomly selected sentences and statistically significant at $(P&lt;0.05)$. SemSim results are significant at $(P&lt;0.01)$. All significance tests refer to a comparison with (Zhu et al., 2019).</p>
<p>BLEU and 57.2 chrF++ without the term. We therefore use a reconstruction term training in the rest of the experiments.</p>
<p>Beam search improves system performance greatly over the greedy baseline with 1.91 BLEU points (see Table 2). With beam size 10 , we obtain 32.32 BLEU and 62.79 chrF++. With nucleus sampling at a cumulative probability mass of 0.9 , performance drops to 28.75 BLEU and 61.19 chrF++. Finally, cycle-consistency re-ranking of the beam search outputs improves performance ( 33.57 BLEU, 64.86 chrF++) over the one best output.</p>
<p>Table 3 compares the best GPT-2M and GPT-2L results, fine-tuned using the reconstruc-
tion term and PENMAN notation. For all scores we test statistical significance with a standard two-tailed student t-test. Our model achieves a large improvement of 1.2 BLEU and 1.3 METEOR scores over the previous state-of-the-art model using GPT-2L and re-scoring. For chrF++, we get different scores from SacreBLEU and the scripts provided by the authors of our baseline systems, achieving comparable results with the former (63.89), and improving over the best score with the latter $(65.01)(P&lt;.01)$.</p>
<p>Table 4 shows human Evaluation results and semantic similarity scores of GPT-2L and GPT-2M compared to (Zhu et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). Our approach produces a large number of high-quality sentences with $41.8 \%$, a significant gain over the previous best system $(20.26 \%)$. Regarding semantic similarity, prior art methods show relatively close scores, a 0.9 points difference, while GPT-2L Rec. improves 1.6 points over the best of these models. It should be noted that differences with (Zhu et al., 2019) for GPT-2L Rec. are statistically significantly with $P&lt;.05$, while differences for GPT-2M Rec are not significant due to the small sample size.</p>
<p>In Table 5 we show three nontrivial examples, where we compare our system outputs with those of previous work. In the first example, the reference sentence contains a grammatical error. Our system not only generates the correct output, but also corrects the error in the reference. The proposed system can generate fluent long sentences as shown in example 2. The third example shows a sentence where all systems including ours fail to generate a correct text.</p>
<h3>4.2 Discussion</h3>
<p>Due to the large amounts of data they are trained on, pre-trained transformer language models can be expected to generate fluent and diverse text (See et al., 2019). It should however be highlighted that fine-tuned GPT-2 learns to produce not only fluent but also adequate text, despite using a sequential representation of an AMR graph as input. As shown in the experimental setup, encoding of relations plays as well a fundamental role in AMR-to-text performance, indicating that GPT-2 attains a fine-grained understanding of the underlying semantics to reach state of the art performance.</p>
<p>While a sequence of PENMAN notation to-</p>
<p>System Generated text
(1) REF: the doctors gave her medication and it 's made her much better .
G2S: the doctor gives her medications and they make her much better .
Transf: doctors give her medications and make her much better .
Our: the doctor gave her the medication and made her feel much better.
Our R.: the doctor gave her the medication and made her " much better " .
(2) REF: at the state scientific center of applied microbiology there is every kind of deadly bacteria that was studied for use in the secret biological weapons program of the soviet union .
G2S: there are every kind of killing <unk> in the state scientific center of applied microbiology to use themselves for soviet union 's secret biological weapons programs .
Transf: there is every kind of bacterium, which is studied in using bacterium for the soviet union secret biological weapons program .
Our: every kind of bacterium that was studied was found at the state scientific center of applied microbiology and was used in soviet secret weapons programs for biological weapons of biology .
Our R.: every kind of bacterium that has been studied and used in soviet secret programs for biological weapons has been in the state scientific center of applied microbiology .
(3) REF: among the nations that have not signed the treaty only india and israel would qualify for admission to the nsg under the israeli proposal .
G2S: only one of the nations who do not sign the treaty are qualified for their proposal to admit the nsg .
Transf: india and israel are only qualified for the nations that do not sign the treaty, but they admitted to the nsg .
Our: india and israel are the only countries eligible to admit to the nsg by proposing a treaty .
Our R.: only india and israel are eligible to admit to the nsg by proposing a treaty .
Table 5: Output examples from four systems of the LDC2017T10 dataset. REF stands for reference, G2S for (Guo et al., 2019) and Transf. for (Zhu et al., 2019). Our is the top beam output for GPT-2L and Our R. is with re-scoring.
kens is far from an optimal encoding of a graph, it is noteworthy how far performance-wise current strong language models can go. Furthermore, It is likely that standard metrics (BLEU, Meteor, chrF++) that rely on a reference text do not properly reflect AMR-to-text quality. An AMR graph corresponds to multiple sentences with the same semantics and these measures are likely biased towards the single available reference. In metrics that are less influenced by the reference text such as human evaluation and semantic similarity, the proposed system shows a larger improvement over the previous systems with close to $50 \%$ of the generated sentences considered excellent or good.</p>
<p>Finally it is worth considering that leveraging pre-trained transformers greatly expands the vocabulary available on AMR-to-text systems. A single AMR graph can correspond to multiple sentences with markedly different surface realizations, but manual annotation of AMR is a time consuming task. Approaches like the one proposed may be a simple solution for generation of diverse text data for AMR parser training or other applications were diversity play a role.</p>
<h2>5 Conclusions</h2>
<p>In this work, we present a language model-based approach for the AMR-to-text generation task. We show that a strong pre-trained transformer language model (GPT-2) can be fine-tuned to generate text directly from the PENMAN notation of an AMR graph. Comparison with state-of-the-art models in BLUE, chrF++, METEOR as well as SemSim and human evaluation metrics show that while simple, this approach can outperform existing methods including methods training transformers from scratch. We also show that cycle consistency-based re-scoring using a conventional AMR parser and the Smatch metric can notably improve the results. Future work will focus on incorporating better encoding of the AMR graph into the current system and exploring data augmentation techniques leveraging the proposed approach.</p>
<h2>Acknowledgments</h2>
<p>We thank the reviewers for their valuable suggestions. We would also like to thank Chunchuan Lyu for his valuable feedback and help.</p>
<h2>References</h2>
<p>Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic QA corpora generation with roundtrip consistency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283.</p>
<p>Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. In 34th AAAI conference on artificial intelligence.</p>
<p>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748-752.</p>
<p>Kris Cao and Stephen Clark. 2019. Factorising amr generation through syntax. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2157-2163.</p>
<p>Marco Damonte and Shay B Cohen. 2019. Structural neural encoders for amr-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649-3658.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A Smith, and Jaime Carbonell. 2016. Generation from abstract meaning
representation using tree transducers. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, pages 731-739.</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14261436 .</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computational Linguistics, 7:297-312.</p>
<p>Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pages 820-828.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751.</p>
<p>Takaaki Hori, Ramon Astudillo, Tomoki Hayashi, Yu Zhang, Shinji Watanabe, and Jonathan Le Roux. 2019. Cycle-consistency training for end-to-end speech recognition. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6271-6275. IEEE.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 146-157.</p>
<p>Qingsong Ma, Johnny Wei, Ondej Bojar, and Yvette Graham. 2019. Results of the wmt19 metrics shared task: Segment-level and strong mt systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62-90.</p>
<p>Tahira Naseem, Abhishek Shah, Hui Wan, Radu Florian, Salim Roukos, and Miguel Ballesteros. 2019. Rewarding Smatch: Transition-based AMR parsing with reinforcement learning.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Maja Popovi. 2017. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612-618.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating english from abstract meaning representations. In Proceedings of the 9th international natural language generation conference, pages 21-25.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pretraining. 2018. URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language_understanding_paper. pdf.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8).</p>
<p>Leonardo FR Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing amr-to-text generation with dual graph representations. arXiv preprint arXiv:1909.00352.</p>
<p>Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D Manning. 2019. Do massively pretrained language models make better storytellers? arXiv preprint arXiv:1909.10705.</p>
<p>Vu Trong Sinh and Nguyen Le Minh. 2019. A study on self-attention mechanism for amr-to-text generation. In International Conference on Applications of Natural Language to Information Systems, pages 321-328. Springer.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr-to-text generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16161626.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. Amr-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics, 8:19-33.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. 2019. Transformers: State-of-theart natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2019. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536.</p>
<p>Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better amr-to-text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages $5462-5471$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.cs.cmu.edu/ alavie/METEOR&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
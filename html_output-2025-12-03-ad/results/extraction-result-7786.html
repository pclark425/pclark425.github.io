<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7786 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7786</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7786</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-273228756</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.07076v1.pdf" target="_blank">MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES</a></p>
                <p><strong>Paper Abstract:</strong> Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7786.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7786.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE-CHEM: Multi-agent framework for hypothesis discovery in chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic LLM-based multi-stage framework that implements a decomposition of P(hypothesis | background) into iterative inspiration retrieval, hypothesis composition, and hypothesis ranking; includes multi-step inspiration retrieval, an evolutionary-unit for mutation/recombination, beam search, and an LLM-based ranking function R(h).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used to run the framework and generate hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training data up to October 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Benchmarked rediscovery + reference-based scoring + automatic LLM scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run MOOSE-Chem on a held-out benchmark of 51 post-2023 chemistry papers (only background provided) using a literature corpus I (titles+abstracts) to retrieve inspirations, compose hypotheses and rank them; evaluate generated hypotheses with reference-based Matched Score (MS) using LLM automatic evaluation and human expert annotation, and measure inspiration retrieval Hit Ratio and ranking quality (Average Rank Ratio).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score (MS), Top MS, Average MS, Hit Ratio (inspiration retrieval), Average Rank Ratio (ranking quality)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Matched Score: 6-point Likert (0–5) measuring textual/methodological overlap to ground-truth hypothesis; Top/Avg MS: highest/mean MS across hypotheses per background; Hit Ratio: fraction of ground-truth inspiration papers retrieved; Average Rank Ratio: normalized average rank of hypotheses (lower = better).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>TOMATO-Chem benchmark (51 high-impact chemistry/materials papers; inspiration corpus I constructed from up to 3,000 top-cited chemistry papers; screening windows of various sizes used)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Benchmark constructed and annotated by three chemistry PhD students (background, inspirations, hypothesis); expert evaluation: for selected outputs two chemistry PhD students rated Matched Score and case-study evaluations; inter-rater agreement reported (hard/soft consistency metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>MOOSE-Chem rediscovered many ground-truth hypotheses with high MS; reported automatic-evaluation scores include (GPT-4o) Top MS ≈ 4.020, Avg MS ≈ 2.564; (Claude-3.5-Sonnet) Top MS 4.471, Avg MS 3.697; (Gemini-1.5-Proto) Top MS 3.686, Avg MS 2.443. Inspiration retrieval Hit Ratio >75% in some settings (two rounds, small screening window).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Compared generated hypotheses to ground-truth (human-published) hypotheses via Matched Score and expert ratings; many generated hypotheses covered core innovations (MS >=3 considered high), but ground-truth hypotheses were not always top-ranked by R(h).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Matched Score measures similarity to a single ground-truth hypothesis (may penalize novel-but-valid alternatives); LLM-based automatic evaluation can be optimistic relative to experts; ranking is imperfect; no automated falsifiability tests or lab validation of generated hypotheses; inspiration retrieval is an OOD task and relies on LLM latent associations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7786.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TOMATO-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TOMATO-Chem benchmark of post-2023 chemistry hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually constructed benchmark of 51 high-impact chemistry/materials papers (published and publicly available in 2024), each decomposed into background question, background survey, one to three inspiration paper titles/reasons, and the ground-truth hypothesis, used to evaluate LLM-driven hypothesis discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used to run evaluations against this benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training data up to October 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Benchmark / evaluation dataset for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reference-based evaluation using Matched Score and expert adjudication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each generated hypothesis is compared to the benchmark's ground-truth hypothesis via a structured Matched Score rubric (0–5); selected top outputs are also evaluated by chemistry PhD experts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score (MS), Top MS, Average MS, Hit Ratio for inspiration retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Matched Score: 0–5 scale (0 = no overlap; 5 = covers all key points similarly to ground truth); Hit Ratio: fraction of ground-truth inspirations retrieved from the literature corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>TOMATO-Chem (51 annotated papers) plus inspiration corpus I (subset of ≈3,000 top-cited chemistry papers; experiments run with |I|=150,300,1000,3000 in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Construction and checks performed by multiple chemistry PhD students; expert re-evaluation of top hypotheses performed by two chemistry PhD students; third expert used for consistency checks in some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as held-out test: MOOSE-Chem recovered many hypotheses with high MS; inspiration retrieval hit ratios reported (e.g., with screening window 15 and two rounds, Hit Ratio up to 83.7% in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Generated hypotheses compared directly to human (published) ground-truth hypotheses via MS; many LLM outputs matched core innovations though not always identical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Reference-based MS penalizes valid alternative hypotheses; benchmark limited to 51 papers and to papers published in a narrow time window (post-Jan 2024); inspiration corpus constructed from titles+abstracts only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7786.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Matched Score (MS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Matched Score — reference-based hypothesis similarity metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-point reference-based evaluation metric (0–5) designed to quantify how closely an LLM-generated hypothesis matches the methodology/key points of a ground-truth published hypothesis, with granular definitions for partial, subset, superset, and near-equality matches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used for automatic MS labeling in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training data up to October 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials Science (applies to hypothesis comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation metric for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Matched Score (reference-based similarity scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare generated hypothesis (gh) to original ground-truth hypothesis (oh) and a listed set of key methodological points; assign MS 0–5 based on coverage and usage of key points: 5 = covers all key points similarly and no flawed extra points; 4 = covers all/3 key points but with flawed extras; 3 = covers two key points; 2 = covers one key point similarly; 1 = covers at least one key point but used differently; 0 = no coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score (single scalar 0–5); Top MS and Average MS aggregated across outputs per background</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Discrete ordinal scale 0–5 (0: no overlap; 1–3: partial overlap increasing by number and fidelity of matched key points; 4: full coverage with flawed extras; 5: full coverage and fidelity).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied on TOMATO-Chem benchmark (51 items) and MOOSE-Chem outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>MS assigned in some experiments by GPT-4o automatically and in other analyses by human experts (chemistry PhD students). Agreement between GPT-4o and experts reported (hard/soft consistency metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as main quantitative target: distributions of MS reported per method and ablation (e.g., MOOSE-Chem Top/Avg MS values reported by various automatic evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>MS measures similarity to human (published) hypotheses; paper reports many LLM outputs achieving MS>=3 (covering two main innovations), indicating substantial overlap with human hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MS measures similarity to a single ground-truth statement (may miss valid novelty); discrete ordinal scale depends on provided key points and human judgement; automatic MS (LLM-based) tends to be 1–2 points higher than experts on average.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7786.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R(h)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R(h) — LLM-based hypothesis rating / ranking function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-prompted rating function that assigns 4 aspect scores (validness, novelty, significance, potential) on 1–5 scales to each generated hypothesis; the average of these aspect scores is used to rank hypotheses for beam search and multi-round selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (LLM used to implement R(h) in experiments); also evaluated with Claude-3.5-Sonnet and Gemini-1.5-Proto</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training data up to October 2023); Claude-3.5-Sonnet and Gemini-1.5-Proto used for robustness checks</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation / ranking method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM aspect-based scoring (validness, novelty, significance, potential)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM to act as a 'diligent and harsh reviewer' rating each hypothesis on four dimensions using a 5-point scale with detailed rubric; compute average score and use it to sort hypotheses; beam search retains top-k hypotheses for next rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Aspect scores (validness, novelty, significance, potential) each 1–5; average rating used as R(h)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Four 1–5 ordinal scales with descriptive anchors (detailed in Appendix A.4); R(h) = mean(validness, novelty, significance, potential).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to MOOSE-Chem outputs on TOMATO-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human graders for R(h) per se; LLM-generated R(h) compared against human expert judgments to assess ranking reliability; agreement analyzed in §5.3.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>R(h) shows a certain ability to rank high-quality hypotheses higher; correlation between number of matched inspirations (#Matched i) and Average Rank Ratio reported (more matched inspirations → better rank).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-based R(h) correlates with expert judgments moderately but can be optimistic; ground-truth hypotheses were not always top-ranked by R(h).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM evaluators may be biased toward well-articulated or more-detailed hypotheses; scoring is heuristic and not a substitute for experimental validation; significance feedback can reduce matched-score alignment by encouraging more creative (less-matching) hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7786.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hit Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hit Ratio (inspiration retrieval metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval metric measuring the fraction of ground-truth inspiration papers that the LLM-selected inspiration set covers, averaged over benchmark items; used to quantify P(i_j | b, h_{j-1}, I) performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (primary LLM used for inspiration retrieval experiments); Llama-series also compared</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training cutoff Oct 2023); Llama variants of different parameter scales in ablation (not parameterized here)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Information retrieval over literature</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Retrieval evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hit Ratio over inspiration corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each background, count how many annotated ground-truth inspiration papers are selected by the LLM during screening rounds and divide by the total number of ground-truth inspirations; report average across benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit Ratio (fraction or percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hit Ratio = (number of selected ground-truth inspiration papers) / (total number of ground-truth inspiration papers) per background, averaged across dataset. Reported as percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Inspiration corpus I constructed from ≈3,000 most-cited Nature chemistry papers; experiments with |I|=150,300,1000,3000</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Ground-truth inspirations annotated by PhD chemists serve as reference; selection by LLM compared to that reference.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>High Hit Ratios reported (e.g., >75% coverage by 4% of corpus in some settings; specific table entries: with screen window 15, two rounds Hit Ratio reached up to ≈83.7% for |I|=300).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A (metric compares LLM retrieval to human-annotated inspirations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Depends on the annotated set of 'ground-truth inspirations' which may be subjective; retrieval evaluated on titles/abstracts only; selection constrained by screening window size and order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7786.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Average Rank Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average Rank Ratio (ranking evaluation metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A normalized metric reporting the average rank position of hypotheses (or ground-truth hypothesis) among generated candidates, used to quantify the effectiveness of R(h) ranking (lower = better).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (R(h) used to rank hypotheses whose ranks are measured by this metric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training cutoff Oct 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / evaluation of hypothesis ranking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Ranking evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Average Rank Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute normalized average ranking position of hypotheses (or ground-truth hypothesis) among generated hypotheses after sorting by R(h); used to show trends (e.g., hypotheses leveraging more ground-truth inspirations tend to have better (lower) rank ratios).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average Rank Ratio (scalar, lower indicates better rank on average)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Average Rank Ratio = mean over examples of (rank_position / number_of_candidates) or similar normalized rank; exact normalization described in paper tables (lower is better).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to MOOSE-Chem outputs on TOMATO-Chem with |I|=300 in ranking experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Rank positions computed from automatic R(h) scores; relationships between #Matched i and Average Rank Ratio reported and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Clear trend: more matched inspirations (#Matched i) correlated with lower (better) Average Rank Ratio; shown in Table 8.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A (metric used to quantify internal ranking quality rather than direct human vs. machine).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Aggregate ranking metric can mask per-item nuances; ranking improvements may partially reflect heuristics in prompts rather than true scientific quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7786.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemistry PhD expert human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual assessment by domain experts (PhD chemists) scoring generated hypotheses (typically using Matched Score rubric); used to validate and calibrate LLM automatic evaluations and to provide ground-truth annotations for the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (human evaluation of LLM outputs and automatic LLM evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expert Matched Score rating and consensus checking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Two chemistry PhD students independently evaluate selected generated hypotheses on the Matched Score rubric; additional expert checks for benchmark annotation; third expert used to assess inter-rater consistency for sampled evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score (0–5) assigned by experts; inter-rater consistency measured by 'hard' and 'soft' agreement rates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Hard consistency: exact-match between two raters (1 if equal else 0); Soft consistency: absolute difference < 2 (1 if true else 0); aggregate rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to MOOSE-Chem outputs on TOMATO-Chem (top hypotheses per background)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Benchmark construction: three chemistry PhD students; expert evaluations: two PhD chemists for many comparisons; third expert used for consistency checks; reported agreement: moderate-to-high (e.g., GPT-4o vs experts: hard consistency ≈34.5%, soft ≈54.2% in one analysis; other comparisons show higher soft agreement up to ≈85.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Experts confirmed many generated hypotheses captured main innovations (MS >=3); expert MS typically lower than automatic LLM MS by ~1–2 points.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Expert ratings used to validate similarity between LLM-generated and human (published) hypotheses; moderate agreement with automatic evaluators indicates reliability of reference-based evaluation but also highlights differences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Domain expertise required and available experts may still disagree; per-topic variability in expert familiarity can affect scores; costly and time-consuming so only top outputs were human-rated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7786.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic evaluation using LLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Proto)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of strong LLMs prompted to assign Matched Score or R(h) aspect scores to generated hypotheses, enabling large-scale automatic evaluation and ablation comparisons across evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Proto</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (cutoff Oct 2023); Claude-3.5-Sonnet; Gemini-1.5-Proto (versions as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (applied to chemistry hypothesis evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated evaluation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based Matched Score / aspect scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt LLMs with the Matched Score instruction (Appendix A.12) or the R(h) prompt (Appendix A.4) to assign scores for each generated hypothesis; compare outputs across evaluators to assess robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score (0–5) and R(h) aspect scores (1–5); Top/Avg MS aggregates</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See Matched Score and R(h) definitions; automatic evaluators produce same scalar outputs as human evaluators for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to MOOSE-Chem outputs on TOMATO-Chem; results reported in Tables 10–14 comparing evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Automatic evaluations compared to human expert ratings to compute agreement (hard/soft consistency); automatic evaluators generally rate outputs 1–2 points higher than experts on average.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Automatic-evaluator results are robust across models: Claude and Gemini produce similar trends to GPT-4o; tables report varying Top/Avg MS per evaluator (e.g., Claude: MOOSE-Chem Top MS 4.471 Avg 3.697; GPT-4o: Top MS 4.020 Avg 2.564).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automatic evaluators enable large-scale comparison of generated hypotheses to ground-truth human hypotheses; show medium-to-high correlation with human expert judgments but with systematic bias upward.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM evaluators can be optimistic and produce plausible rationales that inflate scores; cross-model variance requires careful calibration with expert judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7786.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evolutionary Unit (EU)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary Unit — mutation, feedback, and recombination module for hypothesis composition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary algorithm-inspired module that generates multiple hypothesis 'mutations' linking a background and an inspiration, iteratively refines them via LLM-provided feedback on validness/novelty/clarity/significance, eliminates low-quality variants, and recombines remaining variants to produce refined hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used to generate mutations and provide feedback during EU operations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training data up to October 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry hypothesis composition / creative search</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis composition algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ablation study comparing EU vs non-EU branches using Matched Score and counts of high-MS hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run MOOSE-Chem with and without EU; measure Matched Score distributions, number of high-MS hypotheses from non-EU vs EU branches, and effect on average MS / Top MS to quantify EU contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score distributions, counts of high-MS hypotheses by branch, Average MS, Top MS</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Counts and aggregated MS metrics as in paper tables (e.g., Table 15 reports counts of high-MS hypotheses obtained only from non-EU, only EU, and EU-recombination branches).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on TOMATO-Chem using MOOSE-Chem experimental runs reported in Tables 10 and 15.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Selected outputs from EU and non-EU branches were evaluated by automatic LLM evaluators and by experts for top candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>EU contributes a substantial fraction of high-quality hypotheses: about one-third of high-MS hypotheses arise without mutation, but EU-recombination branch contains more high-quality hypotheses than the non-EU branch; mutation & recombination improves best-performing h but can lower average MS due to exploratory low-quality mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A (EU compared to non-EU internal ablation rather than human generative baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mutation step produces many low-MS (less sensible) candidates that reduce average MS; recombination benefits but increases computational cost; significance feedback can push mutations away from ground-truth similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7786.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON: Scientific inspiration machines optimized for novelty (baseline referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior hypothesis discovery framework (Wang et al., 2024b) that retrieves inspiration via semantic and citation neighbors and optimizes for novelty; implemented as a baseline and re-implemented with LLM-based inspiration retrieval for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Referenced baseline (re-implemented for comparison using LLM retrieval in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific hypothesis generation (general; prior work applied to social science/NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Baseline framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Baseline comparison via Matched Score and Average Rank Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare MOOSE-Chem to SciMON (and other baselines) on TOMATO-Chem using automatic MS evaluation and reported Top/Avg MS and ranking metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Matched Score (Top/Avg MS), Average Rank Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same MS definitions applied to baseline outputs; aggregated Top/Avg MS per method reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>TOMATO-Chem benchmark used for comparative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Baseline outputs evaluated automatically and some top outputs evaluated by experts as with MOOSE-Chem.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported baseline values (example: SciMON Top MS ≈ 2.980 / Avg MS ≈ 2.618 in one table; values vary by evaluator) — MOOSE-Chem outperforms baselines on Top MS and other metrics in reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>MOOSE-Chem shows improved Top MS over SciMON and other baselines in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Baselines that rely on semantic/citation neighbors may retrieve information too similar to background (not true 'inspirations'); differences in retrieval strategy affect novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7786.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7786.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Screening window / beam search parameters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Screening window size and beam search (selection parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational hyperparameters controlling how many candidate papers are considered per screening step (screening window, default 15) and how many top hypotheses are retained for the next round via beam search (beam size default 15); used in ablation studies to measure retrieval/rediscovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used within the screening and beam-selection pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (training cutoff Oct 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Information retrieval / search strategy in hypothesis discovery</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Operational parameters / methods</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ablation studies varying screening window size and beam size</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Vary screening window sizes (e.g., 10, 15, 20, 40, 60) and measure Hit Ratio after 1–4 screening rounds; apply beam search to cap hypothesis explosion across rounds and evaluate effects on Top/Avg MS.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit Ratio, Top MS, Average MS, % of corpus selected after rounds</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported Hit Ratio and selected-corpus-percentage for configurations; beam size controls number of h passed between rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Inspiration corpus I (|I| variable) and TOMATO-Chem outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Ablation outputs evaluated automatically and selected outputs were expert-rated to assess effect on MS.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Smaller screening window sizes (e.g., 15) with multi-round screening can yield higher Hit Ratios (e.g., 83.7% for certain settings) while selecting a smaller fraction of the corpus; beam search default 15 used to maintain tractable branching.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>N/A (hyperparameter effects on retrieval and downstream MS reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Hyperparameter tuning changes coverage vs. compute trade-offs; results sensitive to corpus size and ordering; stochasticity in LLM outputs affects reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature <em>(Rating: 2)</em></li>
                <li>CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
                <li>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7786",
    "paper_id": "paper-273228756",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "MOOSE-Chem",
            "name_full": "MOOSE-CHEM: Multi-agent framework for hypothesis discovery in chemistry",
            "brief_description": "An agentic LLM-based multi-stage framework that implements a decomposition of P(hypothesis | background) into iterative inspiration retrieval, hypothesis composition, and hypothesis ranking; includes multi-step inspiration retrieval, an evolutionary-unit for mutation/recombination, beam search, and an LLM-based ranking function R(h).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used to run the framework and generate hypotheses)",
            "model_size": "GPT-4o (training data up to October 2023)",
            "scientific_domain": "Chemistry / Materials Science",
            "theory_type": "Hypothesis generation",
            "evaluation_method_name": "Benchmarked rediscovery + reference-based scoring + automatic LLM scoring",
            "evaluation_method_description": "Run MOOSE-Chem on a held-out benchmark of 51 post-2023 chemistry papers (only background provided) using a literature corpus I (titles+abstracts) to retrieve inspirations, compose hypotheses and rank them; evaluate generated hypotheses with reference-based Matched Score (MS) using LLM automatic evaluation and human expert annotation, and measure inspiration retrieval Hit Ratio and ranking quality (Average Rank Ratio).",
            "evaluation_metric": "Matched Score (MS), Top MS, Average MS, Hit Ratio (inspiration retrieval), Average Rank Ratio (ranking quality)",
            "metric_definition": "Matched Score: 6-point Likert (0–5) measuring textual/methodological overlap to ground-truth hypothesis; Top/Avg MS: highest/mean MS across hypotheses per background; Hit Ratio: fraction of ground-truth inspiration papers retrieved; Average Rank Ratio: normalized average rank of hypotheses (lower = better).",
            "dataset_or_benchmark": "TOMATO-Chem benchmark (51 high-impact chemistry/materials papers; inspiration corpus I constructed from up to 3,000 top-cited chemistry papers; screening windows of various sizes used)",
            "human_evaluation_details": "Benchmark constructed and annotated by three chemistry PhD students (background, inspirations, hypothesis); expert evaluation: for selected outputs two chemistry PhD students rated Matched Score and case-study evaluations; inter-rater agreement reported (hard/soft consistency metrics).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "MOOSE-Chem rediscovered many ground-truth hypotheses with high MS; reported automatic-evaluation scores include (GPT-4o) Top MS ≈ 4.020, Avg MS ≈ 2.564; (Claude-3.5-Sonnet) Top MS 4.471, Avg MS 3.697; (Gemini-1.5-Proto) Top MS 3.686, Avg MS 2.443. Inspiration retrieval Hit Ratio &gt;75% in some settings (two rounds, small screening window).",
            "comparison_to_human_generated": true,
            "comparison_results": "Compared generated hypotheses to ground-truth (human-published) hypotheses via Matched Score and expert ratings; many generated hypotheses covered core innovations (MS &gt;=3 considered high), but ground-truth hypotheses were not always top-ranked by R(h).",
            "limitations_noted": "Matched Score measures similarity to a single ground-truth hypothesis (may penalize novel-but-valid alternatives); LLM-based automatic evaluation can be optimistic relative to experts; ranking is imperfect; no automated falsifiability tests or lab validation of generated hypotheses; inspiration retrieval is an OOD task and relies on LLM latent associations.",
            "uuid": "e7786.0",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TOMATO-Chem",
            "name_full": "TOMATO-Chem benchmark of post-2023 chemistry hypotheses",
            "brief_description": "A manually constructed benchmark of 51 high-impact chemistry/materials papers (published and publicly available in 2024), each decomposed into background question, background survey, one to three inspiration paper titles/reasons, and the ground-truth hypothesis, used to evaluate LLM-driven hypothesis discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used to run evaluations against this benchmark)",
            "model_size": "GPT-4o (training data up to October 2023)",
            "scientific_domain": "Chemistry / Materials Science",
            "theory_type": "Benchmark / evaluation dataset for hypothesis generation",
            "evaluation_method_name": "Reference-based evaluation using Matched Score and expert adjudication",
            "evaluation_method_description": "Each generated hypothesis is compared to the benchmark's ground-truth hypothesis via a structured Matched Score rubric (0–5); selected top outputs are also evaluated by chemistry PhD experts.",
            "evaluation_metric": "Matched Score (MS), Top MS, Average MS, Hit Ratio for inspiration retrieval",
            "metric_definition": "Matched Score: 0–5 scale (0 = no overlap; 5 = covers all key points similarly to ground truth); Hit Ratio: fraction of ground-truth inspirations retrieved from the literature corpus.",
            "dataset_or_benchmark": "TOMATO-Chem (51 annotated papers) plus inspiration corpus I (subset of ≈3,000 top-cited chemistry papers; experiments run with |I|=150,300,1000,3000 in ablations)",
            "human_evaluation_details": "Construction and checks performed by multiple chemistry PhD students; expert re-evaluation of top hypotheses performed by two chemistry PhD students; third expert used for consistency checks in some analyses.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as held-out test: MOOSE-Chem recovered many hypotheses with high MS; inspiration retrieval hit ratios reported (e.g., with screening window 15 and two rounds, Hit Ratio up to 83.7% in some settings).",
            "comparison_to_human_generated": true,
            "comparison_results": "Generated hypotheses compared directly to human (published) ground-truth hypotheses via MS; many LLM outputs matched core innovations though not always identical.",
            "limitations_noted": "Reference-based MS penalizes valid alternative hypotheses; benchmark limited to 51 papers and to papers published in a narrow time window (post-Jan 2024); inspiration corpus constructed from titles+abstracts only.",
            "uuid": "e7786.1",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Matched Score (MS)",
            "name_full": "Matched Score — reference-based hypothesis similarity metric",
            "brief_description": "A 6-point reference-based evaluation metric (0–5) designed to quantify how closely an LLM-generated hypothesis matches the methodology/key points of a ground-truth published hypothesis, with granular definitions for partial, subset, superset, and near-equality matches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used for automatic MS labeling in experiments)",
            "model_size": "GPT-4o (training data up to October 2023)",
            "scientific_domain": "Chemistry / Materials Science (applies to hypothesis comparison)",
            "theory_type": "Evaluation metric for hypotheses",
            "evaluation_method_name": "Matched Score (reference-based similarity scoring)",
            "evaluation_method_description": "Compare generated hypothesis (gh) to original ground-truth hypothesis (oh) and a listed set of key methodological points; assign MS 0–5 based on coverage and usage of key points: 5 = covers all key points similarly and no flawed extra points; 4 = covers all/3 key points but with flawed extras; 3 = covers two key points; 2 = covers one key point similarly; 1 = covers at least one key point but used differently; 0 = no coverage.",
            "evaluation_metric": "Matched Score (single scalar 0–5); Top MS and Average MS aggregated across outputs per background",
            "metric_definition": "Discrete ordinal scale 0–5 (0: no overlap; 1–3: partial overlap increasing by number and fidelity of matched key points; 4: full coverage with flawed extras; 5: full coverage and fidelity).",
            "dataset_or_benchmark": "Applied on TOMATO-Chem benchmark (51 items) and MOOSE-Chem outputs",
            "human_evaluation_details": "MS assigned in some experiments by GPT-4o automatically and in other analyses by human experts (chemistry PhD students). Agreement between GPT-4o and experts reported (hard/soft consistency metrics).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as main quantitative target: distributions of MS reported per method and ablation (e.g., MOOSE-Chem Top/Avg MS values reported by various automatic evaluators).",
            "comparison_to_human_generated": true,
            "comparison_results": "MS measures similarity to human (published) hypotheses; paper reports many LLM outputs achieving MS&gt;=3 (covering two main innovations), indicating substantial overlap with human hypotheses.",
            "limitations_noted": "MS measures similarity to a single ground-truth statement (may miss valid novelty); discrete ordinal scale depends on provided key points and human judgement; automatic MS (LLM-based) tends to be 1–2 points higher than experts on average.",
            "uuid": "e7786.2",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "R(h)",
            "name_full": "R(h) — LLM-based hypothesis rating / ranking function",
            "brief_description": "An LLM-prompted rating function that assigns 4 aspect scores (validness, novelty, significance, potential) on 1–5 scales to each generated hypothesis; the average of these aspect scores is used to rank hypotheses for beam search and multi-round selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (LLM used to implement R(h) in experiments); also evaluated with Claude-3.5-Sonnet and Gemini-1.5-Proto",
            "model_size": "GPT-4o (training data up to October 2023); Claude-3.5-Sonnet and Gemini-1.5-Proto used for robustness checks",
            "scientific_domain": "Chemistry / Materials Science",
            "theory_type": "Evaluation / ranking method",
            "evaluation_method_name": "LLM aspect-based scoring (validness, novelty, significance, potential)",
            "evaluation_method_description": "Prompt an LLM to act as a 'diligent and harsh reviewer' rating each hypothesis on four dimensions using a 5-point scale with detailed rubric; compute average score and use it to sort hypotheses; beam search retains top-k hypotheses for next rounds.",
            "evaluation_metric": "Aspect scores (validness, novelty, significance, potential) each 1–5; average rating used as R(h)",
            "metric_definition": "Four 1–5 ordinal scales with descriptive anchors (detailed in Appendix A.4); R(h) = mean(validness, novelty, significance, potential).",
            "dataset_or_benchmark": "Applied to MOOSE-Chem outputs on TOMATO-Chem",
            "human_evaluation_details": "No human graders for R(h) per se; LLM-generated R(h) compared against human expert judgments to assess ranking reliability; agreement analyzed in §5.3.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "R(h) shows a certain ability to rank high-quality hypotheses higher; correlation between number of matched inspirations (#Matched i) and Average Rank Ratio reported (more matched inspirations → better rank).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-based R(h) correlates with expert judgments moderately but can be optimistic; ground-truth hypotheses were not always top-ranked by R(h).",
            "limitations_noted": "LLM evaluators may be biased toward well-articulated or more-detailed hypotheses; scoring is heuristic and not a substitute for experimental validation; significance feedback can reduce matched-score alignment by encouraging more creative (less-matching) hypotheses.",
            "uuid": "e7786.3",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Hit Ratio",
            "name_full": "Hit Ratio (inspiration retrieval metric)",
            "brief_description": "A retrieval metric measuring the fraction of ground-truth inspiration papers that the LLM-selected inspiration set covers, averaged over benchmark items; used to quantify P(i_j | b, h_{j-1}, I) performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (primary LLM used for inspiration retrieval experiments); Llama-series also compared",
            "model_size": "GPT-4o (training cutoff Oct 2023); Llama variants of different parameter scales in ablation (not parameterized here)",
            "scientific_domain": "Chemistry / Information retrieval over literature",
            "theory_type": "Retrieval evaluation metric",
            "evaluation_method_name": "Hit Ratio over inspiration corpus",
            "evaluation_method_description": "For each background, count how many annotated ground-truth inspiration papers are selected by the LLM during screening rounds and divide by the total number of ground-truth inspirations; report average across benchmark.",
            "evaluation_metric": "Hit Ratio (fraction or percentage)",
            "metric_definition": "Hit Ratio = (number of selected ground-truth inspiration papers) / (total number of ground-truth inspiration papers) per background, averaged across dataset. Reported as percentage.",
            "dataset_or_benchmark": "Inspiration corpus I constructed from ≈3,000 most-cited Nature chemistry papers; experiments with |I|=150,300,1000,3000",
            "human_evaluation_details": "Ground-truth inspirations annotated by PhD chemists serve as reference; selection by LLM compared to that reference.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "High Hit Ratios reported (e.g., &gt;75% coverage by 4% of corpus in some settings; specific table entries: with screen window 15, two rounds Hit Ratio reached up to ≈83.7% for |I|=300).",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A (metric compares LLM retrieval to human-annotated inspirations).",
            "limitations_noted": "Depends on the annotated set of 'ground-truth inspirations' which may be subjective; retrieval evaluated on titles/abstracts only; selection constrained by screening window size and order.",
            "uuid": "e7786.4",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Average Rank Ratio",
            "name_full": "Average Rank Ratio (ranking evaluation metric)",
            "brief_description": "A normalized metric reporting the average rank position of hypotheses (or ground-truth hypothesis) among generated candidates, used to quantify the effectiveness of R(h) ranking (lower = better).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (R(h) used to rank hypotheses whose ranks are measured by this metric)",
            "model_size": "GPT-4o (training cutoff Oct 2023)",
            "scientific_domain": "Chemistry / evaluation of hypothesis ranking",
            "theory_type": "Ranking evaluation metric",
            "evaluation_method_name": "Average Rank Ratio",
            "evaluation_method_description": "Compute normalized average ranking position of hypotheses (or ground-truth hypothesis) among generated hypotheses after sorting by R(h); used to show trends (e.g., hypotheses leveraging more ground-truth inspirations tend to have better (lower) rank ratios).",
            "evaluation_metric": "Average Rank Ratio (scalar, lower indicates better rank on average)",
            "metric_definition": "Average Rank Ratio = mean over examples of (rank_position / number_of_candidates) or similar normalized rank; exact normalization described in paper tables (lower is better).",
            "dataset_or_benchmark": "Applied to MOOSE-Chem outputs on TOMATO-Chem with |I|=300 in ranking experiments",
            "human_evaluation_details": "Rank positions computed from automatic R(h) scores; relationships between #Matched i and Average Rank Ratio reported and analyzed.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Clear trend: more matched inspirations (#Matched i) correlated with lower (better) Average Rank Ratio; shown in Table 8.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A (metric used to quantify internal ranking quality rather than direct human vs. machine).",
            "limitations_noted": "Aggregate ranking metric can mask per-item nuances; ranking improvements may partially reflect heuristics in prompts rather than true scientific quality.",
            "uuid": "e7786.5",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Human expert evaluation",
            "name_full": "Chemistry PhD expert human evaluation",
            "brief_description": "Manual assessment by domain experts (PhD chemists) scoring generated hypotheses (typically using Matched Score rubric); used to validate and calibrate LLM automatic evaluations and to provide ground-truth annotations for the benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (human evaluation of LLM outputs and automatic LLM evaluations)",
            "model_size": "N/A",
            "scientific_domain": "Chemistry / Materials Science",
            "theory_type": "Human evaluation protocol",
            "evaluation_method_name": "Expert Matched Score rating and consensus checking",
            "evaluation_method_description": "Two chemistry PhD students independently evaluate selected generated hypotheses on the Matched Score rubric; additional expert checks for benchmark annotation; third expert used to assess inter-rater consistency for sampled evaluations.",
            "evaluation_metric": "Matched Score (0–5) assigned by experts; inter-rater consistency measured by 'hard' and 'soft' agreement rates",
            "metric_definition": "Hard consistency: exact-match between two raters (1 if equal else 0); Soft consistency: absolute difference &lt; 2 (1 if true else 0); aggregate rates reported.",
            "dataset_or_benchmark": "Applied to MOOSE-Chem outputs on TOMATO-Chem (top hypotheses per background)",
            "human_evaluation_details": "Benchmark construction: three chemistry PhD students; expert evaluations: two PhD chemists for many comparisons; third expert used for consistency checks; reported agreement: moderate-to-high (e.g., GPT-4o vs experts: hard consistency ≈34.5%, soft ≈54.2% in one analysis; other comparisons show higher soft agreement up to ≈85.4%).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Experts confirmed many generated hypotheses captured main innovations (MS &gt;=3); expert MS typically lower than automatic LLM MS by ~1–2 points.",
            "comparison_to_human_generated": true,
            "comparison_results": "Expert ratings used to validate similarity between LLM-generated and human (published) hypotheses; moderate agreement with automatic evaluators indicates reliability of reference-based evaluation but also highlights differences.",
            "limitations_noted": "Domain expertise required and available experts may still disagree; per-topic variability in expert familiarity can affect scores; costly and time-consuming so only top outputs were human-rated.",
            "uuid": "e7786.6",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Automated LLM evaluators",
            "name_full": "Automatic evaluation using LLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Proto)",
            "brief_description": "Use of strong LLMs prompted to assign Matched Score or R(h) aspect scores to generated hypotheses, enabling large-scale automatic evaluation and ablation comparisons across evaluators.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Proto",
            "model_size": "GPT-4o (cutoff Oct 2023); Claude-3.5-Sonnet; Gemini-1.5-Proto (versions as used in experiments)",
            "scientific_domain": "General (applied to chemistry hypothesis evaluation)",
            "theory_type": "Automated evaluation method",
            "evaluation_method_name": "LLM-based Matched Score / aspect scoring",
            "evaluation_method_description": "Prompt LLMs with the Matched Score instruction (Appendix A.12) or the R(h) prompt (Appendix A.4) to assign scores for each generated hypothesis; compare outputs across evaluators to assess robustness.",
            "evaluation_metric": "Matched Score (0–5) and R(h) aspect scores (1–5); Top/Avg MS aggregates",
            "metric_definition": "See Matched Score and R(h) definitions; automatic evaluators produce same scalar outputs as human evaluators for comparison.",
            "dataset_or_benchmark": "Applied to MOOSE-Chem outputs on TOMATO-Chem; results reported in Tables 10–14 comparing evaluators.",
            "human_evaluation_details": "Automatic evaluations compared to human expert ratings to compute agreement (hard/soft consistency); automatic evaluators generally rate outputs 1–2 points higher than experts on average.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Automatic-evaluator results are robust across models: Claude and Gemini produce similar trends to GPT-4o; tables report varying Top/Avg MS per evaluator (e.g., Claude: MOOSE-Chem Top MS 4.471 Avg 3.697; GPT-4o: Top MS 4.020 Avg 2.564).",
            "comparison_to_human_generated": true,
            "comparison_results": "Automatic evaluators enable large-scale comparison of generated hypotheses to ground-truth human hypotheses; show medium-to-high correlation with human expert judgments but with systematic bias upward.",
            "limitations_noted": "LLM evaluators can be optimistic and produce plausible rationales that inflate scores; cross-model variance requires careful calibration with expert judgements.",
            "uuid": "e7786.7",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Evolutionary Unit (EU)",
            "name_full": "Evolutionary Unit — mutation, feedback, and recombination module for hypothesis composition",
            "brief_description": "An evolutionary algorithm-inspired module that generates multiple hypothesis 'mutations' linking a background and an inspiration, iteratively refines them via LLM-provided feedback on validness/novelty/clarity/significance, eliminates low-quality variants, and recombines remaining variants to produce refined hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used to generate mutations and provide feedback during EU operations)",
            "model_size": "GPT-4o (training data up to October 2023)",
            "scientific_domain": "Chemistry hypothesis composition / creative search",
            "theory_type": "Hypothesis composition algorithm",
            "evaluation_method_name": "Ablation study comparing EU vs non-EU branches using Matched Score and counts of high-MS hypotheses",
            "evaluation_method_description": "Run MOOSE-Chem with and without EU; measure Matched Score distributions, number of high-MS hypotheses from non-EU vs EU branches, and effect on average MS / Top MS to quantify EU contribution.",
            "evaluation_metric": "Matched Score distributions, counts of high-MS hypotheses by branch, Average MS, Top MS",
            "metric_definition": "Counts and aggregated MS metrics as in paper tables (e.g., Table 15 reports counts of high-MS hypotheses obtained only from non-EU, only EU, and EU-recombination branches).",
            "dataset_or_benchmark": "Evaluated on TOMATO-Chem using MOOSE-Chem experimental runs reported in Tables 10 and 15.",
            "human_evaluation_details": "Selected outputs from EU and non-EU branches were evaluated by automatic LLM evaluators and by experts for top candidates.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "EU contributes a substantial fraction of high-quality hypotheses: about one-third of high-MS hypotheses arise without mutation, but EU-recombination branch contains more high-quality hypotheses than the non-EU branch; mutation & recombination improves best-performing h but can lower average MS due to exploratory low-quality mutations.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A (EU compared to non-EU internal ablation rather than human generative baseline).",
            "limitations_noted": "Mutation step produces many low-MS (less sensible) candidates that reduce average MS; recombination benefits but increases computational cost; significance feedback can push mutations away from ground-truth similarity.",
            "uuid": "e7786.8",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SciMON (baseline)",
            "name_full": "SciMON: Scientific inspiration machines optimized for novelty (baseline referenced)",
            "brief_description": "A prior hypothesis discovery framework (Wang et al., 2024b) that retrieves inspiration via semantic and citation neighbors and optimizes for novelty; implemented as a baseline and re-implemented with LLM-based inspiration retrieval for comparison.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "model_name": "Referenced baseline (re-implemented for comparison using LLM retrieval in this study)",
            "model_size": "",
            "scientific_domain": "Scientific hypothesis generation (general; prior work applied to social science/NLP)",
            "theory_type": "Baseline framework",
            "evaluation_method_name": "Baseline comparison via Matched Score and Average Rank Ratio",
            "evaluation_method_description": "Compare MOOSE-Chem to SciMON (and other baselines) on TOMATO-Chem using automatic MS evaluation and reported Top/Avg MS and ranking metrics.",
            "evaluation_metric": "Matched Score (Top/Avg MS), Average Rank Ratio",
            "metric_definition": "Same MS definitions applied to baseline outputs; aggregated Top/Avg MS per method reported in tables.",
            "dataset_or_benchmark": "TOMATO-Chem benchmark used for comparative evaluation",
            "human_evaluation_details": "Baseline outputs evaluated automatically and some top outputs evaluated by experts as with MOOSE-Chem.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Reported baseline values (example: SciMON Top MS ≈ 2.980 / Avg MS ≈ 2.618 in one table; values vary by evaluator) — MOOSE-Chem outperforms baselines on Top MS and other metrics in reported tables.",
            "comparison_to_human_generated": false,
            "comparison_results": "MOOSE-Chem shows improved Top MS over SciMON and other baselines in reported experiments.",
            "limitations_noted": "Baselines that rely on semantic/citation neighbors may retrieve information too similar to background (not true 'inspirations'); differences in retrieval strategy affect novelty metrics.",
            "uuid": "e7786.9",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Screening window / beam search parameters",
            "name_full": "Screening window size and beam search (selection parameters)",
            "brief_description": "Operational hyperparameters controlling how many candidate papers are considered per screening step (screening window, default 15) and how many top hypotheses are retained for the next round via beam search (beam size default 15); used in ablation studies to measure retrieval/rediscovery performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (used within the screening and beam-selection pipeline)",
            "model_size": "GPT-4o (training cutoff Oct 2023)",
            "scientific_domain": "Information retrieval / search strategy in hypothesis discovery",
            "theory_type": "Operational parameters / methods",
            "evaluation_method_name": "Ablation studies varying screening window size and beam size",
            "evaluation_method_description": "Vary screening window sizes (e.g., 10, 15, 20, 40, 60) and measure Hit Ratio after 1–4 screening rounds; apply beam search to cap hypothesis explosion across rounds and evaluate effects on Top/Avg MS.",
            "evaluation_metric": "Hit Ratio, Top MS, Average MS, % of corpus selected after rounds",
            "metric_definition": "Reported Hit Ratio and selected-corpus-percentage for configurations; beam size controls number of h passed between rounds.",
            "dataset_or_benchmark": "Inspiration corpus I (|I| variable) and TOMATO-Chem outputs",
            "human_evaluation_details": "Ablation outputs evaluated automatically and selected outputs were expert-rated to assess effect on MS.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Smaller screening window sizes (e.g., 15) with multi-round screening can yield higher Hit Ratios (e.g., 83.7% for certain settings) while selecting a smaller fraction of the corpus; beam search default 15 used to maintain tractable branching.",
            "comparison_to_human_generated": false,
            "comparison_results": "N/A (hyperparameter effects on retrieval and downstream MS reported).",
            "limitations_noted": "Hyperparameter tuning changes coverage vs. compute trade-offs; results sensitive to corpus size and ordering; stochasticity in LLM outputs affects reproducibility.",
            "uuid": "e7786.10",
            "source_info": {
                "paper_title": "MOOSE-C HEM : L ARGE L ANGUAGE M ODELS FOR R EDISCOVERING U NSEEN C HEMISTRY S CIENTIFIC H YPOTHESES",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "rating": 2,
            "sanitized_title": "unsupervised_word_embeddings_capture_latent_knowledge_from_materials_science_literature"
        },
        {
            "paper_title": "CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback",
            "rating": 2,
            "sanitized_title": "chemreasoner_heuristic_search_over_a_large_language_models_knowledge_space_using_quantumchemical_feedback"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design",
            "rating": 1,
            "sanitized_title": "monte_carlo_thought_search_large_language_model_querying_for_complex_scientific_reasoning_in_catalyst_design"
        }
    ],
    "cost": 0.024083,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MOOSE-CHEM: LARGE LANGUAGE MODELS FOR REDISCOVERING UNSEEN CHEMISTRY SCIENTIFIC HYPOTHESES
27 Oct 2025</p>
<p>Zonglin Yang zonglin001@ntu.edu.sg 
Nanyang Technological University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Wanhao Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>University of Science and Technology of China</p>
<p>Ben Gao 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wuhan University</p>
<p>Tong Xie 
University of New
South Wales</p>
<p>GreenDynamics</p>
<p>Yuqiang Li 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wanli Ouyang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Soujanya Poria 
Singapore University of Technology</p>
<p>Erik Cambria cambria@ntu.edu.sg 
Nanyang Technological University</p>
<p>Dongzhan Zhou zhoudongzhan@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>MOOSE-CHEM: LARGE LANGUAGE MODELS FOR REDISCOVERING UNSEEN CHEMISTRY SCIENTIFIC HYPOTHESES
27 Oct 2025F25C15CFBC5B2B16BA6E1C89E5CC0BABarXiv:2410.07076v6[cs.CL]
Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process.However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry.In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question.We begin with the observation that hypothesis discovery is a seemingly intractable task.To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations.This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses-which together constitute a sufficient set of subtasks for the overall scientific discovery task.We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition.To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis.The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024.Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans. 1</p>
<p>INTRODUCTION</p>
<p>Discovering new science has long been one of the deepest desires of humanity, which can not only satisfy our curiosity to understand the universe but also contribute largely to the prosperity of human society (Coccia, 2019).Recently, there are some breakthroughs indicating that LLMs have the potential to assist scientists in accelerating the discovery process (Luo et al., 2025).Yang et al. (2024b) first find that LLMs can generate novel and valid enough hypotheses evaluated by experts.They focus on the social science domain and make discoveries by developing a multi-agent system, leveraging an assumption that a majority of social science hypotheses can be divided into a research background concept and an inspiration concept.This assumption is largely valid because a social science hypothesis is about how an independent variable can influence another dependent variable (Hair et al., 2007).Si et al. (2024) further validate this finding by employing a large group of scientists to evaluate LLMs' generated hypotheses in the NLP domain and show that LLM can generate more novel but slightly less valid research hypotheses than human researchers.However, it is still unclear LLMs' scientific discovery ability in natural science such as the chemistry domain.Sprueill et al. (2023;2024) adopt LLMs to conduct a search process for catalyst discovery.However, their method is limited in the catalyst discovery domain, and their evaluation relies on whether LLMs can rediscover existing commercially used catalysts, potentially influenced by a data contamination problem.As a result, it is still unclear how good LLMs are for chemistry scientific discovery.</p>
<p>In this paper, we investigate this central research question: Can LLMs automatically discover novel and valid chemistry research hypotheses (even at the Nature level) given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question?With extensive discussions with chemistry experts, we find that the assumption used in social science, that a hypothesis can be divided into background and inspiration, can also apply to a majority of chemistry hypotheses.It is not too surprising, since cognitive science research has shown that creative ideas often result from the cohesive association of two seemingly unrelated pieces of knowledge (Koestler, 1964;Benedek et al., 2012;Lee &amp; Chung, 2024).A main difference is that chemistry might need more than one inspiration (e.g., adding several components to compose a novel chemistry system).With this key insight, we break the seemingly impossible-to-solve central question into three smaller, more practical, and executable fundamental questions that, when summed up, should be very close to a set of sufficient conditions for the central question.Specifically, the smaller questions are (1) whether LLM can identify inspiration papers that have the potential to help with the given research question; (2) given only known knowledge (from background and inspirations), whether LLMs can infer unknown knowledge that is highly likely to be valid; and (3) whether LLM can identify good hypotheses and rank them higher.</p>
<p>To investigate these three questions, we build a benchmark consisting of 51 chemistry papers annotated by chemistry PhD students, breaking every paper into a background, several inspirations, and a hypothesis.The goal is to rediscover the hypothesis with only the background by using LLMs trained with data up to December 2023.The papers are all published in Nature, Science, or a similar level in 2024, and they are only made public on the internet in 2024.The benchmark is designed to be similar to the Mathematical Olympiad Competition (Trinh et al., 2024), to provide several dozens of very difficult and meaningful questions to solve.Along with the benchmark, we propose a ranking task for scientific discovery (along with evaluation criteria), which has been largely overlooked in previous works (Yang et al., 2024a;Wang et al., 2024b).Ranking is important because although AI systems can generate a large number of hypotheses in a relatively short time, verifying them one by one requires a lot of experimental costs.</p>
<p>Motivated by this breakup into three smaller questions, we design a multi-agent framework named MOOSE-CHEM for chemistry scientific discovery.It in general includes three stages: (1) searching through chemistry literature to find inspiration papers, (2) leveraging the inspirations to propose hypotheses for the background research question, and (3) identifying high-quality hypotheses to give them a higher rank.Compared with Yang et al. (2024b)'s method in social science that assumes a similar separation between background and inspiration for hypothesis formulation, MOOSE-CHEM adopts an evolutionary algorithm to foster a broader diversity of approaches in using inspiration for background, thereby capitalizing on the benefits derived from varied mutations.In addition, MOOSE-CHEM also adopts a multi-step design to collect more than one inspirations for chemistry discovery.Finally, it uses an efficient ranking method for better reference for scientists.</p>
<p>We design experiments with the benchmark to test the three fundamental questions and find that LLMs are highly capable.We also test MOOSE-CHEM with the benchmark, mimicking the setting to run it in the wild by only giving a background and a corpus of up to 3,000 chemistry papers to select inspiration.Even in this challenging setting, MOOSE-CHEM can still rediscover many hypotheses with very high similarity with the ground truth ones, covering the main innovations.</p>
<p>Overall, the contributions of this paper are:</p>
<p>• We provide the first mathematical derivation on how to decompose the seemingly impossible-to-solve question P (hypothesis|research background) into many executable and practical smaller steps.This decomposition make P (hypothesis|research background) possible to be practical.</p>
<p>• We develop a scientific discovery framework directly based on the mathematical derivation.Different from previous works, we propose an evolutionary algorithm-based method to better associate background and inspiration, multi-step inspiration retrieval and composition, and an efficient ranking method.In addition, the framework can be applied to chemistry and material science, which are not covered by previous methods.</p>
<p>• We construct a benchmark by three chemistry PhD students, consisting of 51 chemistry papers published on Nature, Science, or a similar level, decomposing each paper into the research background, inspirations, and hypothesis.</p>
<p>• We propose an assumption, grounded in preliminary experiments, that LLMs may already possess numerous knowledge pairs capable of being associated to create novel knowledge-even when scientists have not previously recognized any relationship between them.</p>
<p>• For the first time, we show that an LLM-based framework can largely rediscover the main innovations of many chemistry hypotheses that have been published in Nature and Science.The rediscovery is not because of data contamination, because we have controlled the date of the training corpus of the LLM and the online date of the chemistry papers.(2024) focus on subsequent steps for scientific discovery, mainly developing and conducting experiments.Sprueill et al. (2023;2024) focus on catalyst discovery, but their evaluation relies on whether can rediscover existing commercially used catalysts, which might cause data contamination problem.Kumar et al. (2024) compare different LLMs on scientific discovery in different disciplines.Tshitoyan et al. (2019) show that word embedding obtained from large-scale chemistry literature can recommend materials years before their discovery.Xie et al. (2024) predict emerging thermoelectric materials by summarizing the sentiment in the existing literature.</p>
<p>RELATED WORK</p>
<p>BENCHMARK CONSTRUCTION</p>
<p>The goal of the benchmark, named TOMATO-Chem, is two-fold.Firstly, it is used to analyze LLM's ability in terms of the three smaller questions.Secondly, it serves as a challenge to rediscover naturelevel chemistry hypotheses with only a research background.The setting of the challenge is very similar to a real copilot setting, where scientists tell the copilot about the specific research question they are interested in, and optionally a small survey consisting of several paragraphs summarizing the existing best-performing methods for the research question.</p>
<p>To achieve the goals, we split each collected paper into the following components: <background question, background question (strict), background survey, background survey (strict), one to three inspiration paper titles and their reason to serve as an inspiration, research hypothesis, experiments, reasoning process, summarization of inspirations>.Every component is described by text.</p>
<p>The reason we add a strict version for background question and background survey is that many hypotheses are making relatively minor modifications based on existing methods covered by the survey, and the question can be very insightful to provide a hint on the general direction of the hypothesis.In practice, these situations are entirely possible, especially when the scientist users can provide a more comprehensive survey on existing methods, or contain deep insights in their question.Here, we also keep the strict version to make the task more challenging and encourage developing methods to better assist scientists even when they are also new to their research topic.</p>
<p>The reasoning process indicates the relation between the components of background, inspirations, and hypothesis.For example, the reasoning process can be "background + inspiration 1 + inspiration 2 = hypothesis", or "background + inspiration 1/inspiration 2 + inspiration 3 = hypothesis".</p>
<p>The benchmark consists of 51 chemistry and material science papers and is constructed by multiple chemistry PhD students.We only select those papers published on top chemistry venues and be public on the internet after January 2024.After constructing, the experts check again on (1) whether the identification of the inspirations is correct and whether more inspirations are needed; (2) whether the background does not contain any information in inspirations or hypothesis; and (3) whether the background and the identified inspirations can roughly logically lead to the hypothesis.The complete instruction on the check process is shown in § A.3.   1, such as polymer material and organic material.Around 13 collected benchmark papers are inside the material science domain.Beyond them, more papers have intersections with material science.In this paper, we target both chemistry and material science, but for simplicity, we only refer to them as chemistry in this paper.</p>
<p>METHODOLOGY</p>
<p>FUNDAMENTAL ASSUMPTION AND FOLLOWING DECOMPOSITION</p>
<p>We propose an assumption that a majority of chemistry hypotheses can originate from a research background and several inspirations.This assumption is not only supported by many chemistry researchers whom we have extensive discussions with but also by the cognitive science finding that "creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge" (Koestler, 1964;Benedek et al., 2012;Lee &amp; Chung, 2024).We design our method based on this fundamental assumption.</p>
<p>Denoting background knowledge as b, inspiration knowledge as i, and hypothesis as h, we translate this assumption as:
h = f (b, i 1 , . . . , i k )(1)
Here, k ∈ Z represents the number of inspirations needed for a particular h.Typically in chemistry, k ∈ [1, 3].In other words, given existing knowledge in the background, a majority of chemistry research is about searching knowledge that previously not known to be related to the background but in fact can assist the background, then associate the background knowledge and the searched knowledge in a reasonable way to compose a hypothesis.</p>
<p>Based on this assumption, we can transform the seemingly impossible-to-solve P (h | b) into an equivalent form, where each step in the equivalent form is practical and executable.
P (h | b) ≈ k j=1 P (i j | b, h j−1 , I) • P (h j | b, h j−1 , i j ), where h 0 = ∅ (2)
This decomposition naturally defines a Markov Decision Process (MDP), where each intermediate hypothesis h j−1 and selected inspiration i j constitute a state-action pair, P (i j | b, h j−1 , I) models the policy of selecting the next inspiration, and P (h j | b, h j−1 , i j ) models the state transition toward the next step of hypothesis.Here, I denotes the full (chemistry) literature, representing the entire inspiration space to search for each i.The complete derivation and theoretical justification are provided in § A.2, which forms the core of this paper.</p>
<p>Equation 2 is meaningful in that by decomposing P (h | b) into more practical and executable smaller questions, the seemingly impossible-to-solve P (h | b) itself becomes practical.We analyze how P (i j | b, h j−1 , I) and P (h j | b, h j−1 , i j ) are practical and executable by LLMs in § 5.1 and § 5.2 correspondingly.Now we have clarified the steps to obtain h from b.However, it still might not be enough helpful in practice, since I can be on a large scale, and the search process might find lots of i, and finally lead to lots of h.Moreover, it is very time-consuming for scientists to conduct experiments to verify every single h.Therefore, it would be very helpful if the generated h could be ranked based on quality.Here, we adopt a straightforward and efficient way for ranking.Specifically, we design a rating function R(h), such that R(h) → R. Denoting the full set of generated h as H, we can obtain
P (H ranked ) = P (H, R), where H ranked = {h 1 , h 2 , . . . , h n | R(h i ) ≥ R(h i+1 ) for all i} (3)
Supported by Equation 2 and Equation 3, as a result, to model P (h | b), the only three components we need to model are P (i j | b, h j−1 , I), P (h j | b, h j−1 , i j ), and R(h).The implementation details of the three components are illustrated in the remaining subsections in § 4. Analyses of LLM's ability on the three components are provided in § 5.</p>
<p>THE FRAMEWORK DEVELOPED BASED ON THE ASSUMPTION</p>
<p>THE GENERAL PICTURE</p>
<p>Our methodology is developed based on the fundamental assumption discussed in § 4.1.Specifically, we use LLMs to perform P (i j | b, h j−1 , I), P (h j | b, h j−1 , i j ), and R(h), and organize them into a multi-agent LLM-based framework.The input to the framework is only a background question and/or background survey, together with a (large) chemistry literature corpus to search for inspiration.The output of the framework is a list of ranked research hypothesis.</p>
<p>The framework's design is shown in Figure 1 (overview in Figure 2).It is a direct implementation of Equation 2 and 3. We develop it as simply as possible, retaining only the necessary parts.</p>
<p>In the general picture, given a research background b (research question and/or research survey), the framework first performs P (i 1 | b, h 0 = ∅, I) by screening through the literature corpus I to select many papers i, where each of them has the potential to serve as an inspiration.Then the framework performs P (h 1 | b, i 1 , h 0 = ∅), associating b and each i together to compose h.Then, it ranks h by assigning an evaluation score r on each of h 1 by R(h 1 ).We call these three steps as one round.</p>
<p>Another round means going through the three steps again, based on the previous round's results.</p>
<p>Since normally in chemistry, no more than three inspirations are needed for one hypothesis (k ∈ [1, 3]), the default setting for MOOSE-Chem is to perform three rounds for each b.In every other round, the number of i and h can expand exponentially.Here, we adopt beam search to select a fixed size of the top-ranked h to enter the next round.The default beam size is 15.</p>
<p>4.2.2DESIGN DETAILS OF P (i j | b, h j−1 , I) AND ITS MOTIVATION</p>
<p>We use LLMs to conduct a screening process for P (i j | b, h j−1 , I).Specifically, for each inference, we (1) sequentially select a fixed number of papers from I, where the fixed number is called the screening window size (default is 15); (2) set up a prompt consisting of b, the title and abstract of the selected papers from I, and the previous h (if it is not ∅); and (3) instruct the LLM to generate three titles from the input that can best serve as i for b (and optionally previous h), and give reasons.</p>
<p>In particular, we use LLMs to choose potential inspiration i, but not choose i from citation nor semantic neighbors because i is supposed to be previously not known to be related to b (we have discussed it in § 4.1).If the chosen i is already known to be related to b, then the composed h probably would not be novel.If the chosen i contains similar semantic information with b, then probably it is not necessary to add i at all, since it does not introduce much (any) extra information.</p>
<p>Our bold assumption here is that advanced LLMs, trained on vast scientific literature, may already recognize novel knowledge pairs unknown to any scientist that can be associated to create novel knowledge.However, this may not be too bold, as Tshitoyan et al. ( 2019) showed that unsupervised word embeddings from 3.3 million materials science abstracts could predict functional materials years before their discovery.Here, the functional applications can be seen as b, and the recommended materials can be seen as i, or even directly as h if it is enough similar.It probably indicates that LLMs trained with significantly more literature tokens and parameters might already be able to identify the relation between many knowledge pairs that are unknown to be related by any scientist.</p>
<p>We analyze this assumption in § 5.1.</p>
<p>DESIGN DETAILS OF
P (h j | b, h j−1 , i j ) AND ITS MOTIVATION
The retrieved i is expected to be not known to be related to b; therefore, it might be difficult to figure out an effective way to associate b and i together to compose h.Think of the time when backpropagation is about to be invented.Even if we are very familiar with b (multi-layer logistic regression) and have successfully retrieved i (chain rule in mathematics), can we invent backpropagation?</p>
<p>Our answer is, at least we might need to try multiple times and various ways to leverage the chain rule for multi-layer logistic regression.With this motivation, we develop a simple evolutionary algorithm-based method, shown in the top-right of Figure 1.We call it "evolutionary unit" (EU).</p>
<p>Specifically, given b and i, EU will first generate multiple hypothesis "mutations" m, where each m is a unique way to associate b and i together.Then EU further develops each m independently by providing feedback to each m in terms of validness, novelty, clarity, and significance, and then refining them based on the feedback.Yang et al. (2024b) first propose to provide feedback in terms of validness, novelty, and clarity to refine hypotheses.Here, we add an additional aspect, significance, since significance is an important evaluation criterion in chemistry.We assume the refined hypothesis should be of better quality so that the refined hypothesis is "selected", while the previous hypothesis is "eliminated" by the "environment".Finally EU "recombines" the remaining selected m, leveraging the advantages from every m to propose h to better associate b and i.</p>
<p>DESIGN DETAILS OF R(h) AND ITS MOTIVATION</p>
<p>We adopt a simple and efficient way for R(h), which is to prompt an LLM to output evaluation scores for an input h in terms of validness, novelty, significance, and potential.Validness and novelty are two fundamental requirements for such an inductive reasoning process as scientific discovery (Yang et al., 2024a;b).Significance is added because it is important for chemistry.We additionally add potential, because the generated h are about to be further developed by scientists, so we might want to pick those h that not only are currently in high quality but also have good potential to be further developed.We did not design R(h) in a more complicated way, since there are lots of h to rank, and we might want to save more inference time.Yang et al. (2024b) use the scores as automatic evaluation for generated social science hypotheses and have shown a high consistency score between automatic evaluation and expert evaluation.However, in the chemistry domain, LLMs might not be reliable enough to directly evaluate the generated h (Sprueill et al., 2024).But it might still be able to provide a preliminary quality identifier to h: the ranking of the average score between the four aspects of an h determines whether it will enter the next round of MOOSE-Chem by beam search.To understand how well LLMs can perform R(h), we analyze "how well LLMs can rank chemistry hypotheses" in § 5.3.5.1 HOW WELL CAN LLMS PERFORM P (i j | b, h j−1 , I)?</p>
<p>INVESTIGATION ON FUNDAMENTAL QUESTIONS</p>
<p>Here, we investigate the question (denoted as Q1): "whether LLM can identify inspiration papers which are unknown to be able to associate with the background (or at least unknown to associate in a certain way) but in fact can associate with the background to create novel knowledge?".</p>
<p>We first find 3000 most cited chemistry papers published in Nature, and construct a series of I in size of 150, 300, 1000, and 3000.I is constructed by first adding the ground truth inspiration papers (around 120), then randomly selecting the remaining papers from the 3000 papers, and finally randomizing the order of all the collected papers.Only title and abstract are needed for each paper in I.The default setting is that each inference of LLMs will screen 15 papers from I, and generate three titles that LLMs think can best assist b (and/or previous h).Screening through I for one round, only 20% of I will be selected.Screening another round will only leave 4%, and so on.</p>
<p>We use Hit Ratio as the evaluation metric, which is calculated by the number of selected ground truth inspiration papers divided by the number of all ground truth inspiration papers.All the Hit Ratio numbers shown in the tables are averaged across the 51 papers in the benchmark.</p>
<p>Table 3 shows the main experiment results.The Hit Ratio is surprisingly high: More than 75% of the ground truth inspirations are covered by even only the 4% chosen papers from the chemistry literature corpus.It seems that LLMs are quite capable of finding inspiration papers that are unknown to be able to associate with the background but in fact, can associate with the background to create novel knowledge.It means our bold assumption in § 4.2.2 that "the most advanced LLMs might already know lots of knowledge pairs that are able to associate to create novel knowledge, where the knowledge pairs are not known by any scientist to be related" is possible to be true.For each screen window of 15 papers, 3 papers are selected.</p>
<p>points</p>
<p>Generated hypothesis covers three key points (or covers all the key points) and leverage them similarly as in the groundtruth hypothesis; Extra key points do not have apparent flaws.</p>
<p>points</p>
<p>Generated hypothesis covers three key points (or covers all the key points) and leverage them similarly as in the groundtruth hypothesis; Extra key points have apparent flaws.</p>
<p>points</p>
<p>Generated hypothesis covers two key points and leverage them similarly as in the groundtruth hypothesis, but does not cover more or all key points 2 points Generated hypothesis covers one key point and leverage it similarly as in the groundtruth hypothesis, but does not cover more or all key points 1 point Generated hypothesis covers at least one key point, but is used differently as in the groundtruth hypothesis 0 point Generated hypothesis does not cover any key point Table 6: Description of the Matched Score.</p>
<p>Table 4 shows the ablation study in terms of screen window size.It seems that a smaller window size can lead to better performance: a screen window size of 60 to keep 3 for one round will select 5% of the corpus, and the Hit Ratio is 71.6%; while a screen window size of 15 to keep 3 for two rounds will select only 4% of the corpus, but the Hit Ratio is as high as 83.7%.</p>
<p>Table 5 compares LLMs in different scales on inspiration retrieval ability.The results indicate that LLMs obtain the emergent ability for inspiration retrieval since a rather small parameter size, but then quickly plateau.§ A.9 discusses research background options' influence on inspiration retrieval.</p>
<p>HOW WELL CAN LLMS PERFORM
P (h j | b, h j−1 , i j )?
Here, we investigate the question (denoted as Q2): "Given only known knowledge, whether LLM can reason to unknown knowledge that has high probability to be valid?".</p>
<p>The first challenge to answer Q2 is the evaluation method: The benchmark covers a large range of chemistry topics, and chemistry is a very complex discipline that a slight change of research topic would make a chemist unable to provide a reliable enough evaluation.In fact, a chemistry researcher might not be able to provide a reliable enough evaluation even if the hypothesis is in his domain.</p>
<p>Therefore, we adopt a reference-based evaluation method called "Matched Score" (MS).The descriptions are shown in Table 6.It's on a 6-point Likert scale, roughly containing four stages.Denoting generated hypothesis as gh, and original hypothesis as oh, the four stages are (1) gh ∩ oh = ∅ (0 point); (2) gh ∩ oh ̸ = ∅ (1/2/3 points); (3) gh ⊇ oh (4 points); (4) gh ≈ oh (5 points).</p>
<p>We use MOOSE-Chem to investigate Q2.Specifically, we initialize I as only the ground truth inspiration papers and search i for k round, where k is the number of ground truth i needed for each b.MOOSE-Chem will not retrieve the same i already retrieved in previous rounds, guaranteeing that before generating the final h, the framework has already seen all the ground truth inspirations.</p>
<p>Table 7 shows the results.For each b, the top two h with the highest MS by GPT-4o are selected for expert evaluation (by two chemistry PhD students).It indicates that LLMs are quite capable of associating known knowledge into unknown knowledge that has a high probability to be valid (very close to oh).In addition, providing a survey can assist the new knowledge-discovery process.We discuss the agreement between GPT-4o-based evaluation and expert evaluation in § A.14. Table 8: Relation between the number of matched ground truth i and the average ranking ratio (↓).</p>
<p>HOW WELL CAN LLMS PERFORM R(h)?</p>
<p>Here, we investigate Q3: "whether LLMs can select high-quality h to rank them higher?".</p>
<p>To investigate Q3, we run MOOSE-Chem with every b from the benchmark; |I| = 300, containing all the ground truth i.Every h is given a rating r = R(h), and is ranked based on r.For every generated h, we get the number of ground truth i it leveraged (#Matched i), and evaluate it with a GPT-4o evaluated MS (here MS is -1 means this h has not used any ground truth i).</p>
<p>Table 8 shows the relation between the #Matched i and average ranking ratio (the lower, the better).It shows a clear trend that the more ground truth i is leveraged, the better ranking score h can have.It indicates that h with a higher ranking ratio is more likely to be matched with better i.</p>
<p>Table 9 shows the relation between the GPT-4o evaluated MS and the average ranking ratio.There is a trend that the higher the MS, the better the average rank ratio (when MS ∈ [2,4]).However, the disadvantage of those h without a positive MS is not very significant.It seems that LLMs have a certain ability to rank good h higher.But it is not sure how significant it is, because a part of the reason for these results is that those h generated without ground truth i could be also in high quality.</p>
<p>EXPERIMENT AND ABLATION STUDY</p>
<p>We perform experiments in a setting similar to the copilot in the wild setting.SciMON is a hypothesis discovery framework for the NLP and biochemical domain.It relies on semantic and citation neighbors to retrieve information to assist the background.As a result, the retrieved information could be very related to the background that might not be able to serve as an inspiration.Here, we implement SciMON with LLM-based inspiration retrieval.</p>
<p>Qi et al. ( 2024) work on hypothesis discovery in the biomedical domain.It retrieves information pertinent to the keywords in the background to generate hypotheses.As a result, the retrieved information might compose of a background survey, but not as inspiration.Self-refine is also adopted.</p>
<p>RESULTS</p>
<p>Table 10 shows the baseline results and the ablation study of MOOSE-Chem.It indicates that both mutation &amp; recombination and the multi-step designs can significantly improve the best-performing h.Mutation &amp; recombination leads to a drop of Average MS compared to the MOOSE baseline; we attribute the reason to that the mutation step forces LLMs to generate h different from previous h mutations from the same b and i, and therefore might generate many h that do not make a lot of sense.The assigned MS to these mutation h is low, and therefore lower down the Average MS.</p>
<p>To better understand the performance of MOOSE-Chem in this real copilot setting, for each b the top 4 generated h with the highest MS by GPT-4o are evaluated again by two experts in terms of MS.Table 11 shows the expert evaluation results.Here, the top MS is the highest MS for each b, out of the 4 expert evaluated h for this b.Note that MS rated as three is already very high.Illustrated in Table 6, it means the generated h by MOOSE-Chem (that has not seen h) in the real copilot setting covers two main innovations of the chemistry hypothesis, which is published in Nature, Science or a similar level.Some case studies can be seen in § A.16.</p>
<p>CONCLUSION</p>
<p>We investigated this central question: "Can LLMs automatically discover novel and valid chemistry (including material science) research hypotheses (even those which deserve a publication in Nature, Science, or a similar level) given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question?".We proposed a fundamental assumption to break up this seemingly impossible-to-solve central question into three smaller, more practical, and executable fundamental questions.Then, we investigated LLM's ability on each of them.To this end, we constructed a benchmark consisting of chemistry and material science papers published and only be public in 2024.We also developed an LLM-based multi-agent framework consisting of three stages reflecting the three smaller fundamental questions.Experiments showed that the framework (runs in a copilot in-the-wild setting, with LLMs with training data up to October 2023) can rediscover many hypotheses with very high similarity with the ground-truth ones, covering the main innovations.Figure 2 presents an overview of the input and output of the MOOSE-Chem framework.In this work, the inspiration corpus is initialized using the titles and abstracts of numerous papers, which together constitute the search space.</p>
<p>A.2 PROOF OF THE RIGOROUS DECOMPOSITION OF P (h | b) BASED ON THE FUNDAMENTAL ASSUMPTION</p>
<p>We propose an assumption that a majority of chemistry hypotheses can originate from a research background and several inspirations.This assumption is not only supported by many chemistry researchers whom we have extensive discussions with but also by the cognitive science finding that "creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge" (Koestler, 1964;Benedek et al., 2012;Lee &amp; Chung, 2024).We design our method based on this fundamental assumption.</p>
<p>This assumption is reminiscent of Swanson Linking (Swanson, 1986) in the domain of literaturebased discovery (LBD), also known as the "ABC model", where two concepts A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers.Our assumption differs in that: (1) for a chemistry hypothesis published in a good venue, usually more than one inspiration is needed; (2) background and inspiration are not necessarily linked by a path of intermediate papers;</p>
<p>(3) our assumption is applied to a majority of existing published chemistry hypotheses, while LBD has been considered to only focus on a very specific, narrow type of hypothesis (Wang et al., 2024b).It might indicate that a similar proportion of future chemistry hypotheses can also result from linkages of existing literature.</p>
<p>Denoting background knowledge as b, inspiration knowledge as i, and hypothesis as h, we translate this assumption as:
h = f (b, i 1 , . . . , i k )(4)
Here, k ∈ Z represents the number of inspirations needed for a particular h.Typically in chemistry, k ∈ [1, 3].We further assume each hypothesis h has a unique minimal set of inspirations {i 1 , . . ., i k } that, combined with the background b, determines its formation.We refer to this as the uniqueness assumption.</p>
<p>Equation 4 expresses the idea that, for the majority of chemistry hypotheses (if not all), each hypothesis can be formulated as a composition of background knowledge and additional knowledge elements, which we refer to as inspirations.This functional form reflects a universal pattern in hypothesis formulation: regardless of where the inspiration originates-be it prior literature, serendipitous observation, or discussions with peers-the essential step is to identify correct inspirations and integrate them with existing background knowledge in a meaningful way.In this formulation, the process of first collecting and selecting the appropriate background knowledge and then identifying and integrating suitable inspirations constitutes both a necessary and sufficient condition for generating a valid hypothesis h.</p>
<p>Here's an example in chemistry:</p>
<p>• Research Question: How to obtain D 2 gas more efficiently?</p>
<p>• Background Knowledge: The best performing methods are electrocatalytic methods.</p>
<p>• Inspiration Knowledge 1: Ruthenium as catalyst</p>
<p>• Inspiration Knowledge 2: Nitrogen-doped electrode</p>
<p>• Inspiration Knowledge 3: D 2 O as chemical solution</p>
<p>• Hypothesis: A nitrogen-doped ruthenium (Ru) electrode can effectively catalyze the reductive deuteration of (hetero)arenes in the presence of D 2 O in an electrocatalytic method, leading to efficient D 2 gas production.</p>
<p>Here's an example in AI:</p>
<p>• Research Question: How can we automatically update the parameters of a multi-layer logistic regression model using data?</p>
<p>• Background Knowledge: Multi-layer logistic regression</p>
<p>• Inspiration Knowledge: The chain rule from calculus</p>
<p>• Backpropagation Here's another example in AI:</p>
<p>• Research Question: How can we improve reasoning performance in language models?</p>
<p>• Background Knowledge: Chain-of-Thought prompting</p>
<p>• Inspiration Knowledge: Majority voting over multiple reasoning paths Hypothesis: Self-consistency decoding Here, "background knowledge" and "inspiration knowledge" as illustrated in the example above, can be understood as specific, well-defined knowledge pieces.In practice, however, knowledge retrieval is rarely so clean.Instead of isolating a single knowledge unit, we often retrieve a noisy cluster of information that contains the desired piece along with extraneous content.For instance, when retrieving a paper that includes a relevant inspiration, the paper will inevitably also contain unrelated information that may not be useful for the current research question.Conversely, a single clean inspiration i may be embedded across multiple papers in the literature.This redundancy is beneficial-it increases the likelihood of retrieving i even when searching imperfectly.</p>
<p>In other words, given existing knowledge in the background, a majority of chemistry research is about searching knowledge that previously not known to be related to the background but in fact can assist the background, then associate the background knowledge and the searched knowledge in a reasonable way to compose a hypothesis.Crucially, the inspiration should not be previously known to be related to the background-at least not in a way that has already been used to formulate hypotheses.Otherwise, the resulting hypothesis would lack novelty.This requirement positions the inspiration retrieval task as an inherently out-of-distribution (OOD) problem, where the goal is to surface connections that lie outside established knowledge associations.</p>
<p>Our goal is to transform the seemingly impossible-to-solve P (h | b) into an equivalent form, where each step in the equivalent form is practical and executable.Denoting the full inspiration knowledge space as I, such that P (I) = 1.Then a straightforward way of decomposing P (h | b) is by the chain rule based on Equation 4:
P (h | b) = i1,...,i k P (h, i 1 , . . . , i k | b) (5) = π∈Π k P h, i π(1) , . . . , i π(k) | b (6) = P (h, i 1 , . . . , i k | b) (7) = P (h | b, i 1 ) • P (i 1 | b, I) if k = 1 P (h | b, i 1 , . . . , i k ) • k j=2 P (i j | b, i 1 , . . . , i j−1 , I) • P (i 1 | b, I) if k &gt; 1 (8)
Equation 5 expands P (h | b) by marginalizing over all possible inspiration sequences (i 1 , . . ., i k ) that could, together with the background b, yield h. Equation 6 applies the uniqueness assumption-each hypothesis h corresponds to a unique minimal set of inspirations {i 1 , . . ., i k }, leaving only the order of incorporation as a source of variability.Let Π k denote all permutations of {1, . . ., k}; thus, the marginalization becomes a sum over π ∈ Π k .Equation 7follows from the fixed-order assumption (introduced here to simplify the equation), which posits a canonical constructive order of inspirations (i.e., |Π k | = 1), collapsing the sum to a single term.Finally, Equation 8 expands this joint distribution using the chain rule.</p>
<p>I denotes the full inspiration space-that is, the set of all possible knowledge pieces that could serve as an inspiration for generating a new hypothesis.This space includes not only all existing chemistry knowledge but also potentially relevant knowledge from other disciplines that could be leveraged in formulating novel chemistry hypotheses.However, computing over the full space I is computationally infeasible.To make the problem tractable, we approximate I with a large but manageable subset Î, consisting of approximately 3,000 top cited chemistry papers from the existing chemistry literature.</p>
<p>Equation 8 describes the process of P (h | b) from a knowledge-searching perspective.However, the terms P (h | b, i 1 , . . ., i k ) and P (i j | b, i 1 , . . ., i j−1 , I) may not fully capture how chemistry researchers actually discover new inspirations in practice.One key reason is that researchers typically reason in an incremental fashion, composing hypotheses by integrating one or two knowledge components at a time.It is cognitively and practically difficult to evaluate or integrate all candidate inspirations simultaneously.Instead, researchers iteratively assess partial combinations-gradually building toward a complete hypothesis.</p>
<p>To mimic how chemistry researchers conduct research and make it more practicable, we break P (h | b, i 1 , . . ., i k ) into a series of recursive smaller steps as
P (h k | b, i 1 , . . . , i k ) ≈ P (h k | b, f (b, i 1 , . . . , i k−1 ), i k ) if k &gt; 1 (9) = P (h k | b, h k−1 , i k ) if k &gt; 1 (10)
Similarly, we can break P (i j+1 | b, i 1 , . . ., i j , I) as
P (i k+1 | b, i 1 , . . . , i k , I) ≈ P (i k+1 | b, f (b, i 1 , . . . , i k ), I) if k &gt; 1 (11) = P (i k+1 | b, h k , I) if k &gt; 1 (12)
As a result, to achieve the final h k , we need to obtain {h 1 , . . ., h k−1 } first (if k &gt; 1).In addition, viewing h as a "state" and i as an "action", obtaining h and i through P (h k | b, h k−1 , i k ) and P (i k+1 | b, h k , I) correspondingly indicates a Markov property: (1) a new h depends only on b, its previous h, and the current i; and (2) an i depends only on b, I, and the previous h.</p>
<p>Markov Decision Process (MDP) Formulation.Building upon this Markov property, we now interpret the sequential formation of hypotheses as a decision process.We therefore formalize this process as a Markov Decision Process (MDP), where:</p>
<p>• the state is (b, h j−1 ), encoding the research background and the current intermediate hypothesis;</p>
<p>• the action is the selected inspiration i j ;</p>
<p>• the transition function is P (h j | b, h j−1 , i j ), yielding the next hypothesis state; and</p>
<p>• the reward can be defined as the quality or plausibility of the resulting hypothesis h j (e.g., as judged by an LLM or domain expert).
b i1 − → h 1 i2 − → h 2 ••• − → h k−1 i k − → h k = h,
Each transition remains conditioned on the background knowledge b, though b is omitted from the notation to emphasize the Markov structure of the progression.This MDP view implies that hypothesis discovery can be modeled as a sequential policy choosing inspirations and applying transition dynamics to update the intermediate hypothesis state.We next make this view explicit in the probability space.</p>
<p>Building on this formulation, we interpret the formation of a hypothesis h (specifically, h = h k ) as a constructive process that sequentially integrates a set of inspirations {i 1 , . . ., i k } into intermediate hypothesis states {h 1 , . . ., h k }.</p>
<p>Formally, the conditional probability P (h | b) can be expressed as a marginal over all valid sequences of inspirations that can generate h (supported by assumption in Equation 4 and uniqueness assumption):
P (h | b) = π∈Π k P i π(1) , . . . , i π(k) , h 1 , . . . , h k | b ,(13)
where Π k denotes the set of all permutations of {1, . . ., k} applied to the inspirations {i 1 , . . ., i k } such that the resulting composition yields the final hypothesis h k = h, under the assumptions of Equation 4 and the uniqueness assumption, which specify that h is fully determined by b and {i 1 , . . ., i k }.In this context, h j represents the intermediate hypothesis state obtained after integrating i π(j) at step j.</p>
<p>The degree to which the order of inspirations {i 1 , . . ., i k } affects hypothesis formulation can vary across disciplines.In empirical sciences such as chemistry, the contributions of individual inspirations are largely interchangeable, and their order of integration has limited impact on the final hypothesis, resulting in a large |Π k |.Conversely, in disciplines such as mathematics, where hypotheses (e.g., theorems) often require constructing a specific sequence of lemmas and prior results, the ordering of inspirations is more constrained and may follow a near-deterministic path.</p>
<p>For simplicity of exposition, we adopt the fixed-order assumption introduced earlier-i.e., |Π k | = 1-which selects a canonical constructive order of inspirations {i 1 , . . ., i k } for analysis.Under this assumption, the hypothesis h is constructed through that specific sequence, giving:
P (h | b) = P (i 1 , . . . , i k , h 1 , . . . , h k | b),(14)
with h j denoting the intermediate hypothesis state after incorporating i j , and h k = h.</p>
<p>Therefore, under the uniqueness assumption and fixed-order assumption, for k &gt; 1,
P (h | b) = P (i 1 , . . . , i k , h 1 , . . . , h k | b) (15) = P (i 1 , h 1 | b) • P (i 2 , h 2 | b, i 1 , h 1 ) • . . . • P (i k , h k | b, i 1 , . . . , i k−1 , h 1 , . . . , h k−1 ) (16) ≈ P (i 1 , h 1 | b) • P (i 2 , h 2 | b, h 1 ) • . . . • P (i k , h k | b, h k−1 ) (17) = k j=1 P (i j | b, h j−1 , I) • P (h j | b, h j−1 , i j ), where h 0 = ∅ (18)
Equation 15 follows from the assumption in Equation 4, together with the uniqueness and fixedorder assumptions, which specify that h is determined by b and a unique ordered set of inspirations {i 1 , . . ., i k }.Equation 16 applies the chain rule to factorize the joint distribution.Equation 17follows from the Markov property, assuming that the next (i j , h j ) pair depends only on b and the preceding state h j−1 .Finally, Equation 18 re-applies the chain rule, with P (I) = 1 by definition.</p>
<p>Although starting from k &gt; 1, Derivation 18 covers the situation when k = 1 in Equation 8. Therefore, in sum, we successfully break up the seemingly impossible question P (h | b) into many practical and executable smaller questions as:
P (h | b) ≈ k j=1 P (i j | b, h j−1 , I) • P (h j | b, h j−1 , i j ), where h 0 = ∅ and k ≥ 1(19)
Equation 19 provides the MDP-based decomposition of P (h | b), where each term P (i j | b, h j−1 , I) and P (h j | b, h j−1 , i j ) corresponds respectively to the policy (action selection) and state transition components of the underlying Markov Decision Process.This formulation highlights that scientific hypothesis discovery can be viewed as a sequential decision-making problem, where an agent iteratively selects inspirations to maximize the expected quality (usually compounded evaluation of validity and novelty) of the final hypothesis h k .</p>
<p>Of course, without the fixed-order assumption, a more complete derivation of P (h | b) involves marginalizing over all valid permutations in Π k :
P (h | b) = π∈Π k P (i π(1) , . . . , i π(k) , h 1 , . . . , h k | b)(20)≈ π∈Π k k j=1 P (i π(j) | b, h(π)j−1 , I) • P (h (π) j | b, h(π)j−1 , i π(j) ),(21)
where h
(π) 0 = ∅, h(π) k
= h, and k ≥ 1.Here, h (π) j denotes the intermediate hypothesis state at step j in the permutation π, which results from sequentially incorporating inspirations in the order {i π(1) , . . ., i π(j) }.</p>
<p>A.3 THE FULL INSTRUCTION FOR BENCHMARK CHECKING</p>
<p>Please help us check again before finalizing the decomposition of each paper in the benchmark:</p>
<ol>
<li>
<p>Whether the background question is correct.</p>
</li>
<li>
<p>Background survey shouldn't contain any information/method in inspiration or hypothesis (except if this information/method has been used for this particular background question before).It is encouraged to include the most similar existing method to the proposed method.For example, the proposal is to change BaCl2 to BaSO4.It is encouraged to include BaCl2 in the survey, but SO4 must not be included in the survey (since SO4 belongs to the inspiration).</p>
</li>
<li>
<p>Background question cannot contain any information in inspiration or hypothesis as well: It should be a little bit general question, instead of a specific question asking about how the inspiration can be leveraged to help with the question.It also shouldn't be too general that we can't understand which specific research domain it works on.</p>
</li>
<li>
<p>Whether the identification of inspirations really the main inspirations for this paper, and whether we need more main inspiration(s).</p>
</li>
<li>
<p>Whether the main hypothesis is correct and covers the main key points.5. Whether the background survey + background question + identified inspirations can logically lead to the hypothesis (if not, we might need to identify more inspirations).</p>
</li>
</ol>
<p>Thank you for the efforts!Your contribution is indispensable for the success of this research.Please let me know if you have any questions.</p>
<p>A.4 PROMPT TO OBTAIN R(h)</p>
<p>You are known as a diligent and harsh reviewer in Chemistry and Material Science that will spend much time to find flaws when reviewing and therefore usually gives a relatively much lower score than other reviewers.But when you meet with a hypothesis you truly appreciate, you don't mind to give it good scores.Given a not yet peer reviewed research hypothesis in Chemistry or Material Science domain, try to evaluate the research hypothesis from four research aspects and give score according to evaluation guidelines provided below.All four aspects should be evaluated in a 5 point scale.</p>
<p>Aspect 1: Validness.5 points: The hypothesis is a logical next step from current research, strongly supported by theory, perhaps with some indirect experimental evidence or highly predictive computational results.The experimental verification seems straightforward with a high probability of confirming the hypothesis; 4 points: Here, the hypothesis is well-rooted in existing theory with some preliminary data or computational models supporting it.It extends known science into new but logically consistent areas, where experiments are feasible with current technology, and there's a reasonable expectation of positive results; 3 points: This hypothesis is within the realm of theoretical possibility but stretches the boundaries of what's known.It might combine existing knowledge in very novel ways or predict outcomes for which there's no direct evidence yet.There's a conceptual framework for testing, but success is uncertain; 2 points: While the hypothesis might be grounded in some theoretical aspects, it significantly deviates from current understanding or requires conditions or materials that are currently impossible or highly improbable to achieve or synthesize; 1 point: The hypothesis proposes concepts or outcomes that are not only unsupported by current theory but also contradict well-established principles or data.There's no clear path to experimental testing due to fundamental theoretical or practical barriers.</p>
<p>Aspect 2: Novelty. 5 points: This level of novelty could fundamentally alter our understanding of chemistry or create entirely new fields.It often involves predictions or discoveries that, if proven, would require a significant overhaul of existing chemical theories; 4 points: The hypothesis significantly departs from established norms, potentially redefining how certain chemical phenomena are understood or applied.It might involve entirely new materials or theoretical frameworks; 3 points: This level involves a hypothesis that could potentially lead to new insights or applications.It might challenge minor aspects of current theories or introduce new methodologies or materials; 2 points: The hypothesis introduces a new angle or method within an established framework.It might involve known compounds or reactions but in contexts or combinations not previously explored; 1 point: The hypothesis involves minor tweaks or applications of well-known principles or techniques.It might slightly extend existing knowledge but doesn't introduce fundamentally new concepts.</p>
<p>Aspect 3: Significance.5 points: This hypothesis could fundamentally change one or more branches of chemistry.It might introduce entirely new principles, theories, or methodologies that redefine the boundaries of chemical science; 4 points: This hypothesis challenges current understanding or introduces a concept that could lead to substantial changes in how a particular area of chemistry is viewed or applied.It might lead to new technologies or significant theoretical advancements; 3 points: this hypothesis proposes something new or an innovative approach that could lead to noticeable advancements in a specific area of chemistry.It might open new avenues for research or application but doesn't revolutionize the field; 2 points: This hypothesis might offer a small variation or incremental improvement on existing knowledge.It could potentially refine a known concept but doesn't significantly alter the field; 1 point: The hypothesis addresses a very narrow or already well-established aspect of chemistry.It might confirm what is already known without adding much new insight.</p>
<p>Aspect 4: Potential.5 points: The hypothesis, while potentially intriguing now, holds the promise of being revolutionary with the addition of a key methodological component.This could introduce entirely new concepts  or fields, fundamentally changing our understanding or capabilities in chemistry; 4 points: The hypothesis, though promising, could be transformative with the right methodological enhancement.This enhancement might lead to groundbreaking discoveries or applications, significantly advancing the field; 3 points: The hypothesis, while interesting in its current form, could be significantly elevated with the right methodological addition.This might lead to new insights or applications that go beyond the initial scope; 2 points: The hypothesis currently offers some value but has the potential for more substantial contributions if enhanced with a new methodological approach.This could lead to incremental advancements in understanding or application; 1 point: The hypothesis, as it stands, might be straightforward or well-trodden.Even with methodological enhancements, it's unlikely to significantly expand current knowledge or applications beyond minor improvements.</p>
<p>The hypothesis is: Please give a response to the initial question on scoring the hypothesis from four aspects.Remember that you are a diligent and harsh reviewer.</p>
<p>A.5 AUTOMATIC EVALUATION BY CLAUDE AND GEMINI</p>
<p>To investigate whether the results and corresponding conclusions in the main text are caused by the usage of GPT-4o for automatic evaluation, here we use Claude-3.5-Sonnetand Gemini-1.5-Proto evaluate all of the results that have been evaluated by GPT-4o.</p>
<p>Table 12 covers the contents in Table 7, but with more results on using Claude-3.5-Sonnetand Gemini-1.5-Profor automatic evaluation.When using different LLMs for automatic evaluation, the instruction is the same (can be found in § A.12).The robust results indicate again that LLMs are quite capable of associating known knowledge into unknown knowledge that has a high probability to be valid (very close to oh).</p>
<p>Method</p>
<p>Top MS Average MS SciMON (Wang et al., 2024b) 2.980 2.618 MOOSE (Yang et al., 2024a) 3.039 2.690 Qi et al. (2024) 2 Only the hypotheses with a MS that is higher than the MS threshold are counted.</p>
<p>Table 13 and Table 14 evaluate the same hypotheses with Table 10, but using Claude-3.5-Sonnetand Gemini-1.5-Profor automatic evaluation correspondingly (instead of GPT-4o).The results indicate the robustness of MOOSE-Chem and its components.</p>
<p>A.6 MORE ANALYSIS ON EU</p>
<p>Table 15 shows the number of hypotheses receiving high Matched Score from only non-EU branch, only EU branches, and only EU-recombination branch.Here, only non-EU branch can be seen as the hypotheses obtained directly without mutations.The hypotheses are from the same experiment in Table 10.</p>
<p>The result indicates that about one-third of high-quality hypotheses can be obtained directly without mutations.In addition, the recombination branch contains more high-quality hypotheses than the only non-EU branch.</p>
<p>A.7 EFFECT OF SIGNIFICANCE FEEDBACK</p>
<p>Table 16 presents an ablation study on the significance feedback.The results with significance feedback are from Table 12.</p>
<p>The results indicate that not using significance feedback can even lead to a better performance in terms of the Matched Score metric.We attribute this phenomenon to LLM's ability on creativity: when asked to generate significant hypotheses, LLMs tend to be more deviate from the existing information for more possible significance, resulting in a lower matched score.However, we should note that the matched score only measures the matching degree of one given ground truth hypothesis, and it is possible that the more deviated one is more significant.Inductive reasoning (Yang et al., 2024a) is the most relevant reasoning type.It is about finding rules or hypotheses from observations.Scientific discovery is naturally an ultimate goal of inductive reasoning.</p>
<p>Inductive reasoning is a sub-reasoning type of logical reasoning.The other two sub-reasoning types are deductive reasoning (Clark et al., 2020) and abductive reasoning (Bhagavatula et al., 2020).Yang et al. (2023b) discuss their definitions and differences in detail.</p>
<p>Another relevant reasoning type is commonsense reasoning (Yang et al., 2020;2023a).Scientific discovery can be seen as an opposite task, which is to reason far outside of commonsense, even to discover unknown knowledge.</p>
<p>A.11.2 RETRIEVAL</p>
<p>The retrieval of inspiration is a retrieval task, and RAG (Lewis et al., 2020) also works on retrieval.</p>
<p>The main difference is that the current RAG method would most likely retrieve the information that is semantically the most similar to the input information (research background), while here our goal is to retrieve those information that was not known to be related to the input information before, but in fact is related.We assume that LLMs might have the ability to do it.</p>
<p>A.11.3 SELF CONSISTENCY Self-consistency (Wang et al., 2023;Chen et al., 2023) might have a similar looking to the "evolutionary unit" (EU), as they all have expansion to several branches, and finally collect these branches into one.</p>
<p>A key difference is that EU is to explore more diverse options to choose the optimal one, while self-consistency is to find consistent voting between options.</p>
<p>A.12 PROMPT TO GPT-4O FOR MATCHED SCORE You are helping to evaluate the quality of a proposed research hypothesis in Chemistry by a phd student.The ground truth hypothesis will also be provided to compare.Here, we mainly focus on whether the proposed hypothesis has covered the key points in terms of the methodology in the ground truth hypothesis.You will also be given a summary of the key points in the methodology of the ground truth hypothesis for reference.Please note that for the proposed hypothesis to cover one key point, it is not necessary to explicitly mention the name of the key point, but might also can integrate the key point implicitly in the proposed method.The evaluation criteria is called 'Matched score', which is in a 6-point Likert scale (from 5 to 0).Particularly, 5 points mean that the proposed hypothesis (1) covers all the key points and leverage them similarly as in the methodology of the ground truth hypothesis, and (2) does not contain any extra key point that has apparent flaws; 4 points mean that the proposed hypothesis (1) covers all the key points (or at least three key points) and leverage them similarly as in the methodology of the ground truth hypothesis, (2) but also with extra key points that have apparent flaws; 3 points mean that the proposed hypothesis (1) covers at least two key points and leverage them similarly as in the methodology of the ground truth hypothesis, (2) but does not cover all key points in the ground truth hypothesis, (3) might or might not contain extra key points; 2 points mean that the proposed hypothesis (1) covers at least one key point in the methodology of the ground truth hypothesis, and leverage it similarly as in the methodology of ground truth hypothesis, (2) but does not cover all key points in the ground truth hypothesis, and (3) might or might not contain extra key points; 1 point means that the proposed hypothesis (1) covers at least one key point in the methodology of the ground truth hypothesis, (2) but is used differently as in the methodology of ground truth hypothesis, and (3) might or might not contain extra key points; 0 point means that the proposed hypothesis does not cover any key point in the methodology of the ground truth hypothesis at all.Please note that the total number of key points in the ground truth hypothesis might be less than three, so that multiple points can be given.E.g., there's only one key point in the ground truth hypothesis, and the proposed hypothesis covers the one key point, it's possible to give 2 points, 4 points, and 5 points.a similar way with a similar goal compared to the ground truth hypothesis (not necessarily for the proposed hypothesis to be exactly the same with the groudtruth hypothesis to be classified as 'similar').When judging whether an extra key point has apparent flaws, you should use your own knowledge to judge, but rather than to rely on the count number of pieces of extra key point to judge.</p>
<p>Please evaluate the proposed hypothesis based on the ground truth hypothesis.</p>
<p>The proposed hypothesis is:</p>
<p>The ground truth hypothesis is:</p>
<p>The key points in the ground truth hypothesis are: Please evaluate the proposed hypothesis based on the ground truth hypothesis, and give a score.</p>
<p>A.13 GENERATED HYPOTHESES WITH LOW MATCHED SCORE ARE NOT NECESSARILY BAD</p>
<p>MS only measures the similarity between the generated h and the ground truth h.Receiving an MS as 0 or 1 does not mean the generated h is bad.Only real lab experiments can check each h.</p>
<p>A.14 EVALUATION AGREEMENT BETWEEN EXPERT EVALUATION AND GPT-4O EVALUATION</p>
<p>Table 19 shows the agreement between expert evaluation and automatic evaluation (by GPT-4o) on MS.Hard consistency is assigned to 1 only if the two scores are exactly the same, else is assigned to 0. Soft consistency is assigned to 1 only if the absolute difference between the two scores is less than 2, else is assigned to 0.</p>
<p>The results show a medium to high consistency between expert evaluation and automatic evaluation.</p>
<p>The main reason is that, in practice, the automatic evaluation is usually 1 to 2 points higher than expert evaluation, since GPT-4o can usually find a way to explain how the generated hypothesis is related to the ground truth hypothesis in terms of the main innovations.While this explanation usually is not wrong, the experts might find that compared to the MS given by GPT-4o, the generated hypotheses might not be clear enough to deliver the new innovation information to scientists when using it as a copilot advice.</p>
<p>A.15 AGREEMENT BETWEEN EXPERT EVALUATION</p>
<p>Table 20 shows the consistency score between experts.It is conducted by having the third chemistry expert check the randomly sampled expert evaluation from the previous two chemistry experts.Hard consistency is assigned to 1 only if the two scores are exactly the same, else is assigned to 0. Soft consistency is assigned to 1 only if the absolute difference between the two scores is less than 2, else is assigned to 0.</p>
<p>The results show a high consistency between experts, indicating a high reliability of reference-based expert evaluation.</p>
<p>nesium and lithium as specific examples is problematic.Overall, the proposed hypothesis provides valuable insights and serves as a source of inspiration for further exploration within this domain.</p>
<p>A.16.3 CASE 3 (MS BY EXPERT: 3) Ground truth h (Shibahara et al., 2024): Rh(I)-catalyzed vinylidene formation and enantioselective control via chiral diphosphine ligands can work together to enable a selective [2+2+1] cycloaddition reaction using three different 2π-components.The formation of the vinylidene intermediate ensures reaction selectivity, while the chiral ligands, through their dihedral angles, control the enantiomeric outcome.Together, these factors will result in the highly enantioselective synthesis of chiral 3-methylenecyclopent-1-ene derivatives.</p>
<p>Generated h: An enantioselective [2+2+1] cycloaddition involving cycloalkenes, acetylenecarboxylates, and terminal alkynes can be achieved using a dynamically tunable Rh(I) pincer complex system, specifically incorporating PNP and PONOP ligands, to control the alkyne-to-vinylidene transformation and enhance selectivity towards chiral 3-methylenecyclopent-1-ene derivatives.By adjusting the P-Rh-P bite angle with novel ligand modifications and utilizing a blended solvent system containing toluene and dichloromethane at controlled temperatures (25°C to 80°C), reaction conditions can be fine-tuned to suppress side reactions.Computational methods (DFT, QTAIM, and NBO) will guide the optimization process, supported by real-time monitoring using FTIR and NMR.</p>
<p>Exploring dual-ligand strategies, including phosphine/phosphinite combinations, will offer insights into previously unreported mechanistic pathways, ensuring a reproducible and scalable synthesis method applicable to pharmaceuticals and advanced material synthesis.</p>
<p>Expert's Analysis: The hypothesis generated in Case 2 receives a score of 3, reflecting its accurate identification of the metal and ligand necessary for the catalytic process, as well as its articulation of the mechanism underlying chiral control via ligand geometry.However, the inclusion of mixed solvents, which were not utilized in the actual study, prevents a higher rating.This prediction successfully asserts the necessity of rhodium metal and a phosphine ligand, while also proposing the significance of the chelating angle between the phosphine ligand and rhodium, in alignment with the hypothesis presented in the referenced article.Furthermore, although the DFT calculations mentioned in the hypothesis are not addressed in the article, they represent a promising avenue for future research in organic chemistry.</p>
<p>A.16.4 CASE 4 (MS BY EXPERT: 3) Ground truth h (Suzuki et al., 2024): The hypothesis is that phosphorus ylides, when exposed to photoredox catalysis, can undergo single-electron oxidation to generate radical cations, which engage in radical-polar crossover reactions.These intermediates can then sequentially form C-C and C=C bonds through a formal cycloaddition process, offering a powerful method for constructing six-membered carbocycles from simple substrates.</p>
<p>Generated h: A sophisticated dual-modal photoredox strategy synergistically integrates phosphorus ylides with a synchronized dual-wavelength excitation (450 nm and 520 nm) system to incite a regioselective three-component cycloaddition.This method exploits both net-neutral radical/polar crossover pathways and metal-free carbyne generation to convert inert C-H and C=P bonds into C-C and C=C bonds efficiently.Implemented within an optimized mixed solvent system (acetone/acetonitrile) under an ambient nitrogen atmosphere, the approach utilizes precise kinetic modulation and catalytic control, with computational validation (employing DFT models) underpinning mechanistic insights and regioselectivity.Experimental verification through in situ spectroscopic techniques (e.g., UV-Vis and NMR) enhances synchronization precision, curtailing side reactions.This methodology pledges substantial advancements in sustainable pharmaceutical synthesis and innovative polymer architecture formation, specifically by improving production efficiency for complex molecules and novel materials.</p>
<p>Expert's Analysis: The generated hypothesis also merits a score of 3, as it correctly anticipates the use of photocatalysis and highlights the significant influence of solvent on the reaction.However, since dual wavelength catalysis and solvent mixing were not employed in the actual experiment, a higher score is not warranted.Notably, despite the proposed mixed solvents not being used in the study, their composition comprises the two best-performing single solvents from the actual research, thus providing valuable insights that remain relevant to the ongoing investigation.</p>
<p>Figure 1 :
1
Figure 1: The MOOSE-Chem framework.It receives b and I as input, and outputs a list of ranked h.The bottom-right legend describes the symbols in the figure.</p>
<p>P</p>
<p>(h | b) can be understood as the task to discover high-quality chemistry research hypothesis, given only a background question and/or background survey.Our central question to investigate is how well LLMs can perform P (h | b).Supported by Equation 2 and 3, we break up this main question into three smaller questions: how well can LLMs perform (1) P (i j | b, h j−1 , I), (2) P (h j | b, h j−1 , i j ), and (3) R(h)?All experiments are performed by GPT-4o (its training data is up to October 2023).</p>
<p>Figure 2 :
2
Figure 2: Overview of the input and output of the MOOSE-Chem framework.</p>
<p>Effect of significance feedback (evaluated by Claude-3.5-Sonnet).</p>
<p>Table 1 :
1
Distribution of categories.
CategoryCountPublication Venue CountPolymer Chemistry Organic Chemistry Inorganic Chemistry Analytical Chemistry21 22 3 5Nature / Science Nature Subjournals Other Top Journals27 20 4Total51Total51</p>
<p>Table 2 :
2
Distribution of publication venues.Table1and Table2show the statistics of the benchmark in terms of chemistry category and publication venue.Material science is a sub-category of chemistry and can belong to the categories in Table</p>
<p>Table 3 :
3
Main table for Q1.For each screen window of 15 papers, 3 papers are selected.
Screen window size Hit Ratio (1 round) Hit Ratio (2 round) Hit Ratio (3 round) Hit Ratio (4 round)1098.0%88.9%79.4%56.5%1596.7%83.7%60.8%NA2091.2%76.8%58.8%NA4088.9%54.9%NANA6071.6%53.9%NANA</p>
<p>Table 4 :
4
Ablation table on screen window size for Q1.The corpus size is 300.For each screen window no matter its size, 3 papers are selected to remain for the next round of screening.</p>
<p>Table 5 :
5
Comparison of Llama series and GPT-4o on inspiration retrieval.The corpus size is 300.</p>
<p>Table 7 :
7
Main table for Q2.Average/Top MS means the average/highest Matched Score of all generated h from one b.Table 12 is a more complete version of this table including automatic evaluation results by Claude-3.5-Sonnetand Gemini-1.5-Pro.
5432 1 0 Totalw/ background surveyAverage MS (GPT-4o) 29 18 17 5 051Top MS (GPT-4o)28 1 19 3 0 051Top MS (Experts)9 12 22 6 2 051w/o background surveyAverage MS (GPT-4o) 17 17 19 7 051Top MS (GPT-4o)25 2 19 5 0 051#Matched i3210Average Rank Ratio NA 0.411 0.474 0.521Size03022458 4899</p>
<p>Only background question (strict), background survey (strict), and a chemistry corpus |I| = 300 are provided to the framework.Only the top 4% of I is selected and used to develop h.The evaluation metrics are Top MS and Average MS (the highest/average Matched Score of all generated h from one b), averaging across the benchmark.Experiments are conducted by GPT-4o (training data up to October 2023).MOOSE is a hypothesis discovery framework for the general social science domain.It leverages LLMs to retrieve inspirations and uses self-refine(Madaan et al., 2023)to improve the validness, novelty, and clarity aspects.The difference is that (1) it does not adopt the mutation and recombination step to better associate background and inspiration; (2) it only retrieves one step of inspiration.
6.1 BASELINESMatched Score543210-1Average Rank Ratio 0.489 0.439 0.488 0.501 0.436 0.501 0.503Size21036404427291026451</p>
<p>Table 9 :
9
Relation between the GPT-4o labeled Matched Score and average ranking ratio (↓).
MethodTop MS Average MSSciMON (Wang et al., 2024b)2.5492.281MOOSE (Yang et al., 2024a)2.8822.464Qi et al. (2024)2.6862.356MOOSE-Chem4.0202.564w/o multi-step3.7652.730w/o multi-step &amp; EU2.8632.578</p>
<p>Table 10 :
10
Experiments and ablation study.The Matched Score (MS) is evaluated by GPT-4o (this table), Claude-3.5-Sonnet(Table13),andGemini-1.5-Pro(Table14).
5 4 32 1 0 TotalTop MS (Expert) 0 2 19 16 8 651</p>
<p>Table 11 :
11
MOOSE-Chem runs with |I|=300, mimicking the copilot setting.This table shows the statistics of the top Matched Score across the benchmark.The evaluation is done by experts.</p>
<dl>
<dt>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt.Goal driven discovery of distributional differences via language descriptions.In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.),Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -16, 2023, 2023.URL http://papers.nips.cc/paper_files/paper/2023/hash/7e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html.</dt>
<dd>background: inspiration:: hypothesis mutation: rate score: literature corpus
A APPENDIX A.1 MOOSE-CHEM OVERVIEW I/O FIGURE</dd>
</dl>
<p>Table 12 :
12
Main table for Q2.Average/Top MS means the average/highest Matched Score of all generated h from one b.The numbers represent the statistics of Average/Top MS over the benchmark.
MethodTop MS Average MSSciMON (Wang et al., 2024b)3.8243.529MOOSE (Yang et al., 2024a)3.9023.559Qi et al. (2024)3.4313.092MOOSE-Chem4.4713.697w/o multi-step4.2163.592w/o multi-step &amp; EU3.9413.614Table 13:Experiments and ablation study.The Matched Score is evaluated byClaude-3.5-Sonnet.</p>
<p>Table 14 :
14
Experiments and ablation study.The Matched Score is evaluated by Gemini-1.5-Pro.
.2161.846MOOSE-Chem3.6862.443w/o multi-step3.5882.529w/o multi-step &amp; EU2.9022.631MS threshold only non-EU branch only EU branches only EU-recombination branch51646204195424</p>
<p>Table 15 :
15
Number of hypotheses receiving high Matched Score (MS) from only non-EU branch, only EU branches, and only EU-recombination branch.</p>
<p>Table 19 :
19
Consistency score between expert evaluation and GPT-4o evaluation.</p>
<h1>Comparison Pairs Hard Consistency Score Soft Consistency Score3920.3450.542#Comparison Pairs Hard Consistency Score Soft Consistency Score480.4380.854</h1>
<p>In this case, we should choose score from 4 points and 5 points, depending on the existence and quality of extra key points.'Leveraging a key point similarly as in the methodology of the ground truth hypothesis' means that in the proposed hypothesis, the same (or very related) concept (key point) is used in</p>
<p>Table 20 :
20
Consistency score between experts in expert evaluation.</p>
<p>All code and data can be found in https://github.com/ZonglinY/MOOSE-Chem ‡ Contribution during internship at Shanghai Artificial Intelligence Laboratory. † Corresponding author.
ACKNOWLEDGMENTSThis work is supported by the Shanghai Municipal Science and Technology Major Project.This work is supported by Shanghai Artificial Intelligence Laboratory.This research/project is supported by the Ministry of Education, Singapore under its MOE Academic Research Fund Tier 2 (STEM RIE2025 Award MOE-T2EP20123-0005).We thank Mengsong Wu for his insightful discussions with us, and we thank Yuwei Wan for her efforts to support this research.Table18: Ablation table on background options for Q1.The corpus size is 300.For each screen window of 15 papers, 3 papers are selected.A.8 RANKING OF GROUND TRUTH HYPOTHESES Intuitively if we rank the original hypothesis with the generated hypothesis, the original hypothesis may be ranked at the top for most of the time.But is it?Table17shows the result, where we assign each ground truth hypothesis with a reward value R(h) (in terms of validness, novelty, significance, and potential), and calculate its average rank ratio regarding the framework-generated hypotheses.Surprisingly, the ground truth hypotheses are not ranked to the top.There are three possible reasons:1. LLM does poorly on ranking hypotheses;2. The generated hypotheses tend to describe their novelty and significance (although they are prompted to not to), which might influence the judgment;3. The generated hypotheses may surpass the original in quality.4. The generated hypotheses may sometimes have more details than the ground truth one (since the iterative usage of clarity feedback and refinement).A.9 INFLUENCE OF RESEARCH BACKGROUND OPTIONS TO INSPIRATION RETRIEVALTable18shows the ablation study in terms of whether to use strict background (discussed in § 3) or survey or not.It indicates that a survey can largely help with the inspiration retrieval process.Surprisingly, without a strict background, the Hit Ratio goes down a bit.We attribute it to the reason that mentioning information related to the inspiration will discourage retrieving that inspiration, since in the prompt, we ask LLMs to search for inspirations, and the demonstration example indicates that inspirations should not be too similar to the background (to bring in additional information).A.10 DISCUSSION ON HALLUCINATION AND SCIENTIFIC DISCOVERYIn contrast to the traditional understanding that hallucination is purely a bad thing, LLM's scientific discovery ability in fact counts on its hallucination ability to find novel hypotheses: a novel hypothesis would not have been observed by itself, therefore all novel hypotheses come from the class of hallucination.In essence, the research development of LLMs for automated scientific hypothesis discovery is to develop how to better leverage LLMs to hallucinate an unseen hypothesis that has more possibility to be valid.A The main hypothesis is that a nitrogen-doped ruthenium (Ru) electrode can effectively catalyze the reductive deuteration of (hetero)arenes in the presence of D 2 O, leading to high deuterium incorporation into the resulting saturated cyclic compounds.The findings validate this hypothesis by demonstrating that this electrocatalytic method is highly efficient, scalable, and versatile, suitable for a wide range of substrates.Generated h: A pioneering integrated electrocatalytic system leveraging ruthenium nanoparticles embedded in nitrogen-doped graphene, combined with a dual palladium-coated ion-exchange membrane reactor, will catalyze efficient, scalable, and site-selective reductive deuteration of aromatic hydrocarbons and heteroarenes.Utilizing deuterium sources from both D 2 gas and D 2 O, this system will optimize parameters through real-time machine learning-driven dynamic adjustments.Specific configurations include ruthenium nanoparticle sizes (2-4 nm), nitrogen doping levels (12-14%), precisely engineered palladium membranes (5 micrometers, ensuring 98% deuterium-selective permeability), and advanced cyclic voltammetry protocols (1-5 Hz, -0.5V to -1.5V).Expert's analysis: The proposed hypothesis effectively covers two key points from the ground truth hypothesis: the incorporation of ruthenium (Ru) and the use of D 2 O as a deuterium source within the electrocatalytic system.However, the current content does not detail the mechanism by which Ru-D is produced, which is essential for explaining the process of reductive deuteration.Nevertheless, the results are still insightful.The specific level of nitrogen doping, for example, is highly suggestive and warrants further investigation.Overall, the match remains strong in its alignment with the original hypothesis while also presenting opportunities for deeper exploration.A.16.2 CASE 2 (MS BY EXPERT: 4)Ground truth h(Wang et al., 2024a): By integrating guanidine sulfate (Gdm) 2 SO 4 into a poly vinyl alcohol (PVA) hydrogel and employing directional freezing to create aligned channels, it is possible to achieve a flexible thermogalvanic armor (FTGA) with a Carnot-relative efficiency exceeding 8% while maintaining high mechanical strength.This integration allows for enhanced thermopower and mechanical robustness, exceeding the performance of traditional quasi-solid thermocells.Generated h: Leveraging an integrated multi-layer hydrogel-ion assembly, constructed through advanced freeze-casting, salting-out processes, and bioinformatics-driven macromolecular design, will enhance the Carnot-relative efficiency and mechanical robustness of flexible thermogalvanic devices.This approach reimagines the use of poly(vinyl alcohol) hydrogels and incorporates novel ion-specific interactions informed by the Hofmeister series with uniquely potent ions like magnesium and lithium.Precision ion concentrations (0.15 to 0.45 M) are optimized to amplify thermoelectric responses and ionic conductance.Molecular dynamics simulations, employing detailed quantum chemistry models, will validate the enhanced ionic interactions and entropy modulation.Comprehensive characterization through nanoindentation, electron microscopy, and advanced computational platforms will verify crystalline polymer structures and stratified ionic gel configurations.By setting new benchmarks in efficiency and flexibility, this innovation is poised to revolutionize energy solutions in wearable technologies, outperforming current models in both scalability and application feasibility.Expert's Analysis:The proposed hypothesis effectively covers key points, mirroring the ground truth hypothesis.It incorporates the Hofmeister series by mentioning "ion-specific interactions informed by the Hofmeister series," which aligns with the ground truth's reference to "guanidine sulfate (Gdm) 2 SO 4 " in the hydrogel integration, acknowledging that sulfate ions are a significant component of the Hofmeister series.Additionally, the proposed hypothesis employs "freeze-casting," which reflects the ground truth's "directional freezing" to create structured channels within the hydrogel.This approach enhances both mechanical strength and thermopower-objectives that are consistent with those of the ground truth hypothesis.Moreover, the inclusion of salting-out processes in the proposed hypothesis does indeed contribute to improving the mechanical properties of thermoelectric materials.However, the selection of mag-
Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Associative abilities underlying creativity. Mathias Benedek, Tanja Könen, Aljoscha C Neubauer, Psychology of Aesthetics, Creativity, and the Arts. 632732012</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Wen-Tau Downey, Yejin Yih, Choi, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, 10.1038/s41586-023-06792-0Nat. 62479922023</p>
<p>Electrocatalytic reductive deuteration of arenes and heteroarenes. Faxiang Bu, Yuqi Deng, Jie Xu, Dali Yang, Yan Li, Wu Li, Aiwen Lei, Nature. 2024</p>
<p>Universal self-consistency for large language model generation. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou, 10.48550/arXiv.2311.17311CoRR, abs/2311.17311, 2023</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. Christian Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Why do nations produce science advances and new technology?. Mario Coccia, Technology in society. 591011242019</p>
<p>. Arthur H Joseph F Hair, Philip Money, Mike Samouel, Page, Research methods for business. Education+ Training. 4942007</p>
<p>The act of creation. Arthur Koestler, 1964HutchinsonLondon</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, arXiv:2409.061852024arXiv preprint</p>
<p>An empirical investigation of the impact of chatgpt on creativity. Byung Cheol, Lee , Jaeyeon Chung, Nature Human Behaviour. 2024</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. S H Patrick, Ethan Lewis, Aleksandra Perez, Fabio Piktus, Vladimir Petroni, Naman Karpukhin, Heinrich Goyal, Mike Küttler, Wen-Tau Lewis, Tim Yih, Sebastian Rocktäschel, Douwe Riedel, Kiela, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024arXiv preprint</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/arXiv.2408.062922024</p>
<p>LLM4SR: A survey on large language models for scientific research. Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, 10.48550/arXiv.2501.043062025</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, </p>
<p>Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, New Orleans, LA, USADecember 10 -16, 2023. 2023</p>
<p>Mathematical discoveries from program search with large language models. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, ; Zhou, Mohammadamin Paredes, Alexander Barekatain, Matej Novikov, M Pawan Balog, Emilien Kumar, Dupont, J R Francisco, Jordan S Ruiz, Pengming Ellenberg, Omar Wang, Fawzi, 10.1038/s41586-023-06924-6doi: 10.1038/ S41586-023-06924-6Pushmeet Kohli, and Alhussein Fawzi. 6257995CoLM, abs/2311.05965, 2024. 2024Nat.</p>
<p>Rh-catalysed enantioselective [2+ 2+ 1] cycloaddition reactions using three different 2π-components. Kaito Shibahara, Yoshihito Kayaki, Kairi Yamashiro, Yuki Nagashima, Kohei Fujii, Ken Tanaka, Nature Synthesis. 2024</p>
<p>Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design. Henry Sprueill, Carl Edwards, V Mariefel, Udishnu Olarte, Sanyal, Ji Heng, Sutanay Choudhury, 10.18653/V1/2023.FINDINGS-EMNLPFindings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 6-10, 20232023</p>
<p>10.18653/v1/2023.findings-emnlp.560URL. </p>
<p>CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback. Henry W Sprueill, Carl Edwards, Khushbu Agarwal, V Mariefel, Udishnu Olarte, Conrad Sanyal, Hongbin Johnston, Heng Liu, Sutanay Ji, Choudhury, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, AustriaJuly 21-27, 2024. 2024OpenReview.net</p>
<p>Photocatalytic carbyne reactivity of phosphorus ylides for three-component formal cycloaddition reactions. Ryuhei Suzuki, Taiga Ando, Fritz Deufel, Kohsuke Ohmatsu, Takashi Ooi, Nature Synthesis. 2024</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly. 5621986</p>
<p>Solving olympiad geometry without human demonstrations. H Trieu, Yuhuai Trinh, Quoc V Wu, He Le, Thang He, Luong, 10.1038/s41586-023-06747-5Nat. 62579952024</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. John Vahe Tshitoyan, Leigh Dagdelen, Alexander Weston, Ziqin Dunn, Olga Rong, Kristin A Kononova, Gerbrand Persson, Anubhav Ceder, Jain, 10.1038/s41586-019-1335-8Nat. 57177632019</p>
<p>Ultrastrong, flexible thermogalvanic armor with a carnot-relative efficiency over 8%. Jinpei Wang, Yuxin Song, Fanfei Yu, Yijun Zeng, Chenyang Wu, Xuezhi Qin, Liang Peng, Yitan Li, Yongsen Zhou, Ran Tao, Nature Communications. 15167042024a</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024b1ACL 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Opinion mining by convolutional neural networks for maximizing discoverability of nanomaterials. Tong Xie, Yuwei Wan, Haoran Wang, Ina Østrøm, Shaozhou Wang, Mingrui He, Rong Deng, Xinyuan Wu, Clara Grazian, Chunyu Kit, Bram Hoex, 10.1021/acs.jcim.3c00746J. Chem. Inf. Model. 6472024</p>
<p>Improving event duration prediction via time-aware pre-training. Zonglin Yang, Xinya Du, Alexander M Rush, Claire Cardie, 10.18653/v1/2020.findings-emnlp.302Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational Linguistics16-20 November 2020. 2020EMNLP 2020 of Findings of ACL</p>
<p>End-to-end case-based reasoning for commonsense knowledge base completion. Zonglin Yang, Xinya Du, Erik Cambria, Claire Cardie, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterAssociation for Computational LinguisticsMay 2023aDubrovnik, Croatia</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023). 2023b</p>
<p>Language models as inductive reasoners. Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024St. Julian's, MaltaAssociation for Computational LinguisticsMarch 17-22, 2024. 2024a1</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, 10.18653/v1/2024.findings-acl.804Findings of the Association for Computational Linguistics, ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024band virtual meeting</p>            </div>
        </div>

    </div>
</body>
</html>
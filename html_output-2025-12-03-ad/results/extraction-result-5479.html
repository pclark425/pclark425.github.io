<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5479 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5479</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5479</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-93a9fe0ea501e83a8f2a73ceb6a0431671bc707a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/93a9fe0ea501e83a8f2a73ceb6a0431671bc707a" target="_blank">Large language models predict human sensory judgments across six modalities</a></p>
                <p><strong>Paper Venue:</strong> Scientific Reports</p>
                <p><strong>Paper TL;DR:</strong> Surprisingly, a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality, and provides highly correlated predictions with human data irrespective of whether direct visual input is provided or purely textual descriptors.</p>
                <p><strong>Paper Abstract:</strong> Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality, and provides highly correlated predictions with human data irrespective of whether direct visual input is provided or purely textual descriptors. To study the impact of specific languages, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interaction of language and perception.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5479.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5479.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model from OpenAI; reported here as a multimodal variant (trained on text and image data) and evaluated without fine-tuning on psychophysical similarity and naming tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model from OpenAI. In this paper GPT-4 is described as a multimodal model trained on text and images (co-trained vision+language) and queried via few-shot prompts / chat completion API to produce similarity ratings and color names.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Similarity judgments across six perceptual modalities (pitch, loudness, colors, consonants, taste, timbre)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>perceptual similarity / psychophysics</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Pairwise similarity judgments: for each modality the model was given few-shot in-context examples and then asked to give numerical similarity ratings (0-1) for stimulus pairs (10 elicited ratings per pair, averaged). Modalities: pitch (25 harmonic tones), loudness (8 pure-tone loudness levels), colors (14→23 colors via hex codes/wavelengths), vocal consonants (16 IPA consonant recordings), taste (10 flavors via confusion matrix), timbre (12 instrument timbres via aggregated dissimilarities). Human datasets came from prior literature or newly collected AMT studies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 produced similarity matrices significantly correlated with human data across all six modalities. Reported example correlations: pitch r = 0.92 (95% CI [.91, .92]); colors r = 0.89 (95% CI [.87, .91]); consonants r = 0.57 (95% CI [.56, .59]). (The paper reports correlations for other modalities for various models; these are the reported GPT-4 values explicitly provided.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human inter-rater reliability (IRR) reported for two newly collected modalities: pitch IRR r = 0.90 (95% CI [.87, .92]) based on 55 participants; vocal consonants IRR r = 0.46 (95% CI [.36, .56]) based on 64 participants. Other human datasets (loudness, taste, timbre, colors) come from prior literature and were used as ground-truth similarity matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On pitch and consonants, GPT-4's correlation with human similarity data is comparable to or exceeds human split-half IRR (pitch: GPT-4 r = 0.92 vs IRR 0.90; consonants: GPT-4 r = 0.57 vs IRR 0.46), suggesting GPT-4 reaches or surpasses typical inter-rater agreement on those tasks. Across modalities GPT-4 was among the top-two models in five of six domains; improvement over other models (~Δr ≈ 0.15–0.17) was roughly uniform and not uniquely large for the visual/color domain.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>GPT-4 is multimodal in training but did not show a modality-specific advantage for color beyond its general improvement over earlier text-only models. The reported improvement is attributed to richer textual training rather than clear visual grounding. Performance is reported at the population-average level; individual-trial variability and failure modes are not deeply characterized. Some modalities used confusion matrices converted to similarities (methodological caveat).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5479.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5479.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluated on a forced-choice color naming task in English and Russian to test language-dependent perceptual categorizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal OpenAI model used in few-shot / completion prompting to produce forced-choice color name selections for 330 Munsell colors; prompts run in English and Russian; 10 elicitations per color with a shuffled 15-term color vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Color naming (forced-choice from 15 basic terms) — bilingual (English, Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>categorization / language-perception interaction</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Participants (humans and LLMs) choose the best name for each Munsell color from a pre-specified list of 15 basic color terms. Humans: 103 native English and 51 native Russian participants (online, Prolific). LLMs: prompted to choose from same 15-term lists in each language; 10 responses per color aggregated to a dominant label. Agreement assessed via adjusted Rand index (ARI) between model and human partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 ARI vs humans: English ARI = 0.59 (95% CI [0.56, 0.63]); Russian ARI = 0.54 (95% CI [0.46, 0.54]). GPT-4 produced language-dependent naming maps that replicate cross-linguistic distinctions (e.g., separate light-blue/dark-blue categories in Russian). Notable qualitative mismatch: GPT-4 labeled 46 Munsell colors as 'turquoise' versus 15 in the human data.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baselines: in-lab constrained naming (Lindsey & Brown) ARI = 0.73 (95% CI [0.65,0.73]); free naming ARI = 0.75 (95% CI [0.66,0.75]). Online human data collected here (103 English, 51 Russian) was highly consistent with Lindsey & Brown. Sample sizes: English N=103 (online), Russian N=51 (online).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 is more human-like than GPT-3.5 and GPT-3 (GPT-3.5 English ARI = 0.50; GPT-3 English ARI = 0.39), but GPT-4 underperforms relative to lab human baselines (GPT-4 ARI 0.59 < human lab ARI ~0.73–0.75). GPT-4 does replicate cross-linguistic patterns (English vs Russian) qualitatively, but shows systematic differences in specific categories (e.g., overuse of 'turquoise').</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Although GPT-4 is multimodal, the color-naming advantage is not as large as human-vs-human consistency, indicating limitations in recovering human color categorization from language alone. Color presentation was uncontrolled online for humans, and GPT responses were elicited from text-only prompts using hex codes (no image rendering in the prompt), so differences could reflect mapping from hex to color lexicon rather than perceptual grounding. GPT-4's tendency to over-select certain labels (turquoise) indicates vocabulary or dataset biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5479.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5479.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intermediate ChatGPT variant (improvement over GPT-3) evaluated here via few-shot prompts on the same similarity judgment battery and color naming tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based Chat Completion model (OpenAI GPT-3.5) used via Chat Completion API to produce similarity ratings (temperature 0.7) and color names (temperature 0.7 for naming, 0 for eliciting the 15-term list). Trained on large text corpora; not described as multimodal in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Similarity judgments across six perceptual modalities (pitch, loudness, colors, consonants, taste, timbre)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>perceptual similarity / psychophysics</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same pairwise similarity paradigm as for GPT-4 (few-shot numeric ratings 0-1, 10 elicitations per pair). Models' predicted similarity matrices correlated with human matrices from literature and collected data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported example correlations for GPT-3.5 include loudness r = 0.89 (95% CI [.87, .91]) and taste r = 0.54 (95% CI [.48, .61]) and timbre r = 0.42 (95% CI [.40, .44]) as reported in the paper (some modality-specific correlations were provided for GPT-3.5). Overall correlation magnitudes across modalities were significant and generally > 0.6 for many model–human comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Same human datasets as above (literature datasets and newly collected pitch/consonant data). Inter-rater reliabilities: pitch IRR r = 0.90 (95% CI [.87, .92]); consonants IRR r = 0.46 (95% CI [.36, .56]).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 performed worse than GPT-4 on most modalities but sometimes was the top performer on a specific modality (e.g., loudness r = 0.89 reported for GPT-3.5). Generally underperforms relative to GPT-4 but still achieves significant correlation with human data across modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Performance varies by modality; some correlations (e.g., timbre) are modest, indicating limits to recovering complex auditory/timbre structure from language alone. Model-specific parameter counts and training details are not provided in the paper, so attributing differences to architecture vs. data is speculative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5479.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5479.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluated on the bilingual forced-choice color naming task and compared to human naming maps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat Completion variant of GPT-3 family, prompted to provide forced-choice color names in English and Russian following a 15-term vocabulary elicited from GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Color naming (forced-choice from 15 basic terms) — bilingual (English, Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>categorization / language-perception interaction</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same forced-choice paradigm as for GPT-4. For each Munsell color, GPT-3.5 was queried 10 times and dominant label computed; agreement to human naming partitions computed via adjusted Rand index.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported ARI values: English ARI = 0.50 (95% CI [0.46, 0.52]); Russian ARI = 0.50 (95% CI [0.45, 0.52]).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human ARI baselines as above (lab constrained ~0.73, free naming ~0.75); online human data collected here consistent with lab data. Sample sizes: English N=103, Russian N=51.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 is less aligned with human naming than GPT-4 (GPT-4 English 0.59 vs GPT-3.5 0.50) and substantially below lab human baselines (~0.73–0.75).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>GPT-3.5 shows less sensitivity to cross-linguistic category differences compared to GPT-4; again, mapping from hex codes to lexical categories via text-only prompts may limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5479.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5479.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Earlier-generation large language model evaluated here alongside newer variants on the psychophysical similarity battery and color naming task using the same prompting procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 text-completion model used via the Text Completion API to produce few-shot similarity ratings (temperature 0.7) and forced-choice color names; trained on large text corpora (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Similarity judgments across six perceptual modalities (pitch, loudness, colors, consonants, taste, timbre)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>perceptual similarity / psychophysics</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same pairwise similarity paradigm using in-context examples and numerical 0-1 ratings; model outputs averaged over 10 elicited ratings per pair to form similarity matrices, which were correlated with human matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3 correlated significantly with human data across all domains but at lower strengths than GPT-3.5 and GPT-4; example color naming ARI and similarity correlations for GPT-3 were lower (e.g., color naming English ARI = 0.39). Exact per-modality Pearson r values for GPT-3 similarity matrices are not all explicitly listed in the paper, but reported as lower than newer variants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human IRRs and literature datasets as above; pitch IRR r = 0.90, consonants IRR r = 0.46; color naming lab baselines ARI ~0.73–0.75.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 underperforms GPT-3.5 and GPT-4 on both the similarity battery and the color naming task, achieving significant but weaker correlations/ARI values relative to human data and to the later models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Lower performance relative to GPT-3.5/GPT-4 consistent with being an earlier generation model; the paper attributes performance improvements across models primarily to richer textual training rather than modality-specific grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5479.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5479.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessed on forced-choice color naming in English and Russian as a baseline comparison to GPT-3.5 and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models predict human sensory judgments across six modalities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-only completion model used to perform forced-choice color naming from a 15-term vocabulary via few-shot prompts; responses aggregated to dominant labels per Munsell color.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Color naming (forced-choice from 15 basic terms) — bilingual (English, Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>categorization / language-perception interaction</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>As above: each Munsell color queried 10 times, dominant label computed, agreement with human partitions computed via adjusted Rand index (ARI).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported ARI values: English ARI = 0.39 (95% CI [0.37, 0.42]); Russian ARI = 0.35 (95% CI [0.29, 0.35]).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human ARI baselines from lab experiments: constrained ~0.73 (95% CI [0.65,0.73]) and free naming ~0.75 (95% CI [0.66,0.75]). Online human participants here: English N=103, Russian N=51.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 performs substantially worse than humans and worse than GPT-3.5 and GPT-4 on this naming task, indicating progressive improvement across model versions in approximating human color naming.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Lowest ARI among evaluated LLMs; demonstrates the trend that later models better capture human-like perceptual categorizations but still fall short of lab human consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models predict human sensory judgments across six modalities', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can language models encode perceptual structure without grounding? a case study in color. <em>(Rating: 2)</em></li>
                <li>How does chatgpt rate sound semantics? <em>(Rating: 2)</em></li>
                <li>Words are all you need? capturing human sensory similarity with textual descriptors. <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Visual commonsense in pretrained unimodal and multimodal models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5479",
    "paper_id": "paper-93a9fe0ea501e83a8f2a73ceb6a0431671bc707a",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large language model from OpenAI; reported here as a multimodal variant (trained on text and image data) and evaluated without fine-tuning on psychophysical similarity and naming tasks.",
            "citation_title": "Large language models predict human sensory judgments across six modalities",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based large language model from OpenAI. In this paper GPT-4 is described as a multimodal model trained on text and images (co-trained vision+language) and queried via few-shot prompts / chat completion API to produce similarity ratings and color names.",
            "model_size": null,
            "cognitive_test_name": "Similarity judgments across six perceptual modalities (pitch, loudness, colors, consonants, taste, timbre)",
            "cognitive_test_type": "perceptual similarity / psychophysics",
            "cognitive_test_description": "Pairwise similarity judgments: for each modality the model was given few-shot in-context examples and then asked to give numerical similarity ratings (0-1) for stimulus pairs (10 elicited ratings per pair, averaged). Modalities: pitch (25 harmonic tones), loudness (8 pure-tone loudness levels), colors (14→23 colors via hex codes/wavelengths), vocal consonants (16 IPA consonant recordings), taste (10 flavors via confusion matrix), timbre (12 instrument timbres via aggregated dissimilarities). Human datasets came from prior literature or newly collected AMT studies.",
            "llm_performance": "GPT-4 produced similarity matrices significantly correlated with human data across all six modalities. Reported example correlations: pitch r = 0.92 (95% CI [.91, .92]); colors r = 0.89 (95% CI [.87, .91]); consonants r = 0.57 (95% CI [.56, .59]). (The paper reports correlations for other modalities for various models; these are the reported GPT-4 values explicitly provided.)",
            "human_baseline_performance": "Human inter-rater reliability (IRR) reported for two newly collected modalities: pitch IRR r = 0.90 (95% CI [.87, .92]) based on 55 participants; vocal consonants IRR r = 0.46 (95% CI [.36, .56]) based on 64 participants. Other human datasets (loudness, taste, timbre, colors) come from prior literature and were used as ground-truth similarity matrices.",
            "performance_comparison": "On pitch and consonants, GPT-4's correlation with human similarity data is comparable to or exceeds human split-half IRR (pitch: GPT-4 r = 0.92 vs IRR 0.90; consonants: GPT-4 r = 0.57 vs IRR 0.46), suggesting GPT-4 reaches or surpasses typical inter-rater agreement on those tasks. Across modalities GPT-4 was among the top-two models in five of six domains; improvement over other models (~Δr ≈ 0.15–0.17) was roughly uniform and not uniquely large for the visual/color domain.",
            "notable_differences_or_limitations": "GPT-4 is multimodal in training but did not show a modality-specific advantage for color beyond its general improvement over earlier text-only models. The reported improvement is attributed to richer textual training rather than clear visual grounding. Performance is reported at the population-average level; individual-trial variability and failure modes are not deeply characterized. Some modalities used confusion matrices converted to similarities (methodological caveat).",
            "uuid": "e5479.0",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "Evaluated on a forced-choice color naming task in English and Russian to test language-dependent perceptual categorizations.",
            "citation_title": "Large language models predict human sensory judgments across six modalities",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Multimodal OpenAI model used in few-shot / completion prompting to produce forced-choice color name selections for 330 Munsell colors; prompts run in English and Russian; 10 elicitations per color with a shuffled 15-term color vocabulary.",
            "model_size": null,
            "cognitive_test_name": "Color naming (forced-choice from 15 basic terms) — bilingual (English, Russian)",
            "cognitive_test_type": "categorization / language-perception interaction",
            "cognitive_test_description": "Participants (humans and LLMs) choose the best name for each Munsell color from a pre-specified list of 15 basic color terms. Humans: 103 native English and 51 native Russian participants (online, Prolific). LLMs: prompted to choose from same 15-term lists in each language; 10 responses per color aggregated to a dominant label. Agreement assessed via adjusted Rand index (ARI) between model and human partitions.",
            "llm_performance": "GPT-4 ARI vs humans: English ARI = 0.59 (95% CI [0.56, 0.63]); Russian ARI = 0.54 (95% CI [0.46, 0.54]). GPT-4 produced language-dependent naming maps that replicate cross-linguistic distinctions (e.g., separate light-blue/dark-blue categories in Russian). Notable qualitative mismatch: GPT-4 labeled 46 Munsell colors as 'turquoise' versus 15 in the human data.",
            "human_baseline_performance": "Human baselines: in-lab constrained naming (Lindsey & Brown) ARI = 0.73 (95% CI [0.65,0.73]); free naming ARI = 0.75 (95% CI [0.66,0.75]). Online human data collected here (103 English, 51 Russian) was highly consistent with Lindsey & Brown. Sample sizes: English N=103 (online), Russian N=51 (online).",
            "performance_comparison": "GPT-4 is more human-like than GPT-3.5 and GPT-3 (GPT-3.5 English ARI = 0.50; GPT-3 English ARI = 0.39), but GPT-4 underperforms relative to lab human baselines (GPT-4 ARI 0.59 &lt; human lab ARI ~0.73–0.75). GPT-4 does replicate cross-linguistic patterns (English vs Russian) qualitatively, but shows systematic differences in specific categories (e.g., overuse of 'turquoise').",
            "notable_differences_or_limitations": "Although GPT-4 is multimodal, the color-naming advantage is not as large as human-vs-human consistency, indicating limitations in recovering human color categorization from language alone. Color presentation was uncontrolled online for humans, and GPT responses were elicited from text-only prompts using hex codes (no image rendering in the prompt), so differences could reflect mapping from hex to color lexicon rather than perceptual grounding. GPT-4's tendency to over-select certain labels (turquoise) indicates vocabulary or dataset biases.",
            "uuid": "e5479.1",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "An intermediate ChatGPT variant (improvement over GPT-3) evaluated here via few-shot prompts on the same similarity judgment battery and color naming tasks.",
            "citation_title": "Large language models predict human sensory judgments across six modalities",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Transformer-based Chat Completion model (OpenAI GPT-3.5) used via Chat Completion API to produce similarity ratings (temperature 0.7) and color names (temperature 0.7 for naming, 0 for eliciting the 15-term list). Trained on large text corpora; not described as multimodal in this paper.",
            "model_size": null,
            "cognitive_test_name": "Similarity judgments across six perceptual modalities (pitch, loudness, colors, consonants, taste, timbre)",
            "cognitive_test_type": "perceptual similarity / psychophysics",
            "cognitive_test_description": "Same pairwise similarity paradigm as for GPT-4 (few-shot numeric ratings 0-1, 10 elicitations per pair). Models' predicted similarity matrices correlated with human matrices from literature and collected data.",
            "llm_performance": "Reported example correlations for GPT-3.5 include loudness r = 0.89 (95% CI [.87, .91]) and taste r = 0.54 (95% CI [.48, .61]) and timbre r = 0.42 (95% CI [.40, .44]) as reported in the paper (some modality-specific correlations were provided for GPT-3.5). Overall correlation magnitudes across modalities were significant and generally &gt; 0.6 for many model–human comparisons.",
            "human_baseline_performance": "Same human datasets as above (literature datasets and newly collected pitch/consonant data). Inter-rater reliabilities: pitch IRR r = 0.90 (95% CI [.87, .92]); consonants IRR r = 0.46 (95% CI [.36, .56]).",
            "performance_comparison": "GPT-3.5 performed worse than GPT-4 on most modalities but sometimes was the top performer on a specific modality (e.g., loudness r = 0.89 reported for GPT-3.5). Generally underperforms relative to GPT-4 but still achieves significant correlation with human data across modalities.",
            "notable_differences_or_limitations": "Performance varies by modality; some correlations (e.g., timbre) are modest, indicating limits to recovering complex auditory/timbre structure from language alone. Model-specific parameter counts and training details are not provided in the paper, so attributing differences to architecture vs. data is speculative.",
            "uuid": "e5479.2",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "Evaluated on the bilingual forced-choice color naming task and compared to human naming maps.",
            "citation_title": "Large language models predict human sensory judgments across six modalities",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Chat Completion variant of GPT-3 family, prompted to provide forced-choice color names in English and Russian following a 15-term vocabulary elicited from GPT-4.",
            "model_size": null,
            "cognitive_test_name": "Color naming (forced-choice from 15 basic terms) — bilingual (English, Russian)",
            "cognitive_test_type": "categorization / language-perception interaction",
            "cognitive_test_description": "Same forced-choice paradigm as for GPT-4. For each Munsell color, GPT-3.5 was queried 10 times and dominant label computed; agreement to human naming partitions computed via adjusted Rand index.",
            "llm_performance": "Reported ARI values: English ARI = 0.50 (95% CI [0.46, 0.52]); Russian ARI = 0.50 (95% CI [0.45, 0.52]).",
            "human_baseline_performance": "Human ARI baselines as above (lab constrained ~0.73, free naming ~0.75); online human data collected here consistent with lab data. Sample sizes: English N=103, Russian N=51.",
            "performance_comparison": "GPT-3.5 is less aligned with human naming than GPT-4 (GPT-4 English 0.59 vs GPT-3.5 0.50) and substantially below lab human baselines (~0.73–0.75).",
            "notable_differences_or_limitations": "GPT-3.5 shows less sensitivity to cross-linguistic category differences compared to GPT-4; again, mapping from hex codes to lexical categories via text-only prompts may limit performance.",
            "uuid": "e5479.3",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "Earlier-generation large language model evaluated here alongside newer variants on the psychophysical similarity battery and color naming task using the same prompting procedure.",
            "citation_title": "Large language models predict human sensory judgments across six modalities",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "OpenAI GPT-3 text-completion model used via the Text Completion API to produce few-shot similarity ratings (temperature 0.7) and forced-choice color names; trained on large text corpora (details not provided in this paper).",
            "model_size": null,
            "cognitive_test_name": "Similarity judgments across six perceptual modalities (pitch, loudness, colors, consonants, taste, timbre)",
            "cognitive_test_type": "perceptual similarity / psychophysics",
            "cognitive_test_description": "Same pairwise similarity paradigm using in-context examples and numerical 0-1 ratings; model outputs averaged over 10 elicited ratings per pair to form similarity matrices, which were correlated with human matrices.",
            "llm_performance": "GPT-3 correlated significantly with human data across all domains but at lower strengths than GPT-3.5 and GPT-4; example color naming ARI and similarity correlations for GPT-3 were lower (e.g., color naming English ARI = 0.39). Exact per-modality Pearson r values for GPT-3 similarity matrices are not all explicitly listed in the paper, but reported as lower than newer variants.",
            "human_baseline_performance": "Human IRRs and literature datasets as above; pitch IRR r = 0.90, consonants IRR r = 0.46; color naming lab baselines ARI ~0.73–0.75.",
            "performance_comparison": "GPT-3 underperforms GPT-3.5 and GPT-4 on both the similarity battery and the color naming task, achieving significant but weaker correlations/ARI values relative to human data and to the later models.",
            "notable_differences_or_limitations": "Lower performance relative to GPT-3.5/GPT-4 consistent with being an earlier generation model; the paper attributes performance improvements across models primarily to richer textual training rather than modality-specific grounding.",
            "uuid": "e5479.4",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "Assessed on forced-choice color naming in English and Russian as a baseline comparison to GPT-3.5 and GPT-4.",
            "citation_title": "Large language models predict human sensory judgments across six modalities",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Text-only completion model used to perform forced-choice color naming from a 15-term vocabulary via few-shot prompts; responses aggregated to dominant labels per Munsell color.",
            "model_size": null,
            "cognitive_test_name": "Color naming (forced-choice from 15 basic terms) — bilingual (English, Russian)",
            "cognitive_test_type": "categorization / language-perception interaction",
            "cognitive_test_description": "As above: each Munsell color queried 10 times, dominant label computed, agreement with human partitions computed via adjusted Rand index (ARI).",
            "llm_performance": "Reported ARI values: English ARI = 0.39 (95% CI [0.37, 0.42]); Russian ARI = 0.35 (95% CI [0.29, 0.35]).",
            "human_baseline_performance": "Human ARI baselines from lab experiments: constrained ~0.73 (95% CI [0.65,0.73]) and free naming ~0.75 (95% CI [0.66,0.75]). Online human participants here: English N=103, Russian N=51.",
            "performance_comparison": "GPT-3 performs substantially worse than humans and worse than GPT-3.5 and GPT-4 on this naming task, indicating progressive improvement across model versions in approximating human color naming.",
            "notable_differences_or_limitations": "Lowest ARI among evaluated LLMs; demonstrates the trend that later models better capture human-like perceptual categorizations but still fall short of lab human consistency.",
            "uuid": "e5479.5",
            "source_info": {
                "paper_title": "Large language models predict human sensory judgments across six modalities",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can language models encode perceptual structure without grounding? a case study in color.",
            "rating": 2
        },
        {
            "paper_title": "How does chatgpt rate sound semantics?",
            "rating": 2
        },
        {
            "paper_title": "Words are all you need? capturing human sensory similarity with textual descriptors.",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Visual commonsense in pretrained unimodal and multimodal models",
            "rating": 1
        }
    ],
    "cost": 0.01540425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large language models predict human sensory judgments across six modalities</h1>
<p>Raja Marjieh ${ }^{\mathrm{a}, 1}$, Ilia Sucholutsky ${ }^{\mathrm{b}}$, Pol van Rijn ${ }^{\mathrm{c}}$, Nori Jacoby ${ }^{\mathrm{c}, 2}$, and Thomas L. Griffiths ${ }^{\mathrm{a}, 1 \mathrm{c}, 2}$<br>${ }^{a}$ Department of Psychology, Princeton University, USA; ${ }^{\text {b }}$ Department of Computer Science, Princeton University, USA; ${ }^{\text {c }}$ Max Planck Institute for Empirical Aesthetics, Germany</p>
<p>This manuscript was compiled on June 16, 2023</p>
<h4>Abstract</h4>
<p>Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interaction of language and perception.</p>
<p>perception | representation | NLP | AI | cognitive science
Imagine that you were chosen to be part of an expedition aimed at studying a newly discovered alien species on a distant planet. Your task is to understand the perceptual system of that species. You arrive at the planet and, to your dismay, discover that its inhabitants are long departed and the only thing they left behind is a huge archive of text. How much of the perceptual world of that species can you recover based on text alone? Versions of this question have occupied philosophers for centuries $(1,2)$, and decades of psychological research are beginning to provide glimpses into the rich perceptual content of language and its influence on perception (3-11).</p>
<p>But how can the amount of information that language provides about perception be quantified? Here we propose to do so by eliciting psychophysical judgments from large language models (LLMs) such as GPT-3 and its recent "ChatGPT" variants GPT-3.5 and GPT-4 $(12,13)$. These models are trained on massive text corpora reflecting a substantial chunk of human language and can be queried in a way that is analogous to humans.</p>
<p>Recent research shows that LLMs can be used to study various aspects of cognition (14), including, language processing in the brain (15-17), perception (18-20), cross-modal alignment (21), and morality (22, 23). Here, we use the fact that they are trained on large amounts of language to gain insight into the classic problem of the relationship between language and perception: these models provide a lower bound on the amount of information about perceptual experience that can be extracted from language alone.</p>
<p>We explored this empirically across six modalities, namely, pitch, loudness, colors, consonants, taste, and timbre. Given a stimulus space (e.g., colors) and its stimulus specification (e.g.,
wavelengths or their corresponding hex-codes) we elicit pairwise similarity judgments in a direct analogy to the widespread paradigm of similarity in cognitive science (24) using a carefully crafted prompt that is given to the model to complete (Figure 1A; see Methods). Importantly, whereas four of the modalities were based on classical results from the literature (colors (25), loudness (26), timbre (27) and taste (28)), two human datasets (pitch and vocal consonants) were novel and thus were not part of the training set of the models.</p>
<p>It is worthwhile to note that, unlike the other models, GPT-4 was trained in a multi-modal approach, enabling it to access both written text (similar to the other two variants) and images. This allowed us to examine if the additional sensory information resulted in enhanced performance in the color modality relative to the other domains. Moreover, to further interrogate whether the LLMs' sensory representation was language-dependent, we tested whether they would behave differently in the presence of the same sensory information (a color hex-code), but respond in different languages. To that end, we conducted a color-naming task using a paradigm similar to that of (29) and the World Color Survey $(30,31)$, and constructed human and GPT-3, GPT-3.5, and GPT-4 color-naming maps in both English and Russian.</p>
<h2>Results</h2>
<p>Similarity study. For each dataset, we designed a tailored prompt template that could be filled in with in-context examples and the pair of target stimuli for which we would want the LLM to produce a similarity rating (see Methods and SI for full specification of the prompts and datasets). Across all domains, we elicited 10 ratings per pair of stimuli from each of the GPT models and then constructed aggregate similarity ratings by averaging. We then evaluated the resulting scores by correlating them with human data. The Pearson correlation coefficients between human data and model predictions are shown in Figure 1B (See SI for details regarding computing the correlations and CIs). We see that across all domains, the correlations were significant, and were particularly high for pitch $(r=.92,95 \% \mathrm{CI}[.91, .92]$ for GPT-4), loudness $(r=.89$, $95 \%$ CI $[.87, .91]$ for GPT-3.5), and colors $(r=.89,95 \%$ CI $[.87, .91]$ for GPT-4) (and $&gt;.6$ for all models), followed by moderate but highly significant correlations for consonants $(r=.57,95 \% \mathrm{CI}[.56, .59]$ for GPT-4), taste $(r=.54,95 \% \mathrm{CI}$ $[.48, .61]$ for GPT-3.5) and timbre $(r=.42,95 \% \mathrm{CI}[.40, .44]$</p>
<p>All authors contributed to the design of the experiments and editing the manuscript. Authors RM, NJ, and TLG conceptualized the project. Author IS conducted the LLM experiments. Authors RM and PVR conducted the human experiments. Authors RM, IS, PVR, and NJ analyzed the results.
The authors declare no competing interests.
${ }^{2}$ NJ contributed equally to this work with TLG.
${ }^{1}$ To whom correspondence should be addressed. E-mail: raja.marjieh@princeton.edu</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. A. Schematic of the LLM-based and human similarity judgment elicitation paradigms. B. Correlations between models and human data across six perceptual modalities, namely, pitch, loudness, colors, consonants, taste, and timbre (Pearson $r ; 95 \%$ CIs).
for GPT-3.5). For the two modalities for which we collected data, we could compare model performance to the inter-rater split-half reliability (IRR). The IRRs for pitch and consonants were $r=.90(95 \% \mathrm{CI}[.87, .92])$ and $r=.46(95 \% \mathrm{CI}[.36, .56])$, respectively, suggesting that the performance of GPT-4 is on par with human performance.</p>
<p>We also note that in five out of the six domains, GPT-4 was among the top two models. Interestingly, the improvement relative to the average performance of the other models happened across all modalities with the exception of timbre and loudness, and was not restricted or particularly large for the domain of colors (compare e.g. $\Delta r=.1595 \% \mathrm{CI}[.13, .16]$ for colors vs. $\Delta r=.1695 \% \mathrm{CI}[.15, .17]$ for pitch, $\Delta r=.16$ for taste $95 \% \mathrm{CI}[.12, .19]$, and $\Delta r=.1495 \% \mathrm{CI}[.12, .16]$ for consonants) suggesting that this improvement is driven by richer textual training in GPT-4 rather than the possibility of its inclusion of images in its training set as is currently being debated (18).</p>
<p>Next, to get a finer picture of the LLM-based judgments and see to what extent they reflect human representations, we performed the following analyses. Starting from the domain of pitch, we wanted to see to what extent the LLM data captures a well-known psychological phenomenon that Western listeners tend to associate particular musical intervals or ratios of frequencies (such as the octave or 2:1 frequency ratio) with enhanced similarity (32). To test this we computed the average similarity score over groups of pitch pairs that are separated by the same fixed interval (i.e., the same frequency ratios). Figure 2A shows the resulting average similarity per interval for the models and humans along with an example corresponding smoothed similarity matrix for GPT-3 (smoothing was done by averaging the raw similarity matrix over its sub-diagonals). We can see that apart from the decay as a function of separation (i.e., tones that are far apart in log frequency are perceived as increasingly dissimilar) there is a clear spike precisely at 12 semitones (octave), consistent with the aforementioned phenomenon of "octave equivalence" (33). Moreover, applying multi-dimensional scaling (MDS) (24) to the smoothed similarity matrix whereby the different stimuli are mapped into points in a Euclidean space (also known as "psychological space") such that similar stimuli are mapped to nearby points reveals a clear helical structure with twists that correspond to precisely 12 semitone separations (i.e., octaves) recovering the pitch spiral representation (Figure 2A).</p>
<p>Likewise, applying MDS to the domains of consonants and colors (Figure 2B) reveals highly interpretable representations, namely, the familiar color wheel and a production-based representation for consonants." As an additional test, we asked GPT-4 to provide explanations for the judgments it made (Figure 2C-D), and remarkably, the model resorted to explanations involving the octave, ratios, and harmonic relations for pitch, places of articulation in the vocal tract for consonants, and hue, brightness and color spectra for colors, consistent with the MDS solutions.</p>
<p>Color naming study. The results so far suggest that LLMs can use textual information to form perceptual representations. If this is indeed the case, we hypothesized that representations in LLMs using different languages could be different even in the presence of identical input. This would be consistent with cross-cultural differences in language and perception that were observed in humans (30). To test this, we propose for the first time to test LLMs on an explicit naming task proposed by the seminal work of (29) and further explored across cultures around the globe $(30,31)$.</p>
<p>We thus tested whether LLMs would yield different naming patterns of color hex codes depending on the language of the prompt used to elicit those names (see Methods; Figure 3). Specifically, we presented both humans and LLMs with different colors and asked them to perform a forced-choice naming task by selecting from a pre-specified list of 15 color names (see Methods). We specifically focused on English and Russian as test cases, since Russian speakers are documented to use richer vocabulary to describe what English speakers would otherwise describe as blue and purple $(29,34,35)$. We collected data from 103 native English speakers and 51 native Russian speakers and compared them against LLMs performing the same task, and to the in-lab data of (31) as an additional baseline. ${ }^{\dagger}$</p>
<p>The results are shown in Figure 3. Our first finding was that GPT-4 in both English and Russian were more human-like than the other variants when compared with an adjusted Rand index (see Figure 3A; English: GPT-4 $0.5995 \%$ CI $[0.56,0.63]$, GPT$3.5,0.5095 \%$ CI $[0.46,0.52]$, GPT-3 $0.3995 \%$ CI $[0.37,0.42]$;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. A. Human and LLM similarity marginals and an example GPT-3 corresponding similarity matrix and its three-dimensional MDS solution for pitch. B. MDS solutions for vocal consonants and colors for GPT-4 similarity matrices. To illustrate the structure of the results, we highlighted consonants with the same place of articulation in the vocal tract with the same shape and color, and added a rotated HSV color wheel for the color MDS. C. Example GPT-4 explanations for similarity judgment scores. D. Word clouds for GPT-4 explanations in the domain of pitch, vocal consonants, and colors.</p>
<p>Russian: GPT-4 0.54 95% CI [0.46, 0.54], GPT-3.5 0.50 95% CI [0.45, 0.52], GPT-3 0.35 95% CI [0.29, 0.35]; see Methods). It is evident from our data, however, that LLMs are still not perfect in predicting human color naming as compared to a separate lab-based experiment conducted by Lindsey and Brown (31) (dashed line in Figure 3A top, constrained naming task 0.73 95% CI [0.65,0.73], free naming task 0.75 95% CI [0.66,0.75]). Moreover, the naming of GPT-4 colors differs from human data in some important cases, including the color turquoise, which was selected as the dominant color for 46 Munsell colors in GPT4 versus only 15 in human data. Note, however, that our human English results conducted online and with relative less control over color presentation were highly consistent with (31) that were conducted in the lab and under controlled environment, even though (31) used slightly different paradigms: free naming (consistency to our human data: 0.75 95% CI [0.66, 0.75]) and forced-choice list with a different set of items (0.73 95% CI [0.65, 0.74]).</p>
<p>Importantly, however, GPT-4 appears to replicate crosslingual differences (Figure 3B), for example separating Russian blue and purple into distinct categories for lighter and darker areas (35). Indeed, the color sinij / Синий (Blue) was the dominant category for 18 and 29 Munsell colors for GPT-4 and humans, respectively, and the color golubój / Голубой (Light-blue) was accordingly the dominant category for 33 and 26 colors. Similarly, the color fiołétovyj / Фиолетовый (Violet) was the dominant category for 27 and 32 Munsell colors for GPT-4 and humans, respectively, and the color lilóvyj / Лиловый (Lilac) was accordingly the dominant category for 20 and 18 Munsell colors.</p>
<h3>Discussion</h3>
<p>In this work, we showed how recent advances in large language models and, in particular, their flexible prompt engineering capabilities provide an elegant way for extracting clear quantitative psychophysical signals from text corpora. Our results contribute to a variety of thought-provoking issues in perception and language research. In particular, our findings further support recent research suggesting that people that lack direct sensory experiences (e.g., congenitally blind individuals) could still possess a rich understanding of perceptual concepts through language (e.g., colors (10)). Likewise, the language-dependence of the color representation of GPT-4 in the naming task suggests that the physical stimulus alone (in our case, the color hex code) is insufficient for explaining the behavioral patterns of the model since the same hex codes triggered different color categorizations in Russian and English. This supports the idea that language is not only shaped by perception but can also reshape it (35). Finally, the fact that GPT-4 achieved IRR-level performance without any fine-tuning in the newly collected datasets of pitch and vocal consonants, contributes to our understanding of LLMs' ability to mimic human behavior (13).</p>
<p>We end by discussing some limitations which point towards</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Color naming experiment using 330 Munsell colors from the World Color Survey (top, color space). A. Adjusted Rand index illustrating the alignment between human and LLM experiments ( $95 \%$ CIs). The dashed lines for English represent lab-based free naming and forced-choice naming experiments collected by Lindsey and Brown (31) (data reproduced with permission). B. Data comparison between humans and LLMs in Russian and English. Participants and LLMs were shown colors and were asked to choose from the same 15 -color list. The count of chosen colors for each option is given in parentheses. The color of a response cluster in the maps represents its average color.
future research directions. First, while in our work we experimented with three different types of behavioral data (similarity, explanations, and naming), there are other perceptual measures that one could consider (for example, odd-one-out triplet judgments (36)). Future work could explore these measures and interrogate to what extent they too yield a consistent representation. Second, our work is restricted to population-level averages as a leading-order analysis, a follow-up study could look into the natural variability of the LLM judgments and see to what extent they capture individual-level differences in humans. Third, while our work provides some evidence for LLMs' ability to capture cross-cultural differences, it remains to be seen how far this holds for other languages, especially those that are underrepresented (37). Finally, it is important to point out that the very same massive training pipelines that make LLMs a powerful proxy of human language also make them particularly susceptible to inheriting biases (38). Researchers should be particularly cautious when interpreting the patterns of behavior elicited from LLMs and should always benchmark them against genuine human data.</p>
<p>To conclude, our work showcases how LLMs can be used to explore the limits of what information about the world can be recovered from language alone, and more broadly, it highlights the potential of combining human and machine experiments in order to understand fundamental questions in cognitive science.</p>
<h2>Materials and Methods</h2>
<p>GPT prompt elicitation. The general structure of the prompt elicitation template for the similarity experiments was: one sentence describing the dataset (e.g., "people described pairs of colors using their hex code."), one sentence describing the similarity rating task and scale (e.g., "how similar are the colors in each pair on a scale of 0-1 where 0 is completely dissimilar and 1 is completely similar?"), three sets of three lines each corresponding to two stimuli and their actual similarity rating taken from behavioral experiments which serve as few-shot in-context examples, and an additional set of three lines corresponding to the pair of target stimuli and an associated empty rating field for the model to fill in ("Color one: #FF0000:</p>
<p>Color two: #A020F0. Rating:"). These were necessary to ensure that the model provided numerical values, and in all cases consisted of three fixed and randomly chosen comparisons so that most of the content was left for the model to produce. For each pair of target stimuli, we elicited ten ratings from each GPT model. Across all repetitions of all pairs of stimuli for a given dataset, we used only the same three in-context examples to ensure that the model is exposed to only a very small fraction of the similarity judgments against which its ratings were compared (see SI for full prompts).</p>
<p>For the color-naming experiments, we first elicited 15 basic color names from GPT-4 using the prompt "Name 15 basic colors." and a temperature of 0 (to get the highest probability answers). We then had GPT-4 name the hex code corresponding to each of the WCS colors using the following prompt: "Here is a list of 15 basic color names: <shuffled basic color list>. Which of these names best describes the following color: <hex-code>? Respond only using the name." We repeated this prompt ten times for each WCS color with the basic color list shuffled each time and temperature set to the default 0.7 to elicit ten names per WCS color. We repeated the full procedure with both prompts translated to Russian (GPT-4 also responded to both of these in Russian; see SI for full prompt).</p>
<p>Stimuli. The six human similarity datasets we considered come in two flavors - direct (dis-)similarity ratings and confusion matrices - and from two sources - previous psychological studies from the literature and newly collected datasets. Confusion matrices provide an alternative way to compute similarity scores between stimuli by counting the number of times a stimulus $x$ is confused for a stimulus $y$. By normalizing the counts one gets confusion probabilities $p_{x y}$ which can be converted into similarity scores using the formula $s_{x y}=\sqrt{p_{x y} p_{y x} / p_{x x} p_{y y}}(39,40)$.</p>
<p>Colors This dataset was taken from (25) (also reproduced in (24)) and comprised direct similarity judgments across a set of 14 colors with wavelengths in the range $434-674$ nanometers. We converted wavelengths into RGB using the script at https://hasanyavuz.ozderya. net/?p=211 and then we used the webcolors Python package to convert into hex codes. To get better coverage of the color wheel in Figure 2B we extended the space to 23 color stimuli by interpolating between the original colors in the dataset and eliciting an extended similarity matrix from GPT-4.</p>
<p>Pitch This dataset was collected and made publicly available very recently by a subset of the authors in (41) (see details below). It contains similarity judgments over pairs of 25 harmonic complex tones ( 10 partials and 3dB/octave roll-off) over a two octave range from C4 ( 60 MIDI; 261.626 Hz ) to C6 ( 84 MIDI; 1046.502 Hz ). The</p>
<p>pitch values were separated by 1 semitone steps to account for the fact that pitch perception is logarithmic (33) where the mapping between frequencies $f$ in Hertz and pitch $p$ in semitones are given by $p=12 \log _{2} f / 440+69$.
Vocal consonants This dataset was also collected via an online study (see below) and comprised similarity judgments over 16 recordings of vocal consonants taken from the International Phonetic Association ${ }^{1}$. The vocal consonants considered were b (bay), p (pay), m (may), n (no), g (go), k (cake), d (die), t (tie), f (fee), v (vow), s (so), $\theta$ (thigh), $\delta$ (they), $\chi$ (Jacques), and $f$ (show). The recordings came from two speakers, one male and one female.
Loudness We accessed this dataset via (40) which itself takes the data from (26). The dataset comes in the form of a confusion matrix over 8 pure tones of different loudness values ranging from 71.1 to 74.6 decibels.</p>
<p>Taste This dataset was also accessed via (40) and is taken from (28). The data comes in the form of a confusion matrix over 10 flavors described to participants as salt, salt-substitute, MSG, quinine, acid, sugar, artificial sweetener, salt-sugar, acid-sugar, and quinine-sugar.
Timbre This dataset was assembled in (27) based on 1217 subject's judgments from 5 prior publications. It comprises dissimilarity judgments over 12 instrument timbres: clarinet, saxophone, trumpet, cello, French horn, oboe, flute, English horn, bassoon, trombone, violin, and piano.</p>
<p>Behavioral experiments. To collect similarity judgments over pitch and vocal consonants we deployed two online experiments on Amazon Mechanical Turk (AMT). Overall, 55 participants completed the pitch study and 64 participants completed the vocal consonants study. In addition, we collected color naming data in Russian and British English participants using Prolific ${ }^{\circledR}$. Overall, we recruited 154 participants of which 103 were UK participants and 51 were Russian participants. Experiments were implemented using PsyNet ${ }^{\circledR}$ and Dallinger ${ }^{5}$. See additional details in SI.</p>
<p>Code and Data availability. All data and code used in this work can be accessed via the following link: https://tinyurl.com/fudaby5p, with the exception of the (31) color naming dataset as it is only available upon request from the authors. An interactive visualization of two-dimensional MDS spaces for the 6 modalities is available at: https://computational-audition.github.io/ LLM-psychophysics/all-modalities.html. The human color naming data can be interactively explored via: https://computational-audition.github. io/LLM-psychophysics/color.html.</p>
<p>ACKNOWLEDGMENTS. This research project and related results were made possible with the support of the NOMIS Foundation, and an NSERC fellowship (567554-2022) to IS.</p>
<ol>
<li>D Hume, An Abstract of a Treatise of Human Nature, 1740. (CUP Archive), (1740).</li>
<li>J Locke, An essay concerning human understanding. (Kay \&amp; Troutman), (1847).</li>
<li>RL Goldstone, BJ Rogosky, Using relations within conceptual systems to translate across conceptual systems. Cognition 84, 295-320 (2002).</li>
<li>T Regier, P Kay, N Khetarpal, Color naming reflects optimal partitions of color space. Proc. Natl. Acad. Sci. 104, 1436-1441 (2007).</li>
<li>T Regier, P Kay, Language, thought, and color: Whorf was half right. Trends cognitive sciences 13, 439-446 (2009).</li>
<li>S Dotscheid, S Shayan, A Majid, D Casasanto, The thickness of musical pitch: Psychophysical evidence for linguistic relativity. Psychol. science 24, 613-621 (2013).</li>
<li>N Zaslavsky, C Kemp, T Regier, N Tishby, Efficient compression in color naming and its evolution. Proc. Natl. Acad. Sci. 115, 7937-7942 (2018).</li>
<li>JS Kim, GV EIi, M Bedny, Knowledge of animal appearance among sighted and blind adults. Proc. Natl. Acad. Sci. 116, 11213-11222 (2019).</li>
<li>G Lupyan, RA Rahman, L Boroditsky, A Clark, Effects of language on visual perception. Trends cognitive sciences 24, 930-944 (2020).</li>
<li>JS Kim, B Aheimer, V Montanil-Marrara, M Bedny, Shared understanding of color among sighted and blind adults. Proc. Natl. Acad. Sci. 118, e2020192118 (2021).
${ }^{5}$ https://www.internationalphoneticassociation.org/
${ }^{6}$ https://www.prolific.co
${ }^{7}$ https://psynet.dev/
${ }^{8}$ https://dallinger.readthedocs.io/</li>
<li>G Kawakita, A Zeleznikow-Johnston, N Tsuchiya, M Oizumi, Is my "red" your "red"?: Unsupervised alignment of qualia structures via optimal transport. PsyArXiv (2023).</li>
<li>T Brown, et al., Language models are few-shot learners. Adv. neural information processing systems 33, 1877-1901 (2020).</li>
<li>OpenAI, Opt-4 technical report (2023).</li>
<li>A Srivastava, et al., Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022).</li>
<li>A Goldstein, et al., Shared computational principles for language processing in humans and deep language models. Nat. neuroscience 25, 369-380 (2022).</li>
<li>S Kumar, et al., Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model. BioRxiv pp. 2022-06 (2022).</li>
<li>R Tikochniski, A Goldstein, Y Yeshurun, U Hasson, R Reichart, Perspective changes in human listeners are aligned with the contextual transformation of the word embedding space. Cereb. Cortex p. bhad082 (2023).</li>
<li>M Abdou, et al., Can language models encode perceptual structure without grounding? a case study in color. arXiv preprint arXiv:2109.06129 (2021).</li>
<li>K Siedenburg, C Sallis, How does chatgpt rate sound semantics? arXiv preprint arXiv:2304.07830 (2023).</li>
<li>C Zhang, B Van Durme, Z Li, E Stengel-Eskin, Visual commonsense in pretrained unimodal and multimodal models. arXiv preprint arXiv:2205.01850 (2022).</li>
<li>R Marjieh, et al., Words are all you need? capturing human sensory similarity with textual descriptors. The Eleventh Int. Conf. on Learn. Represent. (2022).</li>
<li>D Dillion, N Tandon, Y Gu, K Gray, Can ai language models replace human participants? Trends Cogn. Sci. (2023).</li>
<li>D Ganguli, et al., The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459 (2023).</li>
<li>RN Shepard, Multidimensional scaling, tree-fitting, and clustering. Science 210, 390-398 (1980).</li>
<li>G Ekman, Dimensions of color vision. The J. Psychol. 38, 467-474 (1954).</li>
<li>DE Konibrot, Theoretical and empirical comparison of lucera choice model and logistic thurstone model of categorical judgment. Percept. \&amp; Psychophys. 24, 193-208 (1978).</li>
<li>P Esting, A Bitton., et al., Generative timbre spaces: regularizing variational auto-encoders with perceptual metrics. arXiv preprint arXiv:1805.08501 (2018).</li>
<li>TP Hettinger, JF Gent, LE Marks, ME Frank, study of taste perception. Percept. \&amp; Psychophys. 61, 1510-1521 (1999).</li>
<li>B Berlin, P Kay. Basic color terms: Their universality and evolution. (Univ of California Press), (1991).</li>
<li>P Kay, B Berlin, L Maffi, WR Merrifield, R Cook, The world color survey. (Citesøer), (2009).</li>
<li>DT Lindsey, AM Brown, The color lexicon of american english. J. vision 14, 17-17 (2014).</li>
<li>RN Shepard, Geometrical approximations to the structure of musical pitch. Psychol. review 89, 305 (1982).</li>
<li>N Jacoby, et al., Universal and non-universal features of musical pitch perception revealed by singing. Curr. Biol. 29, 3229-3243 (2019).</li>
<li>GV Paramei, YA Griber, D Mylonas, An online color naming experiment in russian using munset color samples. Color. Res. \&amp; Appl. 43, 358-374 (2018).</li>
<li>J Winawer, et al., Russian blues reveal effects of language on color discrimination. Proc. national academy sciences 104, 7780-7785 (2007).</li>
<li>MN Hebart, CY Zheng, F Pereira, CI Baker, Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. Nat. human behaviour 4, 1173-1185 (2020).</li>
<li>DE Blasi, J Henrich, E Adamou, D Kemmerer, A Majid, Over-reliance on english hinders cognitive science. Trends cognitive sciences (2022).</li>
<li>TY Zhuo, Y Huang, C Chen, Z Xing, Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv preprint arXiv:2301.12867 (2023).</li>
<li>RN Shepard, Toward a universal law of generalization for psychological science. Science 237, 1317-1323 (1987).</li>
<li>CR Sims, Efficient coding explains the universal law of generalization in human perception. Science 360, 652-656 (2018).</li>
<li>R Marjieh, TL Griffiths, N Jacoby, Musical pitch has multiple psychological geometries. bioRxiv (2023).</li>
<li>P Harrison, et al., Gibbs sampling with people in Advances in Neural Information Processing Systems, eds. H Larochelle, M Ranzato, R Hadsell, MF Balcan, H Lin. (Curran Associates, Inc.), Vol. 33, pp. 10659-10671 (2020).</li>
<li>KJ Woods, MH Siegel, J Tsien ,JH McDermott, Headphone screening to facilitate web-based auditory experiments. Attention, Perception, \&amp; Psychophys. 79, 2064-2072 (2017).</li>
<li>N Kriegeskorte, M Mur, PA Bandettini, Representational similarity analysis-connecting the branches of systems neuroscience. Front. systems neuroscience 2, 4 (2008).</li>
<li>J Clark, The ishihara test for color blindness. Am. J. Physiol. Opt. (1924).</li>
<li>WM Rand, Objective criteria for the evaluation of clustering methods. J. Am. Stat. association 66, 846-850 (1971).</li>
</ol>
<h2>Extended Methods</h2>
<p>Explicit GPT prompts. GPT prompt elicitation experiments were conducted using the OpenAI Text Completion (for GPT-3) and Chat Completion (for GPT-3.5 and GPT-4) APIs. The temperature was set to 0.7 for similarity judgments and 0 for color naming experiments. The exact prompt formats used are shown below.</p>
<h2>Similarity judgments.</h2>
<h2>Color:</h2>
<p>People described pairs of colors using their hex codes. How similar are the two colors in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar? Respond only with the numerical similarity rating. Color 1: #ff5700 Color 2: #ff9b00 Rating 0.76 Color 1: #b3ff00 Color 2: #00ff61 Rating: 0.45 Color 1: #FF0000 Color 2: #00b2ff Rating: 0.02 Color 1: <hex-code1> Color 2: <hex-code2> Rating:</p>
<h2>Pitch:</h2>
<p>People described pairs of musical notes using their frequencies in hertz.
How similar are the musical notes in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Note 1: 587.3295358348151 Hz
Note 2: 987.7666025122483 Hz
Rating: 0.46083740655517463
Note 1: 349.2282314330039 Hz
Note 2: 277.1826309768721 Hz
Rating: 0.743838237117938
Note 1: 415.3046975799451 Hz
Note 2: 987.7666025122483 Hz
Rating: 0.19874605585261726
Note 1: <frequency1>
Note 2: <frequency2>
Rating:</p>
<h2>Vocal consonants:</h2>
<p>People described vocal consonants using the international phonetic alphabet (IPA).
How similar do the vocal consonants in each pair sound on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar? Respond only with the numerical similarity rating.</p>
<p>Vocal Consonant 1: f
Vocal Consonant 2: m
Rating: 0.5
Vocal Consonant 1: n
Vocal Consonant 2: 3
Rating: 0.40740740740740744
Vocal Consonant 1: $\int$
Vocal Consonant 2: $\int$
Rating: 1.0
Vocal Consonant 1: <consonant1>
Vocal Consonant 2: <consonant2>
Rating:</p>
<h2>Loudness:</h2>
<p>People described the loudness of pure tones in decibels (dB).
How similar do the pure tones in each pair sound on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Pure Tone 1: 72.6 dB
Pure Tone 2: 74.1 dB
Rating: 0.3495324720283043
Pure Tone 1: 74.6 dB
Pure Tone 2: 73.6 dB
Rating: 0.5055839477695901
Pure Tone 1: 74.1 dB
Pure Tone 2: 74.1 dB
Rating: 1.0
Pure Tone 1: <loudness1>
Pure Tone 2: <loudness2>
Rating:
Taste:
People described flavors they tasted using words. How similar are the flavors in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Flavor 1: quinine
Flavor 2: artificial sweetener
Rating: 0.0
Flavor 1: artificial sweetener
Flavor 2: salt
Rating: 0.015433904145892428
Flavor 1: quinine-sugar
Flavor 2: acid-sugar
Rating: 0.2539115246067999
Flavor 1: <flavor1>
Flavor 2: <flavor2>
Rating:
Timbre:
People listened to pairs of musical instruments and rated the similarity of their timbre.
How similar is the timbre of the instruments in each pair on a scale of $0-1$ where 0 is completely dissimilar and 1 is completely similar?</p>
<p>Instrument 1: Cello
Instrument 2: Flute
Rating: 0.5604846433040316
Instrument 1: Flute
Instrument 2: Clarinet
Rating: 0.270932601836378
Instrument 1: Trombone
Instrument 2: Bassoon
Rating: 0.2893895067551666
Instrument 1: <instrument1>
Instrument 2: <instrument2>
Rating:</p>
<h2>Color naming.</h2>
<h2>Basic color free-elicitation:</h2>
<p>English:
Name 15 basic colors.
Russian:
Перечислите 15 основных цветов.</p>
<h2>Color naming elicitation:</h2>
<p>English:
Here is a list of 15 basic color names: <shuffled basic color list $>$.
Which of these names best describes the following color: $&lt;$ hex-code $&gt;$ ?
Respond only using the name.
Russian:
Вот список из 15 названий основных цветов: <shuffled basic color list $>$.
Какое из названий цветов лучше всего описывает следуюший цвет: <hex-code>?
Отвечайте только названием одного цвета из списка.
We repeated this prompt ten times for each WCS color with the basic color list shuffled each time and temperature set to the default 0.7 to elicit ten names per WCS color. For each of the ten elicitations per color, if the output was not one of the 15 basic colors we would keep re-querying GPT until it did give a valid color output (GPT-4 is slightly non-deterministic even at temperature 0 due to changes in hardware). If after 10 attempts the response was still invalid, we would return "error" as the color (this response is later discarded from the analysis).</p>
<h2>Additional details of behavioral experiments.</h2>
<p>Similarity Experiments: Participants. To collect similarity judgments over pitch and vocal consonants we deployed two online experiments on Amazon Mechanical Turk (AMT). ${ }^{<em> </em>}$ The recruitment and experimental pipelines were automated using PsyNet (42), a modern framework for experiment design ${ }^{11}$ and deployment which builds on the Dallinger ${ }^{12}$ platform for recruitment automation. Overall, 55 participants completed the pitch study and 64 participants completed the vocal consonants study. Participants were recruited from the United States, were paid $\$ 9-12$ USD per hour, and provided informed consent as approved by the Princeton IRB (#10859) and the Max Planck Ethics Council (#2021_42).</p>
<p>To enhance data quality, participants had to pass a standardized headphone check (43) that ensures good listening conditions and task comprehension, and were required to have successfully completed at least 3000 tasks on AMT. Upon passing the prescreening stage, participants were randomly assigned to rate the similarity between different pairs of stimuli and provided numerical judgments on a 7 Likert scale ranging from 0 (completely dissimilar) to 6 (completely similar). In the pitch experiment, participants provided an average of 80 judgments, and in the vocal consonants experiment an average of 55 judgments. See SI for explicit instructions.</p>
<p>Similarity Experiments: Procedure. Upon providing informed consent and passing the headphone check, participants received the following instructions. In the case of the pitch experiment: "In this experiment we are studying how people perceive sounds. In each round you will be presented with two sounds and your task will be to simply judge how similar those sounds are. You will have seven response options, ranging from 0 ('Completely Dissimilar') to 6 ('Completely Similar'). Choose the one you think is most appropriate. You will also have access to a replay button that will allow you to replay the sounds if needed. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer." Participants were then informed of an additional small quality bonus "The quality of your responses will be automatically monitored, and you will receive a bonus at the end of the experiment in proportion</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to your quality score. The best way to achieve a high score is to concentrate and give each round your best attempt". While the task is subjective in nature, we used consistency as a proxy for quality by repeating 5 random trials at the end of the experiment and computing the Spearman correlation $s$ between the original responses and their repetitions. The final bonus was computed using the formula $\min (\max (0.0,0.1 s), 0.1)$ yielding at most 10 cents. In the main experiment participants were assigned to random stimulus pairs and were instructed to rate their similarity using the following prompt: "How similar are the pair of sounds you just heard?" and provided a response on a Likert scale. The procedure for the vocal consonants similarity experiment was identical up to the specific instructions. Specifically, participants received the following instructions: "In this experiment we are studying how people perceive the sound of vocal consonants. A consonant is a speech sound that is pronounced by partly blocking air from the vocal tract. For example, the sound of the letter $c$ in cat is a consonant, and so is $t$ but not $a$. Similarly, the sound of the combination $s h$ in sheep is a consonant, and so is $p$ but not $e e$. In general, vowel sounds like those of the letters $a, e, i, o$, u are not consonants." The instructions then proceeded: "In each round you will be presented with two different recordings each including one consonant sound and your task will be to simply judge how similar are the sounds of the two spoken consonants. We are not interested in the vowel sounds nor in the voice height, just the sound of the consonants. You will have seven response options, ranging from 0 ('Completely Dissimilar') to 6 ('Completely Similar'). Choose the one you think is most appropriate. Note: no prior expertise is required to complete this task, just choose what you intuitively think is the right answer." Participants were then informed of the quality bonus which was identical to the pitch task, and then rated the similarity between pairs of random consonants based on the following prompt "How similar is the sound of the consonants pronounced by the two speakers?" and a Likert scale as before.</p>
<p>Similarity Experiments: Model Evaluation. We quantified model performance in predicting human similarity judgments by computing the Pearson correlation coefficient between the flattened upper triangle of the LLM-based and human-based similarity matrices (to account for the fact that these matrices are symmetric). This approach is similar to representational similarity analysis (44). To compute $95 \%$ confidence intervals, we bootstrapped with replacement over model predictions with 1,000 repetitions and computed for each repetition the average similarity matrix. We then correlated the upper triangles of each of those matrices with human data to produce a list of correlation coefficients on which we computed confidence intervals.</p>
<p>Color Naming Experiments: Participants. To collect the color naming data in Russian and British English participants, we ran online experiments on Prolific ${ }^{\text {S5 }}$. Overall, we recruited 103 UK participants and 51 Russian participants. All texts in the interface of the experiment (e.g., buttons, instructions, etc) were presented in the native language of the participant. The Russian texts were first automatically translated using DeepL ${ }^{\text {TM }}$ and then manually checked and corrected by a native speaker of Russian (author I.S). Participants had to be raised monolingually and to speak the target language as their mother tongue. Each participant was paid 9 GBP per hour and provided informed consent according to an approved protocol (Max Planck Ethics Council #2021_42). The experiment was implemented using PsyNet (42). Each session starts with a free-elicitation task where participants are asked to provide basic colors:</p>
<h2>Color Naming Experiments: Procedure.</h2>
<h2>Basic color free-elicitation:</h2>
<p>English:
Please name at least 8 basic color names.
Press enter after each color name.
Only use lower-case letters.</p>
<p>Russian:
Укажите не менее 8 названий основных цветов.
Нажмите клавишу Enter после каждого названия цвета.
Используйте только строчные буквы.</p>
<p>Participants may only submit color names without spaces, numbers, or special characters and can only submit the page if they have provided at least eight names. The list of obtained colors is highly overlapping with the GPT-4 list, justifying our choice to use GPT-4 as the basis for the word naming task (colors are sorted by their naming frequency).</p>
<p>Top 15 terms in English:
"blue", "green", "yellow", "red", "purple", "orange", "black", "pink", "white", "brown", "grey", "violet", "indigo", "turquoise", "silver"</p>
<p>Top 15 terms in Russian:
"красный", "синий", "белый", "зеленый", "оранжевый", "желтый", "фиолетовый", "черный", "голубой", "коричневый", "розовый", "серый", "жёлтый", "зелёный", "чёрный"</p>
<p>From the top 15 color terms, 11 (English) and 12 (Russian) color terms are overlapping with the list provided by GPT-4. Before the main experiments, participants received the following instructions:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Color naming instructions:</h2>
<p>English:
During this experiment, you will be presented with a square of a particular color and will be required to select the most suitable color term from a list of options.
Please be aware that some of the colors may be repeated to verify consistency of your choices.
If we detect any inconsistencies in your answers, we may terminate the experiment prematurely.
The best strategy is to answer each question truthfully, as attempting to memorize responses may prove difficult.</p>
<p>Russian:
В ходе этого эксперимента вам будет представлен квадрат определенного цвета, и вам нужно будет выбрать наиболее подходящее название цвета из списка вариантов.
Имейте в виду, что некоторые цвета могут повторяться, чтобы убедиться в согласованности вашего выбора. Если мы обнаружим какие-либо несоответствия в ваших ответах, мы можем досрочно прекратить эксперимент.
Лучшая стратегия - отвечать на каждый вопрос правдиво, так как попытка запомнить ответы может оказаться сложной.</p>
<p>The participants then went through the main experiment:</p>
<h2>Color naming task:</h2>
<p>English:
<square of a particular color>
You will see below a list of 15 basic color names. Which of these names best describes the color above?
$&lt;$ shuffled basic color list presented as buttons $&gt;$</p>
<p>Russian:
<square of a particular color>
Ниже Вы увидите список из 15 основных названий цветов.
Какое из этих названий лучше всего описывает вышеуказанный цвет?
$&lt;$ shuffled basic color list presented as buttons $&gt;$</p>
<p>At the end of the experiment, the participant took a color blindness test (45). Some participants abandoned the experiment prematurely, but we nevertheless included their responses ( 42 English participants, 3 Russian participants). Only a fraction of the participants failed the color blindness test ( 5 of 103 English participants, and 2 of 51 Russian participants). Consistent with the WCS we included all participants including those who failed the color blindness test. In a control analysis, we excluded all color blind individuals and all participants that did not complete the entire session and got nearly identical results (the adjusted Rand index was 0.92 for English experiments and 0.97 for Russian experiments).</p>
<p>Color Naming Experiments: Analysis. For each color, we collected at least 10 responses per LLM variant, and at least 10 forcedchoice human selections per color (English mean 19.30 responses, Russian mean 12.17 responses). Consistent with previous literature for each Munsell's color we selected the most frequently reported term. We then presented the dominant colors in Figure 3B. To aid visualization we average the RGB values of all colors with the same color term, and presented them as the legend and clustered color in that figure. We also listed per color the number of Munsell's colors that were associated with each dominant color term. Figure 3B provides additional information on the degree of agreement for each color. Colors for which less than $50 \%$ and $90 \%$ of the times the dominant color term was selected were indicated by "-" and "*", respectively. If the dominant color term was selected more than $90 \%$ of the time, no marking was used.</p>
<p>Adjusted Rand index. The Rand index (46) is a label-insensitive measure of clustering similarity that instead of relying on specific labels (e.g. "Blue") quantifies the similarity between two clustering partitions by counting pairs of items (in our case Munsell colors) that are clustered consistently and dividing them by the overall number of pairs. This allows to compare different clustering schemes when the vocabulary of labels is not aligned (e.g. English and Russian). Formally, we computed: $R=(b+c) / a$; where $b$ is the number of pairs of items that are in the same subset in one clustering and in the same subset in the other, $c$ is the number of pairs of items that are in different subsets in one clustering and in different subsets in the other and $a$ is the total number of pairs. The Rand index provides high values for two random clusterings, to adjust for this we used the corrected-for-chance version of the Rand index (46), which normalizes the raw value by the expected value of the Rand index for random clusterings. Formally, we have $A R I=\left(R I-R I_{\text {rand }}\right) /(1-$ $R I_{\text {rand }}$ ) where $A R I$ is the adjusted Rand index, $R I$ is the raw Rand index and $R I_{\text {rand }}$ is the expected Rand index for random clusterings. The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples, exactly 1.0 when the clusterings are identical (up to a permutation), and reaches -0.5 for "orthogonal clusters" that are less consistent relative to what is expected by chance. In our case, all values were strictly positive suggesting consistency across languages and experiments. To compute confidence intervals, we created bootstrapped datasets by sampling the responses of each color with replacement and recomputing the dominant selected color name. We then obtained CIs by computing the adjusted Rand index for 1,000 pairs of bootstrapped datasets.</p>
<p>Lindsey and Brown dataset. We compared our experimental data to a dataset by (31), reproduced with permission by Delwin Lindsey. The data contains two experimental conditions conducted in the lab with the same 51 participants. In the first condition, participants were instructed to provide free naming responses. In the second condition, participants were instructed to choose from a pre-specified list of 11 color terms: (Green, Blue, Purple, Pink, White, Brown, Orange, Yellow, Red, Black, and Gray). Despite the fact that the Lindsey \&amp; Brown experiment was conducted in the lab (and not online like our experiments) and that the constrained list in our experiment was somewhat different, the results of both experiments were highly consistent with our human English data (Constrained, $A R I=0.73$ $95 \%$ CI $[0.65,0.74]$, free naming $0.7595 \%$ CI $[0.66,0.75]$ ). In addition to putting an upper bound on the consistency with which the LLM can predict human data (by comparing it with another human experiment), these results prove that despite less control over color presentation compared to the lab, online presentation still provides high-quality color naming data.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{55}$ https://www.prolific.com
${ }^{\text {TM }}$ https://www.deepl.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
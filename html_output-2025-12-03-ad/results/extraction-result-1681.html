<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1681 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1681</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1681</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-258947357</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.17110v1.pdf" target="_blank">IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality</a></p>
                <p><strong>Paper Abstract:</strong> Robotic assembly is a longstanding challenge, requiring contact-rich interaction and high precision and accuracy. Many applications also require adaptivity to diverse parts, poses, and environments, as well as low cycle times. In other areas of robotics, simulation is a powerful tool to develop algorithms, generate datasets, and train agents. However, simulation has had a more limited impact on assembly. We present IndustReal, a set of algorithms, systems, and tools that solve assembly tasks in simulation with reinforcement learning (RL) and successfully achieve policy transfer to the real world. Specifically, we propose 1) simulation-aware policy updates, 2) signed-distance-field rewards, and 3) sampling-based curricula for robotic RL agents. We use these algorithms to enable robots to solve contact-rich pick, place, and insertion tasks in simulation. We then propose 4) a policy-level action integrator to minimize error at policy deployment time. We build and demonstrate a real-world robotic assembly system that uses the trained policies and action integrator to achieve repeatable performance in the real world. Finally, we present hardware and software tools that allow other researchers to reproduce our system and results. For videos and additional details, please see http://sites.google.com/nvidia.com/industreal .</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1681.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1681.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IndustReal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IndustReal (End-to-end RL-based Sim-to-Real Assembly System)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated set of algorithms, tools, and a real-world system that trains contact-rich assembly policies in high-fidelity GPU simulation and transfers them to a Franka Emika Panda robot with no real-world policy adaptation, demonstrating end-to-end detection, grasping, transport, and insertion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda (with wrist-mounted Intel RealSense D435)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF collaborative robot arm (Franka Panda) equipped with a wrist-mounted RGB-D camera (RealSense D435) and general-purpose parallel-finger gripper; used to detect, grasp, transport, and insert industrial-style small parts (pegs, gears, connectors) on an optical breadboard.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (contact-rich assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym + Factory (GPU contact simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>GPU-accelerated physics simulation (Isaac Gym) augmented by the Factory contact module to simulate rigid-body dynamics with contact and friction between complex meshes, precomputed SDFs for contact/reward queries, and high-frequency physics stepping to enable faster-than-real-time RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity contact-rich physics simulation (mesh-based contact, SDFs), optimized for speed on GPU</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>detailed contact dynamics between complex meshes, signed distance fields (SDFs) for geometry alignment and contact queries, mesh interpenetration detection, rigid-body kinematics/dynamics, actuator-level low-level control emulation, observation noise and pose randomization</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no visuotactile sensing modeled (no tactile sensors), limited or no dynamics randomization (they intentionally avoided dynamics randomization), some heuristic dissipative terms were removed or tuned (the paper notes arbitrary heuristic damping is sometimes present and they removed it), and no real-world sensor drift/time-varying parameters were modeled explicitly</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Bench-top workcell with a Franka Panda arm, wrist-mounted Intel RealSense D435 RGB camera, optical breadboard with bolted sockets and loose plugs/parts, standard two-finger gripper (no force/torque sensors), camera calibration + Mask R-CNN-based perception pipeline for 2D pose estimation; experiments measured over hundreds of trials.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>End-to-end assembly behaviors decomposed into Pick, Place, and Insert phases: perception-driven grasping, reaching/placing to low targets in contact, and contact-rich insertion (peg-in-hole, gears onto shafts, electrical connector insertion).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>On-policy reinforcement learning (Proximal Policy Optimization, PPO) trained in simulation using asymmetric actor-critic and GPU parallelism.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate (full insertion), engagement rate (partial insertion), positional/rotational terminal error (mm, rad), and steady-state position error for Reach tasks (mm).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Reported simulation success rates for assembly Insert tasks ~82–92% depending on task and evaluation; joint reported simulated Insert (pegs) success ≈ 88.6% (engagement 96.6%), gears Insert ≈ 82.0% (engagement 85.2%); overall simulation benchmarks 82–99% across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Real-world aggregated success/engagement rates over 600 trials: reported ranges 83–99% for different tasks; example Insert real-world: pegs engagement 86.7% / success 76.7%, gears engagement 95.0% / success 92.5%, connectors engagement 100% / success 85%; end-to-end PPI (Pick-Place-Insert) showed peg success 80%, gear success 97.5%, connector success 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Significant randomization of plug and socket 6-DOF poses (XY ≈ ±10 mm, socket larger randomization up to ±10 cm in some evals), initial height ranges, observation noise added (e.g., target XY noise ±2 mm at test, training observation noise ±1 mm), randomization of yaw (±5°), and randomized trays/angles for pick experiments; explicit dynamics randomization (masses/friction/etc.) was intentionally avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled nonlinear joint friction and imperfect gravity compensation on the real robot, sensor/perception noise and calibration offsets (camera extrinsic bias), mesh interpenetrations and numerical artifacts in simulation, heuristic damping terms in simulation that misrepresent real dissipation, object slip in the gripper and wedging/jamming phenomena, and control/latency differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-fidelity contact simulation (Factory) with precomputed SDFs; simulation-aware policy updates (SAPU) to avoid learning from spurious interpenetrations; SDF-based dense reward for accurate geometric alignment; sampling-based curriculum (SBC) exposing breadth of initial states; careful observation and pose randomization; removal/tuning of heuristic damping in simulator; deployment-time Policy-Level Action Integrator (PLAI) to mitigate steady-state errors from unmodeled friction/gravity; robust vision pipeline and careful camera calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate contact dynamics and low interpenetration rates are critical (they used interpenetration thresholding and weighting); precise geometric alignment via SDF-based rewards was essential (reduced position error from ~12–31 mm to ~3.8 mm); physics frequency and control rates matter (trained at 120 Hz physics, 60 Hz control); realistic small clearances (0.5–0.6 mm) must be modeled; dynamics randomization is not necessary given strong simulator priors but omission requires high simulator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end assembly policies trained in a high-fidelity GPU contact simulator can transfer to a real robot without any real-world policy adaptation when (1) the simulator models contact-rich dynamics well, (2) training procedures explicitly mitigate simulation artifacts (SAPU to avoid interpenetration exploits), (3) dense geometric alignment rewards (SDF-based) and a sampling-based curriculum are used to learn robust insertion behaviors, (4) observation/pose randomization is applied (but not dynamics randomization), and (5) a simple deployment-time integrator (PLAI) reduces steady-state error from unmodeled friction/gravity—yielding repeatable real-world success rates across Pick/Place/Insert tasks comparable to simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1681.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1681.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factory (in Isaac Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factory: Fast contact for robotic assembly (Factory module integrated with NVIDIA Isaac Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPU-accelerated contact simulation module used within Isaac Gym that can handle many contacts between complex meshes and provides precomputed SDFs, enabling faster-than-real-time training of contact-rich manipulation policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Factory: Fast contact for robotic assembly</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>simulation platform (Factory + Isaac Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A high-performance GPU-based physics simulator specialized for rigid-body contact between complex geometries, intended to enable large-scale RL for contact-rich tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>simulation platform for robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym + Factory</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulates rigid-body dynamics with frictional contact between detailed meshes, precomputes SDFs for objects (used both for contact and reward), supports GPU-parallelized rollouts at high physics frequencies, and integrates with RL frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity contact simulation optimized for speed (mesh-based collisions, SDFs, intersection/penetration handling)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>mesh-level contact, signed distance fields (SDFs) for geometry queries, frictional contact responses, many simultaneous contacts per rigid body pair, high-frequency stepping for realistic dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no tactile sensor simulation by default, some dissipative/frictional parameter approximations may be present unless explicitly identified/tuned, limited modeling of actuator-level torque/servo nonlinearities unless provided</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>N/A (this is the simulator used to train policies that were deployed on a Franka Panda real robot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Contact-rich assembly tasks (peg-in-hole, gear insertion, connector insertion) trained in simulation for transfer to real-world robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>On-policy RL (PPO) with large parallelized simulation rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Simulation success rates, positional/rotational error, engagement/full-insertion rates used to predict and correlate with real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Used to obtain high simulated success rates (e.g., Insert pegs ≈ 88.6% under their pipeline and ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Pose randomizations, observation noise added; dynamics randomization intentionally avoided in main pipeline but can be supported by the simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Numerical artifacts such as transient mesh interpenetrations and any heuristic dissipative terms if not carefully controlled; parameter mismatch between simulated friction/gravity and real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Precomputed SDFs (used for both contact generation and reward), high physics frequency, and mesh-quality handling (interpenetration checking) enabled learning policies usable in reality.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate contact modeling and small-clearance geometry fidelity are essential for assembly transfer; the simulator must minimize transient interpenetrations and avoid unrealistic damping for accurate results.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A high-performance GPU contact simulator that models mesh-level contact and exposes SDFs can support training of contact-rich RL policies that transfer to real robots, provided simulation artifacts (interpenetration, damping) are managed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1681.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1681.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAPU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-Aware Policy Update (SAPU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithmic training-time technique that detects mesh interpenetrations per episode and either filters or weights episode returns during PPO updates to discourage policies that exploit simulation artifacts (spurious penetrations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda (policy training context)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Policies for Pick/Place/Insert trained with PPO in Factory/Isaac Gym and deployed on Franka hardware; SAPU is a training technique applied in simulation to improve transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics manipulation (sim-to-real training)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym + Factory (with a GPU-based interpenetration checker implemented using Warp)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulates plug/socket meshes and computes per-episode maximum interpenetration depth by sampling mesh points and querying distances to target mesh.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity contact simulation with explicit interpenetration checking</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>mesh interpenetration depth detection, contact geometry sampling, SDF/closest-point queries</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>does not itself change other simulator dynamics (friction/viscous damping left as-is); relies on existing simulator fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Applied offline during simulation training to produce policies later deployed on Franka robot; no real-world element to SAPU itself.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Improves robustness of Insert policies to avoid exploiting simulation interpenetrations so learned insertion behaviors transfer to the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>PPO with per-episode returns either filtered out if interpenetration > threshold, weighted by interpenetration depth, or both (Filter & Weight strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Simulation Insert success rate conditioned on episodes with low interpenetration (d_max_ip < thresholds), and final real-world transfer success (indirectly improved).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Filter & Weight strategy produced strong simulation success rates (84.9–87.6% across evaluated scenarios) where baseline/filter-only/weight-only sometimes failed to exceed ~40% under certain interpenetration test conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Spurious collisions/interpenetrations in simulation that policies can exploit and which do not exist in reality, leading to non-transferable behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using GPU-based interpenetration checking to (a) filter bad episodes and (b) weight returns by a function of max interpenetration (1 - tanh(d/epsilon)), thereby biasing policy updates toward non-exploitative, physically-plausible behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Simulator must allow reliable detection of interpenetration (mesh sampling and query) and be run at sufficient fidelity/mesh quality so that interpenetration measurements are meaningful; controlling interpenetration during training is critical for learning transferable insertion behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly penalizing or filtering episodes with mesh interpenetration during training prevents policies from exploiting simulation artifacts and substantially improves learned insertion success in simulation (and indirectly supports transfer), with the combined Filter & Weight strategy performing best.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1681.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1681.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SDF-Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Signed Distance Field (SDF)-Based Dense Reward</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense reward defined by querying precomputed signed distance fields at sampled object surface points to create a continuous geometric-alignment signal that strongly improves position/rotation accuracy for assembling complex geometries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda (policy training context)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Training-time reward shaping method used by PPO agents in Factory to learn precise geometric alignment for insertion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics manipulation (sim-to-real reward design)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym + Factory (SDF precomputation via pysdf and Factory's contact SDFs)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Uses precomputed dense voxel SDFs for target object poses and queries SDF values at transformed sample points each timestep to compute an RMS SDF distance reward.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity geometric alignment via dense SDF queries</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>high-resolution geometry alignment, continuous signed distance queries, dense point sampling on meshes, integrated with physics-based contact model</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>SDFs are precomputed for nominal poses and queried via voxel grids (interpolation); dynamic deformation/tactile feedback not modeled</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>SDF reward used only in simulation during training; no SDFs computed on real robot (real-world deployment uses learned policy and perception), though real geometry CAD models existed for simulation/real-world parts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Enables precise alignment and insertion behavior for non-axisymmetric and symmetric parts, reducing terminal positional and rotational error prior to transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>PPO with SDF-based dense reward replacing or augmenting keypoint/chamfer rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Simulation terminal position/rotation error, success (full insertion) and engagement (partial insertion) rates; downstream real-world transfer success improved when trained with SDF reward.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>SDF Query Distance reward achieved terminal errors ≈ 3.80 mm position and 0.086 rad rotation with success rate 88.6% and engagement 96.6%; alternative rewards produced much larger errors (11.7–31.0 mm position, 0.13–0.99 rad rotation) and lower success.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>If geometry is not modeled precisely (mesh resolution / clearance modeling), SDF-based rewards will not reflect true contact alignment; real-world perception/pose estimation noise can degrade policy performance if not randomized during training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Availability of accurate CAD meshes and precomputed SDFs (Factory already computes SDFs for contact), using SDF-based rewards for dense geometric alignment in insertion training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High-quality geometric meshes and accurate SDF queries are essential; SDF-based reward substantially reduces positional and rotational error required for tight-clearance assemblies (≈0.5 mm clearance).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SDF-based dense rewards provide a far more informative alignment signal than collinear/6-DOF keypoints or chamfer distances for insertion tasks, dramatically reducing position/rotation error and increasing insertion success in simulation, which supports transfer to real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1681.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1681.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling-Based Curriculum (SBC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum learning strategy that samples initial states over the full training distribution while progressively raising the lower bound of difficulty to avoid overfitting to partially-inserted states and improve learning for discontinuous contact tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda (policy training context)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Curriculum strategy used during PPO training to generate varied initial heights/poses for plugs above sockets, ensuring exposure to full state distribution and gradual increase in difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics manipulation (curriculum learning for sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Isaac Gym + Factory</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>During training, initial plug heights sampled from Uniform[z_low, z_high] where z_low is increased/decreased automatically based on recent mean success, allowing sampling across the entire range each stage.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>training-time curriculum applied in high-fidelity contact simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>variation in initial geometry poses/heights and resulting contact dynamics within simulator</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>curriculum manipulates initial state distributions only; does not introduce new dynamics modeling</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>No real-world curriculum; SBC used to produce policies that generalize to real-world initial condition variability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Improves robustness of insertion policies to varied initial heights/poses and prevents learning trivial partial-insertion solutions that do not transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>PPO combined with SBC that adaptively advances or reverts curriculum stage by adjusting lower height bound based on mean success rate (advance if >80%, revert if <10%).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Simulation position/rotation error, engagement and full-insertion success rates; downstream real-world generalization improved by better simulated learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Sampling-Based Curriculum achieved Insert (pegs) success 88.6% with position error 3.80 mm, outperforming Baseline and Standard curricula (which had success <=66.8% and position error ~10.7 mm).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Overfitting to narrow initial-state subsets (e.g., partially-inserted plugs) can produce brittle policies that fail under real-world initial pose variance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Maintain sampling across full state range while progressively increasing lower bound of difficulty (z_low) based on success statistics, ensuring exposure to both easy and hard initial conditions from the start.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Curriculum must expose contact discontinuities (e.g., initial states above hole) to prevent overfitting; sampling across full distribution at each stage is crucial for discontinuous contact tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling-Based Curriculum that samples across the entire initial-state range while raising the lower bound of difficulty yields substantially better insertion learning (lower position error and higher success) than standard incremental curricula or no curriculum, improving the likelihood of real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1681.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1681.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy-Level Action Integrator (PLAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple deployment-time controller augmentation that accumulates (integrates) policy-generated incremental actions into a desired state that is tracked by a low-level impedance controller, reducing steady-state error due to unmodeled dynamics like friction and gravity mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda (deployment-time controller augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>At deployment, PLAI modifies how policy actions are applied: actions are composed on the last desired state (integrated) rather than on the instantaneous real state, producing a setpoint that the low-level TSI controller tracks, with optional leaky anti-windup.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics control / sim-to-real deployment</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>n/a (PLAI is a deployment-time method applied when running policies trained in simulation on real hardware; evaluated in sim & real)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Evaluated in simulation (Reach policy under friction/gravity perturbations) and on the real robot to test rejection of unmodeled friction/gravity and steady-state error mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>used in conjunction with high-fidelity simulation for evaluation, but PLAI itself is a controller-level deployment technique</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>PLAI is agnostic to simulator fidelity; evaluated against simulated joint friction and gravity perturbations (e.g., 0.15 Nm joint friction, gravity perturbation 0.12 m/s^2) and in real-world dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>PLAI does not add sensing or dynamics modeling; it modifies action composition logic and relies on the low-level impedance controller to track the integrated setpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Deployed on Franka Panda performing Reach tasks and full assembly tasks; showed lower steady-state position error and variance in real-world experiments compared to nominal action application or classical PID.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Mitigates steady-state errors in reaching and insertion due to unmodeled friction/gravity, improving execution accuracy of policies trained in simulation when applied to real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>No retraining; PLAI is applied at deployment only to policies trained with PPO in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Steady-state positional error (mm) for Reach tasks; downstream assembly success improvements in real-world deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>In simulation under friction/gravity perturbations, PLAI maintained ≈2 mm error and had substantially lower error/variance than Nominal and classical PID (example: steady-state error PLAI ~1.6–2 mm versus PID ~3.5 mm in some tests).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>On real robot Reach tests, PLAI produced ≈2 mm steady-state error whereas the Franka-provided TSI baseline (no RL) had 4.45 mm error; PLAI had lower variance than Nominal action application.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled nonlinear friction, imperfect gravity compensation, and actuator/controller mismatches cause steady-state error when applying simulation-trained incremental policy actions directly to the robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Applying policy actions to an integrated desired-state (rather than current state) and tracking with a low-level PD/TSI controller reduces steady-state error; leaky PLAI (clamping windup) prevents instability when in sustained contact.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>PLAI reduces the need for exact dynamics matching between sim and real for steady-state accuracy, relaxing fidelity requirements for certain unmodeled disturbances (friction/gravity), but does not remove need for accurate contact geometry modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PLAI is a lightweight, deployment-only modification that substantially reduces steady-state positional error and variance caused by unmodeled real-world dynamics (friction and gravity mismatches), outperforming nominal action application and classical PID in both simulation and real-world tests, and thereby facilitating sim-to-real transfer without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Factory: Fast contact for robotic assembly <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>DeXtreme: Transfer of agile in-hand manipulation from simulation to reality <em>(Rating: 2)</em></li>
                <li>Transferring dexterous manipulation from GPU simulation to a remote real-world TriFinger <em>(Rating: 1)</em></li>
                <li>Safely learning visuotactile feedback policies in real for industrial insertion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1681",
    "paper_id": "paper-258947357",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "IndustReal",
            "name_full": "IndustReal (End-to-end RL-based Sim-to-Real Assembly System)",
            "brief_description": "An integrated set of algorithms, tools, and a real-world system that trains contact-rich assembly policies in high-fidelity GPU simulation and transfers them to a Franka Emika Panda robot with no real-world policy adaptation, demonstrating end-to-end detection, grasping, transport, and insertion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda (with wrist-mounted Intel RealSense D435)",
            "agent_system_description": "A 7-DOF collaborative robot arm (Franka Panda) equipped with a wrist-mounted RGB-D camera (RealSense D435) and general-purpose parallel-finger gripper; used to detect, grasp, transport, and insert industrial-style small parts (pegs, gears, connectors) on an optical breadboard.",
            "domain": "general robotics manipulation (contact-rich assembly)",
            "virtual_environment_name": "Isaac Gym + Factory (GPU contact simulator)",
            "virtual_environment_description": "GPU-accelerated physics simulation (Isaac Gym) augmented by the Factory contact module to simulate rigid-body dynamics with contact and friction between complex meshes, precomputed SDFs for contact/reward queries, and high-frequency physics stepping to enable faster-than-real-time RL training.",
            "simulation_fidelity_level": "high-fidelity contact-rich physics simulation (mesh-based contact, SDFs), optimized for speed on GPU",
            "fidelity_aspects_modeled": "detailed contact dynamics between complex meshes, signed distance fields (SDFs) for geometry alignment and contact queries, mesh interpenetration detection, rigid-body kinematics/dynamics, actuator-level low-level control emulation, observation noise and pose randomization",
            "fidelity_aspects_simplified": "no visuotactile sensing modeled (no tactile sensors), limited or no dynamics randomization (they intentionally avoided dynamics randomization), some heuristic dissipative terms were removed or tuned (the paper notes arbitrary heuristic damping is sometimes present and they removed it), and no real-world sensor drift/time-varying parameters were modeled explicitly",
            "real_environment_description": "Bench-top workcell with a Franka Panda arm, wrist-mounted Intel RealSense D435 RGB camera, optical breadboard with bolted sockets and loose plugs/parts, standard two-finger gripper (no force/torque sensors), camera calibration + Mask R-CNN-based perception pipeline for 2D pose estimation; experiments measured over hundreds of trials.",
            "task_or_skill_transferred": "End-to-end assembly behaviors decomposed into Pick, Place, and Insert phases: perception-driven grasping, reaching/placing to low targets in contact, and contact-rich insertion (peg-in-hole, gears onto shafts, electrical connector insertion).",
            "training_method": "On-policy reinforcement learning (Proximal Policy Optimization, PPO) trained in simulation using asymmetric actor-critic and GPU parallelism.",
            "transfer_success_metric": "Task success rate (full insertion), engagement rate (partial insertion), positional/rotational terminal error (mm, rad), and steady-state position error for Reach tasks (mm).",
            "transfer_performance_sim": "Reported simulation success rates for assembly Insert tasks ~82–92% depending on task and evaluation; joint reported simulated Insert (pegs) success ≈ 88.6% (engagement 96.6%), gears Insert ≈ 82.0% (engagement 85.2%); overall simulation benchmarks 82–99% across tasks.",
            "transfer_performance_real": "Real-world aggregated success/engagement rates over 600 trials: reported ranges 83–99% for different tasks; example Insert real-world: pegs engagement 86.7% / success 76.7%, gears engagement 95.0% / success 92.5%, connectors engagement 100% / success 85%; end-to-end PPI (Pick-Place-Insert) showed peg success 80%, gear success 97.5%, connector success 100%.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Significant randomization of plug and socket 6-DOF poses (XY ≈ ±10 mm, socket larger randomization up to ±10 cm in some evals), initial height ranges, observation noise added (e.g., target XY noise ±2 mm at test, training observation noise ±1 mm), randomization of yaw (±5°), and randomized trays/angles for pick experiments; explicit dynamics randomization (masses/friction/etc.) was intentionally avoided.",
            "sim_to_real_gap_factors": "Unmodeled nonlinear joint friction and imperfect gravity compensation on the real robot, sensor/perception noise and calibration offsets (camera extrinsic bias), mesh interpenetrations and numerical artifacts in simulation, heuristic damping terms in simulation that misrepresent real dissipation, object slip in the gripper and wedging/jamming phenomena, and control/latency differences.",
            "transfer_enabling_conditions": "High-fidelity contact simulation (Factory) with precomputed SDFs; simulation-aware policy updates (SAPU) to avoid learning from spurious interpenetrations; SDF-based dense reward for accurate geometric alignment; sampling-based curriculum (SBC) exposing breadth of initial states; careful observation and pose randomization; removal/tuning of heuristic damping in simulator; deployment-time Policy-Level Action Integrator (PLAI) to mitigate steady-state errors from unmodeled friction/gravity; robust vision pipeline and careful camera calibration.",
            "fidelity_requirements_identified": "Accurate contact dynamics and low interpenetration rates are critical (they used interpenetration thresholding and weighting); precise geometric alignment via SDF-based rewards was essential (reduced position error from ~12–31 mm to ~3.8 mm); physics frequency and control rates matter (trained at 120 Hz physics, 60 Hz control); realistic small clearances (0.5–0.6 mm) must be modeled; dynamics randomization is not necessary given strong simulator priors but omission requires high simulator fidelity.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "End-to-end assembly policies trained in a high-fidelity GPU contact simulator can transfer to a real robot without any real-world policy adaptation when (1) the simulator models contact-rich dynamics well, (2) training procedures explicitly mitigate simulation artifacts (SAPU to avoid interpenetration exploits), (3) dense geometric alignment rewards (SDF-based) and a sampling-based curriculum are used to learn robust insertion behaviors, (4) observation/pose randomization is applied (but not dynamics randomization), and (5) a simple deployment-time integrator (PLAI) reduces steady-state error from unmodeled friction/gravity—yielding repeatable real-world success rates across Pick/Place/Insert tasks comparable to simulation.",
            "uuid": "e1681.0",
            "source_info": {
                "paper_title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Factory (in Isaac Gym)",
            "name_full": "Factory: Fast contact for robotic assembly (Factory module integrated with NVIDIA Isaac Gym)",
            "brief_description": "A GPU-accelerated contact simulation module used within Isaac Gym that can handle many contacts between complex meshes and provides precomputed SDFs, enabling faster-than-real-time training of contact-rich manipulation policies.",
            "citation_title": "Factory: Fast contact for robotic assembly",
            "mention_or_use": "use",
            "agent_system_name": "simulation platform (Factory + Isaac Gym)",
            "agent_system_description": "A high-performance GPU-based physics simulator specialized for rigid-body contact between complex geometries, intended to enable large-scale RL for contact-rich tasks.",
            "domain": "simulation platform for robotics manipulation",
            "virtual_environment_name": "Isaac Gym + Factory",
            "virtual_environment_description": "Simulates rigid-body dynamics with frictional contact between detailed meshes, precomputes SDFs for objects (used both for contact and reward), supports GPU-parallelized rollouts at high physics frequencies, and integrates with RL frameworks.",
            "simulation_fidelity_level": "high-fidelity contact simulation optimized for speed (mesh-based collisions, SDFs, intersection/penetration handling)",
            "fidelity_aspects_modeled": "mesh-level contact, signed distance fields (SDFs) for geometry queries, frictional contact responses, many simultaneous contacts per rigid body pair, high-frequency stepping for realistic dynamics",
            "fidelity_aspects_simplified": "no tactile sensor simulation by default, some dissipative/frictional parameter approximations may be present unless explicitly identified/tuned, limited modeling of actuator-level torque/servo nonlinearities unless provided",
            "real_environment_description": "N/A (this is the simulator used to train policies that were deployed on a Franka Panda real robot)",
            "task_or_skill_transferred": "Contact-rich assembly tasks (peg-in-hole, gear insertion, connector insertion) trained in simulation for transfer to real-world robot.",
            "training_method": "On-policy RL (PPO) with large parallelized simulation rollouts.",
            "transfer_success_metric": "Simulation success rates, positional/rotational error, engagement/full-insertion rates used to predict and correlate with real-world performance.",
            "transfer_performance_sim": "Used to obtain high simulated success rates (e.g., Insert pegs ≈ 88.6% under their pipeline and ablations).",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Pose randomizations, observation noise added; dynamics randomization intentionally avoided in main pipeline but can be supported by the simulator.",
            "sim_to_real_gap_factors": "Numerical artifacts such as transient mesh interpenetrations and any heuristic dissipative terms if not carefully controlled; parameter mismatch between simulated friction/gravity and real hardware.",
            "transfer_enabling_conditions": "Precomputed SDFs (used for both contact generation and reward), high physics frequency, and mesh-quality handling (interpenetration checking) enabled learning policies usable in reality.",
            "fidelity_requirements_identified": "Accurate contact modeling and small-clearance geometry fidelity are essential for assembly transfer; the simulator must minimize transient interpenetrations and avoid unrealistic damping for accurate results.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "A high-performance GPU contact simulator that models mesh-level contact and exposes SDFs can support training of contact-rich RL policies that transfer to real robots, provided simulation artifacts (interpenetration, damping) are managed.",
            "uuid": "e1681.1",
            "source_info": {
                "paper_title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SAPU",
            "name_full": "Simulation-Aware Policy Update (SAPU)",
            "brief_description": "An algorithmic training-time technique that detects mesh interpenetrations per episode and either filters or weights episode returns during PPO updates to discourage policies that exploit simulation artifacts (spurious penetrations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda (policy training context)",
            "agent_system_description": "Policies for Pick/Place/Insert trained with PPO in Factory/Isaac Gym and deployed on Franka hardware; SAPU is a training technique applied in simulation to improve transfer.",
            "domain": "robotics manipulation (sim-to-real training)",
            "virtual_environment_name": "Isaac Gym + Factory (with a GPU-based interpenetration checker implemented using Warp)",
            "virtual_environment_description": "Simulates plug/socket meshes and computes per-episode maximum interpenetration depth by sampling mesh points and querying distances to target mesh.",
            "simulation_fidelity_level": "high-fidelity contact simulation with explicit interpenetration checking",
            "fidelity_aspects_modeled": "mesh interpenetration depth detection, contact geometry sampling, SDF/closest-point queries",
            "fidelity_aspects_simplified": "does not itself change other simulator dynamics (friction/viscous damping left as-is); relies on existing simulator fidelity",
            "real_environment_description": "Applied offline during simulation training to produce policies later deployed on Franka robot; no real-world element to SAPU itself.",
            "task_or_skill_transferred": "Improves robustness of Insert policies to avoid exploiting simulation interpenetrations so learned insertion behaviors transfer to the real robot.",
            "training_method": "PPO with per-episode returns either filtered out if interpenetration &gt; threshold, weighted by interpenetration depth, or both (Filter & Weight strategy).",
            "transfer_success_metric": "Simulation Insert success rate conditioned on episodes with low interpenetration (d_max_ip &lt; thresholds), and final real-world transfer success (indirectly improved).",
            "transfer_performance_sim": "Filter & Weight strategy produced strong simulation success rates (84.9–87.6% across evaluated scenarios) where baseline/filter-only/weight-only sometimes failed to exceed ~40% under certain interpenetration test conditions.",
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Spurious collisions/interpenetrations in simulation that policies can exploit and which do not exist in reality, leading to non-transferable behaviors.",
            "transfer_enabling_conditions": "Using GPU-based interpenetration checking to (a) filter bad episodes and (b) weight returns by a function of max interpenetration (1 - tanh(d/epsilon)), thereby biasing policy updates toward non-exploitative, physically-plausible behaviors.",
            "fidelity_requirements_identified": "Simulator must allow reliable detection of interpenetration (mesh sampling and query) and be run at sufficient fidelity/mesh quality so that interpenetration measurements are meaningful; controlling interpenetration during training is critical for learning transferable insertion behaviors.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Explicitly penalizing or filtering episodes with mesh interpenetration during training prevents policies from exploiting simulation artifacts and substantially improves learned insertion success in simulation (and indirectly supports transfer), with the combined Filter & Weight strategy performing best.",
            "uuid": "e1681.2",
            "source_info": {
                "paper_title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SDF-Reward",
            "name_full": "Signed Distance Field (SDF)-Based Dense Reward",
            "brief_description": "A dense reward defined by querying precomputed signed distance fields at sampled object surface points to create a continuous geometric-alignment signal that strongly improves position/rotation accuracy for assembling complex geometries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda (policy training context)",
            "agent_system_description": "Training-time reward shaping method used by PPO agents in Factory to learn precise geometric alignment for insertion tasks.",
            "domain": "robotics manipulation (sim-to-real reward design)",
            "virtual_environment_name": "Isaac Gym + Factory (SDF precomputation via pysdf and Factory's contact SDFs)",
            "virtual_environment_description": "Uses precomputed dense voxel SDFs for target object poses and queries SDF values at transformed sample points each timestep to compute an RMS SDF distance reward.",
            "simulation_fidelity_level": "high-fidelity geometric alignment via dense SDF queries",
            "fidelity_aspects_modeled": "high-resolution geometry alignment, continuous signed distance queries, dense point sampling on meshes, integrated with physics-based contact model",
            "fidelity_aspects_simplified": "SDFs are precomputed for nominal poses and queried via voxel grids (interpolation); dynamic deformation/tactile feedback not modeled",
            "real_environment_description": "SDF reward used only in simulation during training; no SDFs computed on real robot (real-world deployment uses learned policy and perception), though real geometry CAD models existed for simulation/real-world parts.",
            "task_or_skill_transferred": "Enables precise alignment and insertion behavior for non-axisymmetric and symmetric parts, reducing terminal positional and rotational error prior to transfer.",
            "training_method": "PPO with SDF-based dense reward replacing or augmenting keypoint/chamfer rewards.",
            "transfer_success_metric": "Simulation terminal position/rotation error, success (full insertion) and engagement (partial insertion) rates; downstream real-world transfer success improved when trained with SDF reward.",
            "transfer_performance_sim": "SDF Query Distance reward achieved terminal errors ≈ 3.80 mm position and 0.086 rad rotation with success rate 88.6% and engagement 96.6%; alternative rewards produced much larger errors (11.7–31.0 mm position, 0.13–0.99 rad rotation) and lower success.",
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "If geometry is not modeled precisely (mesh resolution / clearance modeling), SDF-based rewards will not reflect true contact alignment; real-world perception/pose estimation noise can degrade policy performance if not randomized during training.",
            "transfer_enabling_conditions": "Availability of accurate CAD meshes and precomputed SDFs (Factory already computes SDFs for contact), using SDF-based rewards for dense geometric alignment in insertion training.",
            "fidelity_requirements_identified": "High-quality geometric meshes and accurate SDF queries are essential; SDF-based reward substantially reduces positional and rotational error required for tight-clearance assemblies (≈0.5 mm clearance).",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "SDF-based dense rewards provide a far more informative alignment signal than collinear/6-DOF keypoints or chamfer distances for insertion tasks, dramatically reducing position/rotation error and increasing insertion success in simulation, which supports transfer to real hardware.",
            "uuid": "e1681.3",
            "source_info": {
                "paper_title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SBC",
            "name_full": "Sampling-Based Curriculum (SBC)",
            "brief_description": "A curriculum learning strategy that samples initial states over the full training distribution while progressively raising the lower bound of difficulty to avoid overfitting to partially-inserted states and improve learning for discontinuous contact tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda (policy training context)",
            "agent_system_description": "Curriculum strategy used during PPO training to generate varied initial heights/poses for plugs above sockets, ensuring exposure to full state distribution and gradual increase in difficulty.",
            "domain": "robotics manipulation (curriculum learning for sim-to-real)",
            "virtual_environment_name": "Isaac Gym + Factory",
            "virtual_environment_description": "During training, initial plug heights sampled from Uniform[z_low, z_high] where z_low is increased/decreased automatically based on recent mean success, allowing sampling across the entire range each stage.",
            "simulation_fidelity_level": "training-time curriculum applied in high-fidelity contact simulation",
            "fidelity_aspects_modeled": "variation in initial geometry poses/heights and resulting contact dynamics within simulator",
            "fidelity_aspects_simplified": "curriculum manipulates initial state distributions only; does not introduce new dynamics modeling",
            "real_environment_description": "No real-world curriculum; SBC used to produce policies that generalize to real-world initial condition variability.",
            "task_or_skill_transferred": "Improves robustness of insertion policies to varied initial heights/poses and prevents learning trivial partial-insertion solutions that do not transfer.",
            "training_method": "PPO combined with SBC that adaptively advances or reverts curriculum stage by adjusting lower height bound based on mean success rate (advance if &gt;80%, revert if &lt;10%).",
            "transfer_success_metric": "Simulation position/rotation error, engagement and full-insertion success rates; downstream real-world generalization improved by better simulated learning.",
            "transfer_performance_sim": "Sampling-Based Curriculum achieved Insert (pegs) success 88.6% with position error 3.80 mm, outperforming Baseline and Standard curricula (which had success &lt;=66.8% and position error ~10.7 mm).",
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Overfitting to narrow initial-state subsets (e.g., partially-inserted plugs) can produce brittle policies that fail under real-world initial pose variance.",
            "transfer_enabling_conditions": "Maintain sampling across full state range while progressively increasing lower bound of difficulty (z_low) based on success statistics, ensuring exposure to both easy and hard initial conditions from the start.",
            "fidelity_requirements_identified": "Curriculum must expose contact discontinuities (e.g., initial states above hole) to prevent overfitting; sampling across full distribution at each stage is crucial for discontinuous contact tasks.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Sampling-Based Curriculum that samples across the entire initial-state range while raising the lower bound of difficulty yields substantially better insertion learning (lower position error and higher success) than standard incremental curricula or no curriculum, improving the likelihood of real-world transfer.",
            "uuid": "e1681.4",
            "source_info": {
                "paper_title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PLAI",
            "name_full": "Policy-Level Action Integrator (PLAI)",
            "brief_description": "A simple deployment-time controller augmentation that accumulates (integrates) policy-generated incremental actions into a desired state that is tracked by a low-level impedance controller, reducing steady-state error due to unmodeled dynamics like friction and gravity mismatch.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda (deployment-time controller augmentation)",
            "agent_system_description": "At deployment, PLAI modifies how policy actions are applied: actions are composed on the last desired state (integrated) rather than on the instantaneous real state, producing a setpoint that the low-level TSI controller tracks, with optional leaky anti-windup.",
            "domain": "robotics control / sim-to-real deployment",
            "virtual_environment_name": "n/a (PLAI is a deployment-time method applied when running policies trained in simulation on real hardware; evaluated in sim & real)",
            "virtual_environment_description": "Evaluated in simulation (Reach policy under friction/gravity perturbations) and on the real robot to test rejection of unmodeled friction/gravity and steady-state error mitigation.",
            "simulation_fidelity_level": "used in conjunction with high-fidelity simulation for evaluation, but PLAI itself is a controller-level deployment technique",
            "fidelity_aspects_modeled": "PLAI is agnostic to simulator fidelity; evaluated against simulated joint friction and gravity perturbations (e.g., 0.15 Nm joint friction, gravity perturbation 0.12 m/s^2) and in real-world dynamics.",
            "fidelity_aspects_simplified": "PLAI does not add sensing or dynamics modeling; it modifies action composition logic and relies on the low-level impedance controller to track the integrated setpoint.",
            "real_environment_description": "Deployed on Franka Panda performing Reach tasks and full assembly tasks; showed lower steady-state position error and variance in real-world experiments compared to nominal action application or classical PID.",
            "task_or_skill_transferred": "Mitigates steady-state errors in reaching and insertion due to unmodeled friction/gravity, improving execution accuracy of policies trained in simulation when applied to real robot.",
            "training_method": "No retraining; PLAI is applied at deployment only to policies trained with PPO in simulation.",
            "transfer_success_metric": "Steady-state positional error (mm) for Reach tasks; downstream assembly success improvements in real-world deployments.",
            "transfer_performance_sim": "In simulation under friction/gravity perturbations, PLAI maintained ≈2 mm error and had substantially lower error/variance than Nominal and classical PID (example: steady-state error PLAI ~1.6–2 mm versus PID ~3.5 mm in some tests).",
            "transfer_performance_real": "On real robot Reach tests, PLAI produced ≈2 mm steady-state error whereas the Franka-provided TSI baseline (no RL) had 4.45 mm error; PLAI had lower variance than Nominal action application.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Unmodeled nonlinear friction, imperfect gravity compensation, and actuator/controller mismatches cause steady-state error when applying simulation-trained incremental policy actions directly to the robot.",
            "transfer_enabling_conditions": "Applying policy actions to an integrated desired-state (rather than current state) and tracking with a low-level PD/TSI controller reduces steady-state error; leaky PLAI (clamping windup) prevents instability when in sustained contact.",
            "fidelity_requirements_identified": "PLAI reduces the need for exact dynamics matching between sim and real for steady-state accuracy, relaxing fidelity requirements for certain unmodeled disturbances (friction/gravity), but does not remove need for accurate contact geometry modeling.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "PLAI is a lightweight, deployment-only modification that substantially reduces steady-state positional error and variance caused by unmodeled real-world dynamics (friction and gravity mismatches), outperforming nominal action application and classical PID in both simulation and real-world tests, and thereby facilitating sim-to-real transfer without retraining.",
            "uuid": "e1681.5",
            "source_info": {
                "paper_title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Factory: Fast contact for robotic assembly",
            "rating": 2,
            "sanitized_title": "factory_fast_contact_for_robotic_assembly"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "DeXtreme: Transfer of agile in-hand manipulation from simulation to reality",
            "rating": 2,
            "sanitized_title": "dextreme_transfer_of_agile_inhand_manipulation_from_simulation_to_reality"
        },
        {
            "paper_title": "Transferring dexterous manipulation from GPU simulation to a remote real-world TriFinger",
            "rating": 1,
            "sanitized_title": "transferring_dexterous_manipulation_from_gpu_simulation_to_a_remote_realworld_trifinger"
        },
        {
            "paper_title": "Safely learning visuotactile feedback policies in real for industrial insertion",
            "rating": 1,
            "sanitized_title": "safely_learning_visuotactile_feedback_policies_in_real_for_industrial_insertion"
        }
    ],
    "cost": 0.02085575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality</p>
<p>Bingjie Tang 
University of Southern California</p>
<p>Michael A Lin 
Stanford University</p>
<p>Iretiayo Akinola 
NVIDIA Corporation</p>
<p>Ankur Handa 
NVIDIA Corporation</p>
<p>Gaurav S Sukhatme 
University of Southern California</p>
<p>Fabio Ramos 
NVIDIA Corporation</p>
<p>University of Sydney</p>
<p>Dieter Fox 
NVIDIA Corporation</p>
<p>University of Washington</p>
<p>Yashraj Narang 
NVIDIA Corporation</p>
<p>Equal Contribution</p>
<p>IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality</p>
<p>Robotic assembly is a longstanding challenge, requiring contact-rich interaction and high precision and accuracy. Many applications also require adaptivity to diverse parts, poses, and environments, as well as low cycle times. In other areas of robotics, simulation is a powerful tool to develop algorithms, generate datasets, and train agents. However, simulation has had a more limited impact on assembly. We present IndustReal, a set of algorithms, systems, and tools that solve assembly tasks in simulation with reinforcement learning (RL) and successfully achieve policy transfer to the real world. Specifically, we propose 1) simulation-aware policy updates, 2) signed-distancefield rewards, and 3) sampling-based curricula for robotic RL agents. We use these algorithms to enable robots to solve contactrich pick, place, and insertion tasks in simulation. We then propose 4) a policy-level action integrator to minimize error at policy deployment time. We build and demonstrate a real-world robotic assembly system that uses the trained policies and action integrator to achieve repeatable performance in the real world. Finally, we present hardware and software tools that allow other researchers to reproduce our system and results. For videos and additional details, please see our project website.</p>
<p>I. INTRODUCTION</p>
<p>Robotic assembly is a longstanding challenge [70,26]. Assembly requires contact-rich interactions and high precision and accuracy; high-mix, low-volume settings also require adaptivity to diverse parts, poses, and environments. Today, robotic assembly is ubiquitous in the automotive, aerospace, and electronics industries. However, assembly robots are typically expensive, achieving precision primarily through hardware rather than intelligence. Moreover, systems often require meticulous engineering of adapters, fixtures, lighting, and robot trajectories. Such efforts demand substantial time and effort from robotics integrators and can result in solutions that are highly sensitive to perturbations of the robotic workcell.</p>
<p>Simulation is an indispensable means to solve engineering challenges. For example, simulators are used for finite element analysis, computational fluid dynamics, and integrated circuit design. Nevertheless, simulation has had a comparably limited impact on robotic assembly. Accurate simulation of geometrically-complex parts can require generating 1k-10k contacts per rigid body pair, followed by solving a nonlinear complementarity problem (NCP) at each contact. Furthermore, robotic assembly tasks require long-horizon sequential decision-making; powerful data-driven methods (e.g., onpolicy RL) have high sample complexity, requiring faster-than- realtime simulation. Achieving such accuracy and performance requirements has only recently become possible [41,48,29].</p>
<p>Given modeling limitations and finite compute, simulation will always differ from reality; this reality gap has been notoriously large for robotics. Sim-to-real transfer methods address this gap through techniques such as system identification and domain randomization. These methods have shown remarkable results in locomotion [30,51,44] and manipulation [5,18,9]. However, sim-to-real efforts for assembly have been scarce.</p>
<p>We present IndustReal, a set of algorithms, systems, and tools for solving contact-rich assembly tasks in simulation and transferring behaviors to reality ( Figure 1). Specifically, our primary contributions are the following:</p>
<p>• Algorithms: For simulation, we propose three methods to allow RL agents to solve contact-rich tasks in a simulator: a simulation-aware policy update (SAPU) to reward the agent when simulation predictions are more reliable, a signed distance field (SDF)-based dense reward to provide an alignment metric between geometrically-complex objects, and a sampling-based curriculum (SBC) to prevent overfitting to earlier stages of a curriculum. For sim-to-real transfer, we also propose a policy-level action integrator (PLAI), which reduces steady-state error in the presence of unmodeled dynamics (e.g., friction). • Benchmarks: We solve several challenging tasks proposed in Factory [48] (pick, place, and insertion tasks for pegs-and-holes and gear assemblies in simulation) with success rates of 82-99%. We provide careful evaluations over 265k simulated trials to show the utility of SAPU, SDF-based rewards, and SBC for solving these tasks. • Systems: We design and demonstrate a real-world system that can perform sim-to-real transfer of our simulationtrained policies, with success rates of 83-99% over 600 trials. We provide careful evaluations to show the utility of PLAI. To our knowledge, this is the first system for sim-to-real of all phases of the assembly problem: from detection, to grasping, to part alignment, to insertion. Our system uses commonly-used robotics hardware and requires no real-world policy adaptation phase. Our secondary contributions are the following:</p>
<p>• Hardware: We present IndustRealKit, which contains CAD models for all parts designed for our setup, as well as a list of all purchased parts. The CAD models can all be printed on a desktop 3D printer. IndustRealKit allows the research community to easily replicate our experimental hardware and benchmark their performance. • Software: We present IndustRealLib, a lightweight Python library that allows users to easily deploy policies trained in NVIDIA Isaac Gym [43] onto a real-world Franka Emika Panda robot [17]. The library also contains code to assist with policy training. IndustRealLib allows the research community to reproduce our robot behaviors. We aim for IndustReal to provide algorithms, benchmark results, and a reproducible system that serve as a path forward for sim-to-real transfer on contact-rich assembly tasks.</p>
<p>II. RELATED WORK</p>
<p>We divide prior work on robotic assembly into three categories: 1) classical approaches leveraging analytical methods [70,45], 2) learning-based approaches leveraging realworld data or experience [75], and 3) RL-based sim-to-real approaches leveraging robotics simulators. We defer a review of (1) and (2) to Appendix A and focus on (3).</p>
<p>A. Sim-to-Real Transfer for Assembly</p>
<p>Over the past few years, there have been a number of impressive efforts in sim-to-real for assembly. These efforts have primarily used MuJoCo [52,11,20,67,79,27] or PyBullet [38,56,54]; have used PPO [56,58,20,27] or DDPG [38,6]; and have aimed to solve peg-in-hole [38,7,54,52,58,11,67,79,27] or NIST-style tasks [7,52,11,79]. However, several of these studies use large clearances (e.g., ≥ 1 mm) and/or large parts in simulation and/or the real world. Furthermore, almost all use force/torque (F/T) sensors to collect observations and/or set thresholds. Most require human demonstrations [38,6,11], a baseline motion plan [56,58,27], and/or fine-tuning in the real-world [7,52]. Finally, all but one [52] focus only on insertion and assume the object is pre-grasped; however, [52] also uses specialized grippers, low-dimensional action spaces, highly-constrained target locations, and a real-world policy adaptation phase.</p>
<p>In contrast, we make several design choices that increase the realism of the problem and encourage reproducibility. First, for software, we use Factory [48] within Isaac Gym, which can solve contact dynamics between highly-complex geometries without simplification. Second, for hardware, we use a collaborative robot (Franka Panda) and RGB-D camera (Intel RealSense D435) that are widespread in research, but far less precise than those in industrial assembly. We use no task-specific grippers and avoid F/T sensors due to their cost, noise, and fragility. We also use realistic part clearances (≤0.5-0.6 mm, aligned with the upper bound of ISO 286). Next, for problem scope, we address sim-to-real for all parts of the assembly sequence (i.e., detection, grasping, alignment, and insertion). We face robustness challenges due to calibration and localization error; moreover, we apply large randomizations of part poses and targets. Finally, in methodology, we achieve sim-to-real transfer without baseline plans or demos, dynamics randomization, or real-world policy adaptation phases.</p>
<p>III. PROBLEM DESCRIPTION</p>
<p>A. Problem Setup</p>
<p>Our problem setup is as follows: a Franka robot is mounted to a work surface. A RealSense D435 camera is mounted to the robot wrist. Industrial-style parts inspired by the NIST Task Board 1 [25,26] are placed upright on the work surface. The parts with extruded features (which we henceforth refer to as plugs) have a randomized 3-DOF pose (x, y, θ) on top of an optical breadboard and are free to move; the parts with mating features (which we call sockets) also have a randomized pose, but are bolted to the breadboard to emulate industrial fixturing. 1 The fundamental task is to perceive, grasp, transport, and insert all the plugs into their corresponding sockets.</p>
<p>Specifically, we aim to perform this task for three types of assemblies from [48] (Figure 2 column 1):</p>
<p>• Pegs and Holes: 2 different classes of pegs (round and rectangular), each with 3 different sizes (max dimension: 8 mm, 12 mm, and 16 mm) must be inserted into corresponding holes (clearances: 0.5-0.6 mm). • Gears and Gearshafts: 3 different gears (diameters: 20 mm, 40 mm, 60 mm) must be inserted onto corresponding gearshafts (diametral clearances: 0.5 mm). • Connectors and Receptacles: 2 different connectors (2prong NEMA 1-15P and 3-prong NEMA 5-15P) must be inserted into corresponding receptacles. We first aim to solve the assembly task in simulation using RL with CAD models of the robot and the objects. We then aim to transfer the policies to the real world.</p>
<p>B. Problem Decomposition</p>
<p>For each category of parts, to facilitate the assembly task, we decompose it into three phases (Figure 2  • Pick: The robot grasps a randomly-positioned plug (i.e., peg, gear, or connector) within its workspace. • Place: The robot transports the grasped plug close to its corresponding socket (i.e., hole, gearshaft, or receptacle). • Insert: The robot brings the grasped plug into contact with its socket and inserts the plug, aligning parts where necessary (e.g., when inserting intermediate gears).</p>
<p>IV. POLICY LEARNING IN SIMULATION</p>
<p>In this section, we first describe our general strategies for policy learning in Sections IV-A-IV-D. Although these strategies were sufficient to train successful Pick and Place policies, they were inadequate for training Insert policies (≈ 12% success rates), motivating our algorithmic development. We describe our 3 simulation-based algorithms and evaluate them on our Insert policies in Sections IV-E-IV-G 2 .</p>
<p>A. Training Environments</p>
<p>We developed our code within the Factory simulation framework [48]. For the Peg and Hole assemblies, we first trained a Reach policy, where the robot learned to move its endeffector to a randomized pose within a large workspace. We then fine-tuned Reach to solve the Pick task by including all peg assets in the scene and redefining success as lifting the pegs. Similarly, we fine-tuned Reach to solve the Place task by initializing the pegs within the robot grippers, including all hole assets in the scene, and redefining success as bringing the pegs to their corresponding holes. Empirically, for the Pick and Place tasks, training in free space and fine-tuning on contact was more efficient than training from scratch. However, we trained an Insert policy from scratch.</p>
<p>For the Gears and Gearshafts assemblies, we did not train policies to solve the Pick or Place tasks; as we later show, we solved those tasks in the real world by executing the corresponding Peg and Hole policies, demonstrating generalization. However, we again trained an Insert policy from scratch. 2 Note that when we evaluate each algorithm, the other two algorithms are used; thus, the evaluations function as ablation studies.</p>
<p>Finally, for the Connectors and Receptacles assemblies, we did not train policies for any phase. Again, we later show that we solved those tasks in the real world via policy transfer.</p>
<p>In summary, for the Peg and Hole assemblies, we trained Pick, Place, and Insert policies, and for the Gears and Gearshafts assemblies, we trained another Insert policy.</p>
<p>B. Formulation</p>
<p>We formulated the problem as a Markov decision process (MDP) with state space S, observation space O, action space A, state transition dynamics T : S × A → S, initial state distribution ρ 0 , reward function r : S → R, horizon length T , and discount factor γ ∈ (0, 1]. The objective was to learn a policy π : O → P(A) that maximized the expected sum of discounted rewards E π [Σ T −1 t=0 γ t r(s t )]. We used proximal policy optimization (PPO) [53] to learn a stochastic policy a ∼ π θ (o) (actor), mapping from observations o ∈ O to actions a ∈ A and parameterized by a network with weights θ; as well as an approximation of the on-policy value function v = V ϕ (s) (critic), mapping from states s ∈ S to value v ∈ V and parameterized by weights ϕ. We used the PPO implementation from rl-games [42]; hyperparameters and architectures are in Table XI. Finally, we aimed to train the policy in simulation and deploy in the real world with no policy adaptation phase on the specific real environment.</p>
<p>C. Observations, Actions, and Rewards</p>
<p>Our observation spaces in simulation and the real world were task-dependent. The observations provided to the actor consisted exclusively of joint angles, gripper/object poses, and/or target poses, as the Franka's joint velocities and joint torques exhibited appreciable noise in the real world. However, we employed asymmetric actor-critic [50], where velocity information was still used to train the critic. Our exact observations for all policies are listed in Table V. Our action spaces for both simulation and the real world were task-independent. The actions consisted of incremental pose targets to a task-space impedance (TSI) controller (specifically, a = [∆x; ∆q], where ∆x is a position error and ∆q is a quaternion error). We learned incremental targets rather than absolute targets because the latter encodes task-specific biases and must be selected from a large spatial range. We used TSI rather than operational-space control (OSC) because Franka provides a high-performance implementation of TSI, and OSC relies on an accurate dynamics model.</p>
<p>Our rewards in simulation were task-dependent. However, all rewards could be expressed in the following general form:
G = w h0 ..w hm H−1 t=0 [w d0 R d0 (t) + ... + w dn R dn (t)] + w s0 R s0 + ... + w sp R sp(1)
where G is the return over the horizon, R d0 ...R dn are distinct dense rewards, H is the horizon length, R s0 ...R sp are terminal success bonuses, w d0 ...w dn and w s0 ...w sp are scaling factors that map distinct rewards into a consistent unit system and weight the importance of each term, and w h0 ...w hm are scaling factors on the return over the entire horizon. Not all terms are used in each phase, and most of our reward formulations are simple. Detailed formulations and success criteria are provided in Table VI and Table IV, respectively.</p>
<p>D. Randomization and Noise</p>
<p>At the start of each episode, we randomized the 6-DOF end-effector and object poses over a large spatial range. In addition, for the Insert policies, we introduced observation noise. As well established, these perturbations are critical for ensuring robustness to initial conditions and sensor noise in the real world. Randomization and noise ranges are provided in Table VII). However, we avoided dynamics randomization [49,18], as we had strong priors on our system dynamics.</p>
<p>E. Simulation-Aware Policy Update (SAPU)</p>
<p>Method: In contact-rich simulators, spurious interpenetrations between assets are unavoidable, especially when executing in real-time ( Figure S15). Unfortunately, in simulation for RL, an agent can exploit inaccurate collision dynamics to maximize reward, learning policies that are unlikely to transfer to the real world [47]. Thus, we propose our first algorithm, a simulation-aware policy update (SAPU), where the agent is encouraged to learn policies that avoid interpenetrations.</p>
<p>Specifically, we implemented a GPU-based interpenetrationchecking module using warp [40]. For a given environment, the module takes as input the plug and socket mesh and associated 6-DOF poses. The module samples N = 1000 points on/inside the mesh of the plug, transforms the points to the socket frame, computes distances to the socket mesh, and returns the max interpenetration depth (algorithm 1). This procedure is performed each episode, and the depth is used to weight the cumulative reward during the policy update.</p>
<p>Evaluation: We evaluate SAPU on the Peg and Hole assembly Insert policy, with the following test cases: . After training policies with each strategy, we tested each policy in simulation over 5 seeds, with 1000 trials per seed; quantified d max ip for each episode; and evaluated success rate over all episodes ( Figure 3). Success was defined as inserting the peg into the hole (Table IV); however, to also ascertain whether success was achieved in the desired way (i.e., by avoiding interpenetration), we computed success rate for episodes where d max  scenario (when all successes were counted), the Baseline, Filter Only, and Weight Only strategies were unable to achieve success rates above 40%. However, the Filter and Weight strategy performed well over all scenarios, with success rates of 84.9-87.6%. Thus, Filter and Weight was not only most effective at learning a policy that avoided interpenetration, but was also most effective at policy learning in general.</p>
<p>Algorithm 1: Interpenetration Checking Per Env.</p>
<p>Input: plug mesh m p , socket mesh m s , plug pose p p , socket pose p s , number of query points N .
1 sample N points in m p → v, v = {v 0 , ..., v N −1 }; 2 transform v to current m p pose p p in m s frame ; 3 for every vertex v ∈ v do 4 compute closest point on m s to v; 5 if v inside m s then 6 calculate interpenetration distance; 7 d max ip = max interpenetration from all v ∈ v to m s ; 8 return d max ip ;
F. SDF-Based Dense Reward Method: Keypoint-based rewards are widely used, as they avoid weighting between distinct position and orientation rewards [3]. However, collinear keypoints (e.g., [48]) underspecify the assembly of non-axisymmetric parts (e.g., rectangular pegs), and non-collinear keypoints overspecify the assembly of symmetric parts, as identical geometries do not alias. Thus, we propose a signed distance field (SDF)-based dense reward, where an SDF is defined as a map ϕ(x) : R 3 → R from an arbitrary point x to its signed distance ϕ(x) to a surface.</p>
<p>Specifically, for each plug mesh in its nominal pose, we use sampling to preselect N = 1000 points on the surface. In addition, for the same mesh in its target pose, we use pysdf to precompute and store the SDF values at each cell of a dense voxel grid containing the mesh. During training, the preselected points are transformed to the frame of the plug mesh in its current pose, and the SDF values are queried at these points. This procedure is performed at each timestep in each environment and is used to generate a reward signal.</p>
<p>Evaluation: We evaluate SDF-Based Dense Reward on the Pegs and Holes assembly Insert policy by comparing the following reward formulations (Table I):</p>
<p>• Collinear Keypoints: Each object has 4 keypoints along its Z-axis; Euclidean distances are summed and averaged. • 6-DOF Keypoints: Each object has 13 keypoints over 3 axes; Euclidean distances are summed and averaged. • Chamfer Distance: Each object has a point cloud defined by its mesh vertices; chamfer distance [13] is computed. • SDF Query Distance: The root-mean-square SDF distance is computed as described earlier.</p>
<p>After training policies with each strategy, we tested each policy in simulation over 5 seeds, with 1000 trials per seed; quantified terminal position and rotation error for each episode; and quantified success rate and engagement rate (Table I). Success was defined as inserting the peg into the hole; engagement was defined as a partial insertion.</p>
<p>The Collinear Keypoints, 6-DOF Keypoints, and Chamfer Distance rewards resulted in appreciable position and rotation errors of 11.7-31.0 mm and 0.13-0.99 rad, respectively, with chamfer distance performing the worst; as follows, success rates varied between 1.8-54.2%. However, the SDF Query Distance reward resulted in errors of just 3.80 mm and 0.086 rad, with a success rate of 88.6% and near-perfect engagement rate of 96.6%. Thus, SDF Query Distance was by far the most effective reward formulation for policy learning. As Factory [48] already precomputes SDFs for all objects for contact generation, we envision a single representationgeneration step for both physics and reward.</p>
<p>G. Sampling-Based Curriculum (SBC)</p>
<p>Method: Curriculum learning [8] is an established approach for solving long-horizon problems; as the agent learns, the difficulty of the task is gradually increased. Nevertheless, for both the Peg and Hole and Gears and Gearshafts assemblies, for the Insert phase, naive implementations of curriculum learning (i.e., increasing initial distance from goal) were ineffective; when the initial peg/gear state was above the hole/gearshafts, the agent failed to progress, likely overfitting to a partially-inserted plug. Thus, we developed Sampling-Based Curriculum (SBC), whereby the agent is exposed to the entire range of initial state distributions from the start of the curriculum, but the lower bound is increased at each stage.</p>
<p>Specifically, let z low denote the lower bound of the initial height of a plug above its socket at a given curriculum stage, and let z high denote a constant upper bound; the initial height of the plug is uniformly sampled from Uniform[z low , z high ]. In addition, let ∆z i and ∆z d denote an increase or decrease in z low , and let p n denote the mean success rate over all environments during episode n. When episode n terminates, we update z low as follows:
z low ←    z low + ∆z i , p n &gt; 80% z low − ∆z d , p n &lt; 10% z low , otherwise.
In general, we enforce ∆z d &lt; ∆z i . We define an increase in z low as an advance to the next stage of the curriculum, and a decrease in z low as a reversion to the previous stage.</p>
<p>Evaluation: We evaluate SBC on the Pegs and Holes assembly Insert phase, with the following test cases:</p>
<p>• Baseline: No curriculum learning is used; z init = z high . • Standard: Peg height is initialized at z low ; at each stage, z low increases, until a max value of z high . • Sampling-Based: Initial peg height is sampled as described earlier.</p>
<p>For the Standard and Sampling-Based strategies, z low was initially 10 mm below the top of the hole, and z high remained constant at 10 mm above. The criterion for advancing to the next stage was an 80% success rate. Engagement was defined as partially inserting the peg into the hole; success was defined as full insertion. We set ∆z i = 5mm and ∆z d = 3mm. Whenever the peg was initialized above the hole, we also perturbed its position along the X-and Y-axes.</p>
<p>After training policies with each strategy, we tested each policy in simulation over 5 seeds, with 1000 trials per seed (Table II). All strategies achieved moderate rotation errors of 0.086-0.096 rad, and the Baseline and Sampling-Based strategies both achieved high engagement rates of 89.2-96.6%. However, Sampling-Based substantially outperformed the others in success rate and position error; it achieved 88.6% and 3.80 mm, respectively, whereas the others performed no better than 66.8% and 10.7 mm. These results substantiate existing evidence that curriculums can facilitate RL when carefully implemented, and importantly, suggest a specific implementation in the case of discontinuous contact.</p>
<p>H. Joint Evaluation</p>
<p>As described in Sections IV-E-IV-G, we proposed three algorithms for improving learning of contact-rich Insert policies: Simulation-Aware Policy Update to adapt to simulator inaccuracy, SDF-Based Dense Reward to quantify alignment for asymmetric or symmetric objects, and Sampling-Based Curriculum to prevent overfitting to initial partial insertions. As a final evaluation, we comprehensively evaluated all three techniques in tandem ( Figure 4 and Table X). When training and testing with moderate state randomization (plug/hole randomization of ±10 mm/±10 cm, respectively) and observation noise (±1 mm), the Pegs and Holes assembly Insert policy achieved success and engagement rates of 88.6% and 96.6%, respectively, whereas the Gears and Gearshafts assembly Insert policy achieved 82.0% and 85.2%. Across all evaluations, worst-case performance was 67.88% (when testing with twice the gearshaft position randomization of training), and best-case was 92.4% (testing with no randomization or noise).   Engage denotes partial insertion. Policies were trained with moderate randomization (plug/socket randomization of ± 10 mm and 10 cm, respectively, and observation noise of ± 1 mm); thus, plots evaluate in-distribution and OOD performance.  </p>
<p>V. POLICY DEPLOYMENT IN REAL WORLD</p>
<p>In this section, we first describe our general strategies for policy deployment in the real world in Sections V-A-V-C. We then describe and evaluate our deployment-time algorithm, the Policy-Level Action Integrator (PLAI), in Section V-D. Finally, we provide comprehensive evaluations and demonstrations of our full real-world system in Section VI.</p>
<p>A. Communications Framework</p>
<p>We illustrate our communications stack in Figure S8. In summary, we developed the IndustRealLib library, which accepts trained policy checkpoints from Isaac Gym as input, and outputs targets for a Franka robot controlled via a taskspace impedance (TSI) controller. The targets are sent to the frankapy Python library, which streams the commands via ROS to the franka-interface C++ library [78]. franka-interface then sends the commands to the libfranka library provided by Franka, which maps the TSI commands to low-level joint torques that are streamed to the robot. Proprioceptive signals are then communicated to IndustRealLib in reverse order.</p>
<p>Latencies of our real-world and simulated systems are summarized in Table VIII. In the real world, physics frequency is approx. infinite, low-level control frequencies (between libfranka and robot) are 1 kHz, and policy control frequencies (between libfranka and IndustRealLib) are 60-100 Hz (limited by inference and ROS). We thus set our physics frequency during training to the highest practical rate (120 Hz) given our compute, and restricted our control rate during training to 60 Hz to prevent aliasing of policy signals during deployment.</p>
<p>B. Perception Pipeline</p>
<p>The primary goal of our perception pipeline is to estimate the 2D poses (x, y, θ) of the parts in the robot frame. Our pipeline consists of 3 separate steps: a one-time camera calibration, a per-experiment workspace mapping, and a perexperiment object detection from a single RGB image. Bounding box centroids and a trivially-specified height are used to construct targets. All details are provided in Appendix B2-B5.</p>
<p>C. Dynamics Strategy</p>
<p>Our policies initially exhibited substantial steady-state error during deployment. We note that a partial resolution was carefully eliminating all arbitrary energy dissipation (e.g., heuristically-applied friction and damping) in the simulator and asset descriptions; dissipative terms are often introduced to facilitate simulation stability, but their values are typically chosen without physical consideration. Simulation and realworld results with/without heuristic damping are in Figure S9.</p>
<p>D. Policy-Level Action Integrator</p>
<p>Method: Robotics simulations can exhibit marked discrepancies with the real world due to incomplete models, inaccurate parameters, and numerical artifacts [14]; although dynamics randomization can improve sim-to-real transfer, it can require substantial training time and effort [2,5,18] and may penalize precision. Inspired by classical PID control, which can minimize steady-state error and reject disturbances on linear systems, we propose a Policy-Level Action Integrator (PLAI), which integrates policy actions during an episode.</p>
<p>An established approach for applying policy actions is
s d t+1 = s t ⊕ a t = s t ⊕ Π(o t ),(2)
where s d t+1 is the desired state, a t is an action expressed as an incremental state target, s t is the current state, o t is the current observation, Π is the policy, and ⊕ computes the state update (e.g., for states defined by position and orientation, ⊕ computes composition with a translation and rotation).</p>
<p>In contrast, PLAI applies policy actions as
s d t+1 = s d t ⊕ a t = s d t ⊕ Π(o t )(3)
Thus, the policy action is applied to the last desired state instead of the current state. Unrolling from t = 0...T ,
s d T = s d 0 ⊕ T −1 i=0 a i = s d 0 ⊕ T −1 i=0 Π(o i ).(4)
where s d 0 is set to s 0 3 . Thus, the desired state at time T is equal to the initial state composed with successive actions over time, effectively integrating them. (Note that this formulation is not open-loop control, as the policy continues to be evaluated on the current observation o t when generating actions.) When coupled with a low-level PD controller (e.g., a TSI controller), PLAI has a close relationship with a standard (nonintegrating) policy coupled with a PID controller; a derivation is provided in Appendix C. Empirically, PLAI requires minimal implementation effort (1-2 lines of code), is simple to tune, and outperforms standard PID in our application.</p>
<p>Like PID controllers, PLAI can experience windup when in contact with the environment; unbounded error accumulates, resulting in unstable dynamics. To mitigate this effect, we also develop Leaky PLAI, which clamps the accumulated control effort. Equations are derived in Appendix C.</p>
<p>Evaluation: From careful observations, discrepancies in simulated and real-world dynamics are primarily caused by nonlinear friction and imperfect gravity compensation on the real robot. Thus, we trained a Reach policy under ideal 3 The summation symbol in Equation 4 is used as shorthand for successive compositions with actions. conditions in simulation, and evaluated the ability of PLAI to reject friction and gravity disturbances in simulation and reality at test time. We evaluated three test-time methods:</p>
<p>• Nominal: Actions are applied to the current state.</p>
<p>• PID: Actions are applied to the current state. PID is used with classical anti-windup for best-case performance. • PLAI: Actions are applied to the desired state. We evaluated each method under three test-time conditions:</p>
<p>• Ideal: No friction or gravity perturbations are applied. • Friction: Joint friction of 0.15 N m is applied to all robot joints (within the range of identified values from [16]). • Gravity: A gravitational perturbation of 0.12 m/s 2 is applied to all robot links. We randomized the initial pose and target pose of the robot and measured steady-state position error ( Figure 5). Notably, PLAI had substantially lower error and variance than Nominal under friction and gravity, had consistently lower error than PID, and maintained ≈ 2 mm error across all test conditions. Next, we conducted a similar experiment in the real world. The Reach policy was deployed with and without PLAI ( Figure 6); friction and gravity perturbations were simply from real-world dynamics. Again, PLAI demonstrated substantially lower error and variance than Nominal, with ≈ 2 mm error. As a final comparison, the TSI implementation provided by Franka (RL-free) was deployed on the same task and resulted in 4.45 mm error. Thus, PLAI is a simple but highly effective means to minimize error with respect to policy targets; moreover, it can be applied exclusively at deployment time.</p>
<p>VI. REAL-WORLD EXPERIMENTS</p>
<p>After developing and validating our algorithms, we performed comprehensive experiments and demos to evaluate our real-world system ( Figure S10). Five types were executed: Pick, Place, Sort, Insert, and Pick-Place-Insert (PPI).</p>
<p>A. Pick Experiment</p>
<p>This experiment evaluated the ability of the real-world system to initiate contact and pick up arbitrarily-placed objects.  Table IX. Experimental Setup: 6 different pegs were randomly placed on top of an optical breadboard with dimensions 450 mm x 300 mm, which itself was located within bounds of 500 mm x 350 mm. The pegs were located in trays that were free to slide; angular perturbations of ±10 deg were applied to the trays containing non-axisymmetric pegs. The goal was for the robot to detect all the pegs and use the simulation-trained Pick policy to pick up the objects before releasing them.</p>
<p>Key Results: The system demonstrated extremely high success rates (98.8%) across all pegs (Table III). Failure cases were one missed detection of a peg, as well as one grasp of both a peg and its corresponding peg tray.</p>
<p>B. Place Experiment</p>
<p>This experiment evaluated the ability of the real-world system to accurately reach low target locations while maintaining contact with a typically-sized object in the gripper.</p>
<p>Experimental Setup: Eight 25 mm x 25 mm trays were randomly placed on top of the breadboard. 20 mm x 20 mm printed targets were centered on top of the trays; the targets were used to measure positional accuracy and consisted of concentric rings, each with a thickness of 2 mm. A laser was rigidly mounted to the grippers ( Figure S12). The goal was for the robot to detect the trays and use the simulation-trained Place policy to guide the laser to the centers of the targets.</p>
<p>Key Results: The system demonstrated low steady-state errors, with a mean distance-to-goal of 4.23 ± 1.96 mm. The error distribution is illustrated in Figure S9b.</p>
<p>C. Sort Demonstration</p>
<p>This experiment qualitatively demonstrated the ability of the robot to execute a realistic sorting procedure.</p>
<p>Experimental Setup: 6 different pegs and 3 different gears were randomly placed on top of the breadboard. The pegs were located in trays, and angular perturbations of ±45 deg were applied. Bins were placed at approximately-determined positions in the workspace. The goal was for the robot to use its Pick and Place policies to detect, pick, place, and drop the round pegs, rectangular pegs, and gears into separate bins.</p>
<p>Key Results: Performance was highly repeatable in practice; please see the supplementary video.</p>
<p>D. Insert Experiment</p>
<p>This experiment evaluated the ability of the real-world system to insert diverse plugs into corresponding sockets, as well as generalize to unseen assets.</p>
<p>Experimental Setup: 6 different pegs, 3 different gears, and 2 different unseen NEMA connectors (2-and 3-prong) were placed imprecisely in the gripper fingers. Holes, gearshafts, and receptacles were mounted to the breadboard. The endeffector was manually guided until the plugs were inserted into their respective sockets; the end-effector pose was recorded as a target. The end-effector was then commanded to a random initial state (Table XII). The robot received an observation of the target with random Xand Y -axis noise ∼ U [−2, 2] mm. 4 The goal was for the robot to use its Insert policies to insert the plugs into their corresponding sockets.</p>
<p>Key Results: The system demonstrated high engagement rates (i.e., partial insertions) across the pegs (86.7%), gears (95.0%), and connectors (100%), as well as moderately-high success rates (i.e., full insertions) across the same objects (76.7%, 92.5%, and 85%). The Pegs and Holes assembly Insert policy also successfully generalized to NEMA connectors, which can be considered extensions of peg-in-hole.</p>
<p>Engagement failures were almost exclusively due to slip between the gripper and object; we hypothesize that a highforce gripper (e.g., Robotiq) would fully resolve this issue. Full-insertion failures were almost exclusively due to the wedging phenomenon, a longstanding topic of research [71].</p>
<p>Informally, when the robot was intentionally perturbed by a human during the Gears and Gearshafts assembly Insert policy, the policy exhibited recovery behavior. In addition, the policy exhibited search behavior on the surface of the socket tray, exploring the vicinity of the observed goal (Figure 7).</p>
<p>E. Pick, Place, and Insert (PPI) Demonstration</p>
<p>This experiment demonstrated the ability of the robot to execute end-to-end assembly; the Insert policies required robustness to error accumulated from Pick and Place.</p>
<p>Experimental Setup: 6 pegs, 3 gears, 2 NEMA connectors, and their corresponding sockets were initialized with the same conditions as the Pick experiment. The goal was for the robot to bring all parts into their assembled configurations.</p>
<p>Key Results: The system demonstrated even higher success rates than during the Insert experiment: 80% and 88.3% success/engagement rates for peg insertion, 97.5% and 100% success/engagement rates for gears, and 100% success/engagement rates for connectors. The higher success rates suggest that the randomization and noise ranges during the Insert experiment may have been particularly adverse.</p>
<p>To our knowledge, IndustReal is the first system to demonstrate RL-based sim-to-real transfer for the end-to-end assembly task (i.e., detection, grasping, part transport, and insertion) without any policy adaptation phase in the real world.   </p>
<p>VII. INDUSTREALKIT AND INDUSTREALLIB</p>
<p>We strongly encourage fellow researchers to reproduce our results and use our platform to investigate their own questions. As follows, we will open-source our two most critical pieces of hardware and software, IndustRealKit and IndustRealLib.</p>
<p>IndustRealKit contains 3D-printable CAD models for all parts we designed, as well as a list of all parts we purchased from external vendors ( Figure S11). The CAD models include 20 parts: 6 peg holders, 6 peg sockets (i.e., extruded holes), 3 gears, 1 gear base (with gearshafts), and 4 NEMA connectors and receptacle holders. The purchasing list includes 17 parts: 6 metal pegs (from the NIST benchmark), 4 NEMA connectors and receptacles, 1 optical breadboard, and fasteners.</p>
<p>IndustRealLib is a lightweight library containing code for policy deployment and training. Specifically, we provide scripts to allow users to quickly deploy policies from Isaac Gym [43] onto a Franka robot. The scripts include a base class that implements our policy-level controllers and sends/receives actions and observations from FrankaPy; task-specific classes that interpret actions from the policy, compute observations, and set targets; and a script that instantiates the classes, loads corresponding policies, and executes them. For the Peg and Hole assemblies, we also provide weights for the Reach, Pick, Place, and Insert policies. We have thus far used IndustRealLib on two different Franka robots in two different cities. Finally, we provide code for training our RL policies, including implementations of SAPU, SDF-Based Reward, and SBC. This code also includes a carefully-reviewed Franka model and simulation parameters validated during this work.</p>
<p>VIII. LIMITATIONS &amp; FUTURE WORK</p>
<p>Our work has limitations, which lend themselves naturally to future research directions. First, like other efforts on sim-toreal transfer for assembly tasks, we have primarily investigated tasks inspired by the NIST benchmark [25]. However, recent work in graphics and robotics has provided a large number of simulation-compatible assets for assembly (e.g., [62]), potentially enabling RL policies that can generalize across widely different categories. Second, our primary failure cases on the real system were due to slip of the object in the gripper and wedging of plugs in their corresponding sockets. We believe that these cases can be resolved by providing the agent with simulated visuotactile readings during training [55,74,68] and using corresponding sensors on the real-world system [77,28], as well as more accurately simulating friction [4]. Third, we do not explore passive mechanical compliance as a means for facilitating policy learning [46]; we believe that optimizing the policy, controller, and passive dynamics simultaneously can significantly help improve task performance. Fourth, our sim-to-real framework currently relies on a high-accuracy simulator and our proposed training-and deployment-time algorithms. However, for some tasks of even higher complexity (e.g., assembly of elastic cables onto pulleys), the simulator may neither be fundamentally accurate nor efficient enough to smoothly train and deploy policies to the real world. We envision the construction of a tight feedback loop from real-world deployments (i.e., a sim-to-real-to-sim loop) as a potentially compelling training strategy [1,33].</p>
<p>IX. CONCLUSIONS</p>
<p>In this paper, we have presented IndustReal, a set of algorithms, systems, and tools to solve benchmark assembly problems in simulation and transfer policies to the real world. The utility of our simulation-based algorithms (SAPU, SDF-Based Dense Reward, and SBC) and real-world algorithm (PLAI) has been demonstrated through careful experiments in simulation and the real world. We provide the first simulation results for a series of benchmark tasks proposed in [48], and most critically, we demonstrate what is, to our knowledge, the first real-world system for RL-based sim-to-real on the endto-end assembly task with no policy adaptation phase. Finally, in the hope of full reproducibility, we provide hardware and software for others in the community to replicate our results. Bingjie T., Michael L. ran experimental evaluations. Yashraj N., Bingjie T., Michael L. wrote the paper. Yashraj N., Bingjie T., Michael L., Fabio R., Ankur H., Gaurav S. revised the paper.</p>
<p>Bingjie T., Yashraj N., Michael L. created the video. Yashraj N., Dieter F., Fabio R., Gaurav S., Ankur H., Iretiayo A. advised the project.</p>
<p>APPENDIX</p>
<p>A. Related Work</p>
<p>Here we review classical and learning-based approaches to robotic assembly and briefly comment on simulation.</p>
<p>1) Classical Approaches: Assembly has been an open challenge in robotics for decades [70,45]. Many analytical methods have been proposed to solve assembly tasks, particularly for the canonical problem of peg-in-hole insertion; these methods have typically utilized geometry, dynamics, mechanical design, and sensing as fundamental tools. Drake [12] proposed remote center compliance (RCC) as a means to mitigate reliance on visual or force sensing during assembly. Whitney et al. [71] described the effects of part geometry, gripper and support stiffness, and friction on contact forces and adverse outcomes (e.g., jamming and wedging). Lozano-Pérez et al. [35] proposed compliant motion planning strategies for assembly. Xia et al. [73] derived a compliant contact model and used the model to avoid jamming and wedging. Huang et al. [22] addressed initial part misalignment using vision, and Tang et al. [60] addressed this challenge via force/torque sensing. The preceding principles and methods are the prevailing means of addressing the annual NIST Assembly Task Board challenge, the established benchmark in robotic assembly [25,66]. Nevertheless, such methods can be highly sensitive to errors in modeling, sensing, and state estimation; perturbations of adapters, fixtures, and calibration; and the introduction of unseen or more complex assets.</p>
<p>2) Learning-Based Approaches: In the past few years, learning-based approaches to assembly have gained popularity in the robotics research community, with many of the efforts focused on RL. Earlier works have typically explored modelbased algorithms, such as guided policy search (GPS) [61] and iterative linear-quadratic-Gaussian control (iLQG) [36]. Despite the sample efficiency of these algorithms, nonlinear and discontinuous contact dynamics have made them challenging to leverage for contact-rich manipulation tasks [58].</p>
<p>A number of recent works have used model-free, off-policy RL algorithms, including classical Q-learning [23], deep-Q networks (DQN) [79], soft actor-critic (SAC) [7], probabilistic embeddings for actor-critic RL (PEARL) [52], hierarchical RL [21], and most popularly, deep deterministic policy gradients (DDPG) [6,39,37,65]. These algorithms are also sample efficient, but can have unfavorable convergence properties.</p>
<p>A smaller number of research efforts have used modelfree, on-policy algorithms, including trust region policy optimization (TRPO) [32], proximal policy optimization (PPO) [20,56,67,48], and asynchronous advantage actor-critic (A3C) [54]. These algorithms have favorable convergence properties and are easy to tune; however, they are highly sample inefficient and can require long training times.</p>
<p>Other recent works have used on-policy or off-policy RL algorithms that can leverage demonstrations (e.g., human demonstrations or reference trajectories and controllers), such as residual learning from demonstration (rLfD), minimumjerk trajectories, or impedance controllers, [11,27,24], in-terleaved Riemannian Motion Policies (RMP) and SAC [31], guided DDPG [13], DDPG from demonstration (DDPGfD) [37,38,64], offline meta-RL with advantage-weighted actorcritic (AWAC) [80], and inverse RL [72]. For efforts using human demonstrations, substantial engineering infrastructure and collection time can be required; demonstrations can be suboptimal; and successful demonstrations can be difficult to reliably obtain during high-precision tasks.</p>
<p>Several learning-based, non-RL approaches have also been proposed. These approaches include using human-initialized self-supervised learning to learn a policy or residual policy with multimodal inputs [57,59,15], as well as learning from videos of human demonstrations using category-level visual representations and 6-DOF tracking [69].</p>
<p>The preceding efforts define the state-of-the-art in learningbased approaches for assembly. Several have demonstrated high success rates and repeatability, shown robustness to small perturbations of initial part poses, and/or shown some degree of generalization across parts; one has even outperformed a solution from professional integrators [37]. However, most successful efforts have required human initializations, demonstrations, or on-policy corrections. Furthermore, purely real-world approaches are inherently difficult to parallelize; may require long training to achieve appreciable robustness (e.g., ∼50 hours in [37]); typically require manual resets; can be impractical for time-consuming, expensive, delicate, or dangerous tasks (e.g., construction); and do not fully leverage the substantial amount of virtual data available for industrial settings (e.g., nearly every existing industrial part originates from a CAD model that can be rendered or simulated).</p>
<p>3) Simulation: To our knowledge, the state-of-the-art in accurate and efficient contact-rich simulation is captured in [48,41,10,29,62,76]. Among these, [48,10] specifically address simulation for robotics tasks, whereas [48] integrates these capabilities within a widely-used robotics simulator [43]. Thus, we leverage [48] as our simulation platform.</p>
<p>B. Real-World System 1) Communications: A schematic of our communications framework is shown in Figure S8. The input to the communications pipeline is a trained RL policy (specifically, a checkpoint file) from Isaac Gym, which is provided to IndustRealLib. The output of the communications pipeline is a set of torque commands communicated to the robot.  2) Camera Calibration: The goal of intrinsic camera calibration for an RGB camera is to determine the relationship between the location of a 3D point in space and its location in the image. We rely on the intrinsic camera parameters provided by Intel through the librealsense and pyrealsense2 libraries.</p>
<p>The goal of extrinsic camera calibration is to determine the 6-DOF pose T robot cam of the camera with respect to the robot frame. To compute extrinsic camera parameters, we first place a 6-inch AprilTag (52h13) onto the work surface and command the robot via the frankx library to random 6-DOF poses in the robot workspace, biased towards having the tag in view, but avoiding direct overhead views due to pose ambiguity. At each viewpoint, we detect the tag and compute the 6-DOF pose T cam tag of the tag with respect to the camera frame via the pupilapriltags library, and we simultaneously query the 6-DOF pose T robot ee of the end-effector in the robot base frame. We collect approximately 30 such samples and then use the Tsai-Lenz method [63] from the OpenCV library to compute the 6-DOF pose T ee cam of the camera with respect to the end-effector. The 6-DOF pose T robot cam of the camera with respect to the robot base frame can then simply be computed as T robot ee T ee cam . We validated the extrinsic parameters by comparing our estimated T ee cam against the corresponding transformation measured in a CAD assembly containing part models of the RealSense camera, camera mount, and end-effector.</p>
<p>3) Object Detection: The goal of our object detection module is to determine the object identities, 2D bounding boxes, and segmentation masks of each of our parts in the RGB image. To perform object detection, we used the implementation of the well-established Mask R-CNN [19] network architecture available in the torchvision library, which was pretrained on the Microsoft COCO dataset [34].</p>
<p>As the COCO dataset does not contain industrial assets, initial tests on our parts resulted in failure. Thus, we finetuned the pretrained model on real-world images of our assets. Specifically, we used the RealSense to capture 10-30 overhead images of each part randomly placed on our work surface, as well as 10 images of the work surface itself. We used Adobe Photoshop to automatically remove the backgrounds from the part images and extracted bounding boxes from the results.</p>
<p>We then divided the images into three different sets: 1) background, round pegs, rectangular pegs, round holes, and rectangular holes, 2) background, small gear, medium-sized gear, and large gear, and 3) background, NEMA 1-15 plug, NEMA 1-15 socket, USB-C plug, and USB-C socket.</p>
<p>For each set, we generated an augmented collection of images that consisted of each non-background element with random translations, rotations, and scaling. The elements were overlaid upon randomly-selected background images, and the composites were subject to color jitter using the kornia library.</p>
<p>Each augmented set of images was then used to train a Mask R-CNN model in pytorch. The categories within the set were added to the pretrained model, and the model was fine-tuned on images from the set to minimize losses over object identities, bounding boxes, and segmentation masks. Each model was trained for 50 epochs with 4000 training images, requiring ≈ 7.5 hours on a single GPU, at which point precision and recall scores were typically above 85%.</p>
<p>When using our trained models at test time, we performed additional data augmentation consisting exclusively of color jitter on captured images. The augmented images were used as input to our detection model. For each object in the image, the image with the highest object-identification confidence score was used to extract the object's identity, bounding box, and segmentation mask. This test-time augmentation improved our robustness to lighting variation and the presence of distractors. The yaw angle of each object was extracted by computing a minimum-area rectangle on the bounding box with OpenCV and calculating the angle of the box with the horizontal. 4) Workspace Mapping: For each detected object, we computed the centroid of its bounding box in image space. As mentioned earlier, the primary goal of our perception pipeline is to estimate the 2D poses (x, y, θ) of the parts on the workbench in the robot frame; thus, we needed to convert the location of the centroid (as well as the yaw angle determined earlier) from image space to 3D space.</p>
<p>In order to perform this transformation, we placed a 3inch AprilTag (53h13) at an arbitrary location in the field of view and computed the pose T cam tag of the tag in the camera frame. With additional knowledge of the size of the tag and the pixel resolution of the camera, distances in image space were mapped to distances in the camera frame. (Strictly speaking, this relation holds only within the plane containing the AprilTag, with decreasing accuracy farther away due to perspective transformations.) The pose T cam tag , the location of the centroid of a particular part, and the distance mapping were used to convert the centroid and yaw angle from image space to the camera frame. Finally, the location of the centroid and the yaw angle were converted from the camera frame to the robot frame using the transformation T robot cam computed earlier. We note that our workspace mapping process was susceptible to error induced by yaw rotations of the camera with respect to the robot frame, which resulted in systematic bias of our perceived locations relative to their actual locations. Thus, we performed a final one-time calibration step, during which we executed the Place experiment (i.e., laser tests) near the four corners of the workspace, estimated the offsets relative to the centers of the targets, averaged the offsets, and subtracted the average from all subsequently-computed 3D locations. 5) Constructing Targets: After execution of the perception pipeline, the robot receives corresponding (x, y, z = h n , α = θ, β = 0.0, γ = 0.0) as end-effector targets. The nominal height h n at which to pick or place each part is specified in advance by the human; in our experience, for industrial-style parts, this specification is trivial (e.g., 25-50% from the top of the part) and requires little to no iteration. 6) Experimental Setup: Our real-world experimental setup consists of a Franka Emika Panda arm with a wrist-mounted RGB-D camera, as well as an optical breadboard upon which mechanical parts are placed. The robot, camera, optical breadboard, and parts are shown in Figure S11 and Figure S10. 7) Summary: We have thus described our full perception pipeline. Camera calibration was performed once before beginning all experiments. The detection and workspace mapping process (aside from the one-time calibration step) were performed at the beginning of each trial that required detections;  in total, these per-trial steps took approximately 10 seconds to execute on our non-realtime system.</p>
<p>C. Policy-Level Action Integrator 1) Standard PLAI: Consider an RL policy that generates relative pose actions; in other words, the action composed with the current pose produces the desired state. In practice, the ability to achieve these desired states depends on passive and active system dynamics; having a large discrepancy between the environment used for training and deployment can lead to poor policy performance, especially when the system uses a more dynamic controller (e.g., a low stiffness task-space impedance controller). In this context, we briefly compare the behavior of PLAI with a lower-level proportional (P) controller, against a standard proportional-integral (PI) controller. 5 The general form of a discrete-time P controller is given by
F [n] = k P e<a href="5">n</a>
where F is the control effort (e.g., force or torque), k P is the proportional gain, and e is an error signal. We define the error signal as the difference between the desired state and the current state:
e[n] = x d [n] − x<a href="6">n</a>
where x is the state and superscript d denotes desired.</p>
<p>For PLAI, we apply actions to the previous desired state:
x d [n] = Π(o[n]) + x d <a href="7">n − 1</a>
In other words, we interpret actions as the difference between the current desired state and the previous desired state:
Π(o[n]) = x d [n] − x d <a href="8">n − 1</a>
We can re-write the error signal as
e[n] = Π(o[n]) + x d [n − 1] − x<a href="9">n</a>
Next, we can roll out the first few timesteps of F :
F [1] = k P Π(o[1]) + x d [0] − x[1] = k P Π(o[1]) + Π(o[0]) + x d [0] − x[1] F [2] = k P Π(o[2]) + x d [1] − x[2] = k P Π(o[2]) + Π(o[1]) + Π(o[0]) + x d [0] − x[2] F [3] = k P Π(o[3]) + x d [2] − x[3] = k P Π(o[3]) + Π(o[2]) + ... + Π(o[0]) + x d [0] − x[3] F [N ] = k P (Π(o[N ]) + Π(o[N − 1]) + ... + Π(o[0]) + x d [0] − x[N ])
More generally,
F [N ] = k P N k=0 (Π(o[k])) + x d [0] − x<a href="10">N </a>
In words, the sum of the policy outputs determines the control setpoint, which is tracked by the P controller. The summation term closely resembles an integral term in a PI controller:
F [N ] = k P e[N ] + k I N k=0 (e[k]) (11) = k P (x d [N ] − x[N ]) + k I N k=0 (Π(o[k]))(12)
The main difference is that the integral term in PLAI is used as a control setpoint, whereas in PI, it is directly converted into control effort. More extensively, 1) PLAI integrates the policy actions to generate a setpoint, which is used by the low-level impedance controller to attract the real state towards the desired state. If the system is disturbed from its current state (e.g., the robot is pushed away), the setpoint will not change instantaneously. Instead, the force vector will pull the system towards the same setpoint. 2) PI integrates the policy actions (in this case, equal to the control error) to generate a corrective force vector.</p>
<p>If the system is disturbed from its current state, the accumulated error will be applied to an unintended state and may become a disturbance to the policy.</p>
<p>2) PLAI for 6-DOF Pose: The PLAI derivation above applies directly to control of Cartesian position; executing PLAI on Cartesian orientation is very similar, as addition and subtraction operators can be replaced by rotation matrix operations. Specifically, addition can be replaced with
R O en+1 = R O en R en en+1(13)
and subtraction can be replaced with
R O en = R O en+1 [R en en+1 ] T(14)
where R O B is the rotation of a frame B relative to the frame O, and R en en+1 is an incremental rotation of the end-effector at time step n + 1 relative to time step n.</p>
<p>3) Leaky PLAI: After the PLAI update (Equation 3), we can simply rewrite s d t+1 as
s d t+1 = s t ⊕ (s d t+1 ⊖ s t )(15)
and update the desired state as
s d t+1 ← s t ⊕ min (s d t+1 ⊖ s t ), ϵ(16)
where ϵ is a threshold transformation.  </p>
<p>D. Additional Simulation Parameters &amp; Results</p>
<p>1) MDP formulation &amp; Parameters in Simulation:</p>
<p>Given our choice of RL, we can formulate the problem as a Markov decision process (MDP) with state space S, observation space O, action space A, state transition dynamics T : S × A → S, initial state distribution ρ 0 , reward function r : S → R, horizon length T , and discount factor γ ∈ (0, 1]. The objective is to learn a policy π : O → P(A) that maximizes the expected sum of discounted rewards E π [Σ T −1 t=0 γ t r(s t )].</p>
<p>We do not assume that state is fully observable in either simulation or the real world (specifically, O ⊊ S). In the real world, the Franka's joint velocities, end-effector velocities, joint torques, and end-effector forces exhibit appreciable noise in free space. In addition, perceptual error leads to noise in object pose estimates, which in turn introduce noise into target poses. Furthermore, without tactile sensing or a 6-DOF tracker, we do not observe the state of objects within the gripper. Table V describes the observation spaces, Table VI describes  the reward formulations, and Table IV describes the task success criteria (i.e., the condition for a terminal reward) for policy training in simulation. In addition, Table VII describes the randomization range used for training.</p>
<p>2) Simulation-Aware Policy Update: Figure S15 shows the distribution of mesh interpenetration distances during a typical training episode. Figure S16 shows an example of mesh interpenetration between a peg and a hole.        VI: Rewards for each policy. Symbol k denotes keypoint positions, x i denotes vertices on the plug, N is the number of vertices sampled from the plug mesh, ϕ(x i ) is the SDF value at a given vertex, d is the mean mesh interpenetration distance, ϵ d is the interpenetration distance threshold, ζ is the magnitude of the randomization range, D is the stage of the curriculum, ∆h is the height difference between current pose and goal pose when the episode is marked as successful, and ϵ k is the keypoint distance error threshold. Note that the reward formulations for the Reach, Pick, and Place policies are simple; those for Insert (Pegs) and Insert (Gears) required more nuance.  </p>
<p>3) Joint Evaluation in Simulation:</p>
<p>Fig. 1 :
1Overview. Top: Simulation-based policy learning for one of our tasks, gear assembly. Middle: Proposed algorithms to facilitate sim-based learning and real-world deployment. Bottom: Successful transfer to the real world.</p>
<p>Fig. 2 :
2Problem setup and decomposition. Column 1: Three types of assemblies. Columns 2-4: Goal states of Pick, Place, and Insert phases.</p>
<p>ip &lt; [0.5, 1, 1.5, 2, ∞] mm. For both the most realistic scenario (when only successes where d max ip &lt; 0.5 mm were counted) and the least realistic</p>
<p>Fig. 3 :
3Evaluation of Simulation-Aware Policy Update. Success rates are computed for episodes where the maximum interpenetration distance was less than the specified value at test time. Boxes indicate median and IQR.</p>
<p>Fig. 4 :
4Joint evaluation of Simulation-Based Policy Update, SDF-Based Dense Reward, and Sampling-Based Curriculum. (A) Pegs and Holes assembly Insert policy. (B) Gears and Gearshafts assembly Insert policy.</p>
<p>Fig. 5 :
5Evaluation of PLAI in simulation. Results of Nominal are annotated when outside of plot bounds. Full-axis plot is inFigure S13.</p>
<p>Fig. 6 :
6Evaluation of PLAI in the real world. Each method was tested on 3 different goals with 20 trials each. Evaluation parameters are in</p>
<p>Fig. 7 :
7Snapshots of real-world experiments. Top row: recovery behavior exhibited by the robot after human perturbation during gear insertion. Bottom row: search behavior exhibited during 3-prong NEMA connector insertion.</p>
<p>ACKNOWLEDGMENTSThe authors thank Michael Noseworthy for advice throughout the project, Bowen Wen and Ajay Mandlekar for advice on perception, Eric Heiden and Balakumar Sundaralingam for advice on writing Warp kernels for SAPU, Lucas Manuelli for help with the initial implementation of IndustRealLib and advice on dynamics and control, Karl Van Wyk and Nathan Ratliff for feedback on PLAI, Viktor Makoviychuk for advice on domain randomization and asymmetric actor-critic, Gavriel State and Kelly Guo for providing support with Isaac Gym, Philipp Reist and Tobias Widmer for providing support with PhysX, and Sandeep Desai and Kenneth Maclean for providing support with hardware setup and 3D printing.CONTRIBUTIONS Bingjie T., Yashraj N., Fabio R. developed SAPU. Bingjie T., Yashraj N., Dieter F. developed SDF reward. Bingjie T., Yashraj N. developed SBC. Michael L., Yashraj N. developed PLAI. Michael L., Yashraj N., Iretiayo A., Bingjie T. developed IndustRealLib. Yashraj N., Michael L. developed IndustRealKit. Bingjie T., Yashraj N., Ankur H. developed the perception module.</p>
<p>Fig. S8 :
S8Schematic of communications framework.</p>
<p>Evaluation of Place policy in simulation and the real world, with/without damping during simulation-based training.</p>
<p>Fig. S10 :
S10Real-world experimental setup.</p>
<p>Fig. S11 :
S11A subset of parts from IndustRealKit.</p>
<p>Fig. S12 :
S12Experimental setup for evaluating Place policy.</p>
<p>Fig. S13 :
S13Evaluation of PLAI in simulation. PLAI is compared to Nominal and PID under three environmental conditions. Ideal indicates no perturbations.</p>
<p>Fig. S14 :
S14Evaluation of PLAI in simulation, illustrating timeseries behavior. PLAI is compared to Nominal and PID over 20 trials under a randomized gravitational disturbance (i.e., gravitational acceleration g ∼ U [0.1, 2.0]m/s 2 ). Position error is measured with respect to final target. Inset view shows that steady-state error is lowest for PLAI (1.6 mm), followed by PID (3.5 mm); Nominal is frequently unable to converge.</p>
<p>Fig. S15 :
S15Histogram of mesh interpenetrations during typical training episode.</p>
<p>Fig. S16 :
S16Visualization of a transient interpenetration event between peg and hole assets with low-quality meshes. Yellow spheres denote mesh vertices. Colorbar is in m.</p>
<p>V: Observations provided to the actor and critic for each policy. For Pick, target pose is the current plug pose; for Place, target pose is the target plug pose; and for Insert (Pegs/Gears), target pose is a target end-effector pose.</p>
<p>• Baseline :
BaselineDo not utilize interpenetration information. • Filter Only: For a given episode, if max interpenetration depth d max ip is greater than ϵ ip = 1 mm, do not use return in policy update. If d max ip &lt; ϵ ip , use return as normal.• Weight Only: For a given episode, weight return by 1 − 
tanh(d max 
ip /ϵ d ), which is bounded by (0, 1]. 
• Filter and Weight: For a given episode, if d max </p>
<p>ip </p>
<blockquote>
<p>ϵ ip , 
do not use return in policy update. If d max </p>
</blockquote>
<p>ip </p>
<p>&lt; ϵ ip , weight 
return by 1 − tanh(d max 
ip /ϵ d ))</p>
<p>TABLE I :
IEvaluation of SDF-Based Dense Reward. Symbol k denotes object keypoint positions, S is a set of points comprising a point cloud (here, we use plug/socket mesh vertices), and x i denotes points sampled from the plug mesh (again, we use vertices). Engage denotes a partial insertion.</p>
<p>TABLE II :
IIEvaluation of Sampling-Based Curriculum. Baseline denotes that no curriculum was used.</p>
<p>TABLE III :
IIIReal-world experimental results for Pick, Insert, and PPI.</p>
<p>Table X evaluates how the Insert policies perform under different randomization and observation noise conditions in simulation. 4) Additional Real Parameters &amp; Results: Table VIII describes physics and control frequencies for simulation and the real world, Table IX describes real-world deployment parameters for the Reach policy, and Table III describes realworld deployment parameters for the remaining policies. obj &gt; 3 * h obj + h table Place Close placement ||k curr− k goal || 2 &lt; ϵ k Insert Peg inserted in hole ∆h &lt; ϵ h &amp; ||k plug − k socket || 2 &lt; ϵ k GearGear inserted on shaft ∆h &lt; ϵ h &amp; ||k gear − k shaft || 2 &lt; ϵ kTask 
Success Criterion 
Condition </p>
<p>Pick 
Successful lift 
h </p>
<p>TABLE IV :
IVSuccess criterion for each policy. Symbols h obj and h table denote heights of the object and table, ∆h denotes height from the hole/shaft base to the peg/gear base, k denotes keypoint positions, ϵ k = 10 cm is the keypoint distance error threshold, and ϵ h = 3 mm is the height error threshold. Actor Critic Actor Critic Actor Critic Actor CriticInput </p>
<p>Dimensionality 
Pick 
Place 
Insert (Pegs) 
Insert (Gears) 
Arm joint angles 
7 
✓ 
✓ 
✓ 
✓ 
✓ 
✓ 
✓ 
✓ 
Fingertip pose 
3 (position) + 4 (quaternion) 
✓ 
✓ 
✓ 
✓ 
✓ 
✓ 
✓ 
✓ 
Object (peg/gear) pose 
3 (position) + 4 (quaternion) 
✓ 
✓ 
✓ 
✓ 
✓ 
Target pose 
3 (position) + 4 (quaternion) 
✓ 
✓ 
✓ 
✓ 
Target pose with noise 
3 (position) + 4 (quaternion) 
✓ 
✓ 
Relative target pose with noise 
3 (position) + 4 (quaternion) 
✓ 
✓ 
Arm joint velocities 
7 
✓ 
✓ 
✓ 
✓ 
Fingertip linear velocity 
3 
✓ 
✓ 
✓ 
✓ 
Fingertip angular velocity 
3 
✓ 
✓ 
✓ 
✓ 
Object (peg/gear) linear velocity 
3 
✓ 
✓ 
✓ 
Object (peg/gear) angular velocity 3 
✓ 
✓ 
✓ 
Target position observation noise 
3 
✓ 
✓ 
Relative target pose 
3 (position) + 4 (quaternion) 
✓ 
✓ </p>
<p>TABLE</p>
<p>Close to target ||k curr − k goal || 2 &lt; ϵ k 1.0 Move close to target ✓ ✓Per-Timestep Reward 
Formulation 
Scale 
Justification 
Reach 
Pick Place Insert (Pegs) Insert (Gears) </p>
<p>Distance to goal 
−||k curr − k goal || 2 
1.0 
Move closer to goal 
✓ 
✓ 
✓ 
SDF-based distance 
− log(Σ N 
i ϕ(x i )/N ) 
10.0 
Align shapes 
✓ 
✓ </p>
<p>Scaling Factor on Return 
Range </p>
<p>Mesh interpenetration 
1 − tanh(d/ϵ d ) 
[0, 1] 
Avoid interpenetration 
✓ 
✓ 
Randomization range 
ζcurr/ζmax 
[0, 1] 
Adapt to randomization 
✓ 
✓ 
Curriculum difficulty 
Dcurr/Dmax 
[0, 1] 
Adapt to difficulty 
✓ 
✓ 
Bonus scale 
1/(△h + 0.1) 
[0, 1] 
Move closer to goal 
✓ 
✓ </p>
<p>Sparse Reward 
Condition 
Value </p>
<p>Task success 
See Table IV 
10.0 
Complete task 
✓ 
✓ 
✓ 
✓ </p>
<p>TABLE</p>
<p>TABLE VIII :
VIIIPhysics and control frequencies for simulation and reality. Physics frequency in the real world is given by c/L, where c is the speed of sound and L is a characteristic lengthscale; c can be approximated by the Newton-Laplace equation.Deploy Method Parameter 
Value </p>
<p>All 
Controller Gains 
[1000, 1000, 1000, 50, 50, 50] 
PLAI 
Pos. Action Scale [0.0005, 0.0005, 0.0005] 
PLAI 
Rot. Action Scale [0.001, 0.001, 0.001] 
Nominal 
Pos. Action Scale [0.01, 0.01, 0.01] 
Nominal 
Rot. Action Scale [0.01, 0.01, 0.01] </p>
<p>TABLE IX :
IXReach policy evaluation parameters in real robot
This broad usage of plug and socket will persist throughout the paper.
To our knowledge, only two other sim-to-real efforts have examined perturbations of this magnitude[11,79]. Both manipulated much larger pegs.
The comparison would also hold for PLAI with a lower-level PD controller, against a PID controller; we omit derivative terms for simplicity.</p>
<p>Anish Shankar, Alex Bewley, and Pannag R Sanketi. i-sim2real: Reinforcement learning of robotic policies in tight human-robot interaction loops. Laura Saminda Abeyruwan, Graesser, B David, Avi D&apos;ambrosio, Singh, arXiv:2207.06572arXiv preprintSaminda Abeyruwan, Laura Graesser, David B D'Ambrosio, Avi Singh, Anish Shankar, Alex Bewley, and Pannag R Sanketi. i-sim2real: Reinforcement learning of robotic policies in tight human-robot interaction loops. arXiv preprint arXiv:2207.06572, 2022.</p>
<p>Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving Rubik's cube with a robot hand. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob Mcgrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, arXiv:1910.07113Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian WengarXiv preprintIlge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Pe- ter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving Rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Transferring dexterous manipulation from GPU simulation to a remote real-world TriFinger. Arthur Allshire, Mayank Mittal, Varun Lodaya, Viktor Makoviychuk, Denys Makoviichuk, Felix Widmaier, Manuel Wüthrich, Stefan Bauer, Ankur Handa, Animesh Garg, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022Arthur Allshire, Mayank Mittal, Varun Lodaya, Vik- tor Makoviychuk, Denys Makoviichuk, Felix Widmaier, Manuel Wüthrich, Stefan Bauer, Ankur Handa, and Ani- mesh Garg. Transferring dexterous manipulation from GPU simulation to a remote real-world TriFinger. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022.</p>
<p>Contact and friction simulation for computer graphics. Sheldon Andrews, Kenny Erleben, Zachary Ferguson, https:/dl.acm.org/doi/pdf/10.1145/3532720.3535640ACM SIGGRAPH Courses. Sheldon Andrews, Kenny Erleben, and Zachary Fer- guson. Contact and friction simulation for computer graphics. In ACM SIGGRAPH Courses. 2022.</p>
<p>Learning dexterous in-hand manipulation. Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Józefowicz, Bob Mcgrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, Wojciech Zaremba, http:/journals.sagepub.com/doi/10.1177/0278364919887447International Journal of Robotics Research. Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Józefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. International Journal of Robotics Research, 2020.</p>
<p>Robotic assembly of timber joints using reinforcement learning. Aleksandra Anna Apolinarska, Matteo Pacher, Hui Li, Nicholas Cote, Rafael Pastrana, Fabio Gramazio, Matthias Kohler, Automation in ConstructionAleksandra Anna Apolinarska, Matteo Pacher, Hui Li, Nicholas Cote, Rafael Pastrana, Fabio Gramazio, and Matthias Kohler. Robotic assembly of timber joints using reinforcement learning. Automation in Construction, 2021.</p>
<p>Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach. C Cristian, Damien Beltran-Hernandez, Petit, G Ixchel, Kensuke Ramirez-Alpizar, Harada, Applied Sciences. Cristian C. Beltran-Hernandez, Damien Petit, Ixchel G. Ramirez-Alpizar, and Kensuke Harada. Variable com- pliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach. Applied Sciences, 2020.</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, https:/dl.acm.org/doi/pdf/10.1145/1553374.1553380International Conference on Machine Learning (ICML). Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In International Conference on Machine Learning (ICML), 2009.</p>
<p>Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, arXiv:2211.11744Visual dexterity: Inhand dexterous manipulation from depth. ward Adelson, and Pulkit AgrawalarXiv preprintTao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Ed- ward Adelson, and Pulkit Agrawal. Visual dexterity: In- hand dexterous manipulation from depth. arXiv preprint arXiv:2211.11744, 2022.</p>
<p>Midas: A multi-joint robotics simulator with intersection-free frictional contact. Yunuo Chen, Minchen Li, Wenlong Lu, Chuyuan Fu, Chenfanfu Jiang, arXiv:2210.00130arXiv preprintYunuo Chen, Minchen Li, Wenlong Lu, Chuyuan Fu, and Chenfanfu Jiang. Midas: A multi-joint robotics simulator with intersection-free frictional contact. arXiv preprint arXiv:2210.00130, 2022.</p>
<p>Stefan Schaal, and Subramanian Ramamoorthy. Residual learning from demonstration: Adapting DMPs for contact-rich manipulation. Todor Davchev, Kevin Sebastian Luck, Michael Burke, Franziska Meier, IEEE Robotics and Automation Letters. Todor Davchev, Kevin Sebastian Luck, Michael Burke, Franziska Meier, Stefan Schaal, and Subramanian Ra- mamoorthy. Residual learning from demonstration: Adapting DMPs for contact-rich manipulation. IEEE Robotics and Automation Letters, 2021.</p>
<p>Using compliance in lieu of sensory feedback for automatic assembly. Samuel Hunt, Drake , Massachusetts Institute of TechnologyPhD thesisSamuel Hunt Drake. Using compliance in lieu of sensory feedback for automatic assembly. PhD thesis, Massachusetts Institute of Technology, 1978.</p>
<p>A learning framework for high precision industrial assembly. Yongxiang Fan, Jieliang Luo, Masayoshi Tomizuka, International Conference on Robotics and Automation (ICRA). Yongxiang Fan, Jieliang Luo, and Masayoshi Tomizuka. A learning framework for high precision industrial as- sembly. In International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Intersection-free rigid body dynamics. Zachary Ferguson, Minchen Li, Teseo Schneider, Francisca Gil-Ureta, Timothy Langlois, Chenfanfu Jiang, Denis Zorin, Danny M Kaufman, Daniele Panozzo, https:/dl.acm.org/doi/10.1145/3450626.3459802ACM Transactions on Graphics. Zachary Ferguson, Minchen Li, Teseo Schneider, Fran- cisca Gil-Ureta, Timothy Langlois, Chenfanfu Jiang, Denis Zorin, Danny M Kaufman, and Daniele Panozzo. Intersection-free rigid body dynamics. ACM Transactions on Graphics, 2021.</p>
<p>Safely learning visuotactile feedback policies in real for industrial insertion. Letian Fu, Huang Huang, Lars Berscheid, Hui Li, Ken Goldberg, Sachin Chitta, arXiv:2210.01340arXiv preprintLetian Fu, Huang Huang, Lars Berscheid, Hui Li, Ken Goldberg, and Sachin Chitta. Safely learning visuo- tactile feedback policies in real for industrial insertion. arXiv preprint arXiv:2210.01340, 2022.</p>
<p>Dynamic identification of the Franka Emika Panda robot with retrieval of feasible parameters using penalty-based optimization. Claudio Gaz, Marco Cognetti, Alexander Oliva, Paolo Robuffo Giordano, Alessandro De Luca, IEEE Robotics and Automation Letters. Claudio Gaz, Marco Cognetti, Alexander Oliva, Paolo Robuffo Giordano, and Alessandro De Luca. Dynamic identification of the Franka Emika Panda robot with retrieval of feasible parameters using penalty-based optimization. IEEE Robotics and Automation Letters, 2019.</p>
<p>The Franka Emika robot: A reference platform for robotics research and education. Sami Haddadin, Sven Parusel, Lars Johannsmeier, Saskia Golz, Simon Gabl, Florian Walch, Mohamadreza Sabaghian, Christoph Jähne, Lukas Hausperger, Simon Haddadin, IEEE Robotics and Automation Magazine. Sami Haddadin, Sven Parusel, Lars Johannsmeier, Saskia Golz, Simon Gabl, Florian Walch, Mohamadreza Sabaghian, Christoph Jähne, Lukas Hausperger, and Si- mon Haddadin. The Franka Emika robot: A reference platform for robotics research and education. IEEE Robotics and Automation Magazine, 2022.</p>
<p>Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, Yashraj Narang, Jean-Francois Lafleche, arXiv:2210.13702Dieter Fox, and Gavriel State. DeXtreme: Transfer of agile in-hand manipulation from simulation to reality. arXiv preprintAnkur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurke- vich, Balakumar Sundaralingam, Yashraj Narang, Jean- Francois Lafleche, Dieter Fox, and Gavriel State. DeX- treme: Transfer of agile in-hand manipulation from simu- lation to reality. arXiv preprint arXiv:2210.13702, 2022.</p>
<p>Mask R-CNN. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, IEEE International Conference on Computer Vision (ICCV. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In IEEE International Confer- ence on Computer Vision (ICCV), 2017.</p>
<p>Towards real-world force-sensitive robotic assembly through deep reinforcement learning in simulations. Marius Hebecker, Jens Lambrecht, Markus Schmitz, IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM). 2021Marius Hebecker, Jens Lambrecht, and Markus Schmitz. Towards real-world force-sensitive robotic assembly through deep reinforcement learning in simulations. In IEEE/ASME International Conference on Advanced In- telligent Mechatronics (AIM), 2021.</p>
<p>Dataefficient hierarchical reinforcement learning for robotic assembly control applications. Zhimin Hou, Jiajun Fei, Yuelin Deng, Jing Xu, IEEE Transactions on Industrial Electronics. Zhimin Hou, Jiajun Fei, Yuelin Deng, and Jing Xu. Data- efficient hierarchical reinforcement learning for robotic assembly control applications. IEEE Transactions on Industrial Electronics, 2021.</p>
<p>Fast peg-and-hole alignment using visual compliance. Shouren Huang, Kenichi Murakami, Yuji Yamakawa, Taku Senoo, Masatoshi Ishikawa, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Shouren Huang, Kenichi Murakami, Yuji Yamakawa, Taku Senoo, and Masatoshi Ishikawa. Fast peg-and-hole alignment using visual compliance. In IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems (IROS), 2013.</p>
<p>Deep reinforcement learning for high precision assembly tasks. Tadanobu Inoue, Giovanni De Magistris, Asim Munawar, Tsuyoshi Yokoya, Ryuki Tachibana, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Tadanobu Inoue, Giovanni De Magistris, Asim Munawar, Tsuyoshi Yokoya, and Ryuki Tachibana. Deep reinforce- ment learning for high precision assembly tasks. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.</p>
<p>Residual reinforcement learning for robot control. Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, Sergey Levine, International Conference on Robotics and Automation (ICRA). Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for robot control. In International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Benchmarking protocols for evaluating small parts robotic assembly systems. Kenneth Kimble, Karl Van Wyk, Joe Falco, Elena Messina, Yu Sun, Mizuho Shibata, Wataru Uemura, Yasuyoshi Yokokohji, IEEE Robotics and Automation Letters. Kenneth Kimble, Karl Van Wyk, Joe Falco, Elena Messina, Yu Sun, Mizuho Shibata, Wataru Uemura, and Yasuyoshi Yokokohji. Benchmarking protocols for evaluating small parts robotic assembly systems. IEEE Robotics and Automation Letters, 2020.</p>
<p>Performance measures to benchmark the grasping, manipulation, and assembly of deformable objects typical to manufacturing applications. Kenneth Kimble, Justin Albrecht, Megan Zimmerman, Joe Falco, Frontiers in Robotics and AI. Kenneth Kimble, Justin Albrecht, Megan Zimmerman, and Joe Falco. Performance measures to benchmark the grasping, manipulation, and assembly of deformable objects typical to manufacturing applications. Frontiers in Robotics and AI, 2022.</p>
<p>Reinforcement learning of impedance policies for peg-in-hole tasks: Role of asymmetric matrices. Shir Kozlovsky, Elad Newman, Miriam Zacksenhouse, IEEE Robotics and Automation Letters. Shir Kozlovsky, Elad Newman, and Miriam Zacksen- house. Reinforcement learning of impedance policies for peg-in-hole tasks: Role of asymmetric matrices. IEEE Robotics and Automation Letters, 2022.</p>
<p>Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation. Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin Maloon, Victoria Rose Most, Dave Stroud, Raymond Santos, Ahmad Byagowi, Gregg Kammerer, Dinesh Jayaraman, Roberto Calandra, IEEE Robotics and Automation Letters. Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin Maloon, Victoria Rose Most, Dave Stroud, Raymond Santos, Ahmad Byagowi, Gregg Kammerer, Dinesh Jayaraman, and Roberto Calandra. Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation. IEEE Robotics and Automation Letters, 2020.</p>
<p>Affine body dynamics: Fast, stable &amp; intersection-free simulation of stiff materials. Lei Lan, Danny M Kaufman, Minchen Li, Chenfanfu Jiang, Yin Yang, ACM Transactions on Graphics. Lei Lan, Danny M Kaufman, Minchen Li, Chenfanfu Jiang, and Yin Yang. Affine body dynamics: Fast, stable &amp; intersection-free simulation of stiff materials. ACM Transactions on Graphics, 2022.</p>
<p>Learning quadrupedal locomotion over challenging terrain. Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, Marco Hutter, https:/www.science.org/doi/10.1126/scirobotics.abc5986Science Robotics. Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science Robotics, 2020.</p>
<p>Guided uncertainty-aware policy optimization: Combining learning and model-based strategies for sample-efficient policy learning. A Michelle, Carlos Lee, Jonathan Florensa, Nathan Tremblay, Animesh Ratliff, Fabio Garg, Dieter Ramos, Fox, IEEE International Conference on Robotics and Automation (ICRA). 2020Michelle A Lee, Carlos Florensa, Jonathan Tremblay, Nathan Ratliff, Animesh Garg, Fabio Ramos, and Di- eter Fox. Guided uncertainty-aware policy optimiza- tion: Combining learning and model-based strategies for sample-efficient policy learning. In IEEE International Conference on Robotics and Automation (ICRA), 2020.</p>
<p>Making sense of vision and touch: Learning multimodal representations for contact-rich tasks. Michelle A Lee, Yuke Zhu, Peter Zachares, Matthew Tan, Krishnan Srinivasan, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg, IEEE Transactions on Robotics. Michelle A. Lee, Yuke Zhu, Peter Zachares, Matthew Tan, Krishnan Srinivasan, Silvio Savarese, Li Fei-Fei, Animesh Garg, and Jeannette Bohg. Making sense of vision and touch: Learning multimodal representations for contact-rich tasks. IEEE Transactions on Robotics, 2020.</p>
<p>Real2sim2real: Selfsupervised learning of physical single-step dynamic actions for planar robot casting. Vincent Lim, Huang Huang, Lawrence Yunliang Chen, Jonathan Wang, Jeffrey Ichnowski, Daniel Seita, Michael Laskey, Ken Goldberg, International Conference on Robotics and Automation (ICRA). 2022Vincent Lim, Huang Huang, Lawrence Yunliang Chen, Jonathan Wang, Jeffrey Ichnowski, Daniel Seita, Michael Laskey, and Ken Goldberg. Real2sim2real: Self- supervised learning of physical single-step dynamic ac- tions for planar robot casting. In International Confer- ence on Robotics and Automation (ICRA), 2022.</p>
<p>Microsoft COCO: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick, European Conference on Computer Vision (ECCV). Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV), 2014.</p>
<p>Automatic synthesis of fine-motion strategies for robots. Tomas Lozano-Pérez, T Matthew, Russell H Mason, Taylor, https:/journals.sagepub.com/doi/abs/10.1177/027836498400300101The International Journal of Robotics Research. Tomas Lozano-Pérez, Matthew T Mason, and Russell H Taylor. Automatic synthesis of fine-motion strategies for robots. The International Journal of Robotics Research, 1984.</p>
<p>Reinforcement learning on variable impedance controller for high-precision robotic assembly. Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, Alice M Agogino, Aviv Tamar, Pieter Abbeel, IEEE International Conference on Robotics and Automation (ICRA). Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, Alice M. Agogino, Aviv Tamar, and Pieter Abbeel. Reinforcement learning on variable impedance controller for high-precision robotic assembly. In IEEE International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Robust multi-modal policies for industrial assembly via reinforcement learning and demonstrations: A large-scale study. Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wenzhao Lian, Chang Su, Mel Vecerik, Ning Ye, Stefan Schaal, Jon Scholz, arXiv:2103.11512arXiv preprintJianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wen- zhao Lian, Chang Su, Mel Vecerik, Ning Ye, Stefan Schaal, and Jon Scholz. Robust multi-modal policies for industrial assembly via reinforcement learning and demonstrations: A large-scale study. arXiv preprint arXiv:2103.11512, 2021.</p>
<p>Dynamic experience replay. Jieliang Luo, Hui Li, Conference on Robot Learning (CoRL). Jieliang Luo and Hui Li. Dynamic experience replay. In Conference on Robot Learning (CoRL), 2019.</p>
<p>A learning approach to robot-agnostic force-guided high precision assembly. Jieliang Luo, Hui Li, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021Jieliang Luo and Hui Li. A learning approach to robot-agnostic force-guided high precision assembly. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.</p>
<p>Warp: A High-performance Python Framework for GPU Simulation and Graphics. Miles Macklin, NVIDIA GPU Technology Conference (GTC). Miles Macklin. Warp: A High-performance Python Framework for GPU Simulation and Graphics. https: //github.com/nvidia/warp, March 2022. NVIDIA GPU Technology Conference (GTC).</p>
<p>Local optimization for robust signed distance field collision. Miles Macklin, Kenny Erleben, Matthias Müller, Nuttapong Chentanez, Stefan Jeschke, Zach Corse, https:/dl.acm.org/doi/10.1145/3384538Proceedings of the ACM on Computer Graphics and Interactive Techniques. the ACM on Computer Graphics and Interactive TechniquesMiles Macklin, Kenny Erleben, Matthias Müller, Nut- tapong Chentanez, Stefan Jeschke, and Zach Corse. Lo- cal optimization for robust signed distance field collision. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 2020.</p>
<p>rlgames: A High-performance Framework for Reinforcement Learning. Denys Makoviichuk, Viktor Makoviychuk, Denys Makoviichuk and Viktor Makoviychuk. rl- games: A High-performance Framework for Reinforce- ment Learning. https://github.com/Denys88/rl games, May 2022.</p>
<p>Ankur Handa, and Gavriel State. Isaac Gym: High performance GPU-based physics simulation for robot learning. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac Gym: High performance GPU-based physics simulation for robot learning. In Neural Infor- mation Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021.</p>
<p>Rapid locomotion via reinforcement learning. Ge Gabriel B Margolis, Kartik Yang, Tao Paigwar, Pulkit Chen, Agrawal, arXiv:2205.02824arXiv preprintGabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit Agrawal. Rapid locomotion via reinforcement learning. arXiv preprint arXiv:2205.02824, 2022.</p>
<p>Mechanics of Robotic Manipulation. T Matthew, Mason, MIT PressMatthew T Mason. Mechanics of Robotic Manipulation. MIT Press, 2001.</p>
<p>Vision-driven compliant manipulation for reliable, high-precision assembly tasks. Bowen Andrew S Morgan, Junchi Wen, Liang, arXiv:2106.14070Abdeslam Boularias, Aaron M Dollar, and Kostas Bekris. arXiv preprintAndrew S Morgan, Bowen Wen, Junchi Liang, Ab- deslam Boularias, Aaron M Dollar, and Kostas Bekris. Vision-driven compliant manipulation for re- liable, high-precision assembly tasks. arXiv preprint arXiv:2106.14070, 2021.</p>
<p>Assessing transferability from simulation to reality for reinforcement learning. Fabio Muratore, Michael Gienger, Jan Peters, IEEE Transactions on Pattern Analysis and Machine Intelligence. Fabio Muratore, Michael Gienger, and Jan Peters. As- sessing transferability from simulation to reality for reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.</p>
<p>Factory: Fast contact for robotic assembly. Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Moravanszky, Gavriel State, Michelle Lu, Robotics: Science and Systems. Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Moravanszky, Gavriel State, Michelle Lu, et al. Factory: Fast contact for robotic assembly. In Robotics: Science and Systems, 2022.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. Marcin Xue Bin Peng, Wojciech Andrychowicz, Pieter Zaremba, Abbeel, IEEE International Conference on Robotics and Automation (ICRA). Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In IEEE International Conference on Robotics and Automation (ICRA), 2018.</p>
<p>Asymmetric actor critic for image-based robot learning. Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, Pieter Abbeel, arXiv:1710.06542arXiv preprintLerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wo- jciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017.</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. Nikita Rudin, David Hoeller, Philipp Reist, Marco Hutter, Conference on Robot Learning (CoRL). 2021Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Conference on Robot Learning (CoRL), 2021.</p>
<p>Metareinforcement learning for robotic industrial insertion tasks. Gerrit Schoettler, Ashvin Nair, Juan Aparicio Ojea, Sergey Levine, Eugen Solowjow, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020Gerrit Schoettler, Ashvin Nair, Juan Aparicio Ojea, Sergey Levine, and Eugen Solowjow. Meta- reinforcement learning for robotic industrial insertion tasks. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Learning to scaffold the development of robotic manipulation skills. Lin Shao, Toki Migimatsu, Jeannette Bohg, IEEE International Conference on Robotics and Automation (ICRA). 2020Lin Shao, Toki Migimatsu, and Jeannette Bohg. Learning to scaffold the development of robotic manipulation skills. In IEEE International Conference on Robotics and Automation (ICRA), 2020.</p>
<p>Taxim: An example-based simulation model for GelSight tactile sensors. Zilin Si, Wenzhen Yuan, IEEE Robotics and Automation Letters. Zilin Si and Wenzhen Yuan. Taxim: An example-based simulation model for GelSight tactile sensors. IEEE Robotics and Automation Letters, 2022.</p>
<p>Simto-real transfer of bolting tasks with tight tolerance. Dongwon Son, Hyunsoo Yang, Dongjun Lee, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020Dongwon Son, Hyunsoo Yang, and Dongjun Lee. Sim- to-real transfer of bolting tasks with tight tolerance. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>InsertionNet: A scalable solution for insertion. Oren Spector, Dotan Di Castro, IEEE Robotics and Automation Letters. Oren Spector and Dotan Di Castro. InsertionNet: A scalable solution for insertion. IEEE Robotics and Automation Letters, 2021.</p>
<p>Deep reinforcement learning for contact-rich skills using compliant movement primitives. Oren Spector, Miriam Zacksenhouse, arXiv:2008.132232020Oren Spector and Miriam Zacksenhouse. Deep rein- forcement learning for contact-rich skills using compliant movement primitives. arXiv:2008.13223 [cs], 2020.</p>
<p>InsertionNet 2.0: Minimal contact multi-step insertion using multimodal multiview sensory input. Oren Spector, Vladimir Tchuiev, Dotan Di Castro, arXiv:2203.01153arXiv preprintOren Spector, Vladimir Tchuiev, and Dotan Di Castro. InsertionNet 2.0: Minimal contact multi-step insertion using multimodal multiview sensory input. arXiv preprint arXiv:2203.01153, 2022.</p>
<p>Autonomous alignment of peg and hole by force/torque measurement for robotic assembly. Te Tang, Hsien-Chung Lin, Yu Zhao, Wenjie Chen, Masayoshi Tomizuka, IEEE International Conference on Automation Science and Engineering (CASE). Te Tang, Hsien-Chung Lin, Yu Zhao, Wenjie Chen, and Masayoshi Tomizuka. Autonomous alignment of peg and hole by force/torque measurement for robotic assembly. In IEEE International Conference on Automation Science and Engineering (CASE), 2016.</p>
<p>Learning robotic assembly from CAD. Garrett Thomas, Melissa Chien, Aviv Tamar, Juan Aparicio Ojea, Pieter Abbeel, IEEE International Conference on Robotics and Automation (ICRA). Garrett Thomas, Melissa Chien, Aviv Tamar, Juan Apari- cio Ojea, and Pieter Abbeel. Learning robotic assem- bly from CAD. In IEEE International Conference on Robotics and Automation (ICRA), 2018.</p>
<p>Assemble them all: Physics-based planning for generalizable assembly by disassembly. Yunsheng Tian, Jie Xu, Yichen Li, Jieliang Luo, Shinjiro Sueda, Hui Li, D D Karl, Wojciech Willis, Matusik, https:/dl.acm.org/doi/pdf/10.1145/3550454.3555525ACM Transactions on Graphics. 2022Yunsheng Tian, Jie Xu, Yichen Li, Jieliang Luo, Shinjiro Sueda, Hui Li, Karl DD Willis, and Wojciech Matusik. Assemble them all: Physics-based planning for general- izable assembly by disassembly. ACM Transactions on Graphics (TOG), 2022.</p>
<p>A new technique for fully autonomous and efficient 3D robotics hand/eye calibration. Y Roger, Reimar K Tsai, Lenz, IEEE Transactions on Robotics and Automation. Roger Y Tsai and Reimar K Lenz. A new technique for fully autonomous and efficient 3D robotics hand/eye cali- bration. IEEE Transactions on Robotics and Automation, 1989.</p>
<p>Mel Vecerik, cs]Todd Hester, cs]Jonathan Scholz, cs]Fumin Wang, cs]Olivier Pietquin, cs]Bilal Piot, cs]Nicolas Heess, cs]Thomas Rothörl, cs]Thomas Lampe, cs]Martin Riedmiller, cs]arXiv:1707.08817Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin Ried- miller. Leveraging demonstrations for deep reinforce- ment learning on robotics problems with sparse rewards. arXiv:1707.08817 [cs], 2018.</p>
<p>A practical approach to insertion with variable socket position using deep reinforcement learning. Mel Vecerik, Oleg Sushkov, David Barker, Thomas Rothorl, Todd Hester, Jon Scholz, IEEE International Conference on Robotics and Automation (ICRA). Mel Vecerik, Oleg Sushkov, David Barker, Thomas Rothorl, Todd Hester, and Jon Scholz. A practical approach to insertion with variable socket position using deep reinforcement learning. In IEEE International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Robots assembling machines: Learning from the World Robot. Christian Felix Von Drigalski, Martin Schlette, Nikolaus Rudorfer, Joshua C Correll, Weiwei Triyonoputro, Tokuo Wan, Tetsuyou Tsuji, Watanabe, Felix Von Drigalski, Christian Schlette, Martin Rudorfer, Nikolaus Correll, Joshua C Triyonoputro, Weiwei Wan, Tokuo Tsuji, and Tetsuyou Watanabe. Robots assembling machines: Learning from the World Robot Summit 2018</p>
<p>. Assembly Challenge. Advanced Robotics. Assembly Challenge. Advanced Robotics, 2020.</p>
<p>Learning sequences of manipulation primitives for robotic assembly. Nghia Vuong, Hung Pham, Quang-Cuong Pham, IEEE International Conference on Robotics and Automation (ICRA). 2021Nghia Vuong, Hung Pham, and Quang-Cuong Pham. Learning sequences of manipulation primitives for robotic assembly. In IEEE International Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Tacto: A fast, flexible, and opensource simulator for high-resolution vision-based tactile sensors. Shaoxiong Wang, Mike Lambeta, Po-Wei Chou, Roberto Calandra, IEEE Robotics and Automation Letters. Shaoxiong Wang, Mike Lambeta, Po-Wei Chou, and Roberto Calandra. Tacto: A fast, flexible, and open- source simulator for high-resolution vision-based tactile sensors. IEEE Robotics and Automation Letters, 2022.</p>
<p>You only demonstrate once: Category-level manipulation from single visual demonstration. Wenzhao Bowen Wen, Kostas Lian, Stefan Bekris, Schaal, arXiv:2201.12716arXiv preprintBowen Wen, Wenzhao Lian, Kostas Bekris, and Stefan Schaal. You only demonstrate once: Category-level manipulation from single visual demonstration. arXiv preprint arXiv:2201.12716, 2022.</p>
<p>Mechanical Assemblies: Their Design, Manufacture, and Role in Product Development. Daniel E Whitney, Oxford University PressDaniel E. Whitney. Mechanical Assemblies: Their De- sign, Manufacture, and Role in Product Development. Oxford University Press, 2004.</p>
<p>Quasi-static assembly of compliantly supported rigid parts. E Daniel, Whitney, Journal of Dynamic Systems, Measurement, and Control. Daniel E Whitney et al. Quasi-static assembly of compli- antly supported rigid parts. Journal of Dynamic Systems, Measurement, and Control, pages 65-77, 1982.</p>
<p>Learning dense rewards for contact-rich manipulation tasks. Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Tomizuka, Stefan Schaal, IEEE International Conference on Robotics and Automation (ICRA). 2021Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Tomizuka, and Stefan Schaal. Learning dense rewards for contact-rich manipulation tasks. In IEEE International Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Dynamic analysis for peg-in-hole assembly with contact deformation. Yanchun Xia, Yuehong Yin, Zhaoneng Chen, https:/link.springer.com/article/10.1007/s00170-005-0047-4The International Journal of Advanced Manufacturing Technology. Yanchun Xia, Yuehong Yin, and Zhaoneng Chen. Dy- namic analysis for peg-in-hole assembly with contact deformation. The International Journal of Advanced Manufacturing Technology, 2006.</p>
<p>Efficient tactile simulation with differentiability for robotic manipulation. Jie Xu, Sangwoon Kim, Tao Chen, Alberto Rodriguez Garcia, Pulkit Agrawal, Wojciech Matusik, Shinjiro Sueda, Conference on Robot Learning (CoRL). 2022Jie Xu, Sangwoon Kim, Tao Chen, Alberto Rodriguez Garcia, Pulkit Agrawal, Wojciech Matusik, and Shinjiro Sueda. Efficient tactile simulation with differentiability for robotic manipulation. In Conference on Robot Learn- ing (CoRL), 2022.</p>
<p>Compare contact model-based control and contact modelfree learning: A survey of robotic peg-in-hole assembly strategies. Jing Xu, Zhimin Hou, Zhi Liu, Hong Qiao, arXiv:1904.05240arXiv preprintJing Xu, Zhimin Hou, Zhi Liu, and Hong Qiao. Com- pare contact model-based control and contact model- free learning: A survey of robotic peg-in-hole assembly strategies. arXiv preprint arXiv:1904.05240, 2019.</p>
<p>Dongwon Son, and Dongjun Lee. Fast and accurate data-driven simulation framework for contact-intensive tight-tolerance robotic assembly tasks. Jaemin Yoon, Minji Lee, arXiv:2202.13098arXiv preprintJaemin Yoon, Minji Lee, Dongwon Son, and Dongjun Lee. Fast and accurate data-driven simulation framework for contact-intensive tight-tolerance robotic assembly tasks. arXiv preprint arXiv:2202.13098, 2022.</p>
<p>GelSight: High-resolution robot tactile sensors for estimating geometry and force. Wenzhen Yuan, Siyuan Dong, Edward H Adelson, Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. GelSight: High-resolution robot tactile sensors for esti- mating geometry and force. Sensors, 2017.</p>
<p>Kevin Zhang, Mohit Sharma, Jacky Liang, Oliver Kroemer, arXiv:2011.02398A modular robotic arm control stack for research: Franka-Interface and FrankaPy. arXiv preprintKevin Zhang, Mohit Sharma, Jacky Liang, and Oliver Kroemer. A modular robotic arm control stack for research: Franka-Interface and FrankaPy. arXiv preprint arXiv:2011.02398, 2020.</p>
<p>Learning insertion primitives with discrete-continuous hybrid action space for robotic assembly tasks. Xiang Zhang, Shiyu Jin, Changhao Wang, Xinghao Zhu, Masayoshi Tomizuka, arXiv:2110.126182021Xiang Zhang, Shiyu Jin, Changhao Wang, Xinghao Zhu, and Masayoshi Tomizuka. Learning insertion primitives with discrete-continuous hybrid action space for robotic assembly tasks. arXiv:2110.12618 [cs], 2021.</p>
<p>Offline meta-reinforcement learning for industrial insertion. Z Tony, Jianlan Zhao, Oleg Luo, Rugile Sushkov, Nicolas Pevceviciute, Jon Heess, Stefan Scholz, Sergey Schaal, Levine, International Conference on Robotics and Automation (ICRA), 2022. Reach Pick &amp; Place Insert (Pegs) Insert (Gears). Tony Z Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevce- viciute, Nicolas Heess, Jon Scholz, Stefan Schaal, and Sergey Levine. Offline meta-reinforcement learning for industrial insertion. In International Conference on Robotics and Automation (ICRA), 2022. Reach Pick &amp; Place Insert (Pegs) Insert (Gears)</p>
<p>. Y-Axis Hand, -0.35, 0.35] m Hand Y-axis [-0.35, 0.35] m Plug to socket ∆Z [0, 20] mm Gear to shaft ∆Z [0, 20Hand Y-axis [-0.35, 0.35] m Hand Y-axis [-0.35, 0.35] m Plug to socket ∆Z [0, 20] mm Gear to shaft ∆Z [0, 20] mm</p>
<p>Ranges for initial pose randomization and observation noise during training. Values were uniformly sampled. Table, Vii, Physics Observations Actions Low-Level Policy Low-Level Policy. TABLE VII: Ranges for initial pose randomization and observation noise during training. Values were uniformly sampled. Physics Observations Actions Low-Level Policy Low-Level Policy</p>
<p>. Pos. Err. (mm) Rot. Err. (rad) Success (%) Engage (%) Pos. Err. (mm) Rot. Err. radPos. Err. (mm) Rot. Err. (rad) Success (%) Engage (%) Pos. Err. (mm) Rot. Err. (rad)</p>
<p>SDF-Based Reward, and Sampling-Based Curriculum. (A) Pegs and Holes assembly Insert policy. (B) Gears and Gearshafts assembly Insert policy. Engage denotes partial insertion. Policies were trained with moderate randomization (i.e., plug randomization of ± 10 mm, socket randomization of ± 10 cm, and observation noise of ± 1 mm); thus, the table evaluates in-distribution and out-of-distribution performance. Each test was executed on 5 seeds, with 1000 trials each. Table X, Joint evaluation of Simulation-Based Policy Update. Reach Pick Place Insert (Pegs) Insert (GearsTABLE X: Joint evaluation of Simulation-Based Policy Update, SDF-Based Reward, and Sampling-Based Curriculum. (A) Pegs and Holes assembly Insert policy. (B) Gears and Gearshafts assembly Insert policy. Engage denotes partial insertion. Policies were trained with moderate randomization (i.e., plug randomization of ± 10 mm, socket randomization of ± 10 cm, and observation noise of ± 1 mm); thus, the table evaluates in-distribution and out-of-distribution performance. Each test was executed on 5 seeds, with 1000 trials each. Reach Pick Place Insert (Pegs) Insert (Gears)</p>
<p>Insert Pegs &amp; Connectors Rot. Action Scale. 0.001, 0.001, 0.001] Insert Pegs Leaky PLAI Threshold [0.05, 0.05, 0.03Insert Pegs &amp; Connectors Controller Gains. 100Pick &amp; Place All Rot. Insert Connectors Leaky PLAI Threshold [0.04, 0.04, 0.05] Insert Gears Controller Gains [300, 300, 300, 50, 50, 50] Insert Gears Pos. Action Scale [0.0005, 0.0005, 0.0004] Insert Gears Rot. Action Scale [0.001, 0.001, 0.001] Insert Gears Leaky PLAI Threshold [0.05, 0.05, 0.05] Insert All Observation Noise XY-axes: [-2, 2] mm Insert All Pos. Randomization XY-axes: [-10, 10] mm Insert All Height Randomization [10, 20] mm above socket/shaft Insert All Rot. Randomization Yaw: [-5, 5] deg Insert All Clearance [0.5, 0.6] mmPick &amp; Place All Controller Gains [1000, 1000, 1000, 50, 50, 50] Pick &amp; Place All Pos. Action Scale [0.002, 0.002, 0.0015] Pick &amp; Place All Rot. Action Scale [0.004, 0.004, 0.004] Insert Pegs &amp; Connectors Controller Gains [1000, 1000, 100, 50, 50, 50] Insert Pegs Pos. Action Scale [0.0006, 0.0006, 0.0004] Insert Connectors Pos. Action Scale [0.0004, 0.0004, 0.0004] Insert Pegs &amp; Connectors Rot. Action Scale [0.001, 0.001, 0.001] Insert Pegs Leaky PLAI Threshold [0.05, 0.05, 0.03] Insert Connectors Leaky PLAI Threshold [0.04, 0.04, 0.05] Insert Gears Controller Gains [300, 300, 300, 50, 50, 50] Insert Gears Pos. Action Scale [0.0005, 0.0005, 0.0004] Insert Gears Rot. Action Scale [0.001, 0.001, 0.001] Insert Gears Leaky PLAI Threshold [0.05, 0.05, 0.05] Insert All Observation Noise XY-axes: [-2, 2] mm Insert All Pos. Randomization XY-axes: [-10, 10] mm Insert All Height Randomization [10, 20] mm above socket/shaft Insert All Rot. Randomization Yaw: [-5, 5] deg Insert All Clearance [0.5, 0.6] mm</p>
<p>Real-world evaluation parameters for Insert. Action scales are scalars applied to position and rotation actions in order to bring robot execution speeds to within comfortable limits. Table Xii, TABLE XII: Real-world evaluation parameters for Insert. Action scales are scalars applied to position and rotation actions in order to bring robot execution speeds to within comfortable limits.</p>            </div>
        </div>

    </div>
</body>
</html>
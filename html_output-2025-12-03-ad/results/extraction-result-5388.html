<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5388 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5388</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5388</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-c7ae496ae37b5c1fca5c0f7e277bc9356172efd1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c7ae496ae37b5c1fca5c0f7e277bc9356172efd1" target="_blank">PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work shows how a small language model could be trained to act as a verifier module for the output of an LLM~ and to iteratively improve its performance via fine-grained corrective instructions, and proposes a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5388.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5388.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>triple-linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearised Knowledge Graph (triples serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple text serialization of a semantic graph as a list of triples (or quadruplets for sub-properties) in the format [subject, predicate, object], used as the textual representation for both prompting LLMs and as input/output for sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearized triples list</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is converted to a flat, linear string/list by writing each triple as [subject, relation, object] (if a triple has a sub-property it is represented as a quadruplet). The linearized string is used directly in prompts to LLMs and as the input sequence for seq2seq models; linearized graphs can be parsed back into directed graphs (the authors re-construct graphs via NetworkX and compare node/edge attributes for exact matching).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graph / semantic graph (sets of RDF-style triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact, human-readable, and straightforward to feed to text-only models; preserves explicit node and edge attributes (triples). Sensitive to lexicalization and surface-form variations (causes low exact-match graph scores); order and formatting variations can reduce strict exact-match metrics (G-F1). Works well with token-based sequence models but can lose structural invariants (head/tail order mistakes are hard to detect downstream unless extra checks are used).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as input/output for Text->Graph (T2G) prompting and Graph->Text (G2T) generation tasks; evaluated in T2G experiments (LLM few-shot prompting) and used as training input for seq2seq G2T fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>T2G evaluation used Triple Match F1 (T-F1), Graph Match F1 (G-F1), G-BERTScore (G-BS), and Graph Edit Distance (GED). Example T2G baseline (ChatGPT) on KELM-sub: T-F1 13.50, G-F1 4.89, G-BS 83.92, GED 13.20 (Table 2). For G2T (fine-tuned T5 on linearized graphs) measured BLEU/METEOR/TER/BERTScore (see GenWiki-HIQ results in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Linearized triple lists are the primary textual representation used in this paper; compared implicitly to raw unstructured text (low overlap) and to models fine-tuned on parallel data. The paper shows that straightforward linearization plus a verifier/augmentation pipeline (PiVe) substantially improves downstream matching scores compared to using raw non-parallel graphs/text alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Exact-match evaluation is brittle to wording/format differences; linearization alone does not correct missing triples or semantic mismatches between graph and text. Verifier modules trained on omission perturbations are needed to address frequent 'missing-triple' errors by LLMs. The verifier approach still cannot detect head/tail swaps in triples (order errors) under the current training heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5388.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PiVe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting with Iterative Verification (PiVe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative framework that trains a small verifier model on perturbed graph-text pairs to detect missing/incorrect triples and uses its corrective outputs to iteratively prompt or offline-correct a large LLM's graph outputs, improving text↔graph conversion quality and enabling data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>verifier-augmented iterative graph augmentation / correction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train a verifier LM (T5 variants) on (text, perturbed_graph) -> corrective instruction (IP) pairs where perturbed graphs have triples omitted or heuristically removed. At inference, take an LLM-generated graph and the source text, run the verifier to predict either 'Correct' or the missing triple(s). Either (a) Iterative Prompting: append verifier predictions as explicit triples in the LLM prompt and re-query the LLM, repeating until verifier outputs 'Correct' or max iterations reached; or (b) Iterative Offline Correction: call the LLM once for an initial graph, then let the verifier iteratively append corrections offline (no more LLM API calls).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graph / semantic graphs (triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Increases coverage/overlap between text and graph; allows fine-grained corrective instructions (missing-triple additions); scalable self-supervised verifier training via perturbations; enables generating improved parallel text-graph pairs from non-parallel corpora; trade-off between cost (multiple LLM calls) and quality (iterative prompting improves more than offline correction).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Applied to Text->Graph (T2G) generation (improving LLM few-shot outputs) and used as a data-augmentation pipeline to create higher-quality parallel Graph->Text (G2T) training data (GenWiki-HIQ). Evaluations include T2G metrics (T-F1, G-F1, G-BS, GED) and G2T/text similarity metrics (BLEU, METEOR, TER, BERTScore) when validating augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PiVe improves LLM T2G scores substantially: e.g., on KELM-sub average T-F1 improved from Base 13.50 to 23.11 after 3 iterations using unified verifier (Table 2). The paper reports average improvement of ~26% across 3 datasets. As a data-augmentation method (GenWiki iterative augmentation) BLEU increased from 4.75 (Base) to 15.93 (Iteration 3), METEOR 18.02 -> 31.05, TER 80.72 -> 72.01, BERTScore 89.47 -> 92.14 (Table 5). Training T5-large G2T on the augmented GenWiki-HIQ yields BLEU 48.17 vs 35.71 for the filtered seed (Table 6), METEOR 42.03 vs 36.67, TER 41.94 vs 65.19, BERTScore 95.44 vs 93.74.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against (a) no-verifier LLM prompting (Base): PiVe consistently improves metrics across datasets; (b) Iterative Offline Correction: iterative prompting (re-querying LLM with verifier-suggested triples) performs better than offline correction (Table 3); (c) SelfRefine (LLM self-feedback): PiVe outperforms SelfRefine baselines which often degrade performance (Table 4); (d) fully fine-tuned seq2seq models: fine-tuned T5 still exceeds few-shot LLM outputs, but PiVe narrows the gap and provides a low-cost way to improve black-box LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>PiVe focuses on missing-triple errors and is insensitive to head/tail order mistakes under current verifier training; verifier can inject redundant or inaccurate triples (observed error modes: redundancy, inaccuracy); iterative prompting incurs API cost (they cap iterations at 3); quality of verifier depends strongly on perturbation strategy—incorrect perturbations (e.g., entity-level swaps) can lead to wrong corrections (Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5388.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>perturbation-omission-verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perturbation-based verifier training (omission & heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised data construction method that perturbs correct graphs (primarily by omitting triples or heuristically removing triples whose subject/object are absent in the text) to create training examples for a verifier model that predicts missing triples or 'Correct'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>perturbed-graph -> corrective-instruction text pairs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given a seed parallel (text, graph) dataset, create positive examples (text + full graph -> 'Correct') and perturbed examples (text + graph with one triple omitted -> target = missing triple). Two perturbation variants: (1) Random omission (drop a triple chosen at random when graph has >1 triple); (2) Heuristic omission (drop triples whose subject and object do not appear in the corresponding text). The resulting (T, perturbed_G) -> IP datasets train a verifier that outputs either 'Correct' or the missing triple.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>semantic graphs / knowledge graphs (triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Highly scalable (authors claim O(|D|*|E|*|R|*|Tr|) possible perturbations by deletion); focuses verifier training on the most common LLM failure mode (missing triples); produces compact corrective textual targets (a triple or the token 'Correct') making verifier a text-to-text model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Verifier effectiveness is evaluated as part of the T2G pipeline (how many missing triples it finds and how corrective suggestions improve LLM outputs). Also ablated in Appendix C comparing alternative perturbations (head/relation/tail swaps) on KELM-sub.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Omission perturbation used to train verifier yields largest T2G improvements (see main results). Alternative perturbations (head/relation/tail swaps) showed mixed/negative effects: relation perturbation marginally increased T-F1 but often decreased G-BS and could introduce wrong corrections (Table 11). Concrete T2G gains with omission-based verifier appear in Table 3/Table 2 (e.g., unified verifier + iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared multiple perturbation strategies: omission-based perturbation is preferred over entity/relation swap perturbations, which sometimes degrade graph-BERTScore and inject errors. The paper recommends omission/heuristic omission as the effective perturbation scheme for verifier training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Perturbations must match real LLM error distribution; entity/relation/tail perturbations are less aligned with LLM mistakes and can train verifiers that produce incorrect corrections. The verifier trained on omissions cannot detect triple-head/tail swaps or subtle relation lexicalization mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5388.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-G2T-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based graph-to-text prompting (zero-shot / few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using large pre-trained LLMs (e.g., Flan-T5-XL zero-shot, ChatGPT, GPT-4) prompted with instructions like 'Transform the semantic graph into a description.' to convert linearized graphs into natural language text without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>prompted graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Provide the linearized triple list as input and a short instruction (zero-shot or with few-shot demonstrations) and let the LLM generate a textual description. Different prompt variants were explored (simple transform prompt, a prompt that asks to generate as many triples as possible, and a two-step chain-of-thought style prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>semantic graphs / knowledge graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>No additional fine-tuning required; performance sensitive to prompt style and number of shots; lower quality than fine-tuned seq2seq models but useful for quick evaluation and human-in-the-loop augmentation; can be used in zero-shot to generate descriptions for quality evaluation of augmented graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph->Text (G2T) generation evaluated with BLEU, METEOR, TER, BERTScore. Also used to assess the quality of augmented graphs (higher similarity between generated description and original text indicates better graph-text overlap).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Flan-T5-XL zero-shot used during GenWiki augmentation to produce descriptions for evaluation: iterative augmentation raised BLEU from 4.75 to 15.93 and BERTScore from 89.47 to 92.14 across iterations (Table 5). Prompt variants and model variants (ChatGPT vs GPT-4) show differing behavior (Figure 3/Table 14/15).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared prompting styles (Prompt1/2/3) and LLM backbones: GPT-4 substantially outperforms ChatGPT in T2G/G2T tasks; prompt design effects vary per model (Prompt2 best on ChatGPT, Prompt1 best on GPT-4). Fine-tuned seq2seq models still outperform zero/few-shot LLM prompting when sufficient parallel data is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5388.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large-G2T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-Large fine-tuned for Graph->Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard encoder-decoder seq2seq approach: T5-Large is fine-tuned on parallel linearized graph->text pairs to generate descriptions from graphs, producing strong G2T quality when parallel data is available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>fine-tuned seq2seq on linearized graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearized triple lists are used as the input sequence to the T5 encoder and the text description as the decoder target; the model is fully fine-tuned on the parallel data (hyperparameters provided in Appendix A).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graph / semantic graph (triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>High performance when trained on sufficiently large, high-quality parallel data (expressive, captures lexicalization patterns), but requires parallel text-graph pairs and associated annotation/collection costs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph->Text (G2T) evaluated on the human-annotated GenWiki test set using BLEU, METEOR, TER, and BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>T5-large trained on GenWiki-HIQ achieves BLEU 48.17, METEOR 42.03, TER 41.94, BERTScore 95.44 on the human-annotated GenWiki test set; by contrast the same model trained on the filtered seed (GenWiki_FINE-f) scores BLEU 35.71, METEOR 36.67, TER 65.19, BERTScore 93.74 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Fine-tuned T5 substantially outperforms few-shot LLM prompting; PiVe's augmentation improves the parallel training data such that fine-tuned T5 benefits significantly (training on GenWiki-HIQ > GenWiki_FINE-f).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5388.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfRefine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where an LLM critiques and refines its own outputs iteratively (self-feedback loop) rather than relying on an external verifier; used as a baseline in this paper and found to be ineffective for structured graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM self-feedback iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The LLM is prompted to assess and improve its own previous output in multiple iterations (no external verifier). In this paper, the authors applied SelfRefine as a baseline for T2G on KELM-sub.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>semantic graphs / knowledge graphs (triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Relies entirely on the LLM's internal ability to judge and correct structured outputs; tends to fail when LLMs lack structured-data training signal and thus produce poor critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Applied to Text->Graph (T2G) generation on KELM-sub and evaluated with T-F1, G-F1, G-BS, GED.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 4 shows SelfRefine results on KELM-sub: Base T-F1 13.50 -> Iteration1 14.54 (small increase) but G-F1 and GED degrade with further iterations (Iteration3 T-F1 13.79, G-F1 2.28, G-BS 84.20, GED 15.18), demonstrating instability and overall poorer performance than PiVe.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>PiVe (external trained verifier + iterative prompting) outperforms SelfRefine; authors argue LLMs are not trained rigorously on structured data and therefore self-feedback is less reliable than a small verifier trained specifically for detecting and correcting structured-graph errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5388.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INFINITY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INFINITY: Unsupervised graph-text mutual conversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised framework for mutual conversion between graphs and text cited as related work; mentioned but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>INFINITY: A simple yet effective unsupervised framework for graph-text mutual conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>unsupervised graph-text mutual conversion (INFINITY)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Prior work (Xu et al., 2022) proposing unsupervised techniques to convert between graphs and text without parallel supervision. The current paper cites it in related work but does not use it experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graphs / semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unsupervised, designed to work without parallel text-graph data; complements supervised/augmentation approaches but not directly compared in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General graph↔text conversion tasks (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Mentioned as an alternative unsupervised approach in the literature; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5388.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5388.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycLeGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CycLeGT: Unsupervised cycle training for graph↔text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised cycle-training approach for graph-to-text and text-to-graph conversion cited as prior work (Guo et al., 2020); mentioned but not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>cycle-trained graph↔text conversion</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Prior method using cycle consistency between graph-to-text and text-to-graph models to learn conversions without parallel supervision; cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graphs / semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unsupervised, imposes cycle-consistency; useful when parallel data is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph-to-text and text-to-graph generation benchmarks in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Cited as relevant prior approach to mutual conversion; no experimental comparison within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from rdf data <em>(Rating: 2)</em></li>
                <li>INFINITY: A simple yet effective unsupervised framework for graph-text mutual conversion <em>(Rating: 2)</em></li>
                <li>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5388",
    "paper_id": "paper-c7ae496ae37b5c1fca5c0f7e277bc9356172efd1",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "triple-linearization",
            "name_full": "Linearised Knowledge Graph (triples serialization)",
            "brief_description": "A simple text serialization of a semantic graph as a list of triples (or quadruplets for sub-properties) in the format [subject, predicate, object], used as the textual representation for both prompting LLMs and as input/output for sequence models.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "linearized triples list",
            "representation_description": "Each graph is converted to a flat, linear string/list by writing each triple as [subject, relation, object] (if a triple has a sub-property it is represented as a quadruplet). The linearized string is used directly in prompts to LLMs and as the input sequence for seq2seq models; linearized graphs can be parsed back into directed graphs (the authors re-construct graphs via NetworkX and compare node/edge attributes for exact matching).",
            "graph_type": "knowledge graph / semantic graph (sets of RDF-style triples)",
            "representation_properties": "Compact, human-readable, and straightforward to feed to text-only models; preserves explicit node and edge attributes (triples). Sensitive to lexicalization and surface-form variations (causes low exact-match graph scores); order and formatting variations can reduce strict exact-match metrics (G-F1). Works well with token-based sequence models but can lose structural invariants (head/tail order mistakes are hard to detect downstream unless extra checks are used).",
            "evaluation_task": "Used as input/output for Text-&gt;Graph (T2G) prompting and Graph-&gt;Text (G2T) generation tasks; evaluated in T2G experiments (LLM few-shot prompting) and used as training input for seq2seq G2T fine-tuning.",
            "performance_metrics": "T2G evaluation used Triple Match F1 (T-F1), Graph Match F1 (G-F1), G-BERTScore (G-BS), and Graph Edit Distance (GED). Example T2G baseline (ChatGPT) on KELM-sub: T-F1 13.50, G-F1 4.89, G-BS 83.92, GED 13.20 (Table 2). For G2T (fine-tuned T5 on linearized graphs) measured BLEU/METEOR/TER/BERTScore (see GenWiki-HIQ results in Table 6).",
            "comparison_to_other_representations": "Linearized triple lists are the primary textual representation used in this paper; compared implicitly to raw unstructured text (low overlap) and to models fine-tuned on parallel data. The paper shows that straightforward linearization plus a verifier/augmentation pipeline (PiVe) substantially improves downstream matching scores compared to using raw non-parallel graphs/text alone.",
            "limitations_or_challenges": "Exact-match evaluation is brittle to wording/format differences; linearization alone does not correct missing triples or semantic mismatches between graph and text. Verifier modules trained on omission perturbations are needed to address frequent 'missing-triple' errors by LLMs. The verifier approach still cannot detect head/tail swaps in triples (order errors) under the current training heuristics.",
            "uuid": "e5388.0",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PiVe",
            "name_full": "Prompting with Iterative Verification (PiVe)",
            "brief_description": "An iterative framework that trains a small verifier model on perturbed graph-text pairs to detect missing/incorrect triples and uses its corrective outputs to iteratively prompt or offline-correct a large LLM's graph outputs, improving text↔graph conversion quality and enabling data augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "verifier-augmented iterative graph augmentation / correction",
            "representation_description": "Train a verifier LM (T5 variants) on (text, perturbed_graph) -&gt; corrective instruction (IP) pairs where perturbed graphs have triples omitted or heuristically removed. At inference, take an LLM-generated graph and the source text, run the verifier to predict either 'Correct' or the missing triple(s). Either (a) Iterative Prompting: append verifier predictions as explicit triples in the LLM prompt and re-query the LLM, repeating until verifier outputs 'Correct' or max iterations reached; or (b) Iterative Offline Correction: call the LLM once for an initial graph, then let the verifier iteratively append corrections offline (no more LLM API calls).",
            "graph_type": "knowledge graph / semantic graphs (triples)",
            "representation_properties": "Increases coverage/overlap between text and graph; allows fine-grained corrective instructions (missing-triple additions); scalable self-supervised verifier training via perturbations; enables generating improved parallel text-graph pairs from non-parallel corpora; trade-off between cost (multiple LLM calls) and quality (iterative prompting improves more than offline correction).",
            "evaluation_task": "Applied to Text-&gt;Graph (T2G) generation (improving LLM few-shot outputs) and used as a data-augmentation pipeline to create higher-quality parallel Graph-&gt;Text (G2T) training data (GenWiki-HIQ). Evaluations include T2G metrics (T-F1, G-F1, G-BS, GED) and G2T/text similarity metrics (BLEU, METEOR, TER, BERTScore) when validating augmentation.",
            "performance_metrics": "PiVe improves LLM T2G scores substantially: e.g., on KELM-sub average T-F1 improved from Base 13.50 to 23.11 after 3 iterations using unified verifier (Table 2). The paper reports average improvement of ~26% across 3 datasets. As a data-augmentation method (GenWiki iterative augmentation) BLEU increased from 4.75 (Base) to 15.93 (Iteration 3), METEOR 18.02 -&gt; 31.05, TER 80.72 -&gt; 72.01, BERTScore 89.47 -&gt; 92.14 (Table 5). Training T5-large G2T on the augmented GenWiki-HIQ yields BLEU 48.17 vs 35.71 for the filtered seed (Table 6), METEOR 42.03 vs 36.67, TER 41.94 vs 65.19, BERTScore 95.44 vs 93.74.",
            "comparison_to_other_representations": "Compared against (a) no-verifier LLM prompting (Base): PiVe consistently improves metrics across datasets; (b) Iterative Offline Correction: iterative prompting (re-querying LLM with verifier-suggested triples) performs better than offline correction (Table 3); (c) SelfRefine (LLM self-feedback): PiVe outperforms SelfRefine baselines which often degrade performance (Table 4); (d) fully fine-tuned seq2seq models: fine-tuned T5 still exceeds few-shot LLM outputs, but PiVe narrows the gap and provides a low-cost way to improve black-box LLMs.",
            "limitations_or_challenges": "PiVe focuses on missing-triple errors and is insensitive to head/tail order mistakes under current verifier training; verifier can inject redundant or inaccurate triples (observed error modes: redundancy, inaccuracy); iterative prompting incurs API cost (they cap iterations at 3); quality of verifier depends strongly on perturbation strategy—incorrect perturbations (e.g., entity-level swaps) can lead to wrong corrections (Appendix C).",
            "uuid": "e5388.1",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "perturbation-omission-verifier",
            "name_full": "Perturbation-based verifier training (omission & heuristic)",
            "brief_description": "A self-supervised data construction method that perturbs correct graphs (primarily by omitting triples or heuristically removing triples whose subject/object are absent in the text) to create training examples for a verifier model that predicts missing triples or 'Correct'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "perturbed-graph -&gt; corrective-instruction text pairs",
            "representation_description": "Given a seed parallel (text, graph) dataset, create positive examples (text + full graph -&gt; 'Correct') and perturbed examples (text + graph with one triple omitted -&gt; target = missing triple). Two perturbation variants: (1) Random omission (drop a triple chosen at random when graph has &gt;1 triple); (2) Heuristic omission (drop triples whose subject and object do not appear in the corresponding text). The resulting (T, perturbed_G) -&gt; IP datasets train a verifier that outputs either 'Correct' or the missing triple.",
            "graph_type": "semantic graphs / knowledge graphs (triples)",
            "representation_properties": "Highly scalable (authors claim O(|D|*|E|*|R|*|Tr|) possible perturbations by deletion); focuses verifier training on the most common LLM failure mode (missing triples); produces compact corrective textual targets (a triple or the token 'Correct') making verifier a text-to-text model.",
            "evaluation_task": "Verifier effectiveness is evaluated as part of the T2G pipeline (how many missing triples it finds and how corrective suggestions improve LLM outputs). Also ablated in Appendix C comparing alternative perturbations (head/relation/tail swaps) on KELM-sub.",
            "performance_metrics": "Omission perturbation used to train verifier yields largest T2G improvements (see main results). Alternative perturbations (head/relation/tail swaps) showed mixed/negative effects: relation perturbation marginally increased T-F1 but often decreased G-BS and could introduce wrong corrections (Table 11). Concrete T2G gains with omission-based verifier appear in Table 3/Table 2 (e.g., unified verifier + iterations).",
            "comparison_to_other_representations": "Compared multiple perturbation strategies: omission-based perturbation is preferred over entity/relation swap perturbations, which sometimes degrade graph-BERTScore and inject errors. The paper recommends omission/heuristic omission as the effective perturbation scheme for verifier training.",
            "limitations_or_challenges": "Perturbations must match real LLM error distribution; entity/relation/tail perturbations are less aligned with LLM mistakes and can train verifiers that produce incorrect corrections. The verifier trained on omissions cannot detect triple-head/tail swaps or subtle relation lexicalization mismatches.",
            "uuid": "e5388.2",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-G2T-prompt",
            "name_full": "LLM-based graph-to-text prompting (zero-shot / few-shot)",
            "brief_description": "Using large pre-trained LLMs (e.g., Flan-T5-XL zero-shot, ChatGPT, GPT-4) prompted with instructions like 'Transform the semantic graph into a description.' to convert linearized graphs into natural language text without fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "prompted graph-to-text generation",
            "representation_description": "Provide the linearized triple list as input and a short instruction (zero-shot or with few-shot demonstrations) and let the LLM generate a textual description. Different prompt variants were explored (simple transform prompt, a prompt that asks to generate as many triples as possible, and a two-step chain-of-thought style prompt).",
            "graph_type": "semantic graphs / knowledge graph triples",
            "representation_properties": "No additional fine-tuning required; performance sensitive to prompt style and number of shots; lower quality than fine-tuned seq2seq models but useful for quick evaluation and human-in-the-loop augmentation; can be used in zero-shot to generate descriptions for quality evaluation of augmented graphs.",
            "evaluation_task": "Graph-&gt;Text (G2T) generation evaluated with BLEU, METEOR, TER, BERTScore. Also used to assess the quality of augmented graphs (higher similarity between generated description and original text indicates better graph-text overlap).",
            "performance_metrics": "Flan-T5-XL zero-shot used during GenWiki augmentation to produce descriptions for evaluation: iterative augmentation raised BLEU from 4.75 to 15.93 and BERTScore from 89.47 to 92.14 across iterations (Table 5). Prompt variants and model variants (ChatGPT vs GPT-4) show differing behavior (Figure 3/Table 14/15).",
            "comparison_to_other_representations": "Compared prompting styles (Prompt1/2/3) and LLM backbones: GPT-4 substantially outperforms ChatGPT in T2G/G2T tasks; prompt design effects vary per model (Prompt2 best on ChatGPT, Prompt1 best on GPT-4). Fine-tuned seq2seq models still outperform zero/few-shot LLM prompting when sufficient parallel data is available.",
            "uuid": "e5388.3",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "T5-large-G2T",
            "name_full": "T5-Large fine-tuned for Graph-&gt;Text generation",
            "brief_description": "A standard encoder-decoder seq2seq approach: T5-Large is fine-tuned on parallel linearized graph-&gt;text pairs to generate descriptions from graphs, producing strong G2T quality when parallel data is available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "fine-tuned seq2seq on linearized graphs",
            "representation_description": "Linearized triple lists are used as the input sequence to the T5 encoder and the text description as the decoder target; the model is fully fine-tuned on the parallel data (hyperparameters provided in Appendix A).",
            "graph_type": "knowledge graph / semantic graph (triples)",
            "representation_properties": "High performance when trained on sufficiently large, high-quality parallel data (expressive, captures lexicalization patterns), but requires parallel text-graph pairs and associated annotation/collection costs.",
            "evaluation_task": "Graph-&gt;Text (G2T) evaluated on the human-annotated GenWiki test set using BLEU, METEOR, TER, and BERTScore.",
            "performance_metrics": "T5-large trained on GenWiki-HIQ achieves BLEU 48.17, METEOR 42.03, TER 41.94, BERTScore 95.44 on the human-annotated GenWiki test set; by contrast the same model trained on the filtered seed (GenWiki_FINE-f) scores BLEU 35.71, METEOR 36.67, TER 65.19, BERTScore 93.74 (Table 6).",
            "comparison_to_other_representations": "Fine-tuned T5 substantially outperforms few-shot LLM prompting; PiVe's augmentation improves the parallel training data such that fine-tuned T5 benefits significantly (training on GenWiki-HIQ &gt; GenWiki_FINE-f).",
            "uuid": "e5388.4",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SelfRefine",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "A method where an LLM critiques and refines its own outputs iteratively (self-feedback loop) rather than relying on an external verifier; used as a baseline in this paper and found to be ineffective for structured graph generation.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "use",
            "representation_name": "LLM self-feedback iterative refinement",
            "representation_description": "The LLM is prompted to assess and improve its own previous output in multiple iterations (no external verifier). In this paper, the authors applied SelfRefine as a baseline for T2G on KELM-sub.",
            "graph_type": "semantic graphs / knowledge graphs (triples)",
            "representation_properties": "Relies entirely on the LLM's internal ability to judge and correct structured outputs; tends to fail when LLMs lack structured-data training signal and thus produce poor critiques.",
            "evaluation_task": "Applied to Text-&gt;Graph (T2G) generation on KELM-sub and evaluated with T-F1, G-F1, G-BS, GED.",
            "performance_metrics": "Table 4 shows SelfRefine results on KELM-sub: Base T-F1 13.50 -&gt; Iteration1 14.54 (small increase) but G-F1 and GED degrade with further iterations (Iteration3 T-F1 13.79, G-F1 2.28, G-BS 84.20, GED 15.18), demonstrating instability and overall poorer performance than PiVe.",
            "comparison_to_other_representations": "PiVe (external trained verifier + iterative prompting) outperforms SelfRefine; authors argue LLMs are not trained rigorously on structured data and therefore self-feedback is less reliable than a small verifier trained specifically for detecting and correcting structured-graph errors.",
            "uuid": "e5388.5",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "INFINITY",
            "name_full": "INFINITY: Unsupervised graph-text mutual conversion",
            "brief_description": "An unsupervised framework for mutual conversion between graphs and text cited as related work; mentioned but not used in experiments.",
            "citation_title": "INFINITY: A simple yet effective unsupervised framework for graph-text mutual conversion.",
            "mention_or_use": "mention",
            "representation_name": "unsupervised graph-text mutual conversion (INFINITY)",
            "representation_description": "Prior work (Xu et al., 2022) proposing unsupervised techniques to convert between graphs and text without parallel supervision. The current paper cites it in related work but does not use it experimentally.",
            "graph_type": "knowledge graphs / semantic graphs",
            "representation_properties": "Unsupervised, designed to work without parallel text-graph data; complements supervised/augmentation approaches but not directly compared in experiments here.",
            "evaluation_task": "General graph↔text conversion tasks (cited as related work).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Mentioned as an alternative unsupervised approach in the literature; no direct empirical comparison in this paper.",
            "uuid": "e5388.6",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CycLeGT",
            "name_full": "CycLeGT: Unsupervised cycle training for graph↔text",
            "brief_description": "An unsupervised cycle-training approach for graph-to-text and text-to-graph conversion cited as prior work (Guo et al., 2020); mentioned but not used experimentally in this paper.",
            "citation_title": "Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training.",
            "mention_or_use": "mention",
            "representation_name": "cycle-trained graph↔text conversion",
            "representation_description": "Prior method using cycle consistency between graph-to-text and text-to-graph models to learn conversions without parallel supervision; cited in related work.",
            "graph_type": "knowledge graphs / semantic graphs",
            "representation_properties": "Unsupervised, imposes cycle-consistency; useful when parallel data is scarce.",
            "evaluation_task": "Graph-to-text and text-to-graph generation benchmarks in prior work.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Cited as relevant prior approach to mutual conversion; no experimental comparison within this paper.",
            "uuid": "e5388.7",
            "source_info": {
                "paper_title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "The webnlg challenge: Generating text from rdf data",
            "rating": 2
        },
        {
            "paper_title": "INFINITY: A simple yet effective unsupervised framework for graph-text mutual conversion",
            "rating": 2
        },
        {
            "paper_title": "Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "rating": 1
        }
    ],
    "cost": 0.021114,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs</h1>
<p>Jiuzhou Han ${ }^{2}$ Nigel Collier ${ }^{2}$ Wray Buntine ${ }^{3}$ Ehsan Shareghi ${ }^{2}$<br>${ }^{2}$ Department of Data Science \&amp; AI, Monash University<br>${ }^{3}$ College of Engineering and Computer Science, VinUniversity<br>${ }^{2}$ Language Technology Lab, University of Cambridge<br>jiuzhou.han@monash.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pretraining data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graphbased generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM (i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) like ChatGPT and GPT-4 (OpenAI, 2023) have been quite successful in solving different generative and reasoning tasks. The combination of their abilities in leveraging in-context learning as well as instruction following have unlocked new state-of-the-art results across the natural language processing (NLP) field. The existing LLMs are mostly pre-trained on a huge volume of unstructured data from the internet including books, articles, webtexts, repositories, Wikipedia, etc. Training on unstructured data naturally leads to relatively poor performance when dealing with tasks that demand organizing text into structured machine-readable format.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Framework of PiVe.</p>
<p>A semantic graph, as a form of graph-structured data, stores information in a machine-accessible way (van Harmelen et al., 2008). Generating a semantic graph from text is known as text-tograph (T2G) generation and is previously attempted mostly by fine-tuning small language models ( Xu et al., 2022; Guo et al., 2020). However, generating graph-structured data remains a challenge for LLMs even in the presence of reasonable number of few-shot examples. In fact, regardless of the number of few-shot examples or prompting style the outputs from LLMs (e.g., GPT-3.5) still contain errors and require correction (§4.4).</p>
<p>In this paper, we focus on how to improve the graph-based generative capability of LLMs. To this end, we propose the Prompting through Iterative Verification (PiVe) framework shown in Figure 1. Specially, PiVe involves leveraging an external verifier module (i.e., a much smaller LM) and incorporating the feedback from verifier module into the prompt. PiVe iteratively utilises the verifier module and refines the prompts, via corrective instructions, before sending them back into the LLM, leading to substantially improved quality of the generated semantic graphs.</p>
<p>In particular, to train the verifier modules, we start from a seed dataset of text and graph (T,G) pairs, and construct an arbitrarily large graphperturbation dataset via a simple procedure which takes any graph $G$ from the seed set and perturbs it arbitrarily on its entities (E), relations (R), or</p>
<p>triples $(\operatorname{Tr})$. The text and perturbed graph (G), along with a corrective description to invert the applied perturbation (IP) form a verification dataset of (T,G,IP) triples which serve as the training data for self-supervised learning of our verifier module. The verification dataset could be as large as desired (i.e., for any seed dataset D, containing graphs of $|\mathrm{E}|$ entities, $|\mathrm{R}|$ relations, $|\mathrm{Tr}|$ triples, it could produce $\mathcal{O}(|D| \times|E| \times|R| \times|\operatorname{Tr}|)$ perturbations only by deleting. ${ }^{2}$ We then devise fine-tuning and instruction-tuning to train domain-specific and unified verifiers, respectively.</p>
<p>During the T2G generation via the LLM (e.g., in the zero-shot setting "Transform the text into a semantic graph: Text: ... Graph:"), the verifier takes the text T, the output graph from the LLM, and sends a corrective signal to the LLM (e.g., "Transform the text into a semantic graph and add the given triples to the generated semantic graph: Text: ... Triples: ... Graph:"). This process continues till the verifier module verifies the output as correct and terminates. We refer to this as Iterative Prompting. Additionally, there is another (more cost effective) mode to the verifier module, which starts by calling the LLM once at the start to get an initial graph, and then the rest of the corrective steps are all applied step-by-step and iteratively through the verifier offline. We refer to this as Iterative Offline Correction.</p>
<p>Our extensive experiment results on three graphbased datasets demonstrate the effectiveness of the proposed PiVe framework in consistently improving the quality of the LLM output via providing iterative corrective guidance by an average of $26 \%$ across 3 datasets. We also create GenWiki-HIQ, a high-quality text-graph dataset and show how verifier module could be leveraged as a data augmentation technique to improve the quality of automatically constructed text-graph datasets.</p>
<h2>2 Basic Definitions</h2>
<p>A semantic graph is a network that represents semantic relations between entities. Each semantic graph has its corresponding verbalisation, and can have different textual representations. A set of triples (i.e., [subject, predicate, object]) represents a semantic graph. Given a text, the task of text-to-graph generation is to query an LLM to generate a semantic graph of the text. The semantic</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>graph should cover the information in the text as much as possible.</p>
<p>To prompt the LLM, we use few-shot by showing an example of T2G in the prompt to specify the expected format of the semantic graph (i.e., set of triples). We report experiments under various number of shots (§4.6). The basic form of instruction we use in the prompt is "Transform the text into a semantic graph." followed by a text and a semantic graph pair as a demonstration. We also show results under different prompting strategies (§4.7). Different demonstrations are used for different datasets to adapt to the style of different datasets. We show the used demonstrations in Appendix G.</p>
<h2>3 The PiVe Framework</h2>
<p>We first explain our training protocol for the verifier module (§3.1), and then present our framework of iterative verification prompting (§3.2).</p>
<h3>3.1 Verifier Module</h3>
<p>The quality of the generated semantic graph from LLM prompting could be quite poor. For instance the LLM often misses triples in the generated graph. In other words, some semantic relations between entities in the text are difficult to be captured for LLMs when they are generating a semantic graph. To detect the missing or incorrect parts of the generated semantic graph, we design a verifier module.</p>
<p>The verifier module is trained on a small pretrained LM (§4.2). A typical graph-based dataset contains parallel text and semantic graph (T, G) pairs. For different graph-based datasets, we use their corresponding training data for the seed dataset to create data for the verifier module. In particular, for each text-graph pair in the original dataset, we create one correct instance and one perturbed instance. We concatenate the text with graph using a separator token $&lt;\mathrm{S}&gt;$ and the target is to generate a specific output, denoted as IP, during training. For correct instances, the IP is simply the word "Correct". For perturbed instances, we have two methods to create them:</p>
<ul>
<li>Random method: if the graph contains more than one triple, we randomly omit one triple from it and concatenate the text with perturbed graph using a separated token $&lt;\mathrm{S}&gt;$. The target is to generate the missing part (e.g., triple Tr).</li>
<li>Heuristic method: Based on the observation that LLMs tend to miss the triples whose subject and object are not in the text, in addition</li>
</ul>
<p>to randomly omitting one triple from the graph we also omit the triple from the graph if subject and object of it are not in the text.</p>
<p>The output to generate for perturbed examples is the missing triple, Tr. By utilising these two methods, we can create an arbitrarily large verification dataset to train a verifier module which will be used at inference time during prompting the LLM.</p>
<h3>3.2 Iterative Prompting</h3>
<p>During the LLM prompting, the generated semantic graphs from LLMs is fed into the verifier module and the outputs from the verifier module is collected. If verifier generated "Correct" in its output, it means we do not need to make changes to the generated graph. Otherwise, the generated output from the verifier is added to the original prompt to create a new prompt. The new prompt is then used to query the LLM again. We repeat the whole process iteratively. The iteration process will stop when no missing triple is predicted or a maximum number of iterations is reached.</p>
<p>New Prompt Design As with the prompt used in the first iteration, we still provide an example in the new prompt for the subsequent iterations. The new prompt is "Transform the text into a semantic graph and also add the given triples to the generated semantic graph." In addition to the text, we also include some triples predicted by the verifier module which LLMs are likely to miss. This explicitly instructs the LLM to generate semantic graph and include the given triples. The given triple set contains the predicted missing triples from each iteration, which prevents the LLM from making the same mistakes as in previous iterations. See Appendix G for the used demonstrations.</p>
<h3>3.3 Iterative Offline Correction</h3>
<p>Similar to Iterative Prompting, the Offline Correction starts from the online LLM, but then continues with the step-by-step verification and correction steps offline. This approach is more cost effective as it relies only on one API call per instance (as opposed to several API calls of iterative prompting), however it is potentially weaker as it relies on the capability of the small verifier LM to both verify and apply the needed corrections. The offline correction stop under the same stopping criterion to Iterative Prompting.</p>
<h2>4 Experiments</h2>
<p>We describe the datasets and pre-processing method (§4.1), introduce the models and implementation details (§4.2) and the evaluation metrics (§4.3). In Subsection 4.4, we describe the main result from PiVe, and compare the two modes of verifier: Iterative Prompting vs. Iterative Offline Correction (§4.5). We then conduct various configurations of shots (§4.6), and prompting (§4.7). In Subsection 5, we show how PiVe could be used for data augmentation of automatically generated graph-text datasets (e.g., GenWiki).</p>
<h3>4.1 Datasets and Preprocessing</h3>
<p>We evaluate PiVe on three graph-based datasets, KELM (Agarwal et al., 2021), WebNLG+2020 (Gardent et al., 2017), GenWiki (Jin et al., 2020).</p>
<p>KELM is a large-scale synthetic corpus that consists of the English Wikidata KG and the corresponding natural text. It has $\sim 15 \mathrm{M}$ sentences synthetically generated using a fine-tuned T5 model. Each graph in KELM is a linearised KG containing a list of triples of the form [subject, relation, object]. If a triple has a sub-property, then it is quadruplet instead. We use a subset $(\sim 60 \mathrm{~K})$ of KELM which is named as KELM-sub. The creation of KELM-sub follows two criteria. We found that most graphs in KELM contain no more than six triples and only around 2,500 graphs contain more than six triples. Therefore, 1) we only consider the graphs with no more than six triples, and 2) we do not consider the graphs containing any triple with a sub-property. Based on these two criteria, for each size of graph (from one triple to six triples), we sampled equal number of ( $\uparrow, \overline{\mathrm{G}}$ ) pairs. In total, the created KELM-sub contains 60,000/1,800/1,800 samples as train/validation/test set.</p>
<p>WebNLG+2020 contains a set of triples extracted from DBpedia (Auer et al., 2007) in 16 distinct DBpedia categories and text description generated using diverse lexicalisation patterns. It contains $\sim 38 \mathrm{~K}$ graphs and each graph has at most three different descriptions.</p>
<p>GenWiki is a large-scale, general-domain dataset collected from general Wikipedia which contains 1.3 million non-parallel text and graphs with shared content. It has two versions: GenWiki ${ }<em _FINE="{FINE" _text="\text">{\text {FULL }}$ ( $\sim 1.3 \mathrm{M}$ ), and a fine version, GenWiki ${ }</em>)$, which adds constraints on the text and}}$ $(\sim 750 \mathrm{~K</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Train</th>
<th>Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td>KELM-sub</td>
<td>110,000</td>
<td>3,300</td>
</tr>
<tr>
<td>WebNLG &amp; GenWiki</td>
<td>70,630</td>
<td>2,500</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the seed datasets for training the verifier modules on three datasets.</p>
<p>graphs to force them to contain highly overlapped entity sets. Both datasets are collected in a scalable and automatic way. GenWiki also has a human-annotated test set of 1,000 parallel text-graph pairs with high quality. Since both GenWiki${}<em _text_FINE="\text{FINE">{\text{FULL}}$ and GenWiki${}</em>$ are non-parallel text and graphs datasets, we cannot use it to train the verifier module. However, the relation types in GenWiki and WebNLG have some overlaps, so we use the verifier module trained on WebNLG+2020 and test it on GenWiki test set.}</p>
<p>We use the method described in Section 3.1 to create the data for training the verifier module. Table 1 shows the statistics of the created training data on these three seed datasets.</p>
<h3>4.2 The LLM and Verifier Modules</h3>
<p>ChatGPT (gpt-3.5-turbo) is used as our default LLM to perform the T2G task. We also experiment with GPT-4 in Subsection 4.7. For verifier module, we use T5-Large <em>Raffel et al. (2020)</em>, and Flan-T5-XXL <em>Chung et al. (2022)</em> as the backbone models for dataset-specific verifier module, and unified verifier module, respectively. T5 models follow the encoder-decoder architecture and treat all NLP tasks as unified text-to-text transduction tasks. Flan-T5 is instruction-fine-tuned version of T5 which was trained on 1,836 NLP tasks initialized from fine-tuned T5 checkpoint. For T5-large, we fine-tune all parameters for separate verifier modules per each dataset. While for Flan-T5-XXL, we use LoRA <em>Hu et al. (2022)</em> as a parameter-efficient fine-tuning method, to train a unified verifier module which can follow the instruction. When using the unified verifier, we specify the dataset name in the instructions as datasets have different naming convention for relations.</p>
<p>The verifier is implemented using Pytorch <em>Paszke et al. (2019)</em> and Transformers <em>Wolf et al. (2020)</em>. For the training, we use Adam optimizer <em>Kingma and Ba (2015)</em>. Details about hyperparameter setting is provided in Appendix A. For the implementation of parameter efficient training method used in Flan-T5-XXL, we use PEFT <em>Mangrulkar et al. (2022)</em> and 8-bit quantization technique <em>Dettmers et al. (2022)</em>. All training was done using a single A40 GPU with 48GB of RAM.</p>
<h3>4.3 Evaluation Metrics</h3>
<p>To evaluate the quality of the generated graphs given the ground-truth graphs, we use four automatic evaluation metrics:</p>
<p>Triple Match F1 (T-F1) calculates F1 score based on the precision and recall between the triples in the generated graph and the ground-truth. We calculate the F1 scores for all test samples and compute the average F1 score as the final triple Match F1 score.</p>
<p>Graph Match F1 (G-F1) focuses on the entirety of the graph and evaluates how many graphs are exactly produced the same. For all test samples, we calculate the F1 score based on the precision and recall between all predicted graphs and all ground-truth graphs. This F1 score is the final Graph Match F1 score. Since graphs are represented in a linearised way, we could not simply use the string match method to check whether two graphs are the same. Instead, we first build directed graphs from linearised graphs using NetworkX <em>Laboratory et al. (2008)</em>, then we consider the two graphs to be the same when all node and edge attributes match.</p>
<p>G-BERTScore (G-BS) is a semantic-level metric proposed by <em>Saha et al. (2021)</em>, which extends the BERTScore <em>Zhang et al. (2020)</em> for graph-matching. It takes graphs as a set of edges and solve a matching problem which finds the best alignment between the edges in predicted graph and those in ground-truth graph. Each edge is considered as a sentence and BERTScore is used to calculate the score between a pair of predicted and ground-truth edges. Based on the best alignment and the overall matching score, the computed F1 score is used as the final G-BERTScore.</p>
<p>Graph Edit Distance (GED) <em>Abu-Aisheh et al. (2015)</em> computes the distance between the predicted graph and the ground-truth graph. It measures how many edit operations (addition, deletion, and replacement of nodes and edges) are required for transforming the predicted graph to a graph isomorphic to the ground-truth graph. Lower GED between two graphs indicates the two graphs are more similar. In practice, the cost of each operation</p>
<p>[1] We also compare the graph-based generative capability between ChatGPT and GPT-3 in Appendix D and report the fine-tuned results on small language model in Appendix E.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Single Verifier Module</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unified Verifier Module</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">KELM-sub</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">17.92</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">85.91</td>
<td style="text-align: center;">12.37</td>
<td style="text-align: center;">19.64</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">86.39</td>
<td style="text-align: center;">12.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">19.46</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">86.57</td>
<td style="text-align: center;">12.08</td>
<td style="text-align: center;">22.11</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">87.31</td>
<td style="text-align: center;">11.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">20.17</td>
<td style="text-align: center;">6.61</td>
<td style="text-align: center;">86.83</td>
<td style="text-align: center;">11.95</td>
<td style="text-align: center;">23.11</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">87.70</td>
<td style="text-align: center;">11.35</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">11.46</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">11.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">18.32</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">18.22</td>
<td style="text-align: center;">13.83</td>
<td style="text-align: center;">89.67</td>
<td style="text-align: center;">11.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">18.57</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">89.82</td>
<td style="text-align: center;">11.22</td>
<td style="text-align: center;">18.55</td>
<td style="text-align: center;">13.88</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">11.20</td>
</tr>
<tr>
<td style="text-align: center;">GenWiki</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">10.99</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">10.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">20.54</td>
<td style="text-align: center;">6.80</td>
<td style="text-align: center;">88.70</td>
<td style="text-align: center;">10.87</td>
<td style="text-align: center;">20.88</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">88.66</td>
<td style="text-align: center;">10.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">6.80</td>
<td style="text-align: center;">88.78</td>
<td style="text-align: center;">10.83</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">88.91</td>
<td style="text-align: center;">10.88</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of using PiVe on three datasets across all metrics. Single verifier module represents single dataset-specific verifier module trained on T5-Large and Unified verifier module is trained on Flan-T5-XXL using instruction-tuning.
is set to be 1 . For each sample, GED is normalized by a normalizing constant which is the upper bound of GED to make sure it is between 0 and 1 . For demonstration, we multiply GED by 100 to show more decimals.</p>
<h3>4.4 Results</h3>
<p>We report the evaluation results of using PiVe with ChatGPT on the test set of three datasets in Table 2. All results presented under Base mean the direct output of the LLM without any verification. By utilising PiVe, on each dataset, we can see the consistent improvement of the quality of the generated graphs. For instance, in GenWiki which uses the same verifier module that was trained on the training data of WebNLG, the improvement of the scores over all metrics indicates the effectiveness of PiVe. Since the graphs are generated by the LLM through one-shot learning, G-F1 as the most strict metric, it is hard to get high G-F1 score (basically aiming for exact match without any minor deviation in wording, spelling, entities, or relations).</p>
<p>On WebNLG and GenWiki datasets, single verifier module performs slightly better than unified verifier module. While on KELM-sub dataset, unified module performs far better. We speculate this is due to the size of training data for KELM-sub verifier module being larger than that for WebNLG and GenWiki (as shown in Table 1). Since unified verifier module combines the training data of different datasets, more training data leads to better performance for instruction-tuning. We conducted human evaluation which we include in Appendix F due to page limit.</p>
<h3>4.5 Iterative Prompting vs. Iterative Offline Correction</h3>
<p>Instead of iteratively prompting the LLM, another way to utilise the results from verifier module is to append the predicted missing triples to the previously generated graph. The results of the comparison between iterative prompting and iterative offline correction using single verifier module and unified verifier module on KELM dataset is shown in Table 3. Iterative offline correction performs worse than iteratively prompting. This might be because iteratively prompting has the chance of doing self-correction. In each iteration, when we prompt the LLM, the generated graphs can probably correct the mistakes that were made in previous iteration. For example, in Figure 4, in Base the predicted relation regarding birth date is "birth year", while the reference is "date of birth". As the PiVe iteration continues, in Iteration 2, the relation "birth year" is regenerated as "date of birth" even though we didn't mention this in the prompt. Due to the page limit, we report the comparison results on WebNLG and GenWiki datasets in Appendix B. Similarly, iterative prompting can achieve better results than iterative offline correction over all using different verifier modules.</p>
<h3>4.6 Impact of More Shots</h3>
<p>While in our main experiments, for cost reason, we used only one-shot demonstrations for the LLM prompting (i.e., GPT-3.5), we show that PiVe is effective in improving the results regardless of the underlying number of shots. Here we report the results of k -shot $(\mathrm{k}=6,8,10)$ with the iterative</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Offline Correction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Time</td>
<td style="text-align: center;">T-F1 $\uparrow$</td>
<td style="text-align: center;">G-F1 $\uparrow$</td>
<td style="text-align: center;">G-BS $\uparrow$</td>
<td style="text-align: center;">GED $\downarrow$</td>
<td style="text-align: center;">Time</td>
<td style="text-align: center;">T-F1 $\uparrow$</td>
<td style="text-align: center;">G-F1 $\uparrow$</td>
<td style="text-align: center;">G-BS $\uparrow$</td>
<td style="text-align: center;">GED $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">Single Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">$+13.5$</td>
<td style="text-align: center;">17.92</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">85.91</td>
<td style="text-align: center;">12.37</td>
<td style="text-align: center;">$+4.5$</td>
<td style="text-align: center;">17.76</td>
<td style="text-align: center;">5.83</td>
<td style="text-align: center;">86.42</td>
<td style="text-align: center;">12.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">$+4.7$</td>
<td style="text-align: center;">19.46</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">86.57</td>
<td style="text-align: center;">12.08</td>
<td style="text-align: center;">$+1.1$</td>
<td style="text-align: center;">18.51</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">86.91</td>
<td style="text-align: center;">12.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">$+2.1$</td>
<td style="text-align: center;">20.17</td>
<td style="text-align: center;">6.61</td>
<td style="text-align: center;">86.83</td>
<td style="text-align: center;">11.95</td>
<td style="text-align: center;">$+0.2$</td>
<td style="text-align: center;">18.55</td>
<td style="text-align: center;">6.17</td>
<td style="text-align: center;">86.94</td>
<td style="text-align: center;">12.18</td>
</tr>
<tr>
<td style="text-align: center;">Unified Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">$+118.9$</td>
<td style="text-align: center;">19.64</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">86.39</td>
<td style="text-align: center;">12.08</td>
<td style="text-align: center;">$+105.0$</td>
<td style="text-align: center;">16.99</td>
<td style="text-align: center;">5.67</td>
<td style="text-align: center;">87.08</td>
<td style="text-align: center;">12.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">$+46.8$</td>
<td style="text-align: center;">22.11</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">87.31</td>
<td style="text-align: center;">11.68</td>
<td style="text-align: center;">$+40.6$</td>
<td style="text-align: center;">17.76</td>
<td style="text-align: center;">5.67</td>
<td style="text-align: center;">87.48</td>
<td style="text-align: center;">12.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">$+21.1$</td>
<td style="text-align: center;">23.11</td>
<td style="text-align: center;">7.50</td>
<td style="text-align: center;">87.70</td>
<td style="text-align: center;">11.35</td>
<td style="text-align: center;">$+10.4$</td>
<td style="text-align: center;">17.85</td>
<td style="text-align: center;">5.67</td>
<td style="text-align: center;">87.52</td>
<td style="text-align: center;">12.96</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between Iterative Prompting and Iterative Offline Correction on KELM-sub dataset across all metrics using Single Verifier and Unified Verifier. Time denotes the total inference time in minutes.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of various number of shots ( $\mathrm{k}=6,8$, 10) on KELM-sub with Iterative Offline Correction. The colors represent Base, and corrective iterations 1, 2, 3.
offline correction (i.e., only using the LLM once to get the initial graph, while the correction steps are all applied step-by-step and offline). Figure 2 demonstrates the results on KELM-sub using unified verifier with iterative offline correction. The results show, as expected, that PiVe still provides consistent improvement even with the increase in the number of shots. Additionally, as the shots grow the improvement from PiVe also increases.</p>
<h3>4.7 Baselines on LLMs</h3>
<p>To probe other prompting techniques as baselines of generating graphs from the LLM, we compare three diverse prompts. The first one we use is the default prompt used across our main experiments. This prompt is fairly direct and simple. Prompt 1: Transform the text into a semantic graph. In the second prompt, we aim to instruct the LLM to generate larger graph with more triples. This is to increase the chance of LLM recovering more triples during the generation. Prompt 2: Transform the text into a semantic graph consisting of a set of triples. Generate as many triples as possible. For the third prompt, inspired by Chain-of-thought [wei2022chain; kojima2022chain], approach, we also ask the LLM to generate the semantic graph in two steps. Prompt 3: Transform the text into a semantic graph consisting of a set of triples. First produce all relations</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">T-F1 $\uparrow$</th>
<th style="text-align: center;">G-F1 $\uparrow$</th>
<th style="text-align: center;">G-BS $\uparrow$</th>
<th style="text-align: center;">GED $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 1</td>
<td style="text-align: center;">14.54</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">84.85</td>
<td style="text-align: center;">13.90</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 2</td>
<td style="text-align: center;">14.33</td>
<td style="text-align: center;">2.44</td>
<td style="text-align: center;">84.57</td>
<td style="text-align: center;">14.43</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 3</td>
<td style="text-align: center;">13.79</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">84.20</td>
<td style="text-align: center;">15.18</td>
</tr>
</tbody>
</table>
<p>Table 4: Self-Refine [madaan2023selfrefine] results on KELM-sub using Single Verifier.
possible, then produce the graph.
We conduct experiments on ChatGPT (gpt-3.5-turbo) and GPT-4 (gpt-4) in 6-shot learning on KELM-sub, using unified verifier with iterative offline correction. The results are shown in Figure 3 (for detailed numbers see Table 14 and Table 15 in Appendix). In general, as expected, GPT-4 performs far better than ChatGPT on the T2G task, but the effect of different prompts varies across these two models. Specifically, on ChatGPT, Prompt 2 achieves the best results while on GPT-4, Prompt 1 is outperforming the rest on most metrics. PiVe can consistently improve the results across all different settings, with the biggest jump in performance emerging in the first iteration, with slight improvements also observed between the second and third iterations of correction.</p>
<p>We also compare PiVe with the recent SelfRefine [madaan2023selfrefine] method, which leverages LLM itself to provide feedbacks for selfrefinement. Table 4 shows the self-refine results on KELM-sub dataset. The results show that selfrefine could not provide effective feedback, thus leading to the performance drop as the iteration goes. The performance gap is obvious comparing with our PiVe result. Since LLMs are not trained as rigorously on structured data compared to text, expecting them to provide meaningful feedbacks on their outputs is expected to fail.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results of using 3 diverse prompts with 6 shot on KELM-sub with Iterative Offline Correction on ChatGPT and GPT-4. The colors represent Base, and corrective iterations 1, 2, 3.</p>
<h3>4.8 Computational Cost and Trade-off</h3>
<p><strong>Training and Inference</strong> The training and inference of both single verifiers and unified verifier are on a single A40 GPU. Each single verifier takes around 6 hours and the unified verifier takes around 40 hours to train. The computation cost for training of verifiers is a feasible one-off cost. Once the training is finished, the inference of the verification of each instance takes 0.15s for single verifier, and 3.5s for unified verifier. See the Time column in Table 3. Different verifiers present performance-speed trade-offs and are significantly effective in augmenting the LLMs.</p>
<p><strong>Stopping Criterion</strong> In theory, the verification module could run till no missing triple is predicted or a maximum number of iterations is reached. However, running more iterations increases the associated cost (i.e., OpenAI API charges). We set a maximum of 3 iterations.</p>
<h3>4.9 PiVe Examples</h3>
<p>In Figure 4, we demonstrate an example from KELM-sub test set using unified verifier. In Base, based on the prediction from the LLM, the verifier module predicts the missing triple ["Francisco Uranga", "occupation", "swimmer"]. By suggesting this missing triple in the next iteration of prompt, the prediction from LLM includes it. Then, the verifier predicts the missing triple ["Francisco Uranga", "sex or gender", "male"]. In Iteration 2, both of these two missing triples are included in the prediction from LLM, and at this time, the verifier predicts "Correct". The prediction from Iteration 2 contains all information correctly in the reference. See another example in Appendix H.</p>
<h3>5 GenWiki-HIQ</h3>
<p>Training a good G2T or T2G model requires a large amount of high-quality parallel text and graph pairs or pre-training protocols to accommodate for lack of data (Han and Shareghi, 2022). However, creating the parallel data by human is a labour-intensive and time-consuming work. Jin et al. (2020) proposed GenWiki, an automatically constructed large dataset containing non-parallel text and graphs with shared content. Although the text and graphs contain shared content, it can still only be used for unsupervised training due to the low entity and relation overlap between text and graph. Our verifier module can naturally serve as a data augmentation tool to improve the overlap between the text and graph of automatically constructed datasets.</p>
<p><strong>Iterative Augmentation.</strong> Based on GenWikiFINE (~750K), first we filter out the text that has little overlap with the graph. After filtering, we got around 110K text-graph pairs called GenWikiFINE-f. Then following the process described in Section 3.1, we use the WebNLG verifier module and the iterative offline correction to improve the coverage and quality of GenWikiFINE-f and formed GenWiki-HIQ. The maximum number of iterations is four.</p>
<p>To evaluate the effectiveness of the verifier module as data augmentation tool, as well the quality of the generated graph, first we use Flan-T5-XL model to generate a description of each graph in zero-shot setting by using the prompt "Transform the semantic graph into a description." for each iteration. Then we leverage automatic quality evaluation metrics to calculate the score between the generated description and the corresponding text. Ideally, the higher the similarity between the graph and the corresponding text, the higher the score of the generated description and corresponding text. We use four commonly used quality evaluation metrics which are BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), BERTScore.</p>
<p><strong>Result.</strong> We used the dataset-specific verifier module to do the data augmentation. We conducted four iterations and the evaluation results are shown in Ta-</p>
<p>Text: Francisco Uranga was born in 1905 and represented Argentina at the 1928 Summer Olympics. He competed in the men's 50 metre freestyle.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Base</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM Prediction: [["Francisco Uranga", "birth year", "1905"], ["Francisco Uranga", "represented", "Argentina"],</td>
</tr>
<tr>
<td style="text-align: left;">["Argentina", "participated in", "1928 Summer Olympics"], ["Francisco Uranga", "competed in", "men's 50 metre</td>
</tr>
<tr>
<td style="text-align: left;">freestyle"]]</td>
</tr>
<tr>
<td style="text-align: left;">Verification Module Output: ["Francisco Uranga", "occupation", "swimmer"]</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 1</td>
</tr>
<tr>
<td style="text-align: left;">LLM Prediction: [["Francisco Uranga", "occupation", "swimmer"], ["Francisco Uranga", "birth year", "1905"],</td>
</tr>
<tr>
<td style="text-align: left;">["Francisco Uranga", "represented", "Argentina"], ["Argentina", "participated in", "1928 Summer Olympics"],</td>
</tr>
<tr>
<td style="text-align: left;">["Francisco Uranga", "competed in", "men's 50 metre freestyle"]]</td>
</tr>
<tr>
<td style="text-align: left;">Verification Module Output: ["Francisco Uranga", "sex or gender", "male"]</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 2</td>
</tr>
<tr>
<td style="text-align: left;">LLM Prediction: [["Francisco Uranga", "occupation", "swimmer"], ["Francisco Uranga", "sex or gender", "male"],</td>
</tr>
<tr>
<td style="text-align: left;">["Francisco Uranga", "date of birth", "1905"], ["Francisco Uranga", "represented", "Argentina"], ["Argentina",</td>
</tr>
<tr>
<td style="text-align: left;">"participated in", "1928 Summer Olympics"], ["Francisco Uranga", "competed in", "men's 50 metre freestyle"]]</td>
</tr>
<tr>
<td style="text-align: left;">Verification Module Output: Correct</td>
</tr>
<tr>
<td style="text-align: left;">Reference: [["Francisco Uranga", "occupation", "swimmer"], ["Francisco Uranga", "date of birth", "01 January 1905"],</td>
</tr>
<tr>
<td style="text-align: left;">["Francisco Uranga", "country of citizenship", "Argentina"], ["Francisco Uranga", "sex or gender", "male"], ["Francisco</td>
</tr>
<tr>
<td style="text-align: left;">Uranga", "participant of", "1928 Summer Olympics"], ["Francisco Uranga", "sport", "Swimming (sport)"]]</td>
</tr>
</tbody>
</table>
<p>Figure 4: An example from KELM-sub test set using unified verifier module.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">BLEU $\uparrow$</th>
<th style="text-align: right;">METEOR $\uparrow$</th>
<th style="text-align: right;">TER $\downarrow$</th>
<th style="text-align: right;">BERTScore $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base</td>
<td style="text-align: right;">4.75</td>
<td style="text-align: right;">18.02</td>
<td style="text-align: right;">80.72</td>
<td style="text-align: right;">89.47</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 1</td>
<td style="text-align: right;">11.86</td>
<td style="text-align: right;">26.25</td>
<td style="text-align: right;">73.79</td>
<td style="text-align: right;">91.49</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 2</td>
<td style="text-align: right;">14.90</td>
<td style="text-align: right;">29.69</td>
<td style="text-align: right;">72.29</td>
<td style="text-align: right;">92.00</td>
</tr>
<tr>
<td style="text-align: left;">Iteration 3</td>
<td style="text-align: right;">15.93</td>
<td style="text-align: right;">31.05</td>
<td style="text-align: right;">72.01</td>
<td style="text-align: right;">92.14</td>
</tr>
</tbody>
</table>
<p>Table 5: Results of iterative augmentation on filtered 110K text-graph pairs from GenWiki ${ }<em _FINE="{FINE" _text="\text">{\text {FINE }}$ across four metrics after each iteration. We take the text-graph pairs from Iteration 3 as the created GenWiki-HIQ dataset.
ble 5. The results in Base represent the scores over non-parallel graph-text pairs from GenWiki ${ }</em>$ on the human annotated GenWiki test set. This indicates that GenWiki-HIQ contains parallel text-graph pairs with high overlap.}}$, which have low overlap between graph and text. By using verifier module iteratively, we add more missing triples to the original graph, thus leading the higher quality scores. As the iteration progresses, fewer missing triples are added and we take the augmented graph-text pairs from the last iteration as the final created GenWiki-HIQ dataset. We also conducted G2T experiments in Appendix 5.1 to further demonstrate the quality of GenWikiHIQ. The G2T model trained on GenWiki-HIQ performs far better than the G2T model trained on GenWiki ${ }_{\text {FINE-f }</p>
<p>Qualitative Example. In Figure 5, we demonstrate an example from the created GenWiki-HIQ</p>
<p>Text: Timma is a village development committee in Bhogour District in the Kosi Zone of eastern Nepal. At the time of the 1991 Nepal census it had a population of 3336 persons living in 621 individual households.</p>
<h2>GenWiki-FINE:</h2>
<p>[Timma, populationTotal, 3336],
[Timma, pushpinMap, Nepal],
[Timma, country, Nepal]]
GenWiki-HIQ:
[Timma, populationTotal, 3336],
[Timma, pushpinMap, Nepal],
[Timma, country, Nepal],
[Timma, is Part Of, Bhogour District Kosi Zone],
[Timma, county Development Committee, Bhogour District],
[Timma, number Of Households, 621]]</p>
<p>Figure 5: An example from GenWiki-HIQ compared to the original graph in GenWiki ${ }<em _FINE="{FINE" _text="\text">{\text {FINE }}$.
dataset and the original graph in GenWiki ${ }</em>$. After the data augmentation process, the graph in GenWiki-HIQ contains more information in text.}</p>
<h3>5.1 G2T Results on GenWiki-HIQ</h3>
<p>To further verify the quality of GenWiki-HIQ dataset, we use T5-large as the backbone model to train a G2T model, which generates the corresponding text based on the graph. Then we test it on the original GenWiki test set containing a 1,000 high-quality human annotated parallel text-graph pairs. As comparison, we also train another G2T</p>
<table>
<thead>
<tr>
<th></th>
<th>BLEUT</th>
<th>METEORt</th>
<th>TER4</th>
<th>BERTScoreT</th>
</tr>
</thead>
<tbody>
<tr>
<td>GenWiki $_{\text {FINE-f }}$</td>
<td>35.71</td>
<td>36.67</td>
<td>65.19</td>
<td>93.74</td>
</tr>
<tr>
<td>GenWiki-HIQ</td>
<td>48.17</td>
<td>42.03</td>
<td>41.94</td>
<td>95.44</td>
</tr>
</tbody>
</table>
<p>Table 6: Results of G2T generation on original GenWiki test set training on different datasets. GenWiki $<em _FINE="{FINE" _text="\text">{\text {FINE-f }}$ contains the filtered 110K text-graph pairs from original GenWiki $</em>$ as described in Section 5. GenWiki-HIQ is the augmented dataset based on GenWiki $}<em _FINE-f="{FINE-f" _text="\text">{\text {FINE-f }}$.
model on GenWiki $</em>$ which is the seed dataset of GenWiki-HIQ.}</p>
<p>The result is demonstrated in Table 6. On original Genwiki test set, the model trained on GenWiki-HIQ performs far better than the model trained on GenWiki $_{\text {FINE-f }}$ across all metrics. This indicates that GenWiki-HIQ contains parallel textgraph pairs with high overlap.</p>
<h2>6 Background and Related Work</h2>
<h3>6.1 In-Context Learning</h3>
<p>With the scaling of model size and training corpus size (Brown et al., 2020; Chowdhery et al., 2022), LLMs demonstrate new abilities of learning from a few demonstrations which contain some training examples (Dong et al., 2023). As a new paradigm, In-Context Learning does not require parameter updates and directly performs predictions on the pre-trained language models. The provided demonstration examples in the prompt follow the same format, which are usually written in natural language templates. By concatenating a query question with the demonstrations in the prompt, LLMs can learn from the given examples and make a prediction of the query question. Previous research (Liu et al., 2022; Lu et al., 2022) has shown that the number and order of the demonstrations can influence the In-Context Learning performance. These are further points of future investigation, which could potentially improve the initial graph produced by the LLM, which could further be corrected with the PiVe framework.</p>
<h3>6.2 Instruction-Tuning</h3>
<p>Instruction-Tuning (Mishra et al., 2022; Wang et al., 2022; Longpre et al., 2023) is a framework of doing multi-task learning, which enables the use of human-readable instructions to guide the prediction of LLMs. This novel training paradigm can improve the performance of the downstream tasks and also shows great generalisation ability on unseen tasks (Chung et al., 2022; Sanh et al., 2022). Wang et al. (2023) proposed a unified information extraction framework based on multitask instruction-tuning. Zhou et al. (2023) utilised instruction-tuning to perform controlled text generation following certain constraints. In our work, we use instruction-tuning to train a unified verifier module, which can follow the instruction to perform predictions on different datasets.</p>
<h3>6.3 Verifiers</h3>
<p>Leveraging small models could further improve the performance of LLMs. Cobbe et al. (2021) proposed to solve math word problem by utilising verifier. The verifier is used to judge the correctness of model-generated solutions. During test time, based on multiple candidate solutions generated, verifier calculates the correctness probability and the final answer will be selected by the verifier from the ranked list. Welleck et al. (2023) proposed selfcorrection, an approach that trains a small model to iteratively apply self-correction. The idea of self-correction looks similar to our PiVe. While Welleck et al. (2023) focuses on the design of a selfcorrecting language model, PiVe presents a very simple verifier module design and a simple data perturbation strategy to train such model. The ideas presented in our work are developed concurrently and independently.</p>
<h2>7 Conclusion</h2>
<p>We proposed PiVe, an iterative verification framework, to improve the graph-based generative capability of LLMs. We illustrated how a simple perturbation technique could be used to build data for training a verifier module which both verifies and corrects outputs from an LLM. We used different training strategies to build both dataset-specific verifiers with fine-tuning, and a unified verifier with instruction-tuning. Our verifier module could act both as an iterative prompting guide to improve outputs of an LLM, as well as an iterative offline correction system that starts from an LLM outputs but continuously improves it offline. The experimental results on three graph-based datasets demonstrates the effectiveness of PiVe. Furthermore, PiVe can also be used as a data augmentation technique to help improve the quality of automatically generated parallel text-graph datasets. By using verifier module, we created GenWiki-HIQ, a dataset containing 110K parallel text and graphs with high overlap for future research in text-graph domain.</p>
<h2>Limitations</h2>
<p>Although the proposed framework is a straightforward and effective method of improving the generative capabilities of black box LLMs in graph generation, it still has some limitations. Firstly, PiVe is only designed for few-shot prompting setting on LLMs, using an external verifier module to enhance their generative capabilities. The improvement is less significant when utilising PiVe on LMs that have been fine-tuned on the task data. Secondly, PiVe is not designed for free-form text generation tasks. Due to the unique aspect of graph, which has a specific structure, it allows for a much more fine-grained detection of errors and enables a richer corrective feedback. Translation between text and other similar modalities of data (e.g., table, SQL) can also effectively leverage our verification mechanism. Thirdly, in this work, we only focus on the triple missing mistake made by LLMs, so that the verifier module is not sensitive to the order of the head entity and tail entity. This means when the order of the head entity and tail entity in a triple of a generated graph from LLMs is incorrect, verifier module is not able to detect this type of mistake. It would be more effective if other error-detection heuristic methods are developed for creating the training dataset of the verifier.</p>
<h2>Ethics Statement</h2>
<p>Our work is built on top of existing pre-trained language models. Our goal was not to attend to alleviate the well-documented issues (e.g., privacy, undesired biases, etc) that such models embody. For this reason, we share the similar potential risks and concerns posed by these models.</p>
<h2>References</h2>
<p>Zeina Abu-Aisheh, Romain Raveaux, Jean-Yves Ramel, and Patrick Martineau. 2015. An exact graph edit distance algorithm for solving pattern recognition problems. In ICPRAM 2015 - Proceedings of the International Conference on Pattern Recognition Applications and Methods, Volume 1, Lisbon, Portugal, 10-12 January, 2015, pages 271-278. SciTePress.</p>
<p>Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June</p>
<p>6-11, 2021, pages 3554-3565. Association for Computational Linguistics.</p>
<p>Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. Dbpedia: A nucleus for a web of open data. In The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007, volume 4825 of Lecture Notes in Computer Science, pages 722-735. Springer.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, pages 65-72. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,</p>
<p>Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. 8-bit optimizers via block-wise quantization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, INLG 2017, Santiago de Compostela, Spain, September 4-7, 2017, pages 124-133. Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang. 2020. Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. CoRR, abs/2006.04702.</p>
<p>Jiuzhou Han and Ehsan Shareghi. 2022. Self-supervised graph masking pre-training for graph-to-text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4845-4853, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang. 2020. Genwiki: A dataset of 1.3 million contentsharing text and graphs for unsupervised graph-totext generation. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 2398-2409. International Committee on Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations,</p>
<p>ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS.</p>
<p>Los Alamos National Laboratory, United States. Department of Energy. Office of Scientific, and Technical Information. 2008. Exploring Network Structure, Dynamics, and Function Using Networkx. United States. Department of Energy.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pages 100-114. Association for Computational Linguistics.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. CoRR, abs/2301.13688.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 80868098. Association for Computational Linguistics.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. 2022. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3470-3487. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311-318. ACL.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8024-8035.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Swarnadeep Saha, Prateek Yadav, Lisa Bauer, and Mohit Bansal. 2021. Explagraphs: An explanation graph generation task for structured commonsense reasoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 77167740. Association for Computational Linguistics.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Matthew G. Snover, Bonnie J. Dorr, Richard M. Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, AMTA 2006, Cambridge, Massachusetts, USA, August 8-12, 2006, pages 223-231. Association for Machine Translation in the Americas.</p>
<p>Frank van Harmelen, Vladimir Lifschitz, and Bruce W. Porter, editors. 2008. Handbook of Knowledge Representation, volume 3 of Foundations of Artificial Intelligence. Elsevier.</p>
<p>Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li, and Chunsai Du. 2023. Instructuie: Multitask instruction tuning for unified information extraction. CoRR, abs/2304.08085.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Siap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5085-5109. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 38-45. Association for Computational Linguistics.</p>
<p>Yi Xu, Luoyi Fu, Zhouhan Lin, Jiexing Qi, and Xinbing Wang. 2022. INFINITY: A simple yet effective unsupervised framework for graph-text mutual conversion. CoRR, abs/2209.10754.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>T5-Large</td>
</tr>
<tr>
<td>Epoch</td>
<td>5</td>
</tr>
<tr>
<td>Batch Size</td>
<td>16</td>
</tr>
<tr>
<td>Optimizer</td>
<td>Adam</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>$2 \times 10^{-5}$</td>
</tr>
<tr>
<td>Warm-up Step</td>
<td>500</td>
</tr>
<tr>
<td>Beam Size</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyperparameters of single verification module.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>FLAN-T5-XXL</td>
</tr>
<tr>
<td>Epoch</td>
<td>2</td>
</tr>
<tr>
<td>Batch Size</td>
<td>48</td>
</tr>
<tr>
<td>Optimizer</td>
<td>Adam</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>$3 \times 10^{-5}$</td>
</tr>
<tr>
<td>Warm-up Step</td>
<td>100</td>
</tr>
<tr>
<td>Beam Size</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Table 8: Hyperparameters of unified verification module.</p>
<p>Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023. Controlled text generation with natural language instructions. CoRR, abs/2304.14293.</p>
<h2>Appendix</h2>
<h2>A Hyperparameter Setting</h2>
<h2>B Additional Experiment Result</h2>
<p>Table 9 and Table 10 show the results of the comparison between iteratively prompt and iterative offline correction on WebNLG and GenWiki datasets.</p>
<h2>C Effect of Perturbation Method</h2>
<p>As described in Section 3.1, we perturb the graph by omitting one triple when building the verifier module of PiVe. In addition, we also investigated other perturbation methods to train a verifier module, such as perturbing the head entity, relation and tail entity. To be specific, for head entity perturbation, if the graph contains more than one triple, we randomly choose one triple and replace the head entity with a different head entity from other triples of the same graph. Likewise, we replace the relation and tail entity for relation perturbation and tail entity perturbation, respectively. The target is to predict the original triple. Then we train different verifier modules using these three perturbation methods on KELM-sub. The results of doing different perturbations using Iterative Offline Correction is shown in Table 11.</p>
<p>Comparing with the result of omitting triple perturbation method shown in Table 3 using Single Verifier with Iterative Offline Correction, these three perturbation methods have varying effects. While the relational perturbation works in terms of T-F1, with more iterations, the G-BS score generally goes down for all these perturbations. This indicates the verifier module could potentially inject wrong corrections if not trained with the proper perturbation mechanism. We speculate the reason is because LLMs are less likely to make mistakes at entity level, so these perturbation methods are not useful for training a verifier module. This also indicates when building a verifier module, choosing reasonable perturbation methods is significant and necessary.</p>
<h2>D ChatGPT vs GPT-3</h2>
<p>To further highlight the generalisation ability of PiVe, in addition to ChatGPT, we also experiment with GPT-3 (text-davinci-003) as the backbone LLM to perform the T2G task. We perform experiments on KELM-sub dataset using iterative prompting and iterative offline correction with different verifiers. The results are shown in Table 13. Compared with the results of using ChatGPT (shown in Table 3), GPT-3 has a better graph-based generative capability. Nonetheless, PiVe can still consistently further improve its results over all settings. Using iterative prompting with the unified verifier can achieve the best result on KELM-sub.</p>
<h2>E Comparison with Fine-tuned Baselines</h2>
<p>While our work focuses on the fundamental question of "How can we improve the generative capabilities of black box LLMs in graph generation?", for completeness we also provide results of finetuned T5 (Raffel et al., 2020) in Table 12. As expected, fine-tuning on large amount of data surpasses few-shot prompting. This underscores the struggle LLMs face in transduction problems, and the need for additional mechanisms (like PiVe) to help LLM improvement.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Offline Correction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">Single Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">11.46</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">11.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">18.32</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">18.03</td>
<td style="text-align: center;">13.55</td>
<td style="text-align: center;">89.16</td>
<td style="text-align: center;">11.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">18.57</td>
<td style="text-align: center;">14.00</td>
<td style="text-align: center;">89.82</td>
<td style="text-align: center;">11.22</td>
<td style="text-align: center;">18.10</td>
<td style="text-align: center;">13.55</td>
<td style="text-align: center;">89.19</td>
<td style="text-align: center;">11.51</td>
</tr>
<tr>
<td style="text-align: center;">Unified Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">11.46</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">13.43</td>
<td style="text-align: center;">89.59</td>
<td style="text-align: center;">11.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">18.22</td>
<td style="text-align: center;">13.83</td>
<td style="text-align: center;">89.67</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">18.02</td>
<td style="text-align: center;">13.49</td>
<td style="text-align: center;">89.21</td>
<td style="text-align: center;">11.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">18.55</td>
<td style="text-align: center;">13.88</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">11.20</td>
<td style="text-align: center;">18.06</td>
<td style="text-align: center;">13.49</td>
<td style="text-align: center;">89.07</td>
<td style="text-align: center;">11.65</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison between Iterative Prompting and Iterative Offline Correction on WebNLG dataset across all metrics using Single Verifier and Unified Verifier.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Offline Correction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">Single Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">10.99</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">10.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">20.54</td>
<td style="text-align: center;">6.80</td>
<td style="text-align: center;">88.70</td>
<td style="text-align: center;">10.87</td>
<td style="text-align: center;">20.24</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">89.00</td>
<td style="text-align: center;">10.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">6.80</td>
<td style="text-align: center;">88.78</td>
<td style="text-align: center;">10.83</td>
<td style="text-align: center;">20.32</td>
<td style="text-align: center;">6.80</td>
<td style="text-align: center;">89.07</td>
<td style="text-align: center;">10.93</td>
</tr>
<tr>
<td style="text-align: center;">Unified Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">10.99</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">10.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">20.88</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">88.66</td>
<td style="text-align: center;">10.90</td>
<td style="text-align: center;">20.37</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">89.08</td>
<td style="text-align: center;">10.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">88.91</td>
<td style="text-align: center;">10.88</td>
<td style="text-align: center;">20.42</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">89.11</td>
<td style="text-align: center;">10.94</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparison between Iterative Prompting and Iterative Offline Correction on GenWiki dataset across all metrics using Single Verifier and Unified Verifier.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T-F1 $\dagger$</th>
<th style="text-align: center;">G-F1 $\dagger$</th>
<th style="text-align: center;">G-BS $\dagger$</th>
<th style="text-align: center;">GED $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Head</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">13.66</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.23</td>
<td style="text-align: center;">13.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">13.65</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">81.99</td>
<td style="text-align: center;">13.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">13.65</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">80.83</td>
<td style="text-align: center;">13.31</td>
</tr>
<tr>
<td style="text-align: center;">Relation</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">15.09</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">83.68</td>
<td style="text-align: center;">12.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">15.29</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">82.90</td>
<td style="text-align: center;">12.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">15.33</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">82.09</td>
<td style="text-align: center;">12.96</td>
</tr>
<tr>
<td style="text-align: center;">Tail</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.92</td>
<td style="text-align: center;">13.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">13.52</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.83</td>
<td style="text-align: center;">13.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">13.51</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.74</td>
<td style="text-align: center;">13.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">13.50</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">83.64</td>
<td style="text-align: center;">13.23</td>
</tr>
</tbody>
</table>
<p>Table 11: Results of doing different perturbations to the graph on KELM-sub to train a Single Verifier with Iterative Offline Correction.</p>
<h2>F Human Evaluation</h2>
<p>We conducted a human evaluation on 105 randomly sampled instances from three datasets (KELM-sub, WebNLG, GenWiki). Specifically, for each dataset, first we took the test set outputs from the first iteration and the last iteration, then we randomly sampled 35 instances from those with different outputs. The output from the first iteration is the original ChatGPT output without using PiVe, and the output from the last iteration is the result after using PiVe. For the evaluation process we recruited three annotators ( 1 PhD graduate and 2 PhD students in Computer Science and NLP) to select, for a given</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">T-F1 $\dagger$</th>
<th style="text-align: center;">G-F1 $\dagger$</th>
<th style="text-align: center;">G-BS $\dagger$</th>
<th style="text-align: center;">GED $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">KELM-sub</td>
<td style="text-align: center;">58.45</td>
<td style="text-align: center;">47.26</td>
<td style="text-align: center;">94.12</td>
<td style="text-align: center;">8.48</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">54.77</td>
<td style="text-align: center;">45.31</td>
<td style="text-align: center;">93.51</td>
<td style="text-align: center;">9.11</td>
</tr>
<tr>
<td style="text-align: center;">GenWiki</td>
<td style="text-align: center;">36.34</td>
<td style="text-align: center;">29.69</td>
<td style="text-align: center;">91.14</td>
<td style="text-align: center;">9.74</td>
</tr>
</tbody>
</table>
<p>Table 12: Fine-tuning results of text-to-graph generation on three datasets on T5-Large model.
text and two graph outputs, which graph matches the text better. Each annotator should only choose one graph per each instance and evaluate all 105 instances.</p>
<p>After annotation, we took majority voting over the result of each instance, then calculated the number of wins for ChatGPT with or without PiVe. The results are shown in Table 16. From the results, we can see ChatGPT with PiVe wins on 85 out of 105 samples and the total winning rate is over $80 \%$. This indicates the PiVe can effectively improve the graph-based generative capability of LLMs.</p>
<p>For the cases that PiVe failed to improve, we did error analysis and found that there were mainly two types of mistakes that PiVe made: redundancy and inaccuracy. In Figure 6, we demonstrate two examples containing these two types of mistakes shown in red text. In the first example, the triple ["Train song Mermaid", "instrument", "Singing"] predicted by PiVe is redundant. In the second example, the relation</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Iterative Offline Correction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
<td style="text-align: center;">T-F1 $\dagger$</td>
<td style="text-align: center;">G-F1 $\dagger$</td>
<td style="text-align: center;">G-BS $\dagger$</td>
<td style="text-align: center;">GED $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">Single Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">15.11</td>
<td style="text-align: center;">7.72</td>
<td style="text-align: center;">83.63</td>
<td style="text-align: center;">12.91</td>
<td style="text-align: center;">15.11</td>
<td style="text-align: center;">7.72</td>
<td style="text-align: center;">83.63</td>
<td style="text-align: center;">12.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">19.55</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">85.90</td>
<td style="text-align: center;">11.98</td>
<td style="text-align: center;">18.97</td>
<td style="text-align: center;">8.72</td>
<td style="text-align: center;">86.47</td>
<td style="text-align: center;">12.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">21.57</td>
<td style="text-align: center;">9.33</td>
<td style="text-align: center;">86.59</td>
<td style="text-align: center;">11.56</td>
<td style="text-align: center;">19.65</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">86.76</td>
<td style="text-align: center;">11.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">22.49</td>
<td style="text-align: center;">9.89</td>
<td style="text-align: center;">87.10</td>
<td style="text-align: center;">11.37</td>
<td style="text-align: center;">19.69</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">86.77</td>
<td style="text-align: center;">11.95</td>
</tr>
<tr>
<td style="text-align: center;">Unified Verifier</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">15.11</td>
<td style="text-align: center;">7.72</td>
<td style="text-align: center;">83.63</td>
<td style="text-align: center;">12.91</td>
<td style="text-align: center;">15.11</td>
<td style="text-align: center;">7.72</td>
<td style="text-align: center;">83.63</td>
<td style="text-align: center;">12.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">21.40</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">86.43</td>
<td style="text-align: center;">11.69</td>
<td style="text-align: center;">20.46</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">87.18</td>
<td style="text-align: center;">11.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">24.37</td>
<td style="text-align: center;">9.50</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">21.54</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">87.56</td>
<td style="text-align: center;">11.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">26.06</td>
<td style="text-align: center;">10.22</td>
<td style="text-align: center;">87.99</td>
<td style="text-align: center;">10.83</td>
<td style="text-align: center;">21.57</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">87.57</td>
<td style="text-align: center;">11.71</td>
</tr>
</tbody>
</table>
<p>Table 13: Results of using GPT-3-davinci as the backbone LLM on KELM-sub dataset over different settings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T-F1 $\dagger$</th>
<th style="text-align: center;">G-F1 $\dagger$</th>
<th style="text-align: center;">G-BS $\dagger$</th>
<th style="text-align: center;">GED $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt 1</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">35.25</td>
<td style="text-align: center;">10.78</td>
<td style="text-align: center;">85.43</td>
<td style="text-align: center;">10.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">36.96</td>
<td style="text-align: center;">12.56</td>
<td style="text-align: center;">88.20</td>
<td style="text-align: center;">10.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">37.25</td>
<td style="text-align: center;">12.61</td>
<td style="text-align: center;">88.47</td>
<td style="text-align: center;">10.13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">37.37</td>
<td style="text-align: center;">12.68</td>
<td style="text-align: center;">88.54</td>
<td style="text-align: center;">10.13</td>
</tr>
<tr>
<td style="text-align: center;">Prompt 2</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">34.46</td>
<td style="text-align: center;">11.56</td>
<td style="text-align: center;">86.07</td>
<td style="text-align: center;">10.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">37.38</td>
<td style="text-align: center;">14.22</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">9.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">37.81</td>
<td style="text-align: center;">14.72</td>
<td style="text-align: center;">88.61</td>
<td style="text-align: center;">9.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">37.85</td>
<td style="text-align: center;">14.89</td>
<td style="text-align: center;">88.62</td>
<td style="text-align: center;">9.23</td>
</tr>
<tr>
<td style="text-align: center;">Prompt 3</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">31.89</td>
<td style="text-align: center;">9.78</td>
<td style="text-align: center;">84.16</td>
<td style="text-align: center;">10.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">36.15</td>
<td style="text-align: center;">13.22</td>
<td style="text-align: center;">87.89</td>
<td style="text-align: center;">9.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">37.03</td>
<td style="text-align: center;">13.94</td>
<td style="text-align: center;">88.29</td>
<td style="text-align: center;">9.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">37.11</td>
<td style="text-align: center;">13.95</td>
<td style="text-align: center;">88.34</td>
<td style="text-align: center;">9.23</td>
</tr>
</tbody>
</table>
<p>Table 14: Results of using diverse prompts with 6-shot learning on KELM-sub with Iterative Offline Correction on ChatGPT.
"date Of Retirement" in the triple ["Alan Shepard","date Of Retirement","1963"] is inaccurate. We speculate these behaviours were caused due to the presence of many similar texts with similar graphs in the training data. During training, PiVe learned the potential connections between these similar graphs, thus leading to redundant and inaccurate triples at prediction.</p>
<h2>G Demonstrations in Prompt</h2>
<p>Figure 7 shows the demonstrations used for KELMsub and Figure 8 shows the demonstrations used for WebNLG and GenWiki. In Iteration 1, we use the demonstration that does not contain the missing triples. For subsequent iterations, we include the missing triples in the demonstration.</p>
<h2>H PiVe Examples</h2>
<p>Figure 9 illustrates another example of PiVe from WebNLG test set using single verification module. In Base, the verification module predict the missing triple ["Agremiação Sportiva Arapiraquense", "ground", "Estádio Municipal Coaracy da Mata Fonseca"], even</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T-F1 $\dagger$</th>
<th style="text-align: center;">G-F1 $\dagger$</th>
<th style="text-align: center;">G-BS $\dagger$</th>
<th style="text-align: center;">GED $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt 1</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">43.46</td>
<td style="text-align: center;">26.50</td>
<td style="text-align: center;">87.60</td>
<td style="text-align: center;">7.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">45.68</td>
<td style="text-align: center;">32.50</td>
<td style="text-align: center;">89.86</td>
<td style="text-align: center;">7.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">45.87</td>
<td style="text-align: center;">33.06</td>
<td style="text-align: center;">90.04</td>
<td style="text-align: center;">7.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">45.87</td>
<td style="text-align: center;">33.06</td>
<td style="text-align: center;">90.05</td>
<td style="text-align: center;">7.31</td>
</tr>
<tr>
<td style="text-align: center;">Prompt 2</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">87.28</td>
<td style="text-align: center;">8.47</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">43.64</td>
<td style="text-align: center;">26.61</td>
<td style="text-align: center;">88.87</td>
<td style="text-align: center;">7.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">43.79</td>
<td style="text-align: center;">27.22</td>
<td style="text-align: center;">88.99</td>
<td style="text-align: center;">7.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">43.79</td>
<td style="text-align: center;">27.28</td>
<td style="text-align: center;">89.00</td>
<td style="text-align: center;">7.91</td>
</tr>
<tr>
<td style="text-align: center;">Prompt 3</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">44.30</td>
<td style="text-align: center;">23.89</td>
<td style="text-align: center;">87.36</td>
<td style="text-align: center;">8.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 1</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">29.61</td>
<td style="text-align: center;">89.22</td>
<td style="text-align: center;">7.49</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 2</td>
<td style="text-align: center;">46.84</td>
<td style="text-align: center;">30.06</td>
<td style="text-align: center;">89.28</td>
<td style="text-align: center;">7.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Iteration 3</td>
<td style="text-align: center;">46.85</td>
<td style="text-align: center;">30.08</td>
<td style="text-align: center;">89.30</td>
<td style="text-align: center;">7.45</td>
</tr>
</tbody>
</table>
<p>Table 15: Results of using diverse prompts with 6-shot learning on KELM-sub with Iterative Offline Correction on GPT-4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;"># with PiVe wins</th>
<th style="text-align: center;"># w/o PiVe wins</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KELM-sub</td>
<td style="text-align: left;">31</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">28</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">GenWiki</td>
<td style="text-align: left;">26</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">85</td>
<td style="text-align: center;">20</td>
</tr>
</tbody>
</table>
<p>Table 16: Human evaluation results on 105 samples from three datasets using ChatGPT with or w/o PiVe.
though there is a similar triple but containing mistakes in the prediction from the LLM. In Iteration 1, the LLM corrects the mistakes in the previous iteration, and also includes the predicted missing triple. Based on the prediction from the LLM, the verification module predict the missing triple ["Campeonato Brasileiro Série C", "country", "Brazil"]. In Iteration 2, the verification module predict "Correct" to the final prediction from the LLM. After three iterations using PiVe, the predicted graph contains all information in the reference.</p>
<p>Text: While pop rock can trace its stylistic roots back to rock music, Reggae music evolved out of different musical genre, known as ska. Interestingly, the Train song, Mermaid, belongs to the genre of pop rock, but is also considered to be of the reggae genre as well.</p>
<p>Reference: [["Mermaid Train song", "genre", "Pop rock"], ["Mermaid Train song", "genre", "Reggae"], ["Pop rock", "stylistic Origin", "Rock music"], ["Reggae", "stylistic Origin", "Ska"]]</p>
<p>ChatGPT w/o PiVe: [["pop rock", "stylistic roots", "rock music"], ["Reggae music", "evolved from", "ska"], ["Train song Mermaid", "genre", "pop rock"], ["Train song Mermaid", "also considered", "reggae genre"]]</p>
<p>ChatGPT with PiVe: [["pop rock", "stylistic roots", "rock music"], ["Reggae music", "evolved out of", "ska"], ["Train song Mermaid", "genre", "pop rock"], ["Train song Mermaid", "genre", "reggae"], ["Train song Mermaid", "instrument", "Singing"]]</p>
<p>Text: Alan Shepard was born November 18th, 1923 in New Hampshire and later died in California. Shepard graduated from NWC with an M.A. in 1957 and was also the Chief of the Astronaut Office in 1963.</p>
<p>Reference: [["Alan Shepard", "birth Date", "1923-11-18"], ["Alan Shepard", "served As Chief Of The Astronaut Office In", "1963"], ["Alan Shepard", "death Place", "California"], ["Alan Shepard", "birth Place", "New Hampshire"], ["Alan Shepard", "alma Mater", "NWC M.A. 1957"]]</p>
<p>ChatGPT w/o PiVe: [["Alan Shepard", "birth date", "November 18th, 1923"], ["Alan Shepard", "birth place", "New Hampshire"], ["Alan Shepard", "death place", "California"], ["Alan Shepard", "education", "M.A. from NWC"], ["Alan Shepard", "occupation", "Chief of the Astronaut Office"]]</p>
<p>ChatGPT with PiVe: [["Alan Shepard", "birth place", "New Hampshire"], ["Alan Shepard", "birth date", "November 18th, 1923"], ["Alan Shepard", "death place", "California"], ["Alan Shepard", "education", "M.A. from NWC"], ["Alan Shepard", "occupation", "Chief of the Astronaut Office"], ["Alan Shepard", "status", "Deceased"], ["Alan Shepard", "date Of Retirement", "1963"]]</p>
<p>Figure 6: Two examples of PiVe making two types of mistakes: redundancy and inaccuracy.</p>
<h1>Demonstration for Iteration 1:</h1>
<p>Transform the text into a semantic graph.
Example:
Text: Shotgate Thickets is a nature reserve in the United Kingdom operated by the Essex Wildlife Trust. Semantic Graph: [["Shotgate Thickets", "instance of", "Nature reserve"], ["Shotgate Thickets", "country", "United Kingdom"], ["Shotgate Thickets", "operator", "Essex Wildlife Trust"]]</p>
<h2>Demonstration for Subsequent Iterations:</h2>
<p>Transform the text into a semantic graph and also add the given triples to the generated semantic graph. Example:
Text: Shotgate Thickets is a nature reserve in the United Kingdom operated by the Essex Wildlife Trust. Triples: ["Shotgate Thickets", "instance of", "Nature reserve"], ["Shotgate Thickets", "country", "United Kingdom"] Semantic graph: [["Shot gate Thickets", "instance of", "Nature reserve"], ["Shotgate Thickets", "country", "United Kingdom"], ["Shotgate Thickets", "operator", "Essex Wildlife Trust"]]</p>
<p>Figure 7: The demonstrations used in prompt for KELM-sub.</p>
<h1>Demonstration for Iteration 1:</h1>
<p>Transform the text into a semantic graph.
Example:
Text: Sportpark De Toekomst is located in Ouder-Amstel, Netherlands. It is owned and operated by AFC Ajax N.V. and their tenants include the Ajax Youth Academy.</p>
<p>Semantic graph: [["Sportpark De Toekomst", "location", "Ouder-Amstel"], ["Sportpark De Toekomst", "country", "Netherlands"], ["Sportpark De Toekomst", "owner", "AFC Ajax N.V."], ["Sportpark De Toekomst", "operator", "AFC Ajax N.V."], ["Sportpark De Toekomst", "tenant", "Ajax Youth Academy"]]</p>
<h2>Demonstration for Subsequent Iterations:</h2>
<p>Transform the text into a semantic graph and also add the given triples to the generated semantic graph. Example:
Text: Sportpark De Toekomst is located in Ouder-Amstel, Netherlands. It is owned and operated by AFC Ajax N.V. and their tenants include the Ajax Youth Academy.</p>
<p>Triples: ["Sportpark De Toekomst", "country", "Netherlands"], ["Sportpark De Toekomst", "operator", "AFC Ajax N.V."]</p>
<p>Semantic graph: [["Sportpark De Toekomst", "location", "Ouder-Amstel"], ["Sportpark De Toekomst", "country", "Netherlands"], ["Sportpark De Toekomst", "owner", "AFC Ajax N.V."], ["Sportpark De Toekomst", "operator", "AFC Ajax N.V."], ["Sportpark De Toekomst", "tenant", "Ajax Youth Academy"]]</p>
<p>Figure 8: The demonstrations used in prompt for WebNLG and GenWiki.</p>
<p>Text: Agremiação Sportiva Arapiraquense with 17000 members play in the Brazil Campeonato Brasileiro Série C league. Estádio Municipal Coaracy da Mata Fonseca in located in Arapiraca and ground is Agremiação Sportiva Arapiraquense.</p>
<h2>Base</h2>
<p>LLM Prediction: [["Agremiação Sportiva Arapiraquense", "member count", "17000"], ["Agremiação Sportiva Arapiraquense", "league", "Campeonato Brasileiro Série C"], ["Estádio Municipal Coaracy da Mata Fonseca", "location", "Arapiraca"], ["Estádio Municipal Coaracy da Mata Fonseca", "home ground", "Agremiação Sportiva Arapiraquense"]]</p>
<p>Verification Module Output: ["Agremiação Sportiva Arapiraquense", "ground", "Estádio Municipal Coaracy da Mata Fonseca"]</p>
<h2>Iteration 1</h2>
<p>LLM Prediction: [["Agremiação Sportiva Arapiraquense", "membership", "17000"], ["Agremiação Sportiva Arapiraquense", "league", "Campeonato Brasileiro Série C"], ["Agremiação Sportiva Arapiraquense", "ground", "Estádio Municipal Coaracy da Mata Fonseca"], ["Estádio Municipal Coaracy da Mata Fonseca", "location", "Arapiraca"]]</p>
<p>Verification Module Output: ["Campeonato Brasileiro Série C", "country", "Brazil"]</p>
<h2>Iteration 2</h2>
<p>LLM Prediction: [["Agremiação Sportiva Arapiraquense", "membership", "17000"], ["Agremiação Sportiva Arapiraquense", "league", "Campeonato Brasileiro Série C"], ["Agremiação Sportiva Arapiraquense", "ground", "Estádio Municipal Coaracy da Mata Fonseca"], ["Estádio Municipal Coaracy da Mata Fonseca", "location", "Arapiraca"], ["Campeonato Brasileiro Série C", "country", "Brazil"]]</p>
<p>Verification Module Output: Correct</p>
<p>Reference: [["Agremiação Sportiva Arapiraquense", "league", "Campeonato Brasileiro Série C"], ["Agremiação Sportiva Arapiraquense", "number Of Members", "17000"], ["Agremiação Sportiva Arapiraquense", "ground", "Estádio Municipal Coaracy da Mata Fonseca"], ["Campeonato Brasileiro Série C", "country", "Brazil"], ["Estádio Municipal Coaracy da Mata Fonseca", "location", "Arapiraca"]]</p>
<p>Figure 9: An example from WebNLG test set using single verification module.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We also try other perturbation methods and show the results in Appendix C.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
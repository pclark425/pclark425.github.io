<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7165 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7165</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7165</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e89ee3f84f1f07229a7ba211bad3465d2c80a325</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e89ee3f84f1f07229a7ba211bad3465d2c80a325" target="_blank">Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel group discussion framework to enrich the set of discussion mechanisms of LLMs, and shows that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs.</p>
                <p><strong>Paper Abstract:</strong> Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7165.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7165.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conquer-and-Merge Discussion (CMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A group-style multi-agent discussion framework introduced in this paper where agents are organized into groups, exchange viewpoints and (within-group) explanations across R rounds, then vote (or use a secretary) to decide the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5 (gpt-35-turbo 0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM instance (Azure gpt-35-turbo 0613) used as agent sessions; prompted with task description, optional answer-format and demonstrations; temperature 0.25 in CMD experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-agent iterative discussion (CMD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents generate a viewpoint and explanation, share viewpoints (and group explanations) with other agents, then update their answers across R rounds; final decision by majority vote or by a secretary in tie cases.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (multi-agent rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECQA; GSM8K; FOLIO-wiki</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ECQA: commonsense QA; GSM8K: grade-school math word problems; FOLIO-wiki: symbolic and natural-language deductive reasoning adapted from FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Single-agent baseline (ChatGPT-3.5) — Direct (no demo): ECQA 63.00% acc; GSM8K 69.00% acc; FOLIO-wiki 70.22% acc; Average (Direct) 67.41% acc. Single-agent (with demo): ECQA 67.00%; GSM8K 83.00%; FOLIO-wiki 76.09%; Average (Demo) 75.63%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>CMD (6 agents, ChatGPT-3.5) — Direct: ECQA 64.00% acc; GSM8K 75.00% acc; FOLIO-wiki 73.26% acc; Average (Direct) 70.75% acc. CMD (6 agents) Demo: ECQA 63.00%; GSM8K 83.00%; FOLIO-wiki 77.39%; Average (Demo) 74.46%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports mixed outcomes: CMD improves over single-agent when no demonstrations are available (Direct setting), but does not consistently outperform a strong single agent when demonstrations are present (Demo setting). Identified multi-agent failure modes include 'judge mistake' (incorrect judge/secretary selection) and 'wrong answer propagation' (agents adopting incorrect consensus). CMD also increases token/read overhead as agents must read many viewpoints. Number of rounds was limited to 3 in experiments; larger R not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7165.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Debate (multi-agent turn-based discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A turn-based multi-agent discussion framework where agents iteratively post arguments and incorporate others' responses into subsequent rounds (previous work used 3 rounds by default).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving factuality and reasoning in language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5 (gpt-35-turbo 0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM instance used as agents in debate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Debate multi-round discussion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents are prompted to address the task initially; their responses are appended into each agent's input for the next round; typically run for three rounds (authors also report runs with 3 and 6 agents).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (multi-agent rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECQA; GSM8K; FOLIO-wiki</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense QA, grade-school math, and deductive reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Single-agent (ChatGPT-3.5) baseline (Direct): ECQA 63.00%; GSM8K 69.00%; FOLIO-wiki 70.22%; Average (Direct) 67.41%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Debate (3 agents, ChatGPT-3.5): Table results — FOLIO-wiki and other tasks: e.g., FOLIO-wiki Direct 70.00% (Debate 3), Debate (3) average (Direct) reported in Table 2 as 71.67% across tasks; with demonstrations performance varies (sometimes slightly lower or similar to single-agent).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Debate does not consistently outperform a single strong agent with demonstrations. Paper notes that on ECQA (commonsense dataset) discussion can hurt performance when demonstrations are present, possibly due to overthinking or added complexity. Also susceptible to wrong-answer propagation across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7165.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAD (Multi-Agent Divergent Debate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework encouraging divergent thinking by assigning participants opposing perspectives and using a judge to decide between viewpoints; repeated until consensus or judge selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encouraging divergent thinking in large language models through multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5 (gpt-35-turbo 0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based LLM instance used as debaters and judge roles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>MAD (divergent multi-agent debate with judge)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two or more agents adopt opposing positions and argue; a judge evaluates which viewpoint is more plausible or prompts further discussion; process iterates until decision.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (multi-agent rounds with judge)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECQA; GSM8K; FOLIO-wiki</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense QA, math, deductive reasoning tasks used to evaluate reasoning improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Single-agent (ChatGPT-3.5) baseline (Direct): ECQA 63.00%; GSM8K 69.00%; FOLIO-wiki 70.22%; Average (Direct) 67.41%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>MAD (3 agents, ChatGPT-3.5) — Table 2 (Direct): ECQA 55.00%; GSM8K 74.00%; FOLIO-wiki 61.25%; Average (Direct) 63.42%. With demonstrations performance remains generally lower than the best methods.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>MAD was the least effective in authors' results; authors hypothesize divergent thinking can introduce irrelevant information and complicate decision-making (i.e., adding noise rather than useful critique). Also exposed to judge mistakes and wrong-answer propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7165.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReConcile</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReConcile (round-table consensus with weighted voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A round-table multi-agent framework using weighted voting, where agents provide confidence levels used as vote weights to reach consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reconcile: Round-table conference improves reasoning via consensus among diverse llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-LLM (Bard bison-001 / Gemini Pro / ChatGPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Round-table discussion with heterogeneous LLM agents (the authors used Bard, Gemini Pro, and ChatGPT-3.5 in multi-LLM experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ReConcile round-table consensus (weighted voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents generate answers with confidence; votes are weighted by reported confidences and aggregated to form final decision after several rounds of discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (multi-agent rounds with weighted voting)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECQA; GSM8K; FOLIO-wiki</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense QA, math, deductive reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Single-agent (Gemini Pro) baseline (Direct): ECQA 74.00%; GSM8K 75.00%; FOLIO-wiki 74.13%; Average (Direct) 74.38%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>ReConcile (Bard, Gemini, ChatGPT) — Direct: ECQA 70.00%; GSM8K 78.00%; FOLIO-wiki 80.34%; Average (Direct) 76.11%. With demonstrations ReConcile (Demo) average 78.36%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multi-LLM ReConcile sometimes improves over weaker single agents (notably on FOLIO-wiki), but results are mixed relative to the strongest single agent (Gemini Pro) with demonstrations. Authors note the multi-LLM interactions can help weaker models but may not surpass a strong single agent when demonstrations are present. Also subject to wrong-answer propagation and tie resolution complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7165.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SingleAgent-StrongPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-agent with strong prompt and demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline single-agent evaluation showing that a single LLM given carefully designed prompts and task-specific demonstrations can match multi-agent discussion performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5; Gemini Pro; Bard (separately reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-session LLMs prompted with detailed question descriptions, answer-format descriptions, and task-specific demonstrations when used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Strong prompting with demonstrations (no multi-agent reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>No explicit self-reflection; instead uses task-specific demonstration(s) and detailed prompt decorations (question description, answer format) to elicit chain-of-thought style reasoning in a single pass.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>non-iterative baseline (single inference with demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECQA; GSM8K; FOLIO-wiki</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense QA, math word problems, and deductive reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Reported single-agent (with demonstrations) performance: ChatGPT-3.5 Demo average 75.63% (ECQA 67.00%; GSM8K 83.00%; FOLIO-wiki 76.09%). Gemini Pro Demo average 78.59% (ECQA 75.00%; GSM8K 81.00%; FOLIO-wiki 79.78%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>N/A (this is the non-iterative baseline). Multi-agent methods often reach similar upper-bound performance when demonstrations are present.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors emphasize that strong single-agent prompting (with demonstrations) often matches or exceeds multi-agent discussion performance; thus multi-agent discussion adds cost (more tokens, API calls) and potential failure modes without consistent gains when demonstrations exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7165.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced iterative self-reflection method where a model generates an answer, critiques it, and then refines the answer based on self-feedback (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial solution, produce self-feedback or critique, then iteratively revise the solution using that feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work only; no experimental numbers provided in this paper. The present paper does not evaluate Self-Refine directly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7165.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching Large Language Models to Self-Debug</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced approach where a model is trained or prompted to detect and correct its own mistakes (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-debug / self-correction</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an answer, then run a secondary process to identify logical/factual errors and revise the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (self-evaluation and correction)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Referenced only; no quantitative evaluation inside this paper. Authors list it among related methods for improving reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7165.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling multiple chains-of-thought and voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced method (Wei et al.) that samples multiple chain-of-thoughts, aggregates via majority vote to improve reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple reasoning chains/answers and use voting/aggregation to pick the most consistent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; not directly evaluated in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7165.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7165.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Polish</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Polish: Problem refinement to enhance reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced technique to refine problem statements before solving to improve reasoning outcomes (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-polish: Enhance reasoning in large language models via problem refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Problem refinement / self-polish</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively reformulate/clarify the input problem (or the model's understanding) to reduce ambiguity before or during reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>iterative problem refinement</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited in related work only; no experiments in this paper that directly apply this technique.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-polish: Enhance reasoning in large language models via problem refinement <em>(Rating: 1)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Encouraging divergent thinking in large language models through multi-agent debate <em>(Rating: 2)</em></li>
                <li>Reconcile: Round-table conference improves reasoning via consensus among diverse llms <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7165",
    "paper_id": "paper-e89ee3f84f1f07229a7ba211bad3465d2c80a325",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "CMD",
            "name_full": "Conquer-and-Merge Discussion (CMD)",
            "brief_description": "A group-style multi-agent discussion framework introduced in this paper where agents are organized into groups, exchange viewpoints and (within-group) explanations across R rounds, then vote (or use a secretary) to decide the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5 (gpt-35-turbo 0613)",
            "model_description": "Chat-based LLM instance (Azure gpt-35-turbo 0613) used as agent sessions; prompted with task description, optional answer-format and demonstrations; temperature 0.25 in CMD experiments.",
            "model_size": null,
            "reflection_method_name": "Multi-agent iterative discussion (CMD)",
            "reflection_method_description": "Agents generate a viewpoint and explanation, share viewpoints (and group explanations) with other agents, then update their answers across R rounds; final decision by majority vote or by a secretary in tie cases.",
            "iteration_type": "generate-then-reflect (multi-agent rounds)",
            "num_iterations": 3,
            "task_name": "ECQA; GSM8K; FOLIO-wiki",
            "task_description": "ECQA: commonsense QA; GSM8K: grade-school math word problems; FOLIO-wiki: symbolic and natural-language deductive reasoning adapted from FOLIO.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Single-agent baseline (ChatGPT-3.5) — Direct (no demo): ECQA 63.00% acc; GSM8K 69.00% acc; FOLIO-wiki 70.22% acc; Average (Direct) 67.41% acc. Single-agent (with demo): ECQA 67.00%; GSM8K 83.00%; FOLIO-wiki 76.09%; Average (Demo) 75.63%.",
            "performance_after_reflection": "CMD (6 agents, ChatGPT-3.5) — Direct: ECQA 64.00% acc; GSM8K 75.00% acc; FOLIO-wiki 73.26% acc; Average (Direct) 70.75% acc. CMD (6 agents) Demo: ECQA 63.00%; GSM8K 83.00%; FOLIO-wiki 77.39%; Average (Demo) 74.46%.",
            "improvement_observed": null,
            "limitations_or_failure_cases": "Paper reports mixed outcomes: CMD improves over single-agent when no demonstrations are available (Direct setting), but does not consistently outperform a strong single agent when demonstrations are present (Demo setting). Identified multi-agent failure modes include 'judge mistake' (incorrect judge/secretary selection) and 'wrong answer propagation' (agents adopting incorrect consensus). CMD also increases token/read overhead as agents must read many viewpoints. Number of rounds was limited to 3 in experiments; larger R not evaluated.",
            "uuid": "e7165.0",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Debate",
            "name_full": "Debate (multi-agent turn-based discussion)",
            "brief_description": "A turn-based multi-agent discussion framework where agents iteratively post arguments and incorporate others' responses into subsequent rounds (previous work used 3 rounds by default).",
            "citation_title": "Improving factuality and reasoning in language models through multiagent debate",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5 (gpt-35-turbo 0613)",
            "model_description": "Chat-based LLM instance used as agents in debate experiments.",
            "model_size": null,
            "reflection_method_name": "Debate multi-round discussion",
            "reflection_method_description": "Agents are prompted to address the task initially; their responses are appended into each agent's input for the next round; typically run for three rounds (authors also report runs with 3 and 6 agents).",
            "iteration_type": "generate-then-reflect (multi-agent rounds)",
            "num_iterations": 3,
            "task_name": "ECQA; GSM8K; FOLIO-wiki",
            "task_description": "Commonsense QA, grade-school math, and deductive reasoning benchmarks.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Single-agent (ChatGPT-3.5) baseline (Direct): ECQA 63.00%; GSM8K 69.00%; FOLIO-wiki 70.22%; Average (Direct) 67.41%.",
            "performance_after_reflection": "Debate (3 agents, ChatGPT-3.5): Table results — FOLIO-wiki and other tasks: e.g., FOLIO-wiki Direct 70.00% (Debate 3), Debate (3) average (Direct) reported in Table 2 as 71.67% across tasks; with demonstrations performance varies (sometimes slightly lower or similar to single-agent).",
            "improvement_observed": null,
            "limitations_or_failure_cases": "Debate does not consistently outperform a single strong agent with demonstrations. Paper notes that on ECQA (commonsense dataset) discussion can hurt performance when demonstrations are present, possibly due to overthinking or added complexity. Also susceptible to wrong-answer propagation across rounds.",
            "uuid": "e7165.1",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MAD",
            "name_full": "MAD (Multi-Agent Divergent Debate)",
            "brief_description": "A multi-agent framework encouraging divergent thinking by assigning participants opposing perspectives and using a judge to decide between viewpoints; repeated until consensus or judge selection.",
            "citation_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5 (gpt-35-turbo 0613)",
            "model_description": "Chat-based LLM instance used as debaters and judge roles.",
            "model_size": null,
            "reflection_method_name": "MAD (divergent multi-agent debate with judge)",
            "reflection_method_description": "Two or more agents adopt opposing positions and argue; a judge evaluates which viewpoint is more plausible or prompts further discussion; process iterates until decision.",
            "iteration_type": "generate-then-reflect (multi-agent rounds with judge)",
            "num_iterations": 3,
            "task_name": "ECQA; GSM8K; FOLIO-wiki",
            "task_description": "Commonsense QA, math, deductive reasoning tasks used to evaluate reasoning improvements.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Single-agent (ChatGPT-3.5) baseline (Direct): ECQA 63.00%; GSM8K 69.00%; FOLIO-wiki 70.22%; Average (Direct) 67.41%.",
            "performance_after_reflection": "MAD (3 agents, ChatGPT-3.5) — Table 2 (Direct): ECQA 55.00%; GSM8K 74.00%; FOLIO-wiki 61.25%; Average (Direct) 63.42%. With demonstrations performance remains generally lower than the best methods.",
            "improvement_observed": false,
            "limitations_or_failure_cases": "MAD was the least effective in authors' results; authors hypothesize divergent thinking can introduce irrelevant information and complicate decision-making (i.e., adding noise rather than useful critique). Also exposed to judge mistakes and wrong-answer propagation.",
            "uuid": "e7165.2",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReConcile",
            "name_full": "ReConcile (round-table consensus with weighted voting)",
            "brief_description": "A round-table multi-agent framework using weighted voting, where agents provide confidence levels used as vote weights to reach consensus.",
            "citation_title": "Reconcile: Round-table conference improves reasoning via consensus among diverse llms",
            "mention_or_use": "use",
            "model_name": "Multi-LLM (Bard bison-001 / Gemini Pro / ChatGPT-3.5)",
            "model_description": "Round-table discussion with heterogeneous LLM agents (the authors used Bard, Gemini Pro, and ChatGPT-3.5 in multi-LLM experiments).",
            "model_size": null,
            "reflection_method_name": "ReConcile round-table consensus (weighted voting)",
            "reflection_method_description": "Agents generate answers with confidence; votes are weighted by reported confidences and aggregated to form final decision after several rounds of discussion.",
            "iteration_type": "generate-then-reflect (multi-agent rounds with weighted voting)",
            "num_iterations": 3,
            "task_name": "ECQA; GSM8K; FOLIO-wiki",
            "task_description": "Commonsense QA, math, deductive reasoning benchmarks.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Single-agent (Gemini Pro) baseline (Direct): ECQA 74.00%; GSM8K 75.00%; FOLIO-wiki 74.13%; Average (Direct) 74.38%.",
            "performance_after_reflection": "ReConcile (Bard, Gemini, ChatGPT) — Direct: ECQA 70.00%; GSM8K 78.00%; FOLIO-wiki 80.34%; Average (Direct) 76.11%. With demonstrations ReConcile (Demo) average 78.36%.",
            "improvement_observed": null,
            "limitations_or_failure_cases": "Multi-LLM ReConcile sometimes improves over weaker single agents (notably on FOLIO-wiki), but results are mixed relative to the strongest single agent (Gemini Pro) with demonstrations. Authors note the multi-LLM interactions can help weaker models but may not surpass a strong single agent when demonstrations are present. Also subject to wrong-answer propagation and tie resolution complexities.",
            "uuid": "e7165.3",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SingleAgent-StrongPrompt",
            "name_full": "Single-agent with strong prompt and demonstrations",
            "brief_description": "Baseline single-agent evaluation showing that a single LLM given carefully designed prompts and task-specific demonstrations can match multi-agent discussion performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5; Gemini Pro; Bard (separately reported)",
            "model_description": "Single-session LLMs prompted with detailed question descriptions, answer-format descriptions, and task-specific demonstrations when used.",
            "model_size": null,
            "reflection_method_name": "Strong prompting with demonstrations (no multi-agent reflection)",
            "reflection_method_description": "No explicit self-reflection; instead uses task-specific demonstration(s) and detailed prompt decorations (question description, answer format) to elicit chain-of-thought style reasoning in a single pass.",
            "iteration_type": "non-iterative baseline (single inference with demonstrations)",
            "num_iterations": null,
            "task_name": "ECQA; GSM8K; FOLIO-wiki",
            "task_description": "Commonsense QA, math word problems, and deductive reasoning benchmarks.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Reported single-agent (with demonstrations) performance: ChatGPT-3.5 Demo average 75.63% (ECQA 67.00%; GSM8K 83.00%; FOLIO-wiki 76.09%). Gemini Pro Demo average 78.59% (ECQA 75.00%; GSM8K 81.00%; FOLIO-wiki 79.78%).",
            "performance_after_reflection": "N/A (this is the non-iterative baseline). Multi-agent methods often reach similar upper-bound performance when demonstrations are present.",
            "improvement_observed": null,
            "limitations_or_failure_cases": "Authors emphasize that strong single-agent prompting (with demonstrations) often matches or exceeds multi-agent discussion performance; thus multi-agent discussion adds cost (more tokens, API calls) and potential failure modes without consistent gains when demonstrations exist.",
            "uuid": "e7165.4",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "A referenced iterative self-reflection method where a model generates an answer, critiques it, and then refines the answer based on self-feedback (cited in related work).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Refine (iterative self-feedback)",
            "reflection_method_description": "Generate an initial solution, produce self-feedback or critique, then iteratively revise the solution using that feedback.",
            "iteration_type": "generate-then-reflect (iterative self-feedback)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work only; no experimental numbers provided in this paper. The present paper does not evaluate Self-Refine directly.",
            "uuid": "e7165.5",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Debug",
            "name_full": "Teaching Large Language Models to Self-Debug",
            "brief_description": "Referenced approach where a model is trained or prompted to detect and correct its own mistakes (cited in related work).",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-debug / self-correction",
            "reflection_method_description": "Generate an answer, then run a secondary process to identify logical/factual errors and revise the answer.",
            "iteration_type": "generate-then-reflect (self-evaluation and correction)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Referenced only; no quantitative evaluation inside this paper. Authors list it among related methods for improving reasoning.",
            "uuid": "e7165.6",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (sampling multiple chains-of-thought and voting)",
            "brief_description": "Referenced method (Wei et al.) that samples multiple chain-of-thoughts, aggregates via majority vote to improve reasoning robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Consistency",
            "reflection_method_description": "Sample multiple reasoning chains/answers and use voting/aggregation to pick the most consistent final answer.",
            "iteration_type": "voting over multiple samples",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; not directly evaluated in experiments here.",
            "uuid": "e7165.7",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Polish",
            "name_full": "Self-Polish: Problem refinement to enhance reasoning",
            "brief_description": "Referenced technique to refine problem statements before solving to improve reasoning outcomes (cited in related work).",
            "citation_title": "Self-polish: Enhance reasoning in large language models via problem refinement",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Problem refinement / self-polish",
            "reflection_method_description": "Iteratively reformulate/clarify the input problem (or the model's understanding) to reduce ambiguity before or during reasoning.",
            "iteration_type": "iterative problem refinement",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Cited in related work only; no experiments in this paper that directly apply this technique.",
            "uuid": "e7165.8",
            "source_info": {
                "paper_title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-polish: Enhance reasoning in large language models via problem refinement",
            "rating": 1
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate",
            "rating": 2
        },
        {
            "paper_title": "Reconcile: Round-table conference improves reasoning via consensus among diverse llms",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.018618999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?</h1>
<p>Qineng Wang ${ }^{1 <em>}$ Zihao Wang ${ }^{2 </em>}$ Ying Su ${ }^{2}$ Hanghang Tong ${ }^{3}$ Yangqiu Song ${ }^{2}$<br>${ }^{1}$ Zhejiang University ${ }^{2}$ HKUST ${ }^{3}$ UIUC<br>wongqineng73@zju.edu.cn, htong@illinois.edu<br>{zwanggc,ysuay,yqsong}@cse.ust.hk</p>
<h4>Abstract</h4>
<p>Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) demonstrate strong abilities in language understanding and generation (OpenAI, 2022, 2023; Touvron et al., 2023a; Zhang et al., 2022a; Chowdhery et al., 2022; Team et al., 2023). However, LLMs still fall short for reasoning tasks due to model deficiencies like hallucination and reasoning perspective mistakes ( Xu et al., 2023a). To overcome these issues, numerous works have been proposed by simulating human reasoning. Inspired by Society of Mind (Minsky, 1988), multi-agent discussion frameworks such as Debate (Du et al., 2023), MAD (Liang et al., 2023), and ReConcile (Chen et al., 2023a) present a novel approach by involving multiple AI agents. Powered by LLMs, these agents autonomously engage in discussions on given topics, improving the reasoning abilities of LLMs by emulating the human discussion process. To further improve the performance, most of multi-agent discussion frameworks leverage task-specific examples, which are often termed as demonstrations (Min et al., 2022). This is</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparative performance of single-agent settings and multi-agent discussion frameworks on FOLIOwiki dataset.
based on the insights that LLMs can learn from the context demonstrations (Brown et al., 2020). We note that all these discussion frameworks claim that they outperform the conventional methods with a single agent, such as Chain-of-Thought (Wei et al., 2022).</p>
<p>Figure 1 presents a comparison between singleagent settings and multi-agent discussion frameworks on FOLIO-wiki dataset (Zhang et al., 2023b; Han et al., 2022). In this figure, 'Demo' means that the tested single agent is provided with a demonstration case, 'Q-Desc.' indicates that the single agent is provided with detailed question description, while 'Direct' refers to the single agent without demonstrations. We observe that the single agent 'Demo' tends to reach a performance upper bound similar to that of all discussion frameworks, which will be further elaborated on later. This result suggests that the previous claim is NOT fully established.</p>
<p>Based on the observation from Figure 1, in this paper, we conduct systematic experiments to provide in-depth analysis. As a newly emergent topic, the number of available discussion frameworks still remains limited. To provide a more comprehensive and detailed comparison between the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p><strong>Figure 2:</strong> Our proposed design pipeline of multi-agent discussion frameworks. This pipeline operates by having agents starting with a kick-start prompt. Then, agents will start discussion by obeying the rules defined in the algorithm and come to a result in the end.</p>
<p>Single-agent settings and discussion frameworks, as demonstrated in Figure 3, we propose a new discussion framework named CMD, which is inspired by human group discussion process. Our discussion pipeline considers multiple design aspects shown in Figure 2 (see Section 2.1 for more details). We further carry out a range of experiments over standard reasoning benchmarks (ECQA (Aggarwal et al., 2021), GSM8k (Cobbe et al., 2021), and FOLIOwiki (Han et al., 2022; Zhang et al., 2023b)) using various configurations within different parts of this pipeline.</p>
<p>We find that multi-agent discussion does <em>not</em> necessarily enhance reasoning when the prompt provided to an agent is sufficiently robust, which aligns with the observation from Figure 1. Additionally, our experimental results reveal that multi-agent discussion frameworks outperform single-agent setups when no demonstrations are provided. We also find that agents powered by weaker LLMs like Bard (Anil et al., 2023) can improve its performance on reasoning with the assistance of the stronger LLMs like Gemini Pro (Team et al., 2023) during interaction.</p>
<p>In summary, our analysis provides a new way of understanding when to use multi-agent discussion on reasoning. Our contributions are listed as follows: (1) We propose a new multi-agent discussion framework CMD, which simulates human group discussion process; (2) We observe that single agents with a strong prompt match the performance of a multi-agent discussion using equally robust prompts; (3) We identify two common types of discussion errors: judge mistake and wrong answer propagation; (4) We find that without demonstrations, multi-agent discussions surpass the single agent; (5) In multi-LLMs multi-agent discussions, agents with stronger LLMs help improve performance of agents with weaker LLMs.</p>
<h2>2 Preliminary</h2>
<h3>2.1 What is Multi-Agent Discussion?</h3>
<p>Multi-agent discussion refers to an interactive setup where multiple agents, each powered by an LLM, engage in an autonomous dialogue. Each agent is given a prompt that outlines the necessary background knowledge and guides its behavior throughout the discussion. Once the topic is given, these agents can carry on the conversation independently.</p>
<p>Figure 2 illustrates a discussion design pipeline, which is structured into four main elements: the Kick-start prompt, agents, algorithm, and result. The Kick-start prompt supplies essential details such as background knowledge and the topic for discussion. This information is then fed to an agent, which operates based on various LLMs. Next, the algorithm lays out the specific rules of discussion, including the number of rounds, decision-making processes (whether by a judge or through voting), and the structure of the discussion (such as hierarchical). Lastly, 'Result' describes how the final decisions are rendered and presented. We further provide a view from symmetry for prompt-related mechanism (including discussion) in Appendix A.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overview of the Conquer-and-Merge Discussion (CMD) Framework.</p>
<h3>2.2 Existing Discussion Frameworks</h3>
<p>Previous works on multi-agent discussion frameworks have already covered several common discussion paradigms. In this paper, we consider the following discussion frameworks for our experiments: Debate (Du et al., 2023), MAD (Liang et al., 2023) and ReConcile (Chen et al., 2023a).</p>
<p><strong>Debate</strong> replicates a simple turn-based discussion among agents. Initially, all agents are prompted to address the assigned task, and their respective responses are then incorporated into each agent's input for the subsequent round. Typically, this process consists of three rounds of discussion.</p>
<p><strong>MAD</strong> introduces divergent thinking by involving two participants to discuss a task from opposing perspectives. A judge then steps in to evaluate which viewpoint seems more plausible or if further discussion is needed. The discussion is repeated until a consensus is reached or the judge favors a particular solution.</p>
<p><strong>ReConcile</strong> implements a round-table discussion with the agents powered by multiple LLMs. It reaches a decision through a weighted voting system. In this process, agents provide a confidence level for their answers, and agents in ReConcile utilize these confidence levels as weights to cast votes and arrive at a final decision.</p>
<p>Despite these advancements, the number of multi-agent frameworks is still limited, and none has yet to consider one of the most prevalent forms of discussion: group discussion.</p>
<h3>3 CMD: Conquer-and-Merge Discussion</h3>
<p>To provide a thorough comparison in following experiments, we identify and rectify the gap in previous research, which has not suggested a group-discussion-based framework. To address this, we introduce a novel framework for multi-agent group discussion, referred to as CMD. In CMD, suppose there are <em>n</em> agents A = {<em>A<sub>i</sub></em>}<sup><em>n</em></sup><sub><em>i</em>=1</sub> discussing a question Q, and agent <em>A<sub>i</sub></em> is powered by an LLM L<sub><em>i</em></sub>. We use an array H to store the history responses. The agents will discuss for R rounds. During each turn <em>r</em>, every agent <em>A<sub>i</sub></em> generates a response (<em>v<sub>i</sub></em>, <em>e<sub>i</sub></em>) = CMD(H|Q, <em>r</em>), where <em>v<sub>i</sub></em> is the viewpoint and <em>e<sub>i</sub></em> denotes the explanation. Detailed description can be seen in Appendix B.</p>
<h4>3.1 Message-Passing Algorithm</h4>
<p>For the synchronization of agent communications, we propose a message-passing algorithm. Previous works on multi-agent discussion frameworks focus exclusively on specific scenarios without designing a universal algorithm to synchronize agent messages across various discussions. Moreover, discussion forms vary in architecture and agents can be powered by different LLMs, where each LLM usually possesses different calling protocols. Therefore, we design a message-passing algorithm using a multi-threaded way to overcome these issues. The algorithm establishes a receiving map M for every agent to store the messages they should get in the next turn. For each message <em>m<sub>j</sub></em> from Agent <em>A<sub>i</sub></em>, the algorithm first confirms the receivers A<sub><em>r</em></sub>, then adds <em>m<sub>j</sub></em> into the receiving map M by every agent A<sub><em>k</em></sub> ∈ A<sub><em>r</em></sub>. When the next turn begins, the algorithm will automatically push stored messages from M to corresponding agents. Please refer to Appendix B.4 for more detailed pseudo code.</p>
<h3>3.2 Three Stages of CMD</h3>
<p>As illustrated in Figure 3, CMD consists of three stages: group discussion, voting and the final decision stage. Typically, the final decision stage is reserved for instances of a tie and is otherwise unnecessary. Below is a breakdown of each stage involved in the CMD framework, and detailed introduction can be seen in Appendix B.3.
Stage 1: Group Discussion. During this stage, agents $\mathcal{A}$ are divided into groups $\mathcal{G}=\left{G_{k}\right}<em i="i">{k=1}^{t}$ with an equal size. All agents are asked to solve task $\mathcal{Q}$ through discussion. For each agent $A</em>$ rounds of discussions, CMD moves to the voting stage.
Stage 2: Voting. When discussion reaches the maximum number of rounds, all agents $\mathcal{A}$ start voting. Each vote of agent in this discussion is treated equally, therefore, the result is determined by the majority decision. In the event of a tie, CMD transitions to stage 3. Otherwise, the discussion process concludes formally.
Stage 3: Final Decision. If a tie occurs, we introduce an extra agent $S$ in the role of the secretary to make the final decision. Each proposed answer is accompanied by an explanation selected from agents with the same opinion and provided to the secretary for the final determination. To sum up, the final decision is made by either $\operatorname{Vote}(\mathcal{A})$ or $S(V, O \mid \mathcal{Q})$ where $V=\left{v_{i}\right}^{n}$ and $O=\left{o_{i}\right}^{n}$, representing a set of viewpoints and a set of explanations respectively.} \in$ $G_{k}$, all answers and explanations from $A_{i} \in G_{k}$ in the last round are accessible where $A_{j} \in G_{k}$ and $j \neq i$. In contrast, they can only see the answers without explanations from agents in other groups. After $\mathcal{R</p>
<h2>4 Experimental Setups</h2>
<p>In the experiments, we contrast single-agent setup with four established multi-agent discussion frameworks: Debate, MAD, ReConcile, and CMD under various prompt conditions. These methods are introduced in previous Sections 2.2 and 3.</p>
<h3>4.1 Implementation Details and Metrics</h3>
<p>Our experiments are primarily implemented with three advanced LLMs, including ChatGPT-3.5 (OpenAI, 2022), Gemini Pro (Team et al., 2023) and Bard (Anil et al., 2023). In particular, we employ the gpt-35-turbo (0613) instance hosted on Azure OpenAI ${ }^{1}$ for ChatGPT-3.5, while the chat-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>bison-001 model represents Bard via PaLM2 architecture. Gemini Pro and Bard interfaces operate through Google MakerSuite API ${ }^{2}$. A uniform dialogue temperature of 0.25 is configured across LLMs on CMD to ensure consistency. For all multiagent discussion frameworks, we set the maximum discussion round number to 3 . Our evaluations use accuracy to measure performance across all tasks.</p>
<h3>4.2 Downstream Tasks</h3>
<p>The frameworks are implemented on a suite of reasoning tasks, including a commonsense reasoning task and two deductive reasoning tasks: (1) ECQA (Aggarwal et al., 2021): A QA dataset centered on commonsense knowledge, (2) GSM8K (Cobbe et al., 2021): A benchmark consists of math word problems, (3) FOLIO-wiki (Zhang et al., 2023b): A dataset adapted from FOLIO (Han et al., 2022) for both symbolic and natural language deductive reasoning. In alignment with constraints imposed by computational resources and following precedents set by earlier research (Du et al., 2023; Chen et al., 2023a; Liang et al., 2023), a subset of 100 instances from the test sets of both ECQA and GSM8K are selectively sampled. For an in-depth analysis, we choose to conduct a comprehensive experiments of all 460 cases within the curated version of the FOLIO-wiki dataset, which removes the flawed cases to ensure the result authenticity.</p>
<h2>5 Experiments on Single LLM</h2>
<p>In this section, we conduct our experiments using a single LLM, ChatGPT-3.5. To gain the initial insights, we provide an in-depth analysis of the FOLIO-wiki dataset, examining both single-agent settings and multi-agent discussions as detailed in Section 5.1. Afterward, we extend our experiments to two other datasets GSM8K and ECQA in Section 5.2. We also investigate common mistakes made by discussions through a case study in Section 5.3. Finally, we summarize our findings in Section 5.4.</p>
<h3>5.1 Analysis of FOLIO-wiki Dataset</h3>
<p>We begin by examining if multi-agent discussions are more effective than an agent using the strongest prompt on FOLIO-wiki dataset. Drawing from previous research (Wei et al., 2022; Ling et al., 2023) on crafting prompts for reasoning tasks, we divide the prompt into three parts: a detailed question description, which provides an in-depth background</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt Components</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multi-Agent Discussion (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Single <br> Agent (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q-Desc.</td>
<td style="text-align: center;">A-Desc.</td>
<td style="text-align: center;">Demo.</td>
<td style="text-align: center;">MAD (3)</td>
<td style="text-align: center;">Debate (3)</td>
<td style="text-align: center;">Debate (6)</td>
<td style="text-align: center;">CMD (6)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">64.13</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">69.13</td>
<td style="text-align: center;">73.26</td>
<td style="text-align: center;">70.22</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">75.65</td>
<td style="text-align: center;">76.30</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">73.26</td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">68.91</td>
<td style="text-align: center;">71.96</td>
<td style="text-align: center;">71.74</td>
<td style="text-align: center;">73.89</td>
<td style="text-align: center;">71.30</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">71.96</td>
<td style="text-align: center;">70.22</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">71.09</td>
<td style="text-align: center;">73.91</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">75.65</td>
<td style="text-align: center;">74.78</td>
<td style="text-align: center;">77.39</td>
<td style="text-align: center;">76.09</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparative performance of single-agent settings and multi-agent discussions on FOLIO-wiki using ChatGPT-3.5. Abbreviations are: detailed question descriptions (Q-Desc.), and answer format descriptions (ADesc.), demonstrations (Demo.). Only the question itself is used as input when prompt components are disabled. The number next to the framework represents the number of agents.
of the task; an answer format description, which instructs how an agent should reply; and a taskspecific demonstration, which shows an example of a question and answer pair. For this task, we meticulously craft a demonstration for input prompt. We start by labeling each premise. After that, we quote all the premises and relate them to every step in the reasoning process by using these labels, until the final step is reached. A labeled example is similar to the case in Table 5. We then test the performance of various combinations of these components for both single agents and discussions.
Single Agent. We conduct an evaluation of different prompt components with a single agent, and the results are outlined in Table 1. This table shows that for both single-agent settings and multi-agent discussions, the inclusion of a detailed question description or a task-specific demonstration enhances reasoning abilities on the FOLIO-wiki dataset. The detailed question description is helpful because the possible answers to judge the correctness of a given proposition-true, false, or unknown-require clarity. Without such clarity, agents often struggle to differentiate between what is 'false' and what is 'unknown'. Most notably, the addition of a demonstration contributes significantly to improved performance, highlighting its value as the most impactful component, in line with what prior studies have suggested (Min et al., 2022).
Multi-Agent Discussions. We assess the same prompt components within multi-agent discussion setups, and Table 1 reveals three key insights: (1) Demonstrations and detailed question description enhance multi-agent discussions. (2) Despite similar overall performance, most multi-agent discussions do not surpass the single agent when a demonstration is introduced. (3) CMD performs better than both single-agent setups and other multi-agent dis-
cussion frameworks on the FOLIO-wiki dataset. Below is further analysis of our findings.
A Strong Single Agent is Comparable to Discussion Frameworks. Analyzing experiments with single-agent settings and multi-agent discussions on the FOLIO-wiki dataset, we find that taskspecific demonstrations significantly enhance a single agent's performance. Additionally, we establish that a well-supported agent can perform on par with discussion frameworks. Our analysis indicates that prompt engineering can boost reasoning performance in large language models, with demonstrations in both single-agent and multi-agent discussions pushing towards the upper bound of performance.</p>
<h3>5.2 Evaluation on All Tasks</h3>
<p>In Section 5.1, we discover that demonstrations play a pivotal role in enhancing performance on FOLIO-wiki dataset. With this insight in mind, we simplify our evaluation to two prompt scenarios: with (referred as direct) and without demonstrations (referred as demo). The results presented in Table 2 cover all tested reasoning tasks. The findings show: (1) With demonstrations, discussion frameworks and single-agent settings have comparable performance on average. This is consistent with our earlier observations. (2) Without demonstrations, CMD tend to surpass single-agent settings, both on average and in most individual tasks.
When Does Discussion Work Better? Most Multi-agent discussion frameworks, especially CMD, achieve better performance compared with single-agent settings when neither is supported by demonstrations. We believe this is because, during discussions, the input from other agents can introduce new perspectives, leading to a more thorough reasoning process. Therefore, this collaborative</p>
<p>| Method | ECQA | | GSM8K | | FOLIO-wiki | | Average | |
| | Direct | Demo | Direct | Demo | Direct | Demo | Direct | Demo |
| Single Agent | 63.00 | 67.00 | 69.00 | 83.00 | 70.22 | 76.09 | 67.41 | 75.63 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| MAD (3 Agents) | 55.00 | 58.00 | 74.00 | 78.00 | 61.25 | 74.13 | 63.42 | 70.04 |
| Debate (3 Agents) | 67.00 | 65.00 | 78.00 | 81.00 | 70.00 | 75.65 | 71.67 | 73.88 |
| Debate (6 Agents) | 65.00 | 64.00 | 74.00 | 78.00 | 69.13 | 74.78 | 69.38 | 72.26 |
| CMD (6 Agents) | 64.00 | 63.00 | 75.00 | 83.00 | 73.26 | 77.39 | 70.75 | 74.46 |</p>
<p>Table 2: Results for all tasks, with and without demonstration settings included. Using ChatGPT-3.5.
advantage makes multi-agent discussions a more effective option in scenarios lacking specific expert knowledge or detailed examples.
Why Does Discussion Frameworks Perform Differently on Tested Tasks? Table 2 indicates that MAD is the least effective among the frameworks tested. We suspect that this is because MAD incorporates a divergent thinking way, which asks agents to disagree with each other. This can sometimes hinder reasoning by introducing irrelevant information that complicates decision-making. We explore this further with an error analysis in Section 5.3. Additionally, we observed unexpected behaviors from other discussion frameworks under specific conditions or tasks. For example, Debate and CMD perform worse on ECQA dataset when demonstrations are introduced. We hypothesize that because ECQA demands more commonsense knowledge than purely analytical reasoning, the single-source interaction stemming from a single LLM might cause agents to overthink. Instead of clarifying misunderstandings, the increased dialogue may introduce complexity without addressing the underlying knowledge gaps. In contrast to ECQA, datasets like GSM8K and FOLIO-wiki place a greater emphasis on deductive reasoning abilities. In these cases, the discussion process between agents can be beneficial as it allows them to identify and address flaws in each other's reasoning through interaction.</p>
<h3>5.3 Two Discussion Error Types: A Case Study</h3>
<p>Our experiments show that multi-agent discussions can sometimes reach incorrect conclusions on questions that a single agent answers correctly. Figure 4 presents an example from the FOLIO-wiki dataset illustrating this point: a single agent provides the correct answer, but multi-agent discussions lead to an erroneous result. We identify two
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Two common types of errors that may occur in multi-agent discussions are judge mistake and wrong answer propagation. These issues can lead to circumstances where a multi-agent discussion reaches an incorrect conclusion, even if single agent can arrive at the correct one.
unique types of errors in multi-agent discussions: (1) Judge Mistake: This occurs in situations where an agent serves as a judge to decide on the final answer. If there are varying responses among agents, the judge might select the incorrect option as the final verdict. Frameworks like MAD and CMD are susceptible to this error, particularly when decisions are made during a tie. (2) Wrong Answer Propagation: This type of error happens when an agent, influenced by the input from others, deviates from its initial correct answer and adopts an incorrect consensus, spreading the mistake further in the discussion. This is the most common mistake the multi-agent discussion can make, even when most of their initial answers are correct.</p>
<h3>5.4 Summary</h3>
<p>In this section, we evaluate various prompt component combinations for both single-agent settings and multi-agent discussions. Our findings suggest that multi-agent discussions are on par with a single agent when both have access to demonstrations. However, in the absence of demonstrations, multiagent discussions generally outperform a single agent, making them a better option in scenarios where expert knowledge or detailed examples are insufficient. We also highlight unusual outcomes and present a case study to identify two frequent errors in multi-agent discussions: Judge Mistake and Wrong Answer Propagation.</p>
<h2>6 Experiments on Multiple LLMs</h2>
<p>In this section, we expand our experimental scope from a single LLM to multiple LLMs for both single-agent settings and multi-agent discussion frameworks, which allows us to test the validity of our previous findings in multi-LLM scenarios. Specifically, we assess the performance of agents powered by three advanced LLMs: ChatGPT-3.5, Gemini Pro, and Bard. In Section 6.1, we compare the performance of three single-agent configurations, each using a different LLM, against multiagent discussions leveraging all three LLMs. Our round-level analysis in Section 6.2 yields another insight: an agent powered by stronger LLM can enhance the performance of an agent powered by a less capable LLM.</p>
<h3>6.1 Validate Findings on Multiple LLMs Scenarios</h3>
<p>In this section, we evaluate the performance of three single-agent settings, each supported by a different LLM, and two multi-agent discussion frameworks, ReConcile and CMD, across all tasks. We utilize two types of prompt settings for each task: one with demonstrations and one without. For the CMD framework, we organize six agents into two groups of three, with each agent in a group powered by one of the LLMs: ChatGPT-3.5, Gemini Pro, or Bard. The results presented in Table 3 support our previous findings from Section 5 with some slight modifications.
A Strong Single Agent is Comparable to Discussion Frameworks. Based on experimental results from Table 3, we can find that discussion frameworks perform comparably to a single agent powered by Gemini Pro when both are provided</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Round-level performance of each LLM in multi-agent discussions on FOLIO-wiki dataset.
with demonstrations. This consolidates our earlier conclusion that a 'strong' single agent-supported by both a well-designed prompt and a SOTA LLM—can rival the performance of a multi-agent discussion framework.
Discussion Frameworks Outperform Single Agents with No Demonstration. Table 3 reveals that, in multi-LLM scenarios, multi-agent discussions outperform single agents when demonstrations are not provided. This outcome aligns with our previous observations in single LLM settings. Furthermore, CMD and ReConcile demonstrate similar performance when they both have no access to demonstrations and they are both powered by same LLMs. This indicates that our findings are consistent on different multi-LLM multi-agent discussions frameworks.</p>
<h3>6.2 Enhancing Agents in Weaker LLMs with Support from Stronger LLMs</h3>
<p>As shown in Table 3, single agents using Bard show the least effectiveness in reasoning tasks such as those in the FOLIO-wiki dataset. However, multiLLM multi-agent frameworks remain competitive. To understand how a less advanced LLM like Bard performs during multi-agent discussions, we further study the round-by-round performance of each LLM engaged in the discussions. Figure 5 demonstrates that agents with less capable LLMs like Bard and ChatGPT-3.5 gradually enhance their performance over consecutive rounds with the support of the more robust LLM, Gemini Pro. We infer that throughout the discussion, Gemini Pro assists in bridging the gaps in knowledge and reasoning for the less advanced LLMs, guiding towards a stronger line of reasoning. Notably, although there is a slight drop in the performance of Gemini Pro during the second round, it demonstrates resilience and recovers swiftly, largely maintaining its superior performance. Again, this finding is applicable to different multi-LLM discussions, specifically for</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method Category</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">ECQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FOLIO-wiki</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">Demo</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">Demo</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">Demo</td>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">Demo</td>
</tr>
<tr>
<td style="text-align: center;">Single Agent</td>
<td style="text-align: center;">Bard</td>
<td style="text-align: center;">66.00</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">47.00</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">71.96</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">63.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gemini Pro</td>
<td style="text-align: center;">74.00</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">81.00</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">79.78</td>
<td style="text-align: center;">74.38</td>
<td style="text-align: center;">78.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT-3.5</td>
<td style="text-align: center;">63.00</td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">69.00</td>
<td style="text-align: center;">83.00</td>
<td style="text-align: center;">70.22</td>
<td style="text-align: center;">76.09</td>
<td style="text-align: center;">67.41</td>
<td style="text-align: center;">75.63</td>
</tr>
<tr>
<td style="text-align: center;">Discussion</td>
<td style="text-align: center;">ReConcile (Bard, Gemini, ChatGPT)</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">78.00</td>
<td style="text-align: center;">83.00</td>
<td style="text-align: center;">80.34</td>
<td style="text-align: center;">81.09</td>
<td style="text-align: center;">76.11</td>
<td style="text-align: center;">78.36</td>
</tr>
<tr>
<td style="text-align: center;">Group Discussion</td>
<td style="text-align: center;">CMD (Bard, Gemini, ChatGPT)</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">78.00</td>
<td style="text-align: center;">82.00</td>
<td style="text-align: center;">79.78</td>
<td style="text-align: center;">81.96</td>
<td style="text-align: center;">76.93</td>
<td style="text-align: center;">78.66</td>
</tr>
</tbody>
</table>
<p>Table 3: Results from single-agent and CMD across multiple LLMs on all tasks evaluated with two types of prompts: with demonstrations and without.
both ReConcile and CMD.</p>
<h2>7 Related Work</h2>
<h3>7.1 Prompting LLM for Reasoning</h3>
<p>Recent researches have experienced great progresses in building powerful LLMs (Brown et al., 2020; OpenAI, 2022, 2023) or exploring the strategy of adopting LLMs over many downstream tasks via prompt enigineering.</p>
<p>By training with different knowledge textual sources and parameter size, various LLMs equipped with different reasoning capabilities are constructed, such as OPT (Zhang et al., 2022a), LLaMA (Touvron et al., 2023a,b), BLOOM (Scao et al., 2022), and PaLM (Chowdhery et al., 2022; Anil et al., 2023). Recently, Gemini Pro (Team et al., 2023) extends the capabilities of LLMs to the field of multi-modality.</p>
<p>Numerous advancements have been made in the field of improving reasoning abilities of LLMs with prompt engineering. Chain of Thought (CoT) (Wei et al., 2022; Kojima et al., 2022) is a linear problem-solving approach where each step builds upon the previous one. Fu et al. (2022) propose to apply CoT to multi-step reasoning tasks. To automate the CoT, Auto-CoT (Zhang et al., 2022b) constructs demonstrations by sampling diverse questions and generating reasoning chains. ActivePrompt (Diao et al., 2023) aims to select the most uncertain questions for task-specific annotations. Other prompt strategies designed to enhance reasoning in LLMs include the PS Prompt (Wang et al., 2023), which breaks tasks into subtasks, ToT (Yao et al., 2023a) which expands on the reasoning process by considering multiple paths of reasoning and self-evaluating choices, the effective GoT (Yao et al., 2023b), which frames thoughts as graphs, Natural Program (Ling et al., 2023) which helps to improve the deductive reasoning tasks, re-reading prompt (Xu et al., 2023b) which revisits the ques-
tion information embedded within input prompts.</p>
<h3>7.2 Multi-agent Discussion for Reasoning with LLMs</h3>
<p>Multi-agent discussion utilizes multiple LLMs as agents to collectively discuss and reason given problems in an interactive way. Abundant researches have explored how to improve the reasoning ability of single LLM, while multi-agent discussion among LLMs is still under exploration.</p>
<p>The Multi-Agent Debate framework, introduced by (Du et al., 2023), establishes a mechanism for symmetric discussions among agents. During the same period, the MAD (Multi-Agent Debate) framework (Liang et al., 2023) introduces an asymmetric mechanism design. It assigns different roles (debater and judge) asymmetrically. Other similar works include (Chan et al., 2023). Also, the ReConcile framework (Chen et al., 2023a) exemplifies an asymmetric discussion mechanism by involving different LLMs and using a weighted voting mechanism. To understand discussion more deeply, Zhang et al. (2023a) aim to explain such collaboration mechanism in a social psychology view.</p>
<p>Unlike these works, we aim to explore the potential effects of prompting contents over the discussion process by our defined multi-agent group discussion framework CMD.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we re-examine the claim that multiagent discussions are superior to a single agent in reasoning tasks by conducting systematic experiments. We introduce a novel framework CMD for a comprehensive and fair assessment. By conducting experiments over standard benchmarks, we find that (1) A single agent with a strong prompt and powered by a strong LLM achieves comparable performance with multi-LLM multi-agent discussions; (2) In the absence of demonstrations, multi-agent discussion frameworks outperform single agents on</p>
<p>most tasks; (3) When multiple LLMs are involved in multi-agent discussions, agents with stronger LLMs can enhance the performance of agents with weaker LLMs as discussion progresses.</p>
<h2>9 Ethical Considerations</h2>
<p>Our study employs publicly available datasets and LLMs accesses via official APIs, ensuring responsible and ethical use. Specifically, our ethical considerations can be summarized as follows:
Public Datasets. Datasets we use are designed for academic research. No personal data has been processed.
Licensed API Usage. Our application of LLMs complies with the API usage policies, maintaining fair use standards and respecting intellectual property.
Transparency. We provide detailed experimentation methods to allow for result reproduction and encourage transparent scientific practices.</p>
<h2>10 Limitations</h2>
<p>Our research offers comprehensive experiments to study the performance of a strong single agent and multi-agent discussions. However, several aspects highlighted below can be further refined and explored in future work.
Enhancing Agent Complexity. Currently, all discussion frameworks including CMD considers an LLM session as an AI Agent. This perspective simplifies the the concept of LLM-based AI Agent defined in the literature (Weng, 2023). By integrating more sophisticated techniques such as Tree-ofThought (Yao et al., 2023a) or Cumulative Reasoning (Zhang et al., 2023b), or incorporating with external tools or knowledge bases, we could potentially improve the overall reasoning performance of multi-agent discussions.
Expanding Task Diversity. While our study mainly focuses on reasoning tasks for assessing both single-agent settings and multi-agent discussions, the adaptive nature of discussions allows for a broader types of applications. Future research could explore the use of agent discussions in diverse scenarios such as real-world strategic planning or the integration of agents into interactive gaming environments.
Experimenting with Additional LLMs. Due to computational and financial constraints, our investigation is limited to testing three LLMs-Bard, Gemini Pro, and ChatGPT-3.5. Expanding our anal-
ysis to include additional LLMs could provide a more extensive understanding of the capabilities and variances across different language models, offering valuable insights into the generalizability and scalability of our findings in multi-agent discussion frameworks.</p>
<h2>References</h2>
<p>Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for commonsenseqa: New dataset and models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3050-3065.</p>
<p>Philip W Anderson. 1972. More is different: Broken symmetry and the nature of the hierarchical structure of science. Science, 177(4047):393-396.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. 2023. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201.</p>
<p>Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. 2023a. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023b. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Constantinos Daskalakis and Seth Matthew Weinberg. 2012. Symmetries and optimal multi-dimensional mechanism design. In Proceedings of the 13th ACM conference on Electronic commerce, pages 370-387.</p>
<p>Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active prompting with chain-ofthought for large language models. arXiv preprint arXiv:2302.12246.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.</p>
<p>Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomposing complex questions. arXiv preprint arXiv:2212.04092.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. 2022. Folio: Natural language reasoning with firstorder logic. arXiv preprint arXiv:2209.00840.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Jean-Jacques Laffont and David Martimort. 2000. Mechanism design with collusion and correlation. Econometrica, 68(2):309-342.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118.</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872.</p>
<p>Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M Dai. 2022. Mind's eye: Grounded language model reasoning through simulation. arXiv preprint arXiv:2210.05359.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842.</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv:2201.06009.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Marvin Minsky. 1988. Society of mind. Simon and Schuster.</p>
<p>OpenAI. 2022. Chatgpt. https://openai.com/ blog/chatgpt.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Kristopher Tapp. 2021. Symmetry. Springer.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. arXiv preprint arXiv:2305.04091.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Lilian Weng. 2023. Llm-powered autonomous agents. lilianweng.github.io.</p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. 2022. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561.</p>
<p>Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. Self-polish: Enhance reasoning in large language models via problem refinement. arXiv preprint arXiv:2305.14497.</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023a. Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841.</p>
<p>Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, and Jian-guang Lou. 2023b. Re-reading improves reasoning in language models. arXiv preprint arXiv:2309.06275.</p>
<p>Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji. 2023. Rcot: Detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought. arXiv preprint arXiv:2305.11499.</p>
<p>Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, and Xiaodan Liang. 2022. Logicsolver: Towards
interpretable math word problem solving with logical prompt-enhanced learning. arXiv preprint arXiv:2205.08232.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas I. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Yao Yao, Zuchao Li, and Hai Zhao. 2023b. Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. arXiv preprint arXiv:2305.16582.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488.</p>
<p>Jintian Zhang, Xin Xu, and Shumin Deng. 2023a. Exploring collaboration mechanisms for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a. Opt: Open pre-trained transformer language models. arXiv e-prints, pages arXiv-2205.</p>
<p>Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. 2023b. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<h1>A Discussion Engineering and Agent Symmetry</h1>
<p>This section introduces a general framework to understand the discussion engineering of large language models. Let $\mathcal{L}$ be a Large Language Model (LLM) and $x$ be the input text, the forward inference generates a response of surprising quality, which is written as</p>
<p>$$
\hat{y}=\mathcal{L}(x)
$$</p>
<p>Given the high price of obtaining one large language model, one essential research question is to unlock the reasoning capability of large language models so that they can perform better on various tasks. One of the key directions is prompt engineering, where the format and content of the input texts are decorated to improve performances for downstream tasks. Specifically, for a task $\mathcal{T}$, the original input $x$ is decorated as a task-specific prompt input $p(x ; \mathcal{T}, \mathcal{L})$. The function $p(\cdot ; \mathcal{T}, \mathcal{L})$ is denoted as a prompt decorator for $\mathcal{T}$ and $\mathcal{L}$. Then, output $\hat{y}$ is generated by prompt engineering, which is written as</p>
<p>$$
\hat{y}=\mathcal{L}(p(x ; \mathcal{T}, \mathcal{L}))
$$</p>
<p>Another way to improve the reasoning capability of $\mathcal{L}(x)$ is to leverage multiple times of inference of large language models, leading to a way larger space for mechanism design. The output $\hat{y}$ derived by mechanism $\mathcal{M}$ is considered as</p>
<p>$$
\hat{y}=M\left(x ;\left{\left(\mathcal{L}<em i="i">{i}, p</em>}\left(\cdot, \mathcal{T}, \mathcal{L<em i="1">{i}\right)\right)\right}</em>\right)
$$}^{n</p>
<p>where $M$ is the mechanism pipeline, $\mathcal{L}<em i="i">{i}$ is the $i$-th inference of LLM, and $p</em>}\left(\cdot, \mathcal{T}, \mathcal{L<em i="i">{i}\right)$ is the corresponding prompt decorator. A mechanism $\mathcal{M}$ is designed by jointly considering $\mathcal{M}=\left(M,\left{\left(\mathcal{L}</em>\right)$, i.e., how to organize prompt decorated LLM inferences into the mechanism pipeline $M$.}, p_{i}\right)\right}_{i=1}^{n</p>
<p>The discussion engineering, characterized by the mechanism $\mathcal{M}=\left(M,\left{\left(\mathcal{L}<em i="i">{i}, p</em>\right)\right}<em i="i">{i=1}^{n}\right)$, includes careful designs at three levels: (1) the ( $i$-th) inference $\mathcal{L}</em>}$ of an LLM, (2) the prompt decorator $p_{i}$ for the inference $\mathcal{L<em i="i">{i}$, and (3) the mechanism $M\left(\cdot,\left{\mathcal{L}</em>$. Under our framework, several examples are discussed.}, p_{i}\right}\right)$ to organize the inferences of LLMs. We further introduce the symmetry of the multi-agent system of LLMs to demonstrate the complexity of $\mathcal{M</p>
<h2>A. 1 Agent symmetry in discussion engineering</h2>
<p>Symmetry and its breaking is a fundamental concept and widely investigated in science (Anderson, 1972). Symmetry also plays an important role in the context of mechanism design of multiple agents (Laffont and Martimort, 2000; Daskalakis and Weinberg, 2012). Here we introduce a formal description to justify the agent symmetry of the mechanism $\mathcal{M}$.
Definition 1 (Computational graph). Given the mechanism $\mathcal{M}=\left(M,\left{\left(\mathcal{L}<em i="i">{i}, p</em>\right)\right}<em i="i">{i=1}^{n}\right)$, let $G=(V \cup$ ${x, y}, E)$ be the directed graph whose node set $V \cup{x, y}$ includes the inference operation of LLM and the computational input node and output nodes. $x$ is the node to emit the input text, $y$ is the node that takes the inputs from one or multiple inferences and emits the final output, and $v</em>} \in V$ is the inference node that describes the $i$-th LLM inference with $\left(\mathcal{L<em i="i">{i}, p</em>$.}\right)$. The directed edge in $\left(v_{i}, v_{k}\right) \in E$ describes the output of inference node $v_{i}$ is then the input of inference node $v_{k</p>
<p>We note that the graph $G$ contains all the necessary information to determine the mechanism $\mathcal{M}$.
Then we introduce how the LLM agents are associated with the computational graph. To make an LLM conversational session, the entire conversational history is always concatenated as part of the input of each inference call, resulting in many additional connections of the computational graph. The presence of agents allows us to conceptually assume the agents "know" the information in the conversation history, and then the complexity of the graph can be largely reduced. In this paper, we consider the computational graph with agents, so the connections that feed conversation history to the inference nodes are ignored for simplicity.</p>
<p>Let $\mathcal{A}=\left{A_{1}, \ldots, A_{m}\right}$ be the set of $m$ discussion agents. The inference call of an agent also concatenates its conversation history, denoted as $A_{j}(x)$.</p>
<p>Definition 2 (Agent assignment). Let $\mathcal{A}=\left{A_{1}, \ldots, A_{m}\right}$ be the set of $m$ discussion agents. Each inference node $v_{i} \in V$ is assigned to an agent $A_{j} \in \mathcal{A}$. Let $[n]$ present integers from 1 to $n, \alpha:[n] \mapsto[n]$ is the assignment map that assigns the $i$-th inference to the $\alpha(i)$-th agent. Let $P \in{0,1}^{n \times m}$ be the agent-inference assignment matrix, such that</p>
<p>$$
P_{i j}= \begin{cases}1 &amp; j=\alpha(i) \ 0 &amp; j \neq \alpha(i)\end{cases}
$$</p>
<p>The assignment matrix $P$ is equivalent to the assignment map $\alpha$.
Then a multi-agent discussion mechanism is defined by the triple $D=(\mathcal{M}, \mathcal{A}, \alpha)=(G, \mathcal{A}, \alpha)$, which is the central object of discussion engineering. Then, we can further introduce the concept of the coloring of the computational graph.
Definition 3 (Agent coloring of the computational graph). Given the the multi-agent discussion $D=$ $(G, \mathcal{A}, \alpha)$, then each inference node $v_{i}$ is described by $\left(\mathcal{L}<em i="i">{i}, p</em>\right)$ be a colored computational graph.}\right)=\left(A_{\alpha(i)}, p_{i}\right)$. Let $c_{i}^{D}=\left(A_{\alpha(i)}, p_{i}\right)$ be the color of $v_{i}$ and $C^{D}\left(v_{i}\right)=c_{i}^{D}$ be the color mapping of nodes, and $\mathcal{C}^{D}=\left(V, E, C^{D</p>
<p>Then the agent symmetry is established by the permutation operation over the agent set $\mathcal{A}$. Then we can discuss the symmetry by the agent permutation. We begin with the concept of mechanism invariance.
Definition 4 (Mechanism invariance under the agent permutation). Given the multi-agent discussion $D=(G, \mathcal{A}, \alpha)$ and a permutation mapping $\pi:[m] \mapsto[m]$, the new discussion $D_{\pi}=(G, \mathcal{A}, \pi \circ \alpha)$ is derived by applying $\pi$ to the agents. We say a discussion $D$ is invariant under $\pi$ if and only if there is an isomorphism $\phi$ between two colored graphs $\mathcal{C}^{D}=\left(V, E, C^{D}\right)$ and $\mathcal{C}^{D_{\pi}}=\left(V, E, C^{D_{\pi}}\right)$, such that
(1) $\phi$ is a bijection,
(2) $\forall\left(v_{i}, v_{j}\right) \in E,\left(\phi\left(v_{i}\right), \phi\left(v_{j}\right)\right) \in E$,
(3) $\forall v \in V, C^{D}(v)=C^{D_{\pi}}(\phi(v))$.</p>
<p>We can also define the model invariance to justify the symmetry in the multi-model setting (Chen et al., 2023a).
Definition 5 (Model invariance under the agent permutation). Given the agent $\mathcal{A}=\left{A_{1}, \ldots, A_{m}\right}$, and their underlying LLM $\left{L_{1}, . ., L_{m}\right}$ (i.e., ChatGPT, GPT4, etc.), the permutation $\pi:[m] \mapsto[m]$ is invariant if for $i=1, \ldots, m L_{i}=L_{\pi(i)}$.</p>
<p>Following the definition of invariance, the symmetry group of agents is naturally defined following the standard algebra (Tapp, 2021). The largest possible symmetry group for a discussion of $m$ agents is the group $S_{m}$. However, the group $S_{m}$ is not always the symmetry group of a given multi-agent discussion mechanism $D$, the reasons for not achieving the largest symmetry group can be due to the asymmetry in the mechanism and asymmetry in models, respectively. Furthermore, for asymmetric mechanisms, there are two major ways of symmetry breaking. Referring to the three conditions of establishing the isomorphism, the dissatisfaction of condition (2) implies the asymmetry in the computational graph or asymmetry in the mechanism pipeline $M$, and that of condition (3) implies the asymmetry in prompt decorators.</p>
<h1>B CMD: Conquer and Merge Discussion Framework</h1>
<h2>B. 1 Motivation</h2>
<p>The Debate framework proposes that an increased number of agents and discussion rounds will result in improved performance in multi-agent discussions (Du et al., 2023). Also, the ReConcile framework asserts that a greater number of discussion rounds leads to a higher level of consensus among agents, and the higher the consensus among agents, the more accurate the discussion outcomes become (Chen et al., 2023a). However, as the number of agents increases in a discussion, it leads to increased overhead in each</p>
<p>round of discussion: 1. Each agent has to read more viewpoints from others, resulting in a sharp rise in input tokens. 2. The increase in input tokens puts significant pressure on language models with context token limitations. Inspired by real-life group discussions, we propose a variant framework for Debate called CMD. Fig 3 shows the overview of our method.</p>
<h1>B. 2 Problem Definition</h1>
<p>Assume that there are $n$ agents $\mathcal{A}=\left{A_{i}\right}<em i="i">{i=1}^{m}$ are discussing the given debate task $\mathcal{Q}$, and each agent is a session created from a LLM. Suppose that the maximum number of the discussion rounds is $R$, the current round is $r$, the current discussion level is $L$, and the current active agents set is $\mathcal{A}^{\prime}$. For each agent $A</em>$ is} \in \mathcal{A}^{\prime}$, an answer it generates is $A n s_{i}=\left(v_{i}, e_{i}\right)$, where $v_{i}$ denotes the viewpoint and $e_{i}$ denotes the explanation. A debate history is $H^{(r)}=\bigcup_{i=1}^{t} A n s_{i}$ where $t=\left|\mathcal{A}^{\prime}\right| . A n s_{i}^{(r)}$ is generated through the input prompt $A_{i}\left(\mathcal{Q}, A n s_{i}^{(r-1)}, O_{i}^{(r-1)}\right)$, where $O_{i}^{(r)}$ indicates the opinions generated by $A_{j \neq i}$ in $(r-1)$-th round. The formal definition of $O_{i}^{(r)</p>
<p>$$
O_{i}^{(r)}=\bigcup_{j=1, j \neq i}^{\left|H^{(r-1)}\right|}\left(v_{i}, e_{i} \cdot \mathbb{1}\left{\operatorname{Group}^{(L)}\left(A_{i}\right)=\operatorname{Group}^{(L)}\left(A_{j}\right)\right}\right)
$$</p>
<p>This represents that $A_{i}$ will receive all information from the group members while it can only receive viewpoints from agents that are not in the same group. Our goal is to obtain the final result $a$ through an unweighted vote. In the event of a tie, either $a$ can be made by a secretary $S$, or representatives from each group will proceed to the next level of discussion until the tie is resolved. Each discussion group has only one representative. Note that $\mathcal{Q}$ can be described differently for each $A_{i}$. For example, different agents may be asked to hold different views at first.</p>
<h2>B. 3 CMD Stages</h2>
<p>Generally, there are three stages in CMD. In stage 1, the group map and all states will be initialized, then all the participants will generate their initial answers. Then, in stage 2, the participants will continue the discussion during the remaining rounds in groups. When the discussion round reaches the maximum number, it moves to stage 3. In this stage, all participants will vote to get the final answer.</p>
<ul>
<li>Stage 1 : Group Discussion Initialization. In this stage, $\mathcal{A}^{\prime}$ will be initialized as all participants. All participating agents are initially assigned names in uppercase letters, and they are then grouped in sets of three. For getMaxLevel, if secretary mode is on, then it will be set to 1 . Otherwise, it will be determined based on the current number of agents. For getGroupMap, the algorithm will automatically generate the groups for all levels. Higher-level groups are generated among the current representatives. All active agents will generate their first response $A n s_{i}$.</li>
<li>
<p>Stage 2 : Multi Rounds Discussion. In this stage, the active agents will continue to discuss in the remaining rounds. Assume current round number is $r$, for each agent $A_{i} \in \mathcal{A}^{\prime}$,</p>
</li>
<li>
<p>The algorithm will start to update the opinions history $O_{i}^{(r-1)}$ from $H$. For each record $h_{j}$ whose sender $A_{j}(j \neq i)$ stored in $H$, if $A_{j}$ and $A_{i}$ are in the same group, $O_{i}^{(r-1)}$ will record both the viewpoint and explanation from $h_{j}$; otherwise, $O_{i}^{(r-1)}$ will only record the viewpoint from $h_{j}$. After traversing all the records in $H, O_{i}$ will first gather all opinions of agents from other groups, then $O_{i}^{(r-1)}$ will gather local group explanations based on group members opinions.</p>
</li>
<li>To save up tokens, last round history $H$ will be reset. Then, the new prompt $p_{i}^{(r)}$ will be generated based on $\left(\mathcal{Q}, A n s_{i}^{(r-1)}, O_{i}^{(r-1)}\right) . A_{i}$ will make a response $A n s_{i}=\left(v_{i}, e_{i}\right)$ when given $p_{i}^{(r)}$. At last, $A n s_{i}$ will be appended to current round history $H$ for the further use.</li>
<li>
<p>Repeat 1. and 2. until the maximum number of discussion rounds is reached.</p>
</li>
<li>
<p>Stage 3 : Vote for the Final Result. In this stage, all agents will vote based on their final viewpoints stored in the history $H^{(r)}$ to obtain the result $a$. If there is no tie, then $a$ will be the final result, thus the discussion is over. If there is a tie and secretary mode is on, then the final result will be obtained</p>
</li>
</ul>
<h1>Algorithm 1 CMD: A Conquer and Merge Style Multi-Agents Discussion Framework</h1>
<p>Require: Debate Task $\mathcal{Q}$, Maximum Discussion Rounds $R$, Agents $\mathcal{A}=\left{A_{i}\right}<em _max="\max">{i=1}^{n}$, Secretary $S$
Ensure: Final Result $a$
function $\operatorname{CMD}(\mathcal{Q}, R, \mathcal{A}, S)$
$\mathcal{A}^{\prime} \leftarrow \mathcal{A}$
$\triangleright$ Initialize current active agents
$N g \leftarrow n / 3$
$r \leftarrow 0, L \leftarrow 0$
$\triangleright$ Initialize group numbers, every 3 agents a group
$L</em>, S)$
$\triangleright$ Assign agents as groups based on discussion levels
$M_{g} \leftarrow$ genGroupMap $\left(\mathcal{A}, N_{g}, L_{\max }\right)$
$\triangleright$ Initialize answer, others' opinions and history
$A n s_{i} \leftarrow \emptyset, O_{i} \leftarrow \emptyset, H \leftarrow[]$
while $L \leq L_{\max }$ do
while $r \leq R$ do
if $r&gt;0$ then
Update $O_{i}$ from $H$
end if
$H \leftarrow[]$
for each $A_{i} \in \mathcal{A}^{\prime}$ do
$A n s_{i} \leftarrow A_{i}\left(\mathcal{Q}, A n s_{i}, O_{i}\right)$
$\triangleright A n s_{i}=\left(v_{i}, e_{i}\right)$
end for
$r \leftarrow r+1$
end while
$a \leftarrow$ AnswerVote $(H)$
if $a \neq$ Tie then break
else if $S$ is not None then
$\triangleright$ Check if the secretary mode is on
$a \leftarrow S(\mathcal{Q}, H)$; break
else
$\triangleright$ Secretary mode is off, representative mode is on
$L \leftarrow L+1$
$\mathcal{A}^{\prime} \leftarrow$ currentActiveAgents $\left(L, M_{g}\right)$
end if
$r \leftarrow 0$
$\triangleright$ The higher-level discussion has commenced
end while
return $a$
end function
through $S$ based on the viewpoints and explanations from all sides. If there is a tie and representative mode is on, then the discussion will move to the higher level, and deactivate agents that do not represent their groups. The representatives will be assigned to new groups, and return to Stage 2 for further discussion. If there is still a tie, new representatives will be designated for further discussion, until either the tie is resolved or only one agent is activated for the discussion.} \leftarrow \operatorname{getMaxLevel}(\mathcal{A</p>
<h2>B. 4 Message-Passing Algorithm</h2>
<p>Below is the detailed message-passing algorithm that synchronize all agents messages during the discussion process. This algorithm supports various discussion architectures.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nl">MesSync</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">Message</span><span class="o">-</span><span class="n">Passing</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Multi</span><span class="o">-</span><span class="n">Agents</span><span class="w"> </span><span class="n">Communication</span>
<span class="nl">Require</span><span class="p">:</span><span class="w"> </span><span class="n">Discussion</span><span class="w"> </span><span class="k">Rule</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">Agents</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">A</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="err">\{</span><span class="n">A_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">n</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">Agent</span><span class="w"> </span><span class="n">Attribute</span><span class="w"> </span><span class="nc">Table</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="err">\{</span><span class="n">b</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="n">t_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">n</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">Agent</span><span class="w"> </span><span class="n">Initial</span>
<span class="w">    </span><span class="n">Prompt</span><span class="w"> </span><span class="n">Messages</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="err">\{</span><span class="n">p_</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">function</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">MES</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Sync</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">A</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">messages</span><span class="w"> </span><span class="n">storage</span><span class="w"> </span><span class="n">queue</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">send</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">[]</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">messages</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">sent</span><span class="w"> </span><span class="n">queue</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="n">S_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">first</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">speaker</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">d</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">discussion</span><span class="w"> </span><span class="k">depth</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">neq</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}\</span><span class="p">).</span><span class="n">isOver</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">pushSilenceMessage</span><span class="err">}</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">d</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">peekDepth</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Peek</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">discussion</span><span class="w"> </span><span class="k">depth</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">message</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">M_</span><span class="err">{</span><span class="n">d</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">popAllMessagesAtDepth</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">A_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">A</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">m_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}\</span><span class="p">).</span><span class="n">mergeCommonMessages</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">M_</span><span class="err">{</span><span class="n">d</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">A_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">send</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">push</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">m_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">send</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">pop</span><span class="err">}</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="n">isHold</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}\</span><span class="p">).</span><span class="k">hold</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Signal</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">merge</span><span class="w"> </span><span class="n">messages</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="nf">round</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">H</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\{\}</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">map</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">messages</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">hold</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">current</span><span class="w"> </span><span class="k">depth</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">d_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">next</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">d</span><span class="o">+</span><span class="mi">2</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">isHold</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="n">d</span><span class="o">&lt;</span><span class="n">d_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">next</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}\</span><span class="p">).</span><span class="k">hold</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">H</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">m^{\prime}\right.\) .name</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}\</span><span class="p">).</span><span class="n">content</span>
<span class="w">                </span><span class="k">else</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">name</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="n">M_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">raw</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">cup</span><span class="w"> </span><span class="n">H</span><span class="o">[</span><span class="n">S</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="ow">in</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">modifyRawInput</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">M_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">raw</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">out</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">sendText2Bot</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="ow">in</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">out</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">validateOutput</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="ow">in</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">out</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">A</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}\</span><span class="p">).</span><span class="n">getReceivers</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Get</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">receivers</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">message</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">A</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">neq</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                        </span><span class="err">\</span><span class="p">(</span><span class="n">m_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">new</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">genMessage</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">t_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">out</span><span class="w"> </span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">A</span><span class="err">}</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">d_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">next</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">content</span><span class="p">,</span><span class="w"> </span><span class="n">sender</span><span class="p">,</span><span class="w"> </span><span class="n">receivers</span><span class="p">,</span><span class="w"> </span><span class="k">depth</span><span class="p">)</span>
<span class="w">                        </span><span class="err">\</span><span class="p">(</span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">msg</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">push</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">m_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="k">new</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="o">^</span><span class="err">{\</span><span class="n">prime</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">Q_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">send</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="n">cdot</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">pop</span><span class="err">}</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}\</span><span class="p">).</span><span class="n">isOver</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="k">break</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
<span class="k">end</span><span class="w"> </span><span class="k">function</span>
</code></pre></div>

<h1>C An CMD Example</h1>
<p>In the upcoming demonstration, we'll walk you through the CMD framework from the viewpoint of agent A among six agents using ChatGPT-3.5.</p>
<h1>C. 1 Meta Prompt</h1>
<h2>System</h2>
<p>You will be asked to answer a correctness verification task of a deductive reasoing proposition.
Use knowledge only comes from the given premises.
The suffix of your answer should be the your opinion of one of these answer type:</p>
<ol>
<li>[Correct], 2. [Incorrect], 3. [Unknown].</li>
</ol>
<p>Do not omit the brackets.
Table 4: Vanilla Prompt</p>
<h2>C. 2 Round 1 Answer</h2>
<h2>Assistant (ChatGPT-3.5) - Agent A</h2>
<p>First let's write down all the premises with labels:
#1. Neocrepidodera Corpulentas are flea beetles or moths.
#2. The species Neocrepidodera Corpulenta is in the Chrysomelidae family.
#3. There are no moths within the Chrysomelidae family.</p>
<p>Next, let's answer the question step by step with reference to the question and reasoning process:
#4. (by #1, #2) Neocrepidodera Corpulenta is either a flea beetle or a moth, and it is in the Chrysomelidae family.
#5. (by #3, #4) Since there are no moths within the Chrysomelidae family, Neocrepidodera Corpulenta must be a flea beetle.</p>
<p>Final Step (by #5): Neocrepidodera Corpulenta is a flea beetle, so the proposition "There are no flea beetles within the Chrysomelidae family" is [Incorrect].</p>
<h1>C. 3 Middle System and User Prompts in Round 1</h1>
<h2>System</h2>
<p>There are 2 groups of people discussing on the same topic. I will provide you the detailed opinions and reasoning steps from your group member and opinions from other group members. Use these opinions and your previous opinion as additional advice, note that they maybe wrong. Do not copy other's entire answer, modify the part you believe is wrong.
Other group members' opinions:
Three agents think the proposition is Incorrect.
Your group's opinions:
One agent thinks the proposition is Correct. Below is his answer:
First let's write down all the premises with labels:</p>
<p>Next, let's answer the question step by step with reference to the question and reasoning process:
#4. (by #1, #2) Neocrepidodera Corpulenta is either a flea beetle or a moth, and it is in the Chrysomelidae family.
#5. (by #3) There are no moths within the Chrysomelidae family.
#6. (by #4, #5) Neocrepidodera Corpulenta cannot be a moth because there are no moths in the Chrysomelidae family.
Final Step (by #6): The proposition "There are no flea beetles within the Chrysomelidae family" is [Correct] because Neocrepidodera Corpulenta, which is in the Chrysomelidae family, cannot be a moth according to premise #5.
One agent thinks the proposition is Incorrect. Below is his answer:</p>
<p>Next, let's answer the question step by step with reference to the question and reasoning process:
#4. (by #1, #2) Neocrepidodera Corpulenta is a species of Neocrepidodera Corpulentas, so it is either a flea beetle or a moth.
#5. (by #3, #4) Since there are no moths within the Chrysomelidae family, Neocrepidodera Corpulenta must be a flea beetle.
Final Step (by #5): The proposition states that there are no flea beetles within the Chrysomelidae family. However, from premise #2, we know that Neocrepidodera Corpulenta, which is a flea beetle, is in the Chrysomelidae family. Therefore, the proposition is [Incorrect].</p>
<h2>User</h2>
<p>Use the provided opinions and your previous answer as additional advice critically, please provide an updated answer. Make sure to state your opinion over proposition at the end of the response in the given format: [Correct], [Incorrect] or [Unknown], and such format should be only used to determine the correctness of the proposition.</p>
<p>Table 6: Middle System Prompt at the End of Round 1</p>
<h1>C. 4 Round 2 Answer</h1>
<h2>Assistant (ChatGPT-3.5) - Agent A</h2>
<p>Based on the opinions provided by the other group members and my previous answer, I will critically evaluate the reasoning steps and provide an updated answer.
The first agent from my group argues that the proposition is correct. Their reasoning is as follows:
#4. (by #1, #2) Neocrepidodera Corpulenta is either a flea beetle or a moth, and it is in the Chrysomelidae family.
#5. (by #3) There are no moths within the Chrysomelidae family.
#6. (by #4, #5) Neocrepidodera Corpulenta cannot be a moth because there are no moths in the Chrysomelidae family.
Final Step (by #6): The proposition "There are no flea beetles within the Chrysomelidae family" is [Correct] because Neocrepidodera Corpulenta, which is in the Chrysomelidae family, cannot be a moth according to premise #5.
The second agent from my group argues that the proposition is incorrect. Their reasoning is as follows: #4. (by #1, #2) Neocrepidodera Corpulenta is a species of Neocrepidodera Corpulentas, so it is either a flea beetle or a moth.
#5. (by #3, #4) Since there are no moths within the Chrysomelidae family, Neocrepidodera Corpulenta must be a flea beetle.
Final Step (by #5): The proposition states that there are no flea beetles within the Chrysomelidae family. However, from premise #2, we know that Neocrepidodera Corpulenta, which is a flea beetle, is in the Chrysomelidae family. Therefore, the proposition is [Incorrect].
After critically evaluating the reasoning steps, I agree with the second agent from my group that the proposition is incorrect. Neocrepidodera Corpulenta is a species of Neocrepidodera Corpulentas, which are either flea beetles or moths according to premise #1. Since Neocrepidodera Corpulenta is in the Chrysomelidae family according to premise #2, it cannot be concluded that there are no flea beetles within the Chrysomelidae family. Therefore, the proposition is [Incorrect].</p>
<p>Table 7: The Answer of Agent $\mathbf{A}$ in Round 2</p>
<h2>C. 5 Middle System Prompt at the End of Round 2</h2>
<h2>System</h2>
<p>There are 2 groups of people discussing on the same topic. I will provide you the detailed opinions and reasoning steps from your group member and opinions from other group members. Use these opinions and your previous opinion as additional advice, note that they maybe wrong. Do not copy other's entire answer, modify the part you believe is wrong.
Other group members' opinions:
Three agents think the proposition is Incorrect.
Your group's opinions:
Two agents think the proposition is Incorrect. Below are their answers:</p>
<h2>User</h2>
<p>This part is the same user part shown in table 6.</p>
<p>Table 8: Middle System Prompt at the End of Round 2</p>
<h1>C. 6 Round 3 Answer</h1>
<h2>Assistant (ChatGPT-3.5) - Agent A</h2>
<p>Based on the opinions provided by the other group members and my previous answer, here is an updated response:
First, let's write down all the premises with labels:
#1. Neocrepidodera Corpulentas are flea beetles or moths.
#2. The species Neocrepidodera Corpulenta is in the Chrysomelidae family.
#3. There are no moths within the Chrysomelidae family.
Next, let's answer the question step by step with reference to the question and reasoning process:
#4. (by #1, #2) Neocrepidodera Corpulenta is either a flea beetle or a moth, and it is in the Chrysomelidae family.
#5. (by #3) There are no moths within the Chrysomelidae family.
#6. (by #4, #5) Neocrepidodera Corpulenta cannot be a moth because there are no moths in the Chrysomelidae family.
Final Step (by #6): The proposition "There are no flea beetles within the Chrysomelidae family" contradicts the given premises, as Neocrepidodera Corpulenta, which is in the Chrysomelidae family, is a flea beetle. Therefore, the proposition is [Incorrect].
After critically evaluating the reasoning steps and considering the opinions provided by the other group members, I agree that the proposition is [Incorrect]. The given premises state that Neocrepidodera Corpulenta is in the Chrysomelidae family and that it is either a flea beetle or a moth. Since there are no moths within the Chrysomelidae family, it can be concluded that Neocrepidodera Corpulenta must be a flea beetle. Therefore, the proposition "There are no flea beetles within the Chrysomelidae family" contradicts the given premises.</p>
<p>Table 9: The Answer of Agent $\mathbf{A}$ in Round 3</p>
<h2>D CMD Secretary - A Tie Case Solution</h2>
<p>In the event of a tie between two groups, we suggest employing a secretary to resolve such cases. The complete chat transcript below in table 10 illustrates a scenario where the secretary provides the ultimate resolution.</p>
<h1>System</h1>
<p>6 agents are discussing the correctness of a proposition based on the given premises. The given premises are: $\cdots$ The proposition is: "IBM has an office in London or Zurich."
However, now there is a draw:
Three agents think the proposition is Correct. Below is one of their answers: $\cdots$
Three agents think the proposition is Unknown. Below is one of their answers: $\cdots$</p>
<h2>User</h2>
<p>Your task is to carefully determine which opinion is more plausible. Answer opinion types are: [Correct], [Incorrect], [Unknown]. You should give your response in the required format. You are forbidden to copy others' reasoning steps. You can only use the given premises and your reasoning steps to answer the question.
Here are the instructions how you organize your answer format:
First, let's write down all the premises with labels. The labels look like "#{premise_number}."
Next, let's answer the question step by step with reference to the question and reasoing process. There will be a prefix in your every reasoning step with the format "#{number} (by {list_of_premises_and_steps_used})". In your final step, you should come to your conclusion with the format "Final Step (by {list_of_premises_and_steps_used}):".
The suffix of your answer should be the answer type: [Correct], [Incorrect] or [Unknown].
Here are several hints to help you conclude the final answer:</p>
<ol>
<li>Pay attention to the logic of quantifiers and distinguish the meanings of quantifiers such as "all" and "some".</li>
<li>When two quantifiers are connected as the premise of existence, the conclusion may not be established, that is, it is unknown.</li>
</ol>
<h2>Assistant (ChatGPT-3.5) - Secretary</h2>
<p>First, let's write down all the premises with labels:
#1. Evangelos Eleftheriou is a Greek electrical engineer.
#2. Evangelos Eleftheriou worked for IBM in Zurich.
#3. If a company has employees working for them somewhere, then they have an office there.
#4. IBM is a company.
Next, let's answer the question step by step with reference to the question and reasoning process:
#5 (by #4, #3) Since IBM is a company, and according to premise #3, if a company has employees working for them somewhere, then they have an office there. Therefore, IBM has an office in Zurich.
Now, let's consider the opinions of the other group members:</p>
<ul>
<li>One agent thinks the proposition is Correct.</li>
<li>Two agents think the proposition is Unknown.</li>
</ul>
<p>Considering the opinions of the other group members, there is disagreement regarding the correctness of the proposition. However, based on the premises and the reasoning process, it can be concluded that IBM has an office in Zurich.
Final Step (by #5): Since IBM has an office in Zurich, the proposition "IBM has an office in London or Zurich" is [Correct].</p>
<p>Table 10: Secretary to solve a tie case.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://oai.azure.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://ai.google.dev/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
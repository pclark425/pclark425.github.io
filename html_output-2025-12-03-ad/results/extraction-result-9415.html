<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9415 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9415</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9415</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-38c0543aa72b68d1ded4237e8cc5333b165ea249</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38c0543aa72b68d1ded4237e8cc5333b165ea249" target="_blank">LogBERT: Log Anomaly Detection via BERT</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Joint Conference on Neural Network</p>
                <p><strong>Paper TL;DR:</strong> LogBERT is proposed, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT), which outperforms state-of-the-art approaches for anomaly detection.</p>
                <p><strong>Paper Abstract:</strong> Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). LogBERT learns the patterns of normal log sequences by two novel self-supervised training tasks, masked log message prediction and volume of hypersphere minimization. After training, LogBERT is able to capture the patterns of normal log sequences and further detect anomalies where the underlying patterns deviate from expected patterns. The experimental results on three log datasets show that LogBERT outperforms state-of-the-art approaches for anomaly detection.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9415.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9415.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT (Log Anomaly Detection via BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised anomaly detection framework that adapts a BERT-style Transformer encoder to model sequences of parsed log keys, trained with masked-log-key prediction and a hypersphere-volume minimization objective to detect anomalous log sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogBERT (custom BERT-style model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder (BERT-style / bidirectional transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete categorical event sequences (ordered sequences of log keys); sequence-level representations (DIST token embedding) used for scoring</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System/application logs from large computer systems (HDFS, BlueGene/L (BGL), Thunderbird supercomputer logs)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential/behavioral anomalies in log sequences (unexpected or out-of-top-g log keys; sequences with abnormal patterns/orders), plus anomalous individual log keys within sequences</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune / train a BERT-style Transformer encoder on only normal sequences using two self-supervised objectives: (1) Masked Log Key Prediction (MLKP) — randomly mask log keys and predict them via softmax over the log-key vocabulary; (2) Volume of Hypersphere Minimization (VHM) — compute sequence representations using the contextual embedding of a special DIST token and minimize squared distance to the mean center (Deep-SVDD style). At detection time, randomly mask tokens, use the model's top-g predicted keys per mask to form a candidate set; a key not in top-g is counted as anomalous; label the sequence anomalous when > r anomalous keys.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>PCA (counting vectors + PCA), One-Class SVM (OCSVM), Isolation Forest (iForest), LogCluster (clustering), DeepLog (RNN LSTM predictor), LogAnomaly (deep sequential/quantitative anomaly detector)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1-score (per-dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported F1 scores (LogBERT) — HDFS: Precision 87.02, Recall 78.10, F1 82.32; BGL: Precision 89.40, Recall 92.32, F1 90.83; Thunderbird: Precision 96.75, Recall 96.52, F1 96.64. (See Table 2 in paper for full Precision/Recall/F1 per baseline and dataset.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>LogBERT outperforms all listed baselines (traditional unsupervised methods and recent deep-learning baselines) on F1 across all three datasets. It shows large margins versus PCA, OCSVM, and iForest; improves upon DeepLog and LogAnomaly (DeepLog F1s: HDFS 77.34, BGL 86.12, Thunderbird 93.08; LogAnomaly F1s: HDFS 56.19, BGL 74.08, Thunderbird 92.73).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>VHM (hypersphere minimization) alone yields poor performance; benefits of VHM are dataset-dependent (helps short sequences more — e.g., HDFS average length 19 — but is less impactful for long sequences such as BGL (avg length 562) and Thunderbird (avg 326) where MLKP dominates). Sensitivity to ratio of masked tokens: too many masks degrades performance; selection of candidate-set size g trades precision vs recall; hyperparameters m (mask ratio), g (top-g size), r (anomalous-key count threshold), and α (VHM weight) must be tuned; model trained only on normal data requires effective log parsing into keys and sufficient normal examples (~5k sequences used), so transfer/generalization to different logging regimes was not evaluated. Model size/compute/real-time constraints are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Predicting masked log keys (bidirectional context) is an effective objective for modeling normal log sequences and detecting anomalies; Transformer encoders (BERT-style) can outperform RNN (LSTM/GRU) sequence predictors for log anomaly detection. Combining a generative/prediction objective (MLKP) with a representation-regularizing spherical objective (VHM) improves separation of normal vs anomalous sequences—especially for short sequences—while representing an entire sequence via a DIST token's contextual embedding provides an effective sequence-level representation for hypersphere-based scoring. The top-g candidate set strategy (from DeepLog) is reused for robust detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_examples_from_paper</strong></td>
                            <td>['Proposes masked log key prediction (MLKP) and volume of hypersphere minimization (VHM) as the two self-supervised objectives.', 'Uses Transformer encoder with two layers, input/hidden dims reported (input dim 50, hidden 256).', 'Evaluated on three real-world log datasets (HDFS, BGL, Thunderbird) with detailed Precision/Recall/F1 comparisons.']</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9415.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9415.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional Transformer-based language representation model that uses masked token prediction and next-sentence objectives to learn contextualized embeddings; used here as the architectural and training inspiration for LogBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Bidirectional Transformer (masked-language-model pretraining paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Natural language sequences (text) — in this paper, the BERT concept is adapted to log-key sequences</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Originally general text corpora; here applied conceptually to system log sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Not an anomaly detector by itself in the referenced work; in this paper BERT's masked-prediction idea is adapted to detect anomalies in sequences when trained on normal data</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Masked-language-modeling style objective (mask tokens and predict them using bidirectional contextual encoding) — LogBERT adopts the masked prediction idea for log keys.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>BERT's bidirectional context modeling is cited as an advantage over RNN next-step prediction methods; LogBERT adapts BERT-like masked prediction to logs and shows improved detection versus RNN-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The original BERT pretraining approach is for textual data and large-scale pretraining; LogBERT does not report using large-scale BERT pretraining and instead builds a two-layer Transformer trained on log-key sequences, so direct transferability of large pre-trained BERT models to log anomaly detection is not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Bidirectional masked prediction (BERT-style) provides richer context than one-directional next-token prediction and is effective for learning normal sequence patterns in system logs when combined with a sequence-level regularization objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9415.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9415.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-based anomaly detection approach that models log-key sequences with an LSTM to predict the next log key; anomalies are detected when the actual next key is not within the top-g predicted candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepLog (LSTM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete categorical event sequences (log keys); next-step prediction on sequences</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System/application logs (same datasets used for comparison: HDFS, BGL, Thunderbird)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential anomalies; unexpected next log-key events (out-of-top-g predictions) indicating anomalous behavior</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train an LSTM to predict the next log key given previous keys; at detection time, use top-g predicted keys as the candidate normal set; if the observed key is not in top-g, flag it; aggregate flagged keys per sequence to decide sequence anomaly (DeepLog's top-g strategy reused in LogBERT).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported (from Table 2): DeepLog — HDFS: Precision 88.44, Recall 69.49, F1 77.34; BGL: Precision 89.74, Recall 82.78, F1 86.12; Thunderbird: Precision 87.34, Recall 99.61, F1 93.08.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>DeepLog is a strong deep-learning baseline; LogBERT achieves higher F1 on all three datasets (improvement e.g. HDFS F1: 82.32 vs DeepLog 77.34).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RNN next-step prediction is limited to using previous context only (unidirectional), which may miss patterns that require bidirectional context; training objective focuses on next-step correlation and may not explicitly encode global/common patterns shared across normal sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Top-g candidate set method for robustness in next-step prediction is an effective detection heuristic and is reused by LogBERT; comparison shows bidirectional masked prediction (LogBERT) can outperform LSTM next-step predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9415.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9415.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning based anomaly detector for logs that addresses sequential and quantitative anomalies in unstructured logs; used in this paper as a state-of-the-art baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogAnomaly (deep learning approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Deep learning (sequential and quantitative anomaly detection; specific architecture details not reproduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log-key sequences and associated quantitative features</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System/application logs (HDFS, BGL, Thunderbird used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Sequential anomalies and quantitative (value-based) anomalies in logs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Unsupervised deep-learning approach designed to detect both sequence-level and quantitative anomalies in logs (paper used as a baseline; exact method description found in the referenced LogAnomaly paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported (from Table 2): LogAnomaly — HDFS: Precision 94.15, Recall 40.47, F1 56.19; BGL: Precision 73.12, Recall 76.09, F1 74.08; Thunderbird: Precision 86.72, Recall 99.63, F1 92.73.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>LogAnomaly performs well on some metrics/datasets but has lower F1 than LogBERT on all three datasets; LogBERT shows more balanced precision/recall and overall higher F1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>In this paper's experiments, LogAnomaly sometimes shows high precision or high recall but fails to balance both across datasets (e.g., very high precision but low recall on some datasets), indicating dataset-dependent performance and sensitivity to dataset characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogBERT: Log Anomaly Detection via BERT', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 2)</em></li>
                <li>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning <em>(Rating: 2)</em></li>
                <li>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs <em>(Rating: 2)</em></li>
                <li>Deep One-Class Classification <em>(Rating: 2)</em></li>
                <li>Attention Is All You Need <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9415",
    "paper_id": "paper-38c0543aa72b68d1ded4237e8cc5333b165ea249",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT (Log Anomaly Detection via BERT)",
            "brief_description": "A self-supervised anomaly detection framework that adapts a BERT-style Transformer encoder to model sequences of parsed log keys, trained with masked-log-key prediction and a hypersphere-volume minimization objective to detect anomalous log sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogBERT (custom BERT-style model)",
            "model_type": "Transformer encoder (BERT-style / bidirectional transformer)",
            "model_size": null,
            "data_type": "Discrete categorical event sequences (ordered sequences of log keys); sequence-level representations (DIST token embedding) used for scoring",
            "data_domain": "System/application logs from large computer systems (HDFS, BlueGene/L (BGL), Thunderbird supercomputer logs)",
            "anomaly_type": "Sequential/behavioral anomalies in log sequences (unexpected or out-of-top-g log keys; sequences with abnormal patterns/orders), plus anomalous individual log keys within sequences",
            "method_description": "Fine-tune / train a BERT-style Transformer encoder on only normal sequences using two self-supervised objectives: (1) Masked Log Key Prediction (MLKP) — randomly mask log keys and predict them via softmax over the log-key vocabulary; (2) Volume of Hypersphere Minimization (VHM) — compute sequence representations using the contextual embedding of a special DIST token and minimize squared distance to the mean center (Deep-SVDD style). At detection time, randomly mask tokens, use the model's top-g predicted keys per mask to form a candidate set; a key not in top-g is counted as anomalous; label the sequence anomalous when &gt; r anomalous keys.",
            "baseline_methods": "PCA (counting vectors + PCA), One-Class SVM (OCSVM), Isolation Forest (iForest), LogCluster (clustering), DeepLog (RNN LSTM predictor), LogAnomaly (deep sequential/quantitative anomaly detector)",
            "performance_metrics": "Precision, Recall, F1-score (per-dataset)",
            "performance_results": "Reported F1 scores (LogBERT) — HDFS: Precision 87.02, Recall 78.10, F1 82.32; BGL: Precision 89.40, Recall 92.32, F1 90.83; Thunderbird: Precision 96.75, Recall 96.52, F1 96.64. (See Table 2 in paper for full Precision/Recall/F1 per baseline and dataset.)",
            "comparison_to_baseline": "LogBERT outperforms all listed baselines (traditional unsupervised methods and recent deep-learning baselines) on F1 across all three datasets. It shows large margins versus PCA, OCSVM, and iForest; improves upon DeepLog and LogAnomaly (DeepLog F1s: HDFS 77.34, BGL 86.12, Thunderbird 93.08; LogAnomaly F1s: HDFS 56.19, BGL 74.08, Thunderbird 92.73).",
            "limitations_or_failure_cases": "VHM (hypersphere minimization) alone yields poor performance; benefits of VHM are dataset-dependent (helps short sequences more — e.g., HDFS average length 19 — but is less impactful for long sequences such as BGL (avg length 562) and Thunderbird (avg 326) where MLKP dominates). Sensitivity to ratio of masked tokens: too many masks degrades performance; selection of candidate-set size g trades precision vs recall; hyperparameters m (mask ratio), g (top-g size), r (anomalous-key count threshold), and α (VHM weight) must be tuned; model trained only on normal data requires effective log parsing into keys and sufficient normal examples (~5k sequences used), so transfer/generalization to different logging regimes was not evaluated. Model size/compute/real-time constraints are not detailed.",
            "unique_insights": "Predicting masked log keys (bidirectional context) is an effective objective for modeling normal log sequences and detecting anomalies; Transformer encoders (BERT-style) can outperform RNN (LSTM/GRU) sequence predictors for log anomaly detection. Combining a generative/prediction objective (MLKP) with a representation-regularizing spherical objective (VHM) improves separation of normal vs anomalous sequences—especially for short sequences—while representing an entire sequence via a DIST token's contextual embedding provides an effective sequence-level representation for hypersphere-based scoring. The top-g candidate set strategy (from DeepLog) is reused for robust detection.",
            "citation_examples_from_paper": [
                "Proposes masked log key prediction (MLKP) and volume of hypersphere minimization (VHM) as the two self-supervised objectives.",
                "Uses Transformer encoder with two layers, input/hidden dims reported (input dim 50, hidden 256).",
                "Evaluated on three real-world log datasets (HDFS, BGL, Thunderbird) with detailed Precision/Recall/F1 comparisons."
            ],
            "uuid": "e9415.0",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "brief_description": "A bidirectional Transformer-based language representation model that uses masked token prediction and next-sentence objectives to learn contextualized embeddings; used here as the architectural and training inspiration for LogBERT.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_type": "Bidirectional Transformer (masked-language-model pretraining paradigm)",
            "model_size": null,
            "data_type": "Natural language sequences (text) — in this paper, the BERT concept is adapted to log-key sequences",
            "data_domain": "Originally general text corpora; here applied conceptually to system log sequences",
            "anomaly_type": "Not an anomaly detector by itself in the referenced work; in this paper BERT's masked-prediction idea is adapted to detect anomalies in sequences when trained on normal data",
            "method_description": "Masked-language-modeling style objective (mask tokens and predict them using bidirectional contextual encoding) — LogBERT adopts the masked prediction idea for log keys.",
            "baseline_methods": "",
            "performance_metrics": "",
            "performance_results": "",
            "comparison_to_baseline": "BERT's bidirectional context modeling is cited as an advantage over RNN next-step prediction methods; LogBERT adapts BERT-like masked prediction to logs and shows improved detection versus RNN-based baselines.",
            "limitations_or_failure_cases": "The original BERT pretraining approach is for textual data and large-scale pretraining; LogBERT does not report using large-scale BERT pretraining and instead builds a two-layer Transformer trained on log-key sequences, so direct transferability of large pre-trained BERT models to log anomaly detection is not evaluated in the paper.",
            "unique_insights": "Bidirectional masked prediction (BERT-style) provides richer context than one-directional next-token prediction and is effective for learning normal sequence patterns in system logs when combined with a sequence-level regularization objective.",
            "uuid": "e9415.1",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DeepLog",
            "name_full": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "brief_description": "An RNN-based anomaly detection approach that models log-key sequences with an LSTM to predict the next log key; anomalies are detected when the actual next key is not within the top-g predicted candidates.",
            "citation_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "mention_or_use": "use",
            "model_name": "DeepLog (LSTM-based)",
            "model_type": "Recurrent Neural Network (LSTM)",
            "model_size": null,
            "data_type": "Discrete categorical event sequences (log keys); next-step prediction on sequences",
            "data_domain": "System/application logs (same datasets used for comparison: HDFS, BGL, Thunderbird)",
            "anomaly_type": "Sequential anomalies; unexpected next log-key events (out-of-top-g predictions) indicating anomalous behavior",
            "method_description": "Train an LSTM to predict the next log key given previous keys; at detection time, use top-g predicted keys as the candidate normal set; if the observed key is not in top-g, flag it; aggregate flagged keys per sequence to decide sequence anomaly (DeepLog's top-g strategy reused in LogBERT).",
            "baseline_methods": "",
            "performance_metrics": "Precision, Recall, F1-score",
            "performance_results": "Reported (from Table 2): DeepLog — HDFS: Precision 88.44, Recall 69.49, F1 77.34; BGL: Precision 89.74, Recall 82.78, F1 86.12; Thunderbird: Precision 87.34, Recall 99.61, F1 93.08.",
            "comparison_to_baseline": "DeepLog is a strong deep-learning baseline; LogBERT achieves higher F1 on all three datasets (improvement e.g. HDFS F1: 82.32 vs DeepLog 77.34).",
            "limitations_or_failure_cases": "RNN next-step prediction is limited to using previous context only (unidirectional), which may miss patterns that require bidirectional context; training objective focuses on next-step correlation and may not explicitly encode global/common patterns shared across normal sequences.",
            "unique_insights": "Top-g candidate set method for robustness in next-step prediction is an effective detection heuristic and is reused by LogBERT; comparison shows bidirectional masked prediction (LogBERT) can outperform LSTM next-step predictors.",
            "uuid": "e9415.2",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "LogAnomaly",
            "name_full": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "brief_description": "A deep learning based anomaly detector for logs that addresses sequential and quantitative anomalies in unstructured logs; used in this paper as a state-of-the-art baseline.",
            "citation_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "mention_or_use": "use",
            "model_name": "LogAnomaly (deep learning approach)",
            "model_type": "Deep learning (sequential and quantitative anomaly detection; specific architecture details not reproduced in this paper)",
            "model_size": null,
            "data_type": "Log-key sequences and associated quantitative features",
            "data_domain": "System/application logs (HDFS, BGL, Thunderbird used for comparison)",
            "anomaly_type": "Sequential anomalies and quantitative (value-based) anomalies in logs",
            "method_description": "Unsupervised deep-learning approach designed to detect both sequence-level and quantitative anomalies in logs (paper used as a baseline; exact method description found in the referenced LogAnomaly paper).",
            "baseline_methods": "",
            "performance_metrics": "Precision, Recall, F1-score",
            "performance_results": "Reported (from Table 2): LogAnomaly — HDFS: Precision 94.15, Recall 40.47, F1 56.19; BGL: Precision 73.12, Recall 76.09, F1 74.08; Thunderbird: Precision 86.72, Recall 99.63, F1 92.73.",
            "comparison_to_baseline": "LogAnomaly performs well on some metrics/datasets but has lower F1 than LogBERT on all three datasets; LogBERT shows more balanced precision/recall and overall higher F1.",
            "limitations_or_failure_cases": "In this paper's experiments, LogAnomaly sometimes shows high precision or high recall but fails to balance both across datasets (e.g., very high precision but low recall on some datasets), indicating dataset-dependent performance and sensitivity to dataset characteristics.",
            "uuid": "e9415.3",
            "source_info": {
                "paper_title": "LogBERT: Log Anomaly Detection via BERT",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "rating": 2
        },
        {
            "paper_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "rating": 2
        },
        {
            "paper_title": "Deep One-Class Classification",
            "rating": 2
        },
        {
            "paper_title": "Attention Is All You Need",
            "rating": 1
        }
    ],
    "cost": 0.011266,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LogBERT: Log Anomaly Detection via BERT</h1>
<p>Haixuan Guo ${ }^{1}$, Shuhan Yuan ${ }^{1}$, and Xintao $\mathrm{Wu}^{2}$<br>${ }^{1}$ Utah State University, Logan UT<br>ghaixan95@aggiemail.usu.edu, shuhan.yuan@usu.edu<br>${ }^{2}$ University of Arkansas, Fayetteville AR<br>xintaowu@uark.edu</p>
<h4>Abstract</h4>
<p>Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). LogBERT learns the patterns of normal log sequences by two novel selfsupervised training tasks and is able to detect anomalies where the underlying patterns deviate from normal log sequences. The experimental results on three log datasets show that LogBERT outperforms state-of-the-art approaches for anomaly detection.</p>
<p>Keywords: anomaly detection $\cdot$ log sequences $\cdot$ BERT</p>
<h2>1 Introduction</h2>
<p>Online computer systems are vulnerable to various malicious attacks in cyberspace. Detecting anomalous events from online computer systems in a timely manner is the fundamental step to protect the systems. System logs, which record detailed information about computational events generated by computer systems, play an important role in anomaly detection nowadays.</p>
<p>Currently, many traditional machine learning models are proposed for identifying anomalous events from log messages. These approaches extract useful features from log messages and adopt machine learning algorithms to analyze the log data. Due to the data imbalance issue, it is infeasible to train a binary classifier to detect anomalous log sequences. As a result, many unsupervised learning models, such as Principal Component Analysis (PCA) [19], or one class classification models, such as one-class SVM [516], are widely-used to detect anomalies. However, traditional machine learning models, such as one-class SVM, are hard to capture the temporal information of discrete log messages.</p>
<p>Recently, deep learning models, especially recurrent neural networks (RNNs), are widely used for log anomaly detection since they are able to model the sequential data [2|317]. However, there are still some limitations of using RNN for modeling log data. First, although RNN can capture the sequential information by the recurrence formula, it cannot make each $\log$ in a sequence encoding the</p>
<p>context information from both the left and right context. However, it is crucial to observe the complete context information instead of only the information from previous steps when detecting malicious attacks based on log messages. Second, current RNN-based anomaly detection models are trained to capture the patterns of normal sequences by prediction the next log message given previous log messages. This training objective mainly focuses on capturing the correlation among the log messages in normal sequences. When such correlation in a log sequence is violated, the RNN model cannot correctly predict the next log message based on previous ones. Then, we will label the sequence as anomalous. However, only using the prediction of next log message as objective function cannot not explicitly encode the common patterns shared by all normal sequences.</p>
<p>To tackle the existing limitations of RNN-based models, in this work, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). Inspired by the great success of BERT in modeling sequential text data [1], we leverage BERT to capture patterns of normal log sequences. By using the structure of BERT, we expect the contextual embedding of each log entry can capture the information of whole log sequences. To achieve that, we propose two selfsupervised training tasks: 1) masked log key prediction, which aims to correctly predict log keys in normal log sequences that are randomly masked; 2) volume of hypersphere minimization, which aims to make the normal log sequences close to each other in the embedding space. After training, we expect LogBERT encodes the information about normal log sequences. We then derive a criterion to detect anomalous log sequences based on LogBERT. Experimental results on three log datasets show that LogBERT achieves the best performance on log anomaly detection by comparing with various state-of-the-art baselines.</p>
<h1>2 Related Work</h1>
<p>System logs are widely used by large online computer systems for troubleshooting, where each log message is usually a semi-structured text string. The traditional approaches explicitly use the keywords (e.g., "fail") or regular expressions to detect anomalous log entries. However, these approaches cannot detect malicious attacks based on a sequence of operations, where each log entry looks normal, but the whole sequence is anomalous. To tackle this challenge, many rule-based approaches are proposed to identify anomalous events [1120]. Although rule based approaches can achieve high accuracy, they can only identify pre-defined anomalous scenarios and require heavy manual engineering.</p>
<p>As malicious attacks become more complicated, learning-based approaches are proposed. The typical pipeline for these approaches consists of three steps [4]. First, a log parser is adopted to transform log messages to log keys. A feature extraction approach, such as TF-IDF, is then used to build a feature vector to represent a sequence of log keys in a sliding window. Finally, in most cases, an unsupervised approach is applied for detecting the anomalous sequences [189].</p>
<p>Recently, many deep learning-based log anomaly detection approaches are proposed for log anomaly detection [21|2|22|23|8|17]. Most of the existing approaches adopt recurrent neural networks, especially long-short term memory (LSTM) or gated recurrent unit (GRU) to model the normal log key sequences and derive anomalous scores to detect the anomalous log sequences [2|23|17]. In this work, we explore the advanced BERT model to capture the information of log sequences and propose two novel self-supervised tasks to train the model.</p>
<h1>3 LogBERT</h1>
<p>In this section, we introduce our framework, LogBERT, for log sequence anomaly detection. Inspired by BERT [1], LogBERT leverages the Transformer encoder to model log sequences and is trained by novel self-supervised tasks to capture the patterns of normal sequences. Figure 1 shows the whole framework of LogBERT.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: The overview of LogBERT</p>
<h3>3.1 Framework</h3>
<p>Given a sequence of unstructured log messages, we aim to detect whether this sequence is normal or anomalous. In order to represent log messages, following a typical pre-processing approach, we first extract log keys (string templates) from log messages via a log parser (shown in Figure 2). Then, we can define a log sequence as a sequence of ordered log keys $S=\left{k_{1}, \ldots, k_{t}, \ldots, k_{T}\right}$, where $k_{t} \in \mathcal{K}$ indicates the log key in the $t$-th position, and $\mathcal{K}$ indicates a set of log keys extracted from log messages. The goal of this task is to predict whether a new log sequence $S$ is anomalous based on a training dataset $\mathcal{D}=\left{S^{j}\right}_{j=1}^{N}$ that consists of only normal log sequences. To achieve that, LogBERT models the normal sequences and further derive an anomaly detection criterion to identify anomalous sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Log Messages</th>
<th style="text-align: center;">Log Keys</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ CE sym $\mathrm{s}^{\mathrm{s}} \mathrm{s}$, at $\mathrm{s}^{\mathrm{s}} \mathrm{s}$, mask $\mathrm{s}^{\mathrm{s}} \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ ddr: activating redundant bit steering: rank $1 / 2$ by $\mathrm{rad} / \mathrm{s}^{\mathrm{s}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\vdash$ rank $1 / 2$ by $\mathrm{rad} / \mathrm{s}^{\mathrm{s}}$</td>
</tr>
</tbody>
</table>
<p>Fig. 2: Log messages in the BGL dataset and the corresponding log keys extracted by a log parser. The message with red underscore indicates the detailed computational event.</p>
<p>Input Representation. Given a normal log sequence $S^{j}$, we first add a special token, DIST, at the beginning of $S^{j}=\left{k_{1}^{j}, \ldots, k_{t}^{j}, \ldots, k_{T}^{j}\right}$ as the first log key, which is used to represent the whole log sequence based on the structure of Transformer encoder. LogBERT then represents each log key $k_{t}^{j}$ as an input representation $\mathbf{x}<em t="t">{t}^{j}$, where the representation $\mathbf{x}</em>}^{j}$ is a summation of a log key embedding and a position embedding. In this work, we randomly generate a matrix $\mathbf{E} \in \mathbb{R}^{|\mathcal{K}| * d}$ as the log key embedding matrix, where $d$ is the dimension of log key embedding, while the position embeddings $\mathbf{T} \in \mathbb{R}^{T * d}$ are generated by using a sinusoid function to encode the position information of log keys in a sequence [1]. Finally, the input representation of the log key $k_{t}$ is defined as: $\mathbf{x<em k__t="k_{t">{t}^{j}=\mathbf{e}</em>}^{j}}+\mathbf{t<em t="t">{k</em>$.
Transformer Encoder. LogBERT adopts Transformer encoder to learn the contextual relations among log keys in a sequence. Transformer encoder consists of multiple transformer layers. Each transformer layer includes a multi-head self-attention and a position-wise feed forward sub-layer in which a residual connection is employed around each of two sub-layers, followed by layer normalization [15]. The multi-head attention employs $H$ parallel self-attentions to jointly capture different aspect information at different positions over the input log sequence. Formally, for the $l$-th head of the attention layer, the scaled dot-product self-attention is defined as:}^{j}</p>
<p>$$
\text { head }<em l="l">{l}=\operatorname{Attention}\left(\mathbf{X}^{j} \mathbf{W}</em>}^{Q}, \mathbf{X}^{j} \mathbf{W<em l="l">{l}^{K}, \mathbf{X}^{j} \mathbf{W}</em>\right)
$$}^{V</p>
<p>where $\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}}{\sqrt{d_{v}}}\right) \mathbf{V} ; \mathbf{X}^{j} \in \mathbb{R}^{T * d}$ is the input representation of the log sequence; $\mathbf{W}<em l="l">{l}^{Q}, \mathbf{W}</em>}^{K}$ and $\mathbf{W<em v="v">{l}^{V}$ are linear projection weights with dimensions $\mathbb{R}^{d * d</em>$ is the dimension for one head ot the attention layer. Each self-attention makes each key attend to all the log keys in an input sequence and computes the hidden representation for each log key with an attention distribution over the sequence.}}$ for the $l$-th head, and $d_{v</p>
<p>The multi-head attention employs a parallel of self-attentions to jointly capture different aspect information at different log keys. Formally, the multi-head attention concatenates $H$ parallel heads together as:</p>
<p>$$
f\left(\mathbf{X}^{j}\right)=\operatorname{Concat}\left(\text { head }<em H="H">{1}, \ldots, \text { head }</em>
$$}\right) \mathbf{W}^{O</p>
<p>where $\mathbf{W}^{O} \in \mathbb{R}^{h d_{v} * d_{o}}$ is a projection matrix, and $d_{o}$ is the dimension for the output of multi-head attention sub-layer.</p>
<p>Then, the position-wise feed forward sub-layer with a ReLU activation is applied to the hidden representation of each activity separately. Finally, by combining the position-wise feed forward sub-layer and multi-head attention, a transformer layer is defined as:</p>
<p>$$
\text { transformer_layer }\left(\mathbf{X}^{j}\right)=F F N\left(f\left(\mathbf{X}^{j}\right)\right)=\operatorname{ReLU}\left(f\left(\mathbf{X}^{j}\right) \mathbf{W}<em 2="2">{1}\right) \mathbf{W}</em>
$$</p>
<p>where $\mathbf{W}<em 2="2">{1}$ and $\mathbf{W}</em>$ are trained projection matrices.
The Transformer encoder usually consists of multiple transformer layers. We denote $\mathbf{h}<em t="t">{t}^{j}$ as the contextual embedding vector of the log key $k</em>}^{j}$ produced by the Transformer encoder, i.e., $\mathbf{h<em t="t">{t}^{j}=\operatorname{Transformer}\left(x</em>\right)$.}^{j</p>
<h1>3.2 Objective Function</h1>
<p>In order to train the LogBERT model, we propose two self-supervised training tasks to capture the patterns of normal log sequences.
Task I: Masked Log Key Prediction (MLKP). In order to capture the bidirectional context information of log sequences, we train LogBERT to predict the masked log keys in log sequences. In our scenario, LogBERT takes log sequences with random masks as inputs, where we randomly replace a ratio of log keys in a sequence with a specific MASK token. The training objective is to accurately predict the randomly masked log keys. The purpose is to make LogBERT encode the prior knowledge of normal log sequences.</p>
<p>To achieve that, we feed the contextual embedding vector of the $i$-th MASK token in the $j$-th log sequence $\mathbf{h}<em i="i">{\left[\operatorname{MASK}</em>$ :}\right]}^{j}$ to a softmax function, which will output a probability distribution over the entire set of log keys $\mathcal{K</p>
<p>$$
\hat{\mathbf{y}}<em i="i">{\left[\operatorname{MASK}</em>}\right]}^{j}=\operatorname{Softmax}\left(\mathbf{W<em _left_operatorname_MASK="\left[\operatorname{MASK">{C} \mathbf{h}</em><em C="C">{i}\right]}^{j}+\mathbf{b}</em>\right)
$$</p>
<p>where $\mathbf{W}<em C="C">{C}$ and $\mathbf{b}</em>$ are trainable parameters. Then, we adopt the cross entropy loss as the objective function for masked log key prediction, which is defined as:</p>
<p>$$
\mathcal{L}<em j="1">{M L K P}=-\frac{1}{N} \sum</em>}^{N} \sum_{i=1}^{M} \mathbf{y<em i="i">{\left[\operatorname{MASK}</em>}\right]}^{j} \log \hat{\mathbf{y}<em i="i">{\left[\operatorname{MASK}</em>
$$}\right]}^{j</p>
<p>where $\mathbf{y}<em _left_right.="\left[\right.">{\left[\right.}^{j}{ }</em>$ indicates the real log key for the $i$-th masked token, and $M$ is the total number of masked tokens in the $j$-th log sequence. Since the patterns of normal and anomalous log sequences are different, we expect once LogBERT is able to correctly predict the masked log keys, it can distinguish the normal and anomalous log sequences.
Task II: Volume of Hypersphere Minimization (VHM). Inspired by the Deep SVDD approach [13], where the objective is to minimize the volume of a data-enclosing hypersphere, we propose a spherical objective function to regulate the distribution of normal log sequences. The motivation is that normal log sequences should be concentrated and close to each other in the embedding space, while the anomalous log sequences are far to the center of the sphere. We</p>
<p>first derive the representations of normal log sequences and then compute the center representation based on the mean operation. In particular, we consider the contextual embedding vector of the DIST token $\mathbf{h}<em _mathrm_DIST="\mathrm{DIST">{\text {DIST }}^{j}$, which encodes the information of entire log sequence based on the Transformer encoder, as the representation of a log sequence in the embedding space. To make the representations of normal log sequences close to each other, we further derive the center representation of normal log sequences $\mathbf{c}$ in the training set by a mean operation, i.e., $\mathbf{c}=\operatorname{Mean}\left(\mathbf{h}</em>$ close to the center representation c:}}^{j}\right)$. Then, the objective function is to make the representation of normal log sequence $\mathbf{h}_{\text {DIST }}^{j</p>
<p>$$
\mathcal{L}<em j="1">{V H M}=\frac{1}{N} \sum</em>
$$}^{N}\left|\mathbf{h}_{\mathrm{DIST}}^{j}-\mathbf{c}\right|^{2</p>
<p>By minimizing the Equation 6, we expect all the normal log sequences in the training set are close to the center, while the anomalous log sequences have a larger distance to the center. Meanwhile, another advantage of the spherical objective function is that by making the sequence representations close to the center, the Transformer encoder can also leverage the information from other log sequences via the center representation $\mathbf{c}$, since $\mathbf{c}$ encodes all the information of normal log sequences. As a result, the model should be able to predict the masked log keys with higher accuracy for normal log sequences because the normal log sequences should share similar patterns.</p>
<p>Finally, the objective function for training the LogBERT is defined as below:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em H="H" M="M" V="V">{M L K P}+\alpha \mathcal{L}</em>
$$</p>
<p>where $\alpha$ is a hyper-parameter to balance two training tasks.</p>
<h1>3.3 Anomaly Detection</h1>
<p>After training, we can deploy LogBERT for anomalous log sequence detection. The idea of applying LogBERT for log anomaly detection is that since LogBERT is trained on normal log sequences, it can achieve high prediction accuracy on predicting the masked log keys if a testing log sequence is normal. Hence, we can derive the anomalous score of a log sequence based on the prediction results on the MASK tokens. To this end, given a testing log sequence, similar to the training process, we first randomly replace a ratio log keys with MASK tokens and use the randomly-masked log sequence as an input to LogBERT. Then, given a MASK token, the probability distribution calculated based on Equation 4 indicates the likelihood of a log key appeared in the position of the MASK token. Similar to the strategy in DeepLog [2], we build a candidate set consisting of $g$ normal log keys with the top $g$ highest likelihoods computed by $\hat{\mathbf{y}}<em i="i">{\left[\right.$ MASK $\left.</em>$. If the real log key is in the candidate set, we treat the key as normal. In other words, if the observed log key is not in the top- $g$ candidate set predicted by LogBERT, we consider the log key as an anomalous log key. Then, when a log sequence consists of more than $r$ anomalous log keys, we will label this log sequence as}\right]</p>
<p>anomalous. Both $g$ and $r$ are hyper-parameters and will be tuned based on a validation set.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experimental Setup</h3>
<p>Datasets. We evaluate the proposed LogBERT on three log datasets, HDFS, BGL, and Thunderbird. Table 1 shows the statistics of the datasets. For all datasets, we adopt around 5000 normal log sequences for training. The number in the brackets under the column "# Log Keys" indicates the number of unique log keys in the training dataset.</p>
<p>Table 1: Statistics of evaluation datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Log Messages</th>
<th style="text-align: center;"># Anomalies</th>
<th style="text-align: center;"># Log Keys</th>
<th style="text-align: center;"># of Log Sequences in Test Dataset</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">Anomalous</td>
</tr>
<tr>
<td style="text-align: center;">HDFS</td>
<td style="text-align: center;">$11,172,157$</td>
<td style="text-align: center;">284,818</td>
<td style="text-align: center;">46 (15)</td>
<td style="text-align: center;">553,366</td>
<td style="text-align: center;">10,647</td>
</tr>
<tr>
<td style="text-align: center;">BGL</td>
<td style="text-align: center;">$4,747,963$</td>
<td style="text-align: center;">348,460</td>
<td style="text-align: center;">334 (175)</td>
<td style="text-align: center;">10,045</td>
<td style="text-align: center;">2,630</td>
</tr>
<tr>
<td style="text-align: center;">Thunderbird-mimi</td>
<td style="text-align: center;">20,000,000</td>
<td style="text-align: center;">758,562</td>
<td style="text-align: center;">1,165 (866)</td>
<td style="text-align: center;">71,155</td>
<td style="text-align: center;">45,385</td>
</tr>
</tbody>
</table>
<ul>
<li>Hadoop Distributed File System (HDFS) [18]. HDFS dataset is generated by running Hadoop-based map-reduce jobs on Amazon EC2 nodes and manually labeled through handcrafted rules to identify anomalies. HDFS dataset consists of $11,172,157 \log$ messages, of which 284,818 are anomalous. For HDFS, we group log keys into log sequences based on the session ID in each log message. The average length of log sequences is 19 .</li>
<li>BlueGene/L Supercomputer System (BGL) [10]. BGL dataset is collected from a BlueGene/L supercomputer system at Lawrence Livermore National Labs (LLNL). Logs contain alert and non-alert messages identified by alert category tags. The alert messages are considered as anomalous. BGL dataset consists of $4,747,963 \log$ messages, of which 348,460 are anomalous. For BGL, we define a time sliding window as 5 minutes to generate log sequences, where the average length is 562 .</li>
<li>Thunderbird [10]. Thunderbird dataset is another large log dataset collected from a supercomputer system. We select the first $20,000,000 \log$ messages from the original Thunderbird dataset to compose our dataset, of which 758,562 are anomalous. For Thunderbird, we also adopt a time sliding window as 1 minute to generate log sequences, where the average length is 326 .</li>
</ul>
<p>Baselines. We compare our LogBERT model with the following baselines.</p>
<ul>
<li>
<p>Principal Component Analysis (PCA) [19]. PCA builds counting matrix based on the frequency of log keys sequences and then reduces the original counting matrix into a low dimensional space to detect anomalous sequences.</p>
</li>
<li>
<p>One-Class SVM (OCSVM) [14]. One-Class SVM is a well-known one-class classification model and widely used for log anomaly detection [5,16] by only observing the normal data.</p>
</li>
<li>IsolationForest (iForest) [7]. Isolation forest is an unsupervised learning algorithm for anomaly detection by representing features as tree structures.</li>
<li>LogCluster [6]. LogCluster is a clustering based approach, where the anomalous log sequences are detected by comparing with the existing clusters.</li>
<li>DeepLog [2]. DeepLog is a state-of-the-art log anomaly detection approach. DeepLog adopts recurrent neural network to capture patterns of normal log sequences and further identifies the anomalous log sequences based on the performance of log key predictions.</li>
<li>LogAnomaly [23]. Log Anomaly is a deep learning-based anomaly detection approach and able to detect sequential and quantitative log anomalies.</li>
</ul>
<p>Implementation Details. We adopt Drain [3] to parse the log messages into log keys. Regarding baselines, we leverage the package Loglizer [4] to evaluate PCA, OCSVM, iForest as well as LogCluster for anomaly detection and adopt the open source deep learning-based log analysis toolkit to evaluate DeepLog and LogAnomaly ${ }^{3}$. For LogBERT, we construct a Transformer encoder by using two Transformer layers. The dimensions for the input representation and hidden vectors are 50 and 256, respectively. The hyper-parameters, including $\alpha$ in Equation 7, $m$ the ratio of masked log keys for the MKLP task, $r$ the number of predicted anomalous log keys, and $g$ the size of top- $g$ candidate set for anomaly detection are tuned based on a small validation set. In our experiments, both training and detection phases have the same ratio of masked log keys $m$. The code of LogBERT is available online ${ }^{4}$.</p>
<h1>4.2 Experimental Results</h1>
<p>Table 2: Experimental Results on HDFS, BGL, and Thunderbird Datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
</tr>
<tr>
<td style="text-align: center;">PCA</td>
<td style="text-align: center;">5.89</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">98.23</td>
<td style="text-align: center;">16.61</td>
<td style="text-align: center;">37.35</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">54.39</td>
</tr>
<tr>
<td style="text-align: center;">iForest</td>
<td style="text-align: center;">53.60</td>
<td style="text-align: center;">69.41</td>
<td style="text-align: center;">60.49</td>
<td style="text-align: center;">99.70</td>
<td style="text-align: center;">18.11</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;">34.45</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;">OCSVM</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">12.24</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">18.89</td>
<td style="text-align: center;">39.11</td>
<td style="text-align: center;">25.48</td>
</tr>
<tr>
<td style="text-align: center;">LogCluster</td>
<td style="text-align: center;">99.26</td>
<td style="text-align: center;">37.08</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">95.46</td>
<td style="text-align: center;">64.01</td>
<td style="text-align: center;">76.63</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">42.78</td>
<td style="text-align: center;">59.61</td>
</tr>
<tr>
<td style="text-align: center;">DeepLog</td>
<td style="text-align: center;">88.44</td>
<td style="text-align: center;">69.49</td>
<td style="text-align: center;">77.34</td>
<td style="text-align: center;">89.74</td>
<td style="text-align: center;">82.78</td>
<td style="text-align: center;">86.12</td>
<td style="text-align: center;">87.34</td>
<td style="text-align: center;">99.61</td>
<td style="text-align: center;">93.08</td>
</tr>
<tr>
<td style="text-align: center;">LogAnomaly</td>
<td style="text-align: center;">94.15</td>
<td style="text-align: center;">40.47</td>
<td style="text-align: center;">56.19</td>
<td style="text-align: center;">73.12</td>
<td style="text-align: center;">76.09</td>
<td style="text-align: center;">74.08</td>
<td style="text-align: center;">86.72</td>
<td style="text-align: center;">99.63</td>
<td style="text-align: center;">92.73</td>
</tr>
<tr>
<td style="text-align: center;">LogBERT</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.32</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">96.64</td>
</tr>
</tbody>
</table>
<p>Performance on Log Anomaly Detection. Table 2 shows the results of LogBERT as well as baselines on three datasets. We can notice that PCA, Isolation Forest, and OCSVM have poor performance on log anomaly detection.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Although these methods could achieve extremely high precision or recall values, they cannot balance the performance on both precision and recall, which lead to extremely low F1 scores. This could be because using the counting vector to represent a log sequence leads to the loss of temporal information from sequences. LogCluster, which is designed for log anomaly detection, achieves better performance than the PCA, Isolation Forest, and OCSVM. Meanwhile, two deep learning-based baselines, DeepLog and LogAnomaly, significantly outperform the traditional approaches and achieve reasonable F1 scores on three datasets, which show the advantage to adopt deep learning models to capture the patterns of log sequences. Moreover, our proposed LogBERT achieves the highest F1 scores on three datasets with large margins by comparing with all baselines. It indicates that by using self-supervised training tasks, LogBERT can successfully model the normal log sequences and further identify anomalous sequences with high accuracy.</p>
<p>Table 3: Performance of LogBERT base on One Self-supervised Training Task</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">HDFS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BGL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Thunderbird</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F-1 score</td>
</tr>
<tr>
<td style="text-align: left;">MLKP</td>
<td style="text-align: center;">77.54</td>
<td style="text-align: center;">78.65</td>
<td style="text-align: center;">78.09</td>
<td style="text-align: center;">93.16</td>
<td style="text-align: center;">86.46</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">97.07</td>
<td style="text-align: center;">95.90</td>
<td style="text-align: center;">96.48</td>
</tr>
<tr>
<td style="text-align: left;">VHM</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">39.17</td>
<td style="text-align: center;">4.58</td>
<td style="text-align: center;">71.04</td>
<td style="text-align: center;">43.84</td>
<td style="text-align: center;">54.22</td>
<td style="text-align: center;">56.58</td>
<td style="text-align: center;">43.87</td>
<td style="text-align: center;">49.42</td>
</tr>
<tr>
<td style="text-align: left;">Both</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.32</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">96.75</td>
<td style="text-align: center;">96.52</td>
<td style="text-align: center;">96.64</td>
</tr>
</tbody>
</table>
<p>Ablation Studies. In order to further understand our proposed LogBERT, we conduct ablation experiments on three log datasets. LogBERT is trained by two self-supervised tasks. We evaluate the performance of LogBERT by only using one training task each time. When the model is only trained by minimizing the volume of hypersphere, we identify anomalous log sequences by computing distances of the log sequence representations to the center of normal log sequences $\mathbf{c}$. If the distance is larger than a threshold, we consider a log sequence is anomalous. Table 3 shows the experimental results. We can notice that when only using the task of masked log key prediction to train the model, we can still get very good performance on log anomaly detection, which shows the effectiveness of training the model by predicting masked log keys. We can also notice that even we do not train the LogBERT with the task of the volume of hypersphere minimization, LogBERT achieves higher F1 scores than DeepLog on all three datasets, which shows that compared with LSTM, Transformer encoder is better at capturing the patterns of log sequences. Meanwhile, we can observe that when only training the model for minimizing the volume of hypersphere, the performance is poor. It indicates that only using distance as a measure to identify anomalous log sequences cannot achieve good performance. However, combining two self-supervised tasks to train LogBERT can achieve better performance than the models only trained by one task. Especially, for the HDFS dataset, LogBERT trained by two self-supervised tasks gains a large margin in terms of F1 score (82.32) compared with the model only trained by MLKP (78.09). For BGL and</p>
<p>Thunderbird, the improvement of LogBERT is not as significant as the model in HDFS. This could be because the average length of log sequences in BGL (562) and Thunderbird (326) datasets are much larger than the log sequences in HDFS (19). For longer sequences, only predicting the masked log keys can capture the most important patterns of log sequences since there are many more mask tokens in longer sequences. On the other hand, for short log sequences, we cannot have many masks tokens. As a result, the task of the volume of hypersphere minimization can help to boost the performance. Hence, based on Table 3, we can conclude that using two self-supervised tasks to train LogBERT can achieve better performance, especially when the log sequences are relatively short.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Visualization of log sequences by using the contextual embedding of DIST tokens $\mathbf{h}_{\text {DIST }}$. The blue dots indicate the normal log sequences, while the orange ' $x$ ' symbols indicate anomalous log sequences.</p>
<p>Visualization. In order to visualize the log sequences, we adopt locally linear embedding (LLE) algorithm [12] to map the log sequence representations into a two dimensional space, where the contextual embedding of DIST token $\mathbf{h}_{\text {DIST }}$ is used as the representation of a log sequence. We randomly select 1000 normal and 1000 anomalous sequences from the HDFS dataset for visualization. Figure 3 shows the visualization results of log sequences trained by LogBERT with and without the VHM task. We can notice that the normal and anomalous log sequences are mixed together when we trained the model without the VHM task (shown in Figure 3a). On the contrary, as shown in Figure 3b, by incorporating the VHM task, the normal and anomalous log sequences are clearly separated in the latent space, and the normal log sequences group together. Therefore, the visualization presents that the VHM task is effective in regulating the model to split the normal and abnormal data in latent space.
Parameter analysis. We analyze the sensitivity of model performance by tuning various hyper-parameters. Figure 4a shows that the model performance is relatively stable by setting different $\alpha$ values in Equation 7. This is because, for the BGL dataset, the loss from the masked log key prediction dominates the final loss value due to the long log sequences. As a result, the weight for the VHM</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Parameter analysis on the BGL dataset.
task does not have much influence on the performance. Figure 4b shows the performance with different ratios of masked log keys. Note that we use the same ratio in both training and detection phases. We can notice that increasing the ratios of masked log keys in the sequences from 0.1 to 0.5 can slightly increase the F1 scores while keeping increasing the ratios makes the performance worse. This is because while the masked log keys increase in a reasonable range, the model can capture more information about the sequence. However, if a sequence contains too many masked log keys, it loses too much information for making the predictions. Figure 4c shows that when increasing the size of the candidate set as normal log keys, the precision for anomaly detection keeps increasing while the recall is reducing, which meets our expectation. Hence, we need to find the appropriate size of the candidate set to balance the precision and recall for the anomaly detection.</p>
<h1>5 Conclusion</h1>
<p>Log anomaly detection is essential to protect online computer systems from malicious attacks or malfunctions. In this paper, we have developed LogBERT, a novel log anomaly detection model based on BERT. In order to train LogBERT only based on normal log sequences, we have proposed two self-supervised training tasks. One is to predict the masked log keys in log sequences, while the other is to make the normal log sequences close to each other in the embedding space. After training over normal log sequences, LogBERT is able to detect anomalous log sequences. Experimental results on three log datasets have shown that LogBERT outperforms the state-of-the-art approaches for log anomaly detection.</p>
<h2>References</h2>
<ol>
<li>Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] (2018)</li>
<li>
<p>Du, M., Li, F., Zheng, G., Srikumar, V.: DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. (2017).</p>
</li>
<li>
<p>He, P., Zhu, J., Zheng, Z., Lyu, M.R.: Drain: An Online Log Parsing Approach with Fixed Depth Tree. In: 2017 IEEE International Conference on Web Services (ICWS). (2017).</p>
</li>
<li>He, S., Zhu, J., He, P., Lyu, M.R.: Experience Report: System Log Analysis for Anomaly Detection. In: 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE). (2016).</li>
<li>Li, K.L., Huang, H.K., Tian, S.F., Xu, W.: Improving one-class SVM for anomaly detection. In: Proceedings of the 2003 International Conference on Machine Learning and Cybernetics. (2003).</li>
<li>Lin, Q., Zhang, H., Lou, J., Zhang, Y., Chen, X.: Log Clustering Based Problem Identification for Online Service Systems. In: 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (2016)</li>
<li>Liu, F.T., Ting, K.M., Zhou, Z.: Isolation Forest. In: 2008 Eighth IEEE International Conference on Data Mining. pp. 413-422 (2008).</li>
<li>Liu, F., Wen, Y., Zhang, D., Jiang, X., Xing, X., Meng, D.: Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise. In: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. (2019).</li>
<li>Lou, J.G., Fu, Q., Yang, S., Xu, Y., Li, J.: Mining invariants from console logs for system problem detection. In: Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference. (2010)</li>
<li>Oliner, A., Stearley, J.: What supercomputers say: A study of five system logs. In: 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07). pp. 575-584. IEEE (2007)</li>
<li>Pecchia, A., Cinque, M., Cotroneo, D.: Event logs for the analysis of software failures: A rule-based approach. IEEE Transactions on Software Engineering 39(06), $806-821(2013)$.</li>
<li>Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear embedding. science 290(5500), 2323-2326 (2000)</li>
<li>Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S.A., Binder, A., Müller, E., Kloft, M.: Deep One-Class Classification. In: International Conference on Machine Learning. pp. 4393-4402. PMLR (2018)</li>
<li>Schölkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J., Williamson, R.C.: Estimating the Support of a High-Dimensional Distribution. Neural Computation 13(7), 1443-1471 (2001).</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention Is All You Need. arXiv:1706.03762 [cs] (2017)</li>
<li>Wang, Y., Wong, J., Miner, A.: Anomaly intrusion detection using one class SVM. In: Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop, 2004. pp. 358-364 (2004).</li>
<li>Wang, Z., Chen, Z., Ni, J., Liu, H., Chen, H., Tang, J.: Multi-Scale One-Class Recurrent Neural Networks for Discrete Event Sequence Anomaly Detection. In: WSDM (2021)</li>
<li>Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.: Online system problem detection by mining patterns of console logs. In: 2009 Ninth IEEE International Conference on Data Mining. pp. 588-597. IEEE (2009)</li>
<li>
<p>Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.I.: Detecting large-scale system problems by mining console logs. In: Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles. (2009).</p>
</li>
<li>
<p>Yen, T.F., Oprea, A., Onarlioglu, K., Leetham, T., Robertson, W., Juels, A., Kirda, E.: Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. In: Proceedings of the 29th Annual Computer Security Applications Conference. pp. 199-208 (2013)</p>
</li>
<li>Zhang, K., Xu, J., Min, M.R., Jiang, G., Pelechrinis, K., Zhang, H.: Automated IT system failure prediction: A deep learning approach. In: 2016 IEEE International Conference on Big Data (Big Data). pp. 1291-1300 (2016).</li>
<li>Zhang, X., Xu, Y., Lin, Q., Qiao, B., Zhang, H., Dang, Y., Xie, C., Yang, X., Cheng, Q., Li, Z., Chen, J., He, X., Yao, R., Lou, J.G., Chintalapati, M., Shen, F., Zhang, D.: Robust log-based anomaly detection on unstable log data. In: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. (2019).</li>
<li>Zhou, R., Sun, P., Tao, S., Zhang, R., Meng, W., Liu, Y., Zhu, Y., Liu, Y., Pei, D., Zhang, S., Chen, Y.: LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs. In: IJCAI. pp. 4739-4745 (2019)</li>
<li>He, S., Zhu, J., He, P., Lyu, M.R.: Loghub: A Large Collection of System Log Datasets towards Automated Log Analytics. arXiv:2008.06448 [cs] (Aug 2020)</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/donglee-afar/logdeep
${ }^{4}$ https://github.com/HelenGuohx/logbert&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
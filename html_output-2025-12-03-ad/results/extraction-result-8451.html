<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-ba3ef3c9e2a7178b6414f9e71cfdc537c75ec62d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ba3ef3c9e2a7178b6414f9e71cfdc537c75ec62d" target="_blank">Explicit Memory Learning with Expectation Maximization</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EM^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit Memory Learning with Expectation Maximization (EM^2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit-text memory framework that treats stored strategies as latent variables and updates the explicit memory via an EM algorithm; memory stores model inputs plus chain-of-thought outputs, is vectorized for retrieval, and is updated to maximize expected performance on a representative validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EM^2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-guided inference method for LLMs that (1) selects relevant stored demonstrations (text pairs of problem + model reasoning), (2) uses them as in-context guidance for prediction, and (3) updates the explicit memory using an EM-style E-step (estimate contribution of memory strategies τ) and M-step (select updated memory maximizing validation-set likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B (main backbone; experiments extended to LLaMA-3-70B, Mistral-7B, Mixtral, Qwen-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Primary experiments use LLaMA-3-8B as the backbone; authors report EM^2 is model-agnostic and scales with model capability (tested on 70B and several 7B models).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math Word Problems (GSM8K, MultiArith, SingleEq, AddSub, SVAMP, AQuA, MATH), Commonsense QA (StrategyQA, CommonsenseQA, BoolQ, ARC-c), Symbolic Understanding (BigBench subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring multi-step reasoning: (1) arithmetic/math word problems needing chain-of-thought; (2) commonsense QA often requiring implicit multi-step reasoning; (3) symbolic tasks requiring structured reasoning/counting/date understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / question answering / mathematical problem solving / symbolic understanding</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit external textual memory (declarative/episodic explicit memory stored as text and embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Memory stores pairs (X, Ŷ) including the model's chain-of-thought; new predictions pass an entropy filter (normalized entropy ≤ ε) before being considered; stored texts are vectorized (text-embedding-3-large) and retrieval uses cosine similarity to select up to k demonstrations (k ≤ 8); updates are performed via an EM formulation: E-step computes expectations over latent strategies τ; M-step selects a memory subset m ⊂ M_t ∪ Γ(X_t, Ŷ_t) that maximizes summed predicted likelihood on a representative validation set (cluster-center based).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual pairs (input X and model-generated chain-of-thought + answer Ŷ), stored as raw text and embedding vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search via cosine similarity on embeddings; selection limited to top-k (≤8) examples; representative validation set constructed by clustering (KMeans, default k=8) and selecting cluster centers for M-step evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Selected examples: GSM8K single-inference: 82.63% (EM^2) and 83.09% (EM^2* initialized with CoT); GSM8K multiple-inference: 86.35% (EM^2) and 86.43% (EM^2*). Average across math word-problem datasets (excluding MATH) single-inference: 81.43% (EM^2) vs 77.98% (ZS-CoT baseline). EM^2 consistently outperforms fixed-memory baselines across commonsense and symbolic tasks (see figures and Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-memory baseline (ZS-CoT) GSM8K: 76.80% (single inference); CoT fixed-prompt baseline GSM8K: 79.61%; ComplexCoT GSM8K: 78.01%. EM^2 yields +5.83% over ZS-CoT and +3.02% over CoT on GSM8K (single-inference).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations and comparisons reported: (1) Replacing EM^2's learning function with Random selection or FIFO substantially reduces performance on math datasets (Table 3 averages: EM^2 81.43% vs Random 76.25% vs FIFO 76.00%), demonstrating the value of EM-based updates. (2) Clustering algorithm robustness: DBSCAN yields similar performance to KMeans. (3) Embedding model robustness: Sentence-BERT slightly lower; text-embedding-ada-002 achieves similar or slightly better average performance. (4) Memory size, validation-set size, and entropy-threshold ε were swept: memory capacity set to 20, top-k demonstrations up to 8; ε tuned (ε=9 chosen as a good tradeoff). (5) Comparisons to retrieval-memory baselines (MoT, AutoCoT) on MATH and other tasks show EM^2 achieves superior performance and better alignment between memory distribution and test distribution; averaged +2.82% over AutoCoT on commonsense & symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit textual memory can be made to 'learn' (i.e., guarantee improved updates) by treating stored strategies as latent variables and updating via EM: EM^2 consistently improves accuracy across math, commonsense, and symbolic tasks compared to no-memory and fixed-memory baselines; EM^2 is robust to clustering and embedding choices, scales with model capability, and its updated memory can be shared across models (benefiting smaller models when receiving higher-quality memory).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to internal model information such as token probabilities/perplexity to compute normalized entropy and some update evaluations (challenging with closed commercial APIs that only return text); M-step imposes additional computation (but authors argue update cost is manageable since decoding-heavy steps are avoided); current evaluation limited to three task families (future work needed for code, translation, agent-based tasks).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-of-Thought (MoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage retrieval-based memory method that first performs coarse-grained semantic retrieval then fine-grained model filtering to select relevant prior examples; used as a retrieval-memory baseline in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MoT: Memory-of-thought enables ChatGPT to self-improve</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-of-Thought (MoT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval-memory approach which collects model outputs via pre-inference on training data and retrieves examples using semantic similarity followed by model-based filtering to pick high-quality demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Compared under LLaMA-3-8B experimental setup (authors replicated MoT results on LLaMA-3).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Originally designed/used with ChatGPT in MoT paper; here compared after replication on LLaMA-3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH dataset (and other reasoning tasks in comparative experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to supply retrieved demonstrations to improve multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented multi-step reasoning / math problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (pre-collected examples)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Pre-inference on training data to collect candidate examples; retrieval by semantic similarity then model-based filtering to choose final demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored training examples and their model-generated reasoning/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval (coarse) followed by model-filtering (fine-grained).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>EM^2 was compared to MoT on the MATH dataset in aligned settings (authors add training samples to EM^2 and perform updates) and EM^2 achieved superior performance despite MoT's broader search scope; authors attribute EM^2 advantage to its update strategy that better preserves high-quality memories and aligns memory distribution with test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EM^2 outperforms MoT in the paper's MATH experiments; pre-inference retrieval methods like MoT can be effective but EM^2's dynamic updating can yield higher-quality, better-aligned memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>MoT requires pre-inference on training data (extra cost) and its broader retrieval scope does not guarantee retention of the highest-quality memories over time compared to EM^2's update-driven selection.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoCoT (Automatic Chain-of-Thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated example-selection method that chooses demonstrations based on relevance and diversity metrics to build chain-of-thought prompts; used as a retrieval-memory baseline in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chain of thought prompting in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AutoCoT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Automated selection pipeline that identifies and assembles few-shot CoT demonstrations from data using relevance and diversity heuristics; requires pre-inference on training set to gather candidate demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Replicated on LLaMA-3-8B for fair comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>AutoCoT is a data-driven method to construct in-context demonstrations; authors used official implementation to replicate on LLaMA-3.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH and other reasoning benchmarks (commonsense and symbolic tasks used in extended comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides in-context CoT demonstrations selected from training data to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented in-context learning / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented explicit demonstration memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Selects exemplar demonstrations from training data based on relevance and diversity heuristics; these fixed chosen examples are concatenated into the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Selected exemplar questions with chain-of-thought reasoning and answers (text).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Heuristic relevance + diversity selection after pre-inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>EM^2 achieves an average improvement of 2.82% over AutoCoT on commonsense QA and symbolic understanding tasks in the paper's extended comparisons; EM^2's continual update maintains alignment with test distribution whereas AutoCoT's pre-selected static set may become suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamic explicit-memory updating (EM^2) can outperform static retrieval/selection approaches like AutoCoT by continuously optimizing what is retained based on validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>AutoCoT requires pre-inference on training data and constructs a static demonstration set which may not adapt to distribution shifts between training and test.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8451.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8451.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT / ComplexCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) and ComplexCoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fixed-prompt baselines where CoT uses hand-curated few-shot chain-of-thought demonstrations; ComplexCoT uses more detailed, multi-step complex prompts to elicit deeper reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CoT / ComplexCoT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fixed prompt-based methods: CoT provides a small set of hand-picked reasoning demonstrations (static initialization used also as EM^2* initialization); ComplexCoT uses complexity-based prompts to induce detailed multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B (used as baseline backbone in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LLM backbone; CoT/ComplexCoT are prompting techniques rather than model architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems and other reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Guide the LLM with fixed in-context chain-of-thought demonstrations to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>in-context learning with fixed demonstrations / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>fixed explicit prompt memory (static demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Concatenate fixed demonstrations (chain-of-thought examples) into the prompt as static guidance; no dynamic updates during streaming inference.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hand-crafted or pre-selected CoT demonstrations as raw text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Static prompt concatenation (no retrieval at runtime).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example numbers: CoT GSM8K single-inference: 79.61%; ComplexCoT GSM8K single-inference: 78.01%. In multiple-inference settings CoT GSM8K: 85.59% vs ComplexCoT 85.29%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ZS-CoT (no external memory) GSM8K: 76.80% (single-inference).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>EM^2 outperforms both CoT and ComplexCoT across multiple datasets; EM^2's runtime with memory size 8 is comparable to ComplexCoT but yields higher accuracy; EM^2* initialized with CoT demonstrates faster early gains but both initializations converge as memory accumulates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fixed-prompt CoT gives meaningful baseline gains, but dynamic explicit memory learning (EM^2) yields larger improvements and adapts over time; CoT initialization can speed early learning when used to seed EM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fixed demonstrations do not adapt to new incoming data and may become suboptimal as distribution shifts occur; ComplexCoT increases decoding time due to more detailed rationales.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8451.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8451.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZS-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought (ZS-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot prompting method that elicits chain-of-thought by appending 'Let's think step by step' to the prompt; used here as a no-explicit-memory baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ZS-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Zero-shot prompt that aims to trigger internal chain-of-thought reasoning in the LLM without providing external in-context demonstrations or external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B (used as the baseline backbone in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline LLM used to demonstrate performance without external explicit memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems and other reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>No external memory is provided; the model relies on zero-shot chain-of-thought instruction to reason.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>zero-shot reasoning / no external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>None (no external explicit memory; only prompt instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>GSM8K single-inference: 76.80%; Average across listed WMP datasets (excluding MATH) for ZS-CoT: 77.98% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Serves as the 'no-memory' baseline; EM^2 and fixed-memory CoT methods both outperform ZS-CoT significantly on math and other reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing external explicit memory (either fixed CoT demonstrations or EM^2 dynamic memory) improves performance over ZS-CoT; dynamic explicit memory (EM^2) yields the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies solely on prompting and the model's internal implicit knowledge — does not benefit from storing and reusing past reasoning steps or examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8451.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8451.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random/FIFO update ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random selection / First-In-First-Out (FIFO) memory update ablations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple alternative memory update strategies evaluated as ablations to EM^2's EM-based update; both greatly reduce downstream performance compared to EM^2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random / FIFO memory-updates (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Non-learning update strategies that either (a) randomly select memory entries to keep, or (b) evict earliest entries in FIFO order, used to test importance of EM-driven updates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B (evaluated within the same experimental setup as EM^2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same backbone; only the memory update mechanism is replaced for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problems (MATH and other WMP datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Streaming update scenario where memory is incrementally constructed; Random and FIFO are alternative L functions L(M_t, (X_t, Ŷ_t)).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory update ablation in streaming multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit external textual memory (but updated with non-learning rules)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Random: select memory entries at random; FIFO: evict oldest entries as new ones arrive (no evaluation-based selection).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Same as EM^2 (textual (X, Ŷ) pairs), but selection/eviction is random or order-based.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Same retrieval (embedding + cosine similarity) but memory contents differ due to update strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Ablation averages across six WMP datasets (Table 3): Random average 76.25% (vs EM^2 81.43%); FIFO average 76.00% (vs EM^2 81.43%). Example dataset declines shown in Table 3 (e.g., GSM8K: Random 76.42% vs EM^2 82.63%; FIFO 74.37%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct ablation of EM^2's M-step shows large drops in performance when replacing learning-driven updates with Random or FIFO, demonstrating that EM-based optimization of explicit memory is a key contributor to EM^2's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-optimized update policies (Random, FIFO) are significantly worse than EM^2; effective, validation-driven updates are necessary to ensure stored memories improve future predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>These ablations are intentionally simplistic and do not attempt to tune randomness/order heuristics; they serve to highlight the need for principled update mechanisms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MoT: Memory-of-thought enables ChatGPT to self-improve <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>Memoryllm: Towards self-updatable large language models <em>(Rating: 2)</em></li>
                <li>Memory ${ }^{3}$ : Language modeling with explicit memory <em>(Rating: 1)</em></li>
                <li>Inmemory learning: A declarative learning framework for large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8451",
    "paper_id": "paper-ba3ef3c9e2a7178b6414f9e71cfdc537c75ec62d",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "EM^2",
            "name_full": "Explicit Memory Learning with Expectation Maximization (EM^2)",
            "brief_description": "An explicit-text memory framework that treats stored strategies as latent variables and updates the explicit memory via an EM algorithm; memory stores model inputs plus chain-of-thought outputs, is vectorized for retrieval, and is updated to maximize expected performance on a representative validation set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EM^2",
            "agent_description": "A memory-guided inference method for LLMs that (1) selects relevant stored demonstrations (text pairs of problem + model reasoning), (2) uses them as in-context guidance for prediction, and (3) updates the explicit memory using an EM-style E-step (estimate contribution of memory strategies τ) and M-step (select updated memory maximizing validation-set likelihood).",
            "model_name": "LLaMA-3-8B (main backbone; experiments extended to LLaMA-3-70B, Mistral-7B, Mixtral, Qwen-2)",
            "model_description": "Primary experiments use LLaMA-3-8B as the backbone; authors report EM^2 is model-agnostic and scales with model capability (tested on 70B and several 7B models).",
            "task_name": "Math Word Problems (GSM8K, MultiArith, SingleEq, AddSub, SVAMP, AQuA, MATH), Commonsense QA (StrategyQA, CommonsenseQA, BoolQ, ARC-c), Symbolic Understanding (BigBench subsets)",
            "task_description": "Benchmarks requiring multi-step reasoning: (1) arithmetic/math word problems needing chain-of-thought; (2) commonsense QA often requiring implicit multi-step reasoning; (3) symbolic tasks requiring structured reasoning/counting/date understanding.",
            "task_type": "multi-step reasoning / question answering / mathematical problem solving / symbolic understanding",
            "memory_used": true,
            "memory_type": "explicit external textual memory (declarative/episodic explicit memory stored as text and embeddings)",
            "memory_mechanism": "Memory stores pairs (X, Ŷ) including the model's chain-of-thought; new predictions pass an entropy filter (normalized entropy ≤ ε) before being considered; stored texts are vectorized (text-embedding-3-large) and retrieval uses cosine similarity to select up to k demonstrations (k ≤ 8); updates are performed via an EM formulation: E-step computes expectations over latent strategies τ; M-step selects a memory subset m ⊂ M_t ∪ Γ(X_t, Ŷ_t) that maximizes summed predicted likelihood on a representative validation set (cluster-center based).",
            "memory_representation": "Textual pairs (input X and model-generated chain-of-thought + answer Ŷ), stored as raw text and embedding vectors.",
            "memory_retrieval_method": "Semantic search via cosine similarity on embeddings; selection limited to top-k (≤8) examples; representative validation set constructed by clustering (KMeans, default k=8) and selecting cluster centers for M-step evaluation.",
            "performance_with_memory": "Selected examples: GSM8K single-inference: 82.63% (EM^2) and 83.09% (EM^2* initialized with CoT); GSM8K multiple-inference: 86.35% (EM^2) and 86.43% (EM^2*). Average across math word-problem datasets (excluding MATH) single-inference: 81.43% (EM^2) vs 77.98% (ZS-CoT baseline). EM^2 consistently outperforms fixed-memory baselines across commonsense and symbolic tasks (see figures and Table 1).",
            "performance_without_memory": "No-memory baseline (ZS-CoT) GSM8K: 76.80% (single inference); CoT fixed-prompt baseline GSM8K: 79.61%; ComplexCoT GSM8K: 78.01%. EM^2 yields +5.83% over ZS-CoT and +3.02% over CoT on GSM8K (single-inference).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations and comparisons reported: (1) Replacing EM^2's learning function with Random selection or FIFO substantially reduces performance on math datasets (Table 3 averages: EM^2 81.43% vs Random 76.25% vs FIFO 76.00%), demonstrating the value of EM-based updates. (2) Clustering algorithm robustness: DBSCAN yields similar performance to KMeans. (3) Embedding model robustness: Sentence-BERT slightly lower; text-embedding-ada-002 achieves similar or slightly better average performance. (4) Memory size, validation-set size, and entropy-threshold ε were swept: memory capacity set to 20, top-k demonstrations up to 8; ε tuned (ε=9 chosen as a good tradeoff). (5) Comparisons to retrieval-memory baselines (MoT, AutoCoT) on MATH and other tasks show EM^2 achieves superior performance and better alignment between memory distribution and test distribution; averaged +2.82% over AutoCoT on commonsense & symbolic tasks.",
            "key_findings": "Explicit textual memory can be made to 'learn' (i.e., guarantee improved updates) by treating stored strategies as latent variables and updating via EM: EM^2 consistently improves accuracy across math, commonsense, and symbolic tasks compared to no-memory and fixed-memory baselines; EM^2 is robust to clustering and embedding choices, scales with model capability, and its updated memory can be shared across models (benefiting smaller models when receiving higher-quality memory).",
            "limitations_or_challenges": "Requires access to internal model information such as token probabilities/perplexity to compute normalized entropy and some update evaluations (challenging with closed commercial APIs that only return text); M-step imposes additional computation (but authors argue update cost is manageable since decoding-heavy steps are avoided); current evaluation limited to three task families (future work needed for code, translation, agent-based tasks).",
            "uuid": "e8451.0"
        },
        {
            "name_short": "MoT",
            "name_full": "Memory-of-Thought (MoT)",
            "brief_description": "A two-stage retrieval-based memory method that first performs coarse-grained semantic retrieval then fine-grained model filtering to select relevant prior examples; used as a retrieval-memory baseline in this paper's comparisons.",
            "citation_title": "MoT: Memory-of-thought enables ChatGPT to self-improve",
            "mention_or_use": "use",
            "agent_name": "Memory-of-Thought (MoT)",
            "agent_description": "Retrieval-memory approach which collects model outputs via pre-inference on training data and retrieves examples using semantic similarity followed by model-based filtering to pick high-quality demonstrations.",
            "model_name": "Compared under LLaMA-3-8B experimental setup (authors replicated MoT results on LLaMA-3).",
            "model_description": "Originally designed/used with ChatGPT in MoT paper; here compared after replication on LLaMA-3-8B.",
            "task_name": "MATH dataset (and other reasoning tasks in comparative experiments)",
            "task_description": "Used to supply retrieved demonstrations to improve multi-step reasoning performance.",
            "task_type": "retrieval-augmented multi-step reasoning / math problem solving",
            "memory_used": true,
            "memory_type": "retrieval-augmented external memory (pre-collected examples)",
            "memory_mechanism": "Pre-inference on training data to collect candidate examples; retrieval by semantic similarity then model-based filtering to choose final demonstrations.",
            "memory_representation": "Stored training examples and their model-generated reasoning/answers.",
            "memory_retrieval_method": "Semantic retrieval (coarse) followed by model-filtering (fine-grained).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "EM^2 was compared to MoT on the MATH dataset in aligned settings (authors add training samples to EM^2 and perform updates) and EM^2 achieved superior performance despite MoT's broader search scope; authors attribute EM^2 advantage to its update strategy that better preserves high-quality memories and aligns memory distribution with test distribution.",
            "key_findings": "EM^2 outperforms MoT in the paper's MATH experiments; pre-inference retrieval methods like MoT can be effective but EM^2's dynamic updating can yield higher-quality, better-aligned memory.",
            "limitations_or_challenges": "MoT requires pre-inference on training data (extra cost) and its broader retrieval scope does not guarantee retention of the highest-quality memories over time compared to EM^2's update-driven selection.",
            "uuid": "e8451.1"
        },
        {
            "name_short": "AutoCoT",
            "name_full": "AutoCoT (Automatic Chain-of-Thought prompting)",
            "brief_description": "An automated example-selection method that chooses demonstrations based on relevance and diversity metrics to build chain-of-thought prompts; used as a retrieval-memory baseline in this work.",
            "citation_title": "Automatic chain of thought prompting in large language models",
            "mention_or_use": "use",
            "agent_name": "AutoCoT",
            "agent_description": "Automated selection pipeline that identifies and assembles few-shot CoT demonstrations from data using relevance and diversity heuristics; requires pre-inference on training set to gather candidate demonstrations.",
            "model_name": "Replicated on LLaMA-3-8B for fair comparison in this paper.",
            "model_description": "AutoCoT is a data-driven method to construct in-context demonstrations; authors used official implementation to replicate on LLaMA-3.",
            "task_name": "MATH and other reasoning benchmarks (commonsense and symbolic tasks used in extended comparisons)",
            "task_description": "Provides in-context CoT demonstrations selected from training data to improve multi-step reasoning.",
            "task_type": "retrieval-augmented in-context learning / multi-step reasoning",
            "memory_used": true,
            "memory_type": "retrieval-augmented explicit demonstration memory",
            "memory_mechanism": "Selects exemplar demonstrations from training data based on relevance and diversity heuristics; these fixed chosen examples are concatenated into the prompt.",
            "memory_representation": "Selected exemplar questions with chain-of-thought reasoning and answers (text).",
            "memory_retrieval_method": "Heuristic relevance + diversity selection after pre-inference.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "EM^2 achieves an average improvement of 2.82% over AutoCoT on commonsense QA and symbolic understanding tasks in the paper's extended comparisons; EM^2's continual update maintains alignment with test distribution whereas AutoCoT's pre-selected static set may become suboptimal.",
            "key_findings": "Dynamic explicit-memory updating (EM^2) can outperform static retrieval/selection approaches like AutoCoT by continuously optimizing what is retained based on validation performance.",
            "limitations_or_challenges": "AutoCoT requires pre-inference on training data and constructs a static demonstration set which may not adapt to distribution shifts between training and test.",
            "uuid": "e8451.2"
        },
        {
            "name_short": "CoT / ComplexCoT",
            "name_full": "Chain-of-Thought (CoT) and ComplexCoT prompting",
            "brief_description": "Fixed-prompt baselines where CoT uses hand-curated few-shot chain-of-thought demonstrations; ComplexCoT uses more detailed, multi-step complex prompts to elicit deeper reasoning.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "agent_name": "CoT / ComplexCoT",
            "agent_description": "Fixed prompt-based methods: CoT provides a small set of hand-picked reasoning demonstrations (static initialization used also as EM^2* initialization); ComplexCoT uses complexity-based prompts to induce detailed multi-step reasoning.",
            "model_name": "LLaMA-3-8B (used as baseline backbone in the paper)",
            "model_description": "Standard LLM backbone; CoT/ComplexCoT are prompting techniques rather than model architectures.",
            "task_name": "Math word problems and other reasoning benchmarks",
            "task_description": "Guide the LLM with fixed in-context chain-of-thought demonstrations to improve multi-step problem solving.",
            "task_type": "in-context learning with fixed demonstrations / multi-step reasoning",
            "memory_used": true,
            "memory_type": "fixed explicit prompt memory (static demonstrations)",
            "memory_mechanism": "Concatenate fixed demonstrations (chain-of-thought examples) into the prompt as static guidance; no dynamic updates during streaming inference.",
            "memory_representation": "Hand-crafted or pre-selected CoT demonstrations as raw text.",
            "memory_retrieval_method": "Static prompt concatenation (no retrieval at runtime).",
            "performance_with_memory": "Example numbers: CoT GSM8K single-inference: 79.61%; ComplexCoT GSM8K single-inference: 78.01%. In multiple-inference settings CoT GSM8K: 85.59% vs ComplexCoT 85.29%.",
            "performance_without_memory": "ZS-CoT (no external memory) GSM8K: 76.80% (single-inference).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "EM^2 outperforms both CoT and ComplexCoT across multiple datasets; EM^2's runtime with memory size 8 is comparable to ComplexCoT but yields higher accuracy; EM^2* initialized with CoT demonstrates faster early gains but both initializations converge as memory accumulates.",
            "key_findings": "Fixed-prompt CoT gives meaningful baseline gains, but dynamic explicit memory learning (EM^2) yields larger improvements and adapts over time; CoT initialization can speed early learning when used to seed EM^2.",
            "limitations_or_challenges": "Fixed demonstrations do not adapt to new incoming data and may become suboptimal as distribution shifts occur; ComplexCoT increases decoding time due to more detailed rationales.",
            "uuid": "e8451.3"
        },
        {
            "name_short": "ZS-CoT",
            "name_full": "Zero-shot Chain-of-Thought (ZS-CoT)",
            "brief_description": "A zero-shot prompting method that elicits chain-of-thought by appending 'Let's think step by step' to the prompt; used here as a no-explicit-memory baseline.",
            "citation_title": "Large language models are zero-shot reasoners.",
            "mention_or_use": "use",
            "agent_name": "ZS-CoT",
            "agent_description": "Zero-shot prompt that aims to trigger internal chain-of-thought reasoning in the LLM without providing external in-context demonstrations or external memory.",
            "model_name": "LLaMA-3-8B (used as the baseline backbone in experiments)",
            "model_description": "Baseline LLM used to demonstrate performance without external explicit memory.",
            "task_name": "Math word problems and other reasoning benchmarks",
            "task_description": "No external memory is provided; the model relies on zero-shot chain-of-thought instruction to reason.",
            "task_type": "zero-shot reasoning / no external memory",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": "None (no external explicit memory; only prompt instruction).",
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "GSM8K single-inference: 76.80%; Average across listed WMP datasets (excluding MATH) for ZS-CoT: 77.98% (Table 1).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Serves as the 'no-memory' baseline; EM^2 and fixed-memory CoT methods both outperform ZS-CoT significantly on math and other reasoning tasks.",
            "key_findings": "Providing external explicit memory (either fixed CoT demonstrations or EM^2 dynamic memory) improves performance over ZS-CoT; dynamic explicit memory (EM^2) yields the largest gains.",
            "limitations_or_challenges": "Relies solely on prompting and the model's internal implicit knowledge — does not benefit from storing and reusing past reasoning steps or examples.",
            "uuid": "e8451.4"
        },
        {
            "name_short": "Random/FIFO update ablation",
            "name_full": "Random selection / First-In-First-Out (FIFO) memory update ablations",
            "brief_description": "Simple alternative memory update strategies evaluated as ablations to EM^2's EM-based update; both greatly reduce downstream performance compared to EM^2.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random / FIFO memory-updates (ablation)",
            "agent_description": "Non-learning update strategies that either (a) randomly select memory entries to keep, or (b) evict earliest entries in FIFO order, used to test importance of EM-driven updates.",
            "model_name": "LLaMA-3-8B (evaluated within the same experimental setup as EM^2)",
            "model_description": "Same backbone; only the memory update mechanism is replaced for ablation.",
            "task_name": "Math word problems (MATH and other WMP datasets)",
            "task_description": "Streaming update scenario where memory is incrementally constructed; Random and FIFO are alternative L functions L(M_t, (X_t, Ŷ_t)).",
            "task_type": "memory update ablation in streaming multi-step reasoning",
            "memory_used": true,
            "memory_type": "explicit external textual memory (but updated with non-learning rules)",
            "memory_mechanism": "Random: select memory entries at random; FIFO: evict oldest entries as new ones arrive (no evaluation-based selection).",
            "memory_representation": "Same as EM^2 (textual (X, Ŷ) pairs), but selection/eviction is random or order-based.",
            "memory_retrieval_method": "Same retrieval (embedding + cosine similarity) but memory contents differ due to update strategy.",
            "performance_with_memory": "Ablation averages across six WMP datasets (Table 3): Random average 76.25% (vs EM^2 81.43%); FIFO average 76.00% (vs EM^2 81.43%). Example dataset declines shown in Table 3 (e.g., GSM8K: Random 76.42% vs EM^2 82.63%; FIFO 74.37%).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Direct ablation of EM^2's M-step shows large drops in performance when replacing learning-driven updates with Random or FIFO, demonstrating that EM-based optimization of explicit memory is a key contributor to EM^2's gains.",
            "key_findings": "Non-optimized update policies (Random, FIFO) are significantly worse than EM^2; effective, validation-driven updates are necessary to ensure stored memories improve future predictions.",
            "limitations_or_challenges": "These ablations are intentionally simplistic and do not attempt to tune randomness/order heuristics; they serve to highlight the need for principled update mechanisms.",
            "uuid": "e8451.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MoT: Memory-of-thought enables ChatGPT to self-improve",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2
        },
        {
            "paper_title": "Memoryllm: Towards self-updatable large language models",
            "rating": 2
        },
        {
            "paper_title": "Memory ${ }^{3}$ : Language modeling with explicit memory",
            "rating": 1
        },
        {
            "paper_title": "Inmemory learning: A declarative learning framework for large language models",
            "rating": 1
        }
    ],
    "cost": 0.022162249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Explicit Memory Learning with Expectation Maximization</h1>
<p>Zhangyue Yin<em> Qiushi Sun</em> Qipeng Guo<em> Zhiyuan Zeng</em><br><em>Qinyuan Cheng</em> Xipeng Qiu<em><em> Xuanjing Huang</em><br></em>School of Computer Science, Fudan University<br>${ }^{\text {* }}$ The University of Hong Kong ${ }^{\text {a }}$ Shanghai AI Laboratory<br>{yinzy21, cengzy23, chengqy21}@m.fudan.edu.cn<br>qiushisun@connect.hku.hk guoqipeng@pjlab.org.cn<br>{xpqiu, xjhuang}@fudan.edu.cn</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have revolutionized the landscape of natural language processing, demonstrating remarkable abilities across various complex tasks. However, their stateless nature limits the capability to retain information across interactions, hindering performance in scenarios requiring historical context recall. To mitigate this, current approaches primarily use explicit memory to allow LLMs to store useful information, which is accessible, readable, and interpretable. Nevertheless, explicit memory lacks the reliable learning mechanisms of implicit memory, which can be optimized end-to-end. To harness the benefits of both, we introduce $\mathrm{EM}^{2}$, a novel framework enhancing explicit memory updates via the Expectation-Maximization (EM) algorithm. $\mathrm{EM}^{2}$ treats memory as a latent variable, ensuring continual learning and improvement during updates. Experimental results on streaming inference tasks demonstrate that $\mathrm{EM}^{2}$ outperforms existing methods without memory or with static external memory. Our in-depth analysis highlights that $\mathrm{EM}^{2}$ significantly enhances performance across various backbones and memory strategies, providing a robust solution for advancing LLM memory management and enabling explicit memory to learn and improve similarly to implicit memory.</p>
<h2>1 Introduction</h2>
<p>The advent of Large Language Models (LLMs) has shifted the landscape of machine learning, unveiling unprecedented capabilities for handling complex tasks across diverse domains (Ouyang et al., 2022; Achiam et al., 2023; Anthropic, 2024; Reid et al., 2024; Shao et al., 2024; Sun et al., 2024b; Zhao et al., 2023, inter alia). Despite these advancements, a fundamental limitation of LLMs is their statelessness: they do not retain information across invocations (Yao, 2024). This restricts</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison between Explicit and Implicit Memory. Explicit memory is represented through text, storing information directly accessible and readable. Implicit memory is stored in the form of parameters, which underlie the model's learned behaviors and are not directly interpretable. Deep blue indicates the memory currently being activated.
their ability to process and utilize previous interactions in a manner akin to human cognitive processes (Gabrieli, 1998), thereby limiting their utility in scenarios that require retention and recall of historical context (Zhang et al., 2024; Lu et al., 2024; Huang et al., 2023; Durante et al., 2024).</p>
<p>Recent studies have attempted to address this challenge by incorporating external memory mechanisms (Packer et al., 2023; Zhong et al., 2024), which can be categorized into explicit and implicit forms (Barco et al., 2006). As illustrated in Figure 1, explicit memory stores information in a textual format that is directly accessible and readable, such as rules, knowledge, and skills (Gao and Zhang, 2024; Guo et al., 2024). Implicit memory, on the other hand, is parametric, facilitating learning and updates (Wang et al., 2023a;</p>
<p>Ge et al., 2024). While parametric storage allows for end-to-end learning, it often faces issues with training stability (Franke et al., 2018), specification (Sukhbaatar et al., 2015), and interpretability (Zhang et al., 2021). With the increasing ability of LLMs to directly understand text (Brown et al., 2020; Wei et al., 2022a), explicit memory is becoming the dominant method for memory storage in LLMs (Madaan et al., 2022).</p>
<p>Updating is a critical feature of memory (Wang et al., 2024e; Li et al., 2023). Current methods of updating explicit memory include manual revisions (Mei et al., 2024) and self-reflection (Liu et al., 2023a; Shinn et al., 2023; Praas, 2023). Ge et al. (2023) conceptualize LLMs as an operating system (OS) and have developed memory update mechanisms inspired by OS design. Wang et al. (2024a) employ LLMs to autonomously summarize past experiences for enhanced external memory.</p>
<p>It is worth noticing that, LLMs may miss or make mistakes when internalizing knowledge (Yin et al., 2023; Wang et al., 2023b; Yao et al., 2023), and there is no guarantee that newly constructed memory is superior to its predecessors. In contrast, implicit memory, updated through gradients (Graves et al., 2016; Becattini and Uricchio, 2022), ensures learning during the memory update. Current methods for updating explicit memory do not guarantee learning and enhancement during the memory update process, marking a fundamental drawback. The primary reason is the nondifferentiability of textual memory, which means that memory updates lack a clear direction.</p>
<p>To address this, we propose $\mathrm{EM}^{2}$, which treats memory as a latent variable and update it using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). $\mathrm{EM}^{2}$ extracts relevant past experiences to guide current predictions and ensures that the memory is continuously optimized, enabling the model to learn and improve effectively over time. Experimental results on streaming inference tasks show that compared to models without exter-nal/fixed memory, our dynamic memory updating approach significantly enhances performance.</p>
<p>Our main contributions are as follows:</p>
<ul>
<li>We identify that current methods of updating explicit memory lack direction and do not ensure that updated memory is superior to previous versions.</li>
<li>We introduce $\mathrm{EM}^{2}$, which updates explicit memory using the EM algorithm to ensure
continuous learning and enhancement during the memory update process.</li>
<li>Experimental results demonstrate that $\mathrm{EM}^{2}$ significantly improves model performance.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 Memory Mechanism of LLMs</h3>
<p>Memory is fundamental to the development of intelligence (Anderson, 1999). Memory mechanisms in LLMs primarily involve retrieval (Gao et al., 2024), updating (Li et al., 2023), and utilization (Guo et al., 2024) processes. Retrieval aims to fetch relevant and accurate memories from a vast store, directly influencing the outcome's quality (He et al., 2022; Creswell and Shanahan, 2022). Updates include incremental, inductive, and compressive approaches. Incremental updates simply add newly acquired memories without processing them (Hong et al., 2023; Qian et al., 2023). Inductive updates utilize the LLM's capability to amalgamate and summarize memories, thereby narrowing the retrieval scope (Liu et al., 2023a; Didolkar et al., 2024). Compressive updates enhance the efficiency of memory use by condensing texts into vectors (Chevalier et al., 2023; Ge et al., 2024; Mu et al., 2024). The utilization of memory relies on the LLM's contextual understanding and learning capabilities, optimizing model behavior through the injection of text or parameters (Min et al., 2022; Liu et al., 2024; Wang et al., 2024d).</p>
<p>For LLMs, memory can be classified as explicit or implicit (Rovee-Collier et al., 2001; Barco et al., 2006). Explicit memory, also known as declarative memory, refers to forms of memory that can be articulated (Eichenbaum, 1997). It can be stored and retrieved in textual form (Sun et al., 2023; Zhong et al., 2024), offering readability and interpretability (Jiang et al., 2023b; Modarressi et al., 2024). Explicit memory does not depend on a specific model and can be utilized by various models post-generation (Gao and Zhang, 2024; Sun et al., 2024a). Additionally, humans can participate in modifying and refining explicit memory, making it widely applied in LLM memory modules (Wu et al., 2022). Implicit memory, on the other hand, refers to forms of memory that cannot be articulated. This type of memory is stored in parameters and updated through training (Weston et al., 2015; Anything, 2015; Sukhbaatar et al., 2015). Although explicit memory can also be updated through model-driven summarization and induction (Wang et al., 2024a;</p>
<p>Yang et al., 2024), it lacks the clear update targets characteristic of implicit memory, which ensures that the updated state is superior to its previous state.</p>
<h3>2.2 Model Inference</h3>
<p>The inference methods for LLMs predominantly encompass zero-shot, few-shot, and chain-ofthought (Chung et al., 2024). Zero-shot often requires model fine-tuning to equip LLMs with the capability to generate task-specific outputs directly (Raffel et al., 2020; Liu et al., 2021). Brown et al. (2020) observe that providing models with example prompts can significantly enhance their understanding of specific tasks. Currently, In-Context Learning (Dong et al., 2022) has emerged as a fundamental paradigm for addressing tasks using LLMs (Liu et al., 2023b), effectively leveraging minimal input to guide model responses (Min et al., 2022). Wei et al. (2022c) note that guiding models to generate intermediary reasoning steps will boost their performance for reasoning. This enhanced capability typically emerges only in models of certain scales, a phenomenon often referred to as "emergent abilities" (Wei et al., 2022a). Furthermore, recent studies (Wu et al., 2023; Li et al., 2024; Wang et al., 2024b) find that prompts serve a dual function: they not only activate the model's internal memory but also inject effective external knowledge and guidance. Additionally, updating and infusing memory in prompts offers benefits such as interpretability and flexibility (Chang et al., 2024), further enhancing the utility of LLMs in complex inference scenarios (Sahoo et al., 2024).</p>
<h2>3 Preliminary and Task Definition</h2>
<h3>3.1 Explicit Memory Learning</h3>
<p>Memory in AI are designed to mimic the human ability to remember past experiences and utilize this accumulated knowledge to aid in future tasks (Weston et al., 2015). In our model, explicit memory learning is implemented via a memory module $\mathcal{M}$ that stores strategies $\tau$ learned over time, which is formally represented as:</p>
<p>$$
M_{t}=\left{\tau_{1}, \tau_{2}, \ldots, \tau_{K}\right}
$$</p>
<p>where $M_{t}$ represents the state of the memory module at time $t, K$ is the memory size, and each $\tau_{i}$ is a tactic derived from past experiences. The updating of this memory is governed by a learning
function $L$, which adjusts the memory based on new experiences $(X, Y)$ :</p>
<p>$$
M_{t+1}=L\left(M_{t},\left(X_{t}, Y_{t}\right)\right)
$$</p>
<p>Here, $\left(X_{t}, Y_{t}\right)$ represents the input-output pair at time $t$, and the function $L$ determines how the memory should be updated, possibly by adding new strategies, modifying existing ones, or removing outdated strategies based on their relevance and effectiveness in the new context.</p>
<h3>3.2 Expectation Maximization Algorithm</h3>
<p>The Expectation Maximization (EM) algorithm is a powerful statistical tool used for parameter estimation in models with latent variables. It operates in two main steps: the Expectation (E) step and the Maximization (M) step. During the E step, the algorithm estimates the latent variables based on the current estimate of the parameters:</p>
<p>$$
Q\left(\theta \mid \theta^{(t)}\right)=\mathbb{E}_{Z \sim p\left(Z \mid X, \theta^{(t)}\right)}[\log p(X, Z \mid \theta)]
$$</p>
<p>where $\theta^{(t)}$ denotes the parameters at iteration $t$, $X$ is the observed data, $Z$ are the latent variables, and $p\left(Z \mid X, \theta^{(t)}\right)$ is the probability of the latent variables given the observed data and current parameters.</p>
<p>The M step then updates the parameters to maximize the expected log-likelihood found in the E step:</p>
<p>$$
\theta^{(t+1)}=\arg \max _{\theta} Q\left(\theta \mid \theta^{(t)}\right)
$$</p>
<p>This iterative process continues until convergence, making it suitable for complex models where direct likelihood maximization is infeasible (Dempster et al., 1977). The EM algorithm is particularly effective in scenarios where the model parameters include both observed and unobserved (latent) components. By alternating between estimating the hidden components given the parameters and then optimizing the parameters given the hidden components, EM facilitates a more accurate estimation of model parameters.</p>
<h3>3.3 Task Definition</h3>
<p>Given the following stream of data $\mathcal{D}=$ $\left{\left(X_{1}, Y_{1}\right),\left(X_{2}, Y_{2}\right), \ldots,\left(X_{n}, Y_{n}\right)\right}$, where $X_{t}$ represents the observed data at time $t$ and $Y_{t}$ denotes the corresponding true label, the objective is to construct effective memory $M_{t}$ that provides accurate predictions $\hat{Y}_{t}$.</p>
<p>Our primary goal is to minimize the discrepancy between the predicted labels $\hat{Y}<em t="t">{t}$ and the actual labels $Y</em>$ is crucial as it directly influences the model's ability to adapt to new data and make accurate predictions. Therefore, the challenge lies in designing a learning function $L$ that not only updates the memory efficiently but also ensures that these updates result in the accurate anticipation of future samples based on past and present data insights.}$. This is achieved by enhancing the predictive accuracy of the model under the guidance of the evolving memory $M_{t}$. The effectiveness of $M_{t</p>
<h2>4 Methodology</h2>
<h3>4.1 Memory based Inference</h3>
<p>At time $t$, the model receives an input $X_{t}$. In a zeroshot scenario, without any guidance from memory, the model $\xi$ generates the predicted label $\hat{Y}_{t}$ in an autoregressive manner as follows:</p>
<p>$$
P_{\xi}\left(\hat{Y}<em t="t">{t} \mid X</em>}\right)=\prod_{i=1}^{\left|\hat{Y<em _xi="\xi">{t}\right|} P</em>}\left(\hat{y<em t="t">{i} \mid X</em>\right)
$$}, \hat{y}_{&lt;i</p>
<p>To leverage past experiences stored in the memory, we enhance model's capability by introducing a memory-based guidance. Given the current input $X_{t}$, we extract the most relevant information from the current memory state $M_{t}$. This extraction process results in a memory subset $m_{t}$, defined as the set of elements in $M_{t}$ that are most relevant to $X_{t}$. The relevance can be quantified based on similarity measures, heuristic rules, or learned relevance functions. The resulting $m_{t}$ can be formally represented as:</p>
<p>$$
m_{t}=\operatorname{select}\left(M_{t}, X_{t}\right)
$$</p>
<p>where select is a function that retrieves the most relevant memory elements based on $X_{t}$.</p>
<p>With $m_{t}$ as an additional context, the model then generates $\hat{Y}<em t="t">{t}$ using both $m</em>$ to guide the prediction:}$ and $X_{t</p>
<p>$$
P_{\xi}\left(\hat{Y}<em t="t">{t} \mid m</em>}, X_{t}\right)=\prod_{i=1}^{\left|\hat{Y<em _xi="\xi">{t}\right|} P</em>}\left(\hat{y<em t="t">{i} \mid m</em>\right)
$$}, X_{t}, \hat{y}_{&lt;i</p>
<p>This memory-augmented inference mechanism allows the model to effectively utilize historical data, enhancing its predictive accuracy and adaptability in dynamic environments.</p>
<h3>4.2 Memory Module Construction</h3>
<p>The Memory Module $\mathcal{M}$ is constructed by accumulating pairs $\left(X_{i}, \hat{Y}_{i}\right)$ over time. Initially, the memory of the model is empty, representing a state of minimal prior knowledge. As the model processes data and generates predictions, it selectively updates this memory based on the quality and certainty of the information.</p>
<p>To quantify the certainty of each predicted output and determine its eligibility for memory inclusion, we define an uncertainty threshold $\epsilon$. A prediction $\hat{Y}<em i="i">{i}$ is considered high-quality if its normalized entropy, which measures the average uncertainty across all predicted components, is below this threshold. The entropy $H\left(\hat{Y}</em>\right)$ for each prediction is calculated as follows:</p>
<p>$$
H\left(\hat{Y}<em i="i">{i}\right)=-\frac{1}{\left|\hat{Y}</em>}\right|} \sum_{j=1}^{\left|\hat{Y<em _xi="\xi">{i}\right|} \log P</em>}\left(\hat{y<em i="i">{j} \mid X</em>\right) \leq \epsilon
$$}, \hat{y}_{&lt;j</p>
<p>When the above condition is satisfied, indicating that the generated prediction $\hat{Y}_{i}$ is of sufficiently high certainty and quality, it is integrated into the memory using the learning function $L$, as discussed in Section 3.1.</p>
<h3>4.3 Memory Update through Learning Function</h3>
<p>We employ the EM algorithm to design the learning function $L$. As depicted in Figure 2 under (2) and (3), if the generated $\hat{Y}<em t="t">{i}$ satisfies condition 8 , it is fed along with the current memory state $M</em>$ into the learning function $L$. The update equation is:</p>
<p>$$
M_{t+1}=L\left(M_{t},\left(X_{t}, \hat{Y}_{t}\right)\right)
$$</p>
<p>We treat strategies $\tau$ as latent variables $Z$ and $M_{t}$ as the parameter $\theta$ in Eq. 3, transforming the learning process into an EM learning framework.</p>
<h3>4.3.1 Construction of Representative Validation Set</h3>
<p>To evaluate the updates efficiently, we construct a representative validation set $\mathcal{V}$ from the dataset $\mathcal{D}$ not yet included in the memory $M_{t}$. We select cluster centers from $\mathcal{D} \backslash M_{t}$ to form $\mathcal{V}$, reducing redundancy and improving the efficiency of memory updates. The selection can be represented by:</p>
<p>$$
V_{t}=\operatorname{centers}\left(\left{\left(X_{1}, \hat{Y}<em t="t">{1}\right), \ldots,\left(X</em>}, \hat{Y<em t="t">{t}\right)\right} \backslash M</em>\right)
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of $\mathrm{EM}^{2}$ for memory-guided prediction in streaming data. At each timestep $t$, the model receives an input $X_{t}$. (1) utilizes the memory $M_{t}$ to select relevant demonstrations that guide the generation of the prediction $\hat{Y}<em t="t">{t}$. (2) and (3) depict the integration of the newly generated $\hat{Y}</em>$ into the memory updating process, ensuring that the memory evolves with the latest data insights and contributes to future predictions.}$ and the current memory $M_{t</p>
<h3>4.3.2 E-step: Inference Procedure</h3>
<p>Let $\mathcal{V}<em v="v">{t}=\left{\left(X</em>$ and the memory $M$ is calculated as:}, Y_{v}\right)\right}$. Based on Equation 3, the prediction for $Y_{v}$ given $X_{v</p>
<p>$$
\begin{aligned}
P\left(Y_{v} \mid X_{v} ; M\right) &amp; =\sum_{\tau} P\left(Y_{v}, \tau \mid X_{v} ; M\right) \
&amp; =\sum_{\tau} P\left(Y_{v} \mid X_{v}, \tau\right) P\left(\tau \mid X_{v} ; M\right) \
&amp; =\mathbb{E}<em v="v">{\tau \sim P\left(\tau \mid X</em>, \tau\right)\right]
\end{aligned}
$$} ; M\right)}\left[P\left(Y_{v} \mid X_{v</p>
<h3>4.3.3 M-step: Learning Procedure</h3>
<p>The memory is updated based on the maximization step defined as:</p>
<p>$$
M_{t+1}=\underset{m \subset M_{t} \cup \Gamma\left(X_{t}, \hat{Y}<em i="1">{t}\right)}{\arg \max } \sum</em>}^{|\mathcal{V<em i="i">{t}|} P\left(Y</em> ; m\right)
$$} \mid X_{i</p>
<p>where $\Gamma$ represents a function extracting knowledge from $\left(X_{t}, \hat{Y}<em t="t">{t}\right)$ to generate $\tau</em>$, which can be formally represented as:</p>
<p>$$
\tau_{t}=\Gamma\left(X_{t}, \hat{Y}_{t}\right)
$$</p>
<p>This step ensures that the updated memory $M_{t+1}$ performs better on $\mathcal{V}<em t="t">{t}$ than the previous state $M</em>$, effectively capturing the beneficial strategies for future predictions.</p>
<h2>5 Experiment</h2>
<h3>5.1 Evaluation Datasets</h3>
<p>To assess the efficacy of our approach, we evaluate it across three distinct types of tasks: word math problems, commonsense question answering (QA), and symbolic analysis. We utilize the following datasets for these evaluations:</p>
<ul>
<li>Word Math Problem: GSM8K (Cobbe et al., 2021), MultiArith (Roy and Roth, 2015), SingleEq (Koncel-Kedziorski et al., 2016), AddSub (Hosseini et al., 2014), SVAMP (Patel et al., 2021), AQUA (Ling et al., 2017) and MATH (Hendrycks et al., 2021).</li>
<li>Commonsense QA: StrategyQA (Geva et al., 2021), CommonsenseQA (CSQA; Talmor et al., 2019), BoolQ (Clark et al., 2019), the AI2 Reasoning Challenge (ARC-c; Clark et al., 2018).</li>
<li>Symbolic Understanding: Date Understanding, Penguins in a Table, Colored Objects, and Object Counting sourced from BigBench (Suzgun et al., 2023).</li>
</ul>
<p>For a more detailed description of the datasets, please refer to Appendix A.</p>
<h3>5.2 Experiment Settings</h3>
<p>Implementation Details. The inference process of the model not only demonstrates its understand-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">SingleEq</th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single Inference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">76.80</td>
<td style="text-align: center;">94.83</td>
<td style="text-align: center;">89.96</td>
<td style="text-align: center;">84.30</td>
<td style="text-align: center;">81.45</td>
<td style="text-align: center;">40.55</td>
<td style="text-align: center;">29.02</td>
<td style="text-align: center;">77.98</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">79.61</td>
<td style="text-align: center;">96.50</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">85.31</td>
<td style="text-align: center;">82.76</td>
<td style="text-align: center;">42.32</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">79.80</td>
</tr>
<tr>
<td style="text-align: center;">ComplexCoT</td>
<td style="text-align: center;">78.01</td>
<td style="text-align: center;">96.67</td>
<td style="text-align: center;">91.92</td>
<td style="text-align: center;">84.81</td>
<td style="text-align: center;">81.48</td>
<td style="text-align: center;">42.51</td>
<td style="text-align: center;">29.50</td>
<td style="text-align: center;">79.23</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{EM}^{2}$</td>
<td style="text-align: center;">82.63</td>
<td style="text-align: center;">97.77</td>
<td style="text-align: center;">92.71</td>
<td style="text-align: center;">86.32</td>
<td style="text-align: center;">83.91</td>
<td style="text-align: center;">45.27</td>
<td style="text-align: center;">30.12</td>
<td style="text-align: center;">81.43</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{EM}^{2*}$</td>
<td style="text-align: center;">83.09</td>
<td style="text-align: center;">97.83</td>
<td style="text-align: center;">92.71</td>
<td style="text-align: center;">87.59</td>
<td style="text-align: center;">84.19</td>
<td style="text-align: center;">46.45</td>
<td style="text-align: center;">30.22</td>
<td style="text-align: center;">81.98</td>
</tr>
<tr>
<td style="text-align: center;">Multiple Inference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ZS-CoT</td>
<td style="text-align: center;">84.98</td>
<td style="text-align: center;">97.50</td>
<td style="text-align: center;">92.71</td>
<td style="text-align: center;">88.61</td>
<td style="text-align: center;">87.18</td>
<td style="text-align: center;">47.24</td>
<td style="text-align: center;">32.22</td>
<td style="text-align: center;">83.03</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">85.59</td>
<td style="text-align: center;">98.00</td>
<td style="text-align: center;">94.29</td>
<td style="text-align: center;">91.13</td>
<td style="text-align: center;">91.76</td>
<td style="text-align: center;">51.57</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.39</td>
</tr>
<tr>
<td style="text-align: center;">ComplexCoT</td>
<td style="text-align: center;">85.29</td>
<td style="text-align: center;">98.16</td>
<td style="text-align: center;">93.70</td>
<td style="text-align: center;">89.87</td>
<td style="text-align: center;">89.62</td>
<td style="text-align: center;">50.78</td>
<td style="text-align: center;">32.46</td>
<td style="text-align: center;">84.57</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{EM}^{2}$</td>
<td style="text-align: center;">86.35</td>
<td style="text-align: center;">98.83</td>
<td style="text-align: center;">95.86</td>
<td style="text-align: center;">93.41</td>
<td style="text-align: center;">92.51</td>
<td style="text-align: center;">53.14</td>
<td style="text-align: center;">33.82</td>
<td style="text-align: center;">86.68</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{EM}^{2*}$</td>
<td style="text-align: center;">86.43</td>
<td style="text-align: center;">98.83</td>
<td style="text-align: center;">95.66</td>
<td style="text-align: center;">94.43</td>
<td style="text-align: center;">92.55</td>
<td style="text-align: center;">53.93</td>
<td style="text-align: center;">33.96</td>
<td style="text-align: center;">86.97</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on Math Word Problems (Accuracy in \%). The best outcomes are emphasized in bold. Average represents the average performance across all datasets, excluding MATH. $\mathrm{EM}^{2}$ denotes initialization using ZS-CoT, while $\mathrm{EM}^{2 *}$ indicates initialization with CoT demonstrations, highlighted with a $\square$ background. To ensure a fair comparison, the LLaMA-3-8B model (Dubey et al., 2024) is used as the backbone across all methods.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance comparison on (a) commonsense question answering and (b) symbolic understanding tasks. The charts illustrate that $\mathrm{EM}^{2}$ demonstrates a distinct advantage over both no and fixed-memory mechanisms.
ing and analysis of problems but often encapsulates latent knowledge (Buehner et al., 2005). Therefore, we store the model's reasoning process along with the problem as the model memory. In the main experiments, memory is vectorized using text-embedding-3-large, and relevancy is calculated using cosine distance as specified in Eq. 6. To ensure fair comparisons, we limit the selection to a maximum of 8 examples. These vectors are also employed to determine the clustering centers as outlined in Eq. 10. For more details and ablation studies, see Appendix B and C.
Baselines. To validate the efficacy of our approach, we compare it against three baseline methods representing different levels of memory integration: models without memory, with fixed memory, and with retrieval-based memory.</p>
<ul>
<li>No Memory: The Zero-shot CoT (ZS-CoT; Kojima et al., 2022) utilizes the prompt "Let's think step by step" to activate the model's internal reasoning capabilities without relying
on external memory aids.</li>
<li>Fixed Memory: The Chain-of-Thought (CoT; Wei et al., 2022b) employs fixed prompts to guide the model through a reasoning process. ComplexCoT (Fu et al., 2023) extends this by using complex prompts that guide the model to generate more detailed reasoning processes.</li>
<li>Retrieval Memory: The Memory-of-Thought (MoT; Li and Qiu, 2023) incorporates a twostage memory retrieval process, which includes coarse-grained semantic retrieval followed by fine-grained model filtering to select relevant memories. AutoCoT (Zhang et al., 2023) selects examples based on relevance and diversity metrics tailored to the query.</li>
</ul>
<p>In contrast to the main experiment where memory updates are conducted using test samples, MoT and AutoCoT require pre-inference on training data. To ensure a fair comparison, we align the settings with these methods to in Section 5.4.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance comparison of different memory mechanisms across various LLMs.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of different memory updating mechanisms on the MATH dataset.</p>
<p><strong>Backbones.</strong> In the main experiment, we employ LLaMA-3-8B (Dubey et al., 2024). For analysis, we extend our investigations to include more LLMs, including LLaMA-3-70B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023a), Mixtral (Jiang et al., 2024a), and Qwen-2 (Bai et al., 2023).</p>
<h3>5.3 Main Results</h3>
<p><strong>Word Math Problem.</strong> Table 1 presents the results of math word problems. Compared to methods with no memory or fixed memory, our memory learning approach exhibits significant advantages. Notably, on the GSM8K dataset, EM<sup>2</sup> outperforms the ZS-CoT by 5.83% and CoT by 3.02%. This improvement is attributed to the dynamic memory updating mechanism of EM<sup>2</sup>. We utilize two initialization methods: ZS-CoT, where the initial memory is empty, and CoT, which provides eight high-quality demonstrations at initialization. While the CoT initialization ensures better initial performance, the efficacy of both approaches converges as the memory accumulates. For instance, on the SingleEq dataset, results from both initialization methods are identical. Further, we analyze multiple inference scenarios (Wang et al., 2023c) and observe that EM<sup>2</sup> retains a clear advantage. Moreover, as more memories are integrated, the performance gap</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance comparison of retrieval-based memory methods on the MATH dataset.</p>
<p>between the two initialization methods narrows.</p>
<p><strong>Commonsense QA and Symbolic.</strong> The experimental results for commonsense QA and symbolic understanding tasks are shown in Figure 3. We observe that EM<sup>2</sup> effectively enhances model performance on both types of tasks. Notably, EM<sup>2</sup> demonstrates a more pronounced advantage in challenging tasks, such as those involving complex, non-factoid information in the BoolQ dataset, and tasks requiring implicit multi-step reasoning in the StrategyQA dataset. This improvement can be attributed to EM<sup>2</sup>'s memory updating and retrieval mechanisms, which ensure the selection of high-quality and relevant demonstrations.</p>
<h3>5.4 Analysis and Discussion</h3>
<p><strong>Performance on Various Models.</strong> The performance of EM<sup>2</sup> across a range of models is analyzed in Figure 4, focusing on two representative datasets: GSM8K and CSQA. We observe that EM<sup>2</sup> consistently delivers significant performance enhancements across different models. Notably, models with greater computational capabilities benefit more substantially from the EM<sup>2</sup> approach. For instance, despite having a similar number of parameters, Qwen-7B exhibits a greater improvement than Mistral-7B. Moreover, EM<sup>2</sup> proves to be ver-</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Impact of memory swapping on model performance. The horizontal axis represents the proportion of memory injected. The horizontal lines indicate the baseline accuracies for models with fixed memory and EM² initialized with ZS-CoT.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Comparison of EM² with varying memory sizes and fixed memory methods in terms of runtime and accuracy. The horizontal axis represents the runtime in minutes, the vertical axis shows accuracy, and the size of the points indicates the size of the memory.</p>
<p>satile, not only enhancing the performance of dense models but also boosting the efficacy of Mixture of Experts (MoE) models like Mixtral. This adaptability underscores EM²'s effectiveness in leveraging complex memory dynamics across different architectural frameworks.</p>
<h3>Analysis of Memory Updating Mechanism</h3>
<p>The impact of different memory updating strategies on accuracy is analyzed in Figure 5. We experimented with replacing the learning function in Section 4.3 with two simpler updating strategies: random selection and First-In-First-Out (FIFO) (Manurung, 2019). Results on the MATH dataset, particularly in the precalculus subset, show that these changes significantly reduce model performance. The primary reason for this decline can be attributed to the inherent limitations of Random and FIFO strategies, which rely on randomness and sample order, respectively, and cannot guarantee the effectiveness of memory updates. This analysis highlights the efficacy of the EM² approach, which employs the EM algorithm to ensure gradual and effective optimization of memory.</p>
<h3>Comparison of Memory Retrieval Method</h3>
<p>In Figure 6, we compare the EM² with two memory retrieval methods. Both MoT and AutoCoT require pre-inference on the training dataset to gather examples for retrieval. To ensure a fair comparison, we incorporate training samples into EM², first performing memory updates and constructing a representative validation set on the training dataset, before introducing the test set for accuracy calculations. Results on the MATH dataset demonstrate that EM² achieves superior performance compared to traditional memory retrieval methods. Despite having a narrower search scope compared to the broader retrieval range of MoT and AutoCoT, the EM²'s updating strategy ensures the retention of high-quality memories. Moreover, continuous updates maintain alignment between the memory distribution and the test distribution, thereby resulting in enhanced performance.</p>
<h3>Analysis of Memory Sharing</h3>
<p>The memory constructed by EM² is model-agnostic, enabling the transfer and sharing of memories between models. In Figure 7, we explore the effects of exchanging</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Impact of varying the threshold $\epsilon$ on model performance.
memories between LLaMA-3-8B and LLaMA-370B. Each model first performs inference on the training dataset, after which their memories are swapped. As shown in Figure 7a, there is a gradual improvement in the performance of the 8B model as the proportion of memory from the 70B model increases. This indicates that smaller models can benefit from high-quality memories sourced from larger models. Conversely, Figure 7b reveals that the performance of the 70B model remains unaffected by the memory from the 8B model, as lowerquality memories do not enter our memory module.
Analysis of Memory Size. In Figure 8, we analyze the impact of memory size on accuracy and running time. We observe that on the GSM8K and SVAMP datasets, when the number of demonstrations in memory $m_{t}$ is reduced to two, the running time becomes comparable to the method with CoT (Wei et al., 2022c). Thanks to the effective memory updating strategy of $\mathrm{EM}^{2}$, the performance remains significantly superior to the CoT method even with the reduced number of demonstrations. The ComplexCoT method (Fu et al., 2023), which requires multi-step detailed derivations, demands more reasoning time. We note that the running times of ComplexCoT and $\mathrm{EM}^{2}$ with a memory size of eight are comparable, yet $\mathrm{EM}^{2}$ significantly outperforms ComplexCoT in terms of accuracy. The additional computational time for $\mathrm{EM}^{2}$ is attributed to the M-step in Section 4.3.3, whereas the memory update does not involve costly decoding processes, thus not incurring significant overhead.</p>
<p>Analysis of Threshold $\epsilon$. In Figure 9, we analyze the impact of variations in the threshold $\epsilon$ from Eq. 8 on model performance. The results on datasets such as GSM8K and SVAMP indicate that a lower threshold allows low-quality information to enter the memory, which in turn degrades
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Impact of varying the number of clusters on model performance.
the model's performance. Conversely, setting the threshold too high significantly reduces the amount of information entering the memory, diminishing the diversity of the stored data. Therefore, setting $\epsilon$ to 9 offers an optimal balance between high-quality information and diversity within the memory.</p>
<p>Analysis of Number of Clusters. In Figure 10, we evaluate the impact of different cluster counts on model performance. The results on the GSM8K and SVAMP datasets show that a smaller number of clusters reduces the diversity of samples in the representative validation set, which in turn can lower model performance. Initially, when there are fewer samples available, it is challenging to form a meaningful number of clusters. Therefore, setting the number of clusters to eight is found to be appropriate for achieving a good balance between clustering quality and the meaningful segmentation of data.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we analyze the advantages of explicit memory over implicit memory and highlight a critical limitation of the former: its inability to ensure the effectiveness of updates as reliably as implicit memory. To address this, we introduce $\mathrm{EM}^{2}$, which treats memory as a latent variable and iteratively updates it using the EM algorithm, thereby ensuring that updated memories are superior to their predecessors. Experiments show that $\mathrm{EM}^{2}$ offers significant advantages over models without memory and those with fixed memory. Importantly, the performance of $\mathrm{EM}^{2}$ scales with the model's capabilities, suggesting that more powerful models can leverage $\mathrm{EM}^{2}$ to achieve even greater benefits. Additionally, $\mathrm{EM}^{2}$ is model-agnostic, which allows for the transfer and sharing of memory across different models. Analyses reveal that weaker LLMs can significantly benefit from high-quality memories derived from larger counterparts.</p>
<h2>Limitations</h2>
<p>Generalization to a Broader Range of Tasks. While we have analyzed $\mathrm{EM}^{2}$ across three distinct types of tasks, there is potential to extend this approach to a wider array of generative tasks (GozaloBrizuela and Garrido-Merchán, 2023), such as code generation (Jiang et al., 2024b), machine translation (Ganesh et al., 2023), and various agent-based tasks (Wang et al., 2024c). Additionally, the form of memory could also be diversified to include structured data, triplets, user historical information, and more. Our current scope has not yet explored these domains, and we see the exploration of $\mathrm{EM}^{2}$ 's potential in more diverse tasks as an avenue for future work.</p>
<p>Application to Commercial Models. $\mathrm{EM}^{2}$ requires access to internal model information, such as perplexity, to assess the effectiveness of new memories. However, for commercial models that only provide text outputs, such as OpenAI's GPT models (Achiam et al., 2023) or Anthropic's Claude models (Anthropic, 2024), despite their powerful capabilities, applying $\mathrm{EM}^{2}$ remains challenging.</p>
<p>Incorporating Human Supervision. As mentioned in Section 5.4, higher-quality memories can significantly enhance model performance. This paper primarily focuses on memories constructed autonomously by the model. An intriguing question is whether human-supervised memory enhancement and correction could further improve performance. Additionally, how to effectively incorporate human supervision (Wu et al., 2022), such as step-by-step guidance (Lightman et al., 2023), remains an open question for future research.</p>
<h2>Ethics Statement</h2>
<p>Data Privacy. Our approach constructs memory from the model's own outputs and does not require the collection or acquisition of personal data. The prompts and data used in our experiments do not involve any personal or privacy-sensitive information, ensuring compliance with privacy standards.</p>
<p>Environmental Protection. The construction of large language models and the generation of data and memory are likely to become more prevalent, consuming significant computational resources and potentially increasing carbon emissions. We advocate for sustainable AI development, emphasizing the reduction of carbon footprints and the promo-
tion of green AI initiatives to mitigate environmental impacts.</p>
<p>Adherence to Ethical Guidelines. We adhere to ethical guidelines and ensure that our data usage complies with the corresponding dataset licenses. Detailed statistics about the datasets and their respective licenses is listed in Table 2.</p>
<h2>Acknowledge</h2>
<p>This work was supported by the National Natural Science Foundation of China (No. 62236004). The computations in this research were performed using the CFFF platform of Fudan University. We would like to express our sincere gratitude to all the reviewers for their valuable suggestions and assistance, which have significantly contributed to the improvement of this manuscript.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Mike Anderson. 1999. The development of intelligence. Psychology Press.</p>
<p>AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card.</p>
<p>Ask Me Anything. 2015. Dynamic memory networks for natural language processing. Kumar et al. arXiv Pre-Print, 97.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.</p>
<p>Angel Barco, Craig H Bailey, and Eric R Kandel. 2006. Common molecular mechanisms in explicit and implicit memory. Journal of neurochemistry, 97(6):1520-1533.</p>
<p>Federico Becattini and Tiberio Uricchio. 2022. Memory networks. In Proceedings of the 30th ACM International Conference on Multimedia, MM '22, page</p>
<p>7380-7382, New York, NY, USA. Association for Computing Machinery.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Markus Buehner, Stefan Krumm, and Marion Pick. 2005. Reasoning= working memory $\neq$ attention. Intelligence, 33(3):251-272.</p>
<p>Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, and Jingbo Zhu. 2024. Efficient prompting methods for large language models: A survey. Preprint, arXiv:2404.01077.</p>
<p>Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168.</p>
<p>Antonia Creswell and Murray Shanahan. 2022. Faithful reasoning using large language models. Preprint, arXiv:2208.14271.</p>
<p>Arthur P Dempster, Nan M Laird, and Donald B Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1-22.</p>
<p>Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev Arora. 2024. Metacognitive capabilities of llms:</p>
<p>An exploration in mathematical problem solving. Preprint, arXiv:2405.12205.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning. arXiv preprint arXiv:2301.00234.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</p>
<p>Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. 2024. Agent ai: Surveying the horizons of multimodal interaction. arXiv preprint arXiv:2401.03568.</p>
<p>Howard Eichenbaum. 1997. Declarative memory: Insights from cognitive neurobiology. Annual review of psychology, 48(1):547-572.</p>
<p>Jörg Franke, Jan Niehues, and Alex Waibel. 2018. Robust and scalable differentiable neural computer for question answering. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 47-59, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations.</p>
<p>John DE Gabrieli. 1998. Cognitive neuroscience of human memory. Annual review of psychology, 49(1):87115 .</p>
<p>Sahana Ganesh, Vedant Dhotre, Pranav Patil, and Dipti Pawade. 2023. A comprehensive survey of machine translation approaches. In 2023 6th International Conference on Advances in Science and Technology (ICAST), pages 160-165.</p>
<p>Hang Gao and Yongfeng Zhang. 2024. Memory sharing for large language model based agents. Preprint, arXiv:2404.09982.</p>
<p>Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: A survey. Preprint, arXiv:2312.10997.</p>
<p>Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2024. In-context autoencoder for context compression in a large language model. In The Twelfth International Conference on Learning Representations.</p>
<p>Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. 2023. Llm as os, agents as apps: Envisioning aios, agents and the aiosagent ecosystem. Preprint, arXiv:2312.03815.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Roberto Gozalo-Brizuela and Eduardo C. GarridoMerchán. 2023. A survey of generative ai applications. Preprint, arXiv:2306.02781.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. 2016. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471476.</p>
<p>Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. 2024. Empowering working memory for large language model agents. Preprint, arXiv:2312.17259.</p>
<p>Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. Preprint, arXiv:2301.00303.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523-533. Association for Computational Linguistics.</p>
<p>Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1051-1068, Singapore. Association for Computational Linguistics.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a. Mistral 7b. Preprint, arXiv:2310.06825.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024a. Mixtral of experts. Preprint, arXiv:2401.04088.</p>
<p>Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736.</p>
<p>Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024b. A survey on large language models for code generation. Preprint, arXiv:2406.00515.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.</p>
<p>Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023. Large language models with controllable working memory. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1774-1793, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Xiaonan Li and Xipeng Qiu. 2023. MoT: Memory-ofthought enables ChatGPT to self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 63546374, Singapore. Association for Computational Linguistics.</p>
<p>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2024. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In The Twelfth International Conference on Learning Representations.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Preprint, arXiv:2305.20050.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 158-167. Association for Computational Linguistics.</p>
<p>Frederick Liu, Terry Huang, Shihang Lyu, Siamak Shakeri, Hongkun Yu, and Jing Li. 2021. Enct5: A framework for fine-tuning t5 as non-autoregressive models. arXiv preprint arXiv:2110.08426.</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023a. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. Preprint, arXiv:2311.08719.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35.</p>
<p>Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, and Bao Ge. 2024. Understanding llms: A comprehensive overview from training to inference. Preprint, arXiv:2401.02038.</p>
<p>Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Weichao Wang, Xingshan Zeng, Lifeng Shang, Xin Jiang, and Qun Liu. 2024. Self: Self-evolution with language feedback. Preprint, arXiv:2310.00533.</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve GPT-3 after deployment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2833-2861, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Jonson Manurung. 2019. Application of fifo algorithm (first in first out) to simulation queue. Infokum, 7(2, Juni):44-47.</p>
<p>Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. 2024. Aios: Llm agent operating system. Preprint, arXiv:2403.16971.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. 2024. Memllm: Finetuning llms to use an explicit read-write memory. Preprint, arXiv:2404.11672.</p>
<p>Jesse Mu, Xiang Li, and Noah Goodman. 2024. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.</p>
<p>Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. 2023. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Robert Praas. 2023. Self-reflection on chain-of-thought reasoning in large language models.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. arXiv preprint arXiv:2307.07924.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67.</p>
<p>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al.</p>
<ol>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530.</li>
</ol>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Carolyn K Rovee-Collier, Harlene Hayne, and Michael Colombo. 2001. The development of implicit and explicit memory. John Benjamins Publishing Company Amsterdam.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1743-1752. The Association for Computational Linguistics.</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering in large language models: Techniques and applications. Preprint, arXiv:2402.07927.</p>
<p>Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Hang Yan, Fei Yang, Zhe Li, Hujun Bao, and Xipeng Qiu. 2024. Cpt: a pre-trained unbalanced transformer for both chinese language understanding and generation. SCIENCE CHINA Information Sciences, 67(5):152102-.</p>
<p>Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366.</p>
<p>Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.</p>
<p>Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, et al. 2024a. A survey of neural code intelligence: Paradigms, advances and beyond. arXiv preprint arXiv:2403.14734.</p>
<p>Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, and Lingpeng Kong. 2023. Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. arXiv preprint arXiv:2310.00280.</p>
<p>Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shiduo Zhang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang,</p>
<p>Lingling Wu, Zhangyue Yin, Xuanjing Huang, YuGang Jiang, and Xipeng Qiu. 2024b. Moss: An open conversational large language model. Machine Intelligence Research.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Bo Wang, Tianxiang Sun, Hang Yan, Siyin Wang, Qingyuan Cheng, and Xipeng Qiu. 2024a. Inmemory learning: A declarative learning framework for large language models. Preprint, arXiv:2403.02757.</p>
<p>Jianing Wang, Qiushi Sun, Xiang Li, and Ming Gao. 2024b. Boosting language models reasoning with chain-of-knowledge prompting. Preprint, arXiv:2306.06427.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024c. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6).</p>
<p>Liang Wang, Nan Yang, and Furu Wei. 2024d. Learning to retrieve in-context examples for large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1752-1767, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023a. Augmenting language models with long-term memory. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, and Xuan-Jing Huang. 2023b. Hallucination detection for generative large language models by bayesian sequential estimation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15361-15371.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves</p>
<p>chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian McAuley. 2024e. Memoryllm: Towards self-updatable large language models. Preprint, arXiv:2402.04624.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022c. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory networks. Preprint, arXiv:1410.3916.</p>
<p>Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong Ma, and Liang He. 2022. A survey of human-in-the-loop for machine learning. Future Generation Computer Systems, 135:364-381.</p>
<p>Yang Wu, Yanyan Zhao, Zhongyang Li, Bing Qin, and Kai Xiong. 2023. Improving cross-task generalization with step-by-step instructions. SCIENCE CHINA Information Sciences, pages -.</p>
<p>Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and Weinan E. 2024. Memory ${ }^{3}$ : Language modeling with explicit memory. Preprint, arXiv:2407.01178.</p>
<p>Shunyu Yao. 2024. Language agents: From next-token prediction to digital automation.</p>
<p>Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10222-10240, Singapore. Association for Computational Linguistics.</p>
<p>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they don't know? In Findings of the Association for Computational Linguistics: ACL 2023, pages 8653-8665, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Yu Zhang, Peter Tino, Ales Leonardis, and Ke Tang. 2021. A survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence, 5(5):726-742.</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024. A survey on the memory mechanism of large language model based agents. Preprint, arXiv:2404.13501.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. Preprint, arXiv:2303.18223.</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory.</p>
<h2>A Statistics and Details of Datasets</h2>
<p>In our experiments, we selected 14 datasets across three different task categories. These tasks share the common requirement that the model must engage in reasoning and analysis before generating answers. Detailed statistics for each dataset, including the type of answers, the number of evaluation samples, the number of CoT prompting (Wei et al., 2022b) demonstrations used, and the corresponding licenses, are provided in Table 2.</p>
<h2>B Implementation Details</h2>
<p>Baseline Implementation. In our main experiments, we compare $\mathrm{EM}^{2}$ against several baseline methods: ZS-CoT (Kojima et al., 2022), CoT (Wei et al., 2022b), and ComplexCoT (Fu et al., 2023). For ZS-CoT, the phrase "Let's think step by step" is appended to each question to activate the model's reasoning process, a method also adopted for $\mathrm{EM}^{2}$ in Table 1. For CoT and ComplexCoT, we used the official prompts. The prompts used for CoT also serve as the memory initialization for $\mathrm{EM}^{2 *}$,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">DATASET</th>
<th style="text-align: center;">TASK</th>
<th style="text-align: center;">ANSWER FORMAT</th>
<th style="text-align: center;"># EX.</th>
<th style="text-align: center;"># EVAL.</th>
<th style="text-align: center;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,319</td>
<td style="text-align: center;">MIT License</td>
</tr>
<tr>
<td style="text-align: center;">MultiArith (Roy and Roth, 2015)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: center;">SingleEq (Koncel-Kedziorski et al., 2016)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: center;">AddSub (Hosseini et al., 2014)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP (Patel et al., 2021)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">MIT License</td>
</tr>
<tr>
<td style="text-align: center;">AQUA (Ling et al., 2017)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: center;">MATH (Hendrycks et al., 2021)</td>
<td style="text-align: center;">WMP</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5,000</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA (Geva et al., 2021)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">T/F</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">CommonsenseQA (Talmor et al., 2019)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1,221</td>
<td style="text-align: center;">Unspecified</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ (Clark et al., 2019)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">T/F</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3,270</td>
<td style="text-align: center;">CC BY-SA 3.0</td>
</tr>
<tr>
<td style="text-align: center;">ARC-c (Clark et al., 2018)</td>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">299</td>
<td style="text-align: center;">CC BY-SA 4.0</td>
</tr>
<tr>
<td style="text-align: center;">Date Understanding (Suzgun et al., 2023)</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">Penguins in a Table (Suzgun et al., 2023)</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">146</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">Colored Objects (Suzgun et al., 2023)</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">MIT license</td>
</tr>
<tr>
<td style="text-align: center;">Object Counting (Suzgun et al., 2023)</td>
<td style="text-align: center;">Symbolic</td>
<td style="text-align: center;">Multi-choice</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">MIT license</td>
</tr>
</tbody>
</table>
<p>Table 2: Detailed statistics of the datasets utilized in our experiments. # EX. indicates the number of CoT prompting demonstrations used from each dataset. # EVAL. denotes the total number of evaluation samples in each dataset. The datasets are categorized by task type: WMP (Word Math Problem), Commonsense QA, and Symbolic Understanding, as discussed in Section 5.1.
with the number of prompts per dataset detailed in Table 2.</p>
<p>For multiple inference setting, we employ the Self-Consistency method (Wang et al., 2023c) to select the final answer. For MoT (Li and Qiu, 2023) and AutoCoT (Zhang et al., 2023), we replicated results on LLaMA-3 (Dubey et al., 2024) using the official implementation provided by the original authors.</p>
<p>Generation Setting. During our experiments, we obverse that different tasks and LLMs required specific temperature settings to achieve optimal performance. For the LLaMA-3-8B model, ZSCoT perform better with greedy decoding, while CoT necessitated a higher temperature, typically around 0.5 , for best results. For larger models, such as LLaMA-3-70B, setting the temperature to approximately 0.7 was found to be more suitable to foster superior outputs.</p>
<p>For multiple sampling settings, we established the number of samplings at five. We set the memory capacity at 20 . To construct a representative validation set, we use the same number of clusters as in AutoCoT (Zhang et al., 2023). Specifically, we select ten samples from each cluster. Clustering ensures the diversity of selected samples while reducing the computational overhead for each update. Initially, when the number of samples is less than 50 , we select all samples not already in memory to serve as the validation set. The clustering is performed using the KMeans algorithm with the number of clusters set to eight. We set the threshold $\epsilon$ in Eq 8 to 9 . We utilize GitHub Copilot for assist-
ing in the code writing process. Further details and ablation analysis can be found in Section C.</p>
<h2>C Further Analysis</h2>
<p>In this section, we delve into the impact of various hyperparameters on the performance of our algorithm. Additionally, we expand our analysis to include a broader range of clustering algorithms and embedding models to provide a comprehensive understanding of how these factors influence the effectiveness of our approach. All analyses are conducted using the LLaMA-3-8B (Dubey et al., 2024).</p>
<p>Memory Size. In Figure 11, we assess the impact of varying memory sizes on both performance and computation time, using datasets from three different tasks. The experimental results indicate that increasing memory size contributes to improved performance; however, the marginal gains decrease as the memory size continues to expand. Concurrently, there is a significant increase in computational overhead, as evidenced by the increase in processing time measured on a single RTX 4090. The results, displayed in the bar graph within the figure, clearly show that larger memory sizes substantially extend run times. Considering the costs associated with memory retrieval and updates, choosing an appropriate memory size is crucial. Therefore, we set an upper limit of 20 for memory size to balance performance and computational efficiency.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">SingleEq</th>
<th style="text-align: center;">AddSub</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{EM}^{2}$</td>
<td style="text-align: center;">82.63</td>
<td style="text-align: center;">97.77</td>
<td style="text-align: center;">92.71</td>
<td style="text-align: center;">86.32</td>
<td style="text-align: center;">83.91</td>
<td style="text-align: center;">45.27</td>
<td style="text-align: center;">81.43</td>
</tr>
<tr>
<td style="text-align: left;">Cluster Algorithm</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DBSCAN</td>
<td style="text-align: center;">83.47</td>
<td style="text-align: center;">96.50</td>
<td style="text-align: center;">93.50</td>
<td style="text-align: center;">85.82</td>
<td style="text-align: center;">83.45</td>
<td style="text-align: center;">44.09</td>
<td style="text-align: center;">81.13</td>
</tr>
<tr>
<td style="text-align: left;">Embedding Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sentence Bert</td>
<td style="text-align: center;">81.65</td>
<td style="text-align: center;">94.67</td>
<td style="text-align: center;">91.73</td>
<td style="text-align: center;">84.81</td>
<td style="text-align: center;">82.62</td>
<td style="text-align: center;">46.85</td>
<td style="text-align: center;">80.38</td>
</tr>
<tr>
<td style="text-align: left;">Ada-002</td>
<td style="text-align: center;">82.78</td>
<td style="text-align: center;">94.33</td>
<td style="text-align: center;">92.32</td>
<td style="text-align: center;">88.86</td>
<td style="text-align: center;">83.70</td>
<td style="text-align: center;">45.66</td>
<td style="text-align: center;">81.27</td>
</tr>
<tr>
<td style="text-align: left;">Update Mechanism</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">76.42</td>
<td style="text-align: center;">93.00</td>
<td style="text-align: center;">83.85</td>
<td style="text-align: center;">84.81</td>
<td style="text-align: center;">79.25</td>
<td style="text-align: center;">40.16</td>
<td style="text-align: center;">76.25</td>
</tr>
<tr>
<td style="text-align: left;">FIFO</td>
<td style="text-align: center;">74.37</td>
<td style="text-align: center;">91.83</td>
<td style="text-align: center;">85.23</td>
<td style="text-align: center;">85.06</td>
<td style="text-align: center;">80.09</td>
<td style="text-align: center;">39.37</td>
<td style="text-align: center;">76.00</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation analysis on six word math problem datasets. We evaluate the impact of different clustering algorithms, embedding models, and updating mechanisms on performance. "Ada-002" refers to the "text-embedding-ada-002" model.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Impact of memory size on performance and running time. The bar graph represents running time, while the line graph indicates accuracy.</p>
<p>Validation Set Size. In Figure 12, we examine the effects of validation set size on both performance and computation time, employing the same evaluation metrics used for memory size assessment. Our analysis across representative datasets such as GSM8K, ARC, and Date Understanding shows that increasing the size of the validation set can lead to performance improvements. However, these improvements are not substantial; for instance, on the GSM8K dataset, increasing the number of validation samples beyond 80 does not yield significant performance gains. Similarly to the increase in memory size, a larger validation set also leads to longer run times, although not as dramatically. Considering the trade-offs between performance gains and computational costs, it is crucial to select an appropriate validation set size. Therefore, we set the upper limit for validation samples to ten times the number of classes to maintain a balance between effectiveness and efficiency.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Impact of the number of validation set samples. The bar graph illustrates running time, while the line graph shows accuracy.</p>
<p>Cluster Algorithm and Embedding Models. In Table 3, we assess the impact of different clustering algorithms and embedding models on model performance. Our experiments conducted across six math word problem datasets demonstrate that $\mathrm{EM}^{2}$ is robust to the choice of clustering algorithm and embedding models. Specifically, when replacing the KMeans clustering algorithm with DBSCAN, using the default settings of DBSCAN, we observe no significant changes in performance across the datasets. Similarly, substituting text-embedding-3-large with Sentence-BERT (Reimers and Gurevych, 2019) or text-embedding-ada-002 dose not result in any noticeable performance degradation across the datasets. Interestingly, text-embedding-ada-002 even shows a slight average performance improvement over text-embedding-3-large. This phenomenon suggests that the choice of clustering algorithm and embedding models primarily influences the construction of the representative valida-</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Comparison of the $\mathrm{EM}^{2}$ method with Retrieval Memory on (a) commonsense question answering and (b) symbolic understanding tasks.
tion set and does not severely impact the memory updating mechanism of $\mathrm{EM}^{2}$.</p>
<p>Analysis of Memory Updating Mechanism. In Section 5.4, we analyze the impact of altering the memory updating mechanism to Random and FIFO (First-In-First-Out) on the MATH dataset. The results presented in Table 3 demonstrate that similar significant performance declines occur on other math word problem datasets when employing Random and FIFO updating mechanisms. This underscores the importance of designing effective memory updating strategies.</p>
<p>Comparison of Memory Retrieval Method. In Figure 13, we extend our comparison of $\mathrm{EM}^{2}$ with the Memory Retrieval Method to additional tasks. Maintaining the same experimental settings as in Section 5.4, we conducted experiments on Commonsense QA and Symbolic Understanding tasks. The results indicate that $\mathrm{EM}^{2}$ demonstrates a clear advantage on the majority of the datasets, showing an average improvement of $2.82 \%$ over AutoCoT. This highlights the effectiveness of the dynamic memory updating strategy of $\mathrm{EM}^{2}$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>https://openai.com/index/new-embedding-models-and-api-updates&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
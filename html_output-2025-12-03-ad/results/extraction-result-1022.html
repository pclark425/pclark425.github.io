<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1022 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1022</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1022</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-203836976</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1910.02826v1.pdf" target="_blank">Self-Paced Contextual Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Generalization and adaptation of learned skills to novel situations is a core requirement for intelligent autonomous robots. Although contextual reinforcement learning provides a principled framework for learning and generalization of behaviors across related tasks, it generally relies on uninformed sampling of environments from an unknown, uncontrolled context distribution, thus missing the benefits of structured, sequential learning. We introduce a novel relative entropy reinforcement learning algorithm that gives the agent the freedom to control the intermediate task distribution, allowing for its gradual progression towards the target context distribution. Empirical evaluation shows that the proposed curriculum learning scheme drastically improves sample efficiency and enables learning in scenarios with both broad and sharp target context distributions in which classical approaches perform sub-optimally.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1022.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1022.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gate-PointMass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-mass agent in Gate environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 2D point-mass controlled by two PD-controllers whose parameters are optimized by contextual policy-search algorithms; tasks vary by gate x-position and gate width, producing environments of differing difficulty (precision vs global).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Point-mass agent (PD controller parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied simulated agent (point-mass) using contextual policy search; learning algorithm evaluated is Self-Paced Reinforcement Learning (SPRL) and compared to C-REPS, CMA-ES, GoalGAN+C-REPS, SAGG-RIAC+C-REPS. Policy parameterization: Gaussian conditional π(θ|c) with linear features; θ is 14-dimensional (PD gains/offsets).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gate environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D point-mass must steer from start [0,5] to origin while traversing a wall at y=2.5 only through a gate. Task/context c = (gate x-position, gate width). Crashing into wall stops episode. Two target-distribution settings: 'precision' (very small gate far from origin — narrow target distribution) and 'global' (variety of positions and widths — broad target distribution). Reward = exponential of distance to goal minus L2 regularization on actions; episodes limited by crashes.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by gate geometry: gate width (smaller → higher task difficulty) and gate x-position (distance from origin affects control difficulty); measured qualitatively as 'precision' (narrow width) vs 'global' (varied widths/positions). Also state/control dimensionality: 2D position with 14-D policy parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-high depending on gate width; 'precision' setting = high complexity, 'global' setting = medium complexity due to wider range of tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Context distribution variation measured by target context distribution spread (narrow vs broad target µ(c)); sampling/intermediate distribution μ(c) variance and KL divergence to target µ(c) used to quantify variation and progression.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>precision setting = low variation (narrow target distribution); global setting = high variation (broad target distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success indicator (distance to goal < threshold τ = 0.05) and episodic reward; reported as success rate quantiles and reward quantiles across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SPRL shows significantly faster convergence and higher success rates on the 'global' setting and avoids a local optimum in the 'precision' (narrow) setting that C-REPS and CMA-ES often converge to. Exact numeric success rates not provided in text (plotted in Figures 1 and 3). One iteration = 100 rollouts (used for sample-efficiency context).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — paper explicitly discusses the trade-off between optimizing expected reward on current tasks and minimizing distance (KL divergence) to the target context distribution. SPRL controls an intermediate context distribution μ(c) to progress from easier to harder tasks; narrow (sharp) target distributions (low variation) can create local optima that uninformed sampling suffers from, while broad target distributions (high variation) require sample-efficient coverage — SPRL's curriculum (via α schedule) improves performance in both regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Tested: 'precision' setting (high complexity, low variation). SPRL outperforms C-REPS and CMA-ES by avoiding a sub-optimal local optimum and achieving higher success rates (qualitative; plotted results show marked improvement). Exact numerical values not reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Tested: 'global' setting (lower per-task difficulty for many tasks, high variation). SPRL converges significantly faster to the optimum than baselines; yields higher reward/success quantiles (qualitative). Exact numeric values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Not separately reported beyond trivial cases; SPRL initially focuses on easier (low complexity) contexts and then progresses to target distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning / self-paced contextual policy search: SPRL jointly optimizes policy π(θ|c) and intermediate sampling distribution μ(c) with a KL constraint to previous joint distribution and a penalty α · D_KL(μ||µ) to control progression to target µ(c).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agents are evaluated on target context distributions (generalization across contexts). SPRL produces policies that generalize across target contexts better than baselines, especially avoiding poor local optima in narrow-target (precision) tasks and converging faster on varied (global) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>One iteration = 100 policy rollouts; experiments used many iterations and an experience buffer. SPRL demonstrates improved sample efficiency relative to C-REPS, CMA-ES and other curricula (as shown by faster rise in success/reward quantiles), but absolute numbers of iterations to threshold are shown only in plots, not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Controlling sampling distribution μ(c) and progressing from easy to hard tasks (self-paced curriculum) improves sample efficiency and helps avoid local optima in narrow/precise task distributions; narrow target distributions are particularly prone to local optima under uninformed sampling, which SPRL mitigates by starting on easier sub-regions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Paced Contextual Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1022.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1022.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reacher-Obstacles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reacher agent with varying obstacle sizes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified 3D Reacher (OpenAI Gym) in which the end-effector must reach a goal on a table while avoiding obstacles whose sizes form the context; obstacle size variation controls task difficulty and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reacher agent (ProMP policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated robotic arm end-effector control using Probabilistic Movement Primitives (ProMPs) parameterized by θ (40-D). Learning via SPRL (self-paced contextual policy search) and compared to C-REPS, CMA-ES, GoalGAN, SAGG-RIAC and PPO (step-based). Policy π(θ|c) is Gaussian with linear context features; value function approximated with RBFs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robot arm end-effector)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Modified Reacher environment with obstacles</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Goal: move end-effector along a table surface to goal while avoiding four obstacles placed on table; two obstacles' sizes vary and constitute the task/context c. Larger obstacles require more pronounced curved movements to avoid collisions. Touching an obstacle terminates episode. Reward is exponential of distance to goal minus action regularization; success threshold τ = 0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Task difficulty measured by obstacle sizes (larger → higher complexity) and number of obstacles/need for curved trajectories; policy dimensionality (θ is 40-D) increases control complexity. Also dynamics/noise and collision termination add episodic difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high complexity depending on obstacle sizes; experiments include ranges from small obstacles (easier) to large obstacles (harder).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Environment variation measured by distribution over obstacle sizes (contexts); target context distribution µ(c) specifies range of obstacle sizes (broad vs narrow). Also quantified in experiments by KL divergence between μ and µ and by variance lower-bounding until close to target (KL < 20).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (varied obstacle sizes across contexts); SPRL initially focuses on small obstacles (low variation/easier) then moves toward larger obstacles (higher variation/ harder).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episodic reward and success rate (distance threshold τ = 0.05); reward quantiles over 40 runs are reported, and trajectories visualized to assess obstacle avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SPRL attains better final reward and avoids the worse local optimum that C-REPS and CMA-ES converge to; GoalGAN and SAGG-RIAC find similar optima to SPRL but with slower convergence and more variance; PPO failed to solve the task within equivalent interactions. Exact numeric rewards/success rates are shown in figures but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — SPRL demonstrates that starting learning on easier contexts (small obstacles) then increasing difficulty leads to better final policies; high obstacle complexity together with high variation requires incremental curriculum to avoid local optima and improve convergence. The paper explicitly notes initial lower performance on target tasks while curriculum is focusing on easier tasks before improving generalization to hard contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Partially tested: SPRL first trains on low-complexity contexts (small obstacles) even when target distribution includes high variation; eventual performance on broad/higher-variation target is superior to baselines (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Tested: when obstacle sizes are large and varied, SPRL outperforms C-REPS and CMA-ES, achieving higher reward and proper obstacle-avoiding trajectories; PPO failed under same interaction budget.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Easier contexts (small obstacles, low variation) are learned quickly by all algorithms; SPRL uses them as starting points for curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Self-paced curriculum where μ(c) is adapted to focus on easier obstacle sizes first, then progress toward target obstacle-size distribution; α schedule and KL constraints guide progression.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policies generalized across target contexts: SPRL's final policies reached the goal while avoiding obstacles across the target distribution better than baselines (visualized trajectories show successful avoidance whereas some baselines converge to suboptimal behaviors).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>One iteration = 50 rollouts; experiments used fewer samples per iteration than Gate environment (50) with θ dimensionality 40, demonstrating improved sample efficiency of SPRL relative to baselines (convergence occurs in fewer iterations in plots).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum that increases obstacle size gradually improves convergence and final performance; SPRL avoids suboptimal local optima that occur under uninformed sampling methods and is more sample-efficient than C-REPS, CMA-ES, and PPO in this high-dimensional control problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Paced Contextual Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1022.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1022.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ball-in-Cup-WAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Barrett WAM Ball-in-a-Cup (simulated and real)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated Barrett WAM arm uses ProMP policies to learn a sparse-reward Ball-in-a-Cup task; SPRL controls an environment parameter (cup diameter / scale) to create a curriculum from easier (larger cup) to harder (smaller cup) tasks improving learning under sparse rewards and low sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Barrett WAM robot (ProMP policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied physical robot (simulated in MuJoCo then deployed with fine-tuning on real hardware) using Probabilistic Movement Primitives (ProMPs) with θ (15-D in final encoding) learned via SPRL and compared to C-REPS, CMA-ES, GoalGAN, SAGG-RIAC. Task has extremely sparse reward (1 minus L2 on policy params if ball in cup at end, else 0).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent with real-robot transfer (physical robot fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ball-in-a-Cup (Barrett WAM)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Robotic manipulation task where the robot must swing and catch a ball in a cup; reward is sparse (1 if ball in cup at end minus small regularization, 0 otherwise). The experiment allows SPRL to adjust cup diameter/scale (context c) so learning can start with larger cups (easier) and progress to smaller cups (harder). Simulation used MuJoCo; successful simulated behaviors were transferred to the real robot with small manual adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Complexity arises from task sparsity (rare successes) and control precision needed for catching; measured by cup diameter/scale (smaller diameter → higher precision and complexity) and by successful catch probability under the current policy. Policy dimensionality ~15. Also limited sample budget (16 rollouts per iteration) increases effective difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high complexity when cup diameter is small (high precision required); lower complexity for larger cups.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation characterized by the distribution over cup scales/diameters (target µ(c) vs intermediate μ(c)); sample sizes per iteration (16) and whether the cup scale is varied constitute variation levels. Also experimental setup includes limited sample budget increasing effective variation challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>SPRL experiments used progression from high-variation sampling early (large range allowed) to focused low-variation sampling on target; target distribution can be narrow (small cup) representing low variation at high difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (binary: ball in cup within threshold) and final policy reward; success rate quantiles plotted (left plots of Figure 5) and reported across multiple runs (best 10 of 20 used for plotting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: SPRL's final policies clearly outperform C-REPS, CMA-ES, GoalGAN and SAGG-RIAC in success rate under sparse rewards, despite only 16 rollouts per iteration; exact numeric success rates are shown in figure but not enumerated in text. Successful simulated policies were applied to the real robot after small fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — paper highlights that extreme sparsity (high complexity) makes direct learning unlikely; by varying an environment parameter (cup diameter) SPRL creates a curriculum: starting with low-precision, high-variation (large cup) tasks then progressing to precise, low-variation (small cup) target tasks, improving the probability of observing informative (non-zero) rewards and thus sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Tested: small cup target (high complexity, low variation). SPRL achieves better final success rates than baselines by progressively training with larger cups first; exact numeric rates not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Tested implicitly: training starts on larger cups (low per-task difficulty) which are easier (low complexity) but may be sampled across various scales (higher variation); SPRL leverages these to bootstrap learning; performance on these easier contexts is high for all algorithms but only SPRL uses them to build towards harder targets.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Not a focus; trivial large-cup single-context training would be easy for all methods.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Self-paced curriculum where SPRL controls cup diameter context μ(c) to begin with larger cups and progressively reduce diameter toward target µ(c); score-weighted sample updates with KL constraints and α schedule. Experience buffer used; small sample budget (16 rollouts/iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>SPRL-trained policies generalized from simulated varying cup sizes to the small-cup target contexts and were successfully transferred to the real Barrett WAM with minor manual adjustments; baselines produced lower success rates and less robust final policies.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very low sample regime: 16 rollouts per iteration; SPRL was able to learn useful policies under this constraint whereas baselines often failed or learned inferior policies. Exact number of iterations until success is presented in plot form but not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Controlling environment variation (cup diameter) as part of learning curriculum makes otherwise intractable sparse-reward tasks learnable under severe sample constraints; SPRL markedly improves success rates and enables transfer to a real robot with minimal fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Paced Contextual Reinforcement Learning', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Intrinsically motivated goal exploration for active motor learning in robots: A case study <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Active contextual policy search <em>(Rating: 1)</em></li>
                <li>Data-efficient generalization of robot skills with contextual policy search <em>(Rating: 2)</em></li>
                <li>Self-paced curriculum learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1022",
    "paper_id": "paper-203836976",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Gate-PointMass",
            "name_full": "Point-mass agent in Gate environment",
            "brief_description": "A simulated 2D point-mass controlled by two PD-controllers whose parameters are optimized by contextual policy-search algorithms; tasks vary by gate x-position and gate width, producing environments of differing difficulty (precision vs global).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Point-mass agent (PD controller parameters)",
            "agent_description": "Embodied simulated agent (point-mass) using contextual policy search; learning algorithm evaluated is Self-Paced Reinforcement Learning (SPRL) and compared to C-REPS, CMA-ES, GoalGAN+C-REPS, SAGG-RIAC+C-REPS. Policy parameterization: Gaussian conditional π(θ|c) with linear features; θ is 14-dimensional (PD gains/offsets).",
            "agent_type": "simulated agent",
            "environment_name": "Gate environment",
            "environment_description": "2D point-mass must steer from start [0,5] to origin while traversing a wall at y=2.5 only through a gate. Task/context c = (gate x-position, gate width). Crashing into wall stops episode. Two target-distribution settings: 'precision' (very small gate far from origin — narrow target distribution) and 'global' (variety of positions and widths — broad target distribution). Reward = exponential of distance to goal minus L2 regularization on actions; episodes limited by crashes.",
            "complexity_measure": "Characterized by gate geometry: gate width (smaller → higher task difficulty) and gate x-position (distance from origin affects control difficulty); measured qualitatively as 'precision' (narrow width) vs 'global' (varied widths/positions). Also state/control dimensionality: 2D position with 14-D policy parameters.",
            "complexity_level": "low-to-high depending on gate width; 'precision' setting = high complexity, 'global' setting = medium complexity due to wider range of tasks",
            "variation_measure": "Context distribution variation measured by target context distribution spread (narrow vs broad target µ(c)); sampling/intermediate distribution μ(c) variance and KL divergence to target µ(c) used to quantify variation and progression.",
            "variation_level": "precision setting = low variation (narrow target distribution); global setting = high variation (broad target distribution)",
            "performance_metric": "Success indicator (distance to goal &lt; threshold τ = 0.05) and episodic reward; reported as success rate quantiles and reward quantiles across runs.",
            "performance_value": "Qualitative: SPRL shows significantly faster convergence and higher success rates on the 'global' setting and avoids a local optimum in the 'precision' (narrow) setting that C-REPS and CMA-ES often converge to. Exact numeric success rates not provided in text (plotted in Figures 1 and 3). One iteration = 100 rollouts (used for sample-efficiency context).",
            "complexity_variation_relationship": "Yes — paper explicitly discusses the trade-off between optimizing expected reward on current tasks and minimizing distance (KL divergence) to the target context distribution. SPRL controls an intermediate context distribution μ(c) to progress from easier to harder tasks; narrow (sharp) target distributions (low variation) can create local optima that uninformed sampling suffers from, while broad target distributions (high variation) require sample-efficient coverage — SPRL's curriculum (via α schedule) improves performance in both regimes.",
            "high_complexity_low_variation_performance": "Tested: 'precision' setting (high complexity, low variation). SPRL outperforms C-REPS and CMA-ES by avoiding a sub-optimal local optimum and achieving higher success rates (qualitative; plotted results show marked improvement). Exact numerical values not reported in text.",
            "low_complexity_high_variation_performance": "Tested: 'global' setting (lower per-task difficulty for many tasks, high variation). SPRL converges significantly faster to the optimum than baselines; yields higher reward/success quantiles (qualitative). Exact numeric values not provided.",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "Not separately reported beyond trivial cases; SPRL initially focuses on easier (low complexity) contexts and then progresses to target distribution.",
            "training_strategy": "Curriculum learning / self-paced contextual policy search: SPRL jointly optimizes policy π(θ|c) and intermediate sampling distribution μ(c) with a KL constraint to previous joint distribution and a penalty α · D_KL(μ||µ) to control progression to target µ(c).",
            "generalization_tested": true,
            "generalization_results": "Agents are evaluated on target context distributions (generalization across contexts). SPRL produces policies that generalize across target contexts better than baselines, especially avoiding poor local optima in narrow-target (precision) tasks and converging faster on varied (global) tasks.",
            "sample_efficiency": "One iteration = 100 policy rollouts; experiments used many iterations and an experience buffer. SPRL demonstrates improved sample efficiency relative to C-REPS, CMA-ES and other curricula (as shown by faster rise in success/reward quantiles), but absolute numbers of iterations to threshold are shown only in plots, not tabulated.",
            "key_findings": "Controlling sampling distribution μ(c) and progressing from easy to hard tasks (self-paced curriculum) improves sample efficiency and helps avoid local optima in narrow/precise task distributions; narrow target distributions are particularly prone to local optima under uninformed sampling, which SPRL mitigates by starting on easier sub-regions.",
            "uuid": "e1022.0",
            "source_info": {
                "paper_title": "Self-Paced Contextual Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Reacher-Obstacles",
            "name_full": "Reacher agent with varying obstacle sizes",
            "brief_description": "A modified 3D Reacher (OpenAI Gym) in which the end-effector must reach a goal on a table while avoiding obstacles whose sizes form the context; obstacle size variation controls task difficulty and variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reacher agent (ProMP policy)",
            "agent_description": "Simulated robotic arm end-effector control using Probabilistic Movement Primitives (ProMPs) parameterized by θ (40-D). Learning via SPRL (self-paced contextual policy search) and compared to C-REPS, CMA-ES, GoalGAN, SAGG-RIAC and PPO (step-based). Policy π(θ|c) is Gaussian with linear context features; value function approximated with RBFs.",
            "agent_type": "simulated agent (robot arm end-effector)",
            "environment_name": "Modified Reacher environment with obstacles",
            "environment_description": "Goal: move end-effector along a table surface to goal while avoiding four obstacles placed on table; two obstacles' sizes vary and constitute the task/context c. Larger obstacles require more pronounced curved movements to avoid collisions. Touching an obstacle terminates episode. Reward is exponential of distance to goal minus action regularization; success threshold τ = 0.05.",
            "complexity_measure": "Task difficulty measured by obstacle sizes (larger → higher complexity) and number of obstacles/need for curved trajectories; policy dimensionality (θ is 40-D) increases control complexity. Also dynamics/noise and collision termination add episodic difficulty.",
            "complexity_level": "medium-to-high complexity depending on obstacle sizes; experiments include ranges from small obstacles (easier) to large obstacles (harder).",
            "variation_measure": "Environment variation measured by distribution over obstacle sizes (contexts); target context distribution µ(c) specifies range of obstacle sizes (broad vs narrow). Also quantified in experiments by KL divergence between μ and µ and by variance lower-bounding until close to target (KL &lt; 20).",
            "variation_level": "medium-to-high (varied obstacle sizes across contexts); SPRL initially focuses on small obstacles (low variation/easier) then moves toward larger obstacles (higher variation/ harder).",
            "performance_metric": "Episodic reward and success rate (distance threshold τ = 0.05); reward quantiles over 40 runs are reported, and trajectories visualized to assess obstacle avoidance.",
            "performance_value": "Qualitative: SPRL attains better final reward and avoids the worse local optimum that C-REPS and CMA-ES converge to; GoalGAN and SAGG-RIAC find similar optima to SPRL but with slower convergence and more variance; PPO failed to solve the task within equivalent interactions. Exact numeric rewards/success rates are shown in figures but not tabulated in text.",
            "complexity_variation_relationship": "Yes — SPRL demonstrates that starting learning on easier contexts (small obstacles) then increasing difficulty leads to better final policies; high obstacle complexity together with high variation requires incremental curriculum to avoid local optima and improve convergence. The paper explicitly notes initial lower performance on target tasks while curriculum is focusing on easier tasks before improving generalization to hard contexts.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "Partially tested: SPRL first trains on low-complexity contexts (small obstacles) even when target distribution includes high variation; eventual performance on broad/higher-variation target is superior to baselines (qualitative).",
            "high_complexity_high_variation_performance": "Tested: when obstacle sizes are large and varied, SPRL outperforms C-REPS and CMA-ES, achieving higher reward and proper obstacle-avoiding trajectories; PPO failed under same interaction budget.",
            "low_complexity_low_variation_performance": "Easier contexts (small obstacles, low variation) are learned quickly by all algorithms; SPRL uses them as starting points for curriculum.",
            "training_strategy": "Self-paced curriculum where μ(c) is adapted to focus on easier obstacle sizes first, then progress toward target obstacle-size distribution; α schedule and KL constraints guide progression.",
            "generalization_tested": true,
            "generalization_results": "Policies generalized across target contexts: SPRL's final policies reached the goal while avoiding obstacles across the target distribution better than baselines (visualized trajectories show successful avoidance whereas some baselines converge to suboptimal behaviors).",
            "sample_efficiency": "One iteration = 50 rollouts; experiments used fewer samples per iteration than Gate environment (50) with θ dimensionality 40, demonstrating improved sample efficiency of SPRL relative to baselines (convergence occurs in fewer iterations in plots).",
            "key_findings": "Curriculum that increases obstacle size gradually improves convergence and final performance; SPRL avoids suboptimal local optima that occur under uninformed sampling methods and is more sample-efficient than C-REPS, CMA-ES, and PPO in this high-dimensional control problem.",
            "uuid": "e1022.1",
            "source_info": {
                "paper_title": "Self-Paced Contextual Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Ball-in-Cup-WAM",
            "name_full": "Barrett WAM Ball-in-a-Cup (simulated and real)",
            "brief_description": "A simulated Barrett WAM arm uses ProMP policies to learn a sparse-reward Ball-in-a-Cup task; SPRL controls an environment parameter (cup diameter / scale) to create a curriculum from easier (larger cup) to harder (smaller cup) tasks improving learning under sparse rewards and low sample counts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Barrett WAM robot (ProMP policy)",
            "agent_description": "Embodied physical robot (simulated in MuJoCo then deployed with fine-tuning on real hardware) using Probabilistic Movement Primitives (ProMPs) with θ (15-D in final encoding) learned via SPRL and compared to C-REPS, CMA-ES, GoalGAN, SAGG-RIAC. Task has extremely sparse reward (1 minus L2 on policy params if ball in cup at end, else 0).",
            "agent_type": "simulated agent with real-robot transfer (physical robot fine-tuning)",
            "environment_name": "Ball-in-a-Cup (Barrett WAM)",
            "environment_description": "Robotic manipulation task where the robot must swing and catch a ball in a cup; reward is sparse (1 if ball in cup at end minus small regularization, 0 otherwise). The experiment allows SPRL to adjust cup diameter/scale (context c) so learning can start with larger cups (easier) and progress to smaller cups (harder). Simulation used MuJoCo; successful simulated behaviors were transferred to the real robot with small manual adjustments.",
            "complexity_measure": "Complexity arises from task sparsity (rare successes) and control precision needed for catching; measured by cup diameter/scale (smaller diameter → higher precision and complexity) and by successful catch probability under the current policy. Policy dimensionality ~15. Also limited sample budget (16 rollouts per iteration) increases effective difficulty.",
            "complexity_level": "high complexity when cup diameter is small (high precision required); lower complexity for larger cups.",
            "variation_measure": "Variation characterized by the distribution over cup scales/diameters (target µ(c) vs intermediate μ(c)); sample sizes per iteration (16) and whether the cup scale is varied constitute variation levels. Also experimental setup includes limited sample budget increasing effective variation challenge.",
            "variation_level": "SPRL experiments used progression from high-variation sampling early (large range allowed) to focused low-variation sampling on target; target distribution can be narrow (small cup) representing low variation at high difficulty.",
            "performance_metric": "Success rate (binary: ball in cup within threshold) and final policy reward; success rate quantiles plotted (left plots of Figure 5) and reported across multiple runs (best 10 of 20 used for plotting).",
            "performance_value": "Qualitative: SPRL's final policies clearly outperform C-REPS, CMA-ES, GoalGAN and SAGG-RIAC in success rate under sparse rewards, despite only 16 rollouts per iteration; exact numeric success rates are shown in figure but not enumerated in text. Successful simulated policies were applied to the real robot after small fine-tuning.",
            "complexity_variation_relationship": "Yes — paper highlights that extreme sparsity (high complexity) makes direct learning unlikely; by varying an environment parameter (cup diameter) SPRL creates a curriculum: starting with low-precision, high-variation (large cup) tasks then progressing to precise, low-variation (small cup) target tasks, improving the probability of observing informative (non-zero) rewards and thus sample efficiency.",
            "high_complexity_low_variation_performance": "Tested: small cup target (high complexity, low variation). SPRL achieves better final success rates than baselines by progressively training with larger cups first; exact numeric rates not provided in text.",
            "low_complexity_high_variation_performance": "Tested implicitly: training starts on larger cups (low per-task difficulty) which are easier (low complexity) but may be sampled across various scales (higher variation); SPRL leverages these to bootstrap learning; performance on these easier contexts is high for all algorithms but only SPRL uses them to build towards harder targets.",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "Not a focus; trivial large-cup single-context training would be easy for all methods.",
            "training_strategy": "Self-paced curriculum where SPRL controls cup diameter context μ(c) to begin with larger cups and progressively reduce diameter toward target µ(c); score-weighted sample updates with KL constraints and α schedule. Experience buffer used; small sample budget (16 rollouts/iteration).",
            "generalization_tested": true,
            "generalization_results": "SPRL-trained policies generalized from simulated varying cup sizes to the small-cup target contexts and were successfully transferred to the real Barrett WAM with minor manual adjustments; baselines produced lower success rates and less robust final policies.",
            "sample_efficiency": "Very low sample regime: 16 rollouts per iteration; SPRL was able to learn useful policies under this constraint whereas baselines often failed or learned inferior policies. Exact number of iterations until success is presented in plot form but not tabulated.",
            "key_findings": "Controlling environment variation (cup diameter) as part of learning curriculum makes otherwise intractable sparse-reward tasks learnable under severe sample constraints; SPRL markedly improves success rates and enables transfer to a real robot with minimal fine-tuning.",
            "uuid": "e1022.2",
            "source_info": {
                "paper_title": "Self-Paced Contextual Reinforcement Learning",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        },
        {
            "paper_title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study",
            "rating": 2,
            "sanitized_title": "intrinsically_motivated_goal_exploration_for_active_motor_learning_in_robots_a_case_study"
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2,
            "sanitized_title": "reverse_curriculum_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "Active contextual policy search",
            "rating": 1,
            "sanitized_title": "active_contextual_policy_search"
        },
        {
            "paper_title": "Data-efficient generalization of robot skills with contextual policy search",
            "rating": 2,
            "sanitized_title": "dataefficient_generalization_of_robot_skills_with_contextual_policy_search"
        },
        {
            "paper_title": "Self-paced curriculum learning",
            "rating": 1,
            "sanitized_title": "selfpaced_curriculum_learning"
        }
    ],
    "cost": 0.0127855,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Paced Contextual Reinforcement Learning
7 Oct 2019</p>
<p>Pascal Klink klink@ias.tu-darmstadt.de 
Intelligent Autonomous Systems
Technische Universität Darmstadt Germany</p>
<p>Hany Abdulsamad abdulsamad@ias.tu-darmstadt.de 
Intelligent Autonomous Systems
Technische Universität Darmstadt Germany</p>
<p>Boris Belousov belousov@ias.tu-darmstadt.de 
Intelligent Autonomous Systems
Technische Universität Darmstadt Germany</p>
<p>Jan Peters peters@ias.tu-darmstadt.de 
Intelligent Autonomous Systems
Technische Universität Darmstadt Germany</p>
<p>Self-Paced Contextual Reinforcement Learning
7 Oct 20192A5B17AB5B26352E052E8B1521AF820CarXiv:1910.02826v1[cs.LG]Reinforcement LearningCurriculum LearningRobotics
Generalization and adaptation of learned skills to novel situations is a core requirement for intelligent autonomous robots.Although contextual reinforcement learning provides a principled framework for learning and generalization of behaviors across related tasks, it generally relies on uninformed sampling of environments from an unknown, uncontrolled context distribution, thus missing the benefits of structured, sequential learning.We introduce a novel relative entropy reinforcement learning algorithm that gives the agent the freedom to control the intermediate task distribution, allowing for its gradual progression towards the target context distribution.Empirical evaluation shows that the proposed curriculum learning scheme drastically improves sample efficiency and enables learning in scenarios with both broad and sharp target context distributions in which classical approaches perform sub-optimally.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) techniques underpinned the recent successes in decision making [1,2] and robot learning [3,4].While most impressive results were achieved in the classical singletask RL setting with a static environment and a fixed reward function, contextual reinforcement learning holds the promise of driving the next wave of breakthroughs by leveraging similarities between environments and tasks [5,6,7].Such a high-level abstraction of shared task structure permits simultaneous multi-task policy optimization and generalization to novel scenarios through interpolation and extrapolation [8,9,10,11].</p>
<p>A crucial limitation of the contextual RL framework is the assumption that the task distribution is not known beforehand and not controlled by the agent.Despite being effective in the contextual bandit setting [12], where the context is chosen by an underlying unknown (adversarial) stochastic process, this assumption is rather limiting in robotics from a practical and an algorithmic point of view.On one hand, the target context distribution is usually known to the designer of the system, and contextual learning is purposefully applied to generalize a skill over a set of target contexts, be it the whole context space or a specific part of it [13].On the other hand, not exploiting the freedom in adjusting the context distribution to the skill level of the agent can only deteriorate the performance of the learning algorithm.Indeed, advances in gradual and sequential learning, exemplified by shaping [14] and curriculum learning [15], which are loosely based on the insights from behavioral psychology [16] and continuation methods in optimization [17], demonstrate significant improvements in the speed of learning and resilience to local optima.</p>
<p>Defining an appropriate curriculum is crucial for the success of gradual learning approaches.For contextual RL, this means repeatedly choosing a batch of tasks that result in the greatest improvement R(θ, c)p(θ, c)dcdθ, s.where D KL (p(.)||q(.)) is the Kullback-Leibler (KL) divergence between the distribution p being optimized and the previously found joint distribution q.This constraint controls the explorationexploitation trade-off via the hyperparameter and limits information loss between iterations.The remaining constraints are necessary to ensure distribution normalization and marginalization properties.This constrained optimization problem is tackled by formulating the Lagrangian function and solving the primal problem, which yields the following optimality condition for p(θ, c)
p * (θ, c) ∝ q(θ, c) exp A(θ, c) η ,(1)
which can be interpreted as soft-max re-weighting of the joint distribution q by the advantage function A (θ, c) = R(θ, c) − V (c).The parameters η and V (c) are the Lagrangian variables corresponding to the KL and marginalization constraints respectively.Those parameters can be optimized by minimizing the corresponding dual objective, which is derived by plugging the optimal distribution p * (θ, c) back into the primal problem,
G(η, V ) = η + η log E q exp A(θ, c) η .
Given that neither the sampling distribution q(θ, c) nor the advantage A (θ, c) are known analytically, the dual objective can only be approximated under samples from q(θ, c).By assuming a certain function class for the optimal policy π * (θ|c), for example a Gaussian process or an RBF network, the optimality condition in Equation 1 can be satisfied by performing a maximum a posteriori fit under samples drawn from q(θ|c) and µ(c).</p>
<p>Algorithm 1 Self-Paced Reinforcement Learning</p>
<p>Input: Relative entropy bound , offset K α (after which α becomes non-zero), KL-penalty fraction ζ, initial policy q 0 (θ|c), initial sampling distribution q 0 (c), number of iterations K. for k = 1 to K do Collect Data: Sample contexts: c i ∼ q k−1 (c), i = 1, . . ., M Sample and execute actions:
θ i ∼ q k−1 (θ|c i )
Observe reward: R i = R (θ i , c i ) Update Policy and Context Distributions:
Update schedule: α k = 0, if k ≤ K α , else ζ M i=1 Ri M DKL(q k−1 µ) Optimize dual function: η * µ , η * p , V * ← arg max G(η µ , η p , V ) Calculate sample weights: w π i , w μ i ← exp A(θi,ci) η * p , exp β(ci) α k +η * µ
Infer policy and context distributions:
[π * k , μ * k ] ← D = (w π i , w μ i , θ i , c i )|i ∈ [1, M ] Assign policy and context distributions: q k (θ|c) ← π * k (θ|c), q k (c) ← μ * k (c) end for 3 Self-Paced Reinforcement Learning
Following our line of argument in Section 1, we want to transform the contextual stochastic search problem presented in Section 2 from a passive optimization under an unknown distribution µ(c) into one that allows the learning agent to actively optimize an intermediate context distribution μ(c) and initially focus on "easy" tasks and gradually progress towards the target distribution µ(c) by bootstrapping the optimal policy from the previous iterations.</p>
<p>This progression is to be understood as a trade-off between maximizing the objective J = E π,μ [R(θ, c)] by exploiting local information and minimizing the distance to the context distribution of interest µ(c).A connection to numerical continuation methods [17] can be drawn by viewing the intermediate distribution μ(c) as a parameter of the main objective J(π, μ).Under this view, the distribution μ(c) gradually morphs the function J(π) to improve convergence when a good initial guess of π is not available.</p>
<p>In the following, we present our self-paced contextual RL (SPRL) algorithm, which incorporates such intermediate distributions into the framework of C-REPS and allows an agent to optimize its own learning process, under the assumption of a known target context distribution.</p>
<p>We formulate a new constrained optimization problem that takes into consideration a "local" objective of maximizing the expected reward and a "global" objective seeking to move μ(c) towards µ(c) by minimizing D KL (μ||µ), arg max
p,μ C,Θ R(θ, c)p(θ, c)dcdθ − αD KL (μ||µ)(2)s.t. D KL (p(θ, c)||q(θ, c)) ≤ ,(3)C,Θ p(θ, c)dcdθ = 1,(4)Θ p(θ, c)dθ = μ(c), ∀c ∈ C,(5)
where the joint distributions p(.) and q(.) are defined as p(θ, c) = π(θ|c)μ(c) and q(θ, c) = q(θ|c)q(c), respectively.It follows that in every iteration the learning agent is able to optimize both its policy π(θ|c) and sampling distribution μ(c), starting from the uninformative Gaussian priors q(c) and q(θ|c), while following a schedule of α.</p>
<p>Optimality Conditions and Dual Optimization</p>
<p>By constructing the Lagrangian function of the above described optimization problem, we can derive the optimality conditions for p * (θ, c) and μ * (c).However, during experiments we observed that the resulting expressions and the dual objective are numerically unstable for small values of α.A more detailed treatment of those problems can be found in the supplementary material.We can avoid such numerical issues by over-defining the original problem and adding the following conditions, which are implicitly satisfied when the constraints in Equations (3-5) hold,
C μ (c) dc = 1 (6), D KL (μ(c)||q(c)) ≤ .(7)
Solving the new augmented primal problem results in an optimality condition p * (θ, c) which is equivalent to that in Equation (1), while the optimal point μ * (c) is given by μ * (c) ∝ q(c) exp
    α log µ(c) q(c) + V (c) α + η µ     = q(c) exp β(c) α + η µ ,(8)
where η p , η µ and V (c) are the Lagrangian multipliers corresponding to Equations (3, 7, 5) respectively, and are optimized by minimizing the dual objective
G = (η p + η µ ) + η p log E q exp A(θ, c) η p + (α + η µ ) log E q exp β(c) α + η µ .(9)
Equation ( 9) underscores the contributions of Equations (6,7) to numerical robustness, as they introduce the logarithmic function and the temperature η µ to the second expectation, consequently reducing the severity of numerical overflow issues.Algorithm 1 depicts a general sketch of the overall learning procedure.</p>
<p>Interpretation of the Dual Variable V (c)</p>
<p>As already highlighted in [13], V (c) can be interpreted as a context-value function, representing a soft-max operator over the expected reward under the policy q(θ|c).For the case α = 0, we obtain
μ * (c) ∝ q(c) exp V (c) η µ ,
which reveals that μ * (c) shifts the contextual variable's modes of density from areas in the context space with lower expected reward to areas with higher expected reward according to V (c), while constraints (3) and ( 7) limit the rate of such shifts, in order to limit the information loss compared to previous iterations.This insight motivates a slowly increasing schedule of α-values, that allows the agent to concentrate on easier tasks at first before guiding it towards harder tasks.</p>
<p>Practical Aspects</p>
<p>Our approach requires to choose a class of (Bayesian) function approximators for representing the value function V (c) and policy π(θ|c).Moreover, the distributions µ(c) and μ(c) need to be pinned down by a parametric form to allow the sample-weighted updates.</p>
<p>We define the following data-set needed to perform the updates in Equations ( 1) and ( 8)
D = w π i , w μ i , θ i , c i |w π i = exp A(c i ) η p , w μ i = exp β(c i ) α + η µ , i ∈ [1, M ] .
Another important aspect is the choice of α, which penalizes the divergence between μ and µ.While experimenting with the algorithm, choosing α such that the KL-Divergence between current-and target context distribution makes up a certain fraction ζ of the current average reward demonstrated promising performance by automatically adapting to the performance of the algorithm while requiring only one parameter to be tuned.Finally, the hyperparameter may require task-dependent tuning.</p>
<p>Empirical Evaluation</p>
<p>In this section, we demonstrate the benefit of our algorithm by comparing it to C-REPS, CMA-ES [18], GoalGAN [19] and SAGG-RIAC [20].With CMA-ES being a non-contextual algorithm, we only use it in experiments with narrow target distributions, where we then train and evaluate only on the mean of the target context distributions.We will start with a simple point-mass problem, where we evaluate the benefit of our algorithm for broad and narrow target distributions.We then turn towards more challenging tasks, such as a modified version of the reaching task implemented in the OpenAI Gym simulation environment [21] and a sparse Ball-in-a-Cup task.Given that GoalGAN and SAGG-RIAC are algorithm agnostic curriculum generation approaches, we combine them with C-REPS to make the results as comparable as possible.</p>
<p>In all experiments, we use radial basis function (RBF) features to approximate the value function V (c), while the policy π(θ|c) = N (θ|Aφ(c), Σ θ ) uses linear features φ(c).SPRL and C-REPS always use the same number of RBF features for a given environment.In the case of SPRL, we learn the Gaussian context distribution μ(c) and policy π(θ|c) jointly from the sample set D while limiting the change in KL-Divergence.For C-REPS, we use the same scheme but only learn the policy from D. The approach is outlined in the supplementary material.</p>
<p>In our experiments, SPRL always starts with a wide initial sampling distribution μ (c) that, in combination with setting α = 0 for the first K α iterations, allows the algorithm to automatically choose the initial tasks on which learning should take place.After the first K α iterations, we then Figure 3: Success rates achieved by the algorithms on the "precision" (left) and "global" setting (right) of the gate environment.Thick lines represent the 50%-quantiles and shaded areas show the intervals from 10%to 90%-quantile.The quantiles are computed using 40 algorithm executions.One iteration consists of 100 policy rollouts.A policy execution is said to be successful if the final position is sufficiently close to the goal.choose α following the scheme outlined in the previous section.Experimental details that cannot be mentioned due to space limitations can be found in the supplementary material. 1</p>
<p>Gate Environment</p>
<p>In the first environment, the agent needs to steer a point-mass in two-dimensional space from the starting position [0 5] to the goal position at the origin.The dynamics of the point mass are described by a simple linear system subject to a small amount of Gaussian noise.Complexity is introduced by a wall at height y = 2.5, which can only be traversed through a gate.The x-position and width of the gate together define a task c.If the point-mass crashes into the wall, the experiment is stopped and the reward computed based on the current position.The reward function is an exponential of the distance to the goal position with additional L2-Regularization on the generated actions.The point-mass is controlled by two PD-controllers, whose parameters need to be tuned by the agent.The controllers are switched as soon as the point mass reaches the height of the gate, which is why the the desired y-position of the controllers are fixed to 2.5 (the height of the gate) and 0, while all other parameters are controlled by the policy π, making θ a 14-dimensional vector.</p>
<p>We evaluate two setups in this gate environment, which differ in their target context distribution:</p>
<p>In the first one, the agent needs to be able to steer through a very small gate far from the origin ("precision") and in the second it is required to steer through gates with a variety of positions and widths ("global").The two target context distributions are shown in Figure 1.</p>
<p>Figure 1 visualizes the obtained rewards for the investigated algorithms, the evolution of the sampling distribution μ (c) as well as sample tasks from the environment.In the "global" setting, we can see that SPRL converges significantly faster to the optimum while in the "precision" setting, SPRL avoids a local optimum to which C-REPS and CMA-ES converge and which, as can be seen in Figure 3, does not encode desirable behavior.The visualized sampling distributions in Figure 1 indicate that tasks with wide gates positioned at the origin seem to be easier to solve starting from the initially zero-mean Gaussian policy, as in both settings the algorithm first focuses on these kinds of tasks and then subsequently changes the sampling distributions to match the target distribution.Interestingly, the search distribution of CMA-ES did not always converge in the "precision" setting, as can be seen in Figure 1.This behavior persisted across various hyperparameters and population sizes.</p>
<p>Reacher Environment</p>
<p>For the next evaluation, we modify the three-dimensional Reacher environment of the OpenAI Gym toolkit.In this modified version, the goal is to move the end-effector along the surface of a table towards the goal position while avoiding obstacles that are placed on the table.With the obstacles becoming larger, the robot needs to introduce a more pronounced curve movement in order to reach the goal without collisions.To simplify the visualization of the task distribution, we only allow two of the four obstacles to vary in size.The sizes of those two obstacles make up a task c in this environment.Just as in the first environment, the robot should not crash into the obstacles and hence the movement is stopped if one of the four obstacles is touched.The policy π encodes a ProMP [22], from which movements are sampled during training.In this task, θ is a 40-dimensional vector.Please note that the visualization is not completely accurate, as we did not account for the viewpoint of the simulation camera when plotting the trajectories.</p>
<p>Looking at Figure 2, we can see that C-REPS and CMA-ES find a worse optimum compared to SPRL.This local optimum does -just as in the previous experiment -not encode optimal behavior, as we can see in Figure 4. GoalGAN and SAGG-RIAC tend to find the same optimum as SPRL, however with slower convergence and more variance.The sampling distributions visualized in Figure 2 indicate that SPRL focuses on easier tasks with smaller obstacle sizes first and then moves on to the harder, desired tasks.This also explains the initially lower performance of SPRL on the target task compared to C-REPS and CMA-ES.Figure 4 also shows that PPO [23], a step-based reinforcement learning algorithm, is not able to solve the task after the same amount of interaction with the environment, emphasizing the complexity of the learning task.</p>
<p>Sparse Ball-in-a-Cup</p>
<p>We conclude the experimental evaluation with a Ball-in-a-Cup task, in which the reward function exhibits a significant amount of sparsity by only returning a reward of 1 minus an L2 regularization term on the policy parameters, if the ball is in the cup after the policy execution, and 0 otherwise.The robotic platform is a Barrett WAM, which we simulate using the MuJoCo physics engine [24].</p>
<p>The policy again represents a ProMP encoding the desired position of the first, third and fifth joint of the robot.Obviously, achieving the desired task with a poor initial policy is an unlikely event, leading to mostly uninformative rewards and hence a poor learning progress.However, as can be seen in Figure 5, giving the learning agent control over the diameter of the cup significantly improves the learning progress by first training with larger cups and only progressively increasing the precision of the movement.Having access to only 16 samples per iteration, the algorithms did not always learn to achieve the task.However, the final policies learned by SPRL clearly outperform the ones learned by C-REPS, CMA-ES, GoalGAN and SAGG-RIAC.The movements learned in simulation could finally be applied to the robot with a small amount of fine-tuning.</p>
<p>Related Work</p>
<p>The idea of co-evolving the task together with the learner was explored under different names in various contexts.In evolutionary robotics, simulation parameters describing a robot were gradually evolved to match the observations from the real system, while intermediate controllers were learned entirely in simulation [25].Recently, this idea got into the spotlight of reinforcement learning under the name 'sim-to-real transfer' [26].In behavioral psychology, a similar concept is known as shaping [16], and it has direct links to homotopic-continuation methods [17] and computational reinforcement learning [27].In supervised learning, the paradigm of self-paced curriculum learning [28,29] was successfully applied to automatically determine a sequence of training sets with increasing complexity.Conceptually closest to ours is the approach that tackles a much harder problem of learning the optimal curriculum [30], which turns out to be computationally harder than learning the entire task from scratch at once, whereas we propose local auxiliary optimization instead as a surrogate for global optimization that significantly improves sample efficiency of learning.</p>
<p>The process of adaptive task selection can be seen as a form of active learning [31] since the agent learns how to solve harder tasks by eliciting information from the simpler ones.Active learning in turn is closely related to curiosity-driven learning [32], which introduced such approaches as intelligent adaptive curiosity [33] and intrinsic motivation [34] that suggest focusing learning on tasks that promise high change in reward based on the recent history of experiences [20].Curiosity- driven learning was combined with multi-armed bandit algorithms for automatic task selection in reinforcement learning problems [35] and was applied in robotics to learn goal-reaching movements with sparse rewards [36].</p>
<p>The idea of reusing knowledge across related tasks is at the heart of transfer learning in general [37] and transfer in RL in particular [38,39].Prioritization of tasks for which the agent obtains rewards falling into a certain interval of values combined with additional reversibility assumptions was shown to enable learning of high-dimensional object manipulation and maze navigation tasks [19,40].Assuming shared dynamics between tasks and knowledge about the functional form of the reward function allowed to solve a variety of tasks in the classical computer game Doom [41].</p>
<p>Enhanced with universal value function approximators [5], reward-based transfer was extremely successful in robotics applications with sparse reward functions [6].</p>
<p>Finally, the investigation of smoothness assumptions on contextual MDPs [42] is highly relevant to our work, as our algorithm implicitly applies such assumptions to restrict the search space by postulating a linear dependency of the policy mean on the context features.</p>
<p>Conclusion</p>
<p>We proposed a novel approach for generating learning curricula in reinforcement learning problems and developed a practical procedure for simultaneous policy optimization and task sampling distribution adjustment based on an existing information-theoretic contextual policy search algorithm.The progression from 'easy' tasks towards the target distribution of 'hard' tasks allows to solve problems in which classical contextual policy search algorithms cannot find a satisfying solution.</p>
<p>Although our heuristic of choosing α that controls the trade-off between local improvement and progression towards the desired tasks worked sufficiently well in our experiments, we want to find a more rigorous way of choosing α by e.g.maximizing the learning speed towards the target distribution.</p>
<p>Extensions to step-based policy search algorithms, such as [43,44,23], are conceptually straightforward and are expected to further improve the performance by leveraging information from every transition in the environment.Adding a constraint on the lower bound of the policy entropy could furthermore increase the robustness of the algorithm by preserving variance during training on intermediate tasks.</p>
<p>A Numerical Stabilization</p>
<p>We already mentioned that the following optimization problem
arg max p,μ C,Θ R(θ, c)p (θ, c) dcdθ − αD KL (μ||µ) s.t. D KL (p (θ, c) ||q (θ, c)) ≤ C,Θ p (θ, c) dcdθ = 1 Θ p (θ, c) dθ = μ (c) , ∀c ∈ C
poses numerical difficulties, especially for small values of α.We want to discuss this issue in more detail now by looking at the resulting optimal joint-and context distribution
p * (θ, c) ∝ q (θ, c) exp A(θ, c) η μ * (c) ∝ µ (c) exp V (c) α = q (c) q (c) µ (c) exp V (c) α = q (c) exp log µ (c) q (c) + V (c) α .
Note that the expression for μ * was re-written to depend on the current sampling distribution q rather than the target distribution µ.This is necessary as the context samples result from sampling q.</p>
<p>As already pointed out in the main text, V (c) is divided by α in above expression, prohibiting the use of α close to 0, since the resulting exponential term quickly exceeds the largest number that can be represented using double precision floating point numbers.However, especially these values are crucial for allowing the algorithm to select easy tasks in the first iterations of the algorithm.</p>
<p>Furthermore, the log-term in above expression can also result in large negative numbers if the target distribution µ (c) only assigns negligible probability mass to samples from the current sampling distribution q (c), further amplifying numerical problems.</p>
<p>The dual formulation of the considered optimization problem is
G (η, V ) = η + η log E q exp A(θ, c) η + α exp (1) E q exp log µ (c) q (c) + V (c) α .
We see that both problematic terms reappear in this dual, making a numerical optimization very hard for small values of α or target distributions that only assign negligible probability mass to the samples of the current sampling distribution.</p>
<p>As already outlined in the main text, adding the two constraints
C μ (c) dc = 1, D KL (μ(c)||q(c)) ≤
numerically stabilizes the optimization objective by introducing an additional temperature term in the exponential.It can be shown, however, that these additional constraints actually do not further constrain the solution.For this we first note that for the optimal joint-and context distribution p * and μ * it holds that
C,Θ p * (θ, c)dθdc = 1 ⇔ C Θ p * (θ, c)dθ dc = 1 ⇔ C μ * (c)dc = 1.
Furthermore, the following reformulation yields
D KL (p * (θ, c) q(θ, c)) ≤ ⇔ Θ,C p * (θ, c) log p * (θ, c) q(θ, c) dθdc ≤ ⇔ Θ,C p * (θ, c) log μ * (c) q(c) dθdc + Θ,C p * (θ, c) log π * (θ|c) q(θ|c) dθdc ≤ ⇔ C Θ p * (θ, c)dθ log μ * (c) q(c) dc ≤ − Θ,C p * (θ, c) log π * (θ|c) q(θ|c) dθdc ⇔ C μ * (c) log μ * (c) q(c) dc ≤ − E μ * Θ π * (θ|c) log π * (θ|c) q(θ|c) dθ ⇔D KL (μ * (c) q(c)) ≤ − E μ * [D KL (π * (θ|c) q(θ|c))] ,
from which follows that D KL (μ * (c) q(c)) ≤ , as
E μ * [D KL (π * (θ|c) q(θ|c))] ≥ 0.
Another aspect of numerical stabilization are the sample weights that are computed after optimizing the dual function.In C-REPS, the weights w π are derived from
p * (θ, c) ∝ q (θ, c) exp A(θ, c) η p ⇔ π * (θ|c) q (c) ∝ q (θ|c) q (c) exp A(θ, c) η p ⇔ π * (θ|c) ∝ q (θ|c) exp A(θ, c) η p .
In SPRL, the weights w π would slightly differ, as now p * (θ, c) = π * (θ|c) μ * (c) and hence it holds that
π * (θ|c) ∝ q (θ|c) q (c) μ * (c) exp A(θ, c) η p ∝ q (θ|c) exp A(θ, c) η p − β (c) α + η µ .
However, the second term in the exponential had significantly larger magnitudes in our experiments, in turn leading to poor policy updates.Because of this, we decided to use the same policy update as for the regular C-REPS algorithm.Further investigation of this problem and how to regularize it may allow to use the derived weights instead of the ones from the C-REPS algorithm.</p>
<p>B Regularized Policy Updates</p>
<p>In order to enforce the KL-Bound D KL (p(θ, c)||q(θ, c)) ≤ on the policy and context distribution not only during the computation of the weights but also during the actual inference of the new policy and context distribution, the default weighted linear regression and weighted maximum likelihood objectives need to be regularized.</p>
<p>Given a dataset of N weighted samples
D = {(w x i , w y i , x i , y i )|i = 1, . . . , N } , with x i ∈ R dx , y i ∈ R dy , the task of fitting a joint-distribution p(x, y) = p y (y|x)p x (x) = N (y|Aφ(x), Σ y )N (x|µ x , Σ x )
to D while limiting the change with regards to a reference distribution q(x, y) = q y (y|x)q x (x) = N (y| Ãφ(x), Σy )N (x| μx , Σx ),</p>
<p>General Aspects</p>
<p>For the gate and the Reacher experiment, the reward function is given by
R(θ, c) = κ exp − x f (θ) − x g (c) 2 − ν N i=0 u i (θ) T u i (θ) ,
where x f (θ) is the position of the point-mass or end-effector at the end of the policy execution, x g (c) the desired final position, u i (θ) the action applied at time-step i and κ and ν two multipliers that are chosen for each experiment individually.For the visualization of the success rate as well as the computation of the success indicator for the GoalGAN algorithm, the following definition is used: An experiment is considered to be successful, if the distance between final-and desired state is less than a given threshold τ
Success (θ, c) = 1, if x f (θ) − x g (c) 2 &lt; τ, 0, else.
For the Gate and Reacher environment, the threshold is fixed to 0.05, while for the Ball-in-a-Cup environment, the threshold depends on the scale of the cup and the goal is set to be the center of the bottom plate of the cup.</p>
<p>The policies are chosen to be conditional Gaussian distributions N (θ|Aφ(c), Σ θ ), where φ(c) is a feature function.SPRL and C-REPS both use linear policy features in all environments.</p>
<p>In the Reacher and the Ball-in-a-Cup environment, the parameters θ encode a feed-forward policy by weighting several Gaussian basis functions over time
u i (θ) = θ T ψ (t i ) , ψ j (t i ) = b j (t i ) L l=1 b l (t i ) , b j (t i ) = exp (t i − c j ) 2 2L ,
where the centers c j and length L of the basis functions are chosen individually for the experiments.</p>
<p>With that, the policy represents a Probabilistic Movement Primitive [22], whose mean and co-variance matrix are progressively shaped by the learning algorithm to encode movements with high reward.</p>
<p>In order to increase the robustness of SPRL and C-REPS while reducing the sample complexity, an experience buffer storing samples of recent iterations is used.The "size" of this buffer dictates the number of past iterations, whose samples are kept.Hence, in every iteration, C-REPS and SPRL work with N SAMPLES × BUFFER SIZE samples, from which only N SAMPLES are generated by the policy of the current iteration.</p>
<p>We use the CMA-ES implementation given by [45].As it only allows to specify one initial variance for all dimensions of the search distribution, this variance is set to the maximum of the variances contained in the initial co-variance matrices used by SPRL and C-REPS.</p>
<p>For the GoalGAN algorithm, the percentage of samples that are drawn from the buffer containing already solved tasks is fixed to 20%.Since GoalGAN generates the learning curriculum using "Goals of Intermediate Difficulty", it is necessary to execute the policy at least twice in each context.Hence, for 30% of the contexts (after subtracting the previously mentioned 20% of "old" samples), the policy is executed twice.For 10% of the contexts, the policy is executed four times.The noise added to the samples of the GAN NOISE and the number of iterations that pass between the training of the GAN ∆ TRAIN are chosen individually for the experiments.3 )</p>
<p>Figure 6: The columns show visualizations of the point-mass trajectories (upper plots) as well as the obtained rewards (lower plots) in the gate task, when the desired position of the first PD-controller is varied while all other parameters are kept fixed such that a stable control law is obtained.In every column, the gate is positioned at x = 4.0 while the size of it varies from 20 (left), over 3 (middle) to 0.1 (right).</p>
<p>The SAGG-RIAC algorithm requires, besides the probabilities for the sampling modes which are kept as in the original paper, two hyperparameters to be chosen: The maximum number of samples to keep in each region n GOALS as well as the maximum number of recent samples for the competence computation n HIST .</p>
<p>Tables 1 and 2 show the aforementioned hyperparameters for the different algorithms as well as reward function parameters for the different environments.</p>
<p>Gate Experiment</p>
<p>The linear system that describes the behavior of the point-mass is given by
ẋ ẏ = 5 −1 + u + δ, δ ∼ N 0, 2.5 × 10 −3 I .
The point-mass is controlled by two PD-controllers
PD i (x, y) = K i x i − x y i − y + k i , i ∈ [1, 2] , K 1 , K 2 ∈ R 2×2 , k 1 , k 2 ∈ R 2 , x 1 , x 2 , y 1 , y 2 ∈ R,
where x is the x-position of the point mass and y its position on the y-axis.In initial iterations of the algorithm, the sampled PD-controller parameters sometimes make the control law unstable, leading to very large penalties from the L2-regularization of the applied actions and hence to numerical instabilities in SPRL and C-REPS because of very large negative rewards.Because of this, the reward is clipped to always be above 0. Table 1 shows that a large number of samples per iteration for both the "global" and "precision" setting are used.This is purposefully done to keep the influence of the sample size on the algorithm performance as low as possible, as both of these settings serve as a first conceptual benchmark of our algorithm.</p>
<p>Figure 6 helps in understanding, why SPRL drastically improves upon C-REPS especially in the "precision" setting, even with this large amount of samples.For narrow gates, the reward function has a local maximum which tends to attract both C-REPS and CMA-ES, as the chance of sampling a reward close to the true maximum is very unlikely.By first training on contexts in which the global maximum is more likely to be observed and only gradually moving towards the desired contexts, SPRL avoids this sub-optimal solution.</p>
<p>Reacher Experiment</p>
<p>In the Reacher experiment, the ProMP encoded by the policy π has 20 basis functions of width L = 0.03.The centers are evenly spread in the interval [−0.2, 1.2] and the time interval of the movement is normalized to lie in the interval [0, 1] when computing the activations of the basis functions.Since the robot can only move within the xy-plane, θ is a 40-dimensional vector.As we can see in Table 1, the number of samples in each iteration was decreased to 50, which in combination with the increased dimensionality of θ makes the task more challenging.As a side-effect, this reduced the training time, as less simulations need to be executed during training.</p>
<p>While working on the Reacher experiment, we realized that ensuring a minimum amount of variance of the intermediate context distributions of SPRL stabilizes the learning, as for very narrow context distributions, the progression of the algorithm towards the target context distribution becomes very slow.As soon as the current context distribution is sufficiently close to the target one, which in the Reacher experiment is considered to be the case if the KL-Divergence between intermediateand target context distribution drops below a value of 20, the context distribution is allowed to contract without restrictions.Before that happens, the variance is each dimension is lower-bounded by 3 × 10 −5 .</p>
<p>The PPO results are obtained using the version from [46], for which a step-based version of the Reacher experiment is used, in which the reward function is given by r(s, a) = exp −2.5 (x − x g ) 2 + (y − y g ) 2 ,</p>
<p>where s = (x ẋ y ẏ) is the position and velocity of the end-effector, a = (a x a y ) the desired displacement of the end-effector (just as in the regular Reacher task from the OpenAI Gym simulation environment) and x g and y g is the x− and y− position of the goal.When an obstacle is touched, the agent is reset to the initial position.This setup led to the best performance of PPO, while resembling the structure of the episodic learning task used by the other algorithms (a version in which the episode ends as soon as an obstacle is touched led to a lower performance of PPO).</p>
<p>To ensure that the poor performance of PPO is not caused by an inadequate choice of hyperparameters, PPO was run on an easy version of the task in which the two obstacle sizes were set to 0.01, where it encountered no problems in solving the task.</p>
<p>Every iteration of PPO uses 3600 environment steps, which corresponds to 24 trajectory executions in the episodic setting.PPO uses an entropy coefficient of 10 −3 , γ = 0.999 and λ = 1.The neural network that learns the value function as well as the policy has two dense hidden layers with 164 neurons and tanh activation functions.The number of minibatches is set to 5 while the number of optimization epochs is set to 15.The standard deviation in each action dimension is initialized to 1, giving the algorithm enough initial variance, as the actions are clipped to the interval [−1, 1] before being applied to the robot.</p>
<p>Ball-in-a-Cup Experiment</p>
<p>For the Ball-in-a-Cup environment, the 9 basis functions of the ProMP are spread over the interval [−0.01, 1.01] and have width L = 0.0035.Again, the time interval of the movement is normalized to lie in the interval [0, 1] when computing the basis function activations.The ProMP encodes the offset of the desired position from the initial position.By setting the first and last two basis functions</p>
<p>t. D KL (p(θ, c)||q(θ, c)) ≤ , C,Θ p(θ, c)dcdθ = 1, Θ p(θ, c)dθ = µ(c), ∀c ∈ C,</p>
<p>Figure 1 :
1
Figure 1: The left plots show the reward achieved by the algorithms on the "precision" (top row) and "global" setting (bottom row) on the target context distributions in the gate environment.Thick lines represent the 50%-quantiles and shaded areas the intervals from 10%to 90%-quantile of 40 algorithm executions.One iteration consists of 100 policy rollouts.The right plot shows the evolution of the sampling distribution μ (c) (colored areas) of one run together with the target distribution µ (c) (black line).The small images on the right visualize the task for different gate positions and widths.The crosses mark the corresponding positions in the context space.</p>
<p>Figure 2 :
2
Figure 2: The left plots show the 50%-quantiles (thick lines) and the intervals from 10%to 90%quantile (shaded areas) of the reward achieved by the investigated algorithms on the target context distribution in the Reacher environment.The quantiles are computed from 40 algorithm executions.One iteration consists of 50 policy rollouts.Colored areas in the right plot show the sampling distribution μ (c) at different iterations of one SPRL run together with the target distribution (black line).The legend on the right shows the iteration that corresponds to a specific color.Small images on the right visualize different contexts with black crosses marking the corresponding positions in context space.</p>
<p>Figure 4 :
4
Figure 4: Trajectories generated by the final policies learned with SPRL, C-REPS, CMA-ES, Goal-GAN, SAGG-RIAC and PPO in the reacher environment.The trajectories should reach the red dot while avoiding the cyan boxes.Please note that the visualization is not completely accurate, as we did not account for the viewpoint of the simulation camera when plotting the trajectories.</p>
<p>Figure 5 :
5
Figure 5: The left plots show the 50%-quantiles (thick lines) and intervals from 10%to 90%-quantile (shaded areas) of the success rate of the investigated algorithms for the sparse Ball-in-a-Cup task.The quantiles were computed from the 10 best runs out of 20.One iteration consists of 16 policy rollouts.Colored areas in the right plot show the sampling distribution μ (c) at different iterations of one SPRL run together with the target distribution (black line).The small images on the right visualize the task on the real robot (upper) and in simulation with a scale of 2.5 (lower).</p>
<p>Table 1 :
1
Important parameters of SPRL and C-REPS in the conducted experiments.The meaning of the symbols correspond to those presented in the algorithm from the main text and introduced in this appendix.
nSAMPLES BUFFER SIZEζKακνGATE "GLOBAL"0.25100100.002 140 10 10 −4GATE "PRECISION"0.4100100.02140 10 10 −4REACHER0.550100.159020 10 −1BALL-IN-A-CUP0.351653.015−−</p>
<p>Table 2 :
2
Important parameters of GoalGAN and SAGG-RIAC in the conducted experiments.The meaning of the symbols correspond to those introduced in this appendix.
NOISE∆TRAIN nGOALS nHISTGATE "GLOBAL"0.055100500GATE "PRECISION"0.055100200REACHER0.1580300BALL-IN-A-CUP0.05350120
Code is publicly available under https://github.com/psclklnk/self-paced-rl
AcknowledgmentsCalculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt.This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 640554 (SKILLS4ROBOTS) and from DFG project PA3179/1-1 (ROBOLEAP).Since the distributions p x , p y , q x and q y are Gaussians, the KL-Divergences can be expressed analytically.Setting the derivative of the Lagrangian with respect to the optimization variables to zero yields to following expressions of the optimization variables in terms of the multiplier η and the samples fromAbove equations yield a simple way of enforcing the KL-Bound on the joint distribution: Since η is zero if the constraint on the allowed KL-Divergence is not active, A, Σ y , µ x and Σ x can be first computed with η = 0 and only if the allowed KL-Divergence is exceeded, η needs to be found by searching the root ofwhere p y and p x are expressed as given by above formulas and hence implicitly depend on η.As this is a one-dimensional root finding problem, simple algorithms can be used for this task.C Further Experimental DetailsThis section is composed of further relevant details on the experiments, which could not be included in the main text due to space limitations.The first part of this section contains general aspects that appeal to more than one experiment as well as tables with important parameters of SPRL, C-REPS, GoalGAN and SAGG-RIAC for the different environments.The remaining parts introduce and discuss details specific to the individual experiments.to 0 in each of the three dimensions, the movement always starts in the initial position and returns to it after the movement execution.All in all, θ is a 15-dimensional vector.The reward function is defined asencoding a preference over movements that deviate as less as possible from the initial position while still solving the task.Looking back at Table1, the value of ζ stands out, as it is significantly higher than in the other experiments.We suppose that such a large value of ζ is needed because of the shape of the reward function, which creates a large drop in reward if the policy is sub-optimal.Because of this, the incentive required to encourage the algorithm to shift probability mass towards contexts in which the current policy is sub-optimal needs to be significantly higher than in the other experiments.Just as for the Reacher experiment, we also lower-bound the variance of the intermediate context distributions to 0.01 until the KL-Divergence between intermediate and target distribution falls below a value of 200.After learning the movements in simulation, the successful runs were executed on the real robot.Due to simulation bias, just replaying the trajectories did not work satisfyingly.At this stage, we could have increased the variance of the movement primitive and re-trained on the real robot.As sim-to-real transfer is, however, not the focus of this paper, we decided to manually adjust the execution speed of the movement primitive by a few percent, which yielded the desired result.
Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 51875405292015</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 55076763542017</p>
<p>Policy search for motor primitives in robotics. J Kober, J Peters, NIPS. 2009</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, P Abbeel, JMLR. 1712016</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, ICML. 2015</p>
<p>Hindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O P Abbeel, W Zaremba, NIPS. 2017</p>
<p>Temporal difference models: Model-free deep rl for model-based control. V Pong, S Gu, M Dalal, S Levine, ICLR2018</p>
<p>Variational inference for policy search in changing situations. G Neumann, ICML. 2011</p>
<p>Online multi-task learning for policy gradient methods. H B Ammar, E Eaton, P Ruvolo, M Taylor, ICML. 2014</p>
<p>Data-efficient generalization of robot skills with contextual policy search. A G Kupcsik, M P Deisenroth, J Peters, G Neumann, AAAI. 2013</p>
<p>Reinforcement learning vs human programming in tetherball robot games. S Parisi, H Abdulsamad, A Paraschos, C Daniel, J Peters, IROS. 2015</p>
<p>Bandit Algorithms (pre-print). T Lattimore, C Szepesvári, 2019Cambridge University Press</p>
<p>M P Deisenroth, G Neumann, J Peters, A survey on policy search for robotics. Foundations and Trends R in Robotics. 20132</p>
<p>Flexible shaping: How learning in small steps helps. K A Krueger, P Dayan, Cognition. 11032009</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, ICML. 2009</p>
<p>The behavior of organisms: An experimental analysis. B F Skinner, 1990BF Skinner Foundation</p>
<p>Numerical continuation methods: An introduction. E L Allgower, K Georg, 2012Springer Science &amp; Business Media13</p>
<p>Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). N Hansen, S D Müller, P Koumoutsakos, Evolutionary computation. 1112003</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, ICML. 2018</p>
<p>Intrinsically motivated goal exploration for active motor learning in robots: A case study. A Baranes, P.-Y Oudeyer, IROS. 2010</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Openai gym. 2016</p>
<p>Probabilistic movement primitives. A Paraschos, C Daniel, J R Peters, G Neumann, NIPS. 2013</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, IROS. 2012</p>
<p>Once more unto the breach: Co-evolving a robot and its simulator. J Bongard, H Lipson, ALIFE2004</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, arXiv:1810.056872018arXiv preprint</p>
<p>What does shaping mean for computational reinforcement learning. T Erez, W D Smart, ICDL. 2008</p>
<p>Self-paced learning for latent variable models. M P Kumar, B Packer, D Koller, NIPS. 2010</p>
<p>Self-paced curriculum learning. L Jiang, D Meng, Q Zhao, S Shan, A G Hauptmann, AAAI. 2015</p>
<p>Learning curriculum policies for reinforcement learning. S Narvekar, P Stone, AAMAS. 2019</p>
<p>Active learning. B Settles, Synthesis Lectures on Artificial Intelligence and Machine Learning. 20126</p>
<p>P.-Y Oudeyer, arXiv:1802.10546Computational theories of curiosity-driven learning. 2018arXiv preprint</p>
<p>Intelligent adaptive curiosity: A source of self-development. P.-Y Oudeyer, Proceedings of the 4th International Workshop on Epigenetic Robotics. the 4th International Workshop on Epigenetic Robotics2004117Lund University Cognitive Studies</p>
<p>Intrinsically motivated learning of hierarchical collections of skills. A G Barto, S Singh, N Chentanez, ICDL2004</p>
<p>Active contextual policy search. A Fabisch, J H Metzen, JMLR. 1512014</p>
<p>Accuracy-based curriculum learning in deep reinforcement learning. P Fournier, O Sigaud, M Chetouani, P.-Y Oudeyer, arXiv:1806.096142018arXiv preprint</p>
<p>A survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 22102010</p>
<p>Transfer learning for reinforcement learning domains: A survey. M E Taylor, P Stone, JMLR. 10Jul. 2009</p>
<p>Transfer in reinforcement learning: a framework and a survey. A Lazaric, Reinforcement Learning. Springer2012</p>
<p>Reverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, P Abbeel, CoRL2017</p>
<p>A Dosovitskiy, V Koltun, arXiv:1611.01779Learning to act by predicting the future. 2016arXiv preprint</p>
<p>Markov decision processes with continuous side information. A Modi, N Jiang, S Singh, A Tewari, ALT2018</p>
<p>Relative entropy policy search. J Peters, K Mülling, Y Altun, AAAI. 2010</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, ICML. 2015</p>
<p>CMA-ES/pycma on Github. N Hansen, Y Akimoto, P Baudis, Feb. 2019</p>
<p>A Hill, A Raffin, M Ernestus, A Gleave, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, Stable baselines. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
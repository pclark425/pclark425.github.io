<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1113 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1113</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1113</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-8840383</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1711.05560v1.pdf" target="_blank">Variational Adaptive-Newton Method for Explorative Learning</a></p>
                <p><strong>Paper Abstract:</strong> We present the Variational Adaptive Newton (VAN) method which is a black-box optimization method especially suitable for explorative-learning tasks such as active learning and reinforcement learning. Similar to Bayesian methods, VAN estimates a distribution that can be used for exploration, but requires computations that are similar to continuous optimization methods. Our theoretical contribution reveals that VAN is a second-order method that unifies existing methods in distinct fields of continuous optimization, variational inference, and evolution strategies. Our experimental results show that VAN performs well on a wide-variety of learning tasks. This work presents a general-purpose explorative-learning method that has the potential to improve learning in areas such as active learning and reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1113.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1113.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAN-Active</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Adaptive-Newton with Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the VAN variational-optimization algorithm to drive adaptive experimental design in classification by maintaining a Gaussian posterior q(θ)=N(µ,Σ) and selecting data via an acquisition function (entropy) computed from predictive probabilities estimated with samples from q.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VAN (with active learning for logistic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent is logistic-regression classifier whose parameter posterior q(θ)=N(µ,Σ) is learned using VAN (natural-gradient mirror descent in mean-parameter space). Key components: (1) distributional parameter estimate (µ,Σ), (2) Monte-Carlo predictive sampling from q to obtain predictive probabilities, (3) entropy-based acquisition score to select informative examples, (4) diagonal or full-covariance VAN updates (sVAN / sVAN-D) for scalable learning.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning (entropy acquisition with predictive uncertainty estimated via q(θ))</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each iteration the agent estimates p(y|x) by Monte Carlo integration over θ~q(θ) (Eq. 27–28), computes the entropy acquisition score H(x) = −Σ p(y|x) log p(y|x) for each candidate pool example, ranks the pool and selects the top-M examples as the next minibatch. q(θ) is updated with VAN using averaged gradients/Hessians under q, so the acquisition function evolves as the posterior tightens.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Binary logistic-regression classification (USPS dataset experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Supervised classification problem with an unlabeled pool of inputs; unknown labels for pool examples (learning from queried labels), input feature space continuous/finite-dimensional; not an MDP or temporally sequential environment. The agent operates under epistemic uncertainty about labels (partial information about labels until queried).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Standard classification dataset (USPS): moderate dimensionality (D=256) and moderate pool size (N dataset size reported in paper), minibatch active selection M=10 in experiments; no sequential episode structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: VAN with active learning (sVAN-Active) is reported to be much more data-efficient and more stable than sVAN with passive/stochastic sampling on USPS (Figure 3); fewer passes through data required to reach comparable test loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Qualitative: sVAN with stochastic / random minibatch selection achieves slower learning and less stability compared to sVAN with active acquisition; exact numeric metrics not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: Active selection using entropy + q(θ) sampling led to noticeably fewer training passes to reach a given test loss (paper states 'much more data efficient' and shows curves), exact numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration of the input space is achieved by selecting high-entropy (high-uncertainty) examples (information-seeking); exploitation arises as q(θ) tightens and the acquisition scores reflect reduced uncertainty. The mechanism is explicit information-gain style active selection using predictive uncertainty from the learned posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against stochastic learning (sVAN without active selection) and standard adaptive-gradient methods for other tasks (AdaGrad etc.); for active-learning experiment the direct comparison is sVAN (passive sampling) vs sVAN-Active.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Using VAN's maintained parameter distribution to compute predictive uncertainty via MC sampling and an entropy acquisition score yields significantly improved data efficiency and stability in active learning for logistic regression (USPS dataset). The paper demonstrates that maintaining q(θ) (vs point estimates) enables a principled acquisition function and improved selection of informative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No numeric quantification of gains presented; the method relies on approximate MC estimates of predictive probabilities (sampling cost), and experiments use modest minibatch sizes (M=10) — scalability/overhead of acquisition in very large pools not quantified. Also active-learning benefit shown for the logistic regression task; generality to other tasks not extensively evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Adaptive-Newton Method for Explorative Learning', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1113.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1113.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAN-ParamExpl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Adaptive-Newton for Parameter-based Exploration in Deep RL (sVAN / sVAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of VAN (and its Gauss-Newton variant VAG) to parameter-space exploration for policy optimization: maintain a Gaussian distribution over policy parameters and update its mean and covariance jointly with VAN updates to drive exploration in continuous control tasks (DDPG/DPG actor-critic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>sVAN / sVAG applied to parameter-based exploration DDPG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An actor-critic RL agent (Deterministic Policy Gradient / DDPG) where the actor policy parameters θ are sampled from a Gaussian q(θ)=N(µ,Σ) (parameter-space noise). The distribution parameters µ and Σ are updated using stochastic VAN (sVAN) or stochastic Variational Adaptive Gauss-Newton (sVAG); for scalability a diagonal mean-field covariance (sVAN-D / sVAG-D) is used. The critic Q network is learned in standard fashion (squared Bellman residual), and target networks are used.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>parameter-based exploration (sampling policy parameters from q(θ) for exploration), i.e., an information-seeking exploration via maintaining and adapting a distribution over policy parameters</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each interaction step the agent samples θ ~ q(θ)=N(µ,Σ), executes the deterministic policy π_θ, collects (s,a,r,s') in replay, and periodically updates (µ,Σ) using sVAN updates (Eq. 17 / 51–52): µ is moved along the averaged gradient scaled by an adaptive precision matrix P (running sum of averaged Hessians), and Σ (or its precision s) is updated by accumulating expected Hessian (or Gauss-Newton outer product) terms under q; diagonal approximations and MC sampling are used for large networks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>OpenAI Gym continuous-control benchmarks (Half-Cheetah; also tested on Pendulum)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous state and action spaces (continuous control); stochasticity arises from environment transitions and policy sampling; exploration-challenging tasks where informative feedback may be sparse; standard simulated physics benchmarks in OpenAI Gym / MuJoCo-like domains.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks like Half-Cheetah are moderately high-dimensional continuous-control problems (multidimensional state/action vectors, multi-step episodes); exact state/action dimensions and episode lengths are not provided in the paper, but these are standard continuous-control tasks used in DDPG evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: VAN-based methods (sVAN-D-1 and sVAG-D-1) significantly outperform baseline exploration by V-SGD and no-exploration SGD on Half-Cheetah (Figure 4); sVAN-D-10 (more MC samples) achieves improved early data-efficiency relative to sVAN-D-1. Specific numerical cumulative rewards or learning curves are shown in the paper figures but numeric values are not reported in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Qualitative: Baselines include V-SGD (learning mean and covariance independently via V-SGD) which finds a good policy but exhibits unstable performance that degenerates over time; SGD with no exploration performs poorly and learns a suboptimal solution. Exact numeric comparisons are not listed in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: VAN variants show higher sample efficiency on Half-Cheetah: sVAN-D-10 shows better early-stage data efficiency than sVAN-D-1, and both outperform V-SGD and SGD. No absolute sample counts or reward thresholds are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is performed by sampling policy parameter θ from q(θ), enabling richer policy variability than action-space noise. Exploitation occurs as µ converges toward higher-return parameters and Σ shrinks via VAN updates (precision P accumulates Hessian information). The balance is adaptive: VAN jointly updates mean and covariance using natural-gradient/second-order information so exploration magnitude naturally decays as posterior concentrates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: SGD (no parameter-space exploration), V-SGD (independent updates of µ and Σ via variational SGD), sVAG (Gauss-Newton variant), and diagonal approximations thereof (sVAN-D, sVAG-D). The paper also references related prior works (parameter-space noise methods, NES, Salimans et al.) but the empirical baselines are SGD and V-SGD variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Parameter-based exploration driven by VAN (joint natural-gradient updates of µ and Σ with adaptive scaling via accumulated precision matrix) leads to substantially improved learning in Half-Cheetah versus baseline DDPG with action noise or independent parameter updates; diagonal approximations (sVAN-D, sVAG-D) are effective and sVAG-D and sVAN-D performed similarly on this task, indicating Gauss-Newton and Hessian-reparameterization approximations can both work well. More Monte Carlo samples for expectation estimates (sVAN-D-10) improve early data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No advantage observed on the Pendulum task (authors conjecture this problem does not require rich exploration); computing or inverting full Hessian is infeasible for large networks—paper resorts to diagonal mean-field approximations which may limit representational fidelity of uncertainty; V-SGD baseline suffered instability (degeneration) implying naive independent updates of Σ can be problematic. Exact numeric reward values, significance tests, and broader generalization across more environments are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Adaptive-Newton Method for Explorative Learning', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring parameter space in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Parameter space noise for exploration <em>(Rating: 2)</em></li>
                <li>Natural evolution strategies <em>(Rating: 2)</em></li>
                <li>Evolution Strategies as a Scalable Alternative to Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Bayesian Active Learning for Classification and Preference Learning <em>(Rating: 2)</em></li>
                <li>Noisy networks for exploration <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1113",
    "paper_id": "paper-8840383",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "VAN-Active",
            "name_full": "Variational Adaptive-Newton with Active Learning",
            "brief_description": "Use of the VAN variational-optimization algorithm to drive adaptive experimental design in classification by maintaining a Gaussian posterior q(θ)=N(µ,Σ) and selecting data via an acquisition function (entropy) computed from predictive probabilities estimated with samples from q.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "VAN (with active learning for logistic regression)",
            "agent_description": "Agent is logistic-regression classifier whose parameter posterior q(θ)=N(µ,Σ) is learned using VAN (natural-gradient mirror descent in mean-parameter space). Key components: (1) distributional parameter estimate (µ,Σ), (2) Monte-Carlo predictive sampling from q to obtain predictive probabilities, (3) entropy-based acquisition score to select informative examples, (4) diagonal or full-covariance VAN updates (sVAN / sVAN-D) for scalable learning.",
            "adaptive_design_method": "active learning (entropy acquisition with predictive uncertainty estimated via q(θ))",
            "adaptation_strategy_description": "At each iteration the agent estimates p(y|x) by Monte Carlo integration over θ~q(θ) (Eq. 27–28), computes the entropy acquisition score H(x) = −Σ p(y|x) log p(y|x) for each candidate pool example, ranks the pool and selects the top-M examples as the next minibatch. q(θ) is updated with VAN using averaged gradients/Hessians under q, so the acquisition function evolves as the posterior tightens.",
            "environment_name": "Binary logistic-regression classification (USPS dataset experiment)",
            "environment_characteristics": "Supervised classification problem with an unlabeled pool of inputs; unknown labels for pool examples (learning from queried labels), input feature space continuous/finite-dimensional; not an MDP or temporally sequential environment. The agent operates under epistemic uncertainty about labels (partial information about labels until queried).",
            "environment_complexity": "Standard classification dataset (USPS): moderate dimensionality (D=256) and moderate pool size (N dataset size reported in paper), minibatch active selection M=10 in experiments; no sequential episode structure.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: VAN with active learning (sVAN-Active) is reported to be much more data-efficient and more stable than sVAN with passive/stochastic sampling on USPS (Figure 3); fewer passes through data required to reach comparable test loss.",
            "performance_without_adaptation": "Qualitative: sVAN with stochastic / random minibatch selection achieves slower learning and less stability compared to sVAN with active acquisition; exact numeric metrics not reported in the paper.",
            "sample_efficiency": "Qualitative: Active selection using entropy + q(θ) sampling led to noticeably fewer training passes to reach a given test loss (paper states 'much more data efficient' and shows curves), exact numbers not provided.",
            "exploration_exploitation_tradeoff": "Exploration of the input space is achieved by selecting high-entropy (high-uncertainty) examples (information-seeking); exploitation arises as q(θ) tightens and the acquisition scores reflect reduced uncertainty. The mechanism is explicit information-gain style active selection using predictive uncertainty from the learned posterior.",
            "comparison_methods": "Compared against stochastic learning (sVAN without active selection) and standard adaptive-gradient methods for other tasks (AdaGrad etc.); for active-learning experiment the direct comparison is sVAN (passive sampling) vs sVAN-Active.",
            "key_results": "Using VAN's maintained parameter distribution to compute predictive uncertainty via MC sampling and an entropy acquisition score yields significantly improved data efficiency and stability in active learning for logistic regression (USPS dataset). The paper demonstrates that maintaining q(θ) (vs point estimates) enables a principled acquisition function and improved selection of informative examples.",
            "limitations_or_failures": "No numeric quantification of gains presented; the method relies on approximate MC estimates of predictive probabilities (sampling cost), and experiments use modest minibatch sizes (M=10) — scalability/overhead of acquisition in very large pools not quantified. Also active-learning benefit shown for the logistic regression task; generality to other tasks not extensively evaluated in this work.",
            "uuid": "e1113.0",
            "source_info": {
                "paper_title": "Variational Adaptive-Newton Method for Explorative Learning",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "VAN-ParamExpl",
            "name_full": "Variational Adaptive-Newton for Parameter-based Exploration in Deep RL (sVAN / sVAG)",
            "brief_description": "Application of VAN (and its Gauss-Newton variant VAG) to parameter-space exploration for policy optimization: maintain a Gaussian distribution over policy parameters and update its mean and covariance jointly with VAN updates to drive exploration in continuous control tasks (DDPG/DPG actor-critic).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "sVAN / sVAG applied to parameter-based exploration DDPG",
            "agent_description": "An actor-critic RL agent (Deterministic Policy Gradient / DDPG) where the actor policy parameters θ are sampled from a Gaussian q(θ)=N(µ,Σ) (parameter-space noise). The distribution parameters µ and Σ are updated using stochastic VAN (sVAN) or stochastic Variational Adaptive Gauss-Newton (sVAG); for scalability a diagonal mean-field covariance (sVAN-D / sVAG-D) is used. The critic Q network is learned in standard fashion (squared Bellman residual), and target networks are used.",
            "adaptive_design_method": "parameter-based exploration (sampling policy parameters from q(θ) for exploration), i.e., an information-seeking exploration via maintaining and adapting a distribution over policy parameters",
            "adaptation_strategy_description": "At each interaction step the agent samples θ ~ q(θ)=N(µ,Σ), executes the deterministic policy π_θ, collects (s,a,r,s') in replay, and periodically updates (µ,Σ) using sVAN updates (Eq. 17 / 51–52): µ is moved along the averaged gradient scaled by an adaptive precision matrix P (running sum of averaged Hessians), and Σ (or its precision s) is updated by accumulating expected Hessian (or Gauss-Newton outer product) terms under q; diagonal approximations and MC sampling are used for large networks.",
            "environment_name": "OpenAI Gym continuous-control benchmarks (Half-Cheetah; also tested on Pendulum)",
            "environment_characteristics": "Continuous state and action spaces (continuous control); stochasticity arises from environment transitions and policy sampling; exploration-challenging tasks where informative feedback may be sparse; standard simulated physics benchmarks in OpenAI Gym / MuJoCo-like domains.",
            "environment_complexity": "Benchmarks like Half-Cheetah are moderately high-dimensional continuous-control problems (multidimensional state/action vectors, multi-step episodes); exact state/action dimensions and episode lengths are not provided in the paper, but these are standard continuous-control tasks used in DDPG evaluations.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: VAN-based methods (sVAN-D-1 and sVAG-D-1) significantly outperform baseline exploration by V-SGD and no-exploration SGD on Half-Cheetah (Figure 4); sVAN-D-10 (more MC samples) achieves improved early data-efficiency relative to sVAN-D-1. Specific numerical cumulative rewards or learning curves are shown in the paper figures but numeric values are not reported in-text.",
            "performance_without_adaptation": "Qualitative: Baselines include V-SGD (learning mean and covariance independently via V-SGD) which finds a good policy but exhibits unstable performance that degenerates over time; SGD with no exploration performs poorly and learns a suboptimal solution. Exact numeric comparisons are not listed in the text.",
            "sample_efficiency": "Qualitative: VAN variants show higher sample efficiency on Half-Cheetah: sVAN-D-10 shows better early-stage data efficiency than sVAN-D-1, and both outperform V-SGD and SGD. No absolute sample counts or reward thresholds are provided in the text.",
            "exploration_exploitation_tradeoff": "Exploration is performed by sampling policy parameter θ from q(θ), enabling richer policy variability than action-space noise. Exploitation occurs as µ converges toward higher-return parameters and Σ shrinks via VAN updates (precision P accumulates Hessian information). The balance is adaptive: VAN jointly updates mean and covariance using natural-gradient/second-order information so exploration magnitude naturally decays as posterior concentrates.",
            "comparison_methods": "Compared against: SGD (no parameter-space exploration), V-SGD (independent updates of µ and Σ via variational SGD), sVAG (Gauss-Newton variant), and diagonal approximations thereof (sVAN-D, sVAG-D). The paper also references related prior works (parameter-space noise methods, NES, Salimans et al.) but the empirical baselines are SGD and V-SGD variants.",
            "key_results": "Parameter-based exploration driven by VAN (joint natural-gradient updates of µ and Σ with adaptive scaling via accumulated precision matrix) leads to substantially improved learning in Half-Cheetah versus baseline DDPG with action noise or independent parameter updates; diagonal approximations (sVAN-D, sVAG-D) are effective and sVAG-D and sVAN-D performed similarly on this task, indicating Gauss-Newton and Hessian-reparameterization approximations can both work well. More Monte Carlo samples for expectation estimates (sVAN-D-10) improve early data efficiency.",
            "limitations_or_failures": "No advantage observed on the Pendulum task (authors conjecture this problem does not require rich exploration); computing or inverting full Hessian is infeasible for large networks—paper resorts to diagonal mean-field approximations which may limit representational fidelity of uncertainty; V-SGD baseline suffered instability (degeneration) implying naive independent updates of Σ can be problematic. Exact numeric reward values, significance tests, and broader generalization across more environments are not provided.",
            "uuid": "e1113.1",
            "source_info": {
                "paper_title": "Variational Adaptive-Newton Method for Explorative Learning",
                "publication_date_yy_mm": "2017-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring parameter space in reinforcement learning",
            "rating": 2,
            "sanitized_title": "exploring_parameter_space_in_reinforcement_learning"
        },
        {
            "paper_title": "Parameter space noise for exploration",
            "rating": 2,
            "sanitized_title": "parameter_space_noise_for_exploration"
        },
        {
            "paper_title": "Natural evolution strategies",
            "rating": 2,
            "sanitized_title": "natural_evolution_strategies"
        },
        {
            "paper_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "evolution_strategies_as_a_scalable_alternative_to_reinforcement_learning"
        },
        {
            "paper_title": "Bayesian Active Learning for Classification and Preference Learning",
            "rating": 2,
            "sanitized_title": "bayesian_active_learning_for_classification_and_preference_learning"
        },
        {
            "paper_title": "Noisy networks for exploration",
            "rating": 1,
            "sanitized_title": "noisy_networks_for_exploration"
        }
    ],
    "cost": 0.01120325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Variational Adaptive-Newton Method for Explorative Learning</p>
<p>Mohammad Emtiyaz Khan 
Center for Advanced Intelligence Project (AIP)
RIKEN
TokyoJapan</p>
<p>Wu Lin 
Center for Advanced Intelligence Project (AIP)
RIKEN
TokyoJapan</p>
<p>Voot Tangkaratt 
Center for Advanced Intelligence Project (AIP)
RIKEN
TokyoJapan</p>
<p>Zuozhu Liu 
Center for Advanced Intelligence Project (AIP)
RIKEN
TokyoJapan</p>
<p>Didrik Nielsen 
Center for Advanced Intelligence Project (AIP)
RIKEN
TokyoJapan</p>
<p>Variational Adaptive-Newton Method for Explorative Learning</p>
<p>We present the Variational Adaptive Newton (VAN) method which is a black-box optimization method especially suitable for explorativelearning tasks such as active learning and reinforcement learning. Similar to Bayesian methods, VAN estimates a distribution that can be used for exploration, but requires computations that are similar to continuous optimization methods. Our theoretical contribution reveals that VAN is a second-order method that unifies existing methods in distinct fields of continuous optimization, variational inference, and evolution strategies. Our experimental results show that VAN performs well on a wide-variety of learning tasks. This work presents a general-purpose explorative-learning method that has the potential to improve learning in areas such as active learning and reinforcement learning.</p>
<p>Introduction</p>
<p>Throughout our life, we continue to learn about the world by sequentially exploring it. We acquire new experiences using old ones, and use the new ones to learn even more. How can we design methods that can perform such "explorative" learning to obtain good generalizations? This is an open question in artificial intelligence and machine learning.</p>
<p>One such approach is based on Bayesian methods. This approach has not only been used as a theoretical model of human cognitive development (Perfors et al., 2011) but also been applied to a wide variety of practical explorativelearning tasks, e.g., in active learning and Bayesian optimization to select informative data examples (Houlsby et al., 2011;Gal et al., 2017;Brochu et al., 2010;Fishel and Loeb, 2012), and in reinforcement learning to learn through interactions (Wyatt, 1998;Strens, 2000). Unfortunately, Bayesian methods are computationally demanding because computation of the posterior distribution is a difficult task, especially for large-scale problems. In contrast, non-Bayesian methods, such as those based on continuous optimization methods, are generally computationally much cheaper, but they cannot directly exploit the mechanisms of Bayesian exploration because they do not estimate the posterior distribution. This raises the following question: how can we design explorative-learning methods that compute a distribution just like Bayesian methods, but cost similar to optimization methods?</p>
<p>In this paper, we propose such a method. Our method can be used to solve generic unconstrained functionminimization problems 1 that take the following form:
θ * = argmin θ f (θ), where θ ∈ R D .
(1)</p>
<p>A wide variety of problems in supervised, unsupervised, and reinforcement learning can be formulated in this way. Instead of directly solving the above problem, our method solves it indirectly by first taking the expectation of f (θ) with respect to an unknown probability distribution q(θ|η), and then solving the following minimization problem: </p>
<p>where minimization is done with respect to the parameter η of the distribution q. This approach is referred to as the Variational Optimization (VO) approach by Staines and Barber (2012) and can lead us to the minimum θ * because L(η) is an upper bound on the minimum value of f , i.e., min θ f (θ) ≤ E q(θ|η) [f (θ)]. Therefore minimizing L(η) minimizes f (θ), and when the distribution q puts all its mass on θ * , we recover the minimum value. This type of function minimization is commonly used in many areas of stochastic search such as evolution strategies (Hansen and Ostermeier, 2001;Wierstra et al., 2008). In our problem context, this formulation is advantageous because it enables learning via exploration, where exploration is facilitated through the distribution q(θ|η).</p>
<p>Our main contribution is a new method to solve (2) by using a mirror-descent algorithm. We show that our algorithm is a second-order method which solves the original problem (1), even though it is designed to solve the problem (2). Due to its similarity to Newton's method, we refer to our method as the Variational Adaptive Newton (VAN) method. Figure 1 shows an example of our method for a one-dimensional non-convex function.</p>
<p>We establish connections of our method to many existing methods in continuous optimization, variational inference, and evolution strategies, and use these connections to derive new algorithms for explorative learning. Below, we summarize the contributions made in the rest of the paper:</p>
<p>• In Section 3, we derive VAN and establish it as a second-order method. In Section 4, we derive computationally-efficient versions of VAN and discuss their relations to adaptive-gradient methods.</p>
<p>• In Section 5 and 6, we show connections to variational inference methods and natural evolution strategy (Wierstra et al., 2008).</p>
<p>• In Section 7, we apply our method to supervised leaning, unsupervised learning, active learning, and reinforcement learning. In Section 8, we discuss relevance and limitations of our approach.</p>
<p>This work presents a general-purpose explorative-learning method that has the potential to improve learning in areas such as active learning and reinforcement learning.</p>
<p>Variational Optimization</p>
<p>We will focus on solving the problem (2) since it enables estimation of a distribution that can be used for exploration. In this paper, we will use the Gaussian distribution q(θ). The problem (2) can then be rewritten as follows,
min µ,Σ E N (θ|µ,Σ) [f (θ)] := L(µ, Σ)(3)
where q is the Gaussian distribution q(θ|η) := N (θ|µ, Σ) with µ being the mean and Σ being the covariance, and η = {µ, Σ}. The function L is differentiable under mild conditions even when f is not differentiable, as discussed in Staines and Barber (2012). This makes it possible to apply gradient-based optimization methods to optimize it.</p>
<p>A straightforward approach to minimize L is to use Stochastic Gradient Descent (SGD) as shown below,
V-SGD : µ t+1 = µ t − ρ t ∇ µ L t (4) Σ t+1 = Σ t − ρ t ∇ Σ L t(5)
where ρ t &gt; 0 is a step size at iteration t, ∇ denotes an unbiased stochastic-gradient estimate, and L t = L(µ t , Σ t ).  Huszar (2017). The top figure shows the function f (θ) = sinc(θ) with a blue curve. The global minima is at θ * = 1 although there are many local minima and maxima as well. The second plot shows the VO objective L(µ, σ) = E q [f (θ)] for the Gaussian q = N (θ|µ, σ 2 ).</p>
<p>The red points and arrows show the iterations of our VAN method initialized at µ = −3.2 and σ = 1.5. The θ values corresponding to the value of µ are marked in the top figures where we see that these iterations converge to the global minima of f and avoids other local optima. When we use a Newton's method initialized at the same point, i.e., θ 0 = −3.2, it converges to the local minima at -3.5. VAN can avoid such local minima because it optimizes in the space of µ and σ 2 . The progression of the distribution q is shown in the bottom figure, where darker curves indicate higher iterations. We see that the distribution q is flat in the beginning which enables more exploration which in turn helps the method to avoid local minima. As desired, the distribution peaks around θ * as iterations increase.</p>
<p>We refer to this approach as Variational SGD or simply V-SGD to differentiate it from the standard SGD that optimizes f (θ) in the θ space.</p>
<p>The V-SGD approach is simple and can also work well when used with adaptive-gradient methods to adapt the step-size, e.g., AdaGrad and RMSprop. However, as pointed by Wierstra et al. (2008), it has issues, especially when REINFORCE (Williams, 1992) is used to estimate the gradients of f (θ). Wierstra et al. (2008) argue that the V-SGD update becomes increasingly unstable when the covariance is small, while becoming very small when the covariance is large. To fix these problems, Wierstra et al. (2008) proposed a natural-gradient method. Our method is also a natural-gradient method, but, as we show in the next section, its updates are much simpler and they lead to a second-order method which is similar to Newton's method.</p>
<p>Variational Adaptive-Newton Method</p>
<p>VAN is a natural-gradient method derived using a mirrordescent algorithm. Due to this, the updates of VAN are fundamentally different from V-SGD. We will show that VAN adapts the step-sizes in a very similar spirit to the adaptive-gradient methods. This property will be crucial in establishing connections to Newton's method. VAN can be derived by making two minor modifications to the V-SGD objective. Note that the V-SGD update in (4) and (5) are solutions of following optimization problem:
η t+1 = argmin η η, ∇ η L t + 1 2ρ t η − η t 2 .(6)
The equivalence to the update (4) and (5) can be shown by simply taking the derivative with respect to η of (6) and setting it to zero:
∇ η L t + 1 ρ t (η t+1 − η t ) = 0,(7)
which results in the update (4) and (5). A simple interpretation of this optimization problem is that, in V-SGD, we choose the next point η along the gradient but contain it within a scaled 2 -ball centered at the current point η t . This interpretation enables us to obtain VAN by making two minor modifications to V-SGD.</p>
<p>The first modification is to replace the Euclidean distance · 2 by a Bregman divergence which results in the mirrordescent method. Note that, for exponential-family distributions, the Kullback-Leibler (KL) divergence corresponds to the Bregman divergence (Raskutti and Mukherjee, 2015). Using the KL divergence results in natural-gradient updates which results in better steps when optimizing the parameter of a probability distribution (Amari, 1998).</p>
<p>The second modification is to optimize the VO objective with respect to the mean parameterization of the Gaussian distribution m := {µ, µµ T + Σ} instead of the parameter η := {µ, Σ}. We emphasize that this modification does not change the solution of the optimization problem since the Gaussian distribution is a minimal exponential family and the relationship between η and m is oneto-one (see Section 3.2 and 3.4.1 of Wainwright and Jordan (2008) on the basics of exponential family and meanparameterization respectively).</p>
<p>The two modifications give us the following problem:
m t+1 = argmin m m, ∇ m L t + 1 β t D KL [q q t ],(8)
where q := q(θ|m), q t := q(θ|m t ), and D KL [q q t ] = E q [log(q/q t )] denotes the KL divergence. The convergence of this procedure is guaranteed under mild conditions (Ghadimi et al., 2014).</p>
<p>As shown in Appendix A, a solution to this optimization problem is given by
µ t+1 = µ t − β t Σ t+1 ∇ µ L t (9) Σ −1 t+1 = Σ −1 t + 2β t ∇ Σ L t(10)
The above update differs from those of V-SGD in two ways. First, here we update the precision matrix Σ −1 while in V-SGD we update the covariance matrix Σ. However, both updates use the gradient with respect to Σ. Second, the step-size for µ t+1 are adaptive in the above update since β t is scaled by the covariance Σ t+1 .</p>
<p>The above updates corresponds to a second-order method which is very similar to Newton's method. We can show this using the following identities (Opper and Archambeau, 2009):
∇ µ E q [f (θ)] = E q [∇ θ f (θ)] ,(11)∇ Σ E q [f (θ)] = 1 2 E q ∇ 2 θθ f (θ) .(12)
By substituting these into (9) and (10), we get the following updates which we call the VAN method:
VAN: µ t+1 = µ t − β t P −1 t+1 E qt ∇ θ f (θ) (13) P t+1 = P t + β t E qt ∇ 2 θθ f (θ) (14)
where P t := Σ −1 t is the precision matrix and q t := N (θ|µ t , Σ t ). The precision matrix P t contains a runningsum of the past averaged Hessians, and the searchdirection for the mean is obtained by scaling the averaged gradients by the inverse of P t . If we compare Eq. 9 to the following update of Newton's method,
θ t+1 = θ t − ρ t ∇ 2 θθ f (θ t ) −1 [∇ θ f (θ t )] ,(15)
we can see that the Hessian matrix is replaced by the Precision matrix in the VAN update. Due to this connection to Newton's method and the use of an adaptive scaling matrix P t , we call our method the Variational Adaptive-Newton (VAN) method.</p>
<p>The averaged gradient and running sum of averaged Hessian allow VAN to avoid some types of local optima. Figure 1 shows such a result when minimizing 2 f (θ) = sinc(θ) with initial solution at θ = −3.2. Clearly, both the gradient and Hessian at θ 0 suggest updating θ towards a local optimum at θ = −3.5. However, VAN computes an averaged gradient over samples from q(θ) which yields steeper descent directions pointing towards the global optimum. The adaptive scaling of the steps further ensures a smooth convergence.</p>
<p>The averaging property of VAN is strikingly different from other second-order optimization methods. We expect VAN to be more robust due to averaging of gradients and Hessians. Averaging is particularly useful for optimization of stochastic objectives. For such objectives, application of Newton's method is difficult because reliably selecting a step-size is difficult. Several variants of Newton's method have been proposed to solve this difficulty, e.g., based on quasi-Newton methods (Byrd et al., 2016) or incremental approaches (Gürbüzbalaban et al., 2015), or by simply adapting mini-batch size (Mokhtari et al., 2016). VAN is most similar to the incremental approach of (Gürbüzbalaban et al., 2015) where a running sum of past Hessians is used instead of just a single Hessian. In VAN however the Hessian is replaced by the average of Hessians with respect to q. For stochastic objectives, VAN differs substantially from existing approaches and it has the potential to be a viable alternative to them.</p>
<p>An issue with using the Hessian is that it is not always positive semi-definite, for example, for non-convex problems. For such cases, we can use a Gauss-Newton variant shown below Bertsekas (1999) which we call the Variational Adaptive Gauss-Newton (VAG) Method:
VAG: P t+1 = P t + β t E qt [∇ θ f (θ)] [∇ θ f (θ)] T . (16)</p>
<p>VAN for Large-Scale Problems</p>
<p>Applying VAN to problems with large number of parameters is not feasible because we cannot compute the exact Hessian matrix. In this section, we describe several variants of VAN that scale to large problems. Our variants are similar to existing adaptive-gradient methods such as AdaGrad Duchi et al. (2011) and AROW (Crammer et al., 2009). We derive these variants by using a mean-field approximation for q. Our derivation opens up the possibility of a new framework for designing computationally efficient second-order methods by using structured distributions q.</p>
<p>One of the most common way to obtain scalability is to use a diagonal approximation of the Hessian. In our case, this approximation corresponds to a distribution q with diagonal covariance, i.e., q(θ|η)
= D d=1 N (θ d |µ d , σ 2 d ), where σ 2
d is the variance. This is a common approximation in variational inference methods and is called the mean-field approximation (Bishop, 2006). Let us denote the precision parameters by s d = 1/σ 2 d , and a vector containing them by s. Using this Gaussian distribution in the update (13) and (14), we get the following diagonal version of VAN, which we call VAN-D:
VAN-D: µ t+1 = µ t − β t diag(s t+1 ) −1 E qt [∇ θ f (θ)] s t+1 = s t + β t E qt <a href="17">h(θ)</a>
where diag(s) is a diagonal matrix containing the vector s as its diagonal and h(θ) is the diagonal of the Hessian ∇ 2 θθ f (θ). The VAN-D update requires computation of the expectation of the diagonal of the Hessian, which could still be difficult to compute. Fortunately, we can compute its approximation easily by using the reparameterization trick (Kingma and Ba, 2014). This is possible in our framework because we can express the expectation of the Hessian as gradients of an expectation, as shown below:
E q ∇ 2 θ d θ d f (θ) = 2∇ σ 2 d [E q [f (θ)]] ,(18)= 2∇ σ 2 d E N ( |0,1) [f (µ d + σ d )] , (19) = 2E N ( |0,1) ∇ σ 2 d f (µ d + σ d ) ,(20)
where the first step is obtained using (11). In general, we can use the stochastic approximation by simultaneous perturbation (SPSS) method (Bhatnagar, 2007;Spall, 2000) to compute derivatives. A recent paper by Salimans et al. (2017) showed that this type of computation can also be parallelized which is extremely useful for large-scale learning. Note that these tricks cannot be applied directly to standard continuous-optimization methods. The presence of expectation with respect to q in VO enables us to leverage such stochastic approximation methods for large-scale learning.</p>
<p>Finally, when f (θ) corresponds to a supervised or unsupervised learning problem with large data, we could compute its gradients by using stochastic methods. We use this version of VAN in our large-scale experiments and call it sVAN, which is an abbreviation for stochastic-VAN.</p>
<p>Relationship with Adaptive-Gradient Methods</p>
<p>The VAN-D update given in (17) is closely-related to an adaptive-gradient method called AdaGrad (Duchi et al., 2011) which uses the following updates:
AdaGrad: θ t+1 = θ t − ρ t diag(s t+1 ) −1/2 g(θ t ) (21) s t+1 = s t + <a href="22">g(θ t ) g(θ t )</a>
Comparing these updates to (17), we can see that both Ada-Grad and VAN-D compute the scaling vector s t using a moving average. However, there are some major differences between their updates: 1. VAN-D uses average gradients instead of a gradient at a point, 2. VAN-D does not raise the scaling matrix to the power of 1/2, 3. The update of s t in VAN-D uses the diagonal of the Hessian instead of the squared gradient values used in AdaGrad. It is possible to use the squared gradient in VAN-D but since we can compute the Hessian using the reparameterization trick, we do not have to make further approximations.</p>
<p>VAN can be seen as a generalization of an adaptive method called AROW (Crammer et al., 2009). AROW uses a mirror descent algorithm that is very similar to ours, but has been applied only to problems which use the Hinge Loss.</p>
<p>Our approach not only generalizes AROW but also brings new insights connecting ideas from many different fields.</p>
<p>VAN for Variational Inference</p>
<p>Variational inference (VI) enables a scalable computation of the Bayesian posterior and therefore can also be used for explorative learning. In fact, VI is closely related to VO. In VI, we compute the posterior approximation q(θ|η) for a model p(y, θ) with data y by minimizing the following objective:
min η E q(θ|η) − log p(y, θ) q(θ|η) := L V I (η)(23)
We can perform VI by using VO type of method on the function inside the square bracket. A small difference here is that the function to be optimized also depends on parameters η of q. Conjugate-computation variational inference (CVI) is a recent approach for VI by Khan and Lin (2017). We note that by applying VAN to the variational objective, one recovers CVI. VAN however is more general than CVI since it applies to many other problems other than VI. A direct consequence of our connection is that CVI, just like VAN, is also a second-order method to optimize f V I (θ), and is related to adaptive-gradient methods as well. Therefore, using CVI should give better results than standard methods that use update similar to V-SGD, e.g., black-box VI method of Ranganath et al. (2014).</p>
<p>VAN for Evolution Strategies</p>
<p>VAN performs natural-gradient update in the parameterspace of the distribution q (as discussed earlier in Section 3). The connection to natural-gradients is based on a recent result by Raskutti and Mukherjee (2015) that shows that the mirror descent using a KL divergence for exponential-family distributions is equivalent to a natural-gradient descent. The natural-gradient corresponds to the one obtained using the Fisher information of the exponential family distribution. In our case, the mirror-descent algorithm (8) uses the Bregman divergence that corresponds to the KL divergence between two Gaussians. Since the Gaussian is an exponential-family distribution, mirror descent (8) is equivalent to natural-gradient descent in the dual Riemannian manifold of a Gaussian. Therefore, VAN takes a natural-gradient step by using a mirror-descent step.</p>
<p>Natural Evolution Strategies (NES) (Wierstra et al., 2008) is also a natural-gradient algorithm to solve the VO problem in the context of evolution strategies. NES directly applies natural-gradient descent to optimize for µ and Σ and this yields an infeasible algorithm since the Fisher information matrix has O(D 4 ) parameters. To overcome this issue, Wierstra et al. (2008) proposed a sophisticated reparameterization that reduces the number of parameters to O(D 2 ). VAN, like NES, also has O(D 2 ) parameters, but with much simpler updates rules due to the use of mirror descent in the mean-parameter space.</p>
<p>Applications and Experimental Results</p>
<p>In this section, we apply VAN to a variety of learning tasks to establish it as a general-purpose learning tool, and also to show that it performs comparable to continuous optimization algorithm while extending the scope of their application. Our first application is supervised learning with Lasso regression. Standard second-order methods such as Newton's method cannot directly be applied to such problems because of discontinuity. For this problem, we show that VAN enables stochastic second-order optimization which is faster than existing second-order methods such as iterative-Ridge regression. We also apply VAN to supervised learning with logistic regression and unsupervised learning with Variational Auto-Encoder, and show that stochastic VAN gives comparable results to existing methods such as AdaGrad. Finally, we show two application of VAN for explorative learning, namely active learning for logistic regression and parameter-space exploration for deep reinforcement learning.  (13) and (14). Stochastic VAN implies that the gradients in the updates are estimated by using minibatches of data. The suffix 'Exact' indicates that the expectations with respect to q are computed exactly, i.e., without resorting to Monte Carlo (MC) sampling. This is possible for the Lasso objective as shown in Staines and Barber (2012) and also for logistic regression as shown in Marlin et al. (2011). The suffix 'D' indicates that a diagonal covariance with the update (17) is used. The suffix 'Active' indicates that minibatches are selected using an active learning method. Final, VAG corresponds to the Gauss-Newton type update shown in (16). For all methods, except 'sVAN-Exact', we use MC sampling to approximate the expectation with respect to q. In our plot, we indicate the number of samples by adding it as a suffix, e.g., sVAN-10 is the stochastic VAN method with 10 MC samples.</p>
<p>Supervised Learning: Lasso Regression
Given N example pairs {y i , x i } N i=1 with x i ∈ R D ,
in lasso regression, we minimize the following loss that contains an 1 -regularization:
f (θ) = N i=1 (y i − θ T x i ) 2 + λ D d=1 |θ d |,(24)
Because the function is non-differentiable, we cannot directly apply gradient-based methods to solve the problem. For the same reason, it is also not possible to use secondorder methods such as Newton's method. VAN can be applied to this method since expectation of |θ d | is twice differentiable. We use the gradient and Hessian expression given in Staines and Barber (2012).</p>
<p>We compare VAN and sVAN with the iterative-ridge method (iRidge), which is also a second-order method. We compare on two datasets: Bank32nh (N = 8192, D = 256, N train = 7290, λ = 104.81) and YearPredictionMSD (N = 515345, D = 90, N train = 448350, λ = 5994.84). We set λ values using a validation set where we picked the value that gives minimum error over multiple values of λ on a grid. The iRidge implementation is based on minFunc implementation by Mark Schmidt. For sVAN, the size of the mini-batch used to train Bank32nh and YearPrediction-MSD are M = 30 and M = 122 respectively. We report the absolute difference of parameters, θ −θ * where θ is the parameters estimated by a method and θ * is the parameters optimal value (found by iRidge). For VAN the estimated value is equal to the mean µ of the distribution. Results are shown in Figure 2 (a) and (b), where we observe that VAN and iRidge perform comparably, but sVAN is more data-efficient than them in the first few passes. Results on multiple runs show very similar trends.</p>
<p>In conclusion, VAN enables application of a stochastic second-order method to a non-differentiable problem where existing second-order method and their stochastic versions cannot be applied directly.</p>
<p>Supervised Learning: Logistic Regression</p>
<p>In logistic regression, we minimize the following:
f (θ) = N i=1 log(1 + e yi(θ xi) ) + λ D d=1 θ 2 d ,(25)
where y i ∈ {−1, +1} is the label.</p>
<p>We compare VAN to Newton's method and AdaGrad both of which standard algorithms for batch and stochastic learning, respectively, on convex problems. We use VAN, its stochastic version sVAN, and the diagonal version sVAN-D. We use three real-world datasets from the libSVM database ( Results are shown in Figure 2 (d)-(j). The first row, with plots (d)-(f), shows comparison of Batch methods, where we see that VAN converges at a comparable rate to Newton's method. The second row, with plots (h)-(j), shows the performance of the stochastic learning. Since sVAN uses the full Hessian, it converges faster than sVAN-D and Ada-Grad which use a diagonal approximation. sVAN-D shows overall similar performance to AdaGrad. The main advantage of sVAN over AdaGrad is that sVAN maintains a distribution q(θ) which can be used to evaluate the uncertainty of the learned solution, as shown in the next subsection on active learning.</p>
<p>In conclusion, VAN and sVAN give comparable results to Newton's method while sVAN-D gives comparable results to AdaGrad.</p>
<p>Active Learning for using VAN</p>
<p>An important difference between active and stochastic learning is that an active learning agent can query for its training data examples in each iteration. In active learning for classification, examples in a pool of input data {x i } N i=1 are ranked using an acquisition score which measures how informative an example is for learning. We pick the top M data examples as the mini-batch. Active learning is expected to be more data efficient than stochastic learning since the learning agent focuses on the most informative data samples.</p>
<p>In our experiments, we use the entropy score (Schein and Ungar, 2007) as the acquisition score to select data example for binary logistic regression:
H(x) = − c∈{−1,1} p(y = c|x) log p(y = c|x),(26)
where p(y = c|x) is the estimated probability that the label for input x takes a value y = c. Within our VAN framework, we estimate these probabilities using distributions q(θ) = N (θ|µ t , Σ t ) at iteration t. We use the following approximation that computes the probability using samples from q:
p t (y = c|x) ≈ p(y = c|x, θ)q(θ|µ t , Σ t )dθ (27) ≈ 1 S S s=1 p(y = c|x, θ (s) t ),(28)
where S is the number of MC samples, θ (s) t are sample from q, and p(y|x, θ) is the logistic likelihood. Figure 3 compares the performance on on the USPS dataset with active learning by VAN for mini-batch of 10 examples. The result clearly shows that VAN with active learning is much more data efficient and stable than VAN with stochastic learning.</p>
<p>Unsupervised Learning with Variational Auto-Encoder</p>
<p>We apply VAN to optimize the parameters of variational auto-encoder (VAE) (Kingma and Welling, 2013). Given observations {y i } N i=1 ∼ p(y), VAE models the data points using a neural-network decoder p(y i |z i , θ d ) with input z i and parameters θ d . The input z i are probabilistic and follow a prior p(z i ). The encoder is also parameterized with a neural-network but follows a different distribution q(z i |y i , θ e ) with parameters θ e . The goal is to learn θ := {θ d , θ e } by minimizing the following,
f (θ) = − N i=1 E q(zi|y i ,θe) [log p(y i |z i , θ d )] +D KL [q(z i |y i , θ e ) p(z i )]](29)
where z i is the latent vector, and θ = {θ 1 , θ 2 } are parameters to be learned. Similar to previous work, we assume p(z i ) to be a standard Gaussian, and use a Gaussian en- We train the model for the binary MNIST dataset (N = 70, 000, D = 256, N train = 60, 000) with mini-batches of size M = 100 and set the dimensionality of the latent variable to K = 3. and measure the learning performance by the imputed log-loss of the test set using a procedure similar to Rezende and Mohamed (2015).</p>
<p>We compare our methods to adaptive-gradient methods, namely AdaGrad (Duchi et al., 2011) andRMSprop (Tieleman andHinton, 2012). For all the methods, we tune the step-size using a validation set and report the test log-loss after convergence. Figure 2 (c) shows the results of 5 runs of all methods. We see that all methods perform similar to each other. RMSprop has high variance among all methods. sVAN-D-1 is slightly worse than sVAG-D-1, which is expected since for nonconvex problems Gauss-Newton is a better approximation than using the Hessian.</p>
<p>Parameter Based Exploration in Deep Reinforcement Learning</p>
<p>Exploration is extremely useful in reinforcement learning (RL), especially in environment where obtaining informative feedback is rare. Unlike most learning tasks where a training dataset is readily available, RL agents needs to explore the state-action space to collect data. An explorative agent that always chooses random actions might never obtain good data for learning the optimal policy. On the other hand, an exploitative agent that always chooses the current optimal action(s) may never try suboptimal actions that can lead to learning a better policy. Thus, striking a balance between exploration and exploitation is a must for an effective learning. However, finding such a balance is an open problem in RL.</p>
<p>In this section, we apply VAN to enable efficient exploration by using the parameter-based exploration (Rückstieß et al., 2010). In standard RL setup, we wish to learn a parameter θ of a parametric policy π for action a = π(s; θ) given state s. We seek an optimal θ such that a state-action sequence (s 1 , a 1 , s 2 . . . ) maximizes the expected returns where s t and a t are state and action at time t, respectively. To facilitate exploration, a common approach is to perturb the action by a random noise , e.g., we can simply add it to the action a = π(s; θ) + . In contrast, in parameterbased exploration, exploration is facilitated in the parameter space as the name suggest, i.e., we sample the policy parameter θ from a distribution N (θ|µ, Σ). Our goal therefore is to learn the distribution parameters µ and Σ.</p>
<p>The parameter-based exploration is better than the actionspace exploration because in the former a small perturbation of a parameter result in significant explorative behaviour in the action space. However, to get a similar behaviour through an action-space exploration, the extent of the noise needs to be very large which leads to instability (Rückstieß et al., 2010).</p>
<p>The existing methods for parameter-based exploration independently learn µ and Σ (similar to V-SGD) (Rückstieß et al., 2010;Plappert et al., 2017;Fortunato et al., 2017). However, this method can be increasingly unstable as learning progresses (as discussed in Section 2). Since VAN exploits the geometry of the Gaussian distribution to jointly learn µ and Σ, we expect it to perform better than the existing methods that use V-SGD.</p>
<p>We consider the deep deterministic policy gradient (DDPG) method (Silver et al., 2014;Lillicrap et al., 2015), where a local optimal θ is obtained by minimizing
f (θ) = −E p(s) Q(s, π(s, θ)) ,(30)
where Q(s, a) is an estimated of the expected return and p(s) a state distribution. Both π and Q are neural networks. We compare the performance of sVAN and sVAG against two baseline methods denoted by SGD and V-SGD. SGD refers to DPG where there are no exploration at all, and V-SGD refers to an extension of DPG where the mean and covariance of the parameter distribution are learned using the update (4) and (5). Due to the large number of neural networks parameters, we use diagonal approximation to the Hessian for all methods. More details of experiments are given in Appendix B.</p>
<p>The result in Figure 4 shows the performance on Half-Cheetah from OpenAI Gym (Brockman et al., 2016) for 5 runs, where we see that VAN based methods significantly outperform existing methods. sVAN-D-1 and sVAG-D-1 both perform equally well. This suggests that both Hessian approximations obtained by using the reparameterization trick shown in (20) and the Gauss-Newton approximation shown in (16), respectively, are equally accurate for this problem. We can also see that sVAN-D-10 has better data efficiency than sVAN-D-1 especially in the early stages of learning. V-SGD is able to find a good policy during learning but has unstable performance that degenerates over time, as mentioned previously. On the other hand, SGD performs very poorly and learns a suboptimal solution. This strongly suggests that good exploration strategy is crucial to learn good policies of Half-Cheetah.</p>
<p>We also tried these comparisons on the 'pendulum' problem in OpenAI Gym where we did not observe significant advantages from explorations. We believe that this is because this problem does not benefit from using exploration and pure-exploitative methods are good enough for these problems. More extensive experiments are required to validate the results presented in this section.</p>
<p>Discussion and Conlcusions</p>
<p>We proposed a general-purpose explorative-learning method called VAN. VAN is derived within the variationaloptimization problem by using a mirror-descent algorithm. We showed that VAN is a second-order method and is related to existing methods in continuous optimization, variational inference, and evolution strategies. We proposed computationally-efficient versions of VAN for large-scale learning. Our experimental results showed that VAN works reasonably well on a wide-variety of learning problems.</p>
<p>For problems with high-dimensional parameters θ, computing and inverting the full Hessian matrix is computationally infeasible. One line of possible future work is to develop versions of VAN that can deal with issue without making a diagonal approximation.</p>
<p>It is straightforward to extend VAN to non-Gaussian distributions. Our initial work, not discussed in this paper, suggests that student-t distribution and Cauchy distribution could be useful candidate. However, it is always possible to use other types of distributions, for example, to minimize discrete optimization problem within a stochastic relaxation framework (Geman and Geman, 1984).</p>
<p>Another venue is to evaluate the impact of exploration on fields such as active learning and reinforcement learning.</p>
<p>In this paper, we have provided some initial results. An extensive application of the methods developed in this paper to real-world problems is required to further understand their advantages and disadvantages compared to existing methods.</p>
<p>The main strength of VAN lies in exploration, using which it can potentially accelerate and robustify optimization. Compared to Bayesian methods, VAN offers a computationally cheap alternative to perform explorative learning. Using such cheap explorations, VAN has the potential to solve difficult learning problems such as deep reinforcement learning, active learning, and life-long learning.</p>
<p>A Derivation of VAN</p>
<p>Denote the mean parameters of q t (θ) by m t which is equal to the expected value of the sufficient statistics φ(θ), i.e., m t := E qt [φ(θ)]. The mirror descent update at iteration t is given by the solution to
m t+1 = argmin m m, ∇ m L t + 1 β t D KL [q q t ] (31) = argmin m E q φ(θ), ∇ m L t + log (q/q t ) 1/βt (32) = argmin m E q   log exp φ(θ), ∇ m L t q 1/βt q 1/βt t   (33) = argmin m E q   log   q 1/βt q 1/βt t exp φ(θ), − ∇ m L t     (34) = argmin m 1 β t E q   log   q q t exp φ(θ), −β t ∇ m L t     (35) = argmin m 1 β t D KL q q t exp φ(θ), −β t ∇ m L t /Z t .(36)
where Z is the normalizing constant of the distribution in the denominator which is a function of the gradient and step size.</p>
<p>Minimizing this KL divergence gives the update
q t+1 (θ) ∝ q t (θ) exp φ(θ), −β t ∇ m L t .(37)
By rewriting this, we see that we get an update in the natural parameters λ t of q t (θ), i.e.
λ t+1 = λ t − β t ∇ m L t .(38)
Recalling that the mean parameters of a Gaussian q(θ) = N (θ|µ, Σ) are m (1) = µ and M (2) = Σ + µµ T and using the chain rule, we can express the gradient ∇ m L t in terms of µ and Σ,
∇ m (1) L = ∇ µ L − 2 ∇ Σ L µ (39) ∇ M (2) L = ∇ Σ L.(40)
Finally, recalling that the natural parameters of a Gaussian q(θ) = N (θ|µ, Σ) are λ (1) = Σ −1 µ and λ (2) = − 1 2 Σ −1 , we can rewrite the VAN updates in terms of µ and Σ,
Σ −1 t+1 = Σ −1 t + 2β t ∇ Σ L t (41) µ t+1 = Σ t+1 Σ −1 t µ t − β t ∇ µ L t − 2 ∇ Σ L t µ t (42) = Σ t+1 Σ −1 t + 2β t ∇ Σ L t µ t − β t Σ t+1 ∇ µ L t (43) = µ t − β t Σ t+1 ∇ µ L t .(44)</p>
<p>B Details on the RL experiment</p>
<p>In this section, we give details of the parameter-based exploration task in reinforcement learning (RL). An important open question in reinforcement learning is how to efficiently explore the state and action space. An agent always acting greedily according to the policy results in a pure exploitation. Exploration is necessary to visit inferior state and actions once in while to see if they might really be better. Traditionally, exploration is performed in the action space by, e.g., injecting noise to the policy output. However, injecting noise to the action space may not be sufficient in problems where the reward is sparse, i.e., the agent rarely observes the reward of their actions. In such problems, the agent requires a rich explorative behaviour in which noises in the action space cannot provide. An alternative approach is to perform exploration in the parameter space (Rückstieß et al., 2010). In this section, we demonstrate that variational distribution q(θ) obtained using VAN can be straightforwardly used for such exploration in parameter space, θ.</p>
<p>B.1 Background</p>
<p>First, we give a brief background on reinforcement learning (RL). RL aims to solve the sequential decision making problem where at each discrete time step t an agent observes a state s t and selects an action a t using a policy π, i.e., a t ∼ π(a|s t ).</p>
<p>The agent then receives an immediate reward r t = r(s t , a t ) and observes a next state s t ∼ p(s |s t , a t ). The goal in RL is to learn the optimal policy π * which maximizes the expected returns E ∞ t γ t−1 r t where γ is the discounted factor and the expectation is taken over a sequence of densities π(a|s t ) and p(s |s t , a t ).</p>
<p>A central component of RL algorithms is the state-action value function or the Q-function Q π (s, a) gives the expected return after executing an action a in a state s and following the policy π afterwards. Formally, it is defined as follows:
Q π (s, a) = E ∞ t=1 γ t−1 r t |s 1 = s, a 1 = a .(45)
The Q-function also satisfies a recursive relation also known as the Bellman equation:</p>
<p>Q π (s, a) = r(s, a) + γE p(s |s,a),π(a |s ) [Q π (s , a )] .</p>
<p>Using the Q-function, the goal of reinforcement learning can be simply stated as finding a policy which maximizes the expected Q-function, i.e., π * = argmax π p(s)π(a|s)Q π (s, a)dsda.</p>
<p>In practice, the policy is represented by a parameterized function such as neural networks with policy parameter θ and the goal is to instead find the optimal parameters θ .</p>
<p>B.1.1 Deterministic Policy Gradients</p>
<p>Our parameter-based exploration via VAN can be applied to any reinforcement learning algorithms which rely on gradient ascent to optimize the policy parameter θ. For demonstration, we focus on a simple yet efficient algorithm called the deterministic policy gradients algorithm (DPG) (Silver et al., 2014). Simply speaking, DPG aims to find a deterministic policy that maximizes the action-value function by gradient ascent. Since in practice the action-value function is unknown, DPG learns a function approximator Q v (s, a) with a parameter v such that Q v (s, a) ≈ Q π θ (s, a). Then, DPG finds θ * which locally minimize an objective f (θ) = −E p(s) [Q v (s, π θ (s))] by gradient ascent where the gradient is given by ∇ θ f (θ) = −∇ θ p(s)Q v (s, π θ (s))ds = − p(s)∇ θ Q v (s, π θ (s))ds = − p(s)∇ a Q v (s, a)| a=π θ (s) ∇ θ π θ (s)ds.</p>
<p>The parameter v of Q v (s, a) may be learned by any policy evaluation methods. Here, we adopted the approach proposed by (Lillicrap et al., 2015) which minimizes the squared Bellman residual to the slowly moving target action-value function. More precisely, v is updated by </p>
<p>where the expectation is taken over p(s) and p(s |s, a). TheQṽ andπθ denote target networks which are separate function approximators that slowly tracks Q v and π θ , respectively. The target networks help at stabilizing the learning procedure (Lillicrap et al., 2015).</p>
<p>Algorithm 1 Parameter-based exploration DPG via VAN 1: while Not converged do 2:</p>
<p>Observe state s t , sample parameter θ ∼ N (θ|µ, Σ), take action a t = π θ (s t ), observe reward r t and next state s t .</p>
<p>3:</p>
<p>Add (s t , a t , r t , s t ) to a replay buffer D.</p>
<p>4:</p>
<p>for i = 1, . . . , K do 5:</p>
<p>Drawn N minibatch samples {(s i , a i , r i , s i )} N i=1 from D.</p>
<p>6:</p>
<p>Update Q v (s, a) by gradient descent:
v ← v + α v ∇ v 1 N N i=1 (Q v (s i , a i ) − y i ) 2 ,
where y i = r(s i , a i ) + γQṽ(s i , πμ(s i )).</p>
<p>7:</p>
<p>Update parameter of q(θ) by sVAN in Eq. (17) with
∇ σ L(µ t , σ 2 t ) = − 1 N M N i=1 M j=1 ∇ σ π θj (s i )∇ a Q v (s i , a)| a=π θ j (si) ,(51)∇ µ L(µ t , σ 2 t ) = − 1 N M N i=1 M j=1 ∇ µ π θj (s i )∇ a Q v (s i , a)| a=π θ j (si) ,(52)
where {θ j } M j=1 ∼ q(θ).</p>
<p>8:</p>
<p>Update target network parametersṽ andμ by moving average with, e.g., step size τ = 0.001:</p>
<p>v ← (1 − τ )ṽ + τ v, µ ← (1 − τ )μ + τ µ.</p>
<p>9:</p>
<p>end for 10: end while Overall, DPG is an actor-critic algorithm that iteratively update the critic (action-value function) by taking gradient of Eq.(49) and update the actor (policy) by the gradient Eq.(48). However, the crucial issue of DPG is that it uses a deterministic policy and does not perform exploration by itself. In practice, exploration is done for DPG by injecting a noise to the policy output, i.e., a = π θ (s) + where is a noise from some random process such as Gaussian noise. However, as discussed about, action-space noise may be insufficient in some problems. Next, we show that VAN can be straightforwardly applied to DPG to obtain parameter-based exploration DPG.</p>
<p>B.1.2 Parameter-based Exploration DDPG</p>
<p>To perform parameter-based exploration, we can relax the policy-gradient objective f (θ) by assuming that the parameter θ is sampled from a distribution q(θ) := N (θ|µ, Σ), and solve the following optimization problem:
min µ,Σ E N (θ|µ,Σ) <a href="50">f (θ)</a>
This is exactly the VO problem of (2). The stochasticity of θ through q(θ) allows the agent to explore the state and action space by varying its policy parameters. This exploration strategy is advantageous since the agent can now exhibits much more richer explorative behaviours when compared with exploration by action noise injection.</p>
<p>Algorithm 1 outlines parameter-based exploration DPG via VAN.</p>
<p>(θ|η) [f (θ)] := L(η).</p>
<p>Figure 1 :
1Illustrative application of VAN on an example by</p>
<p>Chang and Lin, 2011): 'breast-cancerscale' (N = 683, D = 10, N train = 341, λ = 1.88), 'USPS' (N = 1, 540, D = 256, N train = 770, λ = 6.21), and 'a1a' (N = 32, 561, D = 123, N train = 1, 605, λ = 67.23). We compare the log-loss on the test set computed as follows: i log(1+exp(y i (θ T x i ))/N test whereθ is the parameter estimate and N test is the number of examples in the test set. For sVAN and sVAN-D, we use a mini-batch size of 10 for 'breast-cancer-scale' dataset and a mini-batch of size 100 for the rest of the datasets.</p>
<p>Figure 2 :
2Experimental results on different learning tasks. (a-b): Lasso regression. (c): VAE. (d-f): Logistic regression with VAN and Newton's method. (h-j): Logistic regression with sVAN, sVAN-D and AdaGrad. Datasets are specified in the title. M refers to the mini-batch size for stochastic methods.</p>
<p>Figure 3 :Figure 4 :
34Active learning on logistic regression where we see that sVAN with active learning gives good results with fewer number of passes through the data. Parameter-based exploration for DDPG where we see than VAN based methods perform better than exploration using V-SGD or plain SGD with no exploration.coder and a Bernoulli decoder, both parameterized by neutral networks.</p>
<p>E
Q v (s, a) − r(s, a) − γQṽ(s ,πθ(s )) 2 ,</p>
<p>Table 1 :
1Variants of VAN used in our experiments. Exact Stochastic VAN with no MC sampling. sVAN-D Stochastic VAN with diagonal covariance Σ. sVAN-Active Stochastic VAN using active learning. sVAG Stochastic Variational Adaptive Gauss-Newton Method. sVAG-D Stochastic VAG with diagonal covariance Σ.Method 
Description 
VAN 
Variational Adaptive-Newton Method using (13) and (14). 
sVAN 
Stochastic VAN (gradient estimated using mini-batches). 
sVAN-</p>
<p>Table 1
1summarizes various versions of VAN compared in our experiments. The first method is the VAN method which implements the update shown in
This is a black-box optimization problem in the sense that we may not have access to an analytical form of the function or its derivatives, but we might be able to approximate them at a point.
This example is discussed in a blog byHuszar (2017) </p>
<p>Natural gradient works efficiently in learning. S.-I Amari, Neural computation. 102Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural computation, 10(2):251-276.</p>
<p>Nonlinear programming. D P Bertsekas, Athena Scientific. Bertsekas, D. P. (1999). Nonlinear programming. Athena Scientific.</p>
<p>Adaptive Newton-based multivariate smoothed functional algorithms for simulation optimization. S Bhatnagar, ACM Transactions on Modeling and Computer Simulation (TOMACS). 1812Bhatnagar, S. (2007). Adaptive Newton-based multivari- ate smoothed functional algorithms for simulation op- timization. ACM Transactions on Modeling and Com- puter Simulation (TOMACS), 18(1):2.</p>
<p>C M Bishop, Pattern recognition. Machine Learning. 128Bishop, C. M. (2006). Pattern recognition. Machine Learn- ing, 128:1-58.</p>
<p>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, De Freitas, N , arXiv:1012.2599arXiv preprintBrochu, E., Cora, V. M., and De Freitas, N. (2010). A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599.</p>
<p>Openai gym. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym.</p>
<p>A stochastic quasi-Newton method for largescale optimization. R H Byrd, S L Hansen, J Nocedal, Y Singer, SIAM Journal on Optimization. 262Byrd, R. H., Hansen, S. L., Nocedal, J., and Singer, Y. (2016). A stochastic quasi-Newton method for large- scale optimization. SIAM Journal on Optimization, 26(2):1008-1031.</p>
<p>LIBSVM: A library for support vector machines. C.-C Chang, C.-J Lin, 27. Software available at. 2Chang, C.-C. and Lin, C.-J. (2011). LIBSVM: A li- brary for support vector machines. ACM Transac- tions on Intelligent Systems and Technology, 2:27:1- 27:27. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</p>
<p>Adaptive regularization of weight vectors. K Crammer, A Kulesza, M Dredze, Advances in neural information processing systems. Crammer, K., Kulesza, A., and Dredze, M. (2009). Adap- tive regularization of weight vectors. In Advances in neu- ral information processing systems, pages 414-422.</p>
<p>Adaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer, The Journal of Machine Learning Research. 12Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive sub- gradient methods for online learning and stochastic op- timization. The Journal of Machine Learning Research, 12:2121-2159.</p>
<p>Bayesian exploration for intelligent identification of textures. J A Fishel, G E Loeb, Frontiers in neurorobotics, 6Fishel, J. A. and Loeb, G. E. (2012). Bayesian exploration for intelligent identification of textures. Frontiers in neu- rorobotics, 6.</p>
<p>M Fortunato, M G Azar, B Piot, J Menick, I Osband, A Graves, V Mnih, R Munos, D Hassabis, O Pietquin, arXiv:1706.10295Noisy networks for exploration. arXiv preprintFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295.</p>
<p>Deep Bayesian Active Learning with Image Data. Y Gal, R Islam, Z Ghahramani, ArXiv eprintsGal, Y., Islam, R., and Ghahramani, Z. (2017). Deep Bayesian Active Learning with Image Data. ArXiv e- prints.</p>
<p>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. S Geman, D Geman, IEEE Transactions on pattern analysis and machine intelligence. 16Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of im- ages. IEEE Transactions on pattern analysis and ma- chine intelligence, 1(6):721-741.</p>
<p>Minibatch stochastic approximation methods for nonconvex stochastic composite optimization. S Ghadimi, G Lan, H Zhang, Mathematical Programming. Ghadimi, S., Lan, G., and Zhang, H. (2014). Mini- batch stochastic approximation methods for nonconvex stochastic composite optimization. Mathematical Pro- gramming, pages 1-39.</p>
<p>A globally convergent incremental Newton method. M Gürbüzbalaban, A Ozdaglar, P Parrilo, Mathematical Programming. 1511Gürbüzbalaban, M., Ozdaglar, A., and Parrilo, P. (2015). A globally convergent incremental Newton method. Math- ematical Programming, 151(1):283-313.</p>
<p>Completely derandomized self-adaptation in evolution strategies. N Hansen, A Ostermeier, Evolutionary computation. 92Hansen, N. and Ostermeier, A. (2001). Completely deran- domized self-adaptation in evolution strategies. Evolu- tionary computation, 9(2):159-195.</p>
<p>Bayesian Active Learning for Classification and Preference Learning. N Houlsby, F Huszár, Z Ghahramani, M Lengyel, ArXiv e-printsHoulsby, N., Huszár, F., Ghahramani, Z., and Lengyel, M. (2011). Bayesian Active Learning for Classification and Preference Learning. ArXiv e-prints.</p>
<p>Evolution Strategies, Variational Optimisation and Natural ES. F Huszar, Huszar, F. (2017). Evolution Strategies, Variational Optimisation and Natural ES. http://www.inference.vc/ evolution-strategies-variational-optimisation-and-natural-es-2/.</p>
<p>Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. M E Khan, W Lin, arXiv:1703.04265arXiv preprintKhan, M. E. and Lin, W. (2017). Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models. arXiv preprint arXiv:1703.04265.</p>
<p>Adam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Auto-encoding variational Bayes. D P Kingma, M Welling, arXiv:1312.6114arXiv preprintKingma, D. P. and Welling, M. (2013). Auto-encoding vari- ational Bayes. arXiv preprint arXiv:1312.6114.</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, abs/1509.02971CoRRLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Contin- uous control with deep reinforcement learning. CoRR, abs/1509.02971.</p>
<p>Piecewise bounds for estimating Bernoulli-logistic latent Gaussian models. B Marlin, M Khan, Murphy , K , International Conference on Machine Learning. Marlin, B., Khan, M., and Murphy, K. (2011). Piecewise bounds for estimating Bernoulli-logistic latent Gaussian models. In International Conference on Machine Learn- ing.</p>
<p>Adaptive Newton method for empirical risk minimization to statistical accuracy. A Mokhtari, H Daneshmand, A Lucchi, T Hofmann, A Ribeiro, Advances in Neural Information Processing Systems. Mokhtari, A., Daneshmand, H., Lucchi, A., Hofmann, T., and Ribeiro, A. (2016). Adaptive Newton method for empirical risk minimization to statistical accuracy. In Advances in Neural Information Processing Systems, pages 4062-4070.</p>
<p>The Variational Gaussian Approximation Revisited. M Opper, C Archambeau, Neural Computation. 213Opper, M. and Archambeau, C. (2009). The Variational Gaussian Approximation Revisited. Neural Computa- tion, 21(3):786-792.</p>
<p>A tutorial introduction to Bayesian models of cognitive development. A Perfors, J B Tenenbaum, T L Griffiths, F Xu, Cognition. 1203Perfors, A., Tenenbaum, J. B., Griffiths, T. L., and Xu, F. (2011). A tutorial introduction to Bayesian models of cognitive development. Cognition, 120(3):302-321.</p>
<p>M Plappert, R Houthooft, P Dhariwal, S Sidor, R Y Chen, X Chen, T Asfour, P Abbeel, Andrychowicz , M , arXiv:1706.01905Parameter space noise for exploration. arXiv preprintPlappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychow- icz, M. (2017). Parameter space noise for exploration. arXiv preprint arXiv:1706.01905.</p>
<p>Black box variational inference. R Ranganath, S Gerrish, D M Blei, International conference on Artificial Intelligence and Statistics. Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In International conference on Artificial Intelligence and Statistics, pages 814-822.</p>
<p>The information geometry of mirror descent. G Raskutti, S Mukherjee, IEEE Transactions on Information Theory. 613Raskutti, G. and Mukherjee, S. (2015). The information geometry of mirror descent. IEEE Transactions on In- formation Theory, 61(3):1451-1457.</p>
<p>D J Rezende, S Mohamed, arXiv:1505.05770Variational inference with normalizing flows. arXiv preprintRezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770.</p>
<p>Exploring parameter space in reinforcement learning. T Rückstieß, F Sehnke, T Schaul, D Wierstra, Y Sun, J Schmidhuber, Paladyn. 11Rückstieß, T., Sehnke, F., Schaul, T., Wierstra, D., Sun, Y., and Schmidhuber, J. (2010). Exploring parameter space in reinforcement learning. Paladyn, 1(1):14-24.</p>
<p>Evolution Strategies as a Scalable Alternative to Reinforcement Learning. T Salimans, J Ho, X Chen, S Sidor, I Sutskever, ArXiv e-printsSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. ArXiv e-prints.</p>
<p>Active learning for logistic regression: an evaluation. A I Schein, L H Ungar, Machine Learning. 68Schein, A. I. and Ungar, L. H. (2007). Active learning for logistic regression: an evaluation. Machine Learning, 68(3):235-265.</p>
<p>Deterministic policy gradient algorithms. D Silver, G Lever, N Heess, T Degris, D Wierstra, M A Riedmiller, Proceedings of the 31th International Conference on Machine Learning. the 31th International Conference on Machine LearningBeijing, ChinaSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. A. (2014). Deterministic policy gra- dient algorithms. In Proceedings of the 31th Interna- tional Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 387-395.</p>
<p>Adaptive stochastic approximation by the simultaneous perturbation method. J C Spall, IEEE transactions on automatic control. 4510Spall, J. C. (2000). Adaptive stochastic approximation by the simultaneous perturbation method. IEEE transac- tions on automatic control, 45(10):1839-1853.</p>
<p>Variational Optimization. J Staines, D Barber, ArXiv e-printsStaines, J. and Barber, D. (2012). Variational Optimization. ArXiv e-prints.</p>
<p>A Bayesian framework for reinforcement learning. M Strens, Proceedings of the Seventeenth International Conference on Machine Learning. the Seventeenth International Conference on Machine LearningICMLStrens, M. (2000). A Bayesian framework for reinforce- ment learning. In In Proceedings of the Seventeenth International Conference on Machine Learning, pages 943-950. ICML.</p>
<p>Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude. T Tieleman, G Hinton, COURSERA: Neural Networks for Machine Learning 4. Tieleman, T. and Hinton, G. (2012). Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4.</p>
<p>Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning. M J Wainwright, M I Jordan, Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1-2:1- 305.</p>
<p>Natural evolution strategies. D Wierstra, T Schaul, J Peters, J Schmidhuber, Evolutionary Computation. IEEEWierstra, D., Schaul, T., Peters, J., and Schmidhuber, J. (2008). Natural evolution strategies. In Evolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelligence). IEEE Congress on, pages 3381-3387. IEEE.</p>
<p>Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4Williams, R. J. (1992). Simple statistical gradient- following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Exploration and inference in learning from reinforcement. J Wyatt, University of Edinburgh. College of Science and Engineering. School of InformaticsPhD thesisWyatt, J. (1998). Exploration and inference in learning from reinforcement. PhD thesis, University of Edin- burgh. College of Science and Engineering. School of Informatics.</p>            </div>
        </div>

    </div>
</body>
</html>
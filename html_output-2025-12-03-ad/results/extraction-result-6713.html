<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6713 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6713</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6713</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-ed81c644692ec88f59e287366c8cba619948295a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ed81c644692ec88f59e287366c8cba619948295a" target="_blank">Direct Preference Optimization with an Offset</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning and significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.</p>
                <p><strong>Paper Abstract:</strong> Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6713.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6713.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Preference Optimization with an Offset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalization of Direct Preference Optimization (DPO) that enforces the estimated reward of a preferred response to exceed that of a dispreferred response by an offset determined from the difference in their reward/quality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Preference-based fine-tuning with offset (ODPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>contrastive / preference-optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (weights preference pairs by magnitude of preference)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMDB sentiment-control (bootstrapped preference pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Steer generated movie-review continuations toward positive sentiment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average positive-sentiment probability (reward); KL divergence to SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.76</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.04</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ODPO uses a per-pair offset proportional to reward difference (here log-scaled); in the sentiment-control experiments ODPO more frequently appears on the reward-vs-KL Pareto frontier, particularly with smaller HF datasets (|D_HF|=5000). Ablations show log-scaling of offsets and α=1 were effective tradeoffs between reward and KL.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6713.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6713.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Preference Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to directly fine-tune a language model on pairwise human preference data without training a separate reward model or using RL algorithms, by maximizing the Bradley–Terry likelihood expressed in terms of model log-probability ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Direct preference optimization: Your language model is secretly a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Preference-based fine-tuning (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>contrastive / preference-optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous (treats every preference pair equally)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMDB sentiment-control (bootstrapped preference pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Steer generated movie-review continuations toward positive sentiment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average positive-sentiment probability (reward); KL divergence to SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.72</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ODPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.04</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>DPO optimizes pairwise ordering but ignores magnitude of preference; compared to ODPO it generally attains lower reward for similar KL, especially with smaller preference datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6713.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6713.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODPO_toxicity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ODPO (toxicity control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of ODPO to reduce toxic generations by using toxicity classifier scores to set per-pair offsets (log-scaled difference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-neo-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-neo-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Preference-based fine-tuning with offset (ODPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>contrastive / preference-optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (per-pair offsets derived from toxicity-score differences)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>REALToxicityPrompts (adversarial prompts subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce toxicity of model generations for adversarial prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average toxicity probability; KL divergence to SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>When using medium-size HF datasets (8000 or 9000 bootstrapped pairs) ODPO substantially reduces toxicity (authors state 'cutting it almost by half') while maintaining closer KL to SFT compared to DPO; as HF dataset size grows performance of DPO approaches ODPO.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6713.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6713.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPO_toxicity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DPO (toxicity control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of original DPO to reduce toxic generations using bootstrapped pairwise preferences from a toxicity classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Direct preference optimization: Your language model is secretly a reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-neo-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-neo-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Preference-based fine-tuning (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>contrastive / preference-optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous (no per-pair magnitude weighting)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>REALToxicityPrompts (adversarial prompts subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce toxicity of model generations for adversarial prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average toxicity probability; KL divergence to SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ODPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>DPO struggled to materially reduce toxicity in the limited-data settings where ODPO succeeded; increasing HF data narrows the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6713.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6713.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODPO_summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ODPO (summarization with human Likert offsets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ODPO applied to summarization where the offset between two summaries is set from human Likert-scale score differences; evaluated via GPT-4 win-rate against human-written references.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTJ-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPTJ-6B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Preference-based fine-tuning with offset (ODPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>contrastive / preference-optimization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (offsets from human Likert score differences)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>REDDIT TL;DR (human-rated subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate summaries preferred by humans (quality judged on 7-point Likert scale); compare generated summaries to human references</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>win rate against human-written summaries (GPT-4 judged); head-to-head win rate vs DPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>53.6</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>DPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Across sampling temperatures ODPO had higher average win rates vs human-written summaries than DPO; authors report ODPO significantly outperforms DPO at temperatures 0 and 0.5. Head-to-head ODPO win rates over DPO at temperatures [0,0.25,0.5,0.75,1] are [51,50,62,48,57] percent respectively (per-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6713.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6713.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SLiC-HF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence Likelihood Calibration with Human Feedback (SLiC-HF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-level contrastive method that calibrates sequence likelihoods to prefer better outputs by enforcing an offset hyperparameter δ between preferred and dispreferred responses (δ is a fixed hyperparameter, not response-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SLiC-HF: Sequence likelihood calibration with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Sequence likelihood calibration (SLiC-HF)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>contrastive / margin-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous (fixed margin δ independent of pair)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>IMDB sentiment-control (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Steer generations toward positive sentiment (same experimental setup as DPO/ODPO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average positive-sentiment probability (reward); KL divergence to SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.71</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ODPO</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.05</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Compared in Table 1: SLiC runs did not appear on the Pareto frontier for reward vs KL in the experiments; SLiC uses a fixed margin rather than response-dependent offsets, which the authors contrast with ODPO's per-pair offsets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6713.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6713.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Related_methods_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Related RLHF / preference methods (PPO, RLHF, IPO, KTO, RRHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related methods mentioned in related work: RLHF using PPO and reward models; IPO, KTO, RRHF are variants/alternatives to DPO and sequence-level contrastive approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RLHF (PPO); Identity Preference Optimization (IPO); Kahneman-Tversky Optimization (KTO); RRHF</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>varied (RL, contrastive, unpaired-preference)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (different approaches to use preference data; some treat pairs uniformly, some use alternative losses or unpaired data)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>These methods are discussed in related work: PPO-based RLHF requires a reward model and RL tuning; IPO bounds DPO's unbounded preference function; KTO uses unpaired labeled examples; RRHF uses ranking losses without offsets. The paper positions ODPO relative to these by emphasizing per-pair offseting of DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Direct Preference Optimization with an Offset', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Direct preference optimization: Your language model is secretly a reward model. <em>(Rating: 2)</em></li>
                <li>SLiC-HF: Sequence likelihood calibration with human feedback <em>(Rating: 2)</em></li>
                <li>RRHF: Rank responses to align language models with human feedback without tears <em>(Rating: 2)</em></li>
                <li>Learning to summarize with human feedback <em>(Rating: 2)</em></li>
                <li>RealToxicityPrompts: Evaluating neural toxic degeneration in language models <em>(Rating: 1)</em></li>
                <li>Identity Preference Optimization (IPO) <em>(Rating: 1)</em></li>
                <li>Human-centered loss functions (HALOs) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6713",
    "paper_id": "paper-ed81c644692ec88f59e287366c8cba619948295a",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "ODPO",
            "name_full": "Direct Preference Optimization with an Offset",
            "brief_description": "A generalization of Direct Preference Optimization (DPO) that enforces the estimated reward of a preferred response to exceed that of a dispreferred response by an offset determined from the difference in their reward/quality scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-Large",
            "model_size": "GPT2-Large",
            "reasoning_method_name": "Preference-based fine-tuning with offset (ODPO)",
            "reasoning_method_type": "contrastive / preference-optimization",
            "reasoning_style_diversity": "diverse (weights preference pairs by magnitude of preference)",
            "benchmark_name": "IMDB sentiment-control (bootstrapped preference pairs)",
            "task_description": "Steer generated movie-review continuations toward positive sentiment",
            "performance_metric": "average positive-sentiment probability (reward); KL divergence to SFT",
            "performance_value": 0.76,
            "comparison_target_method": "DPO",
            "performance_difference": 0.04,
            "statistical_significance": null,
            "analysis_notes": "ODPO uses a per-pair offset proportional to reward difference (here log-scaled); in the sentiment-control experiments ODPO more frequently appears on the reward-vs-KL Pareto frontier, particularly with smaller HF datasets (|D_HF|=5000). Ablations show log-scaling of offsets and α=1 were effective tradeoffs between reward and KL.",
            "ablation_study_present": true,
            "uuid": "e6713.0",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DPO",
            "name_full": "Direct Preference Optimization",
            "brief_description": "A method to directly fine-tune a language model on pairwise human preference data without training a separate reward model or using RL algorithms, by maximizing the Bradley–Terry likelihood expressed in terms of model log-probability ratios.",
            "citation_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "mention_or_use": "use",
            "model_name": "GPT2-Large",
            "model_size": "GPT2-Large",
            "reasoning_method_name": "Preference-based fine-tuning (DPO)",
            "reasoning_method_type": "contrastive / preference-optimization",
            "reasoning_style_diversity": "homogeneous (treats every preference pair equally)",
            "benchmark_name": "IMDB sentiment-control (bootstrapped preference pairs)",
            "task_description": "Steer generated movie-review continuations toward positive sentiment",
            "performance_metric": "average positive-sentiment probability (reward); KL divergence to SFT",
            "performance_value": 0.72,
            "comparison_target_method": "ODPO",
            "performance_difference": -0.04,
            "statistical_significance": null,
            "analysis_notes": "DPO optimizes pairwise ordering but ignores magnitude of preference; compared to ODPO it generally attains lower reward for similar KL, especially with smaller preference datasets.",
            "ablation_study_present": false,
            "uuid": "e6713.1",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ODPO_toxicity",
            "name_full": "ODPO (toxicity control)",
            "brief_description": "Application of ODPO to reduce toxic generations by using toxicity classifier scores to set per-pair offsets (log-scaled difference).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-neo-2.7B",
            "model_size": "GPT-neo-2.7B",
            "reasoning_method_name": "Preference-based fine-tuning with offset (ODPO)",
            "reasoning_method_type": "contrastive / preference-optimization",
            "reasoning_style_diversity": "diverse (per-pair offsets derived from toxicity-score differences)",
            "benchmark_name": "REALToxicityPrompts (adversarial prompts subset)",
            "task_description": "Reduce toxicity of model generations for adversarial prompts",
            "performance_metric": "average toxicity probability; KL divergence to SFT",
            "performance_value": null,
            "comparison_target_method": "DPO",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "When using medium-size HF datasets (8000 or 9000 bootstrapped pairs) ODPO substantially reduces toxicity (authors state 'cutting it almost by half') while maintaining closer KL to SFT compared to DPO; as HF dataset size grows performance of DPO approaches ODPO.",
            "ablation_study_present": false,
            "uuid": "e6713.2",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DPO_toxicity",
            "name_full": "DPO (toxicity control)",
            "brief_description": "Application of original DPO to reduce toxic generations using bootstrapped pairwise preferences from a toxicity classifier.",
            "citation_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "mention_or_use": "use",
            "model_name": "GPT-neo-2.7B",
            "model_size": "GPT-neo-2.7B",
            "reasoning_method_name": "Preference-based fine-tuning (DPO)",
            "reasoning_method_type": "contrastive / preference-optimization",
            "reasoning_style_diversity": "homogeneous (no per-pair magnitude weighting)",
            "benchmark_name": "REALToxicityPrompts (adversarial prompts subset)",
            "task_description": "Reduce toxicity of model generations for adversarial prompts",
            "performance_metric": "average toxicity probability; KL divergence to SFT",
            "performance_value": null,
            "comparison_target_method": "ODPO",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "DPO struggled to materially reduce toxicity in the limited-data settings where ODPO succeeded; increasing HF data narrows the gap.",
            "ablation_study_present": false,
            "uuid": "e6713.3",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ODPO_summarization",
            "name_full": "ODPO (summarization with human Likert offsets)",
            "brief_description": "ODPO applied to summarization where the offset between two summaries is set from human Likert-scale score differences; evaluated via GPT-4 win-rate against human-written references.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPTJ-6B",
            "model_size": "GPTJ-6B",
            "reasoning_method_name": "Preference-based fine-tuning with offset (ODPO)",
            "reasoning_method_type": "contrastive / preference-optimization",
            "reasoning_style_diversity": "diverse (offsets from human Likert score differences)",
            "benchmark_name": "REDDIT TL;DR (human-rated subset)",
            "task_description": "Generate summaries preferred by humans (quality judged on 7-point Likert scale); compare generated summaries to human references",
            "performance_metric": "win rate against human-written summaries (GPT-4 judged); head-to-head win rate vs DPO",
            "performance_value": 53.6,
            "comparison_target_method": "DPO",
            "performance_difference": null,
            "statistical_significance": true,
            "analysis_notes": "Across sampling temperatures ODPO had higher average win rates vs human-written summaries than DPO; authors report ODPO significantly outperforms DPO at temperatures 0 and 0.5. Head-to-head ODPO win rates over DPO at temperatures [0,0.25,0.5,0.75,1] are [51,50,62,48,57] percent respectively (per-paper).",
            "ablation_study_present": true,
            "uuid": "e6713.4",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SLiC-HF",
            "name_full": "Sequence Likelihood Calibration with Human Feedback (SLiC-HF)",
            "brief_description": "A sequence-level contrastive method that calibrates sequence likelihoods to prefer better outputs by enforcing an offset hyperparameter δ between preferred and dispreferred responses (δ is a fixed hyperparameter, not response-dependent).",
            "citation_title": "SLiC-HF: Sequence likelihood calibration with human feedback",
            "mention_or_use": "use",
            "model_name": "GPT2-Large",
            "model_size": "GPT2-Large",
            "reasoning_method_name": "Sequence likelihood calibration (SLiC-HF)",
            "reasoning_method_type": "contrastive / margin-based",
            "reasoning_style_diversity": "homogeneous (fixed margin δ independent of pair)",
            "benchmark_name": "IMDB sentiment-control (comparison)",
            "task_description": "Steer generations toward positive sentiment (same experimental setup as DPO/ODPO)",
            "performance_metric": "average positive-sentiment probability (reward); KL divergence to SFT",
            "performance_value": 0.71,
            "comparison_target_method": "ODPO",
            "performance_difference": -0.05,
            "statistical_significance": null,
            "analysis_notes": "Compared in Table 1: SLiC runs did not appear on the Pareto frontier for reward vs KL in the experiments; SLiC uses a fixed margin rather than response-dependent offsets, which the authors contrast with ODPO's per-pair offsets.",
            "ablation_study_present": false,
            "uuid": "e6713.5",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Related_methods_mention",
            "name_full": "Related RLHF / preference methods (PPO, RLHF, IPO, KTO, RRHF)",
            "brief_description": "Related methods mentioned in related work: RLHF using PPO and reward models; IPO, KTO, RRHF are variants/alternatives to DPO and sequence-level contrastive approaches.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "RLHF (PPO); Identity Preference Optimization (IPO); Kahneman-Tversky Optimization (KTO); RRHF",
            "reasoning_method_type": "varied (RL, contrastive, unpaired-preference)",
            "reasoning_style_diversity": "mixed (different approaches to use preference data; some treat pairs uniformly, some use alternative losses or unpaired data)",
            "benchmark_name": "",
            "task_description": "",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "These methods are discussed in related work: PPO-based RLHF requires a reward model and RL tuning; IPO bounds DPO's unbounded preference function; KTO uses unpaired labeled examples; RRHF uses ranking losses without offsets. The paper positions ODPO relative to these by emphasizing per-pair offseting of DPO.",
            "ablation_study_present": null,
            "uuid": "e6713.6",
            "source_info": {
                "paper_title": "Direct Preference Optimization with an Offset",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model.",
            "rating": 2
        },
        {
            "paper_title": "SLiC-HF: Sequence likelihood calibration with human feedback",
            "rating": 2
        },
        {
            "paper_title": "RRHF: Rank responses to align language models with human feedback without tears",
            "rating": 2
        },
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 2
        },
        {
            "paper_title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "rating": 1
        },
        {
            "paper_title": "Identity Preference Optimization (IPO)",
            "rating": 1
        },
        {
            "paper_title": "Human-centered loss functions (HALOs)",
            "rating": 1
        }
    ],
    "cost": 0.01638775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Direct Preference Optimization with an Offset</h1>
<p>Afra Amini Tim Vieira Ryan Cotterell<br>{afra.amini, ryan.cotterell}@inf.ethz.ch tim.f.vieira@gmail.com<br>ETHzürich</p>
<h4>Abstract</h4>
<p>Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.</p>
<h2>0 https://github.com/rycolab/odpo</h2>
<h2>1 Introduction</h2>
<p>Reinforcement learning from human feedback (RLHF) is a key building block in training the most modern large language models. The algorithm aligns the language model's responses to human preferences (Ouyang et al., 2022; Touvron et al., 2023; Anil et al., 2023; OpenAI et al., 2023). A typical implementation of RLHF is as follows. First, humans compare paired responses from a language model, i.e., they determine which of the responses is better, e.g., more helpful or less toxic. Second, a reward model is trained to give higher rewards to the responses preferred by humans. Lastly, a reinforcement learning algorithm is used to update
the model's parameters to maximize the expected reward, while not diverging too much from the model's initial parameters.</p>
<p>The aforementioned implementation of RLHF, however, is tedious for two reasons. First, the reward model is usually a model as large as the language model itself, which is expensive to train and store. Second, reinforcement learning algorithms are known to be very sensitive to the choice of hyperparameters (Zheng et al., 2023), and, thus, hard to tune. Therefore, a fruitful line of work attempts to design alternative simpler implementations of RLHF (Welleck et al., 2020; Lu et al., 2022; Zhao et al., 2023).</p>
<p>A highly successful and straightforward alternative to RLHF is direct preference optimization (DPO; Rafailov et al., 2023). Unlike the conventional implementation of RLHF, DPO does not train a reward model and sidesteps the use of any reinforcement learning algorithm. Instead, it finetunes the language model's weights to maximize the likelihood of the preference data directly using the Bradley-Terry model (Bradley and Terry, 1952). Intuitively, each DPO gradient update to the model's parameters increases the likelihood of the preferred response and decreases the likelihood of the dispreferred response.</p>
<p>However, DPO only takes the ordering between the model's responses into account and not the extent to which one response is preferred over another. In many settings, the difference in quality between the two responses is known-either by asking humans (Stiennon et al., 2020; Touvron et al., 2023) ${ }^{1}$ or through point-wise scores, e.g., toxicity scores, sentiment scores, code-based execution scores, etc. In that context, a natural question is the following: how can we make use of this information to better align language models with human preferences?</p>
<p>To answer this question, we propose a new method, which we term DPO with an offset</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ODPO takes into account the extent to which one output should be preferred over another. The model has to put more probability mass on the preferred output compared to the dispreferred output by an offset that is determined based on how much the winning output is preferred over the losing output.</p>
<p>(ODPO). ODPO is a generalization of DPO that incorporates the difference between responses when modeling preference data. The intuition behind ODPO is simple; it requires the language model to increase the likelihood of the preferred responses compared to the dispreferred responses by an offset that is determined based on the difference between their associated reward values. Therefore, the larger the reward of the preferred response in comparison to the dispreferred response, the higher the likelihood needs to be over the likelihood of the dispreferred response. We further show that when the offset is set to zero, ODPO is equivalent to DPO. This process is illustrated in Fig. 1.</p>
<p>In our experiments, we fine-tune language models of various sizes with DPO and ODPO and compare the two methods. In cases where a ground-truth reward function is given, we measure two competing metrics: (i) the average reward associated with the generations from the fine-tuned model, and (ii) the KL divergence between the language model before and after the fine-tuning. Based on the results of sentiment and toxicity control tasks, we observe that ODPO more often appears on the Pareto frontier of reward and KL compared to DPO. We then apply ODPO to a summarization task, where we use the scores given by humans on a Likert scale to define an offset between the two summaries. We observe that on average and across different sampling temperatures, ODPO results in a higher win rate over human-written summaries compared to DPO.</p>
<h2>2 Preliminaries</h2>
<p>Given a prompt $\boldsymbol{x} \in \Sigma^{<em>}$, a language model $\pi_{\boldsymbol{\theta}}$ is a distribution over the responses $\boldsymbol{y} \in \Sigma^{</em>}$, where $\Sigma$ is the alphabet.<sup>2</sup> As is standard, we parameterize the conditional distribution over responses given a prompt as an autoregressive language model $\pi_{\boldsymbol{\theta}}(\boldsymbol{y} \mid \boldsymbol{x})$ parameterized by $\boldsymbol{\theta}$ as,</p>
<p>$$
\begin{aligned}
\pi_{\boldsymbol{\theta}}(\boldsymbol{y} \mid \boldsymbol{x}) &amp;= \
\pi_{\boldsymbol{\theta}}(\text{EOS} \mid \boldsymbol{y}, \boldsymbol{x}) &amp; \prod_{t=1}^{T} \pi_{\boldsymbol{\theta}}(y_{t} \mid \boldsymbol{y}_{&lt;t}, \boldsymbol{x})
\end{aligned}
\tag{1}
$$</p>
<p>where EOS ∉ Σ is a distinguished end-of-sequence token. To deploy a language model to a downstream task, it is usually necessary to additionally fine-tune it on high-quality data. Given a dataset of prompts and desired responses $(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}<em _text_SFT="\text{SFT">{\text{SFT}}$, the standard fine-tuning objective (to be maximized) is simply the log-likelihood of $\mathcal{D}</em>$}</p>
<p>$$
\mathcal{J}^{\text{ML}}(\boldsymbol{\theta}) = \operatorname*{\mathbb{E}}<em _text_SFT="\text{SFT">{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}</em>
$$}}} \left[ \log \pi_{\boldsymbol{\theta}}(\boldsymbol{y} \mid \boldsymbol{x}) \right]. \tag{2</p>
<p>We use $\pi_{\text{SFT}}$ to refer to the language model fine-tuned with this objective on $\mathcal{D}_{\text{SFT}}$.</p>
<p>Such a fine-tuning approach can be effective for solving downstream tasks with language models. However, there is a disparity between the maximum-likelihood objective and the goal of the downstream task, which is to leverage the language model to generate high-quality responses, as judged by humans. This misalignment arises partly because the maximum-likelihood objective, Eq. (2), fails to distinguish between major and minor errors. For instance, when fine-tuning a language model to perform summarization, a significant error might involve hallucination, while a minor one might involve a failure to employ the appropriate synonyms (Stiennon et al., 2020). Training with the maximum-likelihood objective encourages the model to assign nonzero probability mass to all responses in $\mathcal{D}_{\text{SFT}}$, even those of lower quality.</p>
<p>To address this issue, one can optimize the expected reward of language model generations instead of using the maximum-likelihood objective.</p>
<p><sup>2</sup>An alphabet is a finite, non-empty set.</p>
<p>Consider a reward function $r(\boldsymbol{x}, \boldsymbol{y})$, estimated from quality assessments performed by humans. The reward function assigns real values to promptresponse pairs; we will discuss in $\S 3$ how such a reward model can be learned from human feedback. Our objective is to make sure that the responses generated by the language model have high quality. Therefore, we can directly optimize the expected reward of the responses generated by the language model,</p>
<p>$$
\mathcal{J}^{\mathrm{KL}}(\boldsymbol{\theta})=\underset{\boldsymbol{x} \sim \mathcal{D}, \boldsymbol{y} \sim \pi_{\boldsymbol{\theta}}(\cdot \mid \boldsymbol{x})}{\mathbb{E}}[r(\boldsymbol{x}, \boldsymbol{y})]
$$</p>
<p>where $\mathcal{D}=\left{\boldsymbol{x}^{(\mathrm{n})}\right}<em _SFT="{SFT" _text="\text">{\mathrm{n}=1}^{\mathcal{N}}$ is a multiset of prompts. To prevent reward hacking (Amodei et al., 2016) and to make sure that we do not diverge too much from the supervised fine-tuned model $\pi</em>$, a regularization term is often added to the objective (Stiennon et al., 2020),}</p>
<p>$$
\begin{aligned}
\mathcal{J}^{\mathrm{KL}}(\boldsymbol{\theta}) &amp; =\underset{\boldsymbol{x} \sim \mathcal{D}, \boldsymbol{y} \sim \pi_{\boldsymbol{\theta}}(\cdot \mid \boldsymbol{x})}{\mathbb{E}}[r(\boldsymbol{x}, \boldsymbol{y})] \
&amp; -\beta D_{\mathrm{KL}}\left[\pi_{\boldsymbol{\theta}}(\boldsymbol{y} \mid \boldsymbol{x}) | \pi_{\mathrm{SFT}}(\boldsymbol{y} \mid \boldsymbol{x})\right]
\end{aligned}
$$</p>
<p>The above objective is optimized using proximal policy optimization (PPO; Schulman et al., 2017) or another actor-critic algorithm (Mnih et al., 2016; Glaese et al., 2022).</p>
<h2>3 Reward Modeling</h2>
<p>Pointwise Rewards. A key component in Eq. (4) is the task-dependent reward function that assigns pointwise real-valued rewards to each output. In many tasks, learning such reward functions is straightforward. For example, in open-ended text generation, a desired attribute could be the presence of indicators of positive sentiment in the generated text, while an undesired attribute could be toxicity. In such cases, the reward model might take the form of a classifier that assesses responses based on their sentiment or toxicity. Similarly, in code generation tasks, the quality of the code can be automatically evaluated, providing another straightforward example.</p>
<p>Pairwise Preferences. Learning a point-wise reward function for tasks like summarization or dialogue generation is more complex. Judging the absolute quality of a summary can depend on several factors, e.g., coherence, faithfulness, and conciseness, which makes it hard to collect human feedback datasets for reward model training. An effec-
tive strategy in such cases is to collect human preferences instead of point-wise judgments (Ziegler et al., 2020; Wu et al., 2021; Ouyang et al., 2022). In this setup, humans are shown two (or more) responses to a prompt $\boldsymbol{x}$ and are asked to select the response they prefer. Therefore, a datapoint in a human feedback dataset is a triple $\left(\boldsymbol{x}, \boldsymbol{y}<em l="l">{w}, \boldsymbol{y}</em>}\right) \sim \mathcal{D<em w="w">{\mathrm{HF}}$, where $\boldsymbol{y}</em>$. Given such a dataset, one needs to learn the point-wise reward function. It is common to assume that the preference data can be modeled by a Bradley-Terry model (Bradley and Terry, 1952),}$ is preferred over $\boldsymbol{y}_{l</p>
<p>$$
\begin{aligned}
&amp; p_{\mathrm{BT}}\left(\boldsymbol{y}<em l="l">{w} \succ \boldsymbol{y}</em>\right) \
&amp; \quad=\frac{\exp \left(r\left(\boldsymbol{x}, \boldsymbol{y}} \mid \boldsymbol{x<em w="w">{w}\right)\right)}{\exp \left(r\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)\right)+\exp \left(r\left(\boldsymbol{x}, \boldsymbol{y<em w="w">{l}\right)\right)} \
&amp; \quad=\sigma\left(r\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)\right)
\end{aligned}
$$}\right)-r\left(\boldsymbol{x}, \boldsymbol{y}_{l</p>
<p>where $\sigma(x)=\frac{1}{1+\exp (-x)}$ is the sigmoid function. Assuming that $\mathcal{D}<em _mathrm_BT="\mathrm{BT">{\mathrm{HF}} \sim p</em>$, parameterized by $\phi$, as a binary classifier that maximizes the following log-likelihood}}$, we can train a reward model $r_{\phi</p>
<p>$$
\mathbb{E}\left[\log \sigma\left(r_{\phi}\left(\boldsymbol{x}, \boldsymbol{y}<em _phi="\phi">{w}\right)-r</em>\right)\right)\right]
$$}\left(\boldsymbol{x}, \boldsymbol{y}_{l</p>
<p>where the expectation is over $\left(\boldsymbol{x}, \boldsymbol{y}<em l="l">{w}, \boldsymbol{y}</em>$. Intuitively, the reward function should assign higher reward values to the responses that are preferred by humans. The estimated reward function is then plugged into Eq. (4) to enable policy estimation.}\right) \sim \mathcal{D}_{\mathrm{HF}</p>
<p>It is often the case that directly optimizing Eq. (4) is complex and resource-intensive. Next, we will elaborate on an alternative method to using RLHF that has demonstrated comparable or even superior effectiveness to PPO.</p>
<h2>4 Direct Preference Optimization</h2>
<p>Rafailov et al. (2023) introduce a method to avoid reward model training and, thus, to directly optimize the language model. Their method, termed direct preference optimization (DPO) works as follows. The critical observation is that the optimal solution that maximizes the Eq. (4) is,</p>
<p>$$
\begin{aligned}
&amp; \pi_{\boldsymbol{\theta}}^{*}(\boldsymbol{y} \mid \boldsymbol{x})= \
&amp; \quad \frac{1}{Z(\boldsymbol{x})} \pi_{\mathrm{SFT}}(\boldsymbol{y} \mid \boldsymbol{x}) \exp \left(\frac{1}{\beta} r(\boldsymbol{x}, \boldsymbol{y})\right)
\end{aligned}
$$</p>
<p>where</p>
<p>$$
Z(\boldsymbol{x})=\sum_{\boldsymbol{y} \in \Sigma^{*}} \pi_{\mathrm{SFT}}(\boldsymbol{y} \mid \boldsymbol{x}) \exp \left(\frac{1}{\beta} r(\boldsymbol{x}, \boldsymbol{y})\right)
$$</p>
<p>$$
\begin{aligned}
\mathcal{L}^{\mathrm{DPO}}(\boldsymbol{\theta}) &amp; =-\underset{\left(\boldsymbol{x}, \boldsymbol{y}<em l="l">{w}, \boldsymbol{y}</em>}\right) \sim \mathcal{D<em _boldsymbol_theta="\boldsymbol{\theta">{\mathrm{Mr}}}{\mathbb{E}}\left[\log \sigma\left(\beta \log \frac{\pi</em>}}\left(\boldsymbol{y<em _mathrm_SFT="\mathrm{SFT">{w} \mid \boldsymbol{x}\right)}{\pi</em>}}\left(\boldsymbol{y<em _boldsymbol_theta="\boldsymbol{\theta">{w} \mid \boldsymbol{x}\right)}-\beta \log \frac{\pi</em>}}\left(\boldsymbol{y<em _mathrm_SFT="\mathrm{SFT">{l} \mid \boldsymbol{x}\right)}{\pi</em>}}\left(\boldsymbol{y<em w="w">{l} \mid \boldsymbol{x}\right)}\right)\right] \
&amp; =-\underset{\left(\boldsymbol{x}, \boldsymbol{y}</em>}, \boldsymbol{y<em _mathrm_Mr="\mathrm{Mr">{l}\right) \sim \mathcal{D}</em>}}}{\mathbb{E}}\left[\log \sigma\left(\widetilde{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)-\widetilde{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)\right)\right] \
\mathcal{L}^{\mathrm{ODPO}}(\boldsymbol{\theta}) &amp; =-\underset{\left(\boldsymbol{x}, \boldsymbol{y}<em l="l">{w}, \boldsymbol{y}</em>}\right) \sim \mathcal{D<em _boldsymbol_theta="\boldsymbol{\theta">{\mathrm{Mr}}}{\mathbb{E}}\left[\log \sigma\left(\widetilde{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em _boldsymbol_theta="\boldsymbol{\theta">{w}\right)-\widetilde{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em r="r">{l}\right)-\Delta</em>\right)\right]
\end{aligned}
$$</p>
<p>Note that $Z(\boldsymbol{x})$ is, in general, difficult to compute as it involves a sum over a countably infinite set. Nevertheless, we can write the reward as a function of $\pi_{\boldsymbol{\theta}}^{*}$ by rearranging the terms,</p>
<p>$$
r(\boldsymbol{x}, \boldsymbol{y})=\beta \log \frac{\pi_{\boldsymbol{\theta}}^{*}(\boldsymbol{y} \mid \boldsymbol{x})}{\pi_{\mathrm{SFT}}(\boldsymbol{y} \mid \boldsymbol{x})}+\beta \log Z(\boldsymbol{x})
$$</p>
<p>Under the assumption that the preference data is well-modeled by the Bradley-Terry model (Eq. (5a)), we substitute the reward in Eq. (6) with Eq. (10), and formulate the loss function in terms of the language model parameters. Therefore, we directly optimize the language model by maximizing the likelihood of the preference data in Eq. (7b). In this equation, $\widetilde{r}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{y})=\beta \log \frac{\pi</em>)$ cancels out.}}(\boldsymbol{y} \mid \boldsymbol{x})}{\widetilde{\pi}_{\mathrm{SFT}}(\boldsymbol{y} \mid \boldsymbol{x})}$ is called the estimated reward. Note that the intractable normalization factor $Z(\boldsymbol{x</p>
<p>Simply put, Eq. (7b) requires the estimated reward for the preferred response to be larger than the estimated reward for the dispreferred response. A strong assumption behind the DPO's loss is that the preference data is well-modeled by a BradleyTerry model. However, the Bradley-Terry model only tells us the probability that one response is preferred over another, and not the extent to which this preference will hold. To enhance DPO, we will introduce our modification to DPO, which generalizes DPO and introduces an offset between the responses in its loss function.</p>
<h2>5 DPO with an Offset</h2>
<p>The intuition behind ODPO is simple: we want the estimated reward for the preferred response to be larger than the estimated reward for the dispreferred response, by an offset that is a function of the actual reward differences assigned to the two responses. To begin our exposition of ODPO, we first discuss the connection between the Bradley-Terry model and Gumbel random variables, which we then use to construct the loss function for ODPO.
Theorem 1. Let $\boldsymbol{y}<em l="l">{w}$ and $\boldsymbol{y}</em>}$ be two responses to a prompt $\boldsymbol{x}$, and let $\widetilde{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$ and $\widetilde{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)$
be their associated estimated rewards. Finally, let $R_{w} \sim \operatorname{Gumbel}\left(\widetilde{r}<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right), 1\right)$ and $R_{l} \sim$ $\operatorname{Gumbel}\left(\widetilde{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>\right), 1\right)$ be Gumbel random variables. Then, we have,</p>
<p>$$
\begin{aligned}
\mathbb{P}\left(R_{w}-R_{l}&gt;0\right) &amp; =p_{\mathrm{BT}}\left(\boldsymbol{y}<em l="l">{w} \succ \boldsymbol{y}</em>\right) \
&amp; =\sigma\left(\Delta_{\widetilde{r}_{\boldsymbol{\theta}}}\right)
\end{aligned}
$$} \mid \boldsymbol{x</p>
<p>where $p_{\mathrm{BT}}\left(\boldsymbol{y}<em l="l">{w} \succ \boldsymbol{y}</em>} \mid \boldsymbol{x}\right)$ is a Bradley-Terry model Eq. (5a) parameterized by $\widetilde{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$ and $\widetilde{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$, and $\Delta_{\widetilde{r<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}} \stackrel{\text { def }}{=} \widetilde{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em _boldsymbol_theta="\boldsymbol{\theta">{w}\right)-\widetilde{r}</em>\right)$ is the difference between the estimated rewards.}}\left(\boldsymbol{x}, \boldsymbol{y}_{l</p>
<p>Proof. The proof is simple and follows directly from the Gumbel-max trick (Maddison and Tarlow, 2017). See App. A for details.</p>
<p>Theorem 2. Let $\boldsymbol{y}<em l="l">{w}$ and $\boldsymbol{y}</em>$ between the responses' reward values,}$ be two responses for a prompt $\boldsymbol{x}$. Given a desired offset $\Delta_{r</p>
<p>$$
\mathbb{P}\left(R_{w}-R_{l}&gt;\Delta_{r}\right)=\sigma\left(\Delta_{\widetilde{r}<em r="r">{\boldsymbol{\theta}}}-\Delta</em>\right)
$$</p>
<p>where $\Delta_{\widetilde{r}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}} \stackrel{\text { def }}{=} \widetilde{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em _boldsymbol_theta="\boldsymbol{\theta">{w}\right)-\widetilde{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em w="w">{l}\right)$.
Proof. The difference between two independent Gumbel variables $R</em>} \sim \operatorname{Gumbel}\left(\widetilde{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right), 1\right)$ and $R_{l} \sim \operatorname{Gumbel}\left(\widetilde{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>$ is,}\right), 1\right)$ follows the logistic distribution. Therefore, the probability of the difference between the noisy estimated rewards being greater than $\Delta_{r</p>
<p>$$
\mathbb{P}\left(R_{w}-R_{l}&gt;\Delta_{r}\right)=1-\mathcal{F}\left(\Delta_{r}\right)
$$</p>
<p>where $\mathcal{F}(\cdot)$ is the CDF for logistic distribution. The mean of this distribution is $\Delta_{\widetilde{r}_{\boldsymbol{\theta}}}$, and the variance is 1 . Substituting the CDF in Eq. (13) with its definition,</p>
<p>$$
\begin{aligned}
\mathbb{P}\left(R_{w}\right. &amp; \left.-R_{l}&gt;\Delta_{r}\right)=1-\mathcal{F}\left(\Delta_{r}\right) \
&amp; =\frac{1}{2}-\frac{1}{2} \tanh \left(\frac{1}{2}\left(\Delta_{r}-\Delta_{\widetilde{r}<em r="r">{\boldsymbol{\theta}}}\right)\right) \
&amp; =\frac{1}{2}-\frac{1}{2}\left(2 \sigma\left(\Delta</em>}-\Delta_{\widetilde{r<em r="r">{\boldsymbol{\theta}}}\right)-1\right) \
&amp; =1-\sigma\left(\Delta</em>}-\Delta_{\widetilde{r<em _widetilde_r="\widetilde{r">{\boldsymbol{\theta}}}\right) \
&amp; =\sigma\left(\Delta</em><em r="r">{\boldsymbol{\theta}}}-\Delta</em>\right)
\end{aligned}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: Steering generated movie reviews towards positive sentiment. Points on the Pareto front are highlighted with a black border. We observe that in all $3$ settings, most (if not all) points on the Pareto front belong to ODPO.</p>
<p>Similar to DPO, we then maximize the likelihood of the preference data, as shown in Eq. (7c). Intuitively, minimizing the above loss function enforces the estimated reward for the preferred response $\boldsymbol{y}<em l="l">{w}$ to be higher than $\boldsymbol{y}</em>=0$, ODPO will be equivalent to DPO.}$ by the offset $\Delta_{r}$. Comparing Eq. (7b) and Eq. (7c), when $\Delta_{r</p>
<p>Designing $\Delta_{r}$. We choose to model the offset as a monotonically increasing function $\mathbf{f}(\cdot)$ of the difference between the scores associated with the responses:</p>
<p>$\Delta_{r}=\alpha \mathbf{f}\left(\operatorname{score}\left(\boldsymbol{x}, \boldsymbol{y}<em l="l">{w}\right)-\operatorname{score}\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)\right) .$ (15)</p>
<p>where $\alpha$ is a hyperparameter that controls the extent to which an offset should be enforced. When $\alpha=0$, ODPO becomes equivalent to DPO. In tasks where a ground-truth reward model $r(\cdot)$ is given we set $\operatorname{score}(\cdot)=r(\cdot)$. In other tasks, one can use the quality scores given by humans to responses as score (see §6.3), or ask humans directly for the difference between the quality of the responses <em>Touvron et al. (2023)</em>. We ablate $\mathbf{f}$ in $\S 7.1$ and $\alpha$ in $\S 7.2$.</p>
<p>Connection to Softmax Margin. We now show how ODPO is connected to softmax margin <em>Gimpel and Smith (2010)</em>. The idea behind the softmax margin is to augment the softmax with a cost function, such that high-cost responses get penalized more heavily. For two responses per prompt, we maximize</p>
<p>$\mathbb{E}<em w="w">{\left(\boldsymbol{x}, \boldsymbol{y}</em>}, \boldsymbol{y<em _widehat_r="\widehat{r">{l}\right)}\left[\log \sigma\left(\Delta</em><em w="w">{\boldsymbol{\theta}}}-\operatorname{cost}\left(\boldsymbol{y}</em>}, \boldsymbol{y<em r="r">{l}\right)\right)\right]$
Importantly, the cost function is by definition a nonnegative function. Therefore, ODPO loss Eq. (7c) is only equivalent to softmax-margin loss if we restrict $\Delta</em> \geq 0$.</p>
<h2>6 Experiments</h2>
<p>In this section, we empirically compare the performance of ODPO with DPO across different tasks. We refer the reader to App. C for the computational budget used for conducting the experiments.</p>
<h3>6.1 Sentiment Control</h3>
<p>In this experiment, we steer generations of GPT2-Large <em>Radford et al. (2019)</em> model towards positive sentiment. Following the experimental setup in <em>Rafailov et al. (2023)</em>, we fine-tune GPT2-Large on the train split of IMDB dataset <em>Maas et al. (2011)</em> until convergence. This gives us the SFT checkpoint $\pi_{\text {SFT }}$.</p>
<p>Reward Model. We use a state-of-the-art binary sentiment classifier with sentiments ${\mathrm{POS}, \mathrm{NEG}}$ as the reward model. Concretely if the sentiment of the response is negative, we set $r(\boldsymbol{x}, \boldsymbol{y})$ to $1-p(\operatorname{NEG} \mid \cdot)$, and if the sentiment is positive $r(\boldsymbol{x}, \boldsymbol{y})=1+p(\operatorname{POS} \mid \cdot)$, where $p$ is given by the classifier. For notational ease, we show $r(\boldsymbol{x}, \boldsymbol{y})$ with $r(\boldsymbol{y})$.</p>
<p>Bootstraping the Preference Data. DPO, as opposed to RLHF, only works on top of preference data. Therefore, in tasks that a ground-truth reward model is given, the preference data needs to be bootstraped. We follow <em>Rafailov et al. (2023)</em> to bootstrap the preference dataset $\left(\mathcal{D}_{\mathrm{HF}}\right)$ from point-wise rewards. Given the reward function, which in this experiment is given by the sentiment classifier, and a dataset of prompts that are prefixes of movie reviews, we proceed as follows. First,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Steering generations away from toxic content. We highlight points on the Pareto front with a black border. We observe that, especially when the size of the dataset is small, ODPO manages to reduce the toxicity better than DPO while not diverging too far from the SFT model.</p>
<p>for each prompt $\boldsymbol{x} \in \mathcal{D}$, $M$ responses are sampled from the language model $\pi_{\text{SFT}}\left(\cdot \mid \boldsymbol{x}\right)^4$ to form a multiset $Y_{\boldsymbol{x}} = {\boldsymbol{y}<em i="1">i}</em>}^M$, where $\boldsymbol{y<em _text_HF="\text{HF">m$ is the $m^{\text{th}}$ response generated for prompt $\boldsymbol{x}$. Next, for each $i \neq j \in {1, \ldots, M}$ a tuple $(\boldsymbol{x}, \boldsymbol{y}_w, \boldsymbol{y}_l)$ is added to $\mathcal{D}</em>_j)$, where}}$ if $r(\boldsymbol{y}_i) \neq r(\boldsymbol{y</p>
<p>$$
(\boldsymbol{y}_w, \boldsymbol{y}_l) = \begin{cases}
(\boldsymbol{y}_i, \boldsymbol{y}_j) &amp; \text{if } r(\boldsymbol{y}_i) &gt; r(\boldsymbol{y}_j) \
(\boldsymbol{y}_j, \boldsymbol{y}_i) &amp; \text{if } r(\boldsymbol{y}_i) &lt; r(\boldsymbol{y}_j)
\end{cases}
$$</p>
<p>While DPO only uses the <em>order</em> of responses' rewards, with ODPO we further use the reward model to determine the offset between the two responses and set $\Delta_r = \log\left(r(\boldsymbol{y}_w) - r(\boldsymbol{y}_l)\right)$. Following Eq. (15), this is equivalent to choosing $\mathbf{f}(\cdot)$ as the log function and setting $\alpha = 1$.</p>
<p>We vary the KL regularization term $\beta$ in ${0.1, 0.2, \ldots, 1} \cup {1, 2, 3, 4, 5}^6$ and sample from the fine-tuned language model $\pi_{\theta}$ two times with two different random seeds. This gives us 28 different samples for each method. We use these samples to approximate the average probability of the generations having positive sentiment as well as the KL divergence between the fine-tuned model $\pi_{\theta}$ and the initial model $\pi_{\text{SFT}}$. Ideally, we want not to diverge too much from the SFT policy, while generating movie reviews with a positive sentiment.</p>
<p>To capture the tradeoff between the KL divergence and achieved reward, we evaluate the two methods based on the Pareto frontier of achieved reward and KL divergence. Concretely, we report the percentage of points on the Pareto set that belong to each method. We compare the performance of ODPO and DPO in 3 different settings by varying the number of data points in the preference dataset, i.e., $|\mathcal{D}<em _text_SFT="\text{SFT">{\text{HF}}|$. As depicted in Fig. 2, in all experimental setups, ODPO is more effective in generating more samples with positive sentiment, while not diverging too far from $\pi</em>| = 5000$.}}$. The difference between the two methods is more pronounced with smaller datasets, i.e., when $|\mathcal{D}_{\text{HF}</p>
<h3>6.2 Toxicity Control</h3>
<p>In this task, our goal is to reduce the toxicity of the generations. We use GPT-neo-2.7b (Black et al., 2021) as the SFT checkpoint, $\pi_{\text{SFT}}$. We adversarially sample the prompts from REALTOXICITYPROMPTS (Gehman et al., 2020), where we sample 10000 prompts that have toxicity scores of more than 0.3. We generate two preference pairs from $\pi_{\text{SFT}}$ for each prompt and compute their rewards using a toxicity classifier. Similar to the previous experiment, we choose the offset as the log scaled differences between the rewards of preferred and dispreferred responses.</p>
<p><sup>4</sup>One can also compare the responses generated by the model with human written responses (Stiennon et al., 2020).</p>
<p><sup>5</sup>In this experiment, we generate two responses per prompt.</p>
<p><sup>6</sup>Within the range of 0 to 1, increased resolution is necessary due to considerable variance in results. However, within the range of 1 to 5, the points are closely clustered, therefore, we increase the step size when covering this range.</p>
<p><sup>7</sup>Further experiments comparing SLiC-HF and ODPO show the same trends App. D.</p>
<p><sup>8</sup>When using more data, e.g., $|\mathcal{D}<em _text_SFT="\text{SFT">{\text{HF}}| = 10000$, we observe the positive sentiment probability gets closer to the maximum value 1., while this comes at the cost of diverging too much from $\pi</em>}}$; we see an order of magnitude larger $D_{\text{KL}}$ with $|\mathcal{D<em _text_HF="\text{HF">{\text{HF}}| = 10000$ compared to $|\mathcal{D}</em>| = 7500$.}</p>
<p><sup>9</sup>Specifically, we use https://huggingface.co/EleutherAI/gpt-neo-2.7B.</p>
<p><sup>10</sup>We use https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Investigating the effect of the offset formulation on the performance of ODPO. Scaling the offset with a log function helps achieve the highest reward values without diverging too much from the SFT model.</p>
<p>We vary the KL regularization term $\beta \in$ ${0.05,0.1,0.2,0.3,0.4,0.5}^{11}$ and sample from the fine-tuned model $\pi_{\boldsymbol{\theta}}$. We evaluate the samples based on their average toxicity probabilities and the KL divergence between $\pi_{\boldsymbol{\theta}}$ and $\pi_{\mathrm{SFT}}$.</p>
<p>We observe that ODPO significantly outperforms DPO when using a dataset of size 8000 or 9000, where all the points on the Pareto front are from ODPO in Fig. 3. Concretely, in those two setups, DPO fails to meaningfully reduce the toxicity of the generations, as the toxicity scores are all close to the toxicity scores of generations from $\pi_{\mathrm{SFT}}$ (shown with a dashed horizontal line). However, ODPO manages to significantly reduce the toxicity of the generations, cutting it almost by half. As we increase the size of the dataset, the performance of DPO gets closer to ODPO.</p>
<h3>6.3 Summarization</h3>
<p>Next, we test our method on a summarization task, where we directly use the scores given by humans to the pair of summaries for designing the offset. We use REDDIT TL;DR dataset collected by Stiennon et al. (2020). We take the subset of this dataset that is assessed by humans for quality using a 7-point Likert scale. For each of the two summaries of a given Reddit post, we add a datapoint to $\mathcal{D}_{\mathrm{HF}}$ if the overall quality score of one is greater than the other. Moreover, we set the offset between the two summaries as the log difference between their</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Win rates of generations from models fine-tuned with DPO and ODPO against human-written summaries. On average, ODPO achieves a higher win rate—significantly so in temperatures 0 and 0.5.</p>
<p>associated scores by humans.</p>
<p>The supervised fine-tuned model ( $\pi_{\mathrm{SFT}}$ ) is a GPTJ-6B (Wang and Komatsuzaki, 2021) model that is fine-tuned on human-written forum post summaries. We further fine-tune this model with DPO and ODPO on $\mathcal{D}_{\mathrm{HF}}$. We use GPT-4 to compare the generations from the fine-tuned models against the reference summary written by humans, as GPT-4 judgments have been shown to strongly correlate with human judgments (Rafailov et al., 2023). See App. E for the prompt used for the evaluation.</p>
<p>We use 100 test prompts and sample from the fine-tuned models with different temperatures. The</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>win rates of different methods against human written summaries are reported in Fig. 5. We observe that both DPO and ODPO improve upon the SFT model. At lower temperatures, i.e., $0,0.25,0.5$, both methods are on average preferred over human-written summaries. These results are consistent with the results reported in <em>Rafailov et al. (2023)</em>.</p>
<p>Importantly, across all sampling temperatures, the average win rate of ODPO is higher than DPO. Specifically, in temperatures 0 and 0.5 ODPO significantly outperforms DPO. We further perform a head-to-head comparison between DPO and ODPO and observe that the win rate of ODPO over DPO is $[51,50,62,48,57]$ at temperatures $[0,0.25,0.5,0.75,1$.$] respectively. For qualitative results refer to App. F.</p>
<h2>7 Ablation Studies</h2>
<p>In this section, we investigate and ablate certain design decisions that were made in the experiments. Specifically, we investigate the effect of the offset hyperparameter $\alpha$, and the scaling function $\mathbf{f}(\cdot)$ on the performance of ODPO.</p>
<h3>7.1 Ablating the Scaling Function</h3>
<p>In our experiments, we scaled the reward difference between the preferred and dispreferred responses with a $\log$ function. To better understand the effect of this scaling function, we compare it to two other alternatives, (i) using the reward difference without scaling, i.e., setting $\mathbf{f}(\cdot)$ to the identity function, and (ii) using the difference between $\log$ of reward values as the offset.</p>
<p>We repeat the movie review generation experiment, where the goal is to generate movie reviews with a positive sentiment. For computational efficiency, we choose $|\mathcal{D}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{HF}}|=5000$. We vary $\beta$ between ${0.1,0.2,\ldots,0.9}\cup{1,2,3,4,5}$ and finetune $\pi</em>$ with the aforementioned offsets.}</p>
<p>The results are depicted in Fig. 4. Overall, ODPO outperforms DPO with all three choices of the offset. However, there exist some notable differences between the three offsets. Scaling the offset with the $\log$ function makes the model not diverge too far from $\pi_{\text{SFT}}$, while achieving high rewards: While the models trained with $\log$ scaled offset (left plot) achieve high rewards (around 0.8) with KL of 0.4, models without $\log$ scaling reach to 0.8 rewards only when the KL between $\pi_{\boldsymbol{\theta}}$ and $\pi_{\text{SFT}}$ is around 1 (right plot). Scaling each reward value separately lies somewhere in between (middle plot).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablating alpha values for movie review generation. The offset is set to $\Delta_{r}=\alpha \log\left(r\left(\boldsymbol{y}<em l="l">{w}\right)-r\left(\boldsymbol{y}</em>$.}\right)\right)$. The highest rewards are associated with models trained with $\alpha=1$, while smaller $\alpha$ leads to lower $D_{\text{KL}</p>
<h3>7.2 Ablating the Offset Hyperparameter</h3>
<p>In the previous experiment, we set $\alpha=1$. To understand the effect of enforcing an offset in the loss function we experiment with different values of $\alpha$. Repeating the movie review experiment, we vary $\alpha \in{0.0,0.1,0.2,0.3,0.5,0.8,1.0}$ and set the offset to $\Delta_{r}=\alpha \log\left(r\left(\boldsymbol{y}<em l="l">{w}\right)-r\left(\boldsymbol{y}</em>$ on a dataset of size 7500 with the ODPO loss Eq. (7c) (with $\beta=0.5$ ), we sample from the models two times with two different random seeds and report the results in Fig. 6.}\right)\right)$. Note that when $\alpha=0$ ODPO loss is exactly equal to DPO loss. After fine-tuning $\pi_{\boldsymbol{\theta}</p>
<p>We observe that higher values of $\alpha$ can lead to higher reward values at the expense of diverging from the SFT model. Lower values of $\alpha$ on average lead to lower reward values. On the Pareto front of the Fig. 6, points with the highest rewards are associated with models trained with $\alpha=1$, while points with lower $D_{\text{KL}}$ are fine-tuned with smaller $\alpha$ values.</p>
<h2>8 Related Work</h2>
<p>In this section, we review alternative methods to RLHF for aligning language models to human preferences.</p>
<p>Sequence-Level Contrastive Methods. The intuition that is shared by these methods is simple: they encourage the language model $\pi_{\boldsymbol{\theta}}$ to assign more probability mass to the preferred response compared to the dispreferred response(s). Zhao et al. (SLiC; 2023) employ a rank calibration loss that requires $\log \pi_{\boldsymbol{\theta}}\left(\boldsymbol{y}<em _boldsymbol_theta="\boldsymbol{\theta">{w} \mid \boldsymbol{x}\right)$ to be greater than $\log \pi</em>\right)$, by an offset $\delta$. Importantly, in their}}\left(\boldsymbol{y}_{l} \mid \boldsymbol{x</p>
<p>formulation $\delta$ is a hyperparameter and does not depend on the the responses $\boldsymbol{y}<em l="l">{w}, \boldsymbol{y}</em>$. Similarly, Yuan et al. (RRHF; 2023) uses a ranking loss without the offset. While intuitive, the objective that contrastive losses are optimizing for, and its connection to the RLHF objective (maximizing the expected reward) is unclear.</p>
<p>DPO and Variants. DPO proposes an alternative method to optimize the regularized expected rewards without using RL algorithms. Importantly, DPO shares the objective with conventional methods for RLHF (Stiennon et al., 2020), but the optimization is done without training a separate reward model nor using RL algorithms. Since the introduction of DPO, several follow-up studies attempted to improve DPO along different dimensions. To prevent DPO from overfitting to the preference dataset, Azar et al. (2023) introduce Identity Preference Optimization (IPO). IPO replaces the unbounded function of preference probabilities in DPO loss formulation with the bounded identity function. Ethayarajh et al. (2023) propose a method called Kahneman-Tversky Optimisation (KTO) that dispenses the need for paired preference data altogether. KTO's loss function relies on unpaired examples that are labeled either as "good" or "bad". Zhou et al. (2024) suggest another variant of DPO specifically for multi-objective alignment.</p>
<p>Our Approach. ODPO attempts to solve another shortcoming of DPO, which is to treat every preference pair equally and not take into account the extent to which the two responses differ from each other. ODPO's loss requires the estimated reward for the preferred response to be larger than the dispreferred response by an offset that depends on the difference between the quality of the responses.</p>
<h2>9 Conclusion</h2>
<p>We propose ODPO, a generalization of DPO for aligning language models with human preferences. Just as with DPO, ODPO does not rely on a pretrained reward model and does not require an RL algorithm. However, in contrast to DPO, ODPO does not treat every preference pair equally and incorporates the extent to which one response should be preferred over another in its loss function. Experiments on a variety of tasks suggest that ODPO is more effective than DPO in aligning language models to human preferences.</p>
<h2>Limitations</h2>
<p>Human Preference Data. Not all datasets with human feedback contain judgments regarding the extent to which one response is preferred over another. In our experiments, we focused on tasks and datasets where we had either access to such information or point-wise reward functions. The results presented in the paper provide strong motivation to ask humans to indicate their degree of preference when collecting human feedback data.</p>
<p>Offset values. Deciding how to scale offset values can depend on the task. In this study, we experimented with offsets based on Likert scores and classifier probabilities. We defer extending ODPO to different tasks for future work.</p>
<h2>Ethical Considerations</h2>
<p>We foresee two main ethical concerns regarding the use of direct preference optimization and, in general, any RLHF method for aligning language models with human preferences. First, as with DPO and other RLHF methods, malicious actors can use ODPO to steer the responses to generate harmful or toxic content. However, we must note that we foresee no particular reason for DPO to be more suitable for malicious use cases compared to DPO or other RLHF methods. Second, reward functions might inherit unwanted biases from the datasets that they were trained on. Therefore, fine-tuning to increase the expected reward can then inject such biases into the language models' generations. We view ODPO as a fine-tuning strategy that can work with any given reward function or preference dataset. Therefore, as we develop more accurate and less biased reward functions, or find more inclusive ways of collecting human feedback, less unwanted bias will propagate to the fine-tuning process.</p>
<h2>Acknowledgements</h2>
<p>We thank Alizée Pace for her feedback on an early version of this paper. Afra Amini is supported by ETH AI Center doctoral fellowship.</p>
<h2>References</h2>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in AI safety. Computing Research Repository, arXiv:1606.06565.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 technical report. Technical report, Google.</p>
<p>Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. 2023. A general theoretical paradigm to understand learning from human preferences. Computing Research Repository, arXiv:2310.12036.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Ralph Allan Bradley and Milton E. Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345 .</p>
<p>Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. 2023. Human-centered loss functions (HALOs). Technical report, Contextual AI.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, Online. Association for Computational Linguistics.</p>
<p>Kevin Gimpel and Noah A. Smith. 2010. Softmaxmargin CRFs: Training log-linear models with cost functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 733-736, Los Angeles, California. Association for Computational Linguistics.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, PoSen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soña Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. Computing Research Repository, arXiv:2209.14375.</p>
<p>Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. QUARK: Controllable Text generation with Reinforced Unlearning. In Advances in Neural Information Processing Systems, volume 35, pages 27591-27609. Curran Associates, Inc.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142-150, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Chris J. Maddison and Danny Tarlow. Gumbel machinery [online]. 2017.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1928-1937, New York, New York, USA. PMLR.</p>
<p>OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,</p>
<p>Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Roichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil</p>
<p>Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023. GPT-4 technical report. Technical report, OpenAI.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. In Advances in Neural Information Processing Systems, volume 36. Curran Associates, Inc.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. Computing Research Repository, arXiv:1707.06347.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedamij Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,</p>
<p>Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Technical report, Meta.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In International Conference on Learning Representations.</p>
<p>Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021. Recursively summarizing books with human feedback. Computing Research Repository, arXiv:2109.10862.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. RRHF: Rank responses to align language models with human feedback without tears. Computing Research Repository, arXiv:2304.05302.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. 2023. SLiC-HF: Sequence likelihood calibration with human feedback. Computing Research Repository, arXiv:2305.10425.</p>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023. Secrets of RLHF in large language models part i: PPO. Computing Research Repository, arXiv:2307.04964.</p>
<p>Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. 2024. Beyond one-preference-for-all: Multi-objective direct preference optimization. Computing Research Repository, arXiv:2310.03708.</p>
<p>Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020. Fine-tuning language models from human preferences. Computing Research Repository, arXiv:1909.08593.</p>
<h1>A Proof of Thm. 1</h1>
<p>Theorem 1. Let $\boldsymbol{y}<em l="l">{w}$ and $\boldsymbol{y}</em>}$ be two responses to a prompt $\boldsymbol{x}$, and let $\widehat{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$ and $\widehat{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$ be their associated estimated rewards. Finally, let $R_{w} \sim \operatorname{Gumbel}\left(\widehat{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right), 1\right)$ and $R_{l} \sim \operatorname{Gumbel}\left(\widehat{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>\right), 1\right)$ be Gumbel random variables. Then, we have,</p>
<p>$$
\begin{aligned}
\mathbb{P}\left(R_{w}-R_{l}&gt;0\right) &amp; =p_{\mathrm{BT}}\left(\boldsymbol{y}<em l="l">{w} \succ \boldsymbol{y}</em>\right) \
&amp; =\sigma\left(\Delta_{\widehat{r}_{\boldsymbol{\theta}}}\right)
\end{aligned}
$$} \mid \boldsymbol{x</p>
<p>where $p_{\mathrm{BT}}\left(\boldsymbol{y}<em l="l">{w} \succ \boldsymbol{y}</em>} \mid \boldsymbol{x}\right)$ is a Bradley-Terry model Eq. (5a) parameterized by $\widehat{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$ and $\widehat{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$, and $\Delta_{\widehat{r<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}} \stackrel{\text { def }}{=} \widehat{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em _boldsymbol_theta="\boldsymbol{\theta">{w}\right)-\widehat{r}</em>\right)$ is the difference between the estimated rewards.}}\left(\boldsymbol{x}, \boldsymbol{y}_{l</p>
<p>Proof. We define a random variable $I$ as,</p>
<p>$$
I \stackrel{\text { def }}{=} \underset{l, w}{\operatorname{argmax}}\left{R_{l}, R_{w}\right}
$$</p>
<p>To prove the theorem, it suffices to prove,</p>
<p>$$
\mathbb{P}(I=w)=\frac{\exp \left(\widehat{r}<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)\right)}{\exp \left(\widehat{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)\right)+\exp \left(\widehat{r<em l="l">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>
$$}\right)\right)</p>
<p>For ease of notation, we define $g_{\widehat{r}<em _boldsymbol_theta="\boldsymbol{\theta">{w}} \stackrel{\text { def }}{=} \operatorname{Gumbel}\left(\widehat{r}</em>}}\left(\boldsymbol{x}, \boldsymbol{y<em w="w">{w}\right), 1\right), \widehat{r}</em>} \stackrel{\text { def }}{=} \widehat{r<em w="w">{\boldsymbol{\theta}}\left(\boldsymbol{x}, \boldsymbol{y}</em>}\right)$, and $\widehat{r<em _boldsymbol_theta="\boldsymbol{\theta">{l} \stackrel{\text { def }}{=} \widehat{r}</em>\right)$. Then, consider the following manipulation}}\left(\boldsymbol{x}, \boldsymbol{y}_{l</p>
<p>$$
\begin{aligned}
\mathbb{P}(I=w) &amp; =\underset{m \sim g_{\widehat{r}<em l="l">{w}}}{\mathbb{E}}\left[\mathbb{P}\left(R</em>&lt;m\right)\right] \
&amp; =\int_{-\infty}^{+\infty} g_{\widehat{r}<em l="l">{w}}(m) \exp \left(-\exp \left(\widehat{r}</em> m \
&amp; =\int_{-\infty}^{+\infty} \exp \left(\widehat{r}}-m\right)\right) \mathrm{d<em w="w">{w}-m-\exp \left(\widehat{r}</em>}-m\right)\right) \exp \left(-\exp \left(\widehat{r<em -_infty="-\infty">{l}-m\right)\right) \mathrm{d} m \
&amp; =\int</em>}^{+\infty} \exp \left(\widehat{r<em w="w">{w}\right) \exp (-m) \exp \left(-\exp (-m)(\underbrace{\exp \left(\widehat{r}</em>}\right)+\exp \left(\widehat{r<em>{l}\right)}</em> m \
&amp; =\exp \left(\widehat{r}})\right) \mathrm{d<em -_infty="-\infty">{w}\right) \int</em> m \
&amp; =\frac{\exp \left(\widehat{r}}^{+\infty} \exp (-m) \exp (-Z \exp (-m)) \mathrm{d<em w="w">{w}\right)}{\exp \left(\widehat{r}</em>
\end{aligned}
$$}\right)+\exp \left(\widehat{r}_{l}\right)</p>
<h2>B Datasets Statistics</h2>
<p>We used the following datasets for either fine-tuning or prompting language models, which is consistent with the intended use case of the datasets. All the datasets are in English. We refer to the corresponding papers for data collection and postprocessing procedures, as well as the demographics of human annotators.</p>
<p>Sentiment Control. We train the SFT model on the train set of IMDB dataset (Maas et al., 2011), which consists of 25000 movie reviews. For fine-tuning with DPO and ODPO, we sample 10000 prompts from the train set and use the language model to generate continuations. For evaluation, we sample 256 prompts from the test set of IMDB dataset.</p>
<p>Toxicity Control. We sample 10000 prompts for fine-tuning and 256 for evaluation from REALToXICITYPrompts (Gehman et al., 2020). The dataset is released under Apache-2.0 license.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\left\lvert\, \mathcal{D}_{\mathrm{HF}}\right.$</th>
<th style="text-align: center;">$=5000$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\left\lvert\, \mathcal{D}_{\mathrm{HF}}\right.$</th>
<th style="text-align: center;">$=7500$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\left\lvert\, \mathcal{D}_{\mathrm{HF}}\right.$</th>
<th style="text-align: center;">$=10000$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">$D_{\mathrm{KL}}\left(\pi_{\boldsymbol{\theta}}\left|\pi_{\mathrm{SFT}}\right.\right.$</td>
<td style="text-align: center;">$\downarrow$</td>
<td style="text-align: center;">Reward $\uparrow$</td>
<td style="text-align: center;">$D_{\mathrm{KL}}\left(\pi_{\boldsymbol{\theta}}\left|\pi_{\mathrm{SFT}}\right.\right.$</td>
<td style="text-align: center;">$\downarrow$</td>
<td style="text-align: center;">Reward $\uparrow$</td>
<td style="text-align: center;">$D_{\mathrm{KL}}\left(\pi_{\boldsymbol{\theta}}\left|\pi_{\mathrm{SFT}}\right.\right.$</td>
<td style="text-align: center;">$\downarrow$</td>
<td style="text-align: center;">Reward $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">SLiC</td>
<td style="text-align: center;">$0.55[0.52,0.56]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.71[0.71,0.72]$</td>
<td style="text-align: center;">$3.56[3.24,3.88]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.81[0.81,0.82]$</td>
<td style="text-align: center;">$209.42[85.94,333.7]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.85[0.84,0.86]$</td>
</tr>
<tr>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">$0.39[0.37,0.4]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.72[0.71,0.73]$</td>
<td style="text-align: center;">$4.13[3.42,5.27]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.83[0.82,0.84]$</td>
<td style="text-align: center;">$32.32[27.57,36.68]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.86[0.85,0.86]$</td>
</tr>
<tr>
<td style="text-align: center;">ODPO</td>
<td style="text-align: center;">$\mathbf{0 . 3 5}[0.32,0.39]$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}[0.75,0.78]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{3 . 0 9}[2.43,4.04]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 8 7}[0.87,0.88]$</td>
<td style="text-align: center;">$\mathbf{1 8 . 7 3}[12.15,29.65]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 8 9}[0.88,0.89]$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparing ODPO to SLiC-HF on sentiment control task. In all 3 experimental setups ODPO achieves lower KL values and higher rewards compared to SLiC.</p>
<p>Summarization. We take 20000 posts under relationships and relationship_advice subreddits in REDDIT TL;DR dataset (Stiennon et al., 2020) and 100 posts for evaluation. The dataset is released under a modified MIT license.</p>
<h1>C Computational Budget</h1>
<p>For sentiment control experiments, a single fine-tuning and evaluation run takes approximately 20 minutes on 2 rtx_4090 GPUs. For toxicity control experiments, a single fine-tuning and evaluation run takes approximately 2 hours on 2 a100_40gb GPUs. For the summarization task, a single fine-tuning and evaluation run takes approximately 15 hours on 2 a100_80gb GPUs. Notably, the reported runtimes are the same for DPO and ODPO, and there is no extra computation cost for ODPO compared to DPO.</p>
<h2>D Comparison to SLiC-HF</h2>
<p>We compare DPO and ODPO to SLiC-HF (Zhao et al., 2023) on the sentiment control task. Following Zhao et al. (2023), we set the margin hyperparameter to 1 for SLiC and $\beta$ to 0.5 for DPO and ODPO. We observe that none of the SLiC runs end up on the Pareto frontier of KL divergence and reward. We report the mean values and 0.9 confidence intervals for $D_{\mathrm{KL}}\left(\pi_{\boldsymbol{\theta}} | \pi_{\mathrm{SFT}}\right)$ and reward in Tab. 1 for 3 dataset sizes.</p>
<h2>E GPT-4 Evaluation Prompt for the Summarization Task</h2>
<p>Following Rafailov et al. (2023) we use the prompt below to evaluate the win rates of generated summaries against human-written summaries.</p>
<p>Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? A good summary is both concise and precise.</p>
<p>Post:
<post>
Summary A:
<summary_a>
Summary B:
<summary_b>
FIRST provide a one-sentence comparison of the two summaries, explaining which you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your choice. Your response should use the format:</p>
<p>Comparison: <one-sentence comparison and explanation>
Preferred: &lt;"A" or "B"&gt;</p>
<h2>F Qualitative Results</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Post</th>
<th style="text-align: center;">We have been dating for two and a half months, but we were friends in <br> high school. We are both divorced, but her kids' dad takes them maybe <br> one day a week. By their bedtime she's too tired to stay up for time for <br> herself. I have more free time because I only have my kids for half the <br> time. <br> What can I say to her to help her get over the mentality she has to take <br> care of all her problems alone? I'm willing to invest whatever it takes <br> because we get along so well. Ask me any info that I may have left out.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DPO Summary</td>
<td style="text-align: center;">dating with benefits is giving my girlfriend anxiety and a mentality of <br> taking care of her problems alone even though she said she wanted <br> a relationship, not a one-man show. Do I need a break-up? Ask me <br> anything.</td>
</tr>
<tr>
<td style="text-align: center;">ODPO Summary</td>
<td style="text-align: center;">Girl is tired when hanging out with me. What can I say to her to help her <br> get over the mentality she has to take care of all her problems alone? We <br> get along so well. Any info that I may have mentioned/left out.</td>
</tr>
<tr>
<td style="text-align: center;">Judgment</td>
<td style="text-align: center;">Summary A [DPO] brings up unnecessary details and confusion while <br> Summary B [ODPO] is concise, straightforward and better delivers the <br> post's message.</td>
</tr>
</tbody>
</table>
<p>Table 2: An example of two summaries sampled with temperature 1, and its corresponding judgment from GPT-4. Summaries are shown to the model in random order and are tagged with A or B. For clarity, we add the method in brackets.</p>
<table>
<thead>
<tr>
<th>Post</th>
<th>I don't really have a question or anything. I just need to vent and maybe <br> reach out to those who have been in similar situations. <br> My sister told me yesterday that she's pregnant. At first I was thrilled. <br> But then reality hit me. <em>I can't have children.</em> <br> I'm so happy for my sister and my brother-in-law, I really am. But they <br> weren't planning on having children for at least a few more years. My <br> sister's never really wanted to be a mom. Me, on the other hand, have <br> always lived for it. As a kid when people asked what I want to be when I <br> grow up, I always said, "a mom." <br> But, 3 years ago I found out I can never have children. I was devastated <br> but have tried to tough it out, tried to believe it isn't true for years. I'm in <br> a serious relationship and we're planning on getting married in a little <br> over a year and starting to <strong>try</strong> having kids immediately, but we will <br> probably end up adopting. While adopting is wonderful and that thought <br> excites me, I still so badly want to be pregnant one day. <br> My sister being pregnant is like a kick in the side. A reminder that I'll <br> never be pregnant. I don't want to be around her, I don't want to be happy <br> for her. I'm jealous. I don't know how to deal with this. I never expected <br> to feel anger towards my sister for getting pregnant. But really, how am I <br> expected to feel about this? <br> I would never dare tell my sister or anyone else in my family how I feel <br> about this. I wouldn't want to upset anyone and I really don't want to be <br> selfish and make this about me. It's just so hard.</th>
</tr>
</thead>
<tbody>
<tr>
<td>DPO Summary</td>
<td>I have a sister who's pregnant and I feel immensely jealous and sad and I <br> don't know how to deal with my feelings. I don't want to have children.</td>
</tr>
<tr>
<td>ODPO Summary</td>
<td>My sister is pregnant and I'm jealous and sad even though I'm happy for <br> her and I would love to be pregnant one day. I don't know how to feel. I <br> really just want to vent.</td>
</tr>
<tr>
<td>Judgment</td>
<td>Summary A [ODPO] better captures the overall emotion, situation, and <br> dilemma of the original post, while Summary B [DPO] falsely claims <br> that the poster does not want children, which contradicts the information <br> provided in the original post.</td>
</tr>
</tbody>
</table>
<p>Table 3: An example of two summaries sampled with temperature 0.75 , and its corresponding judgment from GPT-4. Summaries are shown to the model in random order and are tagged with A or B. For clarity, we add the method in brackets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Post</th>
<th style="text-align: center;">We have been dating for two and a half months, but we were friends in <br> high school. We are both divorced, but her kids' dad takes them maybe <br> one day a week. By their bedtime she's too tired to stay up for time for <br> herself. I have more free time because I only have my kids for half the <br> time. <br> What can I say to her to help her get over the mentality she has to take <br> care of all her problems alone? I'm willing to invest whatever it takes <br> because we get along so well. Ask me any info that I may have left out.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DPO Summary</td>
<td style="text-align: center;">My girlfriend has to take care of her kids and her problems because she <br> doesn't have time to do anything for herself. How can I help her?</td>
</tr>
<tr>
<td style="text-align: center;">ODPO Summary</td>
<td style="text-align: center;">My girlfriend of two months is constantly tired from taking care of her <br> kids. How can I help her get over the mentality she has to take care of <br> everything alone?</td>
</tr>
<tr>
<td style="text-align: center;">Judgment</td>
<td style="text-align: center;">Summary B [ODPO] is more concise and accurate as it mentions the <br> key point about the poster wanting to help his girlfriend overcome her <br> mentality of taking care of everything by herself, compared to Summary <br> A [DPO] which does not convey this aspect clearly.</td>
</tr>
</tbody>
</table>
<p>Table 4: An example of two summaries sampled with temperature 0.5 , and its corresponding judgment from GPT-4. Summaries are shown to the model in random order and are tagged with A or B. For clarity, we add the method in brackets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Post</th>
<th style="text-align: center;">I am currently in college and I met this wonderful girl. We have gone on a few dates and everything appeared to be going great. She wanted to taking things quite slow because she was getting over a recent ex boyfriend. <br> However when I met her roommate she brought up that this girl has a boyfriend who has been acting rather shady towards her. He goes to another college and he has been spending the night "on the couch" at this other girls place frequently. He also has recently been posting on her wall and now I'm rather confused. <br> I know something is up but I really don't know how to handle this. I would really like to be straight forward with her but she seems dodgy whenever I do. I really am beginning to over think things and I almost feel like I was just there to shove in his face. <br> I really hate this situation because I feel like I could have a legitimate relationship with this girl.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DPO Summary</td>
<td style="text-align: center;">Met a girl who has a boyfriend who is spending the night at her place frequently. She has been posting on his wall and I feel like she is hiding something. I don't know how to proceed.</td>
</tr>
<tr>
<td style="text-align: center;">ODPO Summary</td>
<td style="text-align: center;">met girl, she has a boyfriend who is acting shady towards her, she wants to take things slow, I'm confused as to what is going on.</td>
</tr>
<tr>
<td style="text-align: center;">Judgment</td>
<td style="text-align: center;">Summary A [ODPO] provides more accurate information about the situation, specifically pointing out that the girl wants to take things slow, and expresses the poster's confusion more effectively, whereas, Summary B [DPO] provides false information about the girl posting on the boyfriend's wall.</td>
</tr>
</tbody>
</table>
<p>Table 5: An example of two summaries sampled with temperature 0.25 , and its corresponding judgment from GPT-4. Summaries are shown to the model in random order and are tagged with A or B. For clarity, we add the method in brackets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Post</th>
<th style="text-align: center;">My best friend and I have been friends for around 5 years now? She and her boyfriend has been together for a few years alr too. She is the kind of person who always put relationship over friendship, and because she's my best friend, I understand her and never said anything. <br> I understand when she stop replying me for days when she stays over at her boyfriend's house. I understand when she ask me to leave early because she wanted to spend time with her boyfriend. I understand when she said I'm not allowed to be in contact with her boyfriend.(One time she got angry and ignored me for a month because I called her boyfriend to come over when they had a fight) <br> I'm really okay with her being like this. Her boyfriend is an important part of her life and I can accept that. But recently, I can't even get a reply from her. Her text replies are always short and cold. We go to different schools so I don't get to talk/see her everyday. I'm always the one looking for a topic to talk about with her. She only comes looking for me if she and her boyfriend has a fight and she needed somebody to talk to or when her boyfriend is too busy to talk or go out with her. <br> I had confronted her about it, but she said that it's not like that, she didn't mean to make me feel this way. But after that it's always back to the same situation and I always end up feeling used and neglected. <br> Any advice?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DPO Summary</td>
<td style="text-align: center;">My best friend doesn't reply me anymore and I feel like she doesn't care about me anymore. She always put her boyfriend over me. I'm okay with her being like this, but I'm not okay with feeling like she doesn't care about me anymore. Any advice?</td>
</tr>
<tr>
<td style="text-align: center;">ODPO Summary</td>
<td style="text-align: center;">Best friend of 5 years doesn't reply to my texts anymore and always puts her boyfriend over me. I feel like she doesn't care about me anymore. Any advice?</td>
</tr>
<tr>
<td style="text-align: center;">Judgment</td>
<td style="text-align: center;">Summary B [ODPO] is preferred as it encapsulates all the necessary details more concisely and accurately without any unnecessary repetition.</td>
</tr>
</tbody>
</table>
<p>Table 6: An example of two summaries generated with greedy decoding, and its corresponding judgment from GPT-4. Summaries are shown to the model in random order and are tagged with A or B. For clarity, we add the method in brackets.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ For $\beta&gt;0.5$, we observe that no toxicity reduction compared to the SFT model; therefore, for this experiment we only test $\beta \leq 0.5$.
${ }^{11}$ More specifically, we take the posts under relationships and relationship_advice subreddits. For more details regarding the datasets used refer to App. B.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{12}$ Specifically, we use https://huggingface.co/ CarperAl/openai_summarize_tldr_sft
${ }^{14}$ Following Rafailov et al. (2023), we set $\beta=0.5$ for both methods.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7329 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7329</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7329</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-269148410</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.09654v3.pdf" target="_blank">Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in Zero-shot Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7329",
    "paper_id": "paper-269148410",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0056507499999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in Zero-shot Anomaly Detection
7 Apr 2025</p>
<p>Jiaqi Zhu jiaqi_zhu@bit.edu.cn 
Shaofeng Cai shaofeng@comp.nus.edu.sg 0000-0001-8605-076X
Fang Deng dengfang@bit.edu.cn 0000-0002-1111-7285
Beng Chin Ooi 
Junran Wu junran@nus.edu.sg </p>
<p>Beijing Institute of Technology
BeijingChina</p>
<p>National University
Singapore Singapore</p>
<p>Beijing Institute of Technology
BeijingChina</p>
<p>National University
Singapore Singapore</p>
<p>National University
Singapore Singapore</p>
<p>Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in Zero-shot Anomaly Detection
7 Apr 202589EFAD90AC6F1705BD7C42DFD90CAE6310.1145/3664647.3681190arXiv:2404.09654v3[cs.CV]Anomaly DetectionLarge Language ModelZero-shot Learning
Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language.Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts.However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization.In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model.We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM).This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation.We further introduce a novel fine-grained aligner to fuse local pixellevel semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces.Extensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec and 8.9% on VisA compared to state-of-the-art approaches.CCS Conceptsâ€¢ Computing methodologies â†’ Computer vision tasks.</p>
<p>Introduction</p>
<p>Visual anomaly detection (VAD) has gained momentum in a wide spectrum of domains, including industrial quality control [2,39,43,58], video surveillance [11,15,27], medical diagnostics [4,[33][34][35] etc.This complex task involves both anomaly classification and localization for images, i.e., image-level and pixel-level anomaly detection.Inevitably, VAD faces two fundamental challenges due to the nature of its detection targets.First, the diversity of image objects makes the categories of anomalies a long-tail distribution [21,45,64].To address the diverse range of images, a universal, category-agnostic model is required, as opposed to the traditional approach of deploying dedicated models for specific visual inspection tasks.The latter approach is unscalable and inefficient due to the long tail characteristic of the problem [29,37].Second, anomaly images are rare and have great variations [7,20,65].In real-world applications like industrial VAD, collecting a sufficient and diverse training sample set is both costly and time-consuming.This scarcity complicates the training of traditional one-class or unsupervised VAD models, especially in cold-start scenarios [43].</p>
<p>The introduction of zero-shot methods offers a promising solution to these challenges.The emergence of large-scale models [25,41] has revolutionized VAD profoundly.Recently, several large vision-language models (LVLMs) have been introduced for zeroshot VAD [5,14,18,20,62].These works harness the exceptional generalization ability of LVLMs, pre-trained on millions of imagetext pairs, which showcase promising zero-shot performance in both seen and unseen objects.Nonetheless, due to the inherent lack of comprehensive information on data and the absence of explicit supervision, the zero-shot regime remains particularly challenging, with significant potential yet to be exploited compared to fully-supervised benchmarks.</p>
<p>There are two major limitations.First, existing works rely on fixed textual descriptions of images, termed anomaly prompts, including both abnormal and normal prompts.In LVLM-based VAD, anomaly prompts elucidate the semantics of normalities and anomalies and guide the vision modules on how the two states are defined, the quality of which, therefore, plays a critical role in the zero-shot detection capability of LVLMs.The current practice of manually crafting prompts demands extensive domain expertise and considerable time, while also facing the challenge of cross-semantic ambiguity, which is illustrated in Figure 1 and will be discussed in depth in Sec.4.2.This limitation calls for more informative and adaptive anomaly prompts.Second, although LVLMs, trained for image-text cross-modal alignment, can detect anomalies globally by aligning image-level representations with anomaly prompts, they face difficulties in localizing anomalies precisely, i.e., achieving pixel-level detection.Such local pixel-level alignment is central to zero-shot anomaly segmentation [20].</p>
<p>In this paper, we focus on zero-shot modeling and address the limitations of existing models with a proposal called ALFA -Adaptive LLM-empowered model for zero-shot visual anomaly detection with Fine-grained Alignment.We introduce a run-time prompt adaptation strategy to efficiently generate informative and adaptive anomaly prompts, which obviates the need for laborious expert creation and tackles cross-semantic ambiguity.Leveraging the zeroshot capabilities of an LLM [3] that is renowned for its proficient instruction-following abilities [26], this strategy automatically generates diverse informative anomaly prompts for VAD.Next, we present a contextual scoring mechanism to adaptively tailor a set of anomaly prompts for each query image.To fully excavate the local pixel-level semantics, we further propose a novel fine-grained aligner that generalizes the image-text alignment projection from global to local semantic space for precise anomaly localization.This cross-modal aligner enables ALFA to achieve global and local VAD within one unified model without requiring extra data or tuning.We summarize our main contributions as follows:</p>
<p>â€¢ We identify a previously unaddressed issue of cross-semantic ambiguity.In response, we present ALFA, an adaptive LLMempowered model for zero-shot VAD, effectively resolving this challenge without the need for extra data or fine-tuning.â€¢ We propose a run-time prompt adaptation strategy that effectively generates informative anomaly prompts and dynamically adapts a set of anomaly prompts on a per-image basis.â€¢ We develop a fine-grained aligner that learns global to local semantic space projection, and then, generalizes this projection to support precise pixel-level anomaly localization.â€¢ Our comprehensive experiments validate ALFA's capacity for zero-shot VAD across diverse datasets.Moreover, ALFA can be readily extended to the few-shot setting, which achieves stateof-the-art results that are on par or even outperform those of full-shot and fine-tuning-based methods.</p>
<p>Related work</p>
<p>Vision-language modeling.Large Language Models (LLMs) such as GPT [3] and LLaMA [51] have achieved remarkable performance on NLP tasks.Since the introduction of CLIP [41], large Visual-Language Models (LVLMs) like MiniGPT-4 [63], BLIP-2 [30], and PandaGPT [49] have shown promise across a range of languageguided tasks.Without additional fine-tuning, text prompts can be used to extract knowledge in the downstream image-related</p>
<p>LLMï¸ LVLM Encoderï¸</p>
<p>Multi-scale Image Input</p>
<p>A normal image of wood displays a uniform grain pattern with consistent coloration.</p>
<p>Fine-grained Aligner</p>
<p>An abnormal image of wood displays faint discolorations and streaks.</p>
<p>An abnormal image of wood reveals irregular stains.</p>
<p>Contextual Scoring Anomaly? tasks such as zero-shot classification [38], object detection [23], and segmentation [54].Consequently, LVLMs offer the potential to advance language-guided anomaly detection in a zero-shot manner.</p>
<p>In this paper, we delve deeper into exploring how to optimize the utilization of LVLMs for visual anomaly detection (VAD).</p>
<p>Visual anomaly detection.Given the scarcity of anomalies, conventional VAD approaches primarily focus on unsupervised or self-supervised methods relying exclusively on normal images.These approaches fall into two main categories: generative models [29,37,42,55,65,66] that utilize an encoder-decoder framework to minimize the reconstruction error, and feature embedding-based models that detect anomalies by discerning variations in feature distribution between normal and abnormal images.The latter includes one-class methods [36,44,52], memory-based models [17,40,43] and knowledge distillation models [1,13,46,57] hinging on the knowledge captured by networks pre-trained on large dataset.</p>
<p>Recent research has delved into zero-shot VAD, reducing reliance on either normal or abnormal images and offering a unified anomaly detection model applicable across various image categories [5,14,18,20,31,62,67].Notably, WinCLIP [20] pioneers the potential of language-driven zero-shot VAD, leveraging CLIP to extract and aggregate multi-scale image features.MuSc [31] proposes a looser zero-shot approach that utilizes a pre-trained Vision Transformer (ViT) [16] to extract patch-level features and assesses anomaly scores by comparing the similarity of patches between the query image and hundreds of unlabeled images.However, these approaches still suffer from several limitations, which require manual prompt crafting, intricate post-processing of extra data and fine-tuning.In contrast, ALFA is a training-free model for zero-shot VAD, obviating the need for extra data or tuning, and generates informative and adaptive prompts without costly manual design.Probing through visual prompt engineering.In VAD, prompts describe image content to assess anomaly levels by aligning with both normal and abnormal prompts.Traditional prompt engineering [28,32,48] that adjusts the model with learnable tokens is unsuitable due to data requirements.Existing efforts [1,5,9,14,18,20,50] typically hand craft numerous descriptions for detection, e.g., WinCLIP [20]   and SAA [5] employing a prompt regularization strategy.However, these predefined-based approaches are inefficient and suboptimal.Recent studies have explored using LLMs to generate prompts for object recognition [23,56], potentially alleviating the challenge of inefficient prompt design.However, directly applying this approach to VAD tasks leads to cross-semantic ambiguity, caused by textual descriptions encompassing various aspects of an image, some of which may not be present or prominent in the query image.To avoid this, this paper proposes a run-time prompt adaptation strategy utilizing an LLM, coupled with a contextual scoring mechanism, to generate informative and adaptive prompts, which effectively addresses the cross-semantic issue.</p>
<p>Preliminary 3.1 Visual Anomaly Detection</p>
<p>Anomaly detection aims to detect data samples that deviate from the majority or exhibit unusual patterns.Particularly, this paper focuses on visual anomaly detection (VAD), the objectives of which are to (1) detect anomalies globally for images, and (2) localize anomalies for pixels of each image locally, as formulated below: Definition 1 (Visual anomaly detection).Given an image  âˆˆ R  Ã— Ã— , VAD aims to predict whether  and all its individual pixel    , are anomalous or not, where 0 â‰¤  &lt;  and 0 â‰¤  &lt;  .</p>
<p>In this study, we seek to develop a category-agnostic VAD approach that exhibits generalizability across categories   âˆˆ C, allowing the model to readily adapt to new categories without model retraining or parameter fine-tuning.Formally, for âˆ€ âˆˆ   , VAD can be achieved by computing anomaly scores   ,   = M (; Î˜  ) for both global image-level   âˆˆ [0, 1] and local pixel-level   âˆˆ [0, 1]  Ã— Ã—1 , using a detection model M (â€¢) parameterized by Î˜  .</p>
<p>Zero-shot Anomaly Detection with LVLMs</p>
<p>LVLMs provide a unified representation for both vision and language modalities, leveraging contrastive learning-based [8]  LVLMs can be adopted for zero-shot language-guided anomaly detection for images.For instance, given an image   , two predefined text templates, i.e., "a photo of a normal [  ]" and "a photo of an abnormal [  ]" and the extracted text tokens  + and  âˆ’ correspondingly, anomaly detection is achieved by exploiting the visual information extracted by the image encoder and computing an anomaly score for category   :
ğ‘† (ğ‘¥ ğ‘ ğ‘– ğ‘— ) = exp(&lt; ğ‘“ (ğ‘¥ ğ‘ ğ‘– ğ‘— ), ğ‘”(ğ‘¡ âˆ’ ) &gt;) ğ‘¡ âˆˆ {ğ‘¡ + ,ğ‘¡ âˆ’ } exp(&lt; ğ‘“ (ğ‘¥ ğ‘ ğ‘– ğ‘— ), ğ‘”(ğ‘¡) &gt;)(1)
which basically measures the proximity of the image   to the abnormal text template of category   by tapping into the visionlanguage alignment capability of LVLMs.</p>
<p>Methodology 4.1 Overview</p>
<p>In this paper, we propose an LLM-empowered LVLM model ALFA for zero-shot VAD.As shown in Figure 2, ALFA first introduces a run-time prompt (RTP) adaptation strategy to generate informative prompts and adaptively manage a collection of prompts on a per-image basis via a contextual scoring mechanism (see Sec. 4.2).Unlike conventional run-time adaptation approaches, which require fine-tuning their pre-trained models during inference, our strategy functions without the requirement for any parameter update.Furthermore, we present a training-free fine-grained aligner to bridge the cross-modal gap between global and local semantic spaces, enabling precise zero-shot anomaly localization (see Sec. 4.3).</p>
<p>Run-time Prompt Adaptation</p>
<p>The quality of textual prompts significantly influences the zero-shot detection capabilities of LVLMs.A normal image of carpet contains fibers that are evenly distributed and lie in the same direction.</p>
<p>An abnormal image of carpet would look like a pattern of dirt or stains that is not uniform or expected.</p>
<p>An abnormal image of carpet looks like a rug that is torn, frayed, or discolored. of our prompt generation and adaptation process, with more details elaborated below.</p>
<p>Informative prompt generation.Staying in line with the prompt learning trend [24,61], we first employ general expert knowledge to initialize the contrastive-state prompts, unlocking LVLMs' knowledge guided by language.We design unified templates with specific contents to generate comprehensive prompts covering task-relevant concepts thoroughly, which contrasts with prior approaches that either manually define image descriptions or integrate complex multifaceted prompts [5,14].Next, we decompose the unified anomaly prompt into components as: "A {domain} image of a {state} {class} [with {specific details}]", encompassing domain, state, and optional specific details sections.Then, we can readily generate contrastive-state prompts T  = {t +  , t âˆ’  } as the base anomaly detector, where t
+ ğ‘ğ‘  = {ğ‘¡ + ğ‘ğ‘ ,0 , â€¢ â€¢ â€¢ , ğ‘¡ + ğ‘ğ‘ ,ğ‘› + ğ‘ğ‘  }, t âˆ’ ğ‘ğ‘  = {ğ‘¡ âˆ’ ğ‘ğ‘ ,0 , â€¢ â€¢ â€¢ , ğ‘¡ âˆ’ ğ‘ğ‘ ,ğ‘› âˆ’ ğ‘ğ‘  }. ğ‘› +
 and  âˆ’  indicate the number of normal and abnormal prompts generated by the unified template.</p>
<p>Recognizing the potential for domain gaps to introduce language ambiguity, especially with a generic prompt, base anomaly detector derived from the unified template falls short.LLMs are repositories of extensive world knowledge spanning diverse domains, serving as implicit knowledge bases that facilitate effortless natural language queries [10].This knowledge includes visual descriptors, enabling LLMs to furnish insights into image features.To avoid the costly and non-scalable practice of manually crafting prompts using domain-specific knowledge, we efficiently tap into LLMs for more informative prompts.To this end, we design prompts to query an LLM, e.g., "How to identify an abnormal bottle in an image?".We can derive precise descriptions of a wide range of objects in normal and abnormal states as
T ğº = {t + ğ‘” , t âˆ’ ğ‘” }, where t + ğ‘” = {ğ‘¡ + ğ‘”,0 , â€¢ â€¢ â€¢ , ğ‘¡ + ğ‘”,ğ‘› + ğ‘” }, t âˆ’ ğ‘” = {ğ‘¡ âˆ’ ğ‘”,0 , â€¢ â€¢ â€¢ , ğ‘¡ âˆ’ ğ‘”,ğ‘› âˆ’
 }, and  +  and  âˆ’  indicate the number of normal and abnormal prompts generated by LLM.Remark.In dealing with the diverse and unpredictable nature of anomalies, language offers the essential information to discern defects from acceptable deviations.Building upon the insights from [38], we can enhance interpretability in VAD decisions by leveraging the capabilities of LLMs.Specifically, LLM can be employed to produce feature descriptions regarding anomalies.These descriptions can be provided to the LVLM to compute the logarithmic probability of each description pertaining to the query image.By examining the descriptors with high scores, we can gain insights into the model's decision.More details are provided in Section 5.4.</p>
<p>Cross-semantic ambiguity.In an ideal scenario, LVLMs for zeroshot VAD should be capable of recognizing the close correlation between normal images and their respective normal prompts, while identifying a more distant association with abnormal prompts.The relative distances to normal and abnormal prompts are crucial for LVLMs to detect anomalies effectively.However, by visualizing the semantic space of LVLMs, we observed an overlap and intersection in the feature distributions of both normal and abnormal prompts, as depicted in Figure 4 (a).This leads to situations where the features of certain anomalous images are closer to normal prompts while being distant from certain abnormal prompts.We refer to this phenomenon as cross-semantic ambiguity.We attribute this phenomenon to the intricate nature of textual descriptions and the semantic correlation between text and image.This is exacerbated by prompts covering diverse aspects of the image, some of which might not be salient or even absent in certain images.Anomaly detection is thus susceptible to cross-semantic ambiguity poisoning.Therefore, there is a pressing need for an effective remedy to adaptively manage a set of normal and abnormal anomaly prompts corresponding to each query image without semantic overlap.Contextual scoring mechanism.To address the persistent challenge of the cross-semantic ambiguity in VAD, we propose a contextual scoring mechanism, which adaptively adjusts a set of anomaly prompts on a per-image basis.</p>
<p>Specifically, given a query image  âˆˆ R  Ã— Ã— and the vanilla anomaly prompts T  := {T  , T  } = {t +  , t âˆ’  , t +  , t âˆ’  }, their embeddings can be obtained using the pre-trained image and text encoders of LVLMs, denoted as  () âˆˆ R  and () âˆˆ R  , where  âˆˆ T  and  denote the dimension of the latent semantic space.We calculate the cosine similarity between the embeddings of the query image  and normal {t +  , t +  } and abnormal prompts {t âˆ’  , t âˆ’  } respectively as:
ğ‘‘ + ğ‘– (ğ‘¥) = &lt; ğ‘“ (ğ‘¥), ğ‘”(ğ‘¡ + ğ‘– ) &gt;, ğ‘¡ + ğ‘– âˆˆ {t + ğ‘ğ‘  , t + ğ‘” }(2)ğ‘‘ âˆ’ ğ‘— (ğ‘¥) = &lt; ğ‘“ (ğ‘¥), ğ‘”(ğ‘¡ âˆ’ ğ‘— ) &gt;, ğ‘¡ âˆ’ ğ‘— âˆˆ {t âˆ’ ğ‘ğ‘  , t âˆ’ ğ‘” }(3)
Ideally, the distances between images and prompts for normal and abnormal categories should fall into two non-overlapping intervals.Specifically, normal images should be closer to normal prompts, while their distance to abnormal prompts should be farther, and vice versa for abnormal images.However, in practice, considering the heterogeneous nature of textual descriptions, not all descriptions of normal or abnormal conditions can be observed in a single image, which leads to the presence of some redundant or even noisy prompts that could negatively impact the model's detection performance.In this regard, we formulate the contextual score as a logistic function [22] to quantify the prompt's impact in discerning abnormalities from normal occurrences.For each prompt  âˆˆ T  , its contextual score is calculated as follows,
S ğ‘ (ğ‘¡) = D +âˆ’ D +âˆ’ + ğ‘’ âˆ’ğ‘˜ D +âˆ’(4)
where D +âˆ’ = ||D (  (),  +  ) âˆ’ D (  (),  âˆ’  )||, and   () represents the cosine similarity between the prompt  and the query image , and  is an adjustment parameter that controls the slope of the scoring function.Empirically, we set  to 1, ensuring the scoring function exhibits a moderate rate of change beyond the interval.D (â€¢, â€¢) is used to calculate the distance between a point and an interval as follows,
D (ğ‘‘ ğ‘¥ (ğ‘¡ ), ğ‘‘ + ğ‘– ) = max(0, max(min ğ‘– {ğ‘‘ + ğ‘– (ğ‘¥ ) } âˆ’ğ‘‘ ğ‘¥ (ğ‘¡ ), ğ‘‘ ğ‘¥ (ğ‘¡ ) âˆ’max ğ‘– {ğ‘‘ + ğ‘– (ğ‘¥ ) } ) ) D (ğ‘‘ ğ‘¥ (ğ‘¡ ), ğ‘‘ âˆ’ ğ‘— ) = max(0, max(min ğ‘— {ğ‘‘ âˆ’ ğ‘— (ğ‘¥ ) } âˆ’ğ‘‘ ğ‘¥ (ğ‘¡ ), ğ‘‘ ğ‘¥ (ğ‘¡ ) âˆ’max ğ‘— {ğ‘‘ âˆ’ ğ‘— (ğ‘¥ ) } ) )
The contextual score S  () of the prompt  is constrained within the range [0, 1).Considering the interval
[min ğ‘– {ğ‘‘ + ğ‘– (ğ‘¥)}, max ğ‘– {ğ‘‘ + ğ‘– (ğ‘¥)}] and [min ğ‘– {ğ‘‘ âˆ’ ğ‘– (ğ‘¥)}, max ğ‘– {ğ‘‘ âˆ’ ğ‘– (ğ‘¥)}],
when the distance between the prompt and the query image in the semantic space   () places farther from another interval than the one it belongs to, the contextual score approaches 1, indicating a strong relevance, and vice versa.In cases where the distance   () straddles both intervals, the score settles at 0, indicating an indeterminate relevance.Therefore, during inference, we employ the contextual scoring mechanism to filter out prompts with a contextual score of 0, retaining only those in non-overlapping intervals, represented as T := {T + , T âˆ’ }, with T + and T âˆ’ representing the normal and abnormal prompts.</p>
<p>We outline the procedure of RTP adaptation in Algorithm 1, and visualize the feature distribution of prompts processed through the contextual scoring mechanism in Figure 4 (b), which demonstrates that the proposed contextual score effectively addressed the cross-semantic ambiguity.Notably, the anomaly prompt T varies depending on the specific query image, which aligns with the intuitive notion that prompts and their numbers tailored to different object classes should naturally differ.Even within the same class, different images necessitate different emphases on individual prompts.Consequently, the implementation of the contextual scoring mechanism offers an adaptive approach to managing a set of prompts on a per-image basis, which enables the selected prompts that are better suited to the unique characteristics of each query image, thus enhancing the overall effectiveness of anomaly detection.</p>
<p>Fine-grained Aligner</p>
<p>Since anomaly localization requires predicting anomalies at the pixel-level, acquiring dense visual features is necessary.However, LVLMs enforce cross-modal alignment globally for images and text, creating a cross-modal gap between the global prompt embeddings and local patch token embeddings.WinCLIP [20] attempts to address this issue by employing a sliding window to generate patch embeddings in a manner that simulates processing the global image instead of using patch-wise embeddings from the penultimate feature map.However, the localized patch may not encompass the Calculate the contextual score S  ( ) using Eq.( 4) 6:</p>
<p>if   ( )&gt;0 then 7:</p>
<p>Add  into T 8:</p>
<p>end if 9: end for 10: return Anomaly prompts T description of the global image in the text prompt, leading to suboptimal performance.While AnomalyGPT [18] achieves alignment by generating pseudo-anomaly samples and introducing additional training, which is operationally intricate and lacks efficiency.Consequently, we propose a training-free fine-grained aligner to explicitly model the mapping between global and local semantic spaces.</p>
<p>Given a query image  and its corresponding anomaly prompts T , their embeddings can be denoted as  () âˆˆ R  and () âˆˆ R  , where  âˆˆ T and  denotes the dimension of the latent space.Mathematically, the encoder architecture consists of vision transformers (ViT) [16] based on multi-head self-attention (MHSA) and a feedforward network (FFN) with layer normalization (LN) and residual connections that can be expressed as: áº‘ = MHSA(LN(  âˆ’1 )) +   âˆ’1</p>
<p>(5)
ğ‘§ ğ‘™ = FFN(LN( áº‘ğ‘™ )) + áº‘ğ‘™(6)
where MHSA can be further formulated as:
ğ‘ ğ‘™,ğ‘š = ğ‘§ ğ‘™ âˆ’1 ğ‘Š ğ‘™,ğ‘š ğ‘ , ğ‘˜ ğ‘™ = ğ‘§ ğ‘™ âˆ’1 ğ‘Š ğ‘™,ğ‘š ğ‘˜ , ğ‘£ ğ‘™ = ğ‘§ ğ‘™ âˆ’1 ğ‘Š ğ‘™,ğ‘š ğ‘£(7)
 , = softmax(
ğ‘ ğ‘™,ğ‘š ğ‘˜ ğ‘™,ğ‘šğ‘‡ âˆš ğ‘‘ )ğ‘£ ğ‘š , ğ‘š = 1, â€¢ â€¢ â€¢ , ğ‘€(8)ğ‘§ ğ‘™ = concat(ğ‘§ ğ‘™,1 , â€¢ â€¢ â€¢ , ğ‘§ ğ‘™,ğ‘€ )ğ‘Š ğ‘™ ğ‘œ (9)
where
ğ‘§ 0 = [ğ‘£, ğ‘ 1 , â€¢ â€¢ â€¢ , ğ‘ ğ‘ ],
 represents the [CLS] token and  1 , â€¢ â€¢ â€¢ ,   are the patch tokens of the query image  with a resolution  Ã—  , and  denotes the number of attention heads.</p>
<p>In image processing, the Query-Key retrieval pattern at the final layer can be conceptualized as a type of global average pooling mechanism for capturing global visual descriptions.Concurrently, the Value component serves to furnish comprehensive information regarding each position or region within the image.In the current task, our aim is to delve into the interplay between global and local semantic information.Therefore, by adjusting the configuration of the Value, whose ensemble forms the output of the final attention mechanism, we can achieve a more nuanced handling of global and local information by the model.Consequently, the model gains the capability to discern the intricate correlation between global and local in a more adaptable manner.After aligning the local embeddings of the anomaly prompt and dense image patch, we calculate the class token-based anomaly score   () for the image query  and generate an anomaly map using the aligned local embeddings as follows:
ğ‘† ğº (ğ‘¥) = exp(&lt; ğ‘“ (ğ‘¥), ğ‘“ âˆ’ ğ‘‡ğº &gt;) ğ‘“ ğ‘¡ âˆˆ F ğ‘‡ ğº exp(&lt; ğ‘“ (ğ‘¥), ğ‘“ ğ‘¡ &gt;) (10) ğ‘† ğ¿ (ğ‘¥ ğ‘– ğ‘— ) = exp(&lt; ğ‘“ ğ¼,ğ‘– ğ‘— , ğ‘“ âˆ’ ğ‘‡ ğ¿,ğ‘– ğ‘— &gt;) ğ‘“ ğ‘¡ âˆˆ F ğ‘‡ ğ¿ exp(&lt; ğ‘“ ğ¼,ğ‘– ğ‘— , ğ‘“ ğ‘¡ &gt;)(11)
Likewise, we can implement multi-scale masked images to generate multi-scale visual embeddings paired with corresponding prompt embeddings.Using these, we calculate multi-scale anomaly maps and average them through harmonic averaging [20] for anomaly localization of a given query.Relying on the premise that an image can be classified as anomalous upon the detection of a single anomalous patch, the anomaly score for the image query is determined by combining the classification score in Eq. 10 with the maximum value of averaged multi-scale anomaly map as follow: Our ALFA adeptly accommodates few-shot scenarios by employing a memory bank to store patch-level features from normal samples, illustrated in Figure 2. Anomaly localization is subsequently improved on top of   by calculating distances between query patches and their nearest counterparts in the memory bank.
ğ‘† (ğ‘¥) = 1 2 (ğ‘† ğº (ğ‘¥) + max ğ‘– ğ‘— ğ‘† ğ¿ (ğ‘¥ ğ‘– ğ‘— )) (12)</p>
<p>Experiments</p>
<p>In this section, we systematically evaluate ALFA for image-level and pixel-level anomaly detection through quantitative and qualitative analyses on various benchmarks.Ablation studies and explainable VAD results are also presented.</p>
<p>Experimental Setup</p>
<p>Datasets.Our experiments are based on MVTec [2] and VisA [68] benchmarks, both containing high-resolution images with full pixellevel annotations.MVTec includes data for 10 single objects and 5 textures, while VisA includes data for 12 single or multiple object types.As our framework is entirely training-free, we exclusively utilize the test datasets for evaluation.Metrics.We use Area Under the Receiver Operating Characteristic (AUROC), Area Under the Precision-Recall curve (AUPR), and F1-score at optimal threshold (F1-max) as image-level anomaly detection metrics.Besides, we report pixel-wise AUROC (pAUROC), Per-Region Overlap (PRO) scores, and pixel-wise F1-max (pF1-max) in a similar manner to evaluate anomaly localization.Implementation details.We employ the OpenCLIP implementation [19] and its publicly available pre-trained models.Specifically, we use the LAION-400M [47]-based CLIP with ViT-B/16+ as our foundational model and GPT-3.5 (gpt-3.5-turbo-instruct)for anomaly prompt generation.</p>
<p>Zero-shot anomaly detection</p>
<p>In Table 1, we compare ALFA with prior arts on MVTec and VisA benchmarks for both image-level and pixel-level zero-shot anomaly detection.Specifically, we compare ALFA with CLIP-AC [41] for image-level anomaly detection, Trans-MM [6] for pixel-level anomaly detection, and WinCLIP [20] and AnoVL [14] for both image-level and pixel-level anomaly detection.For fairness, we use AnoVL âˆ’ [14] for comparison, representing AnoVL without fine-tuning and data augmentation, while the comparison with the complete AnoVL is presented in Sec.5.6.For both image-level and pixel-level VAD, ALFA demonstrates significant improvements over all baselines across all metrics on both benchmarks.Notably, compared to the runner-up, we achieve a 12.1% enhancement in PRO for pixel-level anomaly detection on MVTec and a 8.9% improvement on VisA.Similarly, for image-level anomaly detection, we outperform the suboptimal method by 1.5% on MVTec and by 4.0% on VisA in AUROC.A detailed breakdown of these gains is presented in Section 5.3 through ablation studies.</p>
<p>Qualitative results.In Figure 5, qualitative results for different objects with various anomalies are showcased.In all instances, ALFA yields an anomaly map that exhibits greater concentration on the ground truth compared to previous methods, aligning with the findings from the quantitative results.Subtle, ALFA fares better under various sizes and quantities of anomalies, demonstrating its versatility.</p>
<p>Ablation Study</p>
<p>Component-wise analysis.Ablation studies on key ALFA modules, including RTP adaptation and the fine-grained aligner, demonstrate their significant contributions to overall detection performance, detailed in Table 2. Employing a one-class design using only normal prompts as a baseline in the first row, we emphasize the significance of template-based and LLM-based prompt generator in capturing various anomalous patterns.The contextual scoring mechanism, denoted as S  in Table 2, further enhances performance by adaptively managing the anomaly prompts customized for each query image without cross-semantic issue.Furthermore, the finegrained aligner proves to be a crucial contributor, especially in enhancing pixel-level anomaly detection in zero-shot scenarios.</p>
<p>Analysis on anomaly prompt.In Table 3, we demonstrate that ALFA achieves superior detection performance while significantly    reducing human efforts in designing prompts.We present the number of prompts per label employed by each method.In general, LVLMs tend to exhibit improved performance with an increase in the number of prompts.However, when cross-semantic ambiguity limits the effectiveness of prompts, increasing their number may not necessarily lead to performance improvement, as evidenced by the results of AnoVL and WinCLIP as shown in Table 3.In ALFA, the range of prompts per label on each class spans from 88 to 216, with an average of 146.Notably, we only design 72 prompts based on the template, a notable 53.2% reduction as compared to over 150 required by baselines, for better detection results, showcasing ALFA's ability to effectively tackle the cross-semantic issue and unlock the full potential of language for zero-shot VAD.We further assess ALFA using GPT-3 (text-davinci-002), GPT-3.5 (gpt-3.5-turbo-instruct),and GPT-4 (gpt-4-turbo) for automatic prompt generation, observing superior performance with more powerful LLM.Moreover, by augmenting the input query of the GPT-3.5 as "state the description beginning with: An abnormal/normal image of {class}", the resulting prompts are formulated to preserve syntactic consistency to the greatest extent possible, aligning with the text in CLIP pre-training dataset.This augmentation contributes to further improved results.</p>
<p>Interpretability</p>
<p>We present results for explainable anomaly detection in Figure 6, where the bars illustrate the descriptor similarity to the image predicted as an anomaly in the CLIP latent space.Concretely, we condition descriptors on the class name by prompting the language model with the input: "Q: What are useful descriptions for distinguishing an anomaly {class} in a photo?A: There are several key descriptions to tell there is an anomaly {class} in a photo: -" where "-" is used to generate point-by-point characterizations as descriptors.Figure 6 shows the top five descriptors that emerge from GPT-3.5, encompassing colors, shapes, and object parts for both class-specific and class-agnostic descriptions.These descriptions enable ALFA to look at cues easily recognizable by humans, enhancing interpretability for decision-making in VAD tasks.</p>
<p>Few-shot Generalization</p>
<p>We expand the capabilities of ALFA to include the few-shot setting, allowing for enhanced performance across scenarios with limited data.We report the mean and standard deviation over 5 random seeds for each measurement in Table 4 and Table 5.We benchmark ALFA against PatchCore [43] and WinCLIP [20].PatchCore utilizes few-shot images for generating nominal information in its memory bank, and the full-shot version of PatchCore will be discussed in Section 5.6.In this setting, ALFA consistently outperforms all baselines across all metrics, highlighting the efficacy of language prompts and multi-modal alignment for VAD.Moreover, with an increase in the shot number, ALFA exhibits improved performance, emphasizing the synergy between language-driven and reference normal image-based models.</p>
<p>As our anomaly score and anomaly map are dual-composite, we conduct further ablation studies on their distinct components, detailed in Table 6.For image-level VAD, as the number of shots increases, the significance of   in anomaly score gradually becomes evident, as it allows the introduction of information from normal images in the memory bank to serve as supervision for VAD.Meanwhile, max   consistently brings further performance improvement based on   .For pixel-level VAD, we assess the impact of image features generated by masks of various scales, using patches as the unit and a patch size of 16Ã—16 in our foundational model.We also report the average inference time per image across different few-shot settings, evaluated on a server with Xeon(R) Silver 4214R CPU @ 2.40GHz (12 cores), 128G memory, and GeForce RTX 3090.We find that integrating image features at different scales notably improves performance by incorporating local information.However, scaling up the size comes at the cost of increased computational demands, impacting inference speed.Thus, we opt for a scale range of [2,3] to achieve an optimal trade-off between inference speed and performance.</p>
<p>Comparison on varied supervised paradigms</p>
<p>We benchmark ALFA against prominent unsupervised and finetunerequired VAD methods in a unified setting for fairness.Most baselines undergo training or fine-tuning on normal samples encompassing all classes within the dataset.Additionally, we include three full/many-shot training-free methods, PatchCore [43], SAA+ [5] and MuSc [31].As depicted in Table 7, our zero-shot ALFA is competitive with the baselines that require more information whether in the form of additional normal samples or training.In the 4-shot scenario, ALFA surpasses most baselines, underscoring the complementary roles between language and vision in VAD.</p>
<p>Conclusions</p>
<p>In this paper, we present an adaptive LLM-empowered model ALFA that focuses on vision-language synergy for VAD.Capitalizing on the robust zero-shot capabilities of LLMs, the proposed runtime prompt adaptation strategy effectively generates informative prompts by tapping into the vast world knowledge encoded in their billion-scale parameters.This adaptation strategy is complemented by a contextual scoring mechanism, ensuring per-image adaptability while mitigating the cross-semantic ambiguity.Additionally, the introduction of a novel training-free fine-grained aligner further bolsters ALFA, generalizing the alignment projection seamlessly from the global to the local level for precise anomaly localization.</p>
<p>Experimental results demonstrate ALFA's superiority over existing zero-shot VAD approaches, providing valuable interpretability.</p>
<p>Figure 1 :
1
Figure 1: Overview of ALFA, a training-free zero-shot VAD model focusing on vision-language synergy.The first and third prompts are generated by an LLM to describe normal and abnormal images, respectively.The second prompt, however, shows an ambiguous description, posing a challenge in accurately determining the image label, a phenomenon known as cross-semantic ambiguity.</p>
<p>Figure 2 :
2
Figure 2: Workflow of ALFA with the run-time prompt adaptation strategy, which generates informative prompts and adaptively manages a collection of prompts on a per-image basis via a contextual scoring mechanism.Furthermore, a fine-grained aligner is introduced to generalize the alignment projection from global to local for precise anomaly localization.</p>
<p>pretraining approaches to learn a shared embedding space.Given million-scale image-text pairs {(    ,     )|0 â‰¤  &lt;   ,   âˆˆ C}, where   is the number of pairs in category   , LVLMs train an image encoder  (â€¢) and a text encoder (â€¢) by maximizing the correlation between  (    ) and (    ) measured in cosine similarity &lt; (    ), (    )&gt;.This strategy effectively aligns images with text prompts in LVLMs.</p>
<p>Figure 3
3
provides a visual overview Q: Describe what a(n) normal/abnormal image of carpet looks likeï¼Ÿ Text Completion LLM â„ï¸ A normal image of carpet would show a smooth, even surface with a consistent pattern or texture.</p>
<p>Figure 3 :
3
Figure 3: Overview of the run-time prompt adaptation.</p>
<p>Figure 4 :
4
Figure 4: Visualization of ALFA's semantic space.</p>
<p>Algorithm 1
1
Run-time Prompt Adaptation Input: Query image , pre-trained image encoder  (â€¢), pre-trained text encoder  (â€¢) Output: Anomaly prompts T := { T + , T âˆ’ } Initialization: Template-based prompt generator G  , LLM-based prompt generator G  1: Generate T  by the template-based prompt generator G  2: Generate T  by the LLM-based prompt generator G  3: Calculate the the cosine similarity between  ( ) and  ( ) as Eq.(2) and Eq.(3),  âˆˆ T  = { T  , T  } 4: for  in T  do 5:</p>
<p>Figure 5 :
5
Figure 5: Qualitative results of zero-shot VAD.Annotated orange regions indicate detected anomalies, showcasing effective localization of ALFA across diverse anomalies (e.g., broken and bent of varying sizes and quantities) within various classes.Specifically, for a dense visual input    =  âŠ™    , where    âˆˆ {0, 1}  Ã— represents the mask that is locally active for a kernel around (, ) and âŠ™ denotes the element-wise product, we can similarly obtain the visual embedding as  ,  =  ( âŠ™   ) âˆˆ R  .In this procedure, the value matrix   ,  for local feature extraction in layer  can be obtained as described in Eq. 7.While the value matrix for global feature extraction  () in layer  can be similarly represented as    .To this end, we can learn a projection from global to local semantic space by a transformation matrix   ,  as    ,     =   ,  .For the text modality, the global anomaly prompt embedding F  := [ +  ,  âˆ’  ] âˆˆ R 2Ã— can be generated by computing embeddings via the text encoder for respective anomaly labels.Next, we project the global anomaly prompt embedding F  into the local semantic space as F  ,  = [ +  ,  ,  âˆ’  ,  ] âˆˆ R 2Ã— according to each patch token embedding F  ,  = [ ,  ] âˆˆ R  by the transformation matrix   ,  .After aligning the local embeddings of the anomaly prompt and dense image patch, we calculate the class token-based anomaly score   () for the image query  and generate an anomaly map using the aligned local embeddings as follows:</p>
<p>is an anomalyin the imagebecause...</p>
<p>Figure 6 :
6
Figure 6: Interpretable VAD results for capsules in Visa benchmark.The top five descriptors are listed as factors influencing the decision-making.</p>
<p>Run-time Prompt Adaptation Contextual Scoring Mechansim Fine-grained Aligner</p>
<p>using a compositional prompt ensemble
Template A {domain} image of a(n) {state}{class} with {specific details}+Q:Describe what a(n) {normal/abnormal} imageLLMï¸of {class} looks like?LVLM TextEncoderï¸Cross-semanticImage Feature ExtractionQuery ImageMulti-scale Visual FeaturesNormal Images (Few-shot only)LVLM Visual Encoderï¸Memory Bank (Few-shot only)</p>
<p>Data Flow Frozen Module Concatenation Few-shot Only Anomaly Map Adaptive Component Local Embedding Space Global Embedding Space Text Embedding Image-level Visual Embedding Patch-level Visual Embedding Locally Aligned Text Embedding
Multi-scaleCross-modalAggregation</p>
<p>Table 1 :
1
The performance of zero-shot anomaly detection.Bold indicates the best performance.
TaskMethodMVTecVisAAUROC AUPRF1-maxAUROC AUPRF1-maxCLIP-AC [41]74.189.587.858.266.474.0Image-levelWinCLIP [20] AnoVL âˆ’ [14]91.8 91.396.5 96.392.9 92.978.1 76.781.2 79.379.0 78.7ALFA93.297.393.981.284.681.9TaskMethodpAUROCPROpF1-max pAUROCPROpF1-maxTrans-MM [6]57.521.912.149.410.23.1MaskCLIP [60]63.740.518.560.927.37.3Pixel-levelWinCLIP [20]85.164.631.779.656.814.8AnoVL âˆ’ [14]86.670.430.183.758.613.5ALFA90.678.936.685.963.815.9</p>
<p>Table 2 :
2
Component-wise analysis of ALFA on MVTec.
RTP adaptationFine-grainedImage-level, Pixel-levelTemplate LLM Sğ‘Aligner(AUROC, pAUROC) (AUPR, PRO) (F1-max, pF1-max)Ã—Ã—Ã—Ã—(34.2, -)(68.9, -)(83.5, -)Ã—âœ“Ã—Ã—(84.8, 75.9)(90.6, 54.8)(89.2, 24.1)Ã—âœ“Ã—âœ“(86.8, 80.2)(92.6, 59.9)(90.5, 27.0)Ã—âœ“âœ“Ã—(87.0, 81.2)(92.8, 60.2)(90.8, 28.2)Ã—âœ“âœ“âœ“(90.5, 84.7)(96.7, 64.4)(92.3, 32.0)âœ“Ã—Ã—Ã—(86.6, 79.4)(92.5, 59.2)(90.6, 26.8)âœ“Ã—Ã—âœ“(87.2, 82.9)(93.8, 61.6)(91.2, 30.1)âœ“Ã—âœ“Ã—(88.0, 83.1)(94.2, 62.2)(91.5, 30.2)âœ“Ã—âœ“âœ“(90.9, 85.2)(96.6, 65.2)(92.4, 32.8)âœ“âœ“Ã—Ã—(89.9, 83.6)(95.2, 62.9)(92.0, 30.7)âœ“âœ“âœ“Ã—(92.0, 85.9)(96.5, 68.8)(93.0, 32.2)âœ“âœ“Ã—âœ“(91.6, 87.2)(96.3, 69.9)(92.8, 33.2)âœ“âœ“âœ“âœ“(93.2, 90.6)(97.3, 78.9)(93.9, 36.6)</p>
<p>Table 3 :
3
Ablation analysis of anomaly prompt on MVTec.</p>
<h1>PromptsMethodsAUROC AUPR F1-max154 (all manual)WinCLIP [20]91.896.592.9462 (all manual)AnoVL [14]91.396.392.9ALFA with GPT-392.296.793.2146 (only 72 manual)ALFA with GPT-3.5 + syntactic consistency92.9 93.297.2 97.393.6 93.9ALFA with GPT-493.797.597.7</h1>
<p>Table 4 :
4
Image-level performance on few-shot VAD.
SetupMethodMVTecVisAAUROCAUPRF1-maxAUROCAUPRF1-maxPatchCore [43]83.4Â±3.092.2Â±1.590.5Â±1.579.9Â±2.982.8Â±2.381.7Â±1.61-shotWinCLIP [20]93.1Â±2.096.5Â±0.993.7Â±1.183.8Â±4.085.1Â±4.083.1Â±1.7ALFA94.5Â±1.597.9Â±1.4 94.9Â±0.9 85.2Â±2.0 87.3Â±2.1 84.9Â±1.6PatchCore [43]86.3Â±3.393.8Â±1.792.0Â±1.581.6Â±4.084.8Â±3.282.5Â±1.82-shotWinCLIP [20]94.4Â±1.397.0Â±0.794.4Â±0.884.6Â±2.485.8Â±2.783.0Â±1.4ALFA95.9Â±0.998.4Â±0.6 95.6Â±0.6 86.4Â±1.2 87.5Â±1.8 85.2Â±1.4PatchCore [43]88.8Â±2.694.5Â±1.592.6Â±1.685.3Â±2.187.5Â±2.184.3Â±1.34-shotWinCLIP [20]95.2Â±1.397.3Â±0.694.7Â±0.887.3Â±1.888.8Â±1.884.2Â±1.6ALFA96.5Â±0.6 98.9Â±0.6 96.0Â±0.7 88.2Â±0.9 89.4Â±1.4 85.5Â±1.2</p>
<p>Table 5 :
5
Pixel-level performance on few-shot VAD.
SetupMethodMVTecVisApAUROCPROpF1-max pAUROCPROpF1-maxPatchCore [43] 92.0Â±1.079.7Â±2.050.4Â±2.195.4Â±0.680.5Â±2.538.0Â±1.91-shotWinCLIP [20]95.2Â±0.587.1Â±1.255.9Â±2.796.4Â±0.485.1Â±2.141.3Â±2.3ALFA96.8Â±0.5 89.6Â±1.2 57.7Â±1.6 97.2Â±0.8 86.4Â±1.2 42.9Â±1.9PatchCore [43] 93.3Â±0.682.3Â±1.353.0Â±1.796.1Â±0.582.6Â±2.341.0Â±3.92-shotWinCLIP [20]96.0Â±0.388.4Â±0.958.4Â±1.796.8Â±0.386.2Â±1.443.5Â±3.3ALFA97.2Â±0.4 91.0Â±0.9 59.9Â±1.6 97.7Â±0.8 87.2Â±1.2 45.6Â±2.0PatchCore [43] 94.3Â±0.584.3Â±1.655.0Â±1.996.8Â±0.384.9Â±1.443.9Â±3.14-shotWinCLIP [20]96.2Â±0.389.0Â±0.859.5Â±1.897.2Â±0.287.6Â±0.947.0Â±3.0ALFA97.6Â±0.3 91.6Â±0.6 60.3Â±1.0 98.1Â±0.4 89.2Â±1.2 47.9Â±2.6</p>
<p>Table 6 :
6
Component-wise analysis of anomaly score and anomaly map on MVTec.
Anomaly Score#shot (AUROC)ğ‘† ğºmax ğ‘† ğ¿0124âœ“Ã—91.2 91.2 91.2 91.2Ã—âœ“86.2 90.6 92.0 94.8âœ“âœ“93.2 94.5 95.9 96.5Multi-scaleAverage#shots (pAUROC)MaskInference Time (s)0124[2]0.64Â±0.0388.9 93.6 95.1 95.8[2, 3]1.16Â±0.0690.6 96.8 97.2 97.6[2, 3, 4]1.92Â±0.1490.9 97.2 97.8 97.9</p>
<p>Table 7 :
7
Comparison of supervised paradigms on MVTec.
MethodsSetupAUROC pAUROC#shotsTraining modePaDiM [12]full-shotUnsupervised84.289.5JNLD [59]full-shotUnsupervised91.388.6UniAD [53]full-shotUnsupervised96.596.8AnoVL [14]0-shotFinetuned91.389.8AnomalyGPT [18]0-shotFinetuned97.493.1AnomalyCLIP [62]0-shotFinetuned91.591.1PatchCore [52]full-shotTraining-free99.698.2SAA+ [5]full-shotTraining-free-81.7MuSc [31]many-shot (42-176)Training-free97.897.3ALFA0-shotTraining-free93.290.6ALFA4-shotTraining-free96.597.6
ACKNOWLEDGMENTSThe work of NUS researchers is partially supported by the Lee Foundation in terms of Beng Chin Ooi's Lee Kong Chian Centennial Professorship fund and NUS Faculty Development Fund.The work of BIT researchers is partially supported by the National Natural Science Foundation of China (NSFC) National Science Fund for Distinguished Young Scholars 62025301.
Zero-shot versus many-shot: Unsupervised texture anomaly detection. Toshimichi Aota, Lloyd Teh Tzer, Takayuki Tong, Okatani, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>MVTec AD-A comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>ARM-Net: Adaptive Relation Modeling Network for Structured Data. Shaofeng Cai, Kaiping Zheng, Gang Chen, H V Jagadish, Beng , Chin Ooi, Meihui Zhang, SIGMOD '21: International Conference on Management of Data, Virtual Event. Guoliang Li, Zhanhuai Li, Stratos Idreos, Divesh Srivastava, ChinaACM2021. June 20-25, 2021</p>
<p>Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Zongwei Du, Liang Gao, Weiming Shen, arXiv:2305.10724Segment Any Anomaly without Training via Hybrid Prompt Regularization. 2023. 2023arXiv preprint</p>
<p>Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. Hila Chefer, Shir Gur, Lior Wolf, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Easynet: An easy network for 3d industrial anomaly detection. Ruitao Chen, Guoyang Xie, Jiaqi Liu, Jinbao Wang, Ziqi Luo, Jinfan Wang, Feng Zheng, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLR2020</p>
<p>Xuhai Chen, Yue Han, Jiangning Zhang, arXiv:2305.17382A Zero-/Few-Shot Anomaly Classification and Segmentation Method for CVPR 2023 VAND Workshop Challenge Tracks 1&amp;2: 1st Place on Zero-shot AD and 4th Place on Few-shot AD. 2023. 2023arXiv preprint</p>
<p>See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, Chuang Gan, arXiv:2301.052262023. 2023arXiv preprint</p>
<p>Training Auxiliary Prototypical Classifiers for Explainable Anomaly Detection in Medical Image Segmentation. Wonwoo Cho, Jeonghoon Park, Jaegul Choo, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Padim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. Springer2021</p>
<p>Anomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization. Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, Xingyu Li, arXiv:2308.159392023. 2023arXiv preprint</p>
<p>Any-shot sequential anomaly detection in surveillance videos. Keval Doshi, Yasin Yilmaz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops2020</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020. 2020arXiv preprint</p>
<p>Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel</p>
<p>AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, arXiv:2308.153662023. 2023arXiv preprint</p>
<p>. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt, 10.5281/zenodo.51437732021If you use this software, please cite it as below.</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023Avinash Ravichandran, and Onkar Dabeer</p>
<p>Robust and Transferable Log-based Anomaly Detection. Peng Jia, Shaofeng Cai, Beng , Chin Ooi, Pinghui Wang, Yiyuan Xiong, 10.1145/3588918Proc. ACM Manag. Data. 1262023. 2023</p>
<p>Why the logistic function? A tutorial discussion on probabilities and neural networks. Jordan Michael, 1995</p>
<p>Multi-Modal Classifiers for Open-Vocabulary Object Detection. Prannay Kaul, Weidi Xie, Andrew Zisserman, arXiv:2306.054932023. 2023arXiv preprint</p>
<p>Maple: Multi-modal prompt learning. Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 352022. 2022</p>
<p>Deep Learning Model for Automated Detection and Classification of Degenerative Cord Signal Abnormality, Spinal Canal and Neural Foraminal Stenosis on Cervical Spine Magnetic Resonance Imaging. Lee, Wu, Liu, Lee, Tan, Huang, Kumar, Ooi, Hallinan, Seminars in Musculoskeletal Radiology. Thieme Medical Publishers, Inc202428A132</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021. 2021arXiv preprint</p>
<p>Cutpaste: Self-supervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023. 2023arXiv preprint</p>
<p>Xurui Li, Ziming Huang, Feng Xue, Yu Zhou, arXiv:2401.16753MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images. 2024. 2024arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021. 2021arXiv preprint</p>
<p>Improving Data Analytics with Fast and Adaptive Regularization. Zhaojing Luo, Shaofeng Cai, Gang Chen, Jinyang Gao, Wang-Chien Lee, Kee Yuan Ngiam, Meihui Zhang, 10.1109/TKDE.2019.2916683IEEE Trans. Knowl. Data Eng. 332021. 2021</p>
<p>Adaptive Knowledge Driven Regularization for Deep Neural Networks. Zhaojing Luo, Shaofeng Cai, Can Cui, Beng , Chin Ooi, Yang Yang, 10.1609/AAAI.V35I10.17067Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. AAAI Press2021. February 2-9, 2021</p>
<p>Regularized Pairwise Relationship based Analytics for Structured Data. Zhaojing Luo, Shaofeng Cai, Yatong Wang, Beng Chin Ooi, 10.1145/3588936Proc. ACM Manag. Data. 1272023. 2023</p>
<p>MOCCA: Multilayer oneclass classification for anomaly detection. Fabio Valerio Massoli, Fabrizio Falchi, Alperen Kantarci, Åeymanur Akti, Hazim Kemal Ekenel, Giuseppe Amato, IEEE Transactions on Neural Networks and Learning Systems. 332021. 2021</p>
<p>Deep generative model using unregularized score for anomaly detection with heterogeneous complexity. Takashi Matsubara, Kazuki Sato, Kenta Hama, Ryosuke Tachibana, Kuniaki Uehara, IEEE Transactions on Cybernetics. 522020. 2020</p>
<p>Sachit Menon, Carl Vondrick, arXiv:2210.07183Visual classification via description from large language models. 2022. 2022arXiv preprint</p>
<p>SINGA: A Distributed Deep Learning Platform. Chin Beng, Kian-Lee Ooi, Sheng Tan, Wei Wang, Qingchao Wang, Gang Cai, Jinyang Chen, Zhaojing Gao, Luo, K H Anthony, Yuan Tung, Zhongle Wang, Meihui Xie, Kaiping Zhang, Zheng, Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM. ACM. the 23rd Annual ACM Conference on Multimedia Conference, MM. ACM2015</p>
<p>Learning memoryguided normality for anomaly detection. Hyunjong Park, Jongyoun Noh, Bumsub Ham, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Selfsupervised predictive convolutional attentive block for anomaly detection. Nicolae-CÄƒtÄƒlin Ristea, Neelu Madan, Tudor Radu, Kamal Ionescu, Fahad Nasrollahi, Thomas B Shahbaz Khan, Mubarak Moeslund, Shah, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard SchÃ¶lkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Deep one-class classification. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius MÃ¼ller, Kloft, International conference on machine learning. PMLR2018</p>
<p>Learning to share visual appearance for multiclass object detection. Ruslan Salakhutdinov, Antonio Torralba, Josh Tenenbaum, CVPR 2011. IEEE2011</p>
<p>Multiresolution knowledge distillation for anomaly detection. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 2021. 2021arXiv preprint</p>
<p>Synthetic prompting: Generating chain-of-thought demonstrations for large language models. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2302.006182023. 2023arXiv preprint</p>
<p>Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, Deng Cai, arXiv:2305.16355Pandagpt: One model to instruction-follow them all. 2023. 2023arXiv preprint</p>
<p>Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection. Masato Tamura, arXiv:2308.111192023. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Patch svdd: Patch-level svdd for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian conference on computer vision. the Asian conference on computer vision2020</p>
<p>A unified model for multi-class anomaly detection. Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>IFSeg: Image-free Semantic Segmentation via Vision-Language Model. Sukmin Yun, Seong Hyeon Park, Paul Hongsuck Seo, Jinwoo Shin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel SkoÄaj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, Hongsheng Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection. Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, Ting Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Anomaly detection for medical images using self-supervised and translation-consistent features. He Zhao, Yuexiang Li, Nanjun He, Kai Ma, Leyuan Fang, Huiqi Li, Yefeng Zheng, IEEE Transactions on Medical Imaging. 402021. 2021</p>
<p>Just noticeable learning for unsupervised anomaly localization and detection. Ying Zhao, 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE2022</p>
<p>Extract free dense labels from clip. Chong Zhou, Chen Change Loy, Bo Dai, European Conference on Computer Vision. Springer2022</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen, arXiv:2310.189612023. 2023arXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023. 2023arXiv preprint</p>
<p>METER: A Dynamic Concept Adaptation Framework for Online Anomaly Detection. Jiaqi Zhu, Shaofeng Cai, Fang Deng, Beng , Chin Ooi, Wenqiao Zhang, Proc. VLDB Endow. VLDB Endow2023. 202317</p>
<p>Adaptive aggregationdistillation autoencoder for unsupervised anomaly detection. Jiaqi Zhu, Fang Deng, Jiachen Zhao, Jie Chen, Pattern Recognition. 1311088972022. 2022</p>
<p>Uaed: Unsupervised abnormal emotion detection network based on wearable mobile device. Jiaqi Zhu, Fang Deng, Jiachen Zhao, Daoming Liu, Jie Chen, IEEE Transactions on Network Science and Engineering. 102023. 2023</p>
<p>Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts. Jiawen Zhu, Guansong Pang, arXiv:2403.064952024. 2024arXiv preprint</p>
<p>Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. Springer2022</p>            </div>
        </div>

    </div>
</body>
</html>
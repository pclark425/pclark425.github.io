<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-272911321</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.17270v2.pdf" target="_blank">Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces Proof of Thought, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate Proof of Thought's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROOF OF THOUGHT (PoT): Neurosymbolic Program Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic framework that has an LLM generate a JSON-based DSL which an interpreter translates to first-order logic (FOL) and verifies with an external theorem prover (Z3); includes a type-safe sort system, explicit rules/KB separation, and iterative feedback loops to reduce generation/compilation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework leverages a pre-trained language model p_θ (unspecified) to generate program-like DSL outputs (LLM-Thoughts) which are parsed by a custom interpreter into FOL for theorem-prover verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neurosymbolic (transformer-generated JSON-DSL → interpreter → FOL → SMT/theorem prover)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program-synthesis from LLM to a JSON DSL, translation to first-order logic, formal verification via theorem prover, with a multi-attempt feedback loop for error correction.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Microsoft Z3 SMT/theorem prover is used to verify FOL formulas produced by the interpreter; interpreter emits Z3-compatible sorts, predicates and queries and uses Z3 results (SAT/UNSAT, counterexamples) as verification and diagnostic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>StrategyQA; Reddit-OSHA (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>StrategyQA: boolean multi‑hop implicit NL reasoning requiring unstated chains of inference. Reddit‑OSHA: curated multimodal (image+text) long‑tail safety/hazard cases from r/OSHA requiring applied logical verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-hop boolean QA (StrategyQA); multimodal hazard verification (Reddit-OSHA) — first‑order logic verification / satisfiability checking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>compilation success rate; precision; recall; F1; false positive rate; win rate (correct hazard identification) on compiled programs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>StrategyQA (sample of 1000): compilation/execute success 82.4%, recall 91.40%, precision 58.22%, F1 71.13%, false positive rate 53.98% (17.6% failed to compile). Reddit‑OSHA: compilation errors reduced from 14.6% → 0% with 3-step feedback; win rate on compiled programs increased from 72% → 81.55%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improves robustness of program generation (compilation errors → 0% on Reddit‑OSHA) and increases verified-win rate on compiled programs (72%→81.55%); direct accuracy comparisons to unconstrained LLM baselines (e.g., GPT‑4 CoT baselines on same dataset) are not strictly comparable because PoT evaluates verification of generated programs and uses different success criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Converting LLM reasoning into an explicit, typed JSON-DSL and verifying via a theorem prover yields verifiable and interpretable reasoning traces and improves compilation/verification robustness with a small feedback loop; PoT attains high recall on StrategyQA and reduces compilation errors on a hard multimodal dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on correctness and coverage of the provided knowledge base and rules; nontrivial fraction of generated programs fail to compile (17.6% in StrategyQA sample) before feedback; high false positive rate (53.98%) on StrategyQA indicates overprediction of positives; scalability to very large datasets and non-boolean outputs not yet demonstrated; computational and engineering overhead from program synthesis + theorem proving; LLM identity/size/training data for PoT runs not specified (limits reproducibility).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JSON-DSL + Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JSON-based Domain-Specific Language and Interpreter (PoT intermediate representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-readable, JSON schema representing sorts, functions, constants, KB, rules, and verifications; interpreter enforces a robust type/sort system, constructs FOL ASTs, normalizes and simplifies expressions, and emits Z3-compatible constructs for proving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a neural model; an intermediate structured representation used in PoT to capture LLM outputs in a typed, machine-parseable form before formalization to FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>structured JSON DSL + interpreter that outputs first-order logic / Z3 constructs</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Explicit programmatic representation of reasoning (sorts, KB, rules, verifications) enabling formal proof verification by external theorem prover; includes preprocessing, normalization, and diagnostic feedback to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Interpreter targets Z3 (SMT solver) and enforces Z3-compatible sorts (Bool, Int, Real, DeclareSort, EnumSort), manages symbol table and quantifier scoping, and returns diagnostic errors (type errors, undefined symbols) to support feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>StrategyQA; Reddit-OSHA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same as PoT benchmarks: StrategyQA (boolean multi-hop QA); Reddit-OSHA (multimodal hazard verification).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formalization/program synthesis for FOL verification; SAT/UNSAT checks; existence/universal verifications; optional optimization constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>compilation success rate; reduction in compilation errors; quality of FOL produced (provable chains)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Interpreter+DSL achieved 82.4% compile/execute success on StrategyQA (with feedback loop) and reduced compilation errors on Reddit-OSHA from 14.6% to 0% using a 3-step feedback loop.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Adding the DSL+interpreter improves syntactic/semantic correctness of LLM outputs and enables theorem-prover checks; compared to raw LLM outputs without structured schema, PoT yields verifiable chains and fewer unchecked errors (but direct numeric baselines not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A typed JSON DSL makes LLM outputs amenable to formalization and verification, enabling human-readable and machine-checkable reasoning traces and early error detection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Design currently focuses on boolean/FOL-style verifications; non-boolean or complex natural-language outputs require extensions; DSL expressivity vs. ease-of-generation trade-offs; depends on LLM generating schema-conformant JSON (hence feedback loops/few-shot prompting required).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6819.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Z3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Z3 Theorem Prover (SMT Solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf SMT/theorem prover used to check satisfiability, validity, and to produce proofs/counterexamples for the FOL formulas generated by the PoT interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Satisfiability Modulo Theories (SMT) solver capable of reasoning about Booleans, integers, reals, uninterpreted sorts and other theories; integrated as external verifier in PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>SMT/theorem prover</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Automated theorem proving / SMT solving for first-order logic formulas (as emitted by the interpreter).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 is the external reasoning backend that receives normalized FOL formulas from the interpreter and returns SAT/UNSAT, models (counterexamples), and proof artifacts used for verifiable conclusions and diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>StrategyQA; Reddit-OSHA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as the verification engine for FOL verifications in both benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic satisfiability and verification (SMT solving)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Not compared to alternative provers in this paper; interpreter is Z3-compatible and designed for modular swapping in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a theorem prover provides logical guarantees: conclusions are correct conditional on the KB and rules; Z3 integration enables counterexample generation for failed verifications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Guarantees only hold if KB/rules accurately model the world; theorem proving scalability and handling of very large KBs or higher-order constructs are potential limitations not explored in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6819.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural language reasoning traces from LLMs to improve performance on complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as the underlying LLM for CoT baselines in multimodal Reddit‑OSHA experiments; a large transformer-based language model (paper does not specify size or training corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (prompted with chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-Thought prompting (step-by-step natural language reasoning), ending with a binary decision.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reddit-OSHA (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Image+text hazard identification and safety verification cases from r/OSHA (long-tail scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multimodal hazard identification / binary decision (with natural language reasoning chain)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>win rate (proportion of correctly identified hazards), reasoning richness (number of sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT baseline: 99.03% win rate; produced concise responses (25–30 sentences on average) on Reddit-OSHA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT used as the simple baseline; more advanced strategies (CoT-SC, ToT, GoT) matched or slightly improved correctness on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT yields very high correctness on the curated Reddit-OSHA set and produces concise reasoning traces, but may be less robust/transparent than explicit formal verification approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Chain-of-Thought produces intermediate natural language steps that are not formally verified; may contain inconsistent or non-grounded substeps and relies on model heuristics rather than formal proof.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6819.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style decoding method that samples multiple independent chain-of-thoughts and aggregates via majority voting to select the final answer, improving robustness to individual chain errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to GPT-4 in baselines; uses multiple sampled reasoning paths per input and majority voting for final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer with self-consistency ensemble over chain-of-thought samples</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate multiple independent CoT reasoning paths (paper used 5 per image) and choose final answer by majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reddit-OSHA (multimodal); cited StrategyQA result for PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Reddit-OSHA hazard identification (multimodal); StrategyQA (boolean multi-hop QA) is cited for PaLM-2 CoT+SC results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multimodal hazard detection; multi-hop boolean QA (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>win rate; accuracy (cited for StrategyQA example)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reddit-OSHA: CoT-SC achieved 100% win rate on curated set (5 independent reasoning paths per image). Cited PaLM-2 result on StrategyQA with few-shot CoT+Self-Consistency: 90.20% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT-SC improved robustness over single-path CoT in cited prior work (e.g., PaLM-2 on StrategyQA); on Reddit-OSHA it achieved perfect correctness on the small curated set.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency can significantly improve CoT robustness by aggregating multiple reasoning trajectories; in this paper CoT-SC yields perfect win rate on the curated Reddit-OSHA samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computationally expensive due to multiple sampled reasoning paths; majority-vote aggregation can mask systematic biases; still lacks formal verification of intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6819.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative search method that explores multiple partial reasoning states in a tree structure to find better solutions than linear CoT, trading time for deeper exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a reasoning strategy with GPT-4, exploring a tree with max depth 3 and breadth 2 per node in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer with tree‑search planning over intermediate thought states</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tree-of-Thoughts search: explore multiple reasoning branches (tree) and select promising branches to continue, enabling backtracking and recombination of partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reddit-OSHA (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multimodal hazard identification dataset; ToT used to produce detailed reasoning across branches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multimodal hazard detection / deliberative problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>win rate; reasoning richness (length/verbosity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ToT achieved 100% win rate on Reddit-OSHA and produced the most verbose reasoning (often >1000 sentences per image).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ToT matched or exceeded simpler strategies in correctness on the curated dataset, producing far richer/longer reasoning traces at greater computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deliberative tree search with LLMs can correct errors of simpler approaches and produce richer reasoning, but at substantial verbosity and likely higher compute.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extremely verbose outputs (>>1000 sentences) and higher computation; still produces unverified natural language reasoning and does not provide formal guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6819.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-of-Thoughts (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative reasoning approach that forms a directed graph of thought states allowing richer connectivity (multiple predecessors/successors) and more flexible combination of ideas than tree structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph of thoughts: Solving elaborate problems with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a baseline reasoning strategy performing iterative graph-based thought construction with 3 iterations per image in the Reddit-OSHA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer with graph-structured reasoning over thought states</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Graph-of-Thoughts: iterative construction of a reasoning graph enabling combination and revisiting of subthoughts across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reddit-OSHA (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multimodal hazard verification set; GoT used to iteratively refine analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multimodal hazard detection / iterative graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>win rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GoT achieved 100% win rate on the curated Reddit-OSHA samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Matches ToT and CoT-SC in correctness on the curated dataset while offering a different structural approach to compositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-structured reasoning can reach perfect correctness on small curated multimodal tasks and allow richer compositionality than linear CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not formally verified; complexity and implementation overhead of graph search not quantified; evaluation on small curated set may not generalize to wider, noisier data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6819.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used as underlying LLM for baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multimodal/LLM used as the base model for all baseline reasoning strategies (CoT, CoT-SC, ToT, GoT) in the Reddit-OSHA experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based language (and vision-capable in the multimodal usage) model provided by OpenAI; the paper uses GPT-4 to run CoT/CoT-SC/ToT/GoT baselines on image+text inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (multimodal in vision-language setup)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting-based reasoning using CoT, CoT-SC, ToT, GoT strategies as experimental baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Reddit-OSHA (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Curated hazard images and descriptions from r/OSHA requiring visual grounding and safety inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multimodal hazard detection and binary safety decision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>win rate; reasoning richness (sentence count)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT 99.03% win rate (concise responses); CoT-SC/ToT/GoT each achieved 100% win rate on the curated Reddit‑OSHA samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>These GPT-4 baselines indicate that advanced prompting/decoding strategies can achieve near-perfect performance on the small curated dataset; PoT focuses on verifiable outputs and shows improvements in compilation robustness but different success criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 paired with advanced reasoning strategies attains extremely high correctness on the curated multimodal hazard set, with trade-offs between concision and reasoning verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High performance on this small curated set may not generalize; reasoning traces are not formally verified and can be internally inconsistent despite correct final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6819.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6819.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2 (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 (cited SOTA example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM-2 is cited as a high-performing LLM that achieved 90.20% accuracy on StrategyQA when using few-shot Chain-of-Thought prompting with Self-Consistency (cited from Anil et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaLM 2 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited large pre-trained transformer language model (Google) that, per the citation, when combined with few-shot CoT and self-consistency achieved high StrategyQA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Few-shot Chain-of-Thought prompting combined with Self-Consistency sampling/aggregation (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>StrategyQA (cited result)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Boolean multi-hop implicit reasoning benchmark requiring hidden reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-hop boolean question answering (implicit reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>90.20% accuracy (PaLM-2 with few-shot CoT + Self-Consistency, cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Cited as a strong existing result on StrategyQA; PoT reports different metrics (verification/compilation/precision/recall) and does not provide a direct apples-to-apples accuracy comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-performing LLMs can reach strong accuracy on StrategyQA with CoT + Self-Consistency, but such approaches lack formal verification of intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited result is from prior work; PaLM-2 CoT outputs are not formally verified in that prior setup and can be opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models. <em>(Rating: 2)</em></li>
                <li>PaLM 2 technical report. <em>(Rating: 2)</em></li>
                <li>On the dangers of stochastic parrots: Can language models be too big?. <em>(Rating: 1)</em></li>
                <li>DeepProbLog: Neural probabilistic logic programming. <em>(Rating: 1)</em></li>
                <li>Scallop: A language for neurosymbolic programming. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6819",
    "paper_id": "paper-272911321",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "PoT",
            "name_full": "PROOF OF THOUGHT (PoT): Neurosymbolic Program Synthesis",
            "brief_description": "A neurosymbolic framework that has an LLM generate a JSON-based DSL which an interpreter translates to first-order logic (FOL) and verifies with an external theorem prover (Z3); includes a type-safe sort system, explicit rules/KB separation, and iterative feedback loops to reduce generation/compilation errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Framework leverages a pre-trained language model p_θ (unspecified) to generate program-like DSL outputs (LLM-Thoughts) which are parsed by a custom interpreter into FOL for theorem-prover verification.",
            "model_size": null,
            "architecture_type": "neurosymbolic (transformer-generated JSON-DSL → interpreter → FOL → SMT/theorem prover)",
            "training_data": null,
            "reasoning_method": "Program-synthesis from LLM to a JSON DSL, translation to first-order logic, formal verification via theorem prover, with a multi-attempt feedback loop for error correction.",
            "external_tool_used": true,
            "external_tool_description": "Microsoft Z3 SMT/theorem prover is used to verify FOL formulas produced by the interpreter; interpreter emits Z3-compatible sorts, predicates and queries and uses Z3 results (SAT/UNSAT, counterexamples) as verification and diagnostic signals.",
            "benchmark_name": "StrategyQA; Reddit-OSHA (multimodal)",
            "benchmark_description": "StrategyQA: boolean multi‑hop implicit NL reasoning requiring unstated chains of inference. Reddit‑OSHA: curated multimodal (image+text) long‑tail safety/hazard cases from r/OSHA requiring applied logical verification.",
            "task_type": "Multi-hop boolean QA (StrategyQA); multimodal hazard verification (Reddit-OSHA) — first‑order logic verification / satisfiability checking",
            "performance_metric": "compilation success rate; precision; recall; F1; false positive rate; win rate (correct hazard identification) on compiled programs",
            "performance_value": "StrategyQA (sample of 1000): compilation/execute success 82.4%, recall 91.40%, precision 58.22%, F1 71.13%, false positive rate 53.98% (17.6% failed to compile). Reddit‑OSHA: compilation errors reduced from 14.6% → 0% with 3-step feedback; win rate on compiled programs increased from 72% → 81.55%.",
            "comparison_with_baseline": "Improves robustness of program generation (compilation errors → 0% on Reddit‑OSHA) and increases verified-win rate on compiled programs (72%→81.55%); direct accuracy comparisons to unconstrained LLM baselines (e.g., GPT‑4 CoT baselines on same dataset) are not strictly comparable because PoT evaluates verification of generated programs and uses different success criteria.",
            "key_findings": "Converting LLM reasoning into an explicit, typed JSON-DSL and verifying via a theorem prover yields verifiable and interpretable reasoning traces and improves compilation/verification robustness with a small feedback loop; PoT attains high recall on StrategyQA and reduces compilation errors on a hard multimodal dataset.",
            "limitations": "Relies on correctness and coverage of the provided knowledge base and rules; nontrivial fraction of generated programs fail to compile (17.6% in StrategyQA sample) before feedback; high false positive rate (53.98%) on StrategyQA indicates overprediction of positives; scalability to very large datasets and non-boolean outputs not yet demonstrated; computational and engineering overhead from program synthesis + theorem proving; LLM identity/size/training data for PoT runs not specified (limits reproducibility).",
            "uuid": "e6819.0",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "JSON-DSL + Interpreter",
            "name_full": "JSON-based Domain-Specific Language and Interpreter (PoT intermediate representation)",
            "brief_description": "A human-readable, JSON schema representing sorts, functions, constants, KB, rules, and verifications; interpreter enforces a robust type/sort system, constructs FOL ASTs, normalizes and simplifies expressions, and emits Z3-compatible constructs for proving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Not a neural model; an intermediate structured representation used in PoT to capture LLM outputs in a typed, machine-parseable form before formalization to FOL.",
            "model_size": null,
            "architecture_type": "structured JSON DSL + interpreter that outputs first-order logic / Z3 constructs",
            "training_data": null,
            "reasoning_method": "Explicit programmatic representation of reasoning (sorts, KB, rules, verifications) enabling formal proof verification by external theorem prover; includes preprocessing, normalization, and diagnostic feedback to the LLM.",
            "external_tool_used": true,
            "external_tool_description": "Interpreter targets Z3 (SMT solver) and enforces Z3-compatible sorts (Bool, Int, Real, DeclareSort, EnumSort), manages symbol table and quantifier scoping, and returns diagnostic errors (type errors, undefined symbols) to support feedback loops.",
            "benchmark_name": "StrategyQA; Reddit-OSHA",
            "benchmark_description": "Same as PoT benchmarks: StrategyQA (boolean multi-hop QA); Reddit-OSHA (multimodal hazard verification).",
            "task_type": "Formalization/program synthesis for FOL verification; SAT/UNSAT checks; existence/universal verifications; optional optimization constraints.",
            "performance_metric": "compilation success rate; reduction in compilation errors; quality of FOL produced (provable chains)",
            "performance_value": "Interpreter+DSL achieved 82.4% compile/execute success on StrategyQA (with feedback loop) and reduced compilation errors on Reddit-OSHA from 14.6% to 0% using a 3-step feedback loop.",
            "comparison_with_baseline": "Adding the DSL+interpreter improves syntactic/semantic correctness of LLM outputs and enables theorem-prover checks; compared to raw LLM outputs without structured schema, PoT yields verifiable chains and fewer unchecked errors (but direct numeric baselines not provided).",
            "key_findings": "A typed JSON DSL makes LLM outputs amenable to formalization and verification, enabling human-readable and machine-checkable reasoning traces and early error detection.",
            "limitations": "Design currently focuses on boolean/FOL-style verifications; non-boolean or complex natural-language outputs require extensions; DSL expressivity vs. ease-of-generation trade-offs; depends on LLM generating schema-conformant JSON (hence feedback loops/few-shot prompting required).",
            "uuid": "e6819.1",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Z3",
            "name_full": "Z3 Theorem Prover (SMT Solver)",
            "brief_description": "An off-the-shelf SMT/theorem prover used to check satisfiability, validity, and to produce proofs/counterexamples for the FOL formulas generated by the PoT interpreter.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Satisfiability Modulo Theories (SMT) solver capable of reasoning about Booleans, integers, reals, uninterpreted sorts and other theories; integrated as external verifier in PoT.",
            "model_size": null,
            "architecture_type": "SMT/theorem prover",
            "training_data": null,
            "reasoning_method": "Automated theorem proving / SMT solving for first-order logic formulas (as emitted by the interpreter).",
            "external_tool_used": true,
            "external_tool_description": "Z3 is the external reasoning backend that receives normalized FOL formulas from the interpreter and returns SAT/UNSAT, models (counterexamples), and proof artifacts used for verifiable conclusions and diagnostics.",
            "benchmark_name": "StrategyQA; Reddit-OSHA",
            "benchmark_description": "Used as the verification engine for FOL verifications in both benchmarks.",
            "task_type": "First-order logic satisfiability and verification (SMT solving)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Not compared to alternative provers in this paper; interpreter is Z3-compatible and designed for modular swapping in future work.",
            "key_findings": "Using a theorem prover provides logical guarantees: conclusions are correct conditional on the KB and rules; Z3 integration enables counterexample generation for failed verifications.",
            "limitations": "Guarantees only hold if KB/rules accurately model the world; theorem proving scalability and handling of very large KBs or higher-order constructs are potential limitations not explored in depth.",
            "uuid": "e6819.2",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step natural language reasoning traces from LLMs to improve performance on complex problems.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used as the underlying LLM for CoT baselines in multimodal Reddit‑OSHA experiments; a large transformer-based language model (paper does not specify size or training corpora).",
            "model_size": null,
            "architecture_type": "transformer (prompted with chain-of-thought)",
            "training_data": null,
            "reasoning_method": "Chain-of-Thought prompting (step-by-step natural language reasoning), ending with a binary decision.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reddit-OSHA (multimodal)",
            "benchmark_description": "Image+text hazard identification and safety verification cases from r/OSHA (long-tail scenarios).",
            "task_type": "Multimodal hazard identification / binary decision (with natural language reasoning chain)",
            "performance_metric": "win rate (proportion of correctly identified hazards), reasoning richness (number of sentences)",
            "performance_value": "CoT baseline: 99.03% win rate; produced concise responses (25–30 sentences on average) on Reddit-OSHA.",
            "comparison_with_baseline": "CoT used as the simple baseline; more advanced strategies (CoT-SC, ToT, GoT) matched or slightly improved correctness on this dataset.",
            "key_findings": "CoT yields very high correctness on the curated Reddit-OSHA set and produces concise reasoning traces, but may be less robust/transparent than explicit formal verification approaches.",
            "limitations": "Chain-of-Thought produces intermediate natural language steps that are not formally verified; may contain inconsistent or non-grounded substeps and relies on model heuristics rather than formal proof.",
            "uuid": "e6819.3",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (CoT-SC)",
            "brief_description": "An ensemble-style decoding method that samples multiple independent chain-of-thoughts and aggregates via majority voting to select the final answer, improving robustness to individual chain errors.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Applied to GPT-4 in baselines; uses multiple sampled reasoning paths per input and majority voting for final decision.",
            "model_size": null,
            "architecture_type": "transformer with self-consistency ensemble over chain-of-thought samples",
            "training_data": null,
            "reasoning_method": "Generate multiple independent CoT reasoning paths (paper used 5 per image) and choose final answer by majority vote.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reddit-OSHA (multimodal); cited StrategyQA result for PaLM-2",
            "benchmark_description": "Reddit-OSHA hazard identification (multimodal); StrategyQA (boolean multi-hop QA) is cited for PaLM-2 CoT+SC results.",
            "task_type": "Multimodal hazard detection; multi-hop boolean QA (as cited)",
            "performance_metric": "win rate; accuracy (cited for StrategyQA example)",
            "performance_value": "Reddit-OSHA: CoT-SC achieved 100% win rate on curated set (5 independent reasoning paths per image). Cited PaLM-2 result on StrategyQA with few-shot CoT+Self-Consistency: 90.20% accuracy.",
            "comparison_with_baseline": "CoT-SC improved robustness over single-path CoT in cited prior work (e.g., PaLM-2 on StrategyQA); on Reddit-OSHA it achieved perfect correctness on the small curated set.",
            "key_findings": "Self-consistency can significantly improve CoT robustness by aggregating multiple reasoning trajectories; in this paper CoT-SC yields perfect win rate on the curated Reddit-OSHA samples.",
            "limitations": "Computationally expensive due to multiple sampled reasoning paths; majority-vote aggregation can mask systematic biases; still lacks formal verification of intermediate steps.",
            "uuid": "e6819.4",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A deliberative search method that explores multiple partial reasoning states in a tree structure to find better solutions than linear CoT, trading time for deeper exploration.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Applied as a reasoning strategy with GPT-4, exploring a tree with max depth 3 and breadth 2 per node in this paper's experiments.",
            "model_size": null,
            "architecture_type": "transformer with tree‑search planning over intermediate thought states",
            "training_data": null,
            "reasoning_method": "Tree-of-Thoughts search: explore multiple reasoning branches (tree) and select promising branches to continue, enabling backtracking and recombination of partial solutions.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reddit-OSHA (multimodal)",
            "benchmark_description": "Multimodal hazard identification dataset; ToT used to produce detailed reasoning across branches.",
            "task_type": "Multimodal hazard detection / deliberative problem solving",
            "performance_metric": "win rate; reasoning richness (length/verbosity)",
            "performance_value": "ToT achieved 100% win rate on Reddit-OSHA and produced the most verbose reasoning (often &gt;1000 sentences per image).",
            "comparison_with_baseline": "ToT matched or exceeded simpler strategies in correctness on the curated dataset, producing far richer/longer reasoning traces at greater computational cost.",
            "key_findings": "Deliberative tree search with LLMs can correct errors of simpler approaches and produce richer reasoning, but at substantial verbosity and likely higher compute.",
            "limitations": "Extremely verbose outputs (&gt;&gt;1000 sentences) and higher computation; still produces unverified natural language reasoning and does not provide formal guarantees.",
            "uuid": "e6819.5",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GoT",
            "name_full": "Graph-of-Thoughts (GoT)",
            "brief_description": "An iterative reasoning approach that forms a directed graph of thought states allowing richer connectivity (multiple predecessors/successors) and more flexible combination of ideas than tree structures.",
            "citation_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Used as a baseline reasoning strategy performing iterative graph-based thought construction with 3 iterations per image in the Reddit-OSHA experiments.",
            "model_size": null,
            "architecture_type": "transformer with graph-structured reasoning over thought states",
            "training_data": null,
            "reasoning_method": "Graph-of-Thoughts: iterative construction of a reasoning graph enabling combination and revisiting of subthoughts across iterations.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reddit-OSHA (multimodal)",
            "benchmark_description": "Multimodal hazard verification set; GoT used to iteratively refine analyses.",
            "task_type": "Multimodal hazard detection / iterative graph reasoning",
            "performance_metric": "win rate",
            "performance_value": "GoT achieved 100% win rate on the curated Reddit-OSHA samples.",
            "comparison_with_baseline": "Matches ToT and CoT-SC in correctness on the curated dataset while offering a different structural approach to compositional reasoning.",
            "key_findings": "Graph-structured reasoning can reach perfect correctness on small curated multimodal tasks and allow richer compositionality than linear CoT.",
            "limitations": "Not formally verified; complexity and implementation overhead of graph search not quantified; evaluation on small curated set may not generalize to wider, noisier data.",
            "uuid": "e6819.6",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4 (baseline)",
            "name_full": "GPT-4 (used as underlying LLM for baselines)",
            "brief_description": "Large multimodal/LLM used as the base model for all baseline reasoning strategies (CoT, CoT-SC, ToT, GoT) in the Reddit-OSHA experiments reported in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based language (and vision-capable in the multimodal usage) model provided by OpenAI; the paper uses GPT-4 to run CoT/CoT-SC/ToT/GoT baselines on image+text inputs.",
            "model_size": null,
            "architecture_type": "transformer (multimodal in vision-language setup)",
            "training_data": null,
            "reasoning_method": "Prompting-based reasoning using CoT, CoT-SC, ToT, GoT strategies as experimental baselines.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Reddit-OSHA (multimodal)",
            "benchmark_description": "Curated hazard images and descriptions from r/OSHA requiring visual grounding and safety inference.",
            "task_type": "Multimodal hazard detection and binary safety decision",
            "performance_metric": "win rate; reasoning richness (sentence count)",
            "performance_value": "CoT 99.03% win rate (concise responses); CoT-SC/ToT/GoT each achieved 100% win rate on the curated Reddit‑OSHA samples.",
            "comparison_with_baseline": "These GPT-4 baselines indicate that advanced prompting/decoding strategies can achieve near-perfect performance on the small curated dataset; PoT focuses on verifiable outputs and shows improvements in compilation robustness but different success criteria.",
            "key_findings": "GPT-4 paired with advanced reasoning strategies attains extremely high correctness on the curated multimodal hazard set, with trade-offs between concision and reasoning verbosity.",
            "limitations": "High performance on this small curated set may not generalize; reasoning traces are not formally verified and can be internally inconsistent despite correct final answers.",
            "uuid": "e6819.7",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PaLM-2 (cited)",
            "name_full": "PaLM-2 (cited SOTA example)",
            "brief_description": "PaLM-2 is cited as a high-performing LLM that achieved 90.20% accuracy on StrategyQA when using few-shot Chain-of-Thought prompting with Self-Consistency (cited from Anil et al., 2023).",
            "citation_title": "PaLM 2 technical report.",
            "mention_or_use": "mention",
            "model_name": "PaLM-2",
            "model_description": "Cited large pre-trained transformer language model (Google) that, per the citation, when combined with few-shot CoT and self-consistency achieved high StrategyQA performance.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Few-shot Chain-of-Thought prompting combined with Self-Consistency sampling/aggregation (as cited).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "StrategyQA (cited result)",
            "benchmark_description": "Boolean multi-hop implicit reasoning benchmark requiring hidden reasoning chains.",
            "task_type": "Multi-hop boolean question answering (implicit reasoning)",
            "performance_metric": "accuracy",
            "performance_value": "90.20% accuracy (PaLM-2 with few-shot CoT + Self-Consistency, cited).",
            "comparison_with_baseline": "Cited as a strong existing result on StrategyQA; PoT reports different metrics (verification/compilation/precision/recall) and does not provide a direct apples-to-apples accuracy comparison.",
            "key_findings": "High-performing LLMs can reach strong accuracy on StrategyQA with CoT + Self-Consistency, but such approaches lack formal verification of intermediate steps.",
            "limitations": "Cited result is from prior work; PaLM-2 CoT outputs are not formally verified in that prior setup and can be opaque.",
            "uuid": "e6819.8",
            "source_info": {
                "paper_title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models.",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "PaLM 2 technical report.",
            "rating": 2,
            "sanitized_title": "palm_2_technical_report"
        },
        {
            "paper_title": "On the dangers of stochastic parrots: Can language models be too big?.",
            "rating": 1,
            "sanitized_title": "on_the_dangers_of_stochastic_parrots_can_language_models_be_too_big"
        },
        {
            "paper_title": "DeepProbLog: Neural probabilistic logic programming.",
            "rating": 1,
            "sanitized_title": "deepproblog_neural_probabilistic_logic_programming"
        },
        {
            "paper_title": "Scallop: A language for neurosymbolic programming.",
            "rating": 1,
            "sanitized_title": "scallop_a_language_for_neurosymbolic_programming"
        }
    ],
    "cost": 0.019963,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROOF OF THOUGHT : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning
23 Oct 2024</p>
<p>Debargha Ganguly debargha@case.edu 
Srinivasan Iyengar sriyengar@microsoft.com 
Vipin Chaudhary 
Shivkumar Kalyanaraman </p>
<p>Case Western Reserve University</p>
<p>Microsoft Corporation</p>
<p>Case Western Reserve University</p>
<p>Microsoft Corporation</p>
<p>PROOF OF THOUGHT : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning
23 Oct 20248BD7E4C4AE3A6B66AB0DDB892C5C8C6CarXiv:2409.17270v2[cs.AI]
Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences.This research introduces PROOF OF THOUGHT, a framework that enhances the reliability and transparency of LLM outputs.Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny.Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts.This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes.Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications.We demonstrate PROOF OF THOUGHT's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios.By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have revolutionized the field of AI and enabled a wide range of applications.However, as these models are increasingly deployed to process unstructured data and perform complex tasks autonomously, their inconsistent reasoning capabilities remain a critical limitation [Marcus, 2020].This inconsistency manifests in variable performance across out-of-domain reasoning, negation understanding, and extended logical chains, suggesting a reliance on superficial heuristics [Bender et al., 2021].The implications are far-reaching, particularly in high-stakes domains where reliable and transparent decision-making is crucial [Rudin, 2019].Errors or biases in these contexts could have severe consequences, underscoring the urgent need for more dependable and interpretable AI systems.</p>
<p>Recent advances in prompt engineering have shown promise in addressing these challenges.Techniques such as Chain-of-Thought (CoT) [Wei et al., 2022], Self-Consistency with CoT (CoT-SC) 1. Propose PROOF OF THOUGHT (PoT), a novel approach that leverages the in-context learning and code generation capabilities of LLMs while incorporating their inherent knowledge and spatial understanding.Our system employs a custom interpreter that parses "LLM-Thoughts" (represented as DSL code snippets) to generate First Order Logic programs, which are then verified by a Z3 theorem prover.2. Introduce an intermediate JSON-based DSL (neurosymbolic representation) and the associated interpreter that operates on human-understandable abstract concepts using intuitive, near-English language constructs (see Fig 1).This strikes a balance between the precision required for logical definitions with formal first-order logic proofs and accessibility for non-expert users.3. Benchmark performance over StrategyQA (a boolean multi-hop implicit NLP reasoning benchmark) and a novel multimodal real-world long-tail reasoning problem (Reddit-OSHA Benchmark).This shows that PoT works on complex, and a wide variety of tasks.</p>
<p>PROOF OF THOUGHT enhances the capabilities of LLMs in complex, open-ended scenarios by providing reasoning guarantees, conditioned on correctness of knowledge base and rule specifications, therefore furnishing a framework for human-in-the-loop oversight and verification.</p>
<p>Related Work</p>
<p>Early Integration Attempts laid the groundwork for neuro-symbolic AI.Works like EBL-ANN [Towell and Shavlik, 1994], KBANN [Towell et al., 1990], and C-ILP [d'Avila Garcez et al., 2009] incorporated propositional formulae into neural networks.While pioneering, these approaches struggled with scalability and expressiveness.Knowledge Graph integration advanced the field further.Methods proposed by Chen et al. [2020a] and Kampffmeyer et al. [2019] showed promise in leveraging structured knowledge for improved reasoning.However, maintaining interpretability and explicit rule incorporation remained challenging.Differentiable Logic Programming frameworks like DeepProbLog [Manhaeve et al., 2018] and Scallop [Li et al., 2023] demonstrated the potential of integrating probabilistic logic programming with neural networks.These approaches enable end-to-end training of neuro-symbolic systems but face limitations in handling complex reasoning tasks and diverse logical formalisms.Gupta and Kembhavi [2023] showed how compositional visual reasoning can be done by program generation without training.</p>
<p>Large Language Models have opened new avenues for neuro-symbolic reasoning.Techniques such as Chain-of-Thought [Wei et al., 2022], Tree-of-Thoughts [Yao et al., 2024], and Graph-of-Thoughts [Besta et al., 2024] have shown impressive results in complex reasoning tasks.However, these methods often produce inconsistent intermediate steps and struggle with out-of-domain reasoning.</p>
<p>Interpretable Concept-based Models, as explored by Kim et al. [2018] and Chen et al. [2020b], aim to increase trust in deep learning models.However, state-of-the-art approaches often rely on high-dimensional concept embeddings that lack clear semantic meaning, limiting their interpretability.</p>
<p>Figure 1: Architecture of the PROOF OF THOUGHT (PoT) framework, illustrating the integration of natural language reasoning with formal logical verification.</p>
<p>Advanced Neuro-symbolic Frameworks like the Deep Concept Reasoner (DCR) [Barbiero et al., 2023] and GENOME [Chen et al., 2023] have made progress in combining neural and symbolic components.DCR constructs syntactic rule structures from concept embeddings, while GENOME introduces a modular approach for visual reasoning tasks.Despite these advancements, challenges in scalability and generalization to diverse task domains persist.A critical issue is reasoning shortcuts, where models use unintended concept semantics.To address this, Marconato et al. [2024] introduced BEARS, improving model calibration and informative annotation acquisition.These developments showcase the potential for imparting robustness and reliability via neuro-symbolic AI systems.</p>
<p>LLM Code Generation and Low-Resource Translation demonstrate the remarkable adaptability of large language models.In code generation, LLMs like Codex [Chen et al., 2021] and AlphaCode [Li et al., 2022] show proficiency across multiple programming languages, often outperforming specialized models.These systems excel at understanding context, generating syntactically correct code, and even solving complex algorithmic problems.Paralleling this, research in low-resource language translation [Zoph et al., 2016, Tanzer et al., 2023] reveals LLMs' ability to rapidly adapt to new languages with minimal examples.Techniques like few-shot learning and cross-lingual transfer enable models to leverage knowledge from high-resource languages to improve performance on low-resource ones.Both domains highlight LLMs' capacity for quick adaptation and generalization, suggesting potential for enhanced neuro-symbolic systems that can efficiently learn and apply formal reasoning across diverse domains with limited training data.</p>
<p>PROOF OF THOUGHT: A Neurosymbolic Reasoning Framework</p>
<p>In this section, we introduce PROOF OF THOUGHT (PoT), a novel framework that bridges NLP with formal logical reasoning to enhance the interpretability and verifiability of LLM outputs.We first outline the foundational concepts and notations that underpin our approach.</p>
<p>Background Concepts &amp; Notation from LLM Reasoning Literature</p>
<p>Let p θ denote a pre-trained language model (LM) with parameters θ.In a conversation, user messages (prompts) and LM replies (thoughts) are exchanged.We use lowercase letters x, y, z, .ToT by allowing more complex connections between thoughts, forming a directed graph structure.In GoT, thoughts can have multiple predecessors and successors, enabling more flexible reasoning paths and the combination of ideas from different branches.This approach allows for cyclic reasoning patterns and can capture more intricate problem-solving strategies.</p>
<p>Framework Overview</p>
<p>"All our knowledge begins with the senses, proceeds then to the understanding, and ends with reason.There is nothing higher than reason."-Immanuel Kant, in Critique of Pure Reason PROOF OF THOUGHT models the LLM's reasoning process as a structured transformation from natural language input to formal logical expressions that can be verified using theorem proving techniques.The framework consists of three primary components :</p>
<p>• Logical Representation Generator G: Maps input x to a logical representation L using p θ .</p>
<p>• Interpreter I: Parses L and constructs formal logical expressions ϕ in first-order logic (FOL).• Theorem Prover T : Verifies the validity of ϕ and provides proofs or counterexamples.</p>
<p>The PoT reasoning process can thus be formalized as:
L = G(x; p θ ); ϕ = I(L); Verification Result = T (ϕ)
Guarantees: By using theorem proving, we rely on the principle of logical consequence, where conclusions are guaranteed to be true if the premises and inference rules, depicted in the logical representations (both of which are human readable, allowing interpretability and verifiability).These logical representations are the ultimate arbiter of truth and validity.The guarantees are what distinguishes PoT from other forms of reasoning.Inductive reasoning (drawing general conclusions from specific observations) can be useful, but doesn't offer the same level of certainty.In contrast, guarantees with theorem proving allow us to establish precise arguments with mathematical truths with absolute confidence, elevating the reasoning process beyond mere conjecture or intuition.</p>
<p>Our PROOF OF THOUGHT framework architecture (shown in Fig 1) introduces a JSON-based Domain Specific Language (DSL) along with the associated interpreter.Next, we discuss the design choices for these in detail.</p>
<p>Design of the JSON-Based Domain-Specific Language (DSL)</p>
<p>The core of our logical reasoning system is built upon a carefully designed JSON-based Domain-Specific Language (DSL) that serves as an intermediate representation for translating reasoning tasks into formal logic.This DSL was created with the primary challenge of being general-purpose enough to accommodate a wide range of reasoning problems.The choice of JSON as the underlying format was deliberate, leveraging its widespread use, human readability, and ease of parsing.This design decision ensures that the logical representations are both machine-parseable and accessible to users who may not have expertise in formal logic or programming.Moreover, JSON's compatibility with structured outputs from AI service providers like OpenAI and Google, which offer guaranteed outputs matching particular schemas, makes it an ideal choice for our system that doesn't rely on retraining models.The key components of the DSL are:</p>
<ol>
<li>
<p>Sorts (S) define the domains or types used in the logic.Let S = {S 1 , S 2 , . . ., S n } be the set of sorts, where each S i represents a specific domain.The inclusion of sorts in our system is a key differentiator.It allows for reasoning over high-level, human-understandable concepts, which is crucial for bridging the gap between natural language problem descriptions and formal logical representations.The sort definition in Fig 2 allows for more intuitive problem representation.For instance, instead of reasoning about abstract entities, we can now reason about Persons, Equipment, Tasks, Locations, and Time.This makes it easier to translate natural language problems into formal logic.The type-safe reasoning catches semantic errors early.For example, if we tried to apply a function meant for Equipment to a Person, the system would catch this error before any reasoning takes place.Moreover, this It allows for easy integration of domainspecific knowledge -for instance, we know that "alice", "bob", and "charlie" are Persons in our system.In future work, we believe this structure has the potential for linking with external databases or knowledge graphs.For example, the "equipment" constants could be linked to an external database containing detailed specifications for each piece of equipment.4. Variables (V) enable clear scoping rules for quantifiers and type-safe substitutions in logical formulas.Let x i : S j denote that variable x i ranges over sort S j (see Fig 2). 5. Knowledge Base (KB) contains axioms or facts assumed to be true within the logical system.Let KB = {φ 1 , φ 2 , . . ., φ m } where each φ i is a well-formed formula in first-order logic.The structured knowledge base allows for separation of axioms from rules and queries, and supports incremental knowledge addition.In Fig 2, the knowledge base contains factual information about the problem domain.It's separate from the rules and verifications, allowing for easy updates and additions without changing the core reasoning system.This structure also allows for potential consistency checking.For instance, we could verify that no person is assigned to two tasks at the same time, or that no task has a negative duration.</p>
</li>
<li>
<p>Rules (R) specify logical constructs, often involving quantifiers and implications.Let R = {r 1 , r 2 , . . ., r l } where each r i is a well-formed formula representing a rule.The explicit representation of rules enables a clear distinction between factual knowledge and inferential knowledge.These rules represent inferential knowledge -knowledge that can be derived from the facts in the knowledge base.They allow for easy addition of domainspecific reasoning patterns.This explicit representation of rules also supports explainable AI.When the system makes an inference, it can point to the specific rule(s) used, making its reasoning process transparent and understandable to users.while ensuring that all tasks are assigned and that each assigned person has at least the minimum required skill level.This ability to combine logical constraints with numerical optimization allows for the representation of complex real-world problems that go beyond pure logical satisfiability.We intend to benchmark this in future work.Here is an example:
• minimize f obj (x) = n i=1 cost(x i ) • subject to: ∀i, 1 ≤ i ≤ n, Safe(x i )</p>
</li>
<li>Actions (A) list what the interpreter has to perform.The only two possible actions are 'verify' and 'optimize'.This simple declaration tells the system what to do with the problem representation we've built up.It provides flexibility in choosing different reasoning or optimization approaches for the same problem representation.For extensibility, this structure also opens up possibilities for meta-reasoning about which actions to take.For instance, the system could analyze the problem structure to decide whether to attempt verification first or go straight to optimization.</li>
</ol>
<p>Design of the Interpreter's Facilities and Capabilities</p>
<p>This section provides an in-depth description of the interpreter's facilities, detailing how it constructs and manipulates logical expressions.</p>
<p>Type System, Sort Management: The interpreter implements a robust type system, managing sorts and ensuring type safety across all expressions.It supports a variety of Z3 compatible sorts, including primitive sorts like Bool, Int, and Real, which form the foundation of the type system.User-defined sorts, known as declared sorts, allow for the representation of specific domains such as Person or Equipment.For situations requiring a finite set of elements, enumerated sorts are available.The type system also accommodates composite sorts, constructed using type constructors, which enable the creation of function sorts or tuple sorts.Throughout its operations, the interpreter rigorously enforces type consistency, ensuring that functions and predicates are applied only to arguments of the correct sorts.</p>
<p>Symbol Table, Scope Management: Central to the interpreter's functionality is a symbol table that maintains mappings from identifiers to their definitions, including variables, constants, and functions.This table is crucial for scope management, particularly when dealing with quantified variables in logical expressions.The parsing process is another key component, where the interpreter builds abstract syntax trees (ASTs) that represent the structure of expressions.This process handles a wide range of logical constructs, from atomic formulas (basic predicates applied to terms) to complex formulas constructed using logical connectives and quantifiers.The interpreter pays special attention to quantifiers, carefully managing bound variables to ensure correct scoping.Additionally, it supports substitution of terms for variables, an essential operation in applying inference rules.</p>
<p>Pre-processing: While the bulk of reasoning is handled by the theorem prover, the interpreter applies basic inference and simplification rules to optimize expressions before passing them on.This includes simplification processes that reduce expressions using logical identities, such as eliminating double negations.Normalization is another crucial step, converting expressions into a standard form (like prenex normal form) to facilitate theorem proving.The interpreter also performs early error detection, identifying contradictory statements or type mismatches before they can cause issues in later stages of processing.</p>
<p>Feedback Loop: Adequate error handling and diagnostics are paramount in the interpreter's design.</p>
<p>It provides detailed error messages to assist the LLM in identifying and correcting issues with its programs.These diagnostics cover a range of potential problems, including type errors that indicate inconsistencies or mismatches in the type system, alerts for undefined symbols when functions, predicates, or constants are used without proper definition, and syntax errors that highlight issues in the structure of logical expressions.</p>
<p>Future Proofing: The interpreter's architecture emphasizes extensibility and customization.Users have the flexibility to extend its capabilities in several ways.They can add new sorts to define additional domains of discourse, expanding the system's ability to represent complex scenarios.The logical language can be enhanced by defining new functions and predicates, allowing users to capture more intricate relationships within their domain of interest.Furthermore, the modular design of the interpreter facilitates integration with different theorem provers or logic systems, enhancing its versatility and applicability to various problem domains.</p>
<p>Results</p>
<p>StrategyQA -Complex Natural Language Reasoning</p>
<p>Task Setup: StrategyQA presents a significant challenge in natural language processing, testing a model's ability to perform multi-hop, implicit reasoning across diverse scenarios.This boolean question answering benchmark requires models to infer unstated reasoning steps, mirroring complex human cognitive processes.For example, "Did Aristotle use a laptop?"requires the implicit chain: "When did Aristotle live?When was the laptop invented?Do these time periods overlap?"This level of abstraction surpasses simple fact retrieval or explicit reasoning tasks in other benchmarks like BoolQ or Twenty Questions (20QA).While state-of-the-art language models have shown impressive performance on StrategyQA (e.g., PaLM-2 achieving 90.20% accuracy with few-shot Chain of Thought and Self Consistency [Anil et al., 2023]), they lack transparency and verifiability in their reasoning process.Our PROOF OF THOUGHT (PoT) framework addresses this limitation by providing complete, explicit, and verifiable reasoning chains.PoT breaks down implicit reasoning steps into explicit logical representations, defines the knowledge base used, and ensures each inference is provable through a theorem prover.</p>
<p>PoT Results: We evaluated PoT on a sample of 1000 questions from the StrategyQA dataset, focusing on the framework's ability to generate syntactically correct programs, produce provable reasoning chains, and match outputs with correct answers.The system, with the inclusion of a 3 step feedback loops (i.e., initial prompt, +2 attempts at resolving), successfully compiled and executed 82.4% of the 1000 processed questions, marking a significant improvement from runs with lower feedback loops.This increase in compilation success rate underscores the effectiveness of our feedback mechanism in addressing and resolving issues in generated logical representations.The system demonstrated strong recall at 91.40%, indicating its proficiency in identifying true positive cases.The F1 score of 71.13% suggests a good balance between precision and recall, though precision (58.22%) presents an area for potential enhancement.The high recall, coupled with a false positive rate of 53.98%, indicates a tendency for the system to overpredict positive cases.This observation points to a need for future refinements in discriminating between positive and negative instances more accurately.While the compilation success rate is encouraging, the 17.6% of questions that failed to compile highlight an area for further improvement.Enhancing the robustness of the code generation process through improved prompting techniques, fine-tuning, and expansion of the feedback loop mechanism could potentially reduce this failure rate in future iterations.</p>
<p>Multimodal Reddit-OSHA Benchmark</p>
<p>Task Setup: We curated 103 samples from the r/OSHA subreddit, featuring individuals in extremely hazardous situations.This dataset represents long-tail, low-probability scenarios, mirroring challenging real-world deployment conditions for health and safety applications.The images encompass a wide range of problems with varied lighting, scene setups, visual clutter, and resolutions.</p>
<p>Baselines: We implemented four reasoning strategies using GPT-4 as the underlying language model: Chain of Thought (CoT), Chain of Thought with Self-Consistency (CoT-SC), Tree of Thought (ToT), and Graph of Thought (GoT).Each baseline processes base64-encoded images and uses a consistent system prompt instructing the model to act as a safety inspector.CoT encourages step-by-step reasoning, concluding with a binary hazard decision.CoT-SC extends this by generating 5 independent reasoning paths per image, with the final decision determined by majority voting.</p>
<p>ToT explores multiple reasoning paths in a tree-like structure with a maximum depth of 3 and a breadth of 2 at each node.GoT implements an iterative reasoning process with 3 iterations per image, building upon previous analyses.Evaluation metrics include win rate (proportion of correctly identified hazards) and reasoning richness (number of sentences in model responses).</p>
<p>Baseline Results: All reasoning strategies demonstrated high performance, with CoT achieving a 99.03% win rate and CoT-SC, ToT, and GoT all achieving perfect 100% win rates.This suggests that advanced reasoning strategies can correct errors made by simpler approaches.Reasoning richness varied significantly, with ToT producing the most detailed responses (often over 1000 sentences per image) and CoT the most concise (25-30 sentences).</p>
<p>PoT Results: Our PROOF OF THOUGHT framework showed remarkable improvements on this dataset with the inclusion of a 3 step feedback loop (i.e., initial prompt, +2 attempts at resolving).Notably, we reduced compilation errors from 14.6% to 0%, demonstrating the effectiveness of our feedback and error correction mechanisms.The win rate on compiled programs increased from 72% to 81.55%, indicating both more reliable code generation and more accurate logical reasoning.</p>
<p>Discussion and Future work</p>
<p>Future research directions include expanding PoT to handle more complex logical structures, that extend past boolean SAT &amp; UNSAT, using one versus all setups, and developing more sophisticated feedback mechanisms to further reduce compilation errors.We intend to explore JSON-like representations for non-boolean responses.Additionally, exploring ways to make the logical representations more accessible to non-expert users and investigating the scalability of PoT to larger, more diverse datasets will be important next steps.Further, integrating PoT with other techniques such as model updates using reinforcement learning, or supervised fine-tuned models on synthetically generated syntactically correct PoT programs might unlock at-scale "System 2" thinking.</p>
<p>Conclusion</p>
<p>PROOF OF THOUGHT bridges the gap between language models' flexibility and formal logic's rigor, offering a promising solution for trustworthy reasoning in vision-language models.By enhancing interpretability and providing reasoning guarantees, PoT addresses critical challenges in AI system accountability and reliability.Our results demonstrate its potential in both natural language and multimodal reasoning tasks, paving the way for more transparent, verifiable AI systems capable of complex reasoning in high-stakes domains.{ " sorts " : [ { " name " : " Person " , " type ": " DeclareSort "} , { " name " : " Animal " ," type ": " DeclareSort "} , { " name " : " Real " ," type ": " RealSort "} ] , " functions " : [ { " name " : " jump_height " ," domain ": [" Person "] ," range ": " Real "} , { " name " : " height " ," domain ": [" Animal "] ," range ": " Real "} ] , " constants " : { " persons " : { " sort " : " Person " ," members ": [" javier_sotomayor "]} , " animals " : { " sort " : " Animal " ," members ": [" average_giraffe "]} } , " variables " : [ { " name " : " p " ," sort ": " Person "} , { " name " : " a " ," sort ": " Animal "} ] , " knowledge_base " : [ { " assertion " : " jump_height ( javier_sotomayor ) == 2.45"} , { " assertion " : " height ( average_giraffe ) == 5.5"} ] , " verifications " : [ { " name " : " Sotomayor Jump Over Giraffe " ," constraint ": " jump_height ( javier_sotomayor ) &gt;= height ( average_giraffe ) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 2. Question: Did the Cherokee people send a delegation to oppose allotment?</p>
<p>Answer: True Predicted Answer: SAT (True)</p>
<p>{ " sorts " : [ { " name " : " Group " ," type ": " DeclareSort "} , { " name " : " Action " ," type ": " DeclareSort "} , { " name " : " Bool " ," type ": " BoolSort "} ] , " functions " : [ { " name " : " send_delegation " ," domain ": [" Group "] ," range ": " Bool "} , { " name " : " oppose_allotment " ," domain ": [" Group "] ," range ": " Bool "} ] , " constants " : { " groups " : { " sort " : " Group " ," members ": [" cherokee_people "]} , " actions " : { " sort " : " Action " ," members ": [" allotment "]} } , " variables " : [ { " name " : " g " ," sort ": " Group "} ] , " knowledge_base " : [ { " assertion " : " send_delegation ( cherokee_people ) "} , { " assertion " : " ForAll ([ g ] , Implies ( send_delegation ( g ) , oppose_allotment ( g ) ) ) " , " variables " : [{ " name " : " g " ," sort ": " Group "}] } ] , " verifications " : [ { " name " : " Cherokee Oppose Allotment " ," constraint ": " oppose_allotment ( cherokee_people ) "} ] , " actions " : [ " ver ify _co ndi tio ns " ] } { " sorts " : [ { " name " : " Person " ," type ": " DeclareSort "} , { " name " : " Equipment " ," type ": " DeclareSort "} , { " name " : " SafetyGear " ," type ": " DeclareSort "} ] , " functions " : [ { " name " : " Using " ," domain ": [" Person " ," Equipment "] ," range ": " BoolSort " } , { " name " : " Wearing " ," domain ": [" Person " ," SafetyGear "] ," range ": " BoolSort "} ] , " constants " : { " persons " : { " sort " : " Person " ," members ": [" worker "]} , " equipments " : { " sort " : " Equipment " ," members ": [" ladder "]} , " safetyGears " : { " sort " : " SafetyGear " ," members ": [" hardHat " ," harness "]} } , " knowledge_base " : [ { " assertion " : " Using ( worker , ladder ) " ," value ": true } , { " assertion " : " Wearing ( worker , hardHat ) " ," value ": false } , { " assertion " : " Wearing ( worker , harness ) " ," value ": false } ] , " rules " : [ { " name " : " Hard Hat Rule " ," forall ": [{" name ": " p " ," sort ": " Person "} , { " name " : " e " ," sort ": " Equipment "}] ," implies ": {" antecedent ": " Using (p , e ) " , " consequent ": " Wearing (p , hardHat ) "}} , { " name " : " Harness Rule " ," forall ": [{" name ": " p " ," sort ": " Person "} , {" name ": " e " ," sort ": " Equipment "}] , " implies " : { " antecedent " : " Using (p , e ) " ," consequent ": " Wearing (p , harness ) "}} ] , " verifications " : [ { " name " : " Verify Hard Hat Compliance " ," constraint ": " Wearing ( worker , hardHat ) "} , { " name " : " Verify Harness Compliance " ," constraint ": " Wearing ( worker , harness ) "} ] , " actions " : [ " ver ify _co ndi tio ns " ] }</p>
<p>Real</p>
<p>HSE Example 2</p>
<p>{ " sorts " : [{ " name " : " Person " ," type ": " DeclareSort "} , { " name " : " Equipment " ," type ": " DeclareSort "} , { " name " : " SafetyEquipment " ," type ": " DeclareSort "} ] , " functions " : [ { " name " : " StandingOn " ," domain ": [" Person " ," Equipment "] ," range ": " BoolSort "} , { " name " : " U s i n g S a f e t y E q u i p m e n t " ," domain ": [" Person " ," SafetyEquipment "] ," range ": " BoolSort "} , { " name " : " IsSafe " ," domain ": [" Person "] ," range ": " BoolSort "} ] , " constants " : { " persons " : { " sort " : " Person " ," members ": [" worker "]} , " equipments " : { " sort " : " Equipment " ," members ": [" forklift " ," pallet "]} , " safetyEquipments " : { " sort " : " SafetyEquipment " ," members ": [" harness "]} } , " knowledgebase " : [ { " assertion " : " StandingOn ( worker , pallet ) " ," value ": true } , { " assertion " : " U s i n g S a f e t y E q u i p m e n t ( worker , harness ) " ," value ": false } ] , " rules " : [ { " name " : " Safety Rule " ," forall ": [{" name ": " p " ," sort ": " Person "}] , " implies " : { " antecedent " : " And ( StandingOn (p , pallet ) , Not ( U s i n g S a f e t y E q u i p m e n t (p , harness ) ) ) " ," consequent ": " Not ( IsSafe ( p ) ) "}} ] , " verifications " : [ { " name " : " Verify Safety " ," constraint ": " IsSafe ( worker ) "} ] , " actions " : [ " ver ify _co ndi tio ns " ] }</p>
<p>HSE Example 3</p>
<p>{ " sorts " : [ { " name " : " Person " ," type ": " DeclareSort "} , { " name " : " Equipment " ," type ": " DeclareSort "} , { " name " : " Location " ," type ": " DeclareSort "} ] , " functions " : [ { " name " : " Worker " ," domain ": [" Person "] ," range ": " BoolSort "} , { " name " : " Using " ," domain ": [" Person " ," Equipment "] ," range ": " BoolSort "} , { " name " : " AtHeight " ," domain ": [" Person "] ," range ": " BoolSort "} , { " name " : " Ha sFa llP rot ect ion " ," domain ": [" Person "] ," range ": " BoolSort "} , { " name " : " Stable " ," domain ": [" Equipment "] ," range ": " BoolSort "} ] , " constants " : { " persons " : { " sort " : " Person " ," members ": [" worker "]} , " equipments " : { " sort " : " Equipment " ," members ": [" ladder " ," scaffold "]} , " locations " : { " sort " : " Location " ," members ": [" worksite "]} } , " knowledge_base " : [ " Worker ( worker ) " ," Using ( worker , ladder ) " ," Using ( worker , scaffold ) " , " AtHeight ( worker ) " , { " assertion " : " Stable ( ladder ) " ," value ": false } , { " assertion " : " Stable ( scaffold ) " ," value ": false } , { " assertion " : " Ha sFa ll Pro tec tio n ( worker ) " ," value ": false } ] , " rules " : [ { " name " : " Safety Rule " ," forall ": [{" name ": " p " ," sort ": " Person "}] , " implies " : { " antecedent " : " And ( Worker ( p ) , AtHeight ( p ) ) " , " consequent ": " Ha sFa llP rot ec tio n ( p ) "} } , { " name " : " Stability Rule " ," forall ": [{" name ": " e " ," sort ": " Equipment "}] , " implies " : { " antecedent " : " Using ( worker , e ) " ," consequent ": " Stable ( e ) "} }] , " verifications " : [ { " name " : " Verify Safety " , " constraint ": " And ( Has Fa llP rot ect ion ( worker ) , Stable ( ladder ) , Stable ( scaffold ) ) "} ] , " actions " : [ " ver ify _co ndi tio ns " ] }</p>
<p>A Qualitative Analysis of Generated DSL Programs and Reasoning Patterns</p>
<p>The generated DSL programs across both the StrategyQA dataset and the OSHA dataset illustrate how formal logical representations can be used to model complex reasoning tasks.In both cases, the structured use of sorts, functions, rules, and verifications ensures that the questions posed are systematically decomposed into logical assertions that can be verified by a theorem prover such as Z3.Here, we analyze key aspects of these programs and how they contribute to effective reasoning.</p>
<p>Sorts and Function Definitions as the Backbone of Logical Modeling</p>
<p>The use of DeclareSort, BoolSort, RealSort, and other basic sorts in the DSL programs serves as the foundation for defining the domains of discourse.For example, in the StrategyQA question involving Javier Sotomayor and a giraffe, the Person and Animal sorts allow the definition of relationships between humans and animals in terms of measurable attributes (e.g., jump height and height).Similarly, in the OSHA-related examples, Person, Equipment, and SafetyGear sorts model the entities relevant to workplace safety.</p>
<p>By defining functions like jump_height, height, Wearing, and Using, we map the relationships between entities and their properties.These functions serve as predicates that are later used in verifications or rule implications.In these cases, the functions provide critical context for understanding the state of the world and the conditions under which certain outcomes (e.g., compliance with safety regulations or reaching a height) hold true.</p>
<p>Knowledge Base and Its Role in Establishing Ground Truth</p>
<p>The knowledge_base section plays a vital role in grounding the reasoning process by introducing factual information, such as the jump height of Javier Sotomayor (2.45 meters) or the height of an average giraffe (5.5 meters).This knowledge is essential for theorem proving because it establishes the foundational truths that the logical system will work with.Similarly, in the OSHA examples, the knowledge base specifies whether a worker is using certain equipment, wearing protective gear, or working at height.</p>
<p>In both datasets, the knowledge base is used to capture the known facts that are assumed to be true at the start of reasoning.This helps set the initial conditions for the logical rules to be applied.</p>
<p>Rules as Key Drivers of Logical Implication</p>
<p>The rules section formalizes the relationships between entities based on conditional logic.These rules encapsulate the domain knowledge and drive the reasoning process.For instance, the "Hard Hat Rule" in the OSHA examples states that if a person is using equipment, they should also be wearing a hard hat, while the "Harness Rule" mandates that a harness should be worn when using certain equipment.In the StrategyQA example, no explicit rules are needed beyond the basic comparison of heights.</p>
<p>These rules introduce a level of generalization that allows the reasoning process to handle not just specific instances but also classes of entities.For example, the rule: { " name " : " Hard Hat Rule " , " forall " : [{ " name " : " p " ," sort ": " Person "} , {" name ": " e " ," sort ": " Equipment "}] , " implies " : { " antecedent " : " Using (p , e ) " , " consequent ": " Wearing (p , hardHat ) "} } is applicable to any person and any equipment.This generalization is key in enabling logical inferences beyond the immediate facts provided in the knowledge base.</p>
<p>Verifications as the Core of Decision-Making</p>
<p>The verifications section in these programs forms the basis of decision-making by checking whether the conditions specified in the knowledge base and rules hold.In the StrategyQA case, the verification checks if Javier Sotomayor's jump height is greater than or equal to the height of an average giraffe.The outcome of this check (UNSAT) indicates that it is false, thus the predicted answer matches the correct answer.</p>
<p>In the OSHA-related examples, verifications are used to ensure compliance with safety rules.For example, the program checks if the worker is wearing a hard hat or harness while using a ladder.These verifications serve as the final step in determining whether the conditions needed for safety are met or not.The results of these verifications provide a direct SAT (true) or UNSAT (false) outcome, which can then be used to assess compliance or answer the given question.</p>
<p>Patterns in Error Detection and Resolution</p>
<p>The DSL framework helps detect inconsistencies or non-compliance in the input data.For example, the OSHA programs can flag situations where workers are using unsafe equipment or failing to follow safety protocols.This is achieved by systematically comparing the facts provided in the knowledge base with the rules and verifications, ensuring that errors are caught before any conclusions are drawn.</p>
<p>Additionally, the explicit use of logical operators like And, Not, and Implies makes it easy to trace the reasoning path when an outcome is SAT (true) or UNSAT (false).This traceability allows users to understand why a particular result was obtained, making the reasoning process more transparent and interpretable.</p>
<p>Overall Analysis and Utility of Generated DSL Programs</p>
<p>Across both the StrategyQA and OSHA datasets, the use of DSL programs enables structured, logical reasoning that is verifiable and interpretable.The modularity of the DSL-where different aspects of</p>
<p>Figure 2 :
2
Figure 2: Example DSL program components of the PROOF OF THOUGHT (PoT) framework for a dummy task assignment verification and optimization problem.The figure displays the JSON-based Domain-Specific Language (DSL) structure, including sort definitions, variables, functions, constants, knowledge base, rules, verifications, and optimization constraints for a workforce management scenario.</p>
<p>Figure 3 :
3
Figure 3: Performance analysis of the Proof of Thought (PoT) framework on the StrategyQA dataset.The includes four visualizations: (1) a stacked bar chart showing questions answered by attempt, (2) a pie chart displaying the final question status, (3) a confusion matrix for predicted vs. true labels, and (4) a bar chart of various performance metrics including accuracy, precision, recall, F1-score, specificity, and false positive rate.</p>
<p>Figure 5 :
5
Figure 5: Performance analysis of the Proof of Thought (PoT) framework on the Multimodal Reddit-OSHA Benchmark dataset with a stacked bar chart showing questions answered by attempt.</p>
<p>Figure 6 :
6
Figure 6: Example 1-3 from the HSE Reddit OSHA Dataset.</p>
<p>For example, some rules are depicted in Fig 2 and here are a few more:</p>
<p>• ∀x : Person, Worker(x) → ∃y : Equipment, Wearing(x, y) • ∀x : Person, ∀y : Equipment, Wearing(x, y) ∧ SafetyGear(y) → Safe(x) 7. Verifications (V) state properties or conditions to be verified by the theorem prover.Let V = {v 1 , v 2 , ..., v p } where each v i is a well-formed formula to be verified.Separating verifications from the knowledge base and rules allows for clear goal-directed reasoning.These verifications (Fig 2)represent specific properties we want to check in our system.They allow for easy testing and validation of the knowledge base and rules.When a verification fails, the system can potentially generate counter-examples, providing valuable insights into why the desired property doesn't hold.For example:• ∀x : Person, Worker(x) → Safe(x) • ∃x : Person, Worker(x) ∧ ¬Safe(x)8.Optimization (optional) (O) sections define problems with objectives and constraints.Let O = (f obj , C) where f obj is the objective function and C is a set of constraints.The optimization problem in Fig 2 seeks to minimize the total skill level of assigned persons</p>
<p>the reasoning process (entities, relationships, rules, and verifications) are clearly separated-ensures that the programs remain adaptable to a variety of problem domains.The reasoning traces provided by these DSL programs offer significant benefits:• Interpretability: The modular structure makes it easy to follow the logical steps leading to a conclusion.• Error Detection: The use of formal logic allows for early detection of contradictions or violations of safety rules.• Scalability: The DSL framework can handle increasingly complex scenarios by adding new sorts, functions, rules, and verifications.• Generalization: Rules written in a generalized form (e.g., using ForAll) can be applied across different entities and scenarios, making the system more flexible.Exploring the possibilities: Satisfiable Neurosymbolic ProgramsOur DSL designed to be very expressive, and future proof for additional scenarios.In this subsection we present some example problems that can be expressed and solved to be found SAT.1. Simple Arithmetic Verification : Verify that there exists an integer x such that x + 2 = 5.{ " sorts " : [ { " name " : " Int " , " type ": " IntSort "} ] , " functions " : [] , " constants " : {} , " knowledge_base " : [] , " rules " : [] , " verifications " : [ { " name " : " verify_addition " , " exists " : [ { " name " : " x " , " sort ": " Int "} ] , " constraint " : " x + 2 == 5" } ] , " actions " : [ " v eri fy _co ndi tio ns " ] } 2. Basic Safety Equipment Rule : Ensure all workers are wearing hard hats.{ " sorts " : [ { " name " : " Person " , " type ": " DeclareSort "} , { " name " : " Equipment " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " Worker " , " domain ": [" Person "] , " range ": " BoolSort "} , { " name " : " Wearing " , " domain ": [" Person " , " Equipment "] , " range ": " BoolSort "} ] , " constants " : { " persons " : { " sort " : " Person " , " members " : [ " alice " , " bob " ] } , " equipments " : { " sort " : " Equipment " , " members " : [ " hardHat " ] } } , " knowledge_base " : [ " Worker ( alice ) " , " Worker ( bob ) " , " Wearing ( alice , hardHat ) " ] , " rules " : [ { " name " : " Hard Hat Rule " , " forall " : [ { " name " : " p " , " sort ": " Person "} ] , " implies " : { " antecedent " : " Worker ( p ) " , " consequent " : " Wearing (p , hardHat ) " } } ] , " verifications " : [ { " name " : " Check Hard Hat Compliance " , " forall " : [ { " name " : " p " , " sort ": " Person "} ] , " implies " : { " antecedent " : " Worker ( p ) " , " consequent " : " Wearing (p , hardHat ) " } } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 3. Parent-Child Relationship : Define a family tree and verify that a grandparent relationship holds.{ " sorts " : [ { " name " : " Person " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " parent_of " , " domain ": [" Person "] , " range ": " Person "} ] , " constants " : { " persons " : { " sort " : " Person " , " members " : [ " alice " , " bob " , " charlie " ] } } , " knowledge_base " : [ " parent_of ( bob ) == alice " , " parent_of ( charlie ) == bob " ] , " rules " : [] , " verifications " : [ { " name " : " Verify Grandparent " , " constraint " : " parent_of ( parent_of ( charlie ) ) == alice " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 4. Transitive Relation Verification : Verify that a transitive property holds in a relation.{" sorts " : [ { " name " : " Element " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " related " , " domain ": [" Element " , " Element "] , " range ": " BoolSort "} ] , " constants " : { " elements " : { " sort " : " Element " , " members " : [ " x " , " y " , " z " ] } } , " knowledge_base " : [ " related (x , y ) " , " related (y , z ) " ] , " rules " : [ { " name " : " Transitive Rule " , " forall " : [ { " name " : " a " , " sort ": " Element "} , { " name " : " b " , " sort ": " Element "} , { " name " : " c " , " sort ": " Element "} ] , " implies " : { " antecedent " : " And ( related (a , b ) , related (b , c ) ) " , " consequent " : " related (a , c ) " } } ] , " verifications " : [ { " name " : " Verify Transitivity " , " constraint " : " related (x , z ) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 5. Scheduling Without Conflicts : Ensure two tasks are scheduled at different times.{ " sorts " : [ { " name " : " Task " , " type ": " DeclareSort "} , { " name " : " TimeSlot " , " type ": " IntSort "} ] , " functions " : [ { " name " : " scheduled_at " , " domain ": [" Task "] , " range ": " TimeSlot "} ] , " constants " : { " tasks " : { " sort " : " Task " , " members " : [ " task1 " , " task2 " ] } } , " knowledge_base " : [] , " rules " : [] , " verifications " : [ { " name " : " Verify Scheduling " , " exists " : [ { " name " : " t1 " , " sort ": " TimeSlot "} , { " name " : " t2 " , " sort ": " TimeSlot "} ] , " constraint " : " And ( scheduled_at ( task1 ) == t1 , scheduled_at ( task2 ) == t2 , t1 != t2 ) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 6. Graph Coloring Problem : Assign colors to nodes such that adjacent nodes have different colors.{ " sorts " : [ { " name " : " Node " , " type ": " DeclareSort "} , { " name " : " Color " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " color_of " , " domain ": [" Node "] , " range ": " Color "} , { " name " : " connected " , " domain ": [" Node " , " Node "] , " range ": " BoolSort "} ] , " constants " : { " nodes " : { " sort " : " Node " , " members " : [ " node1 " , " node2 " , " node3 " ] } , " colors " : { " sort " : " Color " , " members " : [ " red " , " green " , " blue " ] } } , " knowledge_base " : [ " connected ( node1 , node2 ) " , " connected ( node2 , node3 ) " , " connected ( node1 , node3 ) " ] , " rules " : [ { " name " : " Coloring Rule " , " forall " : [ { " name " : " n1 " , " sort ": " Node "} , { " name " : " n2 " , " sort ": " Node "} ] , " implies " : { " antecedent " : " connected ( n1 , n2 ) " , " consequent " : " color_of ( n1 ) != color_of ( n2 ) " } } ] , " verifications " : [ { " name " : " Verify Coloring " , " exists " : [ { " name " : " c1 " , " sort ": " Color "} , { " name " : " c2 " , " sort ": " Color "} , { " name " : " c3 " , " sort ": " Color "} ] , " constraint " : " And ( color_of ( node1 ) == c1 , color_of ( node2 ) == c2 , color_of ( node3 ) == c3) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 7. Health and Safety Scenario : Verify that all workers at heights above 6 feet are wearing safety harnesses.{ " sorts " : [ { " name " : " Person " , " type ": " DeclareSort "} , { " name " : " Equipment " , " type ": " DeclareSort "} , { " name " : " Location " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " Worker " , " domain ": [" Person "] , " range ": " BoolSort "} , { " name " : " At " , " domain ": [" Person " , " Location "] , " range ": " BoolSort "} , { " name " : " Wearing " , " domain ": [" Person " , " Equipment "] , " range ": " BoolSort "} , { " name " : " Height " , " domain ": [" Location "] , " range ": " IntSort "} ] , " constants " : { " persons " : { " sort " : " Person " , " members " : [ " worker1 " , " worker2 " ] } , " equipments " : { " sort " : " Equipment " , " members " : [ " safetyHarness " ] } , " locations " : { " sort " : " Location " , " members " : [ " groundLevel " , " highLevel " ] } } , " knowledge_base " : [ " Worker ( worker1 ) " , " Worker ( worker2 ) " , " At ( worker1 , groundLevel ) " , " At ( worker2 , highLevel ) " , " Height ( groundLevel ) == 0 " , " Height ( highLevel ) == 20 " , " Wearing ( worker1 , safetyHarness ) " ] , " rules " : [ { " name " : " Fall Protection Rule " , " forall " : [ { " name " : " p " , " sort ": " Person "} , { " name " : " l " , " sort ": " Location "} ] , " implies " : { " antecedent " : " And ( Worker ( p ) , At (p , l ) , Height ( l ) &gt; 6) " , " consequent " : " Wearing (p , safetyHarness ) " } } ] , " verifications " : [ { " name " : " Check Fall Protection " , " forall " : [ { " name " : " p " , " sort ": " Person "} , { " name " : " l " , " sort ": " Location "} ] , " implies " : { " antecedent " : " And ( Worker ( p ) , At (p , l ) , Height ( l ) &gt; 6) " , " consequent " : " Wearing (p , safetyHarness ) " } } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 8. Electrical Safety Scenario: Ensure workers using energized equipment above 250V are wearing insulated gloves.{ " sorts " : [ { " name " : " Person " , " type ": " DeclareSort "} , { " name " : " Equipment " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " Worker " , " domain ": [" Person "] , " range ": " BoolSort "} , { " name " : " Using " , " domain ": [" Person " , " Equipment "] , " range ": " BoolSort "} , { " name " : " IsEnergized " , " domain ": [" Equipment "] , " range ": " BoolSort "} , { " name " : " Voltage " , " domain ": [" Equipment "] , " range ": " IntSort "} , { " name " : " Wearing " , " domain ": [" Person " , " Equipment "] , " range ": " BoolSort "} ] , " constants " : { " persons " : { " sort " : " Person " , " members " : [ " worker1 " ] } , " equipments " : { " sort " : " Equipment " , " members " : [ " circuitBreaker " , " insulatedGloves " ] } } , " knowledge_base " : [ " Worker ( worker1 ) " , " Using ( worker1 , circuitBreaker ) " , " IsEnergized ( circuitBreaker ) " , " Voltage ( circuitBreaker ) == 480 " ] , " rules " : [ { " name " : " High Voltage Safety Rule " , " forall " : [ { " name " : " p " , " sort ": " Person "} , { " name " : " e " , " sort ": " Equipment "} ] , " implies " : { " antecedent " : " And ( Worker ( p ) , Using (p , e ) , IsEnergized ( e ) , Voltage ( e ) &gt; 250) " , " consequent " : " Wearing (p , insulatedGloves ) " } } ] , " verifications " : [ { " name " : " Verify Electrical Safety " , " forall " : [ { " name " : " p " , " sort ": " Person "} , { " name " : " e " , " sort ": " Equipment "} ] , " implies " : { " antecedent " : " And ( Worker ( p ) , Using (p , e ) , IsEnergized ( e ) , Voltage ( e ) &gt; 250) " , " consequent " : " Wearing (p , insulatedGloves ) " } } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 9. Chemical Handling Safety : Ensure workers handling corrosive chemicals are wearing gloves and goggles.{ " sorts " : [ { " name " : " Person " , " type ": " DeclareSort "} , { " name " : " Chemical " , " type ": " DeclareSort "} , { " name " : " Equipment " , " type ": " DeclareSort "} ] , " functions " : [ { " name " : " Worker " , " domain ": [" Person "] , " range ": " BoolSort "} , { " name " : " Handling " , " domain ": [" Person " , " Chemical "] , " range ": " BoolSort "} , { " name " : " IsCorrosive " , " domain ": [" Chemical "] , " range ": " BoolSort "} , { " name " : " Wearing " , " domain ": [" Person " , " Equipment "] , " range ": " BoolSort "} ] , " constants " : { " persons " : { " sort " : " Person " , " members " : [ " worker1 " ] } , " chemicals " : { " sort " : " Chemical " , " members " : [ " acid " ] } , " equipments " : { " sort " : " Equipment " , " members " : [ " gloves " , " goggles " ] } } , " knowledge_base " : [ " Worker ( worker1 ) " , " Handling ( worker1 , acid ) " , " IsCorrosive ( acid ) " ] , " rules " : [ { " name " : " Corrosive Chemical Handling Rule " , " forall " : [ { " name " : " p " , " sort ": " Person "} , { " name " : " c " , " sort ": " Chemical "} ] , " implies " : { " antecedent " : " And ( Worker ( p ) , Handling (p , c ) , IsCorrosive ( c ) ) " , " consequent " : " And ( Wearing (p , gloves ) , Wearing (p , goggles ) ) " } } ] , " verifications " : [ { " name " : " Verify Chemical Safety " , " forall " : [ { " name " : " p " , " sort ": " Person "} , { " name " : " c " , " sort ": " Chemical "} ] , " implies " : { " antecedent " : " And ( Worker ( p ) , Handling (p , c ) , IsCorrosive ( c ) ) " , " consequent " : " And ( Wearing (p , gloves ) , Wearing (p , goggles ) ) " } } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 10.Resource Allocation Optimization : Allocate tasks to workers while minimizing total cost.{" sorts " : [ { " name " : " Worker " , " type ": " DeclareSort "} , { " name " : " Task " , " type ": " DeclareSort "} , { " name " : " Cost " , " type ": " IntSort "} ] , " functions " : [ { " name " : " assigned_to " , " domain ": [" Task "] , " range ": " Worker "} , { " name " : " cost_of " , " domain ": [" Worker "] , " range ": " Cost "} ] , " constants " : { " workers " : { " sort " : " Worker " , " members " : [ " worker1 " , " worker2 " ] } , " tasks " : { " sort " : " Task " , " members " : [ " taskA " , " taskB " ] } } , " knowledge_base " : [ " cost_of ( worker1 ) == 50 " , " cost_of ( worker2 ) == 30 " ] , " rules " : [] , " verifications " : [] , " optimization " : { " constraints " : [ " assigned_to ( taskA ) != assigned_to ( taskB ) " ] , " objectives " : [ { " type " : " minimize " , " expression " : " cost_of ( assigned_to ( taskA ) ) + cost_of ( assigned_to ( taskB ) ) " } ] } , " actions " : [ " optimize " ] }Exploring the possibilities: Unsatisfiable Neurosymbolic ProgramsOur DSL designed to be very expressive, and future proof for additional scenarios.In this subsection we present some example problems that can be expressed and solved to be found UNSAT.1. Pigeonhole Principle: A classic unsatisfiable problem where more pigeons than holes cannot be assigned uniquely.{ " sorts " : [ { " name " : " Pigeon " , " type " : " EnumSort " , " values " : [ " p1 " , " p2 " , " p3 " , " p4 " ] } , { " name " : " Hole " , " type " : " EnumSort " , " values " : [ " h1 " , " h2 " , " h3 " ] } ] , " functions " : [ { " name " : " assigned_to " , " domain ": [" Pigeon "] , " range ": " Hole "} ] , " constants " : {} , " knowledge_base " : [] , " rules " : [] , " verifications " : [ { " name " : " Pigeonhole Verification " , " constraint " : " Distinct ( assigned_to ( p1 ) , assigned_to ( p2 ) , assigned_to ( p3 ) , assigned_to ( p4 ) ) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 2. Non-Three-Colorable Graph : A complete graph with four nodes cannot be colored with only three colors without adjacent nodes sharing the same color.{ " sorts " : [ { " name " : " Node " , " type " : " EnumSort " , " values " : [ " n1 " , " n2 " , " n3 " , " n4 " ] } , { " name " : " Color " , " type " : " EnumSort " , " values " : [ " red " , " green " , " blue " ] } ] , " functions " : [ { " name " : " color_of " , " domain ": [" Node "] , " range ": " Color "} , { " name " : " connected " , " domain ": [" Node " , " Node "] , " range ": " BoolSort "} ] , " constants " : {} , " knowledge_base " : [ " connected ( n1 , n2 ) " , " connected ( n1 , n3 ) " , " connected ( n1 , n4 ) " , " connected ( n2 , n3 ) " , " connected ( n2 , n4 ) " , " connected ( n3 , n4 ) " ] , " rules " : [ { " name " : " Coloring Rule " , " forall " : [ { " name " : " n1 " , " sort ": " Node "} , { " name " : " n2 " , " sort ": " Node "} ] , " implies " : { " antecedent " : " And ( connected ( n1 , n2 ) , n1 != n2 ) " , " consequent " : " color_of ( n1 ) != color_of ( n2 ) " } } ] , " verifications " : [ { " name " : " Verify 3 -Coloring " , " constraint " : " True " } ] , " actions " : [ " ver ify _co ndi tio ns " ] }Contradictory Mathematical Constraints{ " sorts " : [ { " name " : " Int " , " type ": " IntSort "} ] , " functions " : [] , " constants " : {} , " knowledge_base " : [] , " rules " : [] , " verifications " : [ { " name " : " Contradictory Constraints " , " exists " : [ { " name " : " x " , " sort ": " Int "} ] , " constraint " : " And ( x &gt; 0 , x &lt; 0) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] }4. An Unsatisfiable Boolean formula in conjunctive normal form (CNF).{" sorts " : [ { " name " : " Bool " , " type ": " BoolSort "} ] , " functions " : [] , " constants " : { " variables " : { " sort " : " Bool " , " members " : [ " A " , " B " ] } } , " knowledge_base " : [] , " rules " : [] , " verifications " : [ { " name " : " Unsatisfiable CNF " , " constraint " : " And (A , Not ( A ) ) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 5. Mutual Exclusivity: Defining two constants that cannot be equal, but also constrained to be equal.{ " sorts " : [ { " name " : " Element " , " type ": " DeclareSort "} ] , " functions " : [] , " constants " : { " elements " : { " sort " : " Element " , " members " : [ " e1 " , " e2 " ] } } , " knowledge_base " : [ " e1 != e2 " , " e1 == e2 " ] , " rules " : [] , " verifications " : [ { " name " : " Mutual Exclusivity Verification " , " constraint " : " True " } ] , " actions " : [ " ver ify _co ndi tio ns " ] }Inconsistent Equations{ " sorts " : [ { " name " : " Int " , " type ": " IntSort "} ] , " functions " : [] , " constants " : {} , " knowledge_base " : [] , " rules " : [] , " verifications " : [ { " name " : " Inconsistent Equations " , " exists " : [ { " name " : " x " , " sort ": " Int "} , { " name " : " y " , " sort ": " Int "} ] , " constraint " : " And ( x + y == 10 , x + y == 5) " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 7. Unsolvable Scheduling Conflict : Tasks that must occur at the same time and also at different times.{ " sorts " : [ { " name " : " Task " , " type ": " EnumSort " , " values ": [" task1 " , " task2 "]} ] , " functions " : [ { " name " : " scheduled_at " , " domain ": [" Task "] , " range ": " IntSort "} ] , " constants " : {} , " knowledge_base " : [ " scheduled_at ( task1 ) == scheduled_at ( task2 ) " , " scheduled_at ( task1 ) != scheduled_at ( task2 ) "] , " rules " : [] , " verifications " : [ { " name " : " Scheduling Conflict Verification " , " constraint " : " True " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 8. Invalid Parent-Child Relationship { " sorts " : [ { " name " : " Person " , " type ": " EnumSort " , " values ": [" bob "]} ] , " functions " : [ { " name " : " parent_of " , " domain ": [" Person "] , " range ": " Person "} ] , " constants " : {} , " knowledge_base " : [ " parent_of ( bob ) == bob " ] , " rules " : [ { " name " : " No Self Parenting Rule " , " forall " : [ { " name " : " p " , " sort ": " Person "} ] , " implies " : { " antecedent " : " True " , " consequent " : " parent_of ( p ) != p " } } ] , " verifications " : [ { " name " : " Self Parenting Verification " , " constraint " : " True " } ] , " actions " : [ " ver ify _co ndi tio ns " ] } 9. Impossible Optimization { " sorts " : [ { " name " : " Int " , " type ": " IntSort "} ] , " functions " : [] , " constants " : {} , " knowledge_base " : [] , " rules " : [] , " verifications " : [] , " optimization " : { " constraints " : [ " x &gt; 0 " , " x &lt; 0 " ] , " objectives " : [ { " type " : " minimize " , " expression " : " x " } ] } , " actions " : [ " optimize " ] }
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Interpretable neural-symbolic concept reasoning. Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Mateo Espinosa Zarlenga, Lucie Charlotte Magister, Alberto Tonda, Pietro Lió, Frederic Precioso, Mateja Jamnik, Giuseppe Marra, International Conference on Machine Learning. PMLR2023</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>A review: Knowledge reasoning over knowledge graph. Expert systems with applications. Xiaojun Chen, Shengbin Jia, Yang Xiang, 2020a141112948</p>
<p>Genome: generative neurosymbolic visual reasoning by growing and reusing modules. Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan, arXiv:2311.049012023arXiv preprint</p>
<p>Concept whitening for interpretable image recognition. Zhi Chen, Yijie Bei, Cynthia Rudin, Nature Machine Intelligence. 2122020b</p>
<p>A survey of the state of explainable ai for natural language processing. Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen, arXiv:2010.007112020arXiv preprint</p>
<p>Neural-symbolic learning systems. Artur S D'avila, Luís C Garcez, Dov M Lamb, Gabbay, 2009Springer</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Machine learning: Trends, perspectives, and prospects. I Michael, Tom M Jordan, Mitchell, Science. 34962452015</p>
<p>Rethinking knowledge graph propagation for zero-shot learning. Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, Eric P Xing, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, International conference on machine learning. PMLR2018</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Scallop: A language for neurosymbolic programming. Ziyang Li, Jiani Huang, Mayur Naik, Proceedings of the ACM on Programming Languages. 72023</p>
<p>Deepproblog: Neural probabilistic logic programming. Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De, Raedt , Advances in neural information processing systems. 201831</p>
<p>Bears make neuro-symbolic models aware of their reasoning shortcuts. Emanuele Marconato, Samuele Bortolotti, Emile Van Krieken, Antonio Vergari, Andrea Passerini, Stefano Teso, arXiv:2402.122402024arXiv preprint</p>
<p>Gary Marcus, arXiv:2002.06177The next decade in ai: four steps towards robust artificial intelligence. 2020arXiv preprint</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Cynthia Rudin, Nature machine intelligence. 152019</p>
<p>A benchmark for learning to translate a new language from one grammar book. Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke Melas-Kyriazi, arXiv:2309.165752023arXiv preprint</p>
<p>Knowledge-based artificial neural networks. G Geoffrey, Jude W Towell, Shavlik, Artificial intelligence. 701-21994</p>
<p>Refinement of approximate domain theories by knowledge-based neural networks. Geoffrey G Towell, Jude W Shavlik, Michiel, Noordewier, Proceedings of the eighth National conference on Artificial intelligence. the eighth National conference on Artificial intelligence19902</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao, arXiv:2310.114412023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Understanding deep learning (still) requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Communications of the ACM. 6432021</p>
<p>Transfer learning for low-resource neural machine translation. Barret Zoph, Deniz Yuret, Jonathan May, Kevin Knight, arXiv:1604.022012016arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2494 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2494</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2494</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-273798715</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.00137v1.pdf" target="_blank">Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration</a></p>
                <p><strong>Paper Abstract:</strong> In missions constrained by finite resources, efficient data collection is critical. Informative path planning, driven by automated decision-making, optimizes exploration by reducing the costs associated with accurate characterization of a target in an environment. Previous implementations of active learning did not consider the action cost for regression problems or only considered the action cost for classification problems. This paper analyzes an AL algorithm for Gaussian Process regression while incorporating action cost. The algorithm's performance is compared on various regression problems to include terrain mapping on diverse simulated surfaces along metrics of root mean square error, samples and distance until convergence, and model variance upon convergence. The cost-dependent acquisition policy doesn't organically optimize information gain over distance. Instead, the traditional uncertainty metric with a distance constraint best minimizes root-mean-square error over trajectory distance. This studys impact is to provide insight into incorporating action cost with AL methods to optimize exploration under realistic mission constraints.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2494.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2494.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dist-Constrained-Var</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distance-Constrained Variance Query Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-learning acquisition policy that selects the next sampling location by maximizing model predictive variance subject to a movement-horizon constraint (radius r_con), trading information gain against action cost (distance traveled).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Distance-Constrained Variance Policy (GP AL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A query policy used inside a Gaussian Process (GP) active-learning loop. At each iteration the policy evaluates candidate unlabeled points and selects the point with maximum predicted variance σ_pred^2(x) but only among points within a spatial movement horizon r_con from the current agent position (i.e., argmax_{||x_j - x_i|| ≤ r_con} σ_pred^2(x_j)). The paper tests multiple movement-horizon values (1Δ,2Δ,3Δ,5Δ,7Δ,10Δ), integrates the policy with a GP regression learner (RBF kernel, hyperparameters optimized by marginal likelihood), and measures distance traveled, samples until convergence, and RMSE upon convergence. This policy operationalizes an explicit action-cost (distance) constraint so the agent preferentially samples informative points that are inexpensive to reach.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Robotic spatial exploration / environmental mapping (planetary/terrestrial surface mapping; e.g., lunar hydroxyl mapping, terrain elevation modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate sampling actions by maximizing model predictive uncertainty (σ^2) but only over candidate points within a specified movement horizon from current location; different horizons trade off local inexpensive sampling vs. broader exploration. Resources (movement cost) are implicitly allocated by restricting choice set to points that incur bounded travel cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly quantified as CPU or wall-clock cost; primary cost proxy is physical action cost measured as 'distance traveled until convergence' (d_c). Samples until convergence (i_c) is also reported as an experiment-acquisition cost metric. Computational training cost (GP training iterations, Adam optimizer steps) is mentioned but not reported as a numeric cost metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model predictive variance σ_pred^2(x) (uncertainty sampling); acquisition uses variance as surrogate for information gain (uncertainty reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration is encouraged by selecting high-variance points; exploitation/cost-awareness is enforced by the movement horizon which constrains selection to nearby points. By varying r_con the system trades between local exploitation (small r_con) and broader exploration (large r_con).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit spatial diversity control via movement-horizon constraints (restricting or enabling broader spatial reach) rather than an explicit diversity objective across hypothesis-space; no explicit diversity-promoting objective (e.g., determinantal point processes) is used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Action-cost (physical travel distance) and sample budget via stopping conditions (max samples or convergence criterion); no monetary budget or explicit computational-resource budget reported.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled by constraining the admissible candidate set via a movement horizon and by using a 2% settling-time based RMSE convergence stopping rule; the movement horizon is tuned to minimize distance while achieving acceptable RMSE.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include RMSE upon convergence e_c (absolute error), normalized RMSE e_n, samples until convergence i_c (scaled), distance until convergence d_c (scaled by grid length), and composite metrics distance-scaled NRMSE e_dc and sample-scaled NRMSE e_ic. Example numeric results (means over 10 trials): Townsend surface for 2Δ: e_c=0.06573, i_c=0.91, d_c=2.69167, e_dc=0.17692, e_ic=0.05960. Conventional vs constrained examples in paper tables illustrate orders-of-magnitude differences in d_c while preserving e_c.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against: (1) conventional uncertainty sampling (unconstrained, choose global max variance), and (2) distance-normalized variance (global, variance/distance score).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Distance-constrained policies achieved comparable RMSE to conventional AL while dramatically reducing distance traveled. Example: on the Townsend surface conventional AL: d_c ≈ 93.4167 with e_c ≈ 0.08521, whereas 2Δ constrained: d_c ≈ 2.69167 with e_c ≈ 0.06573. On Parabola surface: conventional d_c ≈ 13.3380 vs 2Δ d_c ≈ 0.61650 (with comparable or slightly larger e_c depending on horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Order-of-magnitude reduction in distance traveled until convergence compared to conventional uncertainty sampling (examples above show reductions from ~93 to ~2.7 on Townsend; from ~13.3 to ~0.62 on Parabola) while maintaining comparable or better RMSE; sample counts sometimes similar or slightly higher depending on horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper analyzes tradeoffs across movement horizon values: very small horizons (1Δ) lead to poor spatial coverage and higher error; very large horizons approximate unconstrained AL and increase travel; moderate horizons (empirically 2Δ or 7Δ depending on metric/surface) best balance distance, samples, and RMSE. The paper shows different horizons optimize different multi-objective metrics (e.g., 2Δ often best balance of error and distance; 7Δ reduces samples in many cases; 10Δ often achieves lowest RMSE but at higher distance).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: incorporate action cost into acquisition decisions via a movement-horizon constraint; tune horizon to environment complexity—moderate horizons (e.g., 2Δ or 3Δ) provide strong trade-offs for minimizing distance and reaching low RMSE, while larger horizons (7Δ–10Δ) can reduce samples or RMSE at the expense of travel; distance-normalized variants do not consistently minimize RMSE per distance as well as appropriately chosen distance-constrained policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2494.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2494.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dist-Norm-Var</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distance-Normalized Variance Query Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-learning acquisition scoring function that divides predicted variance by the distance from current position (σ^2 / ||x - x_i||), thereby penalizing informative but costly (far) samples and favoring high-variance nearby points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Distance-Normalized Variance Policy (GP AL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A global (unconstrained movement horizon) acquisition function that computes a per-candidate score u(x_j) = σ_pred^2(x_j) / ||x_j - x_i|| and selects the maximizer. This explicitly trades predicted uncertainty against travel cost by normalizing information by distance to the agent's current location. The implementation is integrated with a GP regression model and compared experimentally to distance-constrained and conventional policies.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Robotic spatial exploration / environmental mapping (planetary/terrestrial).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Score candidate experiments by predicted model variance normalized by travel distance; allocate next action to candidate with maximum variance-per-distance, thereby approximating information-per-cost maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly measured as compute resources; action cost is measured as travel distance d_c; samples until convergence i_c also reported. Computational training cost not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predicted variance σ_pred^2(x) used as surrogate for information gain; the acquisition uses σ^2 divided by distance as an information-per-cost metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balances exploration and exploitation by weighing uncertainty (exploration) against immediate travel cost (exploitation of nearby informative points) via division by distance; no explicit horizon means the policy can still choose far points if variance is high enough to overcome distance penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Indirect — by favoring high variance per unit distance, the policy tends to pick nearby informative samples but may still reach diverse regions if their per-distance information is high; no explicit diversity-promoting routine.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Action-cost (distance) and sample budget via convergence condition.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled by normalizing acquisition value by distance; stopping via 2% settling-time RMSE criterion and per-surface sample caps.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same set as other policies (e_c, e_n, i_c, d_c, e_dc, e_ic). Example: Parabola surface Norm (distance-normalized) mean e_c=0.00967, i_c=0.57, d_c=2.78645, e_dc=0.02693, e_ic=0.00546 (from Table IV). Results show Norm often reduces distance relative to conventional but is not always the most distance-efficient compared to small movement-horizon constrained policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against conventional uncertainty sampling and distance-constrained variance policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Distance-normalized policy generally reduces travel relative to conventional AL but is often less distance-efficient than small movement-horizon distance-constrained policies; RMSE performance is comparable to conventional in most cases but did not consistently minimize RMSE per distance as effectively as some constrained horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Moderate reductions in travel relative to conventional but often inferior to the best distance-constrained horizons; examples show Norm d_c values lower than Conv but higher than 2Δ/3Δ in many settings (see Table IV–VI).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Distance-normalized policy trades off travel and information in a continuous per-candidate score; experiments indicate it reduces travel compared to conventional AL but sometimes yields jagged (less distance-efficient) paths and does not always yield the best RMSE-per-distance tradeoff compared to constrained-horizon methods.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Normalizing information by distance is a viable and simple cost-aware strategy, but using a movement-horizon constraint can give better control and empirically superior distance-efficiency for many environments; practitioners should compare normalized-score approaches and constrained selection based on mission dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2494.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2494.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conv-Unc-Samp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conventional Uncertainty Sampling (Global Max Variance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard GP active-learning acquisition policy that selects the unlabeled point with the maximum predicted variance across the entire domain (no action-cost consideration).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conventional Uncertainty Sampling (GP AL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Acquisition function selects x_{i+1} = argmax_{x ∈ D} σ_pred^2(x) without regard to travel cost. Implemented with a GP regression model; used as the baseline in experiments to compare cost-aware alternatives. It often achieves low sample counts and competitive RMSE but can produce very large travel distances and inefficient trajectories in physical exploration tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for spatial regression tasks in robotic exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate each next sampling action purely to the globally most uncertain point (max σ^2), ignoring physical/action costs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not reported; primary experimental costs tracked are samples until convergence and travel distance (d_c), which for this policy tend to be much larger than cost-aware policies.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predicted variance σ_pred^2(x) (uncertainty reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration via global uncertainty maximization; no explicit exploitation of nearby low-cost samples.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond selecting global uncertainty maxima which can cause wide spatial jumps and revisits.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Not cost-aware; stopping based on RMSE convergence and sample caps.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Does not handle physical movement budgets; termination still uses RMSE-based convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same reported metrics. Example numeric values show large travel: Townsend Conv d_c ≈ 93.4167, e_c ≈ 0.08521; on Parabola Conv d_c ≈ 13.3380, e_c ≈ 0.00909. Conventional often attains low samples but with much higher travel distances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Serves as the primary baseline for comparison within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Baseline itself; compared methods generally match or beat its RMSE while substantially reducing travel distance (often by an order of magnitude).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>N/A as baseline; used to quantify gains of cost-aware policies (which achieved ≈10x reductions in travel for similar RMSEs in examples).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Highlights the problem motivating the paper: maximizing information alone without cost leads to inefficient mission trajectories; cost-aware modifications are necessary for resource-constrained exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper concludes that purely information-maximizing allocation is suboptimal under action-cost constraints; cost-aware selection (horizon or normalization) yields better practical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2494.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2494.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-AL-ActionCost</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Active Learning Algorithm with Action Cost</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's integrated AL system: a GP regression learner coupled with cost-aware acquisition policies (distance-constrained and distance-normalized) that incorporate movement/action costs into sample selection to optimize exploration under resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP Active Learning with Action Cost</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An implemented AL pipeline using GPyTorch GPs (RBF kernel, lengthscale optimized by gradient descent, Adam optimizer) that iteratively: (1) trains GP on gathered samples, (2) computes predictive mean and variance over unlabeled grid, (3) selects next sample via configured acquisition policy (conventional, distance-normalized, or distance-constrained), (4) moves agent to sample location and records ground truth, and (5) repeats until convergence. The system explicitly records distance traveled and number of samples and compares multi-objective metrics (NRMSE, samples, distance, composite scaled metrics). Code available at the referenced repository.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Autonomous robotic exploration, informative path planning, spatial regression for planetary/terrestrial mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate measurement actions by acquisition policies that combine predictive uncertainty with distance-based cost considerations (either via normalization or admissible region constraint). Stopping determined by RMSE settling criterion (2% band) and per-surface max sample cap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primary experimental cost proxies: distance traveled (d_c) and samples until convergence (i_c). GP training uses iterative optimization (100 gradient iterations for kernel lengthscale), but CPU/wall time or FLOPs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predictive variance σ_pred^2 used as acquisition-driving information measure (uncertainty reduction). Composite metrics e_dc and e_ic combine NRMSE with distance or samples to summarize information-per-cost performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Mechanism differs by acquisition policy: conventional uses pure uncertainty (exploration), distance-normalized trades σ^2 against distance (continuous trade-off), distance-constrained enforces a hard cost budget per step (constrained-exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit hypothesis-diversity objective; spatial diversity emerges from movement horizon choice and the GP predictive variance surface.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Action-cost (travel distance) and sample-limit via per-surface caps; no explicit monetary or computational resource budget optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget handled implicitly by choosing acquisitions within movement horizon or via normalization; overall mission efficiency evaluated by composite metrics that scale error by distance or samples.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RMSE upon convergence (e_c), normalized RMSE (e_n), samples until convergence (i_c), distance until convergence (d_c), distance-scaled NRMSE (e_dc), sample-scaled NRMSE (e_ic). Representative numbers for best-performing settings are provided in Tables IV–VI of the paper (means over 10 trials).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Conventional uncertainty sampling and distance-normalized variance policies within same GP AL framework.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cost-aware variants matched or improved RMSE while reducing travel distance drastically vs conventional; e.g., Townsend: Conv d_c ≈ 93.4 → 2Δ d_c ≈ 2.69 while attaining similar or lower e_c.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical gains include up to an order-of-magnitude reduction in travel distance to convergence for comparable RMSE; improvements in combined metrics (e_dc, e_ic) shown in tables and figures.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper presents multi-objective comparisons and visualizations: small movement horizons reduce distance but can increase error and required samples; intermediate horizons (2Δ, 3Δ, 7Δ) often best trade-offs; distance-normalized policy reduces travel compared to conventional but may produce jagged paths and be less optimal in RMSE-per-distance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Incorporate action cost into acquisition strategies; tune movement horizon to the environment and mission objective—moderate horizons often provide best balance; do not rely solely on global uncertainty maximization in resource-limited exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2494.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2494.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KrauseNonmyopic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonmyopic Active Learning of Gaussian Processes (Krause & Guestrin)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced nonmyopic GP active-learning approach that models exploration-exploitation trade-offs and optimizes multi-step information gain, cited as related work but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nonmyopic GP Active Learning (literature reference)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A framework that plans multi-step observation sequences by considering future information gains (nonmyopic) and formalizes trade-offs between exploration and exploitation in GP-based sensor placement/active learning. The present paper cites this work as relevant background on exploration-exploitation trade-offs but does not implement or evaluate the nonmyopic algorithm experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sensor placement, spatial active learning, robotic exploration (general GP active learning).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Nonmyopic planning of sequences that maximizes expected cumulative information (or related utility) over a planning horizon; accounts for information gain across multiple steps rather than greedy single-step selection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information / expected information gain over sequences (as in original Krause & Guestrin formulations).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit multi-step planning balances exploration and exploitation by optimizing cumulative expected information.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not described in detail in this paper; original work often encourages spatial diversity through information-theoretic objectives like mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Implicit in planning horizon and sensor/motion constraints in referenced work; not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled via nonmyopic planning and utility maximization under action constraints in the original referenced literature.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Referenced as state-of-the-art in GP AL; not compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Mentioned as background motivating consideration of exploration-exploitation trade-offs; no new tradeoff analysis applied to that method here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper cites the need for integrating action costs into decision-making and positions its distance-aware policies as complementary to nonmyopic frameworks that consider information over multiple steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach <em>(Rating: 2)</em></li>
                <li>Active Learning with Real Annotation Costs <em>(Rating: 2)</em></li>
                <li>Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes <em>(Rating: 2)</em></li>
                <li>Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration <em>(Rating: 2)</em></li>
                <li>Entropy-based Active Learning of Graph Neural Network Surrogate Models for Materials Properties <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2494",
    "paper_id": "paper-273798715",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Dist-Constrained-Var",
            "name_full": "Distance-Constrained Variance Query Policy",
            "brief_description": "An active-learning acquisition policy that selects the next sampling location by maximizing model predictive variance subject to a movement-horizon constraint (radius r_con), trading information gain against action cost (distance traveled).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Distance-Constrained Variance Policy (GP AL)",
            "system_description": "A query policy used inside a Gaussian Process (GP) active-learning loop. At each iteration the policy evaluates candidate unlabeled points and selects the point with maximum predicted variance σ_pred^2(x) but only among points within a spatial movement horizon r_con from the current agent position (i.e., argmax_{||x_j - x_i|| ≤ r_con} σ_pred^2(x_j)). The paper tests multiple movement-horizon values (1Δ,2Δ,3Δ,5Δ,7Δ,10Δ), integrates the policy with a GP regression learner (RBF kernel, hyperparameters optimized by marginal likelihood), and measures distance traveled, samples until convergence, and RMSE upon convergence. This policy operationalizes an explicit action-cost (distance) constraint so the agent preferentially samples informative points that are inexpensive to reach.",
            "application_domain": "Robotic spatial exploration / environmental mapping (planetary/terrestrial surface mapping; e.g., lunar hydroxyl mapping, terrain elevation modeling).",
            "resource_allocation_strategy": "Allocate sampling actions by maximizing model predictive uncertainty (σ^2) but only over candidate points within a specified movement horizon from current location; different horizons trade off local inexpensive sampling vs. broader exploration. Resources (movement cost) are implicitly allocated by restricting choice set to points that incur bounded travel cost.",
            "computational_cost_metric": "Not explicitly quantified as CPU or wall-clock cost; primary cost proxy is physical action cost measured as 'distance traveled until convergence' (d_c). Samples until convergence (i_c) is also reported as an experiment-acquisition cost metric. Computational training cost (GP training iterations, Adam optimizer steps) is mentioned but not reported as a numeric cost metric.",
            "information_gain_metric": "Model predictive variance σ_pred^2(x) (uncertainty sampling); acquisition uses variance as surrogate for information gain (uncertainty reduction).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration is encouraged by selecting high-variance points; exploitation/cost-awareness is enforced by the movement horizon which constrains selection to nearby points. By varying r_con the system trades between local exploitation (small r_con) and broader exploration (large r_con).",
            "diversity_mechanism": "Implicit spatial diversity control via movement-horizon constraints (restricting or enabling broader spatial reach) rather than an explicit diversity objective across hypothesis-space; no explicit diversity-promoting objective (e.g., determinantal point processes) is used.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Action-cost (physical travel distance) and sample budget via stopping conditions (max samples or convergence criterion); no monetary budget or explicit computational-resource budget reported.",
            "budget_constraint_handling": "Handled by constraining the admissible candidate set via a movement horizon and by using a 2% settling-time based RMSE convergence stopping rule; the movement horizon is tuned to minimize distance while achieving acceptable RMSE.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Reported metrics include RMSE upon convergence e_c (absolute error), normalized RMSE e_n, samples until convergence i_c (scaled), distance until convergence d_c (scaled by grid length), and composite metrics distance-scaled NRMSE e_dc and sample-scaled NRMSE e_ic. Example numeric results (means over 10 trials): Townsend surface for 2Δ: e_c=0.06573, i_c=0.91, d_c=2.69167, e_dc=0.17692, e_ic=0.05960. Conventional vs constrained examples in paper tables illustrate orders-of-magnitude differences in d_c while preserving e_c.",
            "comparison_baseline": "Compared against: (1) conventional uncertainty sampling (unconstrained, choose global max variance), and (2) distance-normalized variance (global, variance/distance score).",
            "performance_vs_baseline": "Distance-constrained policies achieved comparable RMSE to conventional AL while dramatically reducing distance traveled. Example: on the Townsend surface conventional AL: d_c ≈ 93.4167 with e_c ≈ 0.08521, whereas 2Δ constrained: d_c ≈ 2.69167 with e_c ≈ 0.06573. On Parabola surface: conventional d_c ≈ 13.3380 vs 2Δ d_c ≈ 0.61650 (with comparable or slightly larger e_c depending on horizon).",
            "efficiency_gain": "Order-of-magnitude reduction in distance traveled until convergence compared to conventional uncertainty sampling (examples above show reductions from ~93 to ~2.7 on Townsend; from ~13.3 to ~0.62 on Parabola) while maintaining comparable or better RMSE; sample counts sometimes similar or slightly higher depending on horizon.",
            "tradeoff_analysis": "Paper analyzes tradeoffs across movement horizon values: very small horizons (1Δ) lead to poor spatial coverage and higher error; very large horizons approximate unconstrained AL and increase travel; moderate horizons (empirically 2Δ or 7Δ depending on metric/surface) best balance distance, samples, and RMSE. The paper shows different horizons optimize different multi-objective metrics (e.g., 2Δ often best balance of error and distance; 7Δ reduces samples in many cases; 10Δ often achieves lowest RMSE but at higher distance).",
            "optimal_allocation_findings": "Recommendation: incorporate action cost into acquisition decisions via a movement-horizon constraint; tune horizon to environment complexity—moderate horizons (e.g., 2Δ or 3Δ) provide strong trade-offs for minimizing distance and reaching low RMSE, while larger horizons (7Δ–10Δ) can reduce samples or RMSE at the expense of travel; distance-normalized variants do not consistently minimize RMSE per distance as well as appropriately chosen distance-constrained policies.",
            "uuid": "e2494.0",
            "source_info": {
                "paper_title": "Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Dist-Norm-Var",
            "name_full": "Distance-Normalized Variance Query Policy",
            "brief_description": "An active-learning acquisition scoring function that divides predicted variance by the distance from current position (σ^2 / ||x - x_i||), thereby penalizing informative but costly (far) samples and favoring high-variance nearby points.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Distance-Normalized Variance Policy (GP AL)",
            "system_description": "A global (unconstrained movement horizon) acquisition function that computes a per-candidate score u(x_j) = σ_pred^2(x_j) / ||x_j - x_i|| and selects the maximizer. This explicitly trades predicted uncertainty against travel cost by normalizing information by distance to the agent's current location. The implementation is integrated with a GP regression model and compared experimentally to distance-constrained and conventional policies.",
            "application_domain": "Robotic spatial exploration / environmental mapping (planetary/terrestrial).",
            "resource_allocation_strategy": "Score candidate experiments by predicted model variance normalized by travel distance; allocate next action to candidate with maximum variance-per-distance, thereby approximating information-per-cost maximization.",
            "computational_cost_metric": "Not explicitly measured as compute resources; action cost is measured as travel distance d_c; samples until convergence i_c also reported. Computational training cost not numerically reported.",
            "information_gain_metric": "Predicted variance σ_pred^2(x) used as surrogate for information gain; the acquisition uses σ^2 divided by distance as an information-per-cost metric.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balances exploration and exploitation by weighing uncertainty (exploration) against immediate travel cost (exploitation of nearby informative points) via division by distance; no explicit horizon means the policy can still choose far points if variance is high enough to overcome distance penalty.",
            "diversity_mechanism": "Indirect — by favoring high variance per unit distance, the policy tends to pick nearby informative samples but may still reach diverse regions if their per-distance information is high; no explicit diversity-promoting routine.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Action-cost (distance) and sample budget via convergence condition.",
            "budget_constraint_handling": "Handled by normalizing acquisition value by distance; stopping via 2% settling-time RMSE criterion and per-surface sample caps.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Same set as other policies (e_c, e_n, i_c, d_c, e_dc, e_ic). Example: Parabola surface Norm (distance-normalized) mean e_c=0.00967, i_c=0.57, d_c=2.78645, e_dc=0.02693, e_ic=0.00546 (from Table IV). Results show Norm often reduces distance relative to conventional but is not always the most distance-efficient compared to small movement-horizon constrained policies.",
            "comparison_baseline": "Compared against conventional uncertainty sampling and distance-constrained variance policies.",
            "performance_vs_baseline": "Distance-normalized policy generally reduces travel relative to conventional AL but is often less distance-efficient than small movement-horizon distance-constrained policies; RMSE performance is comparable to conventional in most cases but did not consistently minimize RMSE per distance as effectively as some constrained horizons.",
            "efficiency_gain": "Moderate reductions in travel relative to conventional but often inferior to the best distance-constrained horizons; examples show Norm d_c values lower than Conv but higher than 2Δ/3Δ in many settings (see Table IV–VI).",
            "tradeoff_analysis": "Distance-normalized policy trades off travel and information in a continuous per-candidate score; experiments indicate it reduces travel compared to conventional AL but sometimes yields jagged (less distance-efficient) paths and does not always yield the best RMSE-per-distance tradeoff compared to constrained-horizon methods.",
            "optimal_allocation_findings": "Normalizing information by distance is a viable and simple cost-aware strategy, but using a movement-horizon constraint can give better control and empirically superior distance-efficiency for many environments; practitioners should compare normalized-score approaches and constrained selection based on mission dynamics.",
            "uuid": "e2494.1",
            "source_info": {
                "paper_title": "Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Conv-Unc-Samp",
            "name_full": "Conventional Uncertainty Sampling (Global Max Variance)",
            "brief_description": "A standard GP active-learning acquisition policy that selects the unlabeled point with the maximum predicted variance across the entire domain (no action-cost consideration).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Conventional Uncertainty Sampling (GP AL)",
            "system_description": "Acquisition function selects x_{i+1} = argmax_{x ∈ D} σ_pred^2(x) without regard to travel cost. Implemented with a GP regression model; used as the baseline in experiments to compare cost-aware alternatives. It often achieves low sample counts and competitive RMSE but can produce very large travel distances and inefficient trajectories in physical exploration tasks.",
            "application_domain": "Active learning for spatial regression tasks in robotic exploration.",
            "resource_allocation_strategy": "Allocate each next sampling action purely to the globally most uncertain point (max σ^2), ignoring physical/action costs.",
            "computational_cost_metric": "Not reported; primary experimental costs tracked are samples until convergence and travel distance (d_c), which for this policy tend to be much larger than cost-aware policies.",
            "information_gain_metric": "Predicted variance σ_pred^2(x) (uncertainty reduction).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration via global uncertainty maximization; no explicit exploitation of nearby low-cost samples.",
            "diversity_mechanism": "No explicit diversity mechanism beyond selecting global uncertainty maxima which can cause wide spatial jumps and revisits.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Not cost-aware; stopping based on RMSE convergence and sample caps.",
            "budget_constraint_handling": "Does not handle physical movement budgets; termination still uses RMSE-based convergence.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Same reported metrics. Example numeric values show large travel: Townsend Conv d_c ≈ 93.4167, e_c ≈ 0.08521; on Parabola Conv d_c ≈ 13.3380, e_c ≈ 0.00909. Conventional often attains low samples but with much higher travel distances.",
            "comparison_baseline": "Serves as the primary baseline for comparison within this paper.",
            "performance_vs_baseline": "Baseline itself; compared methods generally match or beat its RMSE while substantially reducing travel distance (often by an order of magnitude).",
            "efficiency_gain": "N/A as baseline; used to quantify gains of cost-aware policies (which achieved ≈10x reductions in travel for similar RMSEs in examples).",
            "tradeoff_analysis": "Highlights the problem motivating the paper: maximizing information alone without cost leads to inefficient mission trajectories; cost-aware modifications are necessary for resource-constrained exploration.",
            "optimal_allocation_findings": "Paper concludes that purely information-maximizing allocation is suboptimal under action-cost constraints; cost-aware selection (horizon or normalization) yields better practical performance.",
            "uuid": "e2494.2",
            "source_info": {
                "paper_title": "Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GP-AL-ActionCost",
            "name_full": "Gaussian Process Active Learning Algorithm with Action Cost",
            "brief_description": "The paper's integrated AL system: a GP regression learner coupled with cost-aware acquisition policies (distance-constrained and distance-normalized) that incorporate movement/action costs into sample selection to optimize exploration under resource constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GP Active Learning with Action Cost",
            "system_description": "An implemented AL pipeline using GPyTorch GPs (RBF kernel, lengthscale optimized by gradient descent, Adam optimizer) that iteratively: (1) trains GP on gathered samples, (2) computes predictive mean and variance over unlabeled grid, (3) selects next sample via configured acquisition policy (conventional, distance-normalized, or distance-constrained), (4) moves agent to sample location and records ground truth, and (5) repeats until convergence. The system explicitly records distance traveled and number of samples and compares multi-objective metrics (NRMSE, samples, distance, composite scaled metrics). Code available at the referenced repository.",
            "application_domain": "Autonomous robotic exploration, informative path planning, spatial regression for planetary/terrestrial mapping.",
            "resource_allocation_strategy": "Allocate measurement actions by acquisition policies that combine predictive uncertainty with distance-based cost considerations (either via normalization or admissible region constraint). Stopping determined by RMSE settling criterion (2% band) and per-surface max sample cap.",
            "computational_cost_metric": "Primary experimental cost proxies: distance traveled (d_c) and samples until convergence (i_c). GP training uses iterative optimization (100 gradient iterations for kernel lengthscale), but CPU/wall time or FLOPs not reported.",
            "information_gain_metric": "Predictive variance σ_pred^2 used as acquisition-driving information measure (uncertainty reduction). Composite metrics e_dc and e_ic combine NRMSE with distance or samples to summarize information-per-cost performance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Mechanism differs by acquisition policy: conventional uses pure uncertainty (exploration), distance-normalized trades σ^2 against distance (continuous trade-off), distance-constrained enforces a hard cost budget per step (constrained-exploration).",
            "diversity_mechanism": "No explicit hypothesis-diversity objective; spatial diversity emerges from movement horizon choice and the GP predictive variance surface.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Action-cost (travel distance) and sample-limit via per-surface caps; no explicit monetary or computational resource budget optimization.",
            "budget_constraint_handling": "Budget handled implicitly by choosing acquisitions within movement horizon or via normalization; overall mission efficiency evaluated by composite metrics that scale error by distance or samples.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "RMSE upon convergence (e_c), normalized RMSE (e_n), samples until convergence (i_c), distance until convergence (d_c), distance-scaled NRMSE (e_dc), sample-scaled NRMSE (e_ic). Representative numbers for best-performing settings are provided in Tables IV–VI of the paper (means over 10 trials).",
            "comparison_baseline": "Conventional uncertainty sampling and distance-normalized variance policies within same GP AL framework.",
            "performance_vs_baseline": "Cost-aware variants matched or improved RMSE while reducing travel distance drastically vs conventional; e.g., Townsend: Conv d_c ≈ 93.4 → 2Δ d_c ≈ 2.69 while attaining similar or lower e_c.",
            "efficiency_gain": "Empirical gains include up to an order-of-magnitude reduction in travel distance to convergence for comparable RMSE; improvements in combined metrics (e_dc, e_ic) shown in tables and figures.",
            "tradeoff_analysis": "Paper presents multi-objective comparisons and visualizations: small movement horizons reduce distance but can increase error and required samples; intermediate horizons (2Δ, 3Δ, 7Δ) often best trade-offs; distance-normalized policy reduces travel compared to conventional but may produce jagged paths and be less optimal in RMSE-per-distance.",
            "optimal_allocation_findings": "Incorporate action cost into acquisition strategies; tune movement horizon to the environment and mission objective—moderate horizons often provide best balance; do not rely solely on global uncertainty maximization in resource-limited exploration.",
            "uuid": "e2494.3",
            "source_info": {
                "paper_title": "Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "KrauseNonmyopic",
            "name_full": "Nonmyopic Active Learning of Gaussian Processes (Krause & Guestrin)",
            "brief_description": "A referenced nonmyopic GP active-learning approach that models exploration-exploitation trade-offs and optimizes multi-step information gain, cited as related work but not used in experiments.",
            "citation_title": "Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach",
            "mention_or_use": "mention",
            "system_name": "Nonmyopic GP Active Learning (literature reference)",
            "system_description": "A framework that plans multi-step observation sequences by considering future information gains (nonmyopic) and formalizes trade-offs between exploration and exploitation in GP-based sensor placement/active learning. The present paper cites this work as relevant background on exploration-exploitation trade-offs but does not implement or evaluate the nonmyopic algorithm experimentally.",
            "application_domain": "Sensor placement, spatial active learning, robotic exploration (general GP active learning).",
            "resource_allocation_strategy": "Nonmyopic planning of sequences that maximizes expected cumulative information (or related utility) over a planning horizon; accounts for information gain across multiple steps rather than greedy single-step selection.",
            "computational_cost_metric": null,
            "information_gain_metric": "Mutual information / expected information gain over sequences (as in original Krause & Guestrin formulations).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit multi-step planning balances exploration and exploitation by optimizing cumulative expected information.",
            "diversity_mechanism": "Not described in detail in this paper; original work often encourages spatial diversity through information-theoretic objectives like mutual information.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Implicit in planning horizon and sensor/motion constraints in referenced work; not evaluated here.",
            "budget_constraint_handling": "Handled via nonmyopic planning and utility maximization under action constraints in the original referenced literature.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": "Referenced as state-of-the-art in GP AL; not compared experimentally in this paper.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Mentioned as background motivating consideration of exploration-exploitation trade-offs; no new tradeoff analysis applied to that method here.",
            "optimal_allocation_findings": "Paper cites the need for integrating action costs into decision-making and positions its distance-aware policies as complementary to nonmyopic frameworks that consider information over multiple steps.",
            "uuid": "e2494.4",
            "source_info": {
                "paper_title": "Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach",
            "rating": 2,
            "sanitized_title": "nonmyopic_active_learning_of_gaussian_processes_an_explorationexploitation_approach"
        },
        {
            "paper_title": "Active Learning with Real Annotation Costs",
            "rating": 2,
            "sanitized_title": "active_learning_with_real_annotation_costs"
        },
        {
            "paper_title": "Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes",
            "rating": 2,
            "sanitized_title": "informative_path_planning_to_explore_and_map_unknown_planetary_surfaces_with_gaussian_processes"
        },
        {
            "paper_title": "Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration",
            "rating": 2,
            "sanitized_title": "comparing_active_learning_performance_driven_by_gaussian_processes_or_bayesian_neural_networks_for_constrained_trajectory_exploration"
        },
        {
            "paper_title": "Entropy-based Active Learning of Graph Neural Network Surrogate Models for Materials Properties",
            "rating": 1,
            "sanitized_title": "entropybased_active_learning_of_graph_neural_network_surrogate_models_for_materials_properties"
        }
    ],
    "cost": 0.015473499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration*</p>
<p>Hans Mertens hansm@hawaii.edu 
University of Hawai'i at Manoa
96822HonoluluHIUSA</p>
<p>University of Hawai'i at Manoa
96822HonoluluHIUSA</p>
<p>Sapphira Akins 
University of Hawai'i at Manoa
96822HonoluluHIUSA</p>
<p>Member, IEEEFrances Zhu zhuf@hawaii.edu 
University of Hawai'i at Manoa
96822HonoluluHIUSA</p>
<p>Cost-Aware Query Policies in Active Learning for Efficient Autonomous Robotic Exploration*
8C2D4A550131A8800DCCBBDD8528EBCE
In missions constrained by finite resources efficient data collection is critical.Informative path planning, driven by automated decision-making, optimizes exploration by reducing the costs associated with accurate characterization of a target in an environment.Previous implementations of active learning (AL) did not consider the action cost for regression problems or only considered the action cost for classification problems.This paper analyzes an AL algorithm for Gaussian Process (GP) regression while incorporating action cost.The algorithm's performance is compared on various regression problems to include terrain mapping on diverse simulated surfaces along metrics of root mean square (RMS) error, samples and distance until convergence, and model variance upon convergence.The cost-dependent acquisition policy doesn't organically optimize information gain over distance; instead, the traditional uncertainty metric with a distance constraint best minimizes root-mean-square error over trajectory distance.This study's impact is to provide insight into incorporating action cost with AL methods to optimize exploration under realistic mission constraints.</p>
<p>I. INTRODUCTION</p>
<p>In the field of machine learning and robotics, efficiently gathering data is crucial for driving technological advancements.As the demand for data to train machine learning models grows, so do the costs associated with collecting and processing that data.To optimize these efforts, we must move away from indiscriminate data acquisition and focus on strategies that minimize costs while maximizing information gain.Active learning offers a solution by intelligently selecting the most informative data points, reducing the need for excessive sampling and making data collection more cost-effective.The benefits of active learning include improved prediction accuracy, faster convergence to a learned model, reduced data acquisition costs, improved generalization, and robustness to noise [5], [6].This challenge is especially critical in a multitude of diverse applications, such as ocean exploration, where only 5-20% of the oceans have been explored due to the harsh and inaccessible environments [1].Similarly, space missions are expensive and resource-constrained, requiring careful selection of exploration targets to maximize scientific return within limited mission lifetimes [2].In agriculture, the need to monitor vast swaths of land, often beyond the reach of remote sensing technologies, calls for efficient, targeted sampling strategies [3].Even in domains like material discovery, where new materials must be identified from a vast *Research supported by the National Science Foundation (NSF).S. Akins is with the University of Hawai'i at Manoa, Honolulu, HI 96822 USA (e-mail: sakins@hawaii.edu).</p>
<p>search space and prototyping new materials is costly, intelligently selecting experiments can drastically reduce costs and speed up breakthroughs [4].Across these fields, the ability to sample data intelligently is not only a matter of efficiency but also a necessity for advancing technology within practical constraints.</p>
<p>Active learning holds immense promise in maximizing information gain, but much progress in the field of active learning ignores the cost of annotating unlabeled data or measuring a sample [7].The act of annotation occurs when a proposed data point in the input space is presented to an annotator to label; for example, an image to a human to classify as an object class or a location of interest to a robot's sensor to measure a real value.Labeling these samples consumes human, computational, or robotic effort, which should be incorporated into the active learning framework.Applications in which annotation cost is highly emphasized include exploratory robotics, such as extraplanetary rovers mapping the Moon [2], [8] and underwater autonomous vehicles mapping the ocean [9], [10].Practical and scalable active learning algorithms for robotic applications and beyond must include annotation costs to be implemented in reality.</p>
<p>This paper aims to answer the following questions: i) Does considering annotation cost balance information gain and annotation cost?What is the performance gain?ii) What is the annotation-cost query policy that best balances cost and information gain?This paper's contribution is showing that an annotation-conscious query policy does indeed balance cost and information gain, while guiding practitioners on how to design a query policy to incorporate annotation costs to achieve balance.</p>
<p>Section 2 below provides further background into active learning methods and their characterization.Section 3 follows with the methodology which outlines the experiment design, procedures, and campaign, the model hyperparameters, and the benchmark testing surfaces.Section 4 discusses the evaluation metrics and Section 5 presents data gathered from two experiments conducted in virtual environments.Finally, Section 6 summarizes the results and provides recommendations for future courses of action.</p>
<p>II. BACKGROUND</p>
<p>Active learning is a well-established framework aimed at optimizing data acquisition by selecting the most informative data points with minimal labeling or annotation costs.While this approach is valuable in domains where human annotation is costly, it does not fully address the challenges posed by physical exploration tasks where action costs, such as movement or sampling, must also be considered.In the context of spatial exploration, Gaussian Processes (GPs) have been shown to be highly effective in active learning frameworks for optimizing data acquisition.Krause and Guestrin introduce a nonmyopic active learning strategy using GPs, focusing on the trade-off between exploration (uncertainty reduction) and exploitation (near-optimal observation selection) [11], [12].Their approach demonstrates how GPs can be used to efficiently select observations in spatial domains, which is directly relevant to exploration tasks such as planetary missions.While their work optimizes the exploration-exploitation trade-off, it does not account for action cost.Action costs refer to the physical movement of an autonomous system, which is a key consideration in planetary exploration.This gap highlights the need for further integration of action costs into the decisionmaking framework for autonomous exploration.</p>
<p>Active learning (AL) is an iterative process designed to optimize data acquisition by selecting the next sample point in an input space based on a query policy.This policy aims to maximize an objective, such as reducing uncertainty or minimizing prediction error.AL is composed of several key components: the learner, annotator, and query policy.The learner processes historical data to make predictions, while the annotator provides labels or ground truth for selected samples.The query policy determines which points should be labeled next, optimizing data acquisition based on an information criterion.In most conventional AL frameworks, the next sampling point  +1 is chosen by maximizing a query policy ( ̂) as seen in Eq. (1).
𝑥 𝑖+1 = argmax 𝑥∈𝐷 𝑈 𝑔(𝑓 ̂(𝑥))(1)
 ̂()~((), (, ))</p>
<p>The learner  ̂() is defined by Eq. (2).() and (, ) represent the mean and covariance functions, respectively.  is the unlabeled data space.() is the acquisition function, conventionally in the form of information criteria.</p>
<p>The learner becomes more accurate as it receives more data, while the annotator incurs costs for each labeled point.Annotation costs can be uniform, where labeling a point causes the same cost regardless of spatial location (spatially independent).However, annotation costs can be variable, influenced by factors like the distance between sampled points.This is crucial in fields like planetary exploration, where moving between locations has a tangible cost.</p>
<p>The query policy, critical in determining the balance between exploration (sampling from uncertain areas) and exploitation (focusing on well-understood regions), often prioritizes information gain.However, in physically constrained environments, solely maximizing information gain is impractical.Such applications require factoring in both data informativeness and action costs, such as movement or energy usage, when deciding where to sample next.Without accounting for action costs, the algorithm may select points that are informative but too costly to reach, reducing overall mission efficiency.</p>
<p>GP AL algorithms excel in scenarios involving sparse and unevenly distributed data; however, they can struggle with larger datasets due to computational demands.Despite this challenge, GPs have been shown to outperform other models, such as Bayesian Neural Networks (BNNs) in sparse or low dimensional data regimes or in expressing smooth target manifolds [8].This paper builds on the work done in [8], [2] studying active learning for planetary exploration driven by an information query policy.This study focuses on the optimization of the query policy.An ideal algorithm should not only consider sampling efficiency, but also the cost of the respective samples.By integrating both uncertainty and distance metrics into the acquisition function, the algorithm accurately balances the 'cost' and 'reward' of its sampling actions.This set-up allows for an AL algorithm to choose actions that optimize the trade-off between minimizing distance traveled and maximizing information gain.</p>
<p>III. METHODOLOGY</p>
<p>The following sections address the query policies tested, the AL algorithm and the GP model's hyperparameters.Subsequently, the benchmark surfaces and the experimental campaign are presented.</p>
<p>A. Query Policies</p>
<p>In this study, the agent traversing the surface is encoded with an objective function that aims to minimize a learned model's prediction with respect to the ground truth.The model error takes on the form of the 2 norm, also known as root mean squared (RMS) error.The query policy or acquisition function, which guides the agent's sampling decision, can be configured to send the rover to sampling locations based on various criteria.This study compares a traditional query policy configuration (i.e., sampling at the location of highest variance regardless of distance from the current location) and novel policies that weigh both variance and distance required to reach a sampling location: distancenormalized variance and distance constrained variance policies.</p>
<p>A conventional uncertainty sampling policy can be expressed by Eq. ( 3)
𝑥 𝑖+1 = argmax 𝑥 𝑗 ∈𝐷 𝑈 𝜎 𝑝𝑟𝑒𝑑 2 (𝑥 𝑗 )(3)𝜎 𝑝𝑟𝑒𝑑 2 (𝑥) = 𝑑𝑖𝑎𝑔(𝑘(𝑥, 𝑥))(4)
where   2 () represents the model variance across the unlabeled data set   .</p>
<p>The distance-constrained variance query policy determines the next sampling location, which can be restricted to a certain movement horizon set by the algorithm.The concept of a movement horizon is particularly important when considering different types of technology.For example, a satellite would use an unconstrained movement horizon as it can point to any location within a space with relatively low cost.In contrast, a rover operating in-situ will have much higher costs associated with moving from point to point.In this case, the movement horizon can be constrained to a certain grouping of nearby points.The distance-normalized policy and the distanceconstrained policy are outlined in Eq. ( 5) and Eq. ( 6), respectively.
𝑥 𝑖+1 = argmax 𝑥 𝑗 ∈𝐷 𝑢 𝜎 𝑝𝑟𝑒𝑑 2 (𝑥 𝑗 )/‖𝑥 𝑗 − 𝑥 𝑖 ‖(5)𝑥 𝑖+1 = argmax ‖𝑥 𝑗 −𝑥 𝑖 ‖≤𝑟 𝑐𝑜𝑛 𝜎 𝑝𝑟𝑒𝑑 2 (𝑥 𝑗 )(6)
Various distance constraints, or movement horizons, were tested and compared on their effectiveness of decreasing distance traveled and samples necessary for convergence to a low RMSE model of the sampled space for the distanceconstrained policy.Specifically, these distance constraints include: 1∆, 2∆, 3∆, 5∆, 7∆, and 10∆.</p>
<p>The distance-normalized variance policy uses distancenormalized variance in its query policy.The agent's movement while operating under this exploration strategy is unconstrained, as the model is expected to determine the most cost-effective and rewarding point to travel to within the entire sample space.</p>
<p>B. Algorithm</p>
<p>The active learning algorithm utilized for both distancenormalized and distance-constrained variance query policies is outlined in Algorithm 1.Note that all experiments follow the same steps, except for any deviations which are mentioned explicitly.All variables mentioned in Algorithm 1 are defined in Table 1.</p>
<p>Algorithm 1 Gaussian Process Active</p>
<p>C. Model Hyperparameters</p>
<p>The AL Algorithm was developed using the GPyTorch package [4].During simulations, the GP model uses a Radial Basis Function (RBF) kernel.The kernel's length scale is optimized through gradient descent over 100 iterations, with the model trained to maximize the marginal log likelihood.The model's hyperparameters were optimized using the Adam optimizer with a learning rate of 0.1.The code utilized in this study is available in the following repository: https://github.com/xfyna/Action_Cost_AL.git.All computations were performed on KOA, the University of Hawaii's high-performance computing (HPC) cluster.</p>
<p>D. Experiment Environments</p>
<p>The exploration strategies are evaluated through their capabilities of learning three spatial distributions of a target output of varying complexity by traversing a geometric surface shown in Fig. 1 and 2 below.These include the Parabola, Townsend, and Lunar Crater surface geometries.The changing complexity of these surfaces allows the exploration strategies to be evaluated on their convergence rates and various other evaluation metrics, explained in more detail in the following section.Note that Fig. 1 represents the surface the agent traverses and therefore is used to calculate the distance the agent travels.Fig. 2, on the other hand, is the target output distribution that model aims to learn.In the case of the Parabola and Townsend surfaces, the agent is building a model that learns the elevation of the surface.In the case of the Lunar surface, the model learns the hydroxyl content across the surface.The surfaces are characterized by two independent dimensions (planar position  = ( 1 ,  2 )) and a third dependent dimension .For the Parabola and Townsend surfaces, the algebraic relationship between  and position is simulated.For the Lunar Crater, this target is taken from real measurements, with  representing the hydroxyl content across the surface rather than "elevation", as it does for the previous two surfaces.The Parabola surface is described by Eq. ( 7), where  1 and  2 range from [−1: 0.</p>
<p>The Lunar Crater surface is derived from the Lyman Alpha Mapping Project (LAMP) data which collected ultraviolet spectrometry data from the Lunar Reconnaissance Orbiter (LRO) [5].The dataset includes a digital elevation map (DEM) of the lunar south pole, represented by  = ( 1 ,  2 ,  3 ), with a spatial resolution of 5 m and hydroxyl data with a resolution of 250 m.The data contains varying levels of noise, and substantial gaps are evident near the crater rim.</p>
<p>The complexity of these surfaces is defined by the number of local wells and measurement noise, seen in Table 2 in ascending complexity.The code used to calculate the number of wells can be found in the previously linked GitHub repository.</p>
<p>Noisy Lunar</p>
<p>Crater</p>
<p>E. Experimental Campaign</p>
<p>A total of 10 trials were conducted for each query policy tested over each surface type.A simulation trial consists of loading the environment geometry, outlined in Table 2, followed by defining the exploration strategy, as defined in Table 3 below.The experimental simulations conducted measure the total distance the agent travels, the number of samples collected, and the model's RMSE at convergence.Note that the code is configured such that during a run, one trial is completed for all exploration strategies over the designated surface (i.e., the distance-constrained variance policy for 6 movement horizons and the distance-normalized variance policy agent traverses the same surface with random initialization across the space).As such, the surface type must be specified for each experiment, but the exploration strategies do not need to be specified when running the code.</p>
<p>IV. EVALUATION METRICS</p>
<p>The following single metrics are used to assess the performance of the various exploration strategies utilized in this research.RMSE upon convergence,   , is derived from control theory's concept of 2% settling time.The global RMSE between a model prediction and true values are inspected to ensure that there are enough data points for convergence to occur, as well as that the final values of RMSE stay within a 2% band of the final error,   .This 2% error band is found using Eq. ( 9) below.RMSE upon convergence is defined as the upper bound of this error band, as shown in Eq. (10).
∆𝑒 2% = 0.02(𝑒 0 − 𝑒 𝑓 )(9)𝑒 𝑐 = 𝑒 𝑓 + 𝑒 2%(10)
The normalized RMSE (NRMSE),   , is defined by the Eq. ( 13) where the range of target output values across the Parabola, Townsend, and Lunar surfaces are, respectively, 2.00, 5.59, 0.50.
𝑒 𝑛 = 𝑒 𝑐 𝑦 𝑚𝑎𝑥 −𝑦 𝑚𝑖𝑛(11)
Samples until convergence,   , denotes the index at which convergence occurs.It is calculated through the minimization of the difference between the error at an index,   , and the error upon convergence,   , as shown in Eq. ( 12).The values are scaled based on the number of samples taken per surface (i.e., the total number of samples taken until convergence are divided by the max possible samples per surface).The Lunar surface has a stopping condition of 155 samples and the Parabola and Townsend surface has one of 109 samples.
𝑖 𝑐 = argmin 𝑖 ‖𝑒 𝑖 −𝑒 𝑐 ‖ 2
 (12) Distance until convergence,   is a metric that represents the total distance traveled by the agent until convergence is reached.This is calculated as the sum of the radial difference between each waypoint until the sample of convergence is taken, as shown in Eq. ( 13).These values are scaled based on the grid length, as shown in Table 2.
𝑑 𝑐 = ∑ ‖𝑥 𝑘,𝑖+1 −𝑥 𝑘,𝑖 ‖ 2 𝑖 𝑐 𝑖=1 𝑥 𝑚𝑎𝑥 −𝑥 𝑚𝑖𝑛 (13)
The following multi-objective metrics provide insight into the comparative performance of the exploration models, shedding light onto which methods perform better in terms of minimizing distance or samples while maintaining the goal of a convergence to a low error model.Distance-scaled NRMSE,   , scales   by   as shown in Eq. ( 14).The combined metric allows for easier interpretation regarding the various policies and their ability to converge to a low NRMSE while traveling smaller distances.The lower the value, the higher performing the model is.
𝑒 𝑑𝑐 = 𝑒 𝑛 * 𝑑 𝑐(14)
Sample-scaled NRMSE,   , imilarly to distance scaled NRMSE, scales the convergence NRMSE with the convergence samples, as shown in Eq. ( 15).
𝑒 𝑖𝑐 = 𝑒 𝑛 * 𝑖 𝑐(15)
V. RESULTS</p>
<p>Considering distance in query policies can achieve similar RMS error to conventional methods but with at least a magnitude less distance traveled.Distance-constrained variance policies achieve the most distance-efficient exploration, although the most effective movement horizon depends on the environment.To illustrate these results and explore the nuances, this section presents a comprehensive metric comparison and query policy behavior.</p>
<p>A. Metric Comparison</p>
<p>Generally, distance-constrained variance performed better than distance-normalized variance policies.Tables 4 through  6 outline the average performance metrics achieved after 10 trials with the outlined exploration methods.The green highlighted squares signify the highest scoring policy while the red-orange highlighted squares signify the lowest performing policy in the specified metric.The "Norm" movement horizon refers to the unconstrained distancenormalized variance policy and the "Conv" movement horizon denotes performance of the conventional unconstrained query policy.The best policy with respect to   varied significantly by surface type.However, the best policies with respect to   and   were the 2∆ distance-constrained variance and 10∆ distance-constrained variance policies, respectively.Fig. 3 through 5 below highlight the balance these policies offer in terms of convergence to low error with minimal cost.The best policy with respect to   was the 7∆ distance-constrained variance method.The best policy with respect to   was the 2∆ distance-constrained variance method as it converged after traveling a low distance most consistently across all surfaces.On the other hand, the worst performance for both   and   was the 1∆ distance-constrained variance policy and the worst performance in terms of   was the conventional unconstrained method.</p>
<p>Along single metrics, the 10∆, 5∆, and conventional AL policies obtained the lowest convergence NRMSE on the following surfaces, respectively: Townsend, Lunar, and Parabola.Performance in   depends on surface complexity, with higher surface complexity resulting in higher error, as show in Fig. 3. Additionally, NRMSE depends on a policy's movement horizon where a horizon that is too small leads to increased error but once the movement horizon is sufficiently large,   is within a couple percent of the minimum error possible.Notice that the distance-normalized and distanceconstrained policies, apart from the 1∆ horizon, converge to an NRMSE that is comparable with the conventional AL method.This highlights the effectiveness of incorporating action cost into query policies, as no performance in terms of model accuracy is lost, despite the agent traveling less distance across the sampling space.If only considering samples until convergence, Fig. 4 demonstrates the relationship between movement horizon and samples until convergence.The best performing metric is shown to be the 7∆ constraint, with the 10∆ trailing shortly behind.This phenomenon is observed most likely due to the exploration strategy taking larger steps allowing for exploration across a wider area, therefore not requiring as many samples to reach convergence.This is particularly evident with the conventional exploration strategy, which can be seen as taking the least samples across the parabola surface.The small dip that occurs at the 2∆ movement horizon, in Fig. 4, suggests the 2∆ constraint performs well at balancing all metrics.The best performing policies in terms of distance traveled can be seen clumped in the center of the figure at the 2∆ and 3∆ marks.Fig. 5 below illustrates the convergence distance required for the various query policies across surface types.This suggests that the movement horizon is significant in determining the distance the agent traverses until reaching convergence.Comparatively, the unconstrained strategies travel more before reaching convergence.However, the difference between the conventional unconstrained policy and the distance-normalized variance query policy is significant, shown through Fig. 5 where the conventional strategy travels an order of magnitude greater than all other methods, illustrating the importance of distance incorporation into the query policy.On average, the 5∆ constraint travels a larger distance than the other distance-constrained policies.To display multiple objectives, Fig. 6 and 7 present tradeoffs in balace with error along the y-axis and samples, followed by distance, on the x-axis for all three surfaces."N" represents the unconstrained distance-normalized variance method, "C" represents the conventional AL method, and all other methods are denoted by their movement constraint.</p>
<p>For missions that just care about error and number of samples, conventional AL makes total sense, expecially for a simple surface, but gains could be made by incorporating a movement horizon.The optimal location to balance these multiple objectives for policies would be nearest to the origin on both Figures.In Fig. 6, the Lunar surface required the highest number of samples to reach convergence for some methods irregardless of the fact that it is not the most complex surface, as seen by the blue colored labels on Fig. 6.It is possible that the kernel used to model the surface was more fitted towards the Townsend and Parabola surfaces instead of the Lunar surface causing this to occur.The effectiveness of the 7∆ constraint on limiting samples is evident as it is shown to require the least amount of samples for nearly all surface types.Along with this, its convergence NRMSE is comparable or lower than many of the other policies' convergence NRMSE.However, as mentioned previously, the 10∆ method performs the best in terms of   due to its lower convergence NRMSE and similarly low sampling rate.Looking rightward from these constraints and their locations in Fig. 6, the number of samples until convergence increases as the distance horizon decreases.Fig. 7 illustrates the clusters of policy performance in terms of distance and NRMSE across surfaces.Again, a similar visual trend can be seen with the Parabola trials hugging the leftmost side of the graph, followed by the Lunar and Townsend surface to its right (i.e., grouped by surface complexity).Further clustering is demonstrated, most visibly on the Parabola and Lunar surfaces, where lower distance horizon policies (up to 3∆) clump together at a distance metric along the horizontal axis and higher policies are shown to their bottom right, illustrating lower NRMSE convergence at higher distances.Additionally, note the separation of the conventional AL method in terms of distance traveled in comparison to NRMSE, which is generally very similar to the performance of all other models.The 2∆ method balances accuracy and samples better than all other strategies.Fig. 7 also highlights the influence of surface complexity on model performance.The various query policies, ordered in increasing movement horizon, show various levels of coverage and revisits across the example of the Townsend surface.The 1∆ distance-constrained variance query policy struggles to cover sufficient distance as shown in Fig. 8 a).This exploration strategy is shown as traveling in a small, clustered section of space, rather than mapping at or around the corners.The cluster location depends heavily on the initialization of the agent on the surface, leading it to be nearer to an edge or towards the center of a surface.The 3∆ distance-constrained variance query policy shown in Fig. 8 b) achieves more coverage but with a seemingly smooth path, demonstrating its distance efficiency.Moving to Fig. 8 c), the distancenormalized query policy is shown as covering more distance, however, it tends to traverse more jagged paths suggesting a lower distance efficiency.Lastly, Fig. 8 d) illustrates the conventional query policy, which is shown to move extensively across the surface revisiting many similar locations in the process.</p>
<p>To show the evolution of exploration, Fig. 9 displays the mean performance through a solid line and standard deviation by the shaded region across 10 Townsend trials along the metrics of both mean NRMSE and total distance traveled.Performance in terms of efficiency and accuracy can be seen through visual inspection of the area under the curve.Through this figure, the 2∆ method demonstrate its strength in traveling short distances but reducing NRMSE quickly.While the 5∆, 7∆, and 10∆ constrained policies do converge to a lower NRMSE, the distance required to reach such convergence is far greater than that for the 2∆ constraint.The limited performance of the 1∆ movement horizon is highlighted here along with the conventional AL method which is shown to perform extremely poor in comparison to all other policies in terms of convergence to a low RMS error in minimal distance.This study demonstrates the importance of incorporating action costs into active learning frameworks for autonomous robotics.By evaluating various query policies, including distance-constrained and distance-normalized variance methods, we observed that balancing information gain with movement efficiency significantly enhances mission performance.The results show that while distanceconstrained policies reduce the total distance traveled without sacrificing model accuracy, the optimal movement horizon depends on the environment's complexity and mission constraints.These findings provide valuable insights for future applications, such as planetary exploration, where resource constraints and mission efficiency are paramount.</p>
<p>Learning</p>
<p>1 : 6 :
16
Define environemnt ∈ {, , } 2: Define spatial limits (  ,   ,   ,   ) 3: Define query policy ∈ {  ,   , } 4: Define stopping condition   / 4 5: Define noise   2 Define movement horizon 7: Define target output distribution 8: Initialize position in environment 9: for  = 1 to 10 10: Randomly choose   ∈   constrained to 3∆ 11: Measure ground truth  at  +1 12: Add (  ,   ) to   13: Train GP model on   14: for  = 11    15: Train GP model on   16: Predict  ̂ and variance   2 in prediction horizon   17: Use query policy  * to find  +1 18: Traverse to  +1 to measure ground truth  19: Add ( +1 ,  +1 ) to   TABLE I. DEFINITION OF VARIABLES IN ALGORITHM 1 Variable Description   Digital Elevation Map (DEM) resolution, defined as 5 meters for grid sizing and 250 meters for hydroxyl content   Unsampled data points across surface geometry   The starting position of the agent on the surface   Training data set consisting of a sampling location and the ground truth observation  ̂ Predicted scalar expected values   2 Predicted variance values   Movement horizon</p>
<p>Figure 1 .
1
Figure 1.Surface environments that the agent traverses which dictates distance traveled by the agent: a) Parabola, b) Lunar Crater (6 km edge crater elevation), c) Townsend.</p>
<p>Figure 2 .
2
Figure 2. True value of the target outputs the agent learns that dictates RMS error: a) Parabola elevation, b) LAMP data across the DEM for the 6 km crater swath, c) Townsend elevation.</p>
<p>Figure 3 .
3
Figure 3. Average NRMSE upon Convergence across Surfaces</p>
<p>Figure 4 .
4
Figure 4. Average Samples Taken across Surfaces until Convergence</p>
<p>Figure 5 .
5
Figure 5. Average Distance Traveled across Surfaces until Convergence</p>
<p>Figure 6 .
6
Figure 6.Mean NRMSE vs. Mean Samples across All Surfaces for 10 trials of data</p>
<p>Figure 7 .
7
Figure 7. Mean NRMSE vs. Mean Distance until Convergence across All Surfaces for 10 trials</p>
<p>Figure 8 .
8
Figure 8. Simulated Path of the Agent Traversing Townsend Surface with Various Exploration Strategies</p>
<p>Figure 9 .
9
Figure 9. Distance vs. RMS Error Mean and Stanard Deviation Variance Across 10 Trials until Convergence on the Townsend Surface</p>
<p>1: 1].The Townsend surface is defined by Eq. (8), with  1 and  2 ranging from [−2.5: 0.1: 2.5].
𝑦 = 𝑥 1 2 + 𝑥 2 2 + 𝜎 𝑛𝑜𝑖𝑠𝑒 2(7)𝑦 = −(cos((𝑥 1 − 0.1)𝑥 2 ) − 𝑥 1 sin(3𝑥 1 + 𝑥 2 ) + 𝜎 𝑛𝑜𝑖𝑠𝑒 2</p>
<p>TABLE II
II.RANGE OF SIMULATED ENVIRONMENTEnvironment TopographySize of Surface# of Local ExtremaKnown ExpressionNoisy Parabola𝑥 1 ∈ [−1: 0.1: 1] 𝑥 2 ∈ [−1: 0.1: 1]Min: 1 Max: 0Yes</p>
<p>TABLE III
III.RANGE OF EXPLORATION STRATEGIES AND TRIALSCOLLECTEDExploration StrategyMovement Horizon1∆𝑥2∆𝑥Distance-Constrained Variance3∆𝑥 5∆𝑥7∆𝑥10∆𝑥Distance-Normalized VarianceGlobalConventional Variance OnlyGlobal</p>
<p>TABLE IV .
IV
AVERAGE PERFORMANCE METRIC VALUES FOR ALL POLICIES ACROSS THE PARABOLA SURFACE
Movement Horizon𝒆 𝒄𝒊 𝒄𝒅 𝒄𝒆 𝒅𝒄𝒆 𝒊𝒄1∆𝑥0.04485 0.73 0.82750 0.03712 0.032802∆𝑥0.01722 0.61 0.61650 0.01061 0.010423∆𝑥0.03121 0.59 0.65200 0.02035 0.018355∆𝑥0.02663 0.46 1.08950 0.02902 0.012277∆𝑥0.02793 0.45 1.06300 0.02968 0.0126310∆𝑥0.02532 0.48 1.09750 0.02778 0.01215Norm0.00967 0.57 2.78645 0.02693 0.00546Conv0.00909 0.28 13.3380 0.12118 0.00253</p>
<p>TABLE V .
V
AVERAGE PERFORMANCE METRIC VALUES FOR ALL POLICIES ACROSS THE LUNAR CRATER SURFACE
Movement Horizon𝒆 𝒄𝒊 𝒄𝒅 𝒄𝒆 𝒅𝒄𝒆 𝒊𝒄1∆𝑥0.103430.90 2.71065 0.280370.093502∆𝑥0.065540.77 2.36771 0.15517 0.050703∆𝑥0.058720.88 2.63802 0.15491 0.051715∆𝑥0.043040.83 6.07708 0.26154 0.035717∆𝑥0.043250.65 4.93646 0.21351 0.0282710∆𝑥0.044350.67 5.14236 0.22805 0.02985Norm0.095760.92 2.17940 0.20869 0.08800Conv0.059760.72 97.8479 5.84719 0.04318</p>
<p>TABLE VI .
VI
AVERAGE PERFORMANCE METRIC VALUES FOR ALL POLICIES ACROSS THE TOWNSEND SURFACE
Movement Horizon𝒆 𝒄𝒊 𝒄𝒅 𝒄𝒆 𝒅𝒄𝒆 𝒊𝒄1∆𝑥0.16823 0.92 2.60781 0.43872 0.154152∆𝑥0.06573 0.91 2.691670.17692 0.05960
3∆ 0.10846 0.86 2.75357 0.29865 0.09353 5∆ 0.04700 0.83 5.34107 0.25105 0.03912 7∆ 0.05703 0.81 5.06607 0.28891 0.04619 10∆ 0.03989 0.89 5.69286 0.22707 0.03534 Norm 0.05611 0.83 5.68763 0.31914 0.04642 Conv 0.08521 0.91 93.4167 7.96004 0.07713</p>
<p>ACKNOWLEDGMENTThis work was supported by NASA Grant HI-80NSSC21M0334 and the AI Institute in Dynamic Systems, one of the National Artificial Intelligence Research institutes funded by the National Science Foundation (NSF) via award number 2112085.We would like to thank the University of Hawaii's high-performance computing (HPC) cluster team for ensuring the operation of the HPC cluster which allowed us to complete our research, as well.
Mind the gap: comparing exploration effort with global biodiversity patterns and climate projections to determine ocean areas with greatest exploration needs. B R C Kennedy, R D Rotjan, 10.3389/fmars.2023.1219799Front. Mar. Sci. 10Nov. 2023</p>
<p>Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes. A Akemoto, F Zhu, 12</p>
<p>A Survey of Active Learning for Quantifying Vegetation Traits from Terrestrial Earth Observation Data. K Berger, J P Rivera Caicedo, L Martino, M Wocher, T Hank, J Verrelst, 10.3390/rs13020287Remote Sensing. 132287Jan. 2021</p>
<p>Entropy-based Active Learning of Graph Neural Network Surrogate Models for Materials Properties. J Allotey, K T Butler, J Thiyagalingam, 10.1063/5.0065694The Journal of Chemical Physics. 15517174116Nov. 2021</p>
<p>. B Settles, Active Learning Literature Survey. </p>
<p>Active Contrastive Learning With Noisy Labels in Fine-Grained Classification. B Kim, B C Ko, 10.1109/ICEIC61013.2024.104572292024 International Conference on Electronics, Information, and Communication (ICEIC). Jan. 2024</p>
<p>Active Learning with Real Annotation Costs. B Settles, M Craven, L Friedland, </p>
<p>Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration. S Akins, F Zhu, 10.2514/6.2023-4720ASCEND 2023, in ASCEND. American Institute of Aeronautics and Astronautics2023</p>
<p>Informative Path Planning and Mapping with Multiple UAVs in Wind Fields. D.-H Cho, J.-S Ha, S Lee, S Moon, H.-L Choi, Oct. 2016</p>
<p>Obstacleaware Adaptive Informative Path Planning for UAV-based Target Search. A Meera, M Popovic, A Millane, R Siegwart, 2019</p>
<p>Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach. A Krause, C Guestrin, 10.1145/1273496.1273553Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learningNew York, NY, USAAssociation for Computing MachineryJun. 2007ICML '07</p>
<p>A Krause, C Singh, Guestrin, Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies. </p>            </div>
        </div>

    </div>
</body>
</html>
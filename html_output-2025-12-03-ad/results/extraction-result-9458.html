<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9458 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9458</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9458</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-a50ee01a66aa92b664a78b2a0628ac346f53e31d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a50ee01a66aa92b664a78b2a0628ac346f53e31d" target="_blank">ClimateBert: A Pretrained Language Model for Climate-Related Text</a></p>
                <p><strong>Paper Venue:</strong> Social Science Research Network</p>
                <p><strong>Paper TL;DR:</strong> This work proposes CLIMATEBERT, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies, and finds that it leads to a 48% improvement on a masked language model objective.</p>
                <p><strong>Paper Abstract:</strong> Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language that common LMs can not represent accurately. We argue that this shortcoming of today's LMs limits the applicability of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 48% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related downstream tasks like text classification, sentiment analysis, and fact-checking.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9458",
    "paper_id": "paper-a50ee01a66aa92b664a78b2a0628ac346f53e31d",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0036132499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ClimateBert: A Pretrained Language Model for Climate-Related Text</h1>
<p>Nicolas Webersinke, ${ }^{1}$ Mathias Kraus, ${ }^{1}$ Julia Anna Bingler, ${ }^{2}$ Markus Leippold ${ }^{3}$<br>${ }^{1}$ FAU Erlangen-Nuremberg, Germany<br>${ }^{2}$ ETH Zurich, Switzerland<br>${ }^{3}$ University of Zurich, Switzerland<br>nicolas.webersinke@fau.de, mathias.kraus@fau.de, binglerj@ethz.ch, markus.leippold@bf.uzh.ch</p>
<h4>Abstract</h4>
<p>Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language that common LMs can not represent accurately. We argue that this shortcoming of today's LMs limits the applicability of modern NLP to the broad field of text processing of climaterelated texts. As a remedy, we propose ClimateBert, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that ClimateBert leads to a $48 \%$ improvement on a masked language model objective which, in turn, leads to lowering error rates by $3.57 \%$ to $35.71 \%$ for various climate-related downstream tasks like text classification, sentiment analysis, and fact-checking.</p>
<h2>1 Introduction</h2>
<p>Researchers working on climate change-related topics increasingly use natural language processing (NLP) to automatically extract relevant information from textual data. Examples include the sentiment or specificity of language used by companies when discussing climate risks and measuring corporate climate change exposure, which increases transparency to help the public know where we stand on climate change (e.g., Callaghan et al. 2021; Bingler et al. 2022b). Many studies in this domain apply traditional NLP methods, such as dictionaries, bag-of-words approaches or simple extensions thereof (e.g., Grüning 2011; Sautner et al. 2022). However, such analyses face considerable limitations, since climate-related wording could vary substantially by source (Kim and Kang 2018). Deep learning techniques that promise higher accuracy are gradually replacing these approaches (e.g., Kölbel et al. 2020; Luccioni, Baylor, and Duchene 2020; Bingler et al. 2022a; Callaghan et al. 2021; Wang, Chillrud, and McKeown 2021; Friederich et al. 2021). Indeed, it has been shown in related domains that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>deep learning in NLP allows for impressive results, outperforming traditional methods by large margins (Varini et al. 2020).</p>
<p>These deep learning-based approaches make use of language models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretraining and leads to the model learning representations of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al. 2018) with its successors RoBERTA (Liu et al. 2019), Transformer-XL (Dai et al. 2019) and ELECTRA (Clark et al. 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources.</p>
<p>After the pretraining phase, most LMs are trained on additional tasks, the downstream task. For the downstream tasks, the LM builds on and benefits from the word representations and language patterns learned in the pretraining phase. The pre-training benefit is especially large on downstream tasks for which the collection of samples is difficult and, thus, the resulting training datasets are small (hundreds or few thousands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task-specific text exhibits better performance, compared to a model that has been pretrained solely on general text (Araci 2019; Lee et al. 2020).</p>
<p>Hence, a straightforward extension to the standard combination of pretraining is the so-called domain-adaptive pretraining (Gururangan et al. 2020). This approach has recently been studied for various tasks and basically comes in the form of pretraining multiple times - in particular pretraining in the language domain of the downstream task, i.e.,
pretraining (general domain)
+ domain-adaptive
pretraining (downstream domain)
+ training (downstream task).
To date, regardless of the increase in using NLP for climate change related research, a model with climate domainadaptive pretraining has not been publicly available, yet. Research so far rather relied on models pretrained on general language, and fine-tuned on the downstream task. To</p>
<p>fill this gap, our contribution is threefold. First, we introduce ClimateBert, a state-of-the-art language model that is specifically pretrained on climate-related text corpora of various sources, namely news, corporate disclosures, and scientific articles. This language model is designed to support researchers of various disciplines in obtaining better performing NLP models for a manifold of downstream tasks in the climate change domain. Second, to illustrate the strength of ClimateBert, we highlight the performance improvements using ClimateBert on three standard climate-related NLP downstream tasks. Third, to further promote research at the intersection of climate change and NLP, we make the training code and weights of all language models publicly available at GitHub and Hugging Face.</p>
<h2>2 Background</h2>
<p>As illustrated in Figure 1, our LM training approach for ClimateBert comprises all three phases - using an LM pretrained on a general domain, the domain-adaptive pretraining on the climate domain, and the training phase on climaterelated downstream tasks.</p>
<h2>Pretraining on General Domain</h2>
<p>As of 2018, pretraining became the quasi-standard for learning NLP models. First, a neural language model, often with millions of parameters, is trained on large unlabeled corpora in a semi-supervised fashion. By learning on multiple levels which words/word-sequences/sentences appear in the same context, an LM can represent a semantically similar text by similar vectors. Typical objectives for training LMs are the prediction of masked words or the prediction of a label indicating whether two sentences occurred consecutively in the corpora (Devlin et al. 2018).</p>
<p>In the earlier NLP pretraining days, LMs traditionally used regular or convolutional neural networks (Collobert and Weston 2008), or later Long-Short-Term-Memory (LSTM) networks to process text (Howard and Ruder 2018). Todays LMs mostly build on transformer models (e.g., Devlin et al. 2018; Dai et al. 2019; Liu et al. 2019). One of the latter is named RoBERTA (Liu et al. 2019) which was trained on 160GB of various English-language corpora data from BOOKCORPUS (Zhu et al. 2015), WIKIPEDIA, a portion of the CCNEWS dataset (Nagel 2016), OPENWEBTEXT corpus of web content extracted from URLs shared on Reddit (Gokaslan and Cohen 2019), and a subset of CommonCrawl that is said to resemble the story-like style of WINOGRAD schemas (Trinh and Le 2019). While these sources are valuable to build a model working on general language, it has been shown that domain-specific, niche language (such as climate-related text) poses a problem to current state-of-the-art language models (Araci 2019).</p>
<h2>Domain-Specific Pretraining</h2>
<p>As a remedy to inferior performance of general language models when applied to niche topics, multiple language</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>models have been proposed which build on the pretrained models but continue pretraining on their respective domains. FinBERT, LegalBert, MedBert are just a few language models that have been further pretrained on the finance, legal, or medical domain (Araci 2019; Chalkidis et al. 2020; Rasmy et al. 2021). In general, this domain-adaptive pretraining yields more accurate models on downstream tasks (Gururangan et al. 2020).</p>
<p>Domain-specific pretraining requires a decision about which samples to include in the training process. It is still an open debate which sample strategy improves performance best. Various strategies can be applied to extract the text samples on which the LM is further pretrained. For example, while traditional pretraining uses all samples from the pretraining corpus, similar sample selection (Sim-Select) uses only a subset of the corpus, in which the samples are similar to the samples in the downstream task (Ruder and Plank 2017). In contrast, diverse sample selection (DivSelect) uses a subset of the corpus, which includes dissimilar samples compared to the downstream dataset (Ruder and Plank 2017). Previous research has investigated the benefit of these approaches, yet no final conclusion about the efficiency has been obtained. Consequently, we compare these approaches in our experiments.</p>
<h2>NLP on Climate-Related Text</h2>
<p>In the past, climate-related textual analysis often used predefined dictionaries of presumably relevant words and then simply searched for these words within the documents. For example, Cody et al. (2015) use such an approach for climate-related tweets. Similarly, Sautner et al. (2022) use a keyword-based approach to capture firm-level climate change exposure. However, these methods do not account for context. The lack of context is a significant drawback, given the ambiguity of many climate-related words such as "environment," "sustainable," or "climate" itself (Varini et al. 2020).</p>
<p>Only recently, BERT has been used for NLP in climaterelated text. The transformers-based BERT models are capable of accounting for the context of words and have outperformed traditional approaches by large margins across various climate-related datasets (Kölbel et al. 2020; Luccioni, Baylor, and Duchene 2020; Varini et al. 2020; Bingler et al. 2022a; Callaghan et al. 2021; Wang, Chillrud, and McKeown 2021; Friederich et al. 2021; Stammbach et al. 2022). However, this research has also shown that extracting climate-related information from textual sources is a challenge, as climate change is a complex, fast-moving, and often ambiguous topic with scarce resources for popular textbased AI tasks.</p>
<p>While context-based algorithms like BERT can detect a variety of complex and implicit topic patterns in addition to many trivial cases, there remains great potential for improvement in several directions. To our knowledge, none of the above cited work has examined the effects of domain-adaptive pretraining on their specific downstream tasks. Therefore, we investigate whether domain-adaptive pretraining will improve performance for climate changerelated downstream tasks such as text classification, senti-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: Sequence of training phases. Our main contribution is the continued pretraining of language models on the climate domain. In addition, we evaluate the obtained climate domain-specific language models on various downstream tasks.</p>
<p>ment analysis, and fact-checking.</p>
<h2>3 ClimateBert</h2>
<p>In the following, we describe our approach to train ClimateBert. We first list the underlying data sources before describing our sample selection techniques and, finally, the vocabulary augmentation we used for training the language model.</p>
<h3>Text Corpus</h3>
<p>Our goal was to collect a large corpus of text, Corp, that included general and domain-specific climate-related language. We decided to include the following three sources: news articles, research abstracts, and corporate climate reports. We decided not to include full research articles because this language is likely too specific and does not represent general climate language. We also did not include Twitter data, as we assume that these texts are too noisy. In total, we collected 2,046,523 paragraphs of climate-related text (see Table 1).</p>
<p>The News dataset is mainly retrieved from Refinitiv Workspace and includes 135,391 articles tagged with climate change topics such as climate politics, climate actions, and floods and droughts. In addition, we crawled climaterelated news articles from the web.</p>
<p>The Abstracts dataset includes abstracts of climaterelated research articles crawled from the Web of Science, primarily published between 2000 and 2019.</p>
<p>The Reports dataset comprises corporate climate and sustainability reports of more than 600 companies from the years 2015-2020 retrieved from Refinitiv Workspace and the respective company websites.</p>
<p>Given the nature of the datasets, we find a large heterogeneity between the paragraphs in terms of number of words. Unsurprisingly, on average, the paragraphs with the least words come from the News and the Reports datasets. In contrast, Abstracts includes paragraphs with the most words. Table 1 lists these descriptives.</p>
<p>To estimate the benefit from domain-adaptive pretraining, we compare the similarity of our text corpus with the one used for pretraining RoBERTa. Following Gururangan et al. (2020), we consider the vocabulary overlap between both corpora. The resulting overlap of 57.05% highlights the dissimilarity between the two domains and the need to add specific vocabularies. Therefore, we expect to see considerable performance improvements of domain-adaptive pretraining.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Num. of</th>
<th>Avg. num. of words</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>paragraphs</td>
<td>Q1</td>
<td>Mean</td>
<td>Q3</td>
</tr>
<tr>
<td>News</td>
<td>1,025,412</td>
<td>34</td>
<td>56</td>
<td>65</td>
</tr>
<tr>
<td>Abstracts</td>
<td>530,819</td>
<td>165</td>
<td>218</td>
<td>260</td>
</tr>
<tr>
<td>Reports</td>
<td>490,292</td>
<td>34</td>
<td>65</td>
<td>79</td>
</tr>
<tr>
<td>Total</td>
<td>2,046,523</td>
<td>36</td>
<td>107</td>
<td>168</td>
</tr>
</tbody>
</table>
<p>Table 1: Corpus Corp used for pretraining ClimateBert. Q1 and Q3 stand for the 0.25 and 0.75 quantiles, respectively.</p>
<h3>Sample Selection</h3>
<p>Prior work has shown that specific selections of the samples used for pretraining can foster the performance of the LM. In particular, incorporating information from the downstream task by selecting similar or diverse samples has been shown to yield favorable results compared to using all samples from the dataset. We follow both approaches and select samples that are similar or diverse to climate-text using our text classification task (see 5). We experiment with three different strategies from Ruder and Plank (2017) for the selection of samples from our corpus:</p>
<ul>
<li>In the most traditional sample selection strategy, FullSelect, we use all paragraphs from Corp to train ClimateBert_{F}.</li>
<li>In Sim-Select, we select the 70% of samples from Corp, which are most similar to the samples of our text classification task. We use a Euclidean similarity metric for this sample selection strategy. We call this LM ClimateBert_{S}.</li>
<li>In Div-Select, we select the 70% of samples from Corp, which are most diverse compared to the samples from our text classification task. We use the sum between the type-token-ratio and the Shannon-entropy for measuring diversity (Ruder and Plank 2017). This LM is named ClimateBert_{D}.</li>
<li>In Div-Select + Sim-Select, we use the same diversity and similarity metrics as before. We then compute a composite score by summing over their scaled values. We keep the 70% of the samples with the highest composite score to train ClimateBert_{D+S}.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Downstream domain- <br> adaptive pretraining</th>
<th style="text-align: center;">Downstream tasks <br> training</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hyperparameter</td>
<td style="text-align: center;">Value</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">2016</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Number of epochs</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">Patience</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Class weight</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Balanced</td>
</tr>
<tr>
<td style="text-align: left;">Feedforward nonlinearity</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">tanh</td>
</tr>
<tr>
<td style="text-align: left;">Feedforward layer</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Output neurons</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Task dependent</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam beta weights</td>
<td style="text-align: center;">$(0.9,0.999)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Learning rate scheduler</td>
<td style="text-align: center;">Warmup linear</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Weight decay</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Hyperparameters used for the downstream domainadaptive pretraining and the downstream tasks training of Climatebert.</p>
<h2>Vocabulary Augmentation</h2>
<p>We extend the existing vocabulary of the original model to include domain-specific terminology. This allows Climatebert to explicitly learn representations of terminology that frequently occur in a climate-related text but not in the general domain. In particular, we add the 235 most common tokens as new tokens to the tokenizer, thereby extending the size of the vocabulary for our basis language model (DistilRoBERTA) from 50,265 to 50,500. See Appendix C for a list of all added tokens. We also experimented with language models that do not use vocabulary augmentation or add more tokens. However, overall we find improvements using this technique and, thus, apply it to all language models which we pretrain on the climate domain.</p>
<h2>Model Selection</h2>
<p>For all our experiments, we use DistilRoBERTA, a distilled version of RoBERTA from Huggingface, ${ }^{3}$ as our starting point for training (Sanh et al. 2019). All our language models are trained with a masked language modeling objective (i.e., cross-entropy loss on predicting randomly masked tokens). We report all hyperparameters in Table 2. The large batch size of 2016 for training the LM is achieved using gradient accumulation.</p>
<h2>Training on Downstream Task</h2>
<p>After pretraining DistilRoBERTA on CORP, we follow standard practice (Devlin et al. 2018) and pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction. We report all hyperparameters of this feedforward layer in Table 2.</p>
<h2>4 Performance Analysis of Language Model</h2>
<p>Table 3 lists the results after pretraining DistilRoBERTA on CORP with various sample selection strategies. For evaluation, we split CORP randomly into $80 \%$ training data and $20 \%$ validation data. The reported loss is the cross-entropy</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>loss on predicting randomly masked tokens from the validation data. We find that ClimateBERT ${ }_{F}$ leads to the lowest validation loss. This performance is followed by the other Climatebert LMs, which all show similar results. Overall, we find that our domain-adaptive pretraining decreases the cross-entropy loss by $46-48 \%$ compared to the basis DistilRoBERTA, cutting the loss almost in half.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Val. loss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DistilRoBERTA</td>
<td style="text-align: center;">2.238</td>
</tr>
<tr>
<td style="text-align: left;">ClimateBERT $_{F}$</td>
<td style="text-align: center;">1.157</td>
</tr>
<tr>
<td style="text-align: left;">ClimateBERT $_{S}$</td>
<td style="text-align: center;">1.205</td>
</tr>
<tr>
<td style="text-align: left;">ClimateBERT $_{D}$</td>
<td style="text-align: center;">1.204</td>
</tr>
<tr>
<td style="text-align: left;">ClimateBERT $_{D+S}$</td>
<td style="text-align: center;">1.203</td>
</tr>
</tbody>
</table>
<p>Table 3: Loss of our language models on a validation set from our text corpus CORP.</p>
<h2>5 Performance Analysis for Climate-Related Downstream Tasks</h2>
<p>For our experiments, we used the following downstream tasks: text classification, sentiment analysis, and factchecking. Table 4 lists basic statistics about the downstream tasks. We repeated the training and evaluation phase 60 times for each experiment, each time with a random $90 \%$ set of samples for training and the remaining $10 \%$ set for validation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Downstream <br> task</th>
<th style="text-align: center;">Num. of <br> samples</th>
<th style="text-align: left;">Labels</th>
<th style="text-align: center;">Label <br> distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Text classification</td>
<td style="text-align: center;">1220</td>
<td style="text-align: left;">climate-related: yes/no</td>
<td style="text-align: center;">$1000 / 220$</td>
</tr>
<tr>
<td style="text-align: left;">Sentiment analysis</td>
<td style="text-align: center;">1000</td>
<td style="text-align: left;">opportunity/neutral/risk</td>
<td style="text-align: center;">$250 / 408 / 342$</td>
</tr>
<tr>
<td style="text-align: left;">Fact-checking</td>
<td style="text-align: center;">2745</td>
<td style="text-align: left;">claim: support/refute</td>
<td style="text-align: center;">$1943 / 802$</td>
</tr>
</tbody>
</table>
<p>Table 4: Overview of our downstream tasks used for evaluating Climatebert.</p>
<h2>Text Classification</h2>
<p>For our text classification experiment, we use a dataset consisting of hand-selected paragraphs from companies' annual reports or sustainability reports. All paragraphs were annotated as yes (climate-related) or no (not climate-related) by at least four experts from the field using the software prodigy. ${ }^{4}$ See Appendix B for our annotation guidelines. In case of a close verdict or a tie between the annotators, the authors of this paper discussed the paragraph in depth before reaching an agreement.</p>
<p>In the following, Table 5 reports the results of the language models when trained on our text classification task, i.e., whether the text is climate-related or not. Overall, we find that all ClimateBert LMs outperform a non-pre-trained DistilRoBERTA across both metrics for the text classification task. Most notably, domain-adaptive pretraining with similar samples to our downstream tasks</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>(ClimateBERT_{S}) leads to improvements of 32.64% in terms of cross-entropy loss and a reduction in the error rate of the F1 score by 35.71%.</p>
<p>|  | Text classification |  |
| Model | Loss | F1 |
| --- | --- | --- |
| DistilRoBERTA | 0.242_{0.171} | 0.986_{0.010} |
| ClimateBERT_{F} | 0.191_{0.136} | 0.989_{0.010} |
| ClimateBERT_{S} | 0.163_{0.132} | 0.991_{0.008} |
| ClimateBERT_{D} | 0.197_{0.153} | 0.988_{0.009} |
| ClimateBERT_{D+S} | 0.217_{0.153} | 0.988_{0.009} |</p>
<p>Table 5: Results on our text classification task. Reported are the average cross-entropy loss and the average weighted F1 score on the validation sets across 60 evaluation runs. Value subscripts report the standard deviations.</p>
<h2>Sentiment Analysis</h2>
<p>Our next task studies the sentiment behind the climaterelated paragraphs, using the same dataset as in the previous section. In our context, we use the term 'sentiment' to distinguish whether an entity reports on climate-related developments as negative <em>risk</em>, as positive <em>opportunity</em>, or as <em>neutral</em>.</p>
<p>Therefore, we created a second labeled dataset on climaterelated sentiment, for which we asked the annotators to label the paragraphs by one of the three categories — <em>risk</em>, <em>neutral</em>, or <em>opportunity</em>. See Appendix B for our annotation guidelines. Similarly, as before, in case of a close verdict or a tie between the annotators, the authors of this paper discussed the paragraph in depth before reaching an agreement.</p>
<p>Table 6 shows the performance of our models in sentiment prediction. Again, all ClimateBERT LMs outperform the DistilRoBERTA baseline model in terms of F1 score and average cross-entropy loss. The largest improvements can be observed with ClimateBERT_{F}, which amount to a 7.33% lower cross-entropy loss and a 7.42% lower error rate in terms of average F1 score compared to the DistilRoBERTA baseline LM.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>Sentiment analysis</td>
<td></td>
</tr>
<tr>
<td>DistilRoBERTA</td>
<td>0.150_{0.069}</td>
<td>0.825_{0.046}</td>
</tr>
<tr>
<td>ClimateBERT_{F}</td>
<td>0.139_{0.042}</td>
<td>0.838_{0.036}</td>
</tr>
<tr>
<td>ClimateBERT_{S}</td>
<td>0.140_{0.057}</td>
<td>0.836_{0.033}</td>
</tr>
<tr>
<td>ClimateBERT_{D}</td>
<td>0.138_{0.043}</td>
<td>0.835_{0.040}</td>
</tr>
<tr>
<td>ClimateBERT_{D+S}</td>
<td>0.139_{0.043}</td>
<td>0.834_{0.036}</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on our sentiment analysis task in terms of average validation loss and average weighted F1 score across 60 evaluation runs. Subscripts report the standard deviations.</p>
<h2>Fact-Checking</h2>
<p>We now turn to the fact-checking downstream task. We apply our model to a dataset that was proposed by Diggelmann et al. (2020) and comprises 1.5k sentences that make a claim about climate-related topics. This CLIMATE-FEVER dataset is to the best of our knowledge to date the only dataset that focuses on climate change fact-checking. CLIMATE-FEVER adapts the methodology of FEVER, the largest dataset of artificially designed claims, to real-life claims on climate change collected online. The authors of CLIMATE-FEVER find that the surprising, subtle complexity of modeling real-world climate-related claims provides a valuable challenge for general natural language understanding. Working with this dataset, Wang, Chillrud, and McKeown (2021) recently introduced a novel semi-supervised training method to achieve a state-of-the-art (SotA) F1 score of 0.7182 on the fact-checking dataset CLIMATE-FEVER.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Claim:</th>
<th style="text-align: left;">97% consensus on human-caused</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">global warming has been disproven.</td>
</tr>
<tr>
<td style="text-align: left;">Evidence:</td>
<td style="text-align: left;">In a 2019 CBS poll, 64% of the US</td>
</tr>
<tr>
<td style="text-align: left;">REFUTE</td>
<td style="text-align: left;">population said that climate change</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">is a ""crisis"" or a ""serious prob-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">lem"", with 44% saying human ac-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">tivity was a significant contributor.</td>
</tr>
<tr>
<td style="text-align: left;">Claim:</td>
<td style="text-align: left;">The melting Greenland ice sheet is</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">already a major contributor to ris-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ing sea level and if it was eventually lost entirely, the oceans would</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">rise by six metres around the world,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">flooding many of the world's largest</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">cities.</td>
</tr>
<tr>
<td style="text-align: left;">Evidence:</td>
<td style="text-align: left;">The Greenland ice sheet occupies</td>
</tr>
<tr>
<td style="text-align: left;">SUPPORT</td>
<td style="text-align: left;">about 82% of the surface of Green-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">land, and if melted would cause sea</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">levels to rise by 7.2 metres.</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples taken from CLIMATE-FEVER.</p>
<p>Each claim in CLIMATE-FEVER is supported or refuted by evidence sentences (see Table 7), and an evidence sentence can also be classified as giving not enough information. The objective of the model is to classify an evidence sentence to <em>support</em> or <em>refute</em> a claim. To feed this combination of claim and evidence into the model, we concatenate the claims with the related evidence sentences, with a [SEP] token separating them. As in Wang, Chillrud, and McKeown (2021), and for comparison with their results, we filter out all evidence sentences with the label <em>NOT_ENOUGH_INFO</em> in the CLIMATE-FEVER dataset.</p>
<p>Table 8 lists the results of our experiments on the CLIMATE-FEVER dataset. In line with our previous experiments, we find similar or better results for all CLIMATE-BERT LMs across all metrics. Our CLIMATEBERT_{D+S} LM achieves similar cross-entropy loss compared to the basis DistilRoBERTA model, yet pushes the average F1 score from 0.748 to 0.757, which outperforms Wang, Chillrud, and McKeown (2021)'s previous SotA F1 score of 0.7182, and</p>
<p>is hence, to the best of our knowledge, the new SotA on this dataset.</p>
<p>|  | Fact-checking |  |
| Model | Loss | F1 |
| --- | --- | --- |
| DistilRoBERTA | $0.135_{0.017}$ | $0.748_{0.036}$ |
| ClimateBERT ${ }<em 0.020="0.020">{F}$ | $0.134</em>$ |
| ClimateBERT ${ }}$ | $0.755_{0.037<em 0.017="0.017">{S}$ | $0.133</em>$ |
| ClimateBERT ${ }}$ | $0.753_{0.042<em 0.016="0.016">{D}$ | $0.135</em>$ |
| ClimateBERT ${ }}$ | $0.752_{0.042<em 0.018="0.018">{D+S}$ | $0.135</em>$ |}$ | $0.757_{0.044</p>
<p>Table 8: Results on our fact-checking task on ClimateFEVER in terms of average validation loss and average weighted F1 score across 60 evaluation runs. Subscripts report the standard deviations.</p>
<h2>6 Carbon Footprint</h2>
<p>Training deep neural networks in general and large language models in particular, has a significant carbon footprint already today. If the LM research trends continue, this detrimental climate impact will increase considerably. The topic of efficient NLP was also discussed by a working group appointed by the ACL Executive Committee to promote ways that the ACL community can reduce the computational costs of model training (https://public.ukp.informatik.tudarmstadt.de/enlp/Efficient-NLP-policy-document.pdf).
We acknowledge that our work is part of this trend. In total, training ClimateBert caused 115.15 kg CO2 emissions. We use two energy efficient NVIDIA RTX A5000 GPUs: 0.7 kW (power consumption of GPU server) x 350 hours (combined training time of all experiments) x 470 gCO2e/kWh (emission factor in Germany in 2018 according to www.umweltbundesamt.de/publikationen/entwicklung-der-spezifischen-kohlendioxid-7) = 115,149 gCO2e. We list all details about our climate impact in Table 9 in Appendix A. Nevertheless, we decided to carry out this project, as we see the high potential of NLP to support action against climate change. Given our awareness of the carbon footprint of our research, we address this sensitive topic as follows:</p>
<ol>
<li>We specifically decided to focus on DistilRoBERTA, which is a considerably smaller model in terms of number of parameters compared to the non-distilled version and, thus, requires less energy to train. Moreover, we do not crawl huge amounts of data without considering the quality. This way, we try to take into account the issues mentioned by Bender et al. (2021).</li>
<li>Hyperparameter tuning yields considerably higher CO2 emissions in the training stage due to tens or hundreds of different training runs. Note that our multiple training runs on the downstream task are not causing long training times as the downstream datasets are very small compared to the dataset used for training the language model. We therefore refrain from exhaustive hyperparameter tuning. Rather, we build on previous findings.</li>
</ol>
<p>We systematically experimented with a few hyperparameter combinations and found that the hyperparameters proposed by Gururangan et al. (2020) lead to the best results.
3. We would have liked to train and run our model on servers powered by renewable energy. This first best option was unfortunately not available. In order to speed up the energy system transformation required to achieve the global climate targets, we contribute our part by donating Euro 100 to atmosfair. atmosfair was founded in 2005 and is supported by the German Federal Environment Agency. atmosfair offsets carbon dioxide in more than 20 locations: from efficient cookstoves in Nigeria, Ethiopia and India to biogas plants in Nepal and Thailand to solar energy in Senegal and Brazil and renewable energies in Tansania and Indonesia. See www.atmosfair. de/en/offset/fix/. We explicitly refrain from calling this donation a CO2 compensation, and we refrain from a solution that is based on afforestation.</p>
<h2>7 Conclusion</h2>
<p>We propose ClimateBert, the first language model that was pretrained on a large scale dataset of over 2 million climate-related paragraphs. We study various selection strategies to find samples from our corpus which are most helpful for later tasks. Our experiments reveal that our domain-adaptive pretraining leads to considerably lower masked language modeling loss on our climate corpus. We further find that this improvement is also reflected in predictive performance across three essential downstream climaterelated NLP tasks: text classification, the analysis of risk and opportunity statements by corporations, and fact-checking climate-related claims.</p>
<h2>Acknowledgments</h2>
<p>We are very thankful to Jan Minx and Max Callaghan from the Mercator Research Institute on Global Commons and Climate Change (MCC) Berlin for providing us with the data, which is a subset of the data they used in Berrang-Ford et al. (2021) and Callaghan et al. (2021).</p>
<h2>References</h2>
<p>Araci, D. 2019. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint arXiv:1908.10063.
Bender, E. M.; Gebru, T.; McMillan-Major, A.; and Shmitchell, S. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623.
Berrang-Ford, L.; Sietsma, A. J.; Callaghan, M.; Minx, J. C.; Scheelbeek, P. F.; Haddaway, N. R.; Haines, A.; and Dangour, A. D. 2021. Systematic mapping of global research on climate and health: a machine learning review. The Lancet Planetary Health, 5(8): e514-e525.</p>
<p>Bingler, J. A.; Kraus, M.; Leippold, M.; and Webersinke, N. 2022a. Cheap talk and cherry-picking: What ClimateBERT has to say on corporate climate risk disclosures. Finance Research Letters, 102776.
Bingler, J. A.; Kraus, M.; Leippold, M.; and Webersinke, N. 2022b. Cheap talk in corporate climate commitments: The role of active institutional ownership, signaling, materiality, and sentiment. Swiss Finance Institute Research Paper.
Callaghan, M.; Schleussner, C.-F.; Nath, S.; Lejeune, Q.; Knutson, T. R.; Reichstein, M.; Hansen, G.; Theokritoff, E.; Andrijevic, M.; Brecha, R. J.; et al. 2021. Machine-learningbased evidence and attribution mapping of 100,000 climate impact studies. Nature climate change, 11(11): 966-972.
Chalkidis, I.; Fergadiotis, M.; Malakasiotis, P.; Aletras, N.; and Androutsopoulos, I. 2020. LEGAL-BERT: The muppets straight out of law school. arXiv preprint arXiv:2010.02559.
Clark, K.; Luong, M.-T.; Le, Q. V.; and Manning, C. D. 2020. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In International Conference on Learning Representations.
Cody, E. M.; Reagan, A. J.; Mitchell, L.; Dodds, P. S.; and Danforth, C. M. 2015. Climate Change Sentiment on Twitter: An Unsolicited Public Opinion Poll. PLOS ONE, 10(8): e0136092.
Collobert, R.; and Weston, J. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of the 25th International Conference on Machine Learning, 160-167.
Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J. G.; Le, Q.; and Salakhutdinov, R. 2019. Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2978-2988.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Diggelmann, T.; Boyd-Graber, J.; Bulian, J.; Ciaramita, M.; and Leippold, M. 2020. CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims. arXiv preprint arXiv:2012.00614.
Friederich, D.; Kaack, L. H.; Luccioni, A.; and Steffen, B. 2021. Automated Identification of Climate Risk Disclosures in Annual Corporate Reports. arXiv preprint arXiv:2108.01415.
Gokaslan, A.; and Cohen, V. 2019. OpenWebText Corpus.
Grüning, M. 2011. Artificial intelligence measurement of disclosure (AIMD). European Accounting Review, 20(3): 485-519.
Gururangan, S.; Marasović, A.; Swayamdipta, S.; Lo, K.; Beltagy, I.; Downey, D.; and Smith, N. A. 2020. Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8342-8360.
Hershcovich, D.; Webersinke, N.; Kraus, M.; Bingler, J. A.; and Leippold, M. 2022. Towards Climate Awareness in NLP Research. arXiv preprint arXiv:2205.05071.</p>
<p>Howard, J.; and Ruder, S. 2018. Universal Language Model Fine-tuning for Text Classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 328-339.
Kim, D.-Y.; and Kang, S.-W. 2018. Analysis of Recognition of Climate Changes using Word2Vec. International Journal of Pure and Applied Mathematics, 120(6): 5793-5807.
Kölbel, J. F.; Leippold, M.; Rillaerts, J.; and Wang, Q. 2020. Ask BERT: How regulatory disclosure of transition and physical climate risks affects the CDS term structure. Available at SSRN 3616324.
Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and Kang, J. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4): 1234-1240.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Luccioni, A.; Baylor, E.; and Duchene, N. 2020. Analyzing Sustainability Reports Using Natural Language Processing. arXiv preprint arXiv:2011.08073.
Nagel, S. 2016. CC-NEWS.
Rasmy, L.; Xiang, Y.; Xie, Z.; Tao, C.; and Zhi, D. 2021. Med-BERT: pretrained contextualized embeddings on largescale structured electronic health records for disease prediction. NPJ digital medicine, 4(1): 1-13.
Ruder, S.; and Plank, B. 2017. Learning to select data for transfer learning with Bayesian Optimization. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 372-382.
Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.
Sautner, Z.; van Lent, L.; Vilkov, G.; and Zhang, R. 2022. Firm-level Climate Change Exposure. Journal of Finance, forthcoming.
Stammbach, D.; Webersinke, N.; Bingler, J. A.; Kraus, M.; and Leippold, M. 2022. A Dataset for Detecting Real-World Environmental Claims. arXiv preprint arXiv:2209.00507.
Trinh, T. H.; and Le, Q. V. 2019. A Simple Method for Commonsense Reasoning. arXiv preprint arXiv:1806.02847.
Varini, F. S.; Boyd-Graber, J.; Ciaramita, M.; and Leippold, M. 2020. ClimaText: A dataset for climate change topic detection. In Tackling Climate Change with Machine Learning (Climate Change AI) workshop at NeurIPS.
Wang, G.; Chillrud, L.; and McKeown, K. 2021. Evidence based Automatic Fact-Checking for Climate Change Misinformation. International Workshop on Social Sensing on The International AAAI Conference on Web and Social Media.
Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, 19-27.</p>
<h2>Appendix</h2>
<h2>A Climate Performance Model Card</h2>
<p>Table 9 shows our climate performance model card, following Hershcovich et al. (2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ClimateBert</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1. Model publicly available?</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">2. Time to train final model</td>
<td style="text-align: left;">48 hours</td>
</tr>
<tr>
<td style="text-align: left;">3. Time for all experiments</td>
<td style="text-align: left;">350 hours</td>
</tr>
<tr>
<td style="text-align: left;">4. Power of GPU and CPU</td>
<td style="text-align: left;">0.7 kW</td>
</tr>
<tr>
<td style="text-align: left;">5. Location for computations</td>
<td style="text-align: left;">Germany</td>
</tr>
<tr>
<td style="text-align: left;">6. Energy mix at location</td>
<td style="text-align: left;">$470 \mathrm{gCO} 2 \mathrm{eq} / \mathrm{kWh}$</td>
</tr>
<tr>
<td style="text-align: left;">7. CO2eq for final model</td>
<td style="text-align: left;">15.79 kg</td>
</tr>
<tr>
<td style="text-align: left;">8. CO2eq for all experiments</td>
<td style="text-align: left;">115.15 kg</td>
</tr>
<tr>
<td style="text-align: left;">9. Average CO2eq for inference per sample</td>
<td style="text-align: left;">0.62 mg</td>
</tr>
</tbody>
</table>
<p>Table 9: Climate performance model card for ClimateBert.</p>
<h2>B Annotation Guidelines</h2>
<p>For our annotation procedure, we implemented the following general rules. The annotators had to label climaterelevant paragraphs. If the paragraph was climate-relevant, then they had to attach a sentiment to a paragraph. Annotators were asked to apply common sense, e.g., when a given paragraph might not provide all the context, but the context might seem obvious. Moreover, annotators were informed that each annotation should be a $0-1$ decision. Hence, if an annotator was $70 \%$ certain, then this was rounded up to $100 \%$. We asked, on average, five researchers to annotate the same tasks to obtain some measure of dispersion. In case of a close verdict or a tie between the annotators, the authors of this paper discussed the paragraph in depth before reaching an agreement.</p>
<h2>Text classification</h2>
<p>The first task was to label climate-relevant paragraphs. The labels are Yes or No. As a general rule, we determined that just discussing nature/environment can be sufficient, and mentioning clean energy, emissions, fossil fuels, etc., can also be sufficient. It is a Yes, if the paragraph includes some wording on a climate change or environment related topic (including transition and litigation risks, i.e., emission mitigation measures, energy consumption and energy sources etc.; and physical risks, i.e., increase in risk of floods, coastal area exposure, storms etc.). It is a No, if the paragraph is not related to climate policy, climate change or an environmental topic at all. For some examples, see Table 10.</p>
<h2>Sentiment Analysis</h2>
<p>For the sentiment analysis, annotators had to provide labels as to whether a (climate change-related) paragraph talks about a Risk or threat that negatively impacts an entity of interest, i.e. a company (negative sentiment), or whether an entity is referring to some Opportunity arising due to climate change (positive sentiment). The paragraph can also make just a Neutral statement.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Label</th>
<th style="text-align: left;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Sustainability: The Group is subject <br> to stringent and evolving laws, reg- <br> ulations, standards and best prac- <br> tices in the area of sustainabil- <br> ity (comprising corporate gover- <br> nance, environmental management <br> and climate change (specifically <br> capping of emissions), health and <br> safety management and social per- <br> formance) which may give rise <br> to increased ongoing remediation <br> and/or other compliance costs and <br> may adversely affect the Group's <br> business, results of operations, fi- <br> nancial condition and/or prospects.</td>
</tr>
<tr>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Scope 3: Optional scope that in- <br> cludes indirect emissions associ- <br> ated with the goods and services <br> supply chain produced outside the <br> organization. Included are emis- <br> sions from the transport of products <br> from our logistics centres to stores <br> (downstream) performed by exter- <br> nal logistics operators (air, land <br> and sea transport) as well as the <br> emissions associated with electric- <br> ity consumption in franchise stores.</td>
</tr>
<tr>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Risk and risk management Opera- <br> tional risk and compliance risk Op- <br> erational risk is the risk of loss re- <br> sulting from inadequate or failed <br> internal processes, people and sys- <br> tems, or from external events in- <br> cluding legal risk but excluding <br> strategic and reputation risk. It also <br> includes, among other things, tech- <br> nology risk, model risk and out- <br> sourcing risk.</td>
</tr>
</tbody>
</table>
<p>Table 10: Examples for the annotation task climate (Yes/No).</p>
<p>To be more precise, we consider a paragraph relating to risk, if the paragraph mainly talks about 1) business downside risks, potential losses and adverse developments detrimental to the entity 2) and/or about negative impact of an entity's activities on the society/environment 3) and/or associates specific negative adjectives to the anticipated, past or present developments and topics covered.</p>
<p>We consider a paragraph relating to opportunities, if the paragraph mainly talks about 1) business opportunities arising from mitigating climate change, from adapting to climate change etc. which might be beneficial for a specific entity 2) and/or about positive impact of an entity's activities on the society/environment 3) and/or associates specific</p>
<p>positive adjectives to the anticipated, past or present developments and topics covered.</p>
<p>Lastly, we consider a paragraph as neutral if it mainly states facts and developments 1) without putting them into positive or negative perspective for a specific entity and/or the society and/or the environment, 2) and/or does not associate specific positive or negative adjectives to the anticipated, past or present facts stated and topics covered. For some examples, see Table 11.</p>
<h2>C Added Tokens</h2>
<p>'CO2', 'emissions', "", 'temperature', 'environmental', 'soil', 'increase', 'conditions', 'potential', 'increased', 'areas', 'degrees', 'across', 'systems', 'emission', 'precipitation', 'impacts', 'compared', 'countries', 'sustainable', 'provide', 'reduction', 'annual', 'reduce', 'greenhouse', 'approach', 'processes', 'factors', 'observed', 'renewable', 'temperatures', 'distribution', 'studies', 'variability', 'significantly', '--', 'further', 'regions', 'addition', 'showed', "", 'industry', 'consumption', 'regional', 'risks', 'atmospheric', 'supply', 'companies', 'plants', 'biomass', 'electricity', 'respectively', 'activities', 'communities', 'climatic', 'solar', 'investment', 'spatial', 'rainfall', '$\cdot$', 'sustainability', 'costs', 'reduced', '2021', 'influence', 'vegetation', 'sources', 'possible', 'ecosystem', 'scenarios', 'summer', 'drought', 'structure', 'economy', 'considered', 'various', 'atmosphere', 'several', 'technologies', 'transition', 'assessment', 'dioxide', 'ocean', 'fossil', 'patterns', 'waste', 'solutions', 'transport', 'strategy', 'CH4', 'policies', 'understanding', 'concentration', 'customers', 'methane', 'applied', 'increases', 'estimated', 'flood', 'measured', 'thermal', 'concentrations', 'decrease', 'greater', 'following', 'proposed', 'trends', 'basis', 'provides', 'operations', 'differences', 'hydrogen', 'adaptation', 'methods', 'capture', 'variation', 'reducing', 'N2O', 'parameters', 'ecosystems', 'investigated', 'yield', 'strategies', 'indicate', 'caused', 'dynamics', 'obtained', 'efforts', 'coastal', 'become', 'agricultural', 'decreased', 'GHG', 'materials', 'mainly', 'relationship', 'ecological', 'benefits', '+/-', 'challenges', 'nitrogen', 'forests', 'trend', 'estimates', 'towards', 'Committee', 'seasonal', 'developing', 'particular', 'importance', 'tropical', 'ratio', '2030', 'composition', 'employees', 'characteristics', 'scenario', 'measurements', 'plans', 'fuels', 'infrastructure', 'overall', 'responses', 'presented', 'least', 'assess', 'diversity', 'periods', 'delta', 'included', 'already', 'targets', 'achieve', 'affect', 'conducted', 'operating', 'populations', 'variations', 'studied', 'additional', 'construction', 'northern', 'variables', 'soils', 'ensure', 'recovery', 'combined', 'decision', 'practices', 'however', 'determined', 'resulting', 'mitigation', 'conservation', 'estimate', 'identify', 'observations', 'losses', 'productivity', 'agreement', 'monitoring', 'investments', 'pollution', 'contribution', 'opportunities', 'simulations', 'gases', 'statements', 'planning', 'shares', 'sediment', 'flux', 'requirements', 'trees', 'temporal', 'determine', 'southern', 'previous', 'integrated', 'relatively', 'analyses', 'means', '2050', "?", 'uncertainty', 'pandemic', 'fluxes', 'findings', 'moisture', 'consistent', 'decades', 'snow', 'performed', 'contribute', 'crisis'</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Label</th>
<th style="text-align: center;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Opportunity</td>
<td style="text-align: center;">Grid \&amp; Infrastructure and Retail - today represent the energy world of tomorrow. We rank among Europe's market leaders in the grid and retail business and have leading positions in renewables. We intend to spend a total of between Euro 6.5 billion and Euro 7.0 billion in capital throughout the Group from 2017 to 2019.</td>
</tr>
<tr>
<td style="text-align: center;">Opportunity</td>
<td style="text-align: center;">We want to contribute to the transition to a circular economy. The linear economy is not sustainable. We discard a great deal (waste and therefore raw materials, experience, social capital and knowledge) and are squandering value as a result. This is not tenable from an economic and ecological perspective. As investor we can 'direct' companies and with our network, our scale and our influence we can help the movement towards a circular future (creating a sustainable society) further along.</td>
</tr>
<tr>
<td style="text-align: center;">Neutral</td>
<td style="text-align: center;">A similar approach could be used for allocating emissions in the fossil fuel electricity supply chain between coal miners, transporters and generators. We don't invest in fossil fuel companies, but those investors who do should account properly for their role in the production of dangerous emissions from burning fossil fuels.</td>
</tr>
<tr>
<td style="text-align: center;">Neutral</td>
<td style="text-align: center;">Omissions: Emissions associated with joint ventures and investments are not included in the emissions disclosure as they fall outside the scope of our operational boundary. We do not have any emissions associated with heat, steam or cooling. We are not aware of any other material sources of omissions from our emissions reporting.</td>
</tr>
<tr>
<td style="text-align: center;">Risk</td>
<td style="text-align: center;">We estimated that between 36.5 and 52.9 per cent of loans granted to our clients are exposed to transition risks. If the regulator decides to pass ambitious laws to accelerate the transition towards a low-carbon economy, carbon-intensive companies would incur in higher costs, which may prevent them from repaying their debt. In turn, this would weaken our bank's balance sheets. .</td>
</tr>
<tr>
<td style="text-align: center;">Risk</td>
<td style="text-align: center;">American National Insurance Company recognizes that increased claims activity resulting from catastrophic events, whether natural or man-made, may result in significant losses, and that climate change may also affect the affordability and availability of property and casualty insurance and the pricing for such products.</td>
</tr>
</tbody>
</table>
<p>Table 11: Examples for the annotation task sentiment (Opportunity/Neutral/Risk).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ www.huggingface.co/distilroberta-base&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ www.prodi.gy&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8964 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8964</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8964</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e816f788767eec6a8ef0ea9eddd0e902435d4271</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271" target="_blank">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is consistently found that multi-phase adaptive pretraining offers large gains in task performance, and it is shown that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable.</p>
                <p><strong>Paper Abstract:</strong> Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8964",
    "paper_id": "paper-e816f788767eec6a8ef0ea9eddd0e902435d4271",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00626225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</h1>
<p>Suchin Gururangan ${ }^{\dagger}$ Ana Marasović ${ }^{\dagger \diamond}$ Swabha Swayamdipta ${ }^{\dagger}$<br>Kyle Lo ${ }^{\dagger}$ Iz Beltagy ${ }^{\dagger}$ Doug Downey ${ }^{\dagger}$ Noah A. Smith ${ }^{\dagger \diamond}$<br>${ }^{\dagger}$ Allen Institute for Artificial Intelligence, Seattle, WA, USA<br>${ }^{\circ}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington, Seattle, WA, USA<br>{suching, anam, swabhas, kylel, beltagy, dougd, noah}@allenai.org</p>
<h4>Abstract</h4>
<p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p>
<h2>1 Introduction</h2>
<p>Today's pretrained language models are trained on massive, heterogeneous corpora (Raffel et al., 2019; Yang et al., 2019). For instance, ROBERTA (Liu et al., 2019) was trained on over 160GB of uncompressed text, with sources ranging from Englishlanguage encyclopedic and news articles, to literary works and web content. Representations learned by such models achieve strong performance across many tasks with datasets of varying sizes drawn from a variety of sources (e.g., Wang et al., 2018, 2019). This leads us to ask whether a task's textual domain-a term typically used to denote a distribution over language characterizing a given topic or genre (such as "science" or "mystery novels")—is still relevant. Do the latest large pretrained models work universally or is it still helpful to build
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of data distributions. Task data is comprised of an observable task distribution, usually non-randomly sampled from a wider distribution (light grey ellipsis) within an even larger target domain, which is not necessarily one of the domains included in the original LM pretraining domain - though overlap is possible. We explore the benefits of continued pretraining on data from the task distribution and the domain distribution.
separate pretrained models for specific domains?
While some studies have shown the benefit of continued pretraining on domain-specific unlabeled data (e.g., Lee et al., 2019), these studies only consider a single domain at a time and use a language model that is pretrained on a smaller and less diverse corpus than the most recent language models. Moreover, it is not known how the benefit of continued pretraining may vary with factors like the amount of available labeled task data, or the proximity of the target domain to the original pretraining corpus (see Figure 1).</p>
<p>We address this question for one such highperforming model, ROBERTA (Liu et al., 2019) (§2). We consider four domains (biomedical and computer science publications, news, and reviews; §3) and eight classification tasks (two in each domain). For targets that are not already in-domain for ROBERTA, our experiments show that contin-</p>
<p>ued pretraining on the domain (which we refer to as domain-adaptive pretraining or DAPT) consistently improves performance on tasks from the target domain, in both high- and low-resource settings.</p>
<p>Above, we consider domains defined around genres and forums, but it is also possible to induce a domain from a given corpus used for a task, such as the one used in supervised training of a model. This raises the question of whether pretraining on a corpus more directly tied to the task can further improve performance. We study how domainadaptive pretraining compares to task-adaptive pretraining, or TAPT, on a smaller but directly taskrelevant corpus: the unlabeled task dataset (§4), drawn from the task distribution. Task-adaptive pretraining has been shown effective (Howard and Ruder, 2018), but is not typically used with the most recent models. We find that TAPT provides a large performance boost for ROBERTA, with or without domain-adaptive pretraining.</p>
<p>Finally, we show that the benefits from taskadaptive pretraining increase when we have additional unlabeled data from the task distribution that has been manually curated by task designers or annotators. Inspired by this success, we propose ways to automatically select additional task-relevant unlabeled text, and show how this improves performance in certain low-resource cases (§5). On all tasks, our results using adaptive pretraining techniques are competitive with the state of the art.</p>
<p>In summary, our contributions include:</p>
<ul>
<li>a thorough analysis of domain- and taskadaptive pretraining across four domains and eight tasks, spanning low- and high-resource settings;</li>
<li>an investigation into the transferability of adapted LMs across domains and tasks; and</li>
<li>a study highlighting the importance of pretraining on human-curated datasets, and a simple data selection strategy to automatically approach this performance.
Our code as well as pretrained models for multiple domains and tasks are publicly available. ${ }^{1}$</li>
</ul>
<h2>2 Background: Pretraining</h2>
<p>Learning for most NLP research systems since 2018 consists of training in two stages. First, a neural language model (LM), often with millions of parameters, is trained on large unlabeled cor-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>pora. The word (or wordpiece; Wu et al. 2016) representations learned in the pretrained model are then reused in supervised training for a downstream task, with optional updates (fine-tuning) of the representations and network from the first stage.</p>
<p>One such pretrained LM is ROBERTA (Liu et al., 2019), which uses the same transformerbased architecture (Vaswani et al., 2017) as its predecessor, BERT (Devlin et al., 2019). It is trained with a masked language modeling objective (i.e., cross-entropy loss on predicting randomly masked tokens). The unlabeled pretraining corpus for ROBERTA contains over 160 GB of uncompressed raw text from different English-language corpora (see Appendix §A.1). ROBERTA attains better performance on an assortment of tasks than its predecessors, making it our baseline of choice.</p>
<p>Although ROBERTA's pretraining corpus is derived from multiple sources, it has not yet been established if these sources are diverse enough to generalize to most of the variation in the English language. In other words, we would like to understand what is out of ROBERTA's domain. Towards this end, we explore further adaptation by continued pretraining of this large LM into two categories of unlabeled data: (i) large corpora of domain-specific text (§3), and (ii) available unlabeled data associated with a given task (§4).</p>
<h2>3 Domain-Adaptive Pretraining</h2>
<p>Our approach to domain-adaptive pretraining (DAPT) is straightforward-we continue pretraining ROBERTA on a large corpus of unlabeled domain-specific text. The four domains we focus on are biomedical (BIOMED) papers, computer science (CS) papers, newstext from REALNEWS, and AMAZON reviews. We choose these domains because they have been popular in previous work, and datasets for text classification are available in each. Table 1 lists the specifics of the unlabeled datasets in all four domains, as well as ROBERTA's training corpus. ${ }^{1}$</p>
<h3>3.1 Analyzing Domain Similarity</h3>
<p>Before performing DAPT, we attempt to quantify the similarity of the target domain to ROBERTA's pretraining domain. We consider domain vocabularies containing the top 10 K most frequent unigrams (excluding stopwords) in comparably sized</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Pretraining Corpus</th>
<th style="text-align: center;"># Tokens</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">$\mathcal{L}_{\text {RoB. }}$</th>
<th style="text-align: center;">$\mathcal{L}_{\text {DAPT }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BIOMED</td>
<td style="text-align: center;">2.68M full-text papers from S2ORC (Lo et al., 2020)</td>
<td style="text-align: center;">7.55B</td>
<td style="text-align: center;">47GB</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">2.22M full-text papers from S2ORC (Lo et al., 2020)</td>
<td style="text-align: center;">8.10B</td>
<td style="text-align: center;">48GB</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">1.34</td>
</tr>
<tr>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">11.90M articles from REALNEWS (Zellers et al., 2019)</td>
<td style="text-align: center;">6.66B</td>
<td style="text-align: center;">39GB</td>
<td style="text-align: center;">1.08</td>
<td style="text-align: center;">1.16</td>
</tr>
<tr>
<td style="text-align: center;">REVIEWS</td>
<td style="text-align: center;">24.75M Amazon reviews (He and McAuley, 2016)</td>
<td style="text-align: center;">2.11B</td>
<td style="text-align: center;">11GB</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">1.93</td>
</tr>
<tr>
<td style="text-align: center;">ROBERTA (baseline)</td>
<td style="text-align: center;">see Appendix $\S$ A. 1</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">160GB</td>
<td style="text-align: center;">${ }^{2} 1.19$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: List of the domain-specific unlabeled datasets. In columns 5 and 6, we report RoBERTA's masked LM loss on 50 K randomly sampled held-out documents from each domain before ( $\mathcal{L}<em _DAPT="{DAPT" _text="\text">{\text {RoB. }}$ ) and after ( $\mathcal{L}</em>$ ) DAPT (lower implies a better fit on the sample). $\ddagger$ indicates that the masked LM loss is estimated on data sampled from sources similar to RoBERTA's pretraining corpus.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">PT</th>
<th style="text-align: center;">100.0</th>
<th style="text-align: center;">54.1</th>
<th style="text-align: center;">34.5</th>
<th style="text-align: center;">27.3</th>
<th style="text-align: center;">19.2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">News</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">17.3</td>
</tr>
<tr>
<td style="text-align: center;">Reviews</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">12.7</td>
</tr>
<tr>
<td style="text-align: center;">BioMed</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">News</td>
<td style="text-align: center;">Reviews</td>
<td style="text-align: center;">BioMed</td>
<td style="text-align: center;">CS</td>
</tr>
</tbody>
</table>
<p>Figure 2: Vocabulary overlap (\%) between domains. PT denotes a sample from sources similar to RoBERTA's pretraining corpus. Vocabularies for each domain are created by considering the top 10 K most frequent words (excluding stopwords) in documents sampled from each domain.
random samples of held-out documents in each domain's corpus. We use 50 K held-out documents for each domain other than REVIEWS, and 150K held-out documents in ReVIEWS, since they are much shorter. We also sample 50K documents from sources similar to RoBERTA's pretraining corpus (i.e., BookCorpus, Stories, Wikipedia, and RealNEWS) to construct the pretraining domain vocabulary, since the original pretraining corpus is not released. Figure 2 shows the vocabulary overlap across these samples. We observe that RoBERTA's pretraining domain has strong vocabulary overlap with NEWS and REVIEWS, while CS and BioMed are far more dissimilar to the other domains. This simple analysis suggests the degree of benefit to be expected by adaptation of RoBERTA to different domains-the more dissimilar the domain, the higher the potential for DAPT.</p>
<h3>3.2 Experiments</h3>
<p>Our LM adaptation follows the settings prescribed for training RoBERTA. We train RoBERTA on each domain for 12.5 K steps, which amounts to single pass on each domain dataset, on a v3-8 TPU; see other details in Appendix B. This second phase of pretraining results in four domain-adapted LMs, one for each domain. We present the masked LM loss of RoBERTA on each domain before and after DAPT in Table 1. We observe that masked LM loss decreases in all domains except NEWS after DAPT, where we observe a marginal increase. We discuss cross-domain masked LM loss in Appendix $\S$ E.</p>
<p>Under each domain, we consider two text classification tasks, as shown in Table 2. Our tasks represent both high- and low-resource ( $\leq 5 \mathrm{~K}$ labeled training examples, and no additional unlabeled data) settings. For HYPERPARTISAN, we use the data splits from Beltagy et al. (2020). For RCT, we represent all sentences in one long sequence for simultaneous prediction.</p>
<p>Baseline As our baseline, we use an off-the-shelf RoBERTA-base model and perform supervised fine-tuning of its parameters for each classification task. On average, RoBERTA is not drastically behind the state of the art (details in Appendix $\S$ A.2), and serves as a good baseline since it provides a single LM to adapt to different domains.</p>
<p>Classification Architecture Following standard practice (Devlin et al., 2019) we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction (see Table 14 in Appendix for more hyperparameter details).</p>
<p>Results Test results are shown under the DAPT column of Table 3 (see Appendix $\S$ C for validation results). We observe that DAPT improves over RoBERTA in all domains. For BioMed, CS, and REVIEWS, we see consistent improve-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Label Type</th>
<th style="text-align: center;">Train (Lab.)</th>
<th style="text-align: center;">Train (Unl.)</th>
<th style="text-align: center;">Dev.</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Classes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BIOMED</td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">relation classification</td>
<td style="text-align: center;">4169</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2427</td>
<td style="text-align: center;">3469</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ RCT</td>
<td style="text-align: center;">abstract sent. roles</td>
<td style="text-align: center;">18040</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30212</td>
<td style="text-align: center;">30135</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">ACL-ARC</td>
<td style="text-align: center;">citation intent</td>
<td style="text-align: center;">1688</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">139</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCIERC</td>
<td style="text-align: center;">relation classification</td>
<td style="text-align: center;">3219</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">455</td>
<td style="text-align: center;">974</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">HYPERPARTISAN</td>
<td style="text-align: center;">partisanship</td>
<td style="text-align: center;">515</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ AGNEWS</td>
<td style="text-align: center;">topic</td>
<td style="text-align: center;">115000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">7600</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">REVIEWS</td>
<td style="text-align: center;">${ }^{\dagger}$ HELPFULNESS</td>
<td style="text-align: center;">review helpfulness</td>
<td style="text-align: center;">115251</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">25000</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ IMDB</td>
<td style="text-align: center;">review sentiment</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">50000</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">25000</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>Table 2: Specifications of the various target task datasets. $\dagger$ indicates high-resource settings. Sources: CHEMPROT (Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), SCIERC (Luan et al., 2018), HYPERPARTISAN (Kiesel et al., 2019), AGNEWS (Zhang et al., 2015), HELPFULNESS (McAuley et al., 2015), IMDB (Maas et al., 2011).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dom.</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">ROBA.</th>
<th style="text-align: center;">DAPT</th>
<th style="text-align: center;">$\neg$ DAPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BM</td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">81.91 .0</td>
<td style="text-align: center;">84.20 .2</td>
<td style="text-align: center;">79.41 .3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ RCT</td>
<td style="text-align: center;">87.20 .1</td>
<td style="text-align: center;">87.60 .1</td>
<td style="text-align: center;">86.90 .1</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">ACL-ARC</td>
<td style="text-align: center;">63.05 .8</td>
<td style="text-align: center;">75.42 .5</td>
<td style="text-align: center;">66.44 .1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCIERC</td>
<td style="text-align: center;">77.31 .9</td>
<td style="text-align: center;">80.81 .5</td>
<td style="text-align: center;">79.20 .9</td>
</tr>
<tr>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">HYP.</td>
<td style="text-align: center;">86.60 .9</td>
<td style="text-align: center;">88.25 .9</td>
<td style="text-align: center;">76.44 .9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ AGNEWS</td>
<td style="text-align: center;">93.90 .2</td>
<td style="text-align: center;">93.90 .2</td>
<td style="text-align: center;">93.50 .2</td>
</tr>
<tr>
<td style="text-align: center;">REV.</td>
<td style="text-align: center;">${ }^{\dagger}$ HELPFUL.</td>
<td style="text-align: center;">65.13 .4</td>
<td style="text-align: center;">66.51 .4</td>
<td style="text-align: center;">65.12 .8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ IMDB</td>
<td style="text-align: center;">95.00 .2</td>
<td style="text-align: center;">95.40 .2</td>
<td style="text-align: center;">94.10 .4</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of RoBERTA (RoBA.) and DAPT to adaptation to an irrelevant domain ( $\neg$ DAPT). Reported results are test macro- $F_{1}$, except for CHEMPROT and RCT, for which we report micro- $F_{1}$, following Beltagy et al. (2019). We report averages across five random seeds, with standard deviations as subscripts. $\dagger$ indicates high-resource settings. Best task performance is boldfaced. See $\S 3.3$ for our choice of irrelevant domains.
ments over ROBERTA, demonstrating the benefit of DAPT when the target domain is more distant from RoBERTA's source domain. The pattern is consistent across high- and low- resource settings. Although DAPT does not increase performance on AGNEWS, the benefit we observe in HYPERPARTISAN suggests that DAPT may be useful even for tasks that align more closely with RoBERTA's source domain.</p>
<h3>3.3 Domain Relevance for DAPT</h3>
<p>Additionally, we compare DAPT against a setting where for each task, we adapt the LM to a domain outside the domain of interest. This controls for the case in which the improvements over RoBERTA might be attributed simply to exposure to more data,
regardless of the domain. In this setting, for NEWS, we use a CS LM; for REVIEWS, a BIOMED LM; for CS, a NEWS LM; for BIOMED, a REVIEWS LM. We use the vocabulary overlap statistics in Figure 2 to guide these choices.</p>
<p>Our results are shown in Table 3, where the last column ( $\neg$ DAPT) corresponds to this setting. For each task, DAPT significantly outperforms adapting to an irrelevant domain, suggesting the importance of pretraining on domain-relevant data. Furthermore, we generally observe that $\neg$ DAPT results in worse performance than even ROBERTA on end-tasks. Taken together, these results indicate that in most settings, exposure to more data without considering domain relevance is detrimental to end-task performance. However, there are two tasks (SCIERC and ACL-ARC) in which $\neg$ DAPT marginally improves performance over RoBERTA. This may suggest that in some cases, continued pretraining on any additional data is useful, as noted in Baevski et al. (2019).</p>
<h3>3.4 Domain Overlap</h3>
<p>Our analysis of DAPT is based on prior intuitions about how task data is assigned to specific domains. For instance, to perform DAPT for HELPFULNESS, we only adapt to Amazon reviews, but not to any RealNews articles. However, the gradations in Figure 2 suggest that the boundaries between domains are in some sense fuzzy; for example, $40 \%$ of unigrams are shared between REVIEWS and NEWS. As further indication of this overlap, we also qualitatively identify documents that overlap cross-domain: in Table 4, we showcase reviews and REALNEWS articles that are similar to these reviews (other examples can be found in Appendix $\S \mathrm{D})$. In fact, we find that adapting ROBERTA to</p>
<table>
<thead>
<tr>
<th style="text-align: center;">IMDB review</th>
<th style="text-align: center;">RealNews article</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">"The Shop Around the Corner" is one of the great films from director Ernst Lubitsch. In addition to the talents of James Stewart and Margaret Sullavan it's filled with a terrific cast of top character actors such as Frank Morgan and Felix Bressart. [...] The makers of "You've Got Mail" claim their film to be a remake, but that's just nothing but a lot of inflated self praise. Anyway, if you have an affection for romantic comedies of the 1940 's, you'll find "The Shop Around the Corner" to be nothing short of wonderful. Just as good with repeat viewings.</td>
<td style="text-align: center;">[...] Three great festive films... The Shop Around the Corner (1940) Delightful Comedy by Ernst Lubitsch stars James Stewart and Margaret Sullavan falling in love at Christmas. Remade as You've Got Mail. [...]</td>
</tr>
<tr>
<td style="text-align: center;">Helpfulness review</td>
<td style="text-align: center;">RealNews article</td>
</tr>
<tr>
<td style="text-align: center;">Simply the Best! I've owned countless Droids and iPhones, but this one destroys them all. Samsung really nailed it with this one, extremely fast, very pocketable, gorgeous display, exceptional battery life, good audio quality, perfect GPS \&amp; WiFi performance, transparent status bar, battery percentage, ability to turn off soft key lights, superb camera for a smartphone and more! [...]</td>
<td style="text-align: center;">We're living in a world with a new Samsung. [...] more on battery life later [...] Exposure is usually spot on and focusing is very fast. [...] The design, display, camera and performance are all best in class, and the phone feels smaller than it looks. [...]</td>
</tr>
</tbody>
</table>
<p>Table 4: Examples that illustrate how some domains might have overlaps with others, leading to unexpected positive transfer. We highlight expressions in the reviews that are also found in the RealNews articles.</p>
<p>News not as harmful to its performance on REVIEWS tasks (DAPT on NEWS achieves $65.5_{2.3}$ on Helpfulness and $95.0_{0.1}$ on IMDB).</p>
<p>Although this analysis is by no means comprehensive, it indicates that the factors that give rise to observable domain differences are likely not mutually exclusive. It is possible that pretraining beyond conventional domain boundaries could result in more effective DAPT; we leave this investigation to future work. In general, the provenance of data, including the processes by which corpora are curated, must be kept in mind when designing pretraining procedures and creating new benchmarks that test out-of-domain generalization abilities.</p>
<h2>4 Task-Adaptive Pretraining</h2>
<p>Datasets curated to capture specific tasks of interest tend to cover only a subset of the text available within the broader domain. For example, the CHEMPROT dataset for extracting relations between chemicals and proteins focuses on abstracts of recently-published, high-impact articles from hand-selected PubMed categories (Krallinger et al., 2017, 2015). We hypothesize that such cases where the task data is a narrowly-defined subset of the broader domain, pretraining on the task dataset itself or data relevant to the task may be helpful.</p>
<p>Task-adaptive pretraining (TAPT) refers to pretraining on the unlabeled training set for a given task; prior work has shown its effectiveness (e.g. Howard and Ruder, 2018). Compared to domainadaptive pretraining (DAPT; $\S 3$ ), the task-adaptive approach strikes a different trade-off: it uses a far smaller pretraining corpus, but one that is much
more task-relevant (under the assumption that the training set represents aspects of the task well). This makes TAPT much less expensive to run than DAPT, and as we show in our experiments, the performance of TAPT is often competitive with that of DAPT.</p>
<h3>4.1 Experiments</h3>
<p>Similar to DAPT, task-adaptive pretraining consists of a second phase of pretraining ROBERTA, but only on the available task-specific training data. In contrast to DAPT, which we train for 12.5 K steps, we perform TAPT for 100 epochs. We artificially augment each dataset by randomly masking different words (using the masking probability of 0.15 ) across epochs. As in our DAPT experiments, we pass the final layer [CLS] token representation to a task-specific feedforward layer for classification (see Table 14 in Appendix for more hyperparameter details).</p>
<p>Our results are shown in the TAPT column of Table 5. TAPT consistently improves the RoBERTA baseline for all tasks across domains. Even on the news domain, which was part of RoBERTA pretraining corpus, TAPT improves over RoBERTA, showcasing the advantage of task adaptation. Particularly remarkable are the relative differences between TAPT and DAPT. DAPT is more resource intensive (see Table 9 in $\S 5.3$ ), but TAPT manages to match its performance in some of the tasks, such as SciERC. In RCT, HyperPartisan, AGNews, Helpfulness, and IMDB, the results even exceed those of DAPT, highlighting the efficacy of this cheaper adaptation technique.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">ROBERTA</th>
<th style="text-align: center;">Additional Pretraining Phases</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">DAPT + TAPT</td>
</tr>
<tr>
<td style="text-align: center;">BIOMED</td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">$81.9_{1.0}$</td>
<td style="text-align: center;">$84.2_{0.2}$</td>
<td style="text-align: center;">$82.6_{0.4}$</td>
<td style="text-align: center;">84.4 $4_{0.4}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ RCT</td>
<td style="text-align: center;">$87.2_{0.1}$</td>
<td style="text-align: center;">$87.6_{0.1}$</td>
<td style="text-align: center;">$87.7_{0.1}$</td>
<td style="text-align: center;">87.8 $8_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">ACL-ARC</td>
<td style="text-align: center;">$63.0_{5.8}$</td>
<td style="text-align: center;">$75.4_{2.5}$</td>
<td style="text-align: center;">$67.4_{1.8}$</td>
<td style="text-align: center;">75.6 $6_{3.8}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCIERC</td>
<td style="text-align: center;">$77.3_{1.9}$</td>
<td style="text-align: center;">$80.8_{1.5}$</td>
<td style="text-align: center;">$79.3_{1.5}$</td>
<td style="text-align: center;">81.3 $3_{1.8}$</td>
</tr>
<tr>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">Hyperpartisan</td>
<td style="text-align: center;">$86.6_{0.9}$</td>
<td style="text-align: center;">$88.2_{5.9}$</td>
<td style="text-align: center;">90.4 $4_{5.2}$</td>
<td style="text-align: center;">$90.0_{6.6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ AGNews</td>
<td style="text-align: center;">$93.9_{0.2}$</td>
<td style="text-align: center;">$93.9_{0.2}$</td>
<td style="text-align: center;">$94.5_{0.1}$</td>
<td style="text-align: center;">94.6 $6_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">REVIEWS</td>
<td style="text-align: center;">${ }^{\dagger}$ HELPFULNESS</td>
<td style="text-align: center;">$65.1_{3.4}$</td>
<td style="text-align: center;">$66.5_{1.4}$</td>
<td style="text-align: center;">$68.5_{1.9}$</td>
<td style="text-align: center;">68.7 $7_{1.8}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{\dagger}$ IMDB</td>
<td style="text-align: center;">$95.0_{0.2}$</td>
<td style="text-align: center;">$95.4_{0.1}$</td>
<td style="text-align: center;">$95.5_{0.1}$</td>
<td style="text-align: center;">95.6 $6_{0.1}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on different phases of adaptive pretraining compared to the baseline ROBERTA (col. 1). Our approaches are DAPT (col. 2, §3), TAPT (col. 3, §4), and a combination of both (col. 4). Reported results follow the same format as Table 3. State-of-the-art results we can compare to: CHEMPROT (84.6), RCT (92.9), ACL-ARC (71.0), SCIERC (81.8), HYPERPARTISAN (94.8), AGNews (95.5), IMDB (96.2); references in §A.2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">BIOMED</th>
<th style="text-align: left;">RCT</th>
<th style="text-align: left;">CHEMPROT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TAPT</td>
<td style="text-align: left;">$87.7_{0.1}$</td>
<td style="text-align: left;">$82.6_{0.5}$</td>
</tr>
<tr>
<td style="text-align: left;">Transfer-TAPT</td>
<td style="text-align: left;">$87.1_{0.4}(\downarrow 0.6)$</td>
<td style="text-align: left;">$80.4_{0.6}(\downarrow 2.2)$</td>
</tr>
<tr>
<td style="text-align: left;">NEWS</td>
<td style="text-align: left;">Hyperpartisan</td>
<td style="text-align: left;">AGNews</td>
</tr>
<tr>
<td style="text-align: left;">TAPT</td>
<td style="text-align: left;">$89.9_{9.5}$</td>
<td style="text-align: left;">$94.5_{0.1}$</td>
</tr>
<tr>
<td style="text-align: left;">Transfer-TAPT</td>
<td style="text-align: left;">$82.2_{7.7}(\downarrow 7.7)$</td>
<td style="text-align: left;">$93.9_{0.2}(\downarrow 0.6)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">CS</th>
<th style="text-align: left;">ACL-ARC</th>
<th style="text-align: left;">SCIERC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TAPT</td>
<td style="text-align: left;">$67.4_{1.8}$</td>
<td style="text-align: left;">$79.3_{1.5}$</td>
</tr>
<tr>
<td style="text-align: left;">Transfer-TAPT</td>
<td style="text-align: left;">$64.1_{2.7}(\downarrow 3.3)$</td>
<td style="text-align: left;">$79.1_{2.5}(\downarrow 0.2)$</td>
</tr>
<tr>
<td style="text-align: left;">REVIEWS</td>
<td style="text-align: left;">HELPFULNESS</td>
<td style="text-align: left;">IMDB</td>
</tr>
<tr>
<td style="text-align: left;">TAPT</td>
<td style="text-align: left;">$68.5_{1.9}$</td>
<td style="text-align: left;">$95.7_{0.1}$</td>
</tr>
<tr>
<td style="text-align: left;">Transfer-TAPT</td>
<td style="text-align: left;">$65.0_{2.6}(\downarrow 3.5)$</td>
<td style="text-align: left;">$95.0_{0.1}(\downarrow 0.7)$</td>
</tr>
</tbody>
</table>
<p>Table 6: Though TAPT is effective (Table 5), it is harmful when applied across tasks. These findings illustrate differences in task distributions within a domain.</p>
<p>Combined DAPT and TAPT We investigate the effect of using both adaptation techniques together. We begin with ROBERTA and apply DAPT then TAPT under this setting. The three phases of pretraining add up to make this the most computationally expensive of all our settings (see Table 9). As expected, combined domain- and task-adaptive pretraining achieves the best performance on all tasks (Table 5). ${ }^{2}$</p>
<p>Overall, our results show that DAPT followed by TAPT achieves the best of both worlds of domain and task awareness, yielding the best performance. While we speculate that TAPT followed by DAPT would be susceptible to catastrophic forgetting of the task-relevant corpus (Yogatama et al., 2019), alternate methods of combining the procedures may result in better downstream performance. Future work may explore pretraining with a more sophisticated curriculum of domain and task distributions.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Cross-Task Transfer We complete the comparison between DAPT and TAPT by exploring whether adapting to one task transfers to other tasks in the same domain. For instance, we further pretrain the LM using the RCT unlabeled data, fine-tune it with the CHEMPROT labeled data, and observe the effect. We refer to this setting as Transfer-TAPT. Our results for tasks in all four domains are shown in Table 6. We see that TAPT optimizes for single task performance, to the detriment of cross-task transfer. These results demonstrate that data distributions of tasks within a given domain might differ. Further, this could also explain why adapting only to a broad domain is not sufficient, and why TAPT after DAPT is effective.</p>
<h2>5 Augmenting Training Data for Task-Adaptive Pretraining</h2>
<p>In $\S 4$, we continued pretraining the LM for task adaptation using only the training data for a supervised task. Inspired by the success of TAPT, we next investigate another setting where a larger pool of unlabeled data from the task distribution exists,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pretraining</th>
<th style="text-align: center;">BIOMED <br> RCT-500</th>
<th style="text-align: center;">NEWS <br> HyP.</th>
<th style="text-align: center;">REVIEWS <br> IMDB ${ }^{\dagger}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">$79.8_{1.4}$</td>
<td style="text-align: center;">$90.4_{5.2}$</td>
<td style="text-align: center;">$95.5_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT + TAPT</td>
<td style="text-align: center;">$83.0_{0.3}$</td>
<td style="text-align: center;">$90.0_{6.6}$</td>
<td style="text-align: center;">$95.6_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">Curated-TAPT</td>
<td style="text-align: center;">$83.4_{0.3}$</td>
<td style="text-align: center;">$89.9_{9.5}$</td>
<td style="text-align: center;">$95.7_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT + Curated-TAPT</td>
<td style="text-align: center;">$\mathbf{8 3 . 8}_{0.5}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 1}_{3.6}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 8}_{0.1}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Mean test set macro- $F_{1}$ (for HyP. and IMDB) and micro- $F_{1}$ (for RCT-500), with CuratedTAPT across five random seeds, with standard deviations as subscripts. $\dagger$ indicates high-resource settings.
typically curated by humans.
We explore two scenarios. First, for three tasks (RCT, HyPERPartisAn, and IMDB) we use this larger pool of unlabeled data from an available human-curated corpus ( $\S 5.1$ ). Next, we explore retrieving related unlabeled data for TAPT, from a large unlabeled in-domain corpus, for tasks where extra human-curated data is unavailable (§5.2).</p>
<h3>5.1 Human Curated-TAPT</h3>
<p>Dataset creation often involves collection of a large unlabeled corpus from known sources. This corpus is then downsampled to collect annotations, based on the annotation budget. The larger unlabeled corpus is thus expected to have a similar distribution to the task's training data. Moreover, it is usually available. We explore the role of such corpora in task-adaptive pretraining.</p>
<p>Data We simulate a low-resource setting RCT500, by downsampling the training data of the RCT dataset to 500 examples (out of 180 K available), and treat the rest of the training data as unlabeled. The HyPERPartisAn shared task (Kiesel et al., 2019) has two tracks: low- and high-resource. We use 5 K documents from the high-resource setting as Curated-TAPT unlabeled data and the original lowresource training documents for task fine-tuning. For IMDB, we use the extra unlabeled data manually curated by task annotators, drawn from the same distribution as the labeled data (Maas et al., 2011).</p>
<p>Results We compare Curated-TAPT to TAPT and DAPT + TAPT in Table 7. Curated-TAPT further improves our prior results from $\S 4$ across all three datasets. Applying Curated-TAPT after adapting to the domain results in the largest boost in performance on all tasks; in HyPERPartisAn, DAPT + Curated-TAPT is within standard deviation of Curated-TAPT. Moreover, curated-TAPT achieves</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: An illustration of automated data selection (§5.2). We map unlabeled ChemProt and 1M BioMed sentences to a shared vector space using the VAMPIRE model trained on these sentences. Then, for each CHEMPROT sentence, we identify $k$ nearest neighbors, from the BIOMED domain.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pretraining</th>
<th style="text-align: center;">BIOMED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CS <br> ACL-ARC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">RCT-500</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ROBERTA</td>
<td style="text-align: center;">$81.9_{1.0}$</td>
<td style="text-align: center;">$79.3_{0.6}$</td>
<td style="text-align: center;">$63.0_{5.8}$</td>
</tr>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">$82.6_{0.4}$</td>
<td style="text-align: center;">$79.8_{1.4}$</td>
<td style="text-align: center;">$67.4_{1.8}$</td>
</tr>
<tr>
<td style="text-align: center;">RAND-TAPT</td>
<td style="text-align: center;">$81.9_{0.6}$</td>
<td style="text-align: center;">$80.6_{0.4}$</td>
<td style="text-align: center;">$69.7_{3.4}$</td>
</tr>
<tr>
<td style="text-align: center;">50NN-TAPT</td>
<td style="text-align: center;">$83.3_{0.7}$</td>
<td style="text-align: center;">$80.8_{0.6}$</td>
<td style="text-align: center;">$70.7_{2.8}$</td>
</tr>
<tr>
<td style="text-align: center;">150NN-TAPT</td>
<td style="text-align: center;">$83.2_{0.6}$</td>
<td style="text-align: center;">$81.2_{0.8}$</td>
<td style="text-align: center;">$73.3_{2.7}$</td>
</tr>
<tr>
<td style="text-align: center;">500NN-TAPT</td>
<td style="text-align: center;">$83.3_{0.7}$</td>
<td style="text-align: center;">$81.7_{0.4}$</td>
<td style="text-align: center;">$75.5_{1.9}$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">$\mathbf{8 4 . 2}_{0.2}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 5}_{0.5}$</td>
<td style="text-align: center;">$75.4_{2.5}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Mean test set micro- $F_{1}$ (for CHEMPROT and RCT) and macro- $F_{1}$ (for ACL-ARC), across five random seeds, with standard deviations as subscripts, comparing RAND-TAPT (with 50 candidates) and $k \mathrm{NN}-$ TAPT selection. Neighbors of the task data are selected from the domain data.
$95 \%$ of the performance of DAPT + TAPT with the fully labeled RCT corpus (Table 5) with only $0.3 \%$ of the labeled data. These results suggest that curating large amounts of data from the task distribution is extremely beneficial to end-task performance. We recommend that task designers release a large pool of unlabeled task data for their tasks to aid model adaptation through pretraining.</p>
<h3>5.2 Automated Data Selection for TAPT</h3>
<p>Consider a low-resource scenario without access to large amounts of unlabeled data to adequately benefit from TAPT, as well as absence of computational resources necessary for DAPT (see Table 9 for details of computational requirements for different pretraining phases). We propose simple unsuper-</p>
<p>vised methods to retrieve unlabeled text that aligns with the task distribution, from a large in-domain corpus. Our approach finds task-relevant data from the domain by embedding text from both the task and domain in a shared space, then selects candidates from the domain based on queries using the task data. Importantly, the embedding method must be lightweight enough to embed possibly millions of sentences in a reasonable time.</p>
<p>Given these constraints, we employ VAMPIRE (Gururangan et al., 2019; Figure 3), a lightweight bag-of-words language model. We pretrain VAMPIRE on a large deduplicated ${ }^{3}$ sample of the domain ( 1 M sentences) to obtain embeddings of the text from both the task and domain sample. We then select $k$ candidates of each task sentence from the domain sample, in embeddings space. Candidates are selected (i) via nearest neighbors selection ( $k \mathrm{NN}$-TAPT) ${ }^{4}$, or (ii) randomly (RAND-TAPT). We continue pretraining ROBERTA on this augmented corpus with both the task data (as in TAPT) as well as the selected candidate pool.</p>
<p>Results Results in Table 8 show that $k \mathrm{NN}$-TAPT outperforms TAPT for all cases. RAND-TAPT is generally worse than $k \mathrm{NN}$-TAPT, but within a standard deviation arising from 5 seeds for RCT and ACLARC. As we increase $k, k \mathrm{NN}$-TAPT performance steadily increases, and approaches that of DAPT. Appendix F shows examples of nearest neighbors of task data. Future work might consider a closer study of $k \mathrm{NN}$-TAPT, more sophisticated data selection methods, and the tradeoff between the diversity and task relevance of selected examples.</p>
<h3>5.3 Computational Requirements</h3>
<p>The computational requirements for all our adaptation techniques on RCT-500 in the BioMED domain in Table 9. TAPT is nearly 60 times faster to train than DAPT on a single v3-8 TPU and storage requirements for DAPT on this task are 5.8 M times that of TAPT. Our best setting of DAPT + TAPT amounts to three phases of pretraining, and at first glance appears to be very expensive. However, once the LM has been adapted to a broad domain, it can be reused for multiple tasks within that domain, with only a single additional TAPT phase per task. While Curated-TAPT tends to achieve the best cost-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 9: Computational requirements for adapting to the RCT-500 task, comparing DAPT ( $\S 3$ ) and the various TAPT modifications described in $\S 4$ and $\S 5$.
benefit ratio in this comparison, one must also take into account the cost of curating large in-domain data. Automatic methods such as $k \mathrm{NN}$-TAPT are much cheaper than DAPT.</p>
<h2>6 Related Work</h2>
<p>Transfer learning for domain adaptation Prior work has shown the benefit of continued pretraining in domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Lee et al., 2019). ${ }^{5}$ We have contributed further investigation of the effects of a shift between a large, diverse pretraining corpus and target domain on task performance. Other studies (e.g., Huang et al., 2019) have trained language models (LMs) in their domain of interest, from scratch. In contrast, our work explores multiple domains, and is arguably more cost effective, since we continue pretraining an already powerful LM.</p>
<p>Task-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g. in Howard and Ruder, 2018; Phang et al., 2018; Sun et al., 2019). In the presence of domain shift between train and test data distributions of the same task, domain-adaptive pretraining (DAPT) is sometimes used to describe what we term TAPT (Logeswaran et al., 2019; Han and Eisenstein, 2019). Related approaches include language modeling as an auxiliary objective to task classifier fine-tuning (Chronopoulou et al., 2019; Radford et al., 2018) or consider simple syntactic structure of the input while adapting to task-specific</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Domain <br> (Unlabeled)</td>
<td style="text-align: center;">Task <br> (Unlabeled)</td>
<td style="text-align: center;">Task <br> (Labeled)</td>
</tr>
<tr>
<td style="text-align: center;">ROBERTA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT + TAPT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">$k$ NN-TAPT</td>
<td style="text-align: center;">(Subset)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Curated-TAPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(Extra)</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 10: Summary of strategies for multi-phase pretraining explored in this paper.
data (Swayamdipta et al., 2019). We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets. See Table 11 in Appendix $\S \mathrm{A}$ for a summary of multi-phase pretraining strategies from related work.</p>
<p>Data selection for transfer learning Selecting data for transfer learning has been explored in NLP (Moore and Lewis, 2010; Ruder and Plank, 2017; Zhang et al., 2019, among others). Dai et al. (2019) focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in $\S 5.2$. Concurrent to our work, Aharoni and Goldberg (2020) propose data selection methods for NMT based on cosine similarity in embedding space, using DiSTILBERT (Sanh et al., 2019) for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. Khandelwal et al. (2020) introduced $k \mathrm{NN}-\mathrm{LMs}$ that allows easy domain adaptation of pretrained LMs by simply adding a datastore per domain and no further training; an alternative to integrate domain information in an LM. Our study of human-curated data $\S 5.1$ is related to focused crawling (Chakrabarti et al., 1999) for collection of suitable data, especially with LM reliance (Remus and Biemann, 2016).</p>
<p>What is a domain? Despite the popularity of domain adaptation techniques, most research and practice seems to use an intuitive understanding of domains. A small body of work has attempted to address this question (Lee, 2001; Eisenstein et al., 2014; van der Wees et al., 2015; Plank, 2016; Ruder et al., 2016, among others). For instance, Aharoni and Goldberg (2020) define domains by implicit
clusters of sentence representations in pretrained LMs. Our results show that DAPT and TAPT complement each other, which suggests a spectra of domains defined around tasks at various levels of granularity (e.g., Amazon reviews for a specific product, all Amazon reviews, all reviews on the web, the web).</p>
<h2>7 Conclusion</h2>
<p>We investigate several variations for adapting pretrained LMs to domains and tasks within those domains, summarized in Table 10. Our experiments reveal that even a model of hundreds of millions of parameters struggles to encode the complexity of a single textual domain, let alone all of language. We show that pretraining the model towards a specific task or small corpus can provide significant benefits. Our findings suggest it may be valuable to complement work on ever-larger LMs with parallel efforts to identify and use domain- and taskrelevant corpora to specialize models. While our results demonstrate how these approaches can improve RoBERTA, a powerful LM, the approaches we studied are general enough to be applied to any pretrained LM. Our work points to numerous future directions, such as better data selection for TAPT, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation.</p>
<h2>Acknowledgments</h2>
<p>The authors thank Dallas Card, Mark Neumann, Nelson Liu, Eric Wallace, members of the AllenNLP team, and anonymous reviewers for helpful feedback, and Arman Cohan for providing data. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670.</p>
<h2>References</h2>
<p>Roee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. In $A C L$. To appear.</p>
<p>Emily Alsentzer, John Murphy, William Boag, WeiHung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop.</p>
<p>Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. 2019. Cloze-driven pretraining of self-attention networks. In EMNLP.</p>
<p>Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In EMNLP.</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv:2004.05150.</p>
<p>Soumen Chakrabarti, Martin van den Berg, and Byron Dom. 1999. Focused Crawling: A New Approach to Topic-Specific Web Resource Discovery. Comput. Networks, 31:1623-1640.</p>
<p>Tuhin Chakrabarty, Christopher Hidey, and Kathy McKeown. 2019. IMHO fine-tuning improves claim detection. In NAACL.</p>
<p>Ciprian Chelba, Tomas Mikolov, Michael Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH.</p>
<p>Alexandra Chronopoulou, Christos Baziotis, and Alexandros Potamianos. 2019. An embarrassingly simple approach for transfer learning from pretrained language models. In NAACL.</p>
<p>Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Dan Weld. 2019. Pretrained language models for sequential sentence classification. In EMNLP.</p>
<p>Xiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile Paris. 2019. Using similarity measures to select pretraining data for NER. In NAACL.</p>
<p>Franck Dernoncourt and Ji Young Lee. 2017. Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts. In IJCNLP.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL.</p>
<p>Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. 2019. Show your work: Improved reporting of experimental results. In EMNLP.</p>
<p>Jacob Eisenstein, Brendan O'connor, Noah A. Smith, and Eric P. Xing. 2014. Diffusion of lexical change in social media. PloS ONE.</p>
<p>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language processing platform. In NLP-OSS.</p>
<p>Aaron Gokaslan and Vanya Cohen. 2019. OpenWebText Corpus.</p>
<p>Suchin Gururangan, Tam Dang, Dallas Card, and Noah A. Smith. 2019. Variational pretraining for semi-supervised text classification. In $A C L$.</p>
<p>Xiaochuang Han and Jacob Eisenstein. 2019. Unsupervised domain adaptation of contextualized embeddings for sequence labeling. In EMNLP.</p>
<p>Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In WWW.</p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.</p>
<p>Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In $A C L$.</p>
<p>Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019. ClinicalBERT: Modeling clinical notes and predicting hospital readmission. arXiv:1904.05342.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data.</p>
<p>David Jurgens, Srijan Kumar, Raine Hoover, Daniel A. McFarland, and Dan Jurafsky. 2018. Measuring the evolution of a scientific field through citation frames. $T A C L$.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In ICLR. To appear.</p>
<p>Johannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David Corney, Benno Stein, and Martin Potthast. 2019. SemEval2019 Task 4: Hyperpartisan news detection. In SemEval.</p>
<p>Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.</p>
<p>Martin Krallinger, Obdulia Rabal, Saber Ahmad Akhondi, Martín Pérez Pérez, Jésus López Santamaría, Gael Pérez Rodríguez, Georgios Tsatsaronis, Ander Intxaurrondo, José Antonio Baso López, Umesh Nandal, E. M. van Buel, A. Poorna Chandrasekhar, Marleen Rodenburg, Astrid Lægreid, Marius A. Doornenbal, Julen Oyarzábal, Anália Lourenço, and Alfonso Valencia. 2017. Overview of the biocreative vi chemical-protein interaction track. In Proceedings of the BioCreative VI Workshop.</p>
<p>Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, et al. 2015. The chemdner corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics, 7(1):S2.</p>
<p>Jens Kringelum, Sonny Kim Kjærulff, Søren Brunak, Ole Lund, Tudor I. Oprea, and Olivier Taboureau. 2016. ChemProt-3.0: a global chemical biology diseases mapping. In Database.</p>
<p>David YW Lee. 2001. Genres, registers, text types, domains and styles: Clarifying the concepts and navigating a path through the BNC jungle. Language Learning \&amp; Technology.</p>
<p>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: A pre-trained biomedical language representation model for biomedical text mining. Bioinformatics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:1907.11692.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In ACL. To appear.</p>
<p>Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-shot entity linking by reading entity descriptions. In $A C L$.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In EMNLP.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In $A C L$.</p>
<p>Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In ACM SIGIR.</p>
<p>Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal, Swaroop Ranjan Mishra, and Chitta Baral. 2020. Exploring ways to incorporate additional knowledge to improve natural language commonsense question answering. arXiv:1909.08855v3.</p>
<p>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In $A C L$.</p>
<p>Sebastian Nagel. 2016. CC-NEWS.
Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. Scispacy: Fast and robust models for biomedical natural language processing. Proceedings of the 18th BioNLP Workshop and Shared Task.</p>
<p>Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To tune or not to tune? Adapting pretrained representations to diverse tasks. In RepL4NLP.</p>
<p>Jason Phang, Thibault Févry, and Samuel R. Bowman. 2018. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. arXiv:1811.01088.</p>
<p>Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. In KONVENS.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Colin Raffel, Noam Shazeer, Adam Kaleo Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683.</p>
<p>Steffen Remus and Chris Biemann. 2016. DomainSpecific Corpus Expansion with Focused Webcrawling. In LREC.</p>
<p>Sebastian Ruder, Parsa Ghaffari, and John G. Breslin. 2016. Towards a continuous modeling of natural language domains. In Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods.</p>
<p>Sebastian Ruder and Barbara Plank. 2017. Learning to select data for transfer learning with Bayesian optimization. In EMNLP.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. In EMC2 @ NeurIPS.</p>
<p>Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune BERT for text classification? In CCL.</p>
<p>Swabha Swayamdipta, Matthew Peters, Brendan Roof, Chris Dyer, and Noah A Smith. 2019. Shallow syntax in deep water. arXiv:1908.11047.</p>
<p>Tan Thongtan and Tanasanee Phienthrakul. 2019. Sentiment classification using document embeddings trained with cosine similarity. In ACL SRW.</p>
<p>Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. arXiv:1806.02847.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In NeurIPS.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP @ EMNLP.</p>
<p>Marlies van der Wees, Arianna Bisazza, Wouter Weerkamp, and Christof Monz. 2015. What's in a domain? Analyzing genre and topic differences in statistical machine translation. In $A C L$.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-the-art natural language processing. arXiv:1910.03771.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation.</p>
<p>Hu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019a. BERT post-training for review reading comprehension and aspect-based sentiment analysis. In NAACL.</p>
<p>Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019b. Review conversational reading comprehension. arXiv:1902.00821v2.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS.</p>
<p>Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomás Kociský, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. 2019. Learning and evaluating general linguistic intelligence.</p>
<p>Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In NeurIPS.</p>
<p>Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In NeurIPS.</p>
<p>Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. 2019. Curriculum learning for domain adaptation in neural machine translation. In NAACL.</p>
<p>Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV.</p>
<h2>Appendix Overview</h2>
<p>In this supplementary material, we provide: (i) additional information for producing the results in the paper, and (ii) results that we could not fit into the main body of the paper.
Appendix A. A tabular overview of related work described in Section $\S 6$, a description of the corpus used to train RoBERTA in Liu et al. (2019), and references to the state of the art on our tasks.
Appendix B. Details about the data preprocessing, training, and implementation of domain- and taskadaptive pretraining.
Appendix C. Development set results.
Appendix D. Examples of domain overlap.
Appendix E. The cross-domain masked LM loss and reproducibility challenges.
Appendix F. Illustration of our data selection method and examples of nearest neighbours.</p>
<h2>A Related Work</h2>
<p>Table 11 shows which of the strategies for continued pretraining have already been explored in the prior work from the Related Work (§6). As evident from the table, our work compares various strategies as well as their interplay using a pretrained language model trained on a much more heterogeneous pretraining corpus.</p>
<h2>A. 1 RoBERTA's Pretraining Corpus</h2>
<p>ROBERTA was trained on data from BOOKCORPUS (Zhu et al., 2015), ${ }^{6}$ WIKIPEDIA, ${ }^{7}$ a portion of the CCNEWS dataset (Nagel, 2016), ${ }^{8}$ OPENWEBTEXT corpus of Web content extracted from URLs shared on Reddit (Gokaslan and Cohen, 2019), ${ }^{9}$ and a subset of CommonCrawl that it is said to resemble the "story-like" style of WinogRad schemas (STORIES; Trinh and Le, 2018). ${ }^{10}$</p>
<h2>A. 2 State of the Art</h2>
<p>In this section, we specify the models achieving state of the art on our tasks. See the caption of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 5 for the reported performance of these models. For ACL-ARC, that is SCIBERT (Beltagy et al., 2019), a BERT-base model for trained from scratch on scientific text. For CHEMPROT and SCIERC, that is S2ORC-BERT (Lo et al., 2020), a similar model to SciBERT. For AGNews and IMDB, XLNet-large, a much larger model. For RCT, Cohan et al. (2019). For HYPERPARTISAN, LONGFORMER, a modified Transformer language model for long documents (Beltagy et al., 2020). Thongtan and Phienthrakul (2019) report a higher number ( 97.42 ) on IMDB, but they train their word vectors on the test set. Our baseline establishes the first benchmark for the HELPFULNESS dataset.</p>
<h2>B Experimental Setup</h2>
<p>Preprocessing for DAPT The unlabeled corpus in each domain was pre-processed prior to language model training. Abstracts and body paragraphs from biomedical and computer science articles were used after sentence splitting using scispaCy (Neumann et al., 2019). We used summaries and full text of each news article, and the entire body of review from Amazon reviews. For both news and reviews, we perform sentence splitting using spaCy (Honnibal and Montani, 2017).</p>
<p>Training details for DAPT We train RoBERTA on each domain for 12.5 K steps. We focused on matching all the domain dataset sizes (see Table 1) such that each domain is exposed to the same amount of data as for 12.5 K steps it is trained for. AMAZON reviews contain more documents, but each is shorter. We used an effective batch size of 2048 through gradient accumulation, as recommended in Liu et al. (2019). See Table 13 for more hyperparameter details.</p>
<p>Training details for TAPT We use the same pretraining hyperparameters as DAPT, but we artificially augmented each dataset for TAPT by randomly masking different tokens across epochs, using the masking probability of 0.15 . Each dataset was trained for 100 epochs. For tasks with less than 5 K examples, we used a batch size of 256 through gradient accumulation. See Table 13 for more hyperparameter details.</p>
<p>Optimization We used the Adam optimizer (Kingma and Ba, 2015), a linear learning rate scheduler with $6 \%$ warm-up, a maximum learning rate of 0.0005 . When we used a batch size of 256 , we</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">DAPT Domains <br> (if applicable)</th>
<th style="text-align: left;">Tasks</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">DAPT</th>
<th style="text-align: left;">TAPT</th>
<th style="text-align: left;">DAPT <br> + TAPT</th>
<th style="text-align: left;">$k \mathrm{NN}-$ <br> TAPT</th>
<th style="text-align: left;">Curated- <br> TAPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">This Paper</td>
<td style="text-align: left;">biomedical \&amp; computer <br> science papers, news, <br> reviews</td>
<td style="text-align: left;">8 classification <br> tasks</td>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Aharoni and Goldberg (2020)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">NMT</td>
<td style="text-align: left;">DistilBERT + <br> Transformer NMT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">similar</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Alsentzer et al. (2019)</td>
<td style="text-align: left;">clinical text</td>
<td style="text-align: left;">NER, NLI, <br> de-identification</td>
<td style="text-align: left;">(BIO)BERT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Chakrabarty et al. (2019)</td>
<td style="text-align: left;">opinionated claims from <br> Reddit</td>
<td style="text-align: left;">claim detection</td>
<td style="text-align: left;">ULMFiT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Chronopoulou et al. (2019)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">5 classification <br> tasks</td>
<td style="text-align: left;">ULMFiT ${ }^{1}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">similar</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Han and Eisenstein (2019)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">NER in historical <br> texts</td>
<td style="text-align: left;">ELMO, BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Howard and Ruder (2018)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">6 classification <br> tasks</td>
<td style="text-align: left;">ULMFiT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Khandelwal et al. (2020)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">language modeling</td>
<td style="text-align: left;">Transformer LM</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">similar</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Lee et al. (2019)</td>
<td style="text-align: left;">biomedical papers</td>
<td style="text-align: left;">NER, QA, relation <br> extraction</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Logeswaran et al. (2019)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">zero-shot entity <br> linking in Wikia</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Mitra et al. (2020)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">commonsense QA</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Phang et al. (2018)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">GLUE tasks</td>
<td style="text-align: left;">ELMO, BERT, <br> GPT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Radford et al. (2018)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">NLI, QA, <br> similarity, <br> classification</td>
<td style="text-align: left;">GPT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">similar</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Sun et al. (2019)</td>
<td style="text-align: left;">sentiment, question, <br> topic</td>
<td style="text-align: left;">7 classification <br> tasks</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Swayamdipta et al. (2019)</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">NER, parsing, <br> classification</td>
<td style="text-align: left;">ELMO</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">similar</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Xu et al. (2019a)</td>
<td style="text-align: left;">reviews</td>
<td style="text-align: left;">RC, aspect extract., <br> sentiment <br> classification</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Xu et al. (2019b)</td>
<td style="text-align: left;">restaurant reviews, <br> laptop reviews</td>
<td style="text-align: left;">conversational RC</td>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 11: Overview of prior work across strategies for continued pre-training summarized in Table 10. ULMFiT is pretrained on English Wikipedia; ULMFiT ${ }^{1}$ on English tweets; ELMO on the 1BW ORDBENCHMARK (newswire; Chelba et al., 2014); GPT on BookCorpus; BERT on English Wikipedia and BookCorpus. In comparison to these pretraining corpora, RoBERTa's pretraining corpus is substantially more diverse (see Appendix $\S$ A.1).
used a maximum learning rate of 0.0001 , as recommended in Liu et al. (2019). We observe a high variance in performance between random seeds when fine-tuning RoBERTa to HYPERPartisAN, because the dataset is extremely small. To produce final results on this task, we discard and resample degenerate seeds. We display the full hyperparameter settings in Table 13.</p>
<p>Implementation Our LM implementation uses the HuggingFace transformers library (Wolf et al., 2019) ${ }^{11}$ and PyTorch XLA for TPU compatibility. ${ }^{12}$ Each adaptive pretraining exper-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>iment was performed on a single v3-8 TPU from Google Cloud. ${ }^{13}$ For the text classification tasks, we used AllenNLP (Gardner et al., 2018). Following standard practice (Devlin et al., 2019) we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction.</p>
<h2>C Development Set Results</h2>
<p>Adhering to the standards suggested by Dodge et al. (2019) for replication, we report our development set results in Tables 15, 17, and 18.</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h1>D Analysis of Domain Overlap</h1>
<p>In Table 20 we display additional examples that highlight the overlap between IMDB reviews and REALNews articles, relevant for analysis in $\S 3.1$.</p>
<h2>E Analysis of Cross-Domain Masked LM Loss</h2>
<p>In Section $\S 3.2$, we provide RoBERTA's masked LM loss before and after DAPT. We display crossdomain masked-LM loss in Table 12, where we evaluate masked LM loss on text samples in other domains after performing DAPT.</p>
<p>We observe that the cross-domain masked-LM loss mostly follows our intuition and insights from the paper, i.e. RoBERTA's pretraining corpus and News are closer, and BioMED to CS (relative to other domains). However, our analysis in $\S 3.1$ illustrates that REVIEWS and NEWS also have some similarities. This is supported with the loss of RoBERTA that is adapted to NEws, calculated on a sample of REVIEWS. However, RoBERTA that is adapted to REVIEWS results in the highest loss for a NEws sample. This is the case for all domains. One of the properties that distinguishes REVIEWS from all other domains is that its documents are significantly shorter. In general, we find that cross-DAPT masked-LM loss can in some cases be a noisy predictor of domain similarity.</p>
<h2>F $k$-Nearest Neighbors Data Selection</h2>
<p>In Table 21, we display nearest neighbor documents in the BioMED domain identified by our selection method, on the RCT dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Data Sample Unseen During DAPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">BioMED</td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">REVIEWS</td>
</tr>
<tr>
<td style="text-align: left;">DAPT</td>
<td style="text-align: center;">RoBERTA</td>
<td style="text-align: center;">$\mathbf{1 . 1 9}$</td>
<td style="text-align: center;">1.32</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">$\mathbf{1 . 0 8}$</td>
<td style="text-align: center;">2.10</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">BIOMED</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">$\mathbf{0 . 9 9}$</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">1.69</td>
<td style="text-align: center;">2.59</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">$\mathbf{1 . 3 4}$</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">2.78</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">2.16</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">REVIEWS</td>
<td style="text-align: center;">2.07</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">2.44</td>
<td style="text-align: center;">2.27</td>
<td style="text-align: center;">$\mathbf{1 . 9 3}$</td>
</tr>
</tbody>
</table>
<p>Table 12: RoBERTA's (row 1) and domain-adapted RoBERTA's (rows 2-5) masked LM loss on randomly sampled held-out documents from each domain (lower implies a better fit). PT denotes a sample from sources similar to RoBERTA's pretraining corpus. The lowest masked LM for each domain sample is boldfaced.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Computing Infrastructure</th>
<th style="text-align: center;">Google Cloud v3-8 TPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model implementations</td>
<td style="text-align: center;">https://github.com/allenai/tpu_pretrain</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">number of steps</td>
<td style="text-align: center;">100 epochs (TAPT) or 12.5K steps (DAPT)</td>
</tr>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">256 or 2058</td>
</tr>
<tr>
<td style="text-align: center;">maximum learning rate</td>
<td style="text-align: center;">0.0001 or 0.0005</td>
</tr>
<tr>
<td style="text-align: center;">learning rate optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: center;">Adam beta weights</td>
<td style="text-align: center;">$0.9,0.98$</td>
</tr>
<tr>
<td style="text-align: center;">learning rate scheduler</td>
<td style="text-align: center;">None or warmup linear</td>
</tr>
<tr>
<td style="text-align: center;">Weight decay</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Warmup proportion</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: center;">learning rate decay</td>
<td style="text-align: center;">linear</td>
</tr>
</tbody>
</table>
<p>Table 13: Hyperparameters for domain- and task- adaptive pretraining.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Computing Infrastructure</th>
<th style="text-align: center;">Quadro RTX 8000 GPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model implementation</td>
<td style="text-align: center;">https://github.com/allenai/dont-stop-pretraining</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">number of epochs</td>
<td style="text-align: center;">3 or 10</td>
</tr>
<tr>
<td style="text-align: center;">patience</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: center;">dropout</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">feedforward layer</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">feedforward nonlinearity</td>
<td style="text-align: center;">tanh</td>
</tr>
<tr>
<td style="text-align: center;">classification layer</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 14: Hyperparameters for RoBERTA text classifier.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">ROBERTA</th>
<th style="text-align: center;">Additional Pretraining Phases</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">DAPT + TAPT</td>
</tr>
<tr>
<td style="text-align: center;">BIOMED</td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">$83.2_{1.4}$</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">$83.0_{0.6}$</td>
<td style="text-align: center;">84.1 $_{0.5}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{1}$ RCT</td>
<td style="text-align: center;">$88.1_{0.05}$</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">$88.3_{0.1}$</td>
<td style="text-align: center;">88.5 $_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">ACL-ARC</td>
<td style="text-align: center;">$71.3_{2.8}$</td>
<td style="text-align: center;">$73.2_{1.5}$</td>
<td style="text-align: center;">$73.2_{3.6}$</td>
<td style="text-align: center;">78.6 $_{2.9}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciERC</td>
<td style="text-align: center;">$83.8_{1.1}$</td>
<td style="text-align: center;">88.4 $_{1.7}$</td>
<td style="text-align: center;">$85.9_{0.8}$</td>
<td style="text-align: center;">$88.0_{1.3}$</td>
</tr>
<tr>
<td style="text-align: center;">News</td>
<td style="text-align: center;">Hyperpartisan</td>
<td style="text-align: center;">$84.0_{1.5}$</td>
<td style="text-align: center;">$79.1_{3.5}$</td>
<td style="text-align: center;">82.7 $_{5.3}$</td>
<td style="text-align: center;">$80.8_{2.3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{1}$ AGN</td>
<td style="text-align: center;">$94.3_{0.1}$</td>
<td style="text-align: center;">$94.3_{0.1}$</td>
<td style="text-align: center;">$94.7_{0.1}$</td>
<td style="text-align: center;">94.9 $_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">REVIEWS</td>
<td style="text-align: center;">${ }^{1}$ Helpfulness</td>
<td style="text-align: center;">$65.5_{3.4}$</td>
<td style="text-align: center;">$66.5_{1.4}$</td>
<td style="text-align: center;">$69.2_{2.4}$</td>
<td style="text-align: center;">69.4 $_{2.1}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{1}$ IMDB</td>
<td style="text-align: center;">$94.8_{0.1}$</td>
<td style="text-align: center;">$95.3_{0.1}$</td>
<td style="text-align: center;">$95.4_{0.1}$</td>
<td style="text-align: center;">95.7 $_{0.2}$</td>
</tr>
</tbody>
</table>
<p>Table 15: Results on different phases of adaptive pretraining compared to the baseline ROBERTA (col. 1). Our approaches are DAPT (col. 2, §3), TAPT (col. 3, §4), and a combination of both (col. 4). Reported results are development macro- $F_{1}$, except for CHEMPROT and RCT, for which we report micro- $F_{1}$, following Beltagy et al. (2019). We report averages across five random seeds, with standard deviations as subscripts. $\dagger$ indicates high-resource settings. Best task performance is boldfaced. State-of-the-art results we can compare to: CHEMPROT (84.6), RCT (92.9), ACL-ARC (71.0), SCIERC (81.8), HyperPartisAN (94.8), AGNES (95.5), IMDB (96.2); references in $\S$ A. 2 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dom.</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">RoB.</th>
<th style="text-align: center;">DAPT</th>
<th style="text-align: center;">$\neg$ DAPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BM</td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">$83.2_{1.4}$</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">$80.9_{0.5}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{1}$ RCT</td>
<td style="text-align: center;">$88.1_{0.0}$</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">$87.9_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">ACL-ARC</td>
<td style="text-align: center;">$71.3_{2.8}$</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">$68.1_{5.4}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciERC</td>
<td style="text-align: center;">$83.8_{1.1}$</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">$83.9_{0.9}$</td>
</tr>
<tr>
<td style="text-align: center;">NEWS</td>
<td style="text-align: center;">HyP.</td>
<td style="text-align: center;">84.0 $_{1.5}$</td>
<td style="text-align: center;">$79.1_{3.5}$</td>
<td style="text-align: center;">$71.6_{4.6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{1}$ AGN</td>
<td style="text-align: center;">94.3 $_{0.1}$</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">$94.0_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">REV.</td>
<td style="text-align: center;">${ }^{1}$ HelpFul.</td>
<td style="text-align: center;">$65.5_{3.4}$</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">$65.5_{3.0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">${ }^{1}$ IMDB</td>
<td style="text-align: center;">$94.8_{0.1}$</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">$93.8_{0.2}$</td>
</tr>
</tbody>
</table>
<p>Table 16: Development comparison of ROBERTA (ROBA.) and DAPT to adaptation to an irrelevant domain ( $\neg$ DAPT). See $\S 3.3$ for our choice of irrelevant domains. Reported results follow the same format as Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">BIOMED</th>
<th style="text-align: left;">RCT</th>
<th style="text-align: left;">CHEMPROT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TAPT</td>
<td style="text-align: left;">$88.3_{0.1}$</td>
<td style="text-align: left;">$83.0_{0.6}$</td>
</tr>
<tr>
<td style="text-align: left;">Transfer-TAPT</td>
<td style="text-align: left;">$88.0_{0.1}(\downarrow 0.3)$</td>
<td style="text-align: left;">$81.1_{0.5}(\downarrow 1.9)$</td>
</tr>
<tr>
<td style="text-align: left;">NEWS</td>
<td style="text-align: left;">HyperPartisAN</td>
<td style="text-align: left;">AGNES</td>
</tr>
<tr>
<td style="text-align: left;">TAPT</td>
<td style="text-align: left;">$82.7_{3.3}$</td>
<td style="text-align: left;">$94.7_{0.1}$</td>
</tr>
<tr>
<td style="text-align: left;">Transfer-TAPT</td>
<td style="text-align: left;">$77.6_{3.6}(\downarrow 5.1)$</td>
<td style="text-align: left;">$94.4_{0.1}(\downarrow 0.4)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">CS</th>
<th style="text-align: center;">ACL-ARC</th>
<th style="text-align: center;">SCIERC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">$73.2_{3.6}$</td>
<td style="text-align: center;">$85.9_{0.8}$</td>
</tr>
<tr>
<td style="text-align: center;">Transfer-TAPT</td>
<td style="text-align: center;">$74.0_{4.5}(\dagger 1.2)$</td>
<td style="text-align: center;">$85.5_{1.1}(\downarrow 0.4)$</td>
</tr>
<tr>
<td style="text-align: center;">Amazon reviews</td>
<td style="text-align: center;">Helpfulness</td>
<td style="text-align: center;">IMDB</td>
</tr>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">$69.2_{2.4}$</td>
<td style="text-align: center;">$95.4_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">Transfer-TAPT</td>
<td style="text-align: center;">$65.4_{2.7}(\downarrow 3.8)$</td>
<td style="text-align: center;">$94.9_{0.1}(\downarrow 0.5)$</td>
</tr>
</tbody>
</table>
<p>Table 17: Development results for TAPT transferability.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pretraining</th>
<th style="text-align: center;">BIOMED <br> RCT-500</th>
<th style="text-align: center;">NEWS <br> HyperPartisAn</th>
<th style="text-align: center;">REVIEWS <br> ${ }^{1}$ IMDB</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">$80.5_{1.3}$</td>
<td style="text-align: center;">$82.7_{3.3}$</td>
<td style="text-align: center;">$95.4_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT + TAPT</td>
<td style="text-align: center;">$83.9_{0.3}$</td>
<td style="text-align: center;">$80.8_{2.3}$</td>
<td style="text-align: center;">$95.7_{0.2}$</td>
</tr>
<tr>
<td style="text-align: center;">Curated-TAPT</td>
<td style="text-align: center;">$84.4_{0.3}$</td>
<td style="text-align: center;">84.9 $_{1.9}$</td>
<td style="text-align: center;">$95.8_{0.1}$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT + Curated-TAPT</td>
<td style="text-align: center;">84.5 $_{0.3}$</td>
<td style="text-align: center;">$83.1_{3.7}$</td>
<td style="text-align: center;">96.0 $_{0.4}$</td>
</tr>
</tbody>
</table>
<p>Table 18: Mean development set macro- $F_{1}$ (for HyperPartisAn and IMDB) and micro- $F_{1}$ (for RCT-500), with Curated-TAPT across five random seeds, with standard deviations as subscripts. $\dagger$ indicates high-resource settings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pretraining</th>
<th style="text-align: center;">BIOMED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHEMPROT</td>
<td style="text-align: center;">RCT-500</td>
<td style="text-align: center;">ACL-ARC</td>
</tr>
<tr>
<td style="text-align: center;">ROBERTA</td>
<td style="text-align: center;">$83.2_{1.4}$</td>
<td style="text-align: center;">$80.3_{0.5}$</td>
<td style="text-align: center;">$71.3_{2.8}$</td>
</tr>
<tr>
<td style="text-align: center;">TAPT</td>
<td style="text-align: center;">$83.0_{0.6}$</td>
<td style="text-align: center;">$80.5_{1.3}$</td>
<td style="text-align: center;">$73.2_{3.6}$</td>
</tr>
<tr>
<td style="text-align: center;">RAND-TAPT</td>
<td style="text-align: center;">$83.3_{0.5}$</td>
<td style="text-align: center;">$81.6_{0.6}$</td>
<td style="text-align: center;">$78.7_{4.0}$</td>
</tr>
<tr>
<td style="text-align: center;">50NN-TAPT</td>
<td style="text-align: center;">$83.3_{0.8}$</td>
<td style="text-align: center;">$81.7_{0.5}$</td>
<td style="text-align: center;">$70.1_{3.5}$</td>
</tr>
<tr>
<td style="text-align: center;">150NN-TAPT</td>
<td style="text-align: center;">$83.3_{0.9}$</td>
<td style="text-align: center;">$81.9_{0.8}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 5}_{2.2}$</td>
</tr>
<tr>
<td style="text-align: center;">500NN-TAPT</td>
<td style="text-align: center;">$\mathbf{8 4 . 5}_{0.4}$</td>
<td style="text-align: center;">$82.6_{0.4}$</td>
<td style="text-align: center;">$77.4_{2.3}$</td>
</tr>
<tr>
<td style="text-align: center;">DAPT</td>
<td style="text-align: center;">$84.1_{0.5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 5}_{0.8}$</td>
<td style="text-align: center;">$73.2_{1.5}$</td>
</tr>
</tbody>
</table>
<p>Table 19: Mean development set macro- $F_{1}$ (for HyP. and IMDB) and micro- $F_{1}$ (for RCT), across five random seeds, with standard deviations as subscripts, comparing RAND-TAPT (with 50 candidates) and $k$ NN-TAPT selection. Neighbors of the task data are selected from the domain data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">IMDB review</th>
<th style="text-align: center;">REALNESs article</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Spooks is enjoyable trash, featuring some well directed sequences, ridiculous plots and dialogue, and some third rate acting. Many have described this is a UK version of " 24 ", and one can see the similarities. The American version shares the weak silly plots, but the execution is so much slicker, sexier and I suspect, expensive. Some people describe weak comedy as "gentle comedy". This is gentle spy story hour, the exact opposite of anything created by John Le Carre. Give me Smiley any day.</td>
<td style="text-align: center;">[...] Remember poor Helen Flynn from Spooks? In 2002, the headlong BBC spy caper was in such a hurry to establish the high-wire stakes of its morally compromised world that Lisa Faulkner's keen-as-mustard MI5 rookie turned out to be a lot more expendable than her prominent billing suggested. [...] Functioning as both a shocking twist and rather callous statement that No-One Is Safe, it gave the slick drama an instant patina of edginess while generating a record-breaking number of complaints.</td>
</tr>
<tr>
<td style="text-align: center;">The Sopranos is perhaps the most mind-opening series you could possibly ever want to watch. It's smart, it's quirky, it's funny - and it carries the mafia genre so well that most people can't resist watching. The best aspect of this show is the overwhelming realism of the characters, set in the subterranean world of the New York crime families. For most of the time, you really don't know whether the wise guys will stab someone in the back, or buy them lunch. Further adding to the realistic approach of the characters in this show is the depth of their personalities - These are dangerous men, most of them murderers, but by God if you don't love them too. I've laughed at their wisecracks, been torn when they've made err in judgement, and felt scared at the sheer ruthlessness of a serious criminal. [...]</td>
<td style="text-align: center;">The drumbeat regarding the "Breaking Bad" finale has led to the inevitable speculation on whether the final chapter in this serialized gem will live up to the hype or disappoint (thank you, "Dexter," for setting that bar pretty low), with debate, second-guessing and graduate-thesis-length analysis sure to follow. The Most Memorable TV Series Finales of AllTime [...] No ending in recent years has been more divisive than "The Sopranos" - for some, a brilliant flash (literally, in a way) of genius; for others (including yours truly), a too-cute copout, cryptically leaving its characters in perpetual limbo. The precedent to that would be "St. Elsewhere," which irked many with its provocative, surreal notion that the whole series was, in fact, conjured in the mind of an autistic child. [...]</td>
</tr>
<tr>
<td style="text-align: center;">The Wicker Man, starring Nicolas Cage, is by no means a good movie, but I can't really say it's one I regret watching. I could go on and on about the negative aspects of the movie, like the terrible acting and the lengthy scenes where Cage is looking for the girl, has a hallucination, followed by another hallucination, followed by a dream sequence- with a hallucination, etc., but it's just not worth dwelling on when it comes to a movie like this. Instead, here's five reasons why you SHOULD watch The Wicker Man, even though it's bad: 5. It's hard to deny that it has some genuinely creepy ideas to it, the only problem is in its cheesy, unintentionally funny execution. If nothing else, this is a movie that may inspire you to see the original 1973 film, or even read the short story on which it is based. 4. For a cheesy horror/thriller, it is really aesthetically pleasing. [...] NOTE: The Unrated version of the movie is the best to watch, and it's better to watch the Theatrical version just for its little added on epilogue, which features a cameo from James Franco.</td>
<td style="text-align: center;">[...] What did you ultimately feel about "The Wicker Man" movie when all was said and done? [...] I'm a fan of the original and I'm glad that I made the movie because they don't make movies like that anymore and probably the result of what "Wicker Man" did is the reason why they don't make movies like that anymore. Again, it's kind of that "70's sensibility, but I'm trying to do things that are outside the box. Sometimes that means it'll work and other times it won't. Again though I'm going to try and learn from anything that I do. I think that it was a great cast, and Neil La Bute is one of the easiest directors that I've ever worked with. He really loves actors and he really gives you a relaxed feeling on the set, that you can achieve whatever it is that you're trying to put together, but at the end of the day the frustration that I had with 'The Wicker Man,' which I think has been remedied on the DVD because I believe the DVD has the directors original cut, is that they cut the horror out of the horror film to try and get a PG-13 rating. I mean, I don't know how to stop something like that. So I'm not happy with the way that the picture ended, but I'm happy with the spirit with which it was made. [...]</td>
</tr>
<tr>
<td style="text-align: center;">Dr. Seuss would sure be mad right now if he was alive. Cat in the Hat proves to show how movie productions can take a classic story and turn it into a mindless pile of goop. We have Mike Myers as the infamous Cat in the Hat, big mistake! Myers proves he can't act in this film. He acts like a prissy show girl with a thousand tricks up his sleeve. The kids in this movie are all right, somewhere in between the lines of dull and annoying. The story is just like the original with a couple of tweaks and like most movies based on other stories, never tweak with the original story! Bringing in the evil neighbor Quin was a bad idea. He is a stupid villain that would never get anywhere in life. [...]</td>
<td style="text-align: center;">The Cat in the Hat. [...] Based on the book by Dr. Seuss [...] From the moment his tall, red-and-white-striped hat appears at their door, Sally and her brother know that the Cat in the Hat is the most mischievous cat they will ever meet. Suddenly the rainy afternoon is transformed by the Cat and his antics. Will their house ever be the same? Can the kids clean up before mom comes home? With some tricks (and a fish) and Thing Two and Thing One, with the Cat in The Hat, the fun's never done!Dr. Seuss is known worldwide as the imaginative master of children's literature. His books include a wonderful blend of invented and actual words, and his rhymes have helped many children and adults learn and better their understanding of the English language. [...]</td>
</tr>
</tbody>
</table>
<p>Table 20: Additional examples that highlight the overlap between IMDB reviews and REALNESs articles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">During median follow-up of 905 days ( IQR 773-1050), 49 people died and 987 unplanned admissions were recorded ( totalling 5530 days in hospital ).</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Neighbor 0</td>
<td style="text-align: center;">Of this group, 26\% died after discharge from hospital, and the median time to death was 11 days (interquartile range, $4.0-15.0$ days) after discharge.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 1</td>
<td style="text-align: center;">The median hospital stay was 17 days (range 8-26 days), and all the patients were discharged within 1 month.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 2</td>
<td style="text-align: center;">The median hospital stay was 17 days (range 8-26 days).</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 3</td>
<td style="text-align: center;">The median time between discharge and death was 25 days (mean, 59.1 days) and no patient was alive after 193 days.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 4</td>
<td style="text-align: center;">The length of hospital stay after colostomy formation ranged from 3 days to 14 days with a median duration of 6 days ( $*$ IQR of 4 to 8 days).</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Randomized, controlled, parallel clinical trial .</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 0</td>
<td style="text-align: center;">Design: Unblinded, randomised clinical controlled trial.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 1</td>
<td style="text-align: center;">These studies and others led to the phase III randomized trial RTOG 0617/NCCTG 0628/ CALGB 30609.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 2</td>
<td style="text-align: center;">-Definitive randomized controlled clinical trial (RCT):</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 3</td>
<td style="text-align: center;">RCT $\frac{1}{2}$ randomized controlled trial.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 4</td>
<td style="text-align: center;">randomized controlled trial [ Fig. 3(A)].</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Forty primary molar teeth in 40 healthy children aged 5-9 years were treated by direct pulp capping .</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 0</td>
<td style="text-align: center;">In our study, we specifically determined the usefulness of the Er:YAG laser in caries removal and cavity preparation of primary and young permanent teeth in children ages 4 to 18 years.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 1</td>
<td style="text-align: center;">Males watched more TV than females, although it was only in primary school-aged children and on weekdays.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 2</td>
<td style="text-align: center;">Assent was obtained from children and adolescents aged 7-17 years.</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 3</td>
<td style="text-align: center;">Cardiopulmonary resuscitation was not applied to children aged ¡5 years (Table 2).</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor 4</td>
<td style="text-align: center;">It measures HRQoL in children and adolescents aged 2 to 25 years.</td>
</tr>
</tbody>
</table>
<p>Table 21: 5 nearest neighbors of sentences from the RCT dataset (Source) in the BioMED domain (Neighbors $0-4)$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ https://github.com/huggingface/ transformers
${ }^{12}$ https://github.com/pytorch/xla&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{13}$ http://github.com/allenai/ tpu-pretrain&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8956 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8956</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8956</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-0d42221038c05cee8443c5b5af838505ee137dc3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0d42221038c05cee8443c5b5af838505ee137dc3" target="_blank">ART: Automatic multi-step reasoning and tool-use for large language models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program, achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8956.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8956.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (ensembling chain-of-thought outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time ensembling method that samples multiple chain-of-thought (CoT) reasoning traces from a frozen LLM and chooses the most frequent final answer (majority voting) to improve multi-step reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT (API name text-davinci-002), a GPT-family model fine-tuned to follow instructions (exact parameter count not specified in the paper); used as the frozen base LLM in ART experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency (ensembling)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple independent CoT/program outputs from the LLM for the same input and take the most frequent final answer (majority vote). In ART experiments they generated multiple program outputs and ensembled answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various ART-evaluated tasks (subset reported in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selected BigBench/MMLU-style tasks used to evaluate ART, including Simple Text Editing, CS Algorithms, Strategy QA, Physics Questions, Unit Interpretation, and Reasoning about colored objects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported per-task increases when using self-consistency (ensemble of 15 generations): Simple Text Editing 30.67% (vs 27.65% baseline, +3.02 pp); CS Algorithms 90.99% (vs 88.11%, +2.88 pp); Strategy QA 70.76% (vs 66.44%, +4.32 pp); Physics Questions 24.07% (vs 20.37%, +3.70 pp); Unit Interpretation 57.20% (vs 53.99%, +3.21 pp); Reasoning about colored objects 69.11% (vs 64.34%, +4.77 pp). (Numbers taken from Table 5.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Corresponding ART performance without ensembling: Simple Text Editing 27.65%; CS Algorithms 88.11%; Strategy QA 66.44%; Physics Questions 20.37%; Unit Interpretation 53.99%; Reasoning about colored objects 64.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering + sampling: repeated stochastic decoding to obtain multiple CoT/program outputs, then majority-vote (no additional training or external modules).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative per-task improvements shown in Table 5: ensembling 15 model generations consistently improved accuracy across all reported tasks by ~2.9–4.8 percentage points versus single-run ART outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported primary cost is increased compute (explicitly noted: 'at the cost of extra computation'). No specific failure cases where self-consistency degraded performance are reported in the paper; improvements are consistent but modest per-task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to ART without ensembling (single-run). Self-consistency provides a modest additional boost on top of ART; not directly compared to other generate-then-reflect-style methods in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ART: Automatic multi-step reasoning and tool-use for large language models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8956.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8956.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-in-the-loop program editing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-in-the-loop editing of ART programs and tool library</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manual iterative-improvement workflow where humans inspect LLM-generated decompositions (programs), correct sub-steps, add missing steps, or add tool definitions, then re-incorporate edited programs into the task/tool libraries to improve downstream LLM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT (text-davinci-002) used as the frozen LLM in ART; exact architecture/size not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Human-in-the-loop iterative program editing</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Humans (authors) select a small number of erroneous model-generated programs (they edited 5 random instances per task in the pilot), correct sub-step outputs, add/remove sub-steps, or create new tool definitions, then add these corrected programs/tools to the task/tool libraries and re-run ART.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Selected BigBench test tasks (12 tasks reported in Table 6 and discussed in text)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various BigBench tasks (e.g., CS Algorithms, Reasoning about objects, Repeat Copy Logic, Sentence Ambiguity, Strategy QA, Physics, Temporal Sequences, Track Shuffled objects, Unit Interpretation, Word Unscrambling, Simple Text Editing), where human edits target program sub-steps or tools.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>After adding human-edited programs/tools, ART average reported performance on the selected tasks increased to ~79.85% (average reported in Table 6 for ART + Human edits). Per-task examples reported include CS Algorithms improving to 92.73% (from 88.11%), Reasoning about objects improving substantially (reported as reaching high values such as 98.90% in table), Repeat Copy Logic and others showing very large gains; overall ART+Human surpasses best-published GPT-3 baselines on those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>ART baseline average on those tasks reported as ~56.0% (Table 6 average for ART without human edits); specific per-task baselines include CS Algorithms 88.11%, Reasoning about objects 64.34%, Repeat Copy Logic 44.38%, Sentence Ambiguity 73.33%, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Manual editing of the interpretable program-style CoT demonstrations and tool library entries (no LLM fine-tuning), then reuse the edited demonstrations in prompts for future examples.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Table 6 reports large per-task improvements after editing just 5 program instances per task; authors report that ART with minimal human feedback surpasses best-known GPT-3 results by an average of over 20 percentage points on 12 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires human effort and expertise (manual edits). The paper notes some failure modes unrelated to human editing: code generation errors can cause cascading failures in reasoning (particularly for string-manipulation and code-editing tasks). Human edits corrected many but not all such issues. The approach improves specific tasks but depends on quality of human edits and is not fully automatic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Human-edited ART demonstrations produce much larger gains than analogous human edits applied to free-form AutoCoT; after edits, ART outperforms best published GPT-3 baselines on the selected tasks. Compared qualitatively vs AutoCoT and few-shot baselines, human edits on ART yield larger improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ART: Automatic multi-step reasoning and tool-use for large language models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8956.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8956.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Chain-of-Thought prompting (AutoCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero/low-shot method that automatically elicits chain-of-thought (CoT) reasoning traces from an LLM on a small subset of examples (e.g., 5), then uses those generated CoTs as demonstrations for solving other instances of the same task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chain of thought prompting in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / InstructGPT (as evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>AutoCoT was evaluated using GPT-3-class models (the paper uses InstructGPT / GPT-3 family for comparison baselines where appropriate); ART compares against AutoCoT as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Auto-CoT (generate demonstrations then reuse)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Select a random subset of examples (e.g., 5), prompt the LLM with 'Let's think step-by-step' to obtain CoT reasoning for those examples, then include the generated CoT chains as few-shot demonstrations to prompt the model on new instances (single-stage generate-and-use pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BigBench and other reasoning tasks (used as baseline across the paper's benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse BigBench tasks including arithmetic, search, string ops, free-form reasoning; AutoCoT is used as a baseline to measure effectiveness of structured program-format prompts and tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>AutoCoT baseline performances shown in Tables 2 and 3 (examples): GSM8K 21.99% (Table 2), Aqua-rat 34.41%, Elementary Math QA 74.52% (varies by task); overall ART outperforms AutoCoT on most tasks; across BigBench test tasks ART outperforms AutoCoT by an average of ~24.6 percentage points (Table 3 shows 'Δ with ART (Overall) +24.6 vs AutoCoT').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct few-shot / direct prompting baseline performances (no CoT) reported separately (e.g., GSM8K few-shot 7.79%), but AutoCoT itself is the automatically generated CoT baseline (so 'without' would be direct few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineering-based: free-form CoT elicitation via 'Let's think step-by-step' and reusing generated text as demonstrations; no external tool integration in AutoCoT baseline used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>AutoCoT sometimes substantially improves over direct few-shot prompting on arithmetic and commonsense tasks (as cited and observed), but in this paper ART (with structured programs and tool use) outperforms AutoCoT across most evaluated tasks (quantified in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>AutoCoT in this work is free-form and does not support parsed tool calls, so it cannot leverage external tools; as a result it underperforms ART (which supports structured programs + tool use) on many tasks. AutoCoT performance also depends on the quality and representativeness of the seed examples used to elicit CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ART: Automatic multi-step reasoning and tool-use for large language models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8956.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8956.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that uses self-supervision to teach LMs to call external tools by finetuning a model to decide when and how to invoke tools; cited in related work as complementary to ART.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (as used in Toolformer paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Toolformer work used a smaller GPT-J-sized model finetuned with a self-supervision procedure to learn tool-calling; cited here as related work but not used in ART experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-supervision for tool-use (Toolformer)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train/finetune a language model via self-supervision to insert tool calls into its generations; this is a training-stage 'self-teaching' method rather than an inference-time reflection loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA and other benchmarks used in Toolformer paper (cited for comparison in Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks used to evaluate tool-augmented reasoning, including QA datasets like SQuAD, T-REx, SVAMP, MAWPS, NQ, TriviaQA (used for cross-paper comparison in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not evaluated within ART; the paper compares ART results to published Toolformer numbers (Table 4) and reports ART outperforms Toolformer on 5/6 compared tasks (e.g., SQuAD: Toolformer 33.8% vs ART 39.34%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>N/A within this paper (Toolformer numbers are cited from its paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Model finetuning with self-supervision (Toolformer) to learn tool-invocation patterns; mentioned as related work rather than used.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited Toolformer results show improvements over vanilla GPT-3 in many tool-augmented QA settings; ART authors compare ART to Toolformer numbers and find ART competitive or superior on the reported subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Toolformer requires finetuning and is less flexible to swap the base LLM or add new tools at inference-time compared to ART; this limitation is discussed in the paper's related work section.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ART: Automatic multi-step reasoning and tool-use for large language models', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8956",
    "paper_id": "paper-0d42221038c05cee8443c5b5af838505ee137dc3",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (ensembling chain-of-thought outputs)",
            "brief_description": "An inference-time ensembling method that samples multiple chain-of-thought (CoT) reasoning traces from a frozen LLM and chooses the most frequent final answer (majority voting) to improve multi-step reasoning accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "InstructGPT (API name text-davinci-002), a GPT-family model fine-tuned to follow instructions (exact parameter count not specified in the paper); used as the frozen base LLM in ART experiments.",
            "reflection_method_name": "Self-consistency (ensembling)",
            "reflection_method_description": "Generate multiple independent CoT/program outputs from the LLM for the same input and take the most frequent final answer (majority vote). In ART experiments they generated multiple program outputs and ensembled answers.",
            "task_name": "Various ART-evaluated tasks (subset reported in Table 5)",
            "task_description": "Selected BigBench/MMLU-style tasks used to evaluate ART, including Simple Text Editing, CS Algorithms, Strategy QA, Physics Questions, Unit Interpretation, and Reasoning about colored objects.",
            "performance_with_reflection": "Reported per-task increases when using self-consistency (ensemble of 15 generations): Simple Text Editing 30.67% (vs 27.65% baseline, +3.02 pp); CS Algorithms 90.99% (vs 88.11%, +2.88 pp); Strategy QA 70.76% (vs 66.44%, +4.32 pp); Physics Questions 24.07% (vs 20.37%, +3.70 pp); Unit Interpretation 57.20% (vs 53.99%, +3.21 pp); Reasoning about colored objects 69.11% (vs 64.34%, +4.77 pp). (Numbers taken from Table 5.)",
            "performance_without_reflection": "Corresponding ART performance without ensembling: Simple Text Editing 27.65%; CS Algorithms 88.11%; Strategy QA 66.44%; Physics Questions 20.37%; Unit Interpretation 53.99%; Reasoning about colored objects 64.34%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering + sampling: repeated stochastic decoding to obtain multiple CoT/program outputs, then majority-vote (no additional training or external modules).",
            "number_of_iterations": 15,
            "evidence_for_improvement": "Quantitative per-task improvements shown in Table 5: ensembling 15 model generations consistently improved accuracy across all reported tasks by ~2.9–4.8 percentage points versus single-run ART outputs.",
            "limitations_or_failure_cases": "Reported primary cost is increased compute (explicitly noted: 'at the cost of extra computation'). No specific failure cases where self-consistency degraded performance are reported in the paper; improvements are consistent but modest per-task.",
            "comparison_to_other_methods": "Compared directly to ART without ensembling (single-run). Self-consistency provides a modest additional boost on top of ART; not directly compared to other generate-then-reflect-style methods in this work.",
            "ablation_study_results": null,
            "uuid": "e8956.0",
            "source_info": {
                "paper_title": "ART: Automatic multi-step reasoning and tool-use for large language models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Human-in-the-loop program editing",
            "name_full": "Human-in-the-loop editing of ART programs and tool library",
            "brief_description": "A manual iterative-improvement workflow where humans inspect LLM-generated decompositions (programs), correct sub-steps, add missing steps, or add tool definitions, then re-incorporate edited programs into the task/tool libraries to improve downstream LLM performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "InstructGPT (text-davinci-002) used as the frozen LLM in ART; exact architecture/size not specified in the paper.",
            "reflection_method_name": "Human-in-the-loop iterative program editing",
            "reflection_method_description": "Humans (authors) select a small number of erroneous model-generated programs (they edited 5 random instances per task in the pilot), correct sub-step outputs, add/remove sub-steps, or create new tool definitions, then add these corrected programs/tools to the task/tool libraries and re-run ART.",
            "task_name": "Selected BigBench test tasks (12 tasks reported in Table 6 and discussed in text)",
            "task_description": "Various BigBench tasks (e.g., CS Algorithms, Reasoning about objects, Repeat Copy Logic, Sentence Ambiguity, Strategy QA, Physics, Temporal Sequences, Track Shuffled objects, Unit Interpretation, Word Unscrambling, Simple Text Editing), where human edits target program sub-steps or tools.",
            "performance_with_reflection": "After adding human-edited programs/tools, ART average reported performance on the selected tasks increased to ~79.85% (average reported in Table 6 for ART + Human edits). Per-task examples reported include CS Algorithms improving to 92.73% (from 88.11%), Reasoning about objects improving substantially (reported as reaching high values such as 98.90% in table), Repeat Copy Logic and others showing very large gains; overall ART+Human surpasses best-published GPT-3 baselines on those tasks.",
            "performance_without_reflection": "ART baseline average on those tasks reported as ~56.0% (Table 6 average for ART without human edits); specific per-task baselines include CS Algorithms 88.11%, Reasoning about objects 64.34%, Repeat Copy Logic 44.38%, Sentence Ambiguity 73.33%, etc.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Manual editing of the interpretable program-style CoT demonstrations and tool library entries (no LLM fine-tuning), then reuse the edited demonstrations in prompts for future examples.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: Table 6 reports large per-task improvements after editing just 5 program instances per task; authors report that ART with minimal human feedback surpasses best-known GPT-3 results by an average of over 20 percentage points on 12 tasks.",
            "limitations_or_failure_cases": "Requires human effort and expertise (manual edits). The paper notes some failure modes unrelated to human editing: code generation errors can cause cascading failures in reasoning (particularly for string-manipulation and code-editing tasks). Human edits corrected many but not all such issues. The approach improves specific tasks but depends on quality of human edits and is not fully automatic.",
            "comparison_to_other_methods": "Human-edited ART demonstrations produce much larger gains than analogous human edits applied to free-form AutoCoT; after edits, ART outperforms best published GPT-3 baselines on the selected tasks. Compared qualitatively vs AutoCoT and few-shot baselines, human edits on ART yield larger improvements.",
            "ablation_study_results": null,
            "uuid": "e8956.1",
            "source_info": {
                "paper_title": "ART: Automatic multi-step reasoning and tool-use for large language models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Automatic Chain-of-Thought prompting (AutoCoT)",
            "brief_description": "A zero/low-shot method that automatically elicits chain-of-thought (CoT) reasoning traces from an LLM on a small subset of examples (e.g., 5), then uses those generated CoTs as demonstrations for solving other instances of the same task.",
            "citation_title": "Automatic chain of thought prompting in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-3 / InstructGPT (as evaluated in paper)",
            "model_description": "AutoCoT was evaluated using GPT-3-class models (the paper uses InstructGPT / GPT-3 family for comparison baselines where appropriate); ART compares against AutoCoT as a baseline.",
            "reflection_method_name": "Auto-CoT (generate demonstrations then reuse)",
            "reflection_method_description": "Select a random subset of examples (e.g., 5), prompt the LLM with 'Let's think step-by-step' to obtain CoT reasoning for those examples, then include the generated CoT chains as few-shot demonstrations to prompt the model on new instances (single-stage generate-and-use pipeline).",
            "task_name": "BigBench and other reasoning tasks (used as baseline across the paper's benchmarks)",
            "task_description": "Diverse BigBench tasks including arithmetic, search, string ops, free-form reasoning; AutoCoT is used as a baseline to measure effectiveness of structured program-format prompts and tool use.",
            "performance_with_reflection": "AutoCoT baseline performances shown in Tables 2 and 3 (examples): GSM8K 21.99% (Table 2), Aqua-rat 34.41%, Elementary Math QA 74.52% (varies by task); overall ART outperforms AutoCoT on most tasks; across BigBench test tasks ART outperforms AutoCoT by an average of ~24.6 percentage points (Table 3 shows 'Δ with ART (Overall) +24.6 vs AutoCoT').",
            "performance_without_reflection": "Direct few-shot / direct prompting baseline performances (no CoT) reported separately (e.g., GSM8K few-shot 7.79%), but AutoCoT itself is the automatically generated CoT baseline (so 'without' would be direct few-shot).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineering-based: free-form CoT elicitation via 'Let's think step-by-step' and reusing generated text as demonstrations; no external tool integration in AutoCoT baseline used in this paper.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "AutoCoT sometimes substantially improves over direct few-shot prompting on arithmetic and commonsense tasks (as cited and observed), but in this paper ART (with structured programs and tool use) outperforms AutoCoT across most evaluated tasks (quantified in tables).",
            "limitations_or_failure_cases": "AutoCoT in this work is free-form and does not support parsed tool calls, so it cannot leverage external tools; as a result it underperforms ART (which supports structured programs + tool use) on many tasks. AutoCoT performance also depends on the quality and representativeness of the seed examples used to elicit CoT.",
            "ablation_study_results": null,
            "uuid": "e8956.2",
            "source_info": {
                "paper_title": "ART: Automatic multi-step reasoning and tool-use for large language models",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Toolformer (mention)",
            "name_full": "Toolformer: Language models can teach themselves to use tools",
            "brief_description": "A prior method that uses self-supervision to teach LMs to call external tools by finetuning a model to decide when and how to invoke tools; cited in related work as complementary to ART.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "model_name": "GPT-J (as used in Toolformer paper)",
            "model_description": "Toolformer work used a smaller GPT-J-sized model finetuned with a self-supervision procedure to learn tool-calling; cited here as related work but not used in ART experiments.",
            "reflection_method_name": "Self-supervision for tool-use (Toolformer)",
            "reflection_method_description": "Train/finetune a language model via self-supervision to insert tool calls into its generations; this is a training-stage 'self-teaching' method rather than an inference-time reflection loop.",
            "task_name": "QA and other benchmarks used in Toolformer paper (cited for comparison in Table 4)",
            "task_description": "Benchmarks used to evaluate tool-augmented reasoning, including QA datasets like SQuAD, T-REx, SVAMP, MAWPS, NQ, TriviaQA (used for cross-paper comparison in Table 4).",
            "performance_with_reflection": "Not evaluated within ART; the paper compares ART results to published Toolformer numbers (Table 4) and reports ART outperforms Toolformer on 5/6 compared tasks (e.g., SQuAD: Toolformer 33.8% vs ART 39.34%).",
            "performance_without_reflection": "N/A within this paper (Toolformer numbers are cited from its paper).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Model finetuning with self-supervision (Toolformer) to learn tool-invocation patterns; mentioned as related work rather than used.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited Toolformer results show improvements over vanilla GPT-3 in many tool-augmented QA settings; ART authors compare ART to Toolformer numbers and find ART competitive or superior on the reported subset.",
            "limitations_or_failure_cases": "Toolformer requires finetuning and is less flexible to swap the base LLM or add new tools at inference-time compared to ART; this limitation is discussed in the paper's related work section.",
            "ablation_study_results": null,
            "uuid": "e8956.3",
            "source_info": {
                "paper_title": "ART: Automatic multi-step reasoning and tool-use for large language models",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        }
    ],
    "cost": 0.020044,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ART: Automatic multi-step reasoning and tool-use for large language models</h1>
<p>Bhargavi Paranjape ${ }^{1}$ Scott Lundberg ${ }^{2}$ Sameer Singh ${ }^{3}$ Hannaneh Hajishirzi ${ }^{1,4}$ Luke Zettlemoyer ${ }^{1,5}$ Marco Tulio Ribeiro ${ }^{2}$<br>${ }^{1}$ University of Washington, ${ }^{2}$ Microsoft Research, ${ }^{3}$ University of California, Irvine, ${ }^{4}$ Allen Institute of Artificial Intelligence, ${ }^{5}$ Meta AI</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multistep reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.</p>
<h2>1 Introduction</h2>
<p>In-context learning allows large language models (LLMs) to quickly adapt to new tasks simply by using natural language instructions and a few demonstrations as a prompt to the LLM (Xie et al., 2021; Brown et al., 2020; Chowdhery et al., 2022). While this circumvents annotating large datasets or even hosting the LLM itself (since many are available through APIs), there are severe performance limitations around multi-step reasoning (Liu et al., 2022),
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ART generates automatic multi-step decompositions for new tasks by selecting decompositions of related tasks in the task library (A) and selecting and using tools in the tool library alongside LLM generation (B). Humans can optionally edit decompositions (eg. correcting and editing code) to improve performance (C).
math (Patel et al., 2021), having up-to-date information (Komeili et al., 2022), and others. To address these limitations, recent work proposes prompting LLMs to mimic a chain of thought (CoT) for multistep reasoning (Wei et al.; Zhou et al., 2022; Wang et al., 2022; Press et al., 2022; Khot et al., 2022; Arora et al., 2022) or providing them with access to tools (e.g. a calculator or QA model) to enable more complex reasoning steps (Gao et al., 2022;</p>
<p>Chen et al., 2022; Press et al., 2022; Wei et al.; Schick et al., 2023). However, existing methods for chained reasoning with tool use are difficult to extend to new tasks and tools, requiring fine-tuning or prompt-engineering tailored for a specific task (Parisi et al., 2022) or tool (Schick et al., 2023).</p>
<p>In this paper, we present Automatic Reasoning and Tool use (ART), a framework that automatically generates decompositions (multi-step reasoning) for instances of new tasks. The framework also selects and uses the most appropriate available tools (like search engines, and code execution) in individual steps. Given a new task, ART retrieves demonstrations of related tasks from a task library to enable few-shot decomposition and tool use. These demonstrations follow a flexible but structured query language (Beurer-Kellner et al., 2022), such that it is easy to parse intermediate steps, stop generation to call external tools, and resume it after including the output of such tools (Figure 1). ART provides the LLM with demonstrations of how to decompose instances of several related tasks, and how to select and use any tool from the tool library that is represented in these demonstrations. This encourages the model to generalize from demonstrations to decompose a new task and use tools in appropriate places, zero-shot. It also enables users to fix any mistakes in the reasoning chain or add new tools by simply updating the task and tool libraries, providing new demonstrations where necessary (e.g. for the task at hand).</p>
<p>We construct a task library for 15 diverse BigBench (Srivastava et al., 2022) tasks, and evaluate ART on 19 unseen test tasks from BigBench, 6 MMLU tasks, and various tasks used by related work on tool use (SQUAD, TriviaQA, SVAMP, MAWPS). ART consistently matches or outperforms automatically generated CoT reasoning chains on 32 / 34 BigBench and all MMLU tasks, by an average of over 22 percentage points. Tool-use in particular improves performance on test tasks by an average of over 12.3 percentage points, as compared to when no tools are allowed (Table 3). ART improves over direct few-shot prompting by 10.8% percentage points on average across unseen BigBench and MMLU tasks. Improvements are particularly notable on unseen tasks requiring arithmetic and algorithmic reasoning, where ART improves over direct few-shot prompting by 12.5% and previous best-known results for GPT3 that use supervision for decomposition and/or tool use by 6.1% percentage points (Table 3).</p>
<p>Finally, ART enables human intervention and improvement of the reasoning process by simply updating the task and tool libraries with new demonstrations, making it very easy to improve performance on any specific task with minor human feedback. On 12 test tasks, ART with additional human feedback surpasses the best-known results for GPT3 by an average of over 20% points (Table 6).</p>
<h2>2 Related Work</h2>
<p>Scaled finetuning for low-resource adaptation Recent work has shown that finetuning LLMs on a broad range of public NLP datasets (with prefixed instructions) is an effective technique for cross-task generalization (Mishra et al., 2021; Sanh et al., 2021; Khashabi et al., 2020; Wei et al., 2021) in both the zero-shot and few-shot settings. Ouyang et al. (2022) show that aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback for desired model behavior (InstructGPT) further improves in-context learning performance on complex NLP tasks. Chung et al. (2022) show that finetuning on an aggregated mixture of tasks (T0, CoT, dialog, and code datasets) together with scaling models to 540B parameters achieves state-of-the-art in-context learning performance on several benchmarks such as BigBench and MMLU. ART uses API access to InstructGPT and Codex (LLM finetuned on code (Chen et al., 2021)) to leverage their emergent in-context learning abilities. Future improvements in scaled finetuning in LLMs will likely improve the performance on ART.</p>
<h2>Prompting with intermediate reasoning steps</h2>
<p>Chain-of-thought (CoT) prompting (Wei et al., 2022; Suzgun et al., 2022) is a popular gradient-free technique that encourages LLMs to generate intermediate reasoning steps prior to the final answer, with multiple task-specific variants (e.g. Least-to-most prompting (Zhou et al., 2022), Self-Ask (Press et al., 2022), Ask-me-anything (Arora et al., 2022), Successive prompting (Dua et al., 2022), decomposed prompting (Khot et al., 2022)). While such prompts were initially hand-crafted, recent work (Kojima et al., 2022) showed that LLMs can generate CoT-style multi-step reasoning in a zero-shot manner, when prompted with the prefix "Let’s think step-by-step". Zhang et al. (2022) use</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparing ART with related approaches for multi-step reasoning and tool-use</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>CoT</th>
<th>Auto CoT</th>
<th>Toolformer</th>
<th>ART</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-step reasoning</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Limited supervision</td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Tool use</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Extendable libraries</td>
<td></td>
<td></td>
<td></td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Cross-task transfer</td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Human feedback</td>
<td>$\checkmark$</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<p>LLMs to automatically generate such CoT-style prompts-AutoCoT-which are competitive with hand-crafted prompts in their performance on arithmetic and commonsense reasoning tasks. We compare ART, CoT and AutoCoT in Table 1. ART builds on this line of work, introducing a common language that enables cross-task demonstrations and flexible and extensible tool use, improving accuracy of intermediate reasoning steps.</p>
<p>Tool Use There is growing interest in overcoming LLM limitations with external tools such as search engines, web browsers, calculators, translation systems, and python interpreters (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022; Cobbe et al., 2021; Thoppilan et al., 2022; Gao et al., 2022; Chen et al., 2022). Most of these approaches either require large amounts of human supervision (Thoppilan et al., 2022; Komeili et al., 2022) or carefully constructed prompts tailored to specific tasks and particular tools. An alternative line of recent work uses self-supervision to teach LLMs to use search, translation, and a calculator (Schick et al., 2023)Toolformer. In contrast, since ART does not require any additional training or tool-specific prompts, it allows users flexibility both in terms of replacing the underlying LLM (e.g. when a new version of GPT-3 is released), and in replacing or adding new tools (either general-purpose tools or tools that are important for a specific task of interest). We compare ART and Toolformer in Table 1. In Section 3.4, we show how human-in-the-loop feedback - analyzing and debugging LLM generations and extending tool-use - can provide a large boost in the performance of ART while also extending it with new tools. This built-in feedback loop and adaptive capability of ART extends the capabilities of LLMs that are finetuning to follow instructions and use tools.</p>
<h2>3 ART</h2>
<p>With ART, a frozen LLM decomposes instances of a new task into multiple steps (using external tools whenever appropriate), despite not having explicit supervision for decomposition or tool use. In this section, we present an overview of ART, followed by more thorough descriptions of each individual component. We use the Physics Question Answering (PQA) task as a running example, which consists of high-school physics problems.</p>
<h3>3.1 Overview</h3>
<p>In Figure 2, ART is presented with a new task description and input instance. We also assume access to a few input-output pairs (not shown), with no decomposition or tool use supervision.</p>
<p>Prompt building. ART retrieves similar tasks from a task library (Figure 2(A); Section 3.2), and adds instances of those tasks as demonstrations in the prompt.</p>
<p>A demonstration in the task library is written in a specific format, defined by a custom parsing expression grammar ( $P e G$ ) (Section 3.2). The grammar is defined such that each task instance is decomposed into a sequence of sub-steps. Some of these sub-steps contain symbols corresponding to tools in a tool library (Section 3.3). We refer to these decompositions as programs, since the sequential reasoning steps and symbolic calls to tools are similar to a conventional program with function calls.</p>
<p>The resultant prompt consists of programs from related tasks and teaches the LLM how to effectively decompose instances of a new task-related sub-steps and tools in these programs can be used by the LLM for cross-task generalization.</p>
<p>In Figure 2(A), the demonstrations include calls to both search and code tools.</p>
<p>Generation. At generation time (Figure 2(B)), the LLM writes its own program. ART parses the program as it is generated, and pauses generation whenever a tool call is encountered in the generated text, resuming generation after the tool is called and its output is integrated back into the program. As illustrated in the figure, a search engine is used to find the appropriate physics formula, and then the LLM uses code generation and execution to substitute the given values and compute the answer.</p>
<p>Human feedback (optional). Humans can add new decomposition demonstrations to the task library, or add/edit tools in the tool library in order</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A run-through of ART on a new task, Physics QA. (A) Programs of related tasks like anachronisms and Math QA provide few-shot supervision to the LLM - related sub-steps and tools in these programs can be used by the LLM for cross-task generalization (shown in purple). (B) Tool use: Search is used to find the appropriate physics formula, and code generation and execution are used to substitute given values and compute the answer (shown in orange).
to improve performance on a particular task of interest, or in general. In Figure 3(C) a user corrects a specific program by including a step that adds the unit of measurement, and adds this (modified) program to the task library. While most of our experiments do not use such feedback, we show that it is very effective at drastically improving performance when task generalization does not happen automatically. Further, it gives users flexibility to add custom tools without retraining of the LLM.</p>
<h3>3.2 Task Library</h3>
<p>We construct a library of programs for a small seed set of tasks from Big-Bench (Srivastava et al., 2022), a collaborative benchmark that measures the capabilities and limitations of language models. Big-Bench tasks span categories of traditional NLP, mathematics, commonsense reasoning, and question-answering.</p>
<p>Constructing the task library. We identify five skills that are useful across more than half of the tasks in BigBench that encompass text classification or generation of short answers in English (see A.1). We group tasks in the benchmark by these skills into the following clusters:</p>
<ul>
<li>Arithmetic: arithmetic and algebra problems.</li>
<li>Code: Generating and executing python code.</li>
<li>Search and question decomposition: Single or multi-step questions that require search</li>
<li>Free-form reasoning: Explaining step-by-step reasoning in natural language</li>
<li>String Operations: Reformatting/editing strings, checking string entailment, etc.</li>
</ul>
<p>We then select 2-4 tasks from each cluster and write programs (decompositions) for a few instances of each task, including calls to external tools and real outputs of those tools. Examples of programs in each cluster are in Appendix A.1. These programs follow a specific grammar, as outlined below.</p>
<p>Program grammar The program format must be flexible in terms of task inputs, steps, and tool calls, such that a wide variety of NLP tasks can be covered. To do so, we define a query language (Beurer-Kellner et al., 2022) that extends the decomposed prompting format of Khot et al. (2022), since it can represent decomposed reasoning steps sequentially and incorporates function calls to external tools (like other LLMs). Each program consists of a series of nodes - a task input node, several sub-step nodes, and an answer node. The input node contains the task name, a simple instruction describing the task, and the input for an instance of the task: "Answer this high-school Physics question.
Input: Hector yanks...". The input node is followed by a sequence of sub-task nodes, represented as a (query, answer) pair " $Q_{i}: \ldots, # i: \ldots$ ". The sub-task query $Q_{i}$ has a sub-task name and sub-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Human feedback</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q1: [search]...What is the formula for the horizontal component of the tension force?</td>
<td style="text-align: center;">Q1: [string split] What are the letters in "nwist"</td>
</tr>
<tr>
<td style="text-align: center;">#1: ... calculated as Ftens*cosine( $\theta$ )where $\theta$ is ...</td>
<td style="text-align: center;">#1: %s</td>
</tr>
<tr>
<td style="text-align: center;">Q2: [generate code] Use formula Fx = Ftens*cosine( $\theta$ ) to solve: Hanks...</td>
<td style="text-align: center;">Q2: [string permutation] What are the possible permutations of 'nwist'?</td>
</tr>
<tr>
<td style="text-align: center;">#2: $\mathrm{Fx}=\mathrm{T}$ *math.cos(radians) ... print(Fx)</td>
<td style="text-align: center;">#2: ['nwist', 'nwits', 'nwsit', 'nwsti', 'nwtis', 'nwtsi', 'niwst', 'niwts', 'niswt',...</td>
</tr>
<tr>
<td style="text-align: center;">Q3: [code-execute] Execute snippet get the value of "Fx"</td>
<td style="text-align: center;">Q3: [lookup] which word in the list is a common English word ?</td>
</tr>
<tr>
<td style="text-align: center;">#3: 58.9789</td>
<td style="text-align: center;">#3: twins</td>
</tr>
<tr>
<td style="text-align: center;">Q4: [arithmetic] Round the answer to the nearest integer</td>
<td style="text-align: center;">Q4: [EOQ]</td>
</tr>
<tr>
<td style="text-align: center;">#4: 59</td>
<td style="text-align: center;">Ans: twins</td>
</tr>
<tr>
<td style="text-align: center;">Q5: [add unit] Add the appropriate unit of measurement to the answer.</td>
<td style="text-align: center;">Ans: twins</td>
</tr>
<tr>
<td style="text-align: center;">#5: 59 N</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q4: [EOQ]</td>
<td style="text-align: center;">TOOL LIBRARY</td>
</tr>
<tr>
<td style="text-align: center;">Ans: 59 N</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(a) Correcting generated programs by adding additional reasoning steps</td>
<td style="text-align: center;">TASK LIBRARY</td>
</tr>
</tbody>
</table>
<p>Figure 3: Human feedback to ART shown for (a) PQA where reasoning steps are added to the program and; (b) Word unscrambling where tool library is augmented with a new lookup tool.
task input ("Q1: [search] What is the formula..."), while the sub-task answer $# i$ is simply the output of the sub-task ("#1: The horizontal component (Fx) can be calculated..."). The program ends with a dummy sub-task ("Q3: [EOQ]"), followed by a final answer node ("Ans: 59N"). All examples in Figures 1 and 2 follow this format.</p>
<p>Task Retrieval Given a new task, ART retrieves $N$ tasks from the task library to construct a dynamic multi-task prompt. We explore two strategies to retrieve similar tasks, depending on what data is available. If a small number of labeled examples for the new task is available ( $\approx 50$ ), we iterate over all five task clusters and select a few task programs from each cluster to compose the prompt. Ultimately, the task cluster with the highest performance on the held-out set of examples is chosen when predicting on all unlabeled examples from the task. While this strategy requires a held-out set of input-output pairs, no additional supervision is needed to generate a decomposed program.</p>
<p>In the second strategy, we craft a few-shot prompt (Appendix A.2) with task pairs, where each task includes a name, instructions, and a few inputoutput examples. For each pair, we provide a label of "Similar" or "Not similar", and reasoning (e.g. "These are related because they require solving arithmetic word problems"). At run time, we pair the test task with every task in the task library, and choose the highest-ranked ones based on the log probability ratio between "Similar" and "Not similar". We explore both strategies in Section A.2.</p>
<h3>3.3 Tool Library</h3>
<p>Whenever a sub-task query name matches a tool name in the task library (e.g. " $Q_{i}:$ [search]"), generation is stopped and resumed after the tool is called and its output is incorporated into the partially completed program. We seed the tool library with the following tools (all of which have demonstrations in the task library). In particular, we describe the symbols used to represent these tools and their inputs. We also specify how the tool output is incorporated back into the program. Tool-specific implementation details and other tools added to ART during feedback (3.4) are in Appendix A.3.</p>
<p>Search We use SerpAPI ${ }^{2}$, which provides an API for Google search. The input to search is the sequence generated by the LLM after " $Q_{i}:$ [search]". We extract answer box snippets when they are available or combine the top-2 search result snippets together. For PQA in Figure 2(B), the search query is the original input followed by "What is the formula for the horizontal component of tension force?", and the output is ""... horizontal component (Fx) can be calculated as Ftens* $\operatorname{cosine}(\theta)$ ..."".</p>
<p>Code Generation We use the Codex (Chen et al., 2021) model for code generation. Input to code generation is the sequence generated by the LM after the sub-task query symbol " $Q i$ : [generate python code]". This argument is an instruction for code generation and is prompted to Codex as a multi-line comment in Python. For example, in Figure 2, Codex is prompted the instruction ""Use the formula Fx = Ftens * cosine( $\theta$ ) to solve..."" as a comment and generates "T = 72.0,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>theta $=35.0, \ldots, \mathrm{Fx}=\mathrm{T} *$ math.cos(radians)", which is appended to the incomplete program.</p>
<p>Code Execution We run Python code in a virtual Python environment with arithmetic, symbolic, and scientific computing packages pre-installed. The argument to code execute is the previous sub-task's answer sequence " $#(i-1): \ldots$ ", i.e. the python code snippet to be executed. For $i=1$, the task input is used as the argument since it potentially contains the code snippet to be executed. In Figure 2, the code snippet generated in the previous step is executed and the value of variable "Fx" is added to the incomplete program.</p>
<h3>3.4 Human feedback</h3>
<p>ART is specifically designed to be amenable to human feedback since it does not require additional finetuning. Consequently, users can incorporate feedback immediately into ART, by editing the task library and/or the tool library. Since ART generates multi-step reasoning programs that are interpretable, we explore feedback in the form of debugging, i.e. users edit existing programs rather than creating programs from scratch. These edits can be in the form of correcting sub-step outputs, adding/removing sub-steps (with appropriate inputs and answers), adding calls to new tools, etc.</p>
<p>For example, in Figure 3(a) the user edits a program by adding two sub-steps, in order to round the answer to the nearest integer and include the appropriate unit of measurement to the answer. This feedback demonstrates appropriate decompositions for the task, as these operations are still performed by the LLM (the tool library does not have "[arithmetic]" or "[add unit]" APIs). In contrast, in Figure 3(b) the user demonstrates the use of a dictionary "[lookup]" and implements it as a tool in the tool library. While most of our experiments do not rely on such feedback (and thus measure "zero shot" task transfer with no supervision for reasoning/tooluse), we show that simple operations like these can drastically improve performance on target tasks.</p>
<h2>4 Experimental Setup</h2>
<p>Evaluation Datasets In addition to 15 tasks in the task library (Section 3.2), we evaluate ART on 19 additional test tasks from BigBench which also belong to the five task clusters identified in Section 3.2. To check for cross-benchmark generalization, we further evaluate ART on a random subset of tasks from the MMLU benchmark [hendrycks et al., 2020]. Finally, we also evaluate on a subset of tasks used to evaluate Toolformer [Schick et al., 2023], in order to compare ART to a model fine-tuned for tool use.</p>
<p>Details We use InstructGPT (text-davinci-002) as the frozen LLM, and Codex as the code generation tool, with temperature set to 0.3 . We set the number of seed tasks in the prompt to $N=3$ and use 2 demonstration programs from each task. We measure the preferred scoring metric for each task as in [Srivastava et al. (2022)], and report performance averaged over 5 runs.</p>
<p>Baselines ART proposes an automatic framework to generate multi-step reasoning decompositions and use relevant available external tools within those decompositions. We compare with the following baselines:</p>
<ul>
<li>Few-shot/Direct: Prompting LLMs with input-output pairs (but no intermediate reasoning). We use 3 examples for BigBench and 5 examples for MMLU, as done in prior work [Suzgun et al., 2022]. We evaluate this baseline for both, GPT-3 and Codex, and report the higher of the two.</li>
<li>Auto-CoT: A baseline that automatically generates multi-step reasoning in natural language. A random subset of 5 examples is first used to elicit CoT-style reasoning (Input + Let's think step-by-step.). These examples and their generated output form the prompt for other unseen examples of the task. This baseline is free-form and does not include tools, and thus allows us to verify the effectiveness of our query language and task library. We evaluate this baseline for GPT-3.</li>
<li>ART-tool: ART with tool-use turned off, i.e. the LLM generates the output of every substep, to verify the gains from tool use.</li>
<li>GPT-3 Best: Best published GPT-3/Codex (175B) result with multi-step decomposition and/or tool use. These often include additional human supervision to decompose reasoning steps, and external tools to boost performance (with carefully constructed prompts).
Additional details about baselines and GPT-3 best models are in Appendix A.4.</li>
</ul>
<h2>5 Results</h2>
<p>We evaluate ART (without human feedback) on tasks in the task library (5.1), and on a variety</p>
<table>
<thead>
<tr>
<th>Task Name (Cluster)</th>
<th>Few Shot</th>
<th>AutoCot</th>
<th>ART</th>
<th>ART</th>
<th>GPT-3</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>w/o Tool Use</td>
<td></td>
<td>Best</td>
</tr>
<tr>
<td>Anachronisms (Search)</td>
<td>71.3</td>
<td>51.48</td>
<td>70.87</td>
<td>75.66</td>
<td>-</td>
</tr>
<tr>
<td>Musique (Search)</td>
<td>2.03</td>
<td>12.88</td>
<td>10.04</td>
<td>19.19</td>
<td>15.2</td>
</tr>
<tr>
<td>Hindu Knowledge (Search)</td>
<td>85.02</td>
<td>73.03</td>
<td>83.42</td>
<td>87.98</td>
<td>-</td>
</tr>
<tr>
<td>Known Unknown (Search)</td>
<td>68.90</td>
<td>56.09</td>
<td>80.43</td>
<td>80.43</td>
<td>-</td>
</tr>
<tr>
<td>$\Delta$ with ART (Search)</td>
<td>+9.0</td>
<td>+17.44</td>
<td>+4.6</td>
<td></td>
<td>+4.0</td>
</tr>
<tr>
<td>Elementary Math QA (Arithmetic)</td>
<td>56.40</td>
<td>74.52</td>
<td>58.04</td>
<td>68.04</td>
<td>-</td>
</tr>
<tr>
<td>Aqua-rat (Arithmetic)</td>
<td>20.54</td>
<td>34.41</td>
<td>36.29</td>
<td>54.20</td>
<td>54.1</td>
</tr>
<tr>
<td>GSM8K (Arithmetic)</td>
<td>7.79</td>
<td>21.99</td>
<td>53.4</td>
<td>71.00</td>
<td>71.6</td>
</tr>
<tr>
<td>Navigate (Arithmetic)</td>
<td>60.7</td>
<td>61.7</td>
<td>72.4</td>
<td>72.4</td>
<td>85.90</td>
</tr>
<tr>
<td>$\Delta$ with ART (Arithmetic)</td>
<td>+30.0</td>
<td>+18.25</td>
<td>+11.4</td>
<td></td>
<td>-4.7</td>
</tr>
<tr>
<td>Kth letter concatenation (String)</td>
<td>3.2</td>
<td>0.64</td>
<td>8.19</td>
<td>40.00</td>
<td>98.0</td>
</tr>
<tr>
<td>Language games (String)</td>
<td>35.14</td>
<td>18.58</td>
<td>11.19</td>
<td>23.08</td>
<td>-</td>
</tr>
<tr>
<td>Date Understanding (String)</td>
<td>37.53</td>
<td>38.90</td>
<td>52.05</td>
<td>-</td>
<td>70.41</td>
</tr>
<tr>
<td>Auto Debugging (Code)</td>
<td>62.94</td>
<td>38.24</td>
<td>55.29</td>
<td>62.94</td>
<td>-</td>
</tr>
<tr>
<td>Code Description (Code)</td>
<td>97.99</td>
<td>88.67</td>
<td>84.67</td>
<td>88.00</td>
<td>-</td>
</tr>
<tr>
<td>Formal Fallacies (CoT)</td>
<td>44.84</td>
<td>56.4</td>
<td>64.76</td>
<td>-</td>
<td>58.4</td>
</tr>
<tr>
<td>Hyperbation (CoT)</td>
<td>62.72</td>
<td>55.4</td>
<td>80.80</td>
<td>-</td>
<td>72.4</td>
</tr>
<tr>
<td>$\Delta$ with ART (Misc)</td>
<td>+9.6</td>
<td>+16.4</td>
<td>+13.7</td>
<td></td>
<td>-15.4</td>
</tr>
<tr>
<td>$\Delta$ with ART (Overall)</td>
<td>+14.90</td>
<td>+17.17</td>
<td>+7.91</td>
<td></td>
<td>-9.0</td>
</tr>
</tbody>
</table>
<p>Table 2: ART performance on tasks in the task library. (^{1}Human-crafted CoT <em>Wei et al. (2022); Suzgun et al. (2022)</em>, ^{2}Decomposed Prompting <em>Khot et al. (2022)</em>, ^{3}Self-Ask <em>Press et al. (2022)</em>, ^{4}PoT <em>Chen et al. (2022)</em>, ^{5}InstructGPT <em>Ouyang et al. (2022)</em>, ^{7}Code-davinci-002 <em>Chen et al. (2021)</em>). (-) For tasks using CoT reasoning, no tool use is used.</p>
<p>of test tasks from BigBench, MMLU, and QA benchmarks (5.2). Then, we show that ART can be further improved with more compute (self-consistency) and with human feedback (5.3).</p>
<h3>5.1 Results on the task library</h3>
<p>For tasks in the task library, demonstrations in the prompt include two instances of the task itself, along with other instances from tasks in the same cluster. We present results in Table 2, where tasks are organized by skill cluster. Even with decomposition demonstrations for only two instances, ART drastically improves performance over few-shot learning (+14.9 % points on average), in line with prior work on CoT. It does not do as well on language games, code description, and auto debugging — tasks that use code generation and/or code editing models. We observe that code generation errors often lead to cascading errors in reasoning.</p>
<p>Similarly, ART outperforms AutoCoT on most tasks even without any tool use (by 8% points on average). We hypothesize that the program format (and PeG grammar) is better at eliciting multi-step reasoning from models than free-form CoT due to the added structure to the reasoning. When tool use is turned on, ART outperforms AutoCoT on all tasks (+17.7 % points) minus one. Tools are called in $\approx 95\%$ of test instances, and significantly improve performance (+7.91 % points). Gains from tool use are particularly significant for arithmetic tasks that benefit from representing the arithmetic problem as code that executes complex arithmetic accurately (+21.85 on average). This has also been noted in prior work <em>Chen et al. (2022); Gao et al. (2022)</em>.</p>
<p>Compared to the best published GPT-3 results, ART is stronger or comparable in 5/8 tasks. For the others, further investigation indicates that the demonstrations provided by <em>Khot et al. (2022)</em> and <em>Suzgun et al. (2022)</em> are just more effective than the two programs we author for these tasks (we explore further human feedback for these in Appendix A.5). In sum, ART is stronger than few-shot learning and AutoCoT on the library tasks (where we provide 2 labeled decompositions), and comparable to the best published GPT-3 results.</p>
<h3>5.2 Test tasks (cross-task transfer)</h3>
<p>We measure cross-task generalization on test tasks where ART does not use explicit supervision for decomposition and tool use. ART retrieves demonstrations from the task library according to the first strategy in Section 3.2, which uses a small amount</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Name (Cluster)</th>
<th style="text-align: center;">Few Shot</th>
<th style="text-align: center;">AutoCot</th>
<th style="text-align: center;">ART w/o Tool Use</th>
<th style="text-align: center;">ART</th>
<th style="text-align: center;">GPT-3 <br> Best</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test Tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sentence Ambiguity (Search)</td>
<td style="text-align: center;">$70.67^{5}$</td>
<td style="text-align: center;">51.47</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Strategy QA (Search)</td>
<td style="text-align: center;">$55.49^{5}$</td>
<td style="text-align: center;">27.22</td>
<td style="text-align: center;">59.37</td>
<td style="text-align: center;">66.44</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Physics (Search)</td>
<td style="text-align: center;">$70.09^{5}$</td>
<td style="text-align: center;">61.83</td>
<td style="text-align: center;">59.13</td>
<td style="text-align: center;">67.55</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ with ART (Search)</td>
<td style="text-align: center;">$+3.7$</td>
<td style="text-align: center;">$+22.27$</td>
<td style="text-align: center;">$+5.9$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Physics Questions (Arithmetic)</td>
<td style="text-align: center;">$7.02^{5}$</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">6.30</td>
<td style="text-align: center;">20.37</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Operators (Arithmetic)</td>
<td style="text-align: center;">$71.23^{7}$</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">71.80</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Unit interpretation (Arithmetic)</td>
<td style="text-align: center;">$58.2^{7}$</td>
<td style="text-align: center;">41.20</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Repeat copy logic (Arithmetic)</td>
<td style="text-align: center;">$50.01^{7}$</td>
<td style="text-align: center;">15.63</td>
<td style="text-align: center;">31.25</td>
<td style="text-align: center;">44.38</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Object Counting (Arithmetic)</td>
<td style="text-align: center;">$39.2^{7}$</td>
<td style="text-align: center;">26.80</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">87.00</td>
<td style="text-align: center;">$81.20^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Penguins in a table (Arithmetic)</td>
<td style="text-align: center;">$58.23^{7}$</td>
<td style="text-align: center;">40.40</td>
<td style="text-align: center;">68.86</td>
<td style="text-align: center;">77.85</td>
<td style="text-align: center;">$72.34^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning about objects (Arithmetic)</td>
<td style="text-align: center;">$71.00^{7}$</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">45.35</td>
<td style="text-align: center;">64.34</td>
<td style="text-align: center;">$52.69^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Tracking shuffled objects (Arithmetic)</td>
<td style="text-align: center;">$22.39^{7}$</td>
<td style="text-align: center;">19.44</td>
<td style="text-align: center;">18.14</td>
<td style="text-align: center;">37.67</td>
<td style="text-align: center;">$36.32^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ with ART (Arithmetic)</td>
<td style="text-align: center;">$+19.0$</td>
<td style="text-align: center;">$+36.7$</td>
<td style="text-align: center;">$+23.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$+6.1$</td>
</tr>
<tr>
<td style="text-align: center;">Word Unscramble (String)</td>
<td style="text-align: center;">$40.72^{7}$</td>
<td style="text-align: center;">32.44</td>
<td style="text-align: center;">23.03</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Simple Text Editing (Code)</td>
<td style="text-align: center;">$35.31^{5}$</td>
<td style="text-align: center;">30.21</td>
<td style="text-align: center;">20.74</td>
<td style="text-align: center;">27.65</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CS Algorithms (Code)</td>
<td style="text-align: center;">$73.48^{7}$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">41.59</td>
<td style="text-align: center;">88.11</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Sports Understanding (CoT)</td>
<td style="text-align: center;">$69.74^{5}$</td>
<td style="text-align: center;">51.47</td>
<td style="text-align: center;">92.89</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$86.59^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Snarks (CoT)</td>
<td style="text-align: center;">$54.58^{5}$</td>
<td style="text-align: center;">57.24</td>
<td style="text-align: center;">57.13</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$65.2^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Disambiguation QA (Free-form)</td>
<td style="text-align: center;">$55.03^{5}$</td>
<td style="text-align: center;">48.45</td>
<td style="text-align: center;">55.89</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$60.62^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Temporal sequences (CoT)</td>
<td style="text-align: center;">$55.80^{7}$</td>
<td style="text-align: center;">19.70</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$81.8^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Ruin names (CoT)</td>
<td style="text-align: center;">$71.01^{5}$</td>
<td style="text-align: center;">55.28</td>
<td style="text-align: center;">60.22</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ with ART (Misc)</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">24.37</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$-9.4$</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ with ART (Overall)</td>
<td style="text-align: center;">$+6.9$</td>
<td style="text-align: center;">$+24.6$</td>
<td style="text-align: center;">$+16.7$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$-1.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">College Computer Science (Search)</td>
<td style="text-align: center;">41.00</td>
<td style="text-align: center;">43.99</td>
<td style="text-align: center;">63.40</td>
<td style="text-align: center;">67.80</td>
<td style="text-align: center;">$63.6^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">Astronomy (Search)</td>
<td style="text-align: center;">62.10</td>
<td style="text-align: center;">41.48</td>
<td style="text-align: center;">76.71</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">$62.5^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">Business Ethics (Search)</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">77.17</td>
<td style="text-align: center;">81.16</td>
<td style="text-align: center;">$72.7^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">Virology (Search)</td>
<td style="text-align: center;">50.03</td>
<td style="text-align: center;">49.52</td>
<td style="text-align: center;">71.60</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">$50.72^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">Geography (Search)</td>
<td style="text-align: center;">77.67</td>
<td style="text-align: center;">57.07</td>
<td style="text-align: center;">70.30</td>
<td style="text-align: center;">71.71</td>
<td style="text-align: center;">$81.8^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">Mathematics (Arithmetic)</td>
<td style="text-align: center;">36.67</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">39.50</td>
<td style="text-align: center;">45.66</td>
<td style="text-align: center;">$34.5^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">$\Delta$ with ART (MMLU)</td>
<td style="text-align: center;">$+14.6$</td>
<td style="text-align: center;">$+23.7$</td>
<td style="text-align: center;">$+3.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+8.5$</td>
</tr>
</tbody>
</table>
<p>Table 3: ART performance on BigBench tasks and MMLU tasks. ( ${ }^{1}$ Human-crafted CoT (Wei et al., 2022; Suzgun et al., 2022), ${ }^{5}$ InstructGPT (Ouyang et al., 2022), ${ }^{6}$ Scaled instruction finetuning (Chung et al., 2022), ${ }^{7}$ Codedavinci-002 (Chen et al., 2021)).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;">T-REx</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">MAWPS</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT3 (175B)</td>
<td style="text-align: center;">29.90</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: left;">Toolformer</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: left;">ART</td>
<td style="text-align: center;">$39.34(+5.5)$</td>
<td style="text-align: center;">$50.4(-3.1)$</td>
<td style="text-align: center;">$76.2(+46.8)$</td>
<td style="text-align: center;">$71.00(+27.0)$</td>
<td style="text-align: center;">$33.8(+16.1)$</td>
<td style="text-align: center;">$66.13(+17.33)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparing ART results on GPT3 (175B) model and (Schick et al., 2023), which is a smaller GPT-J model finetuned for tool-use. Results are reported from their paper (their code and models are not publicly available).
of labeled input-output pairs to pick a task cluster and sample demonstration programs from that cluster. ${ }^{3}$</p>
<p>BigBench test tasks Even though there is no decomposition or tool use supervision, the results in Table 3 are similar to those for tasks in the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>task library. ART outperforms few-shot learning (6.9 \% points). In particular, ART has significant improvements on arithmetic tasks ( +19.0 ) and is comparable to the few-shot performance on search tasks. Non-grammatical choices in ruin names and choices not in the input in temporal sequences are often incorrect, which the few-shot baseline may potentially learn to ignore, while ART attempts to</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Simple Text</th>
<th>CS</th>
<th>Strategy QA</th>
<th>Physics</th>
<th>Unit</th>
<th>Reasoning about</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Editing</td>
<td>Algorithms</td>
<td></td>
<td>Questions</td>
<td>Interpretation</td>
<td>colored objects</td>
</tr>
<tr>
<td>ART</td>
<td>27.65</td>
<td>88.11</td>
<td>66.44</td>
<td>20.37</td>
<td>53.99</td>
<td>64.34</td>
</tr>
<tr>
<td>+ Self Consistency</td>
<td>$30.67(+3.0)$</td>
<td>$90.99(+2.9)$</td>
<td>$70.76(+4.3)$</td>
<td>$24.07(+3.7)$</td>
<td>$57.20(+3.2)$</td>
<td>$69.11(+4.8)$</td>
</tr>
</tbody>
</table>
<p>Table 5: Improving ART via self-consistency (Wang et al., 2022). Ensembling model generations over 15 runs further boosts performance.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CoT</th>
<th></th>
<th>ART</th>
<th></th>
<th>GPT-3</th>
<th>Human</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>+Human</td>
<td></td>
<td>+ Human</td>
<td>Best</td>
<td>Feedback</td>
<td></td>
</tr>
<tr>
<td>CS Algorithms</td>
<td>0.0</td>
<td>23.0</td>
<td>88.11</td>
<td>92.73</td>
<td>73.48</td>
<td>C: longest common subsequence code</td>
</tr>
<tr>
<td>Reasong about objs.</td>
<td>33.33</td>
<td>67.75</td>
<td>64.34</td>
<td>98.90</td>
<td>71.00</td>
<td>C: Define object, color, count data structure</td>
</tr>
<tr>
<td>Repeat Copy Logic*</td>
<td>15.63</td>
<td>45.22</td>
<td>44.38</td>
<td>80.31</td>
<td>50.01</td>
<td>C: string edit operation</td>
</tr>
<tr>
<td>Sentence Ambiguity</td>
<td>51.47</td>
<td>72.33</td>
<td>73.33</td>
<td>83.67</td>
<td>70.67</td>
<td>C: Constrain queries to extract relevant info.</td>
</tr>
<tr>
<td>Simple Text editing*</td>
<td>30.21</td>
<td>35.31</td>
<td>27.65</td>
<td>36.11</td>
<td>35.31</td>
<td>C: string edit operation</td>
</tr>
<tr>
<td>Strategy QA*</td>
<td>27.22</td>
<td>29.19</td>
<td>66.44</td>
<td>69.15</td>
<td>55.49</td>
<td>C: Constrain queries to extract relevant info.</td>
</tr>
<tr>
<td>Physics*</td>
<td>61.83</td>
<td>68.21</td>
<td>67.55</td>
<td>72.55</td>
<td>70.09</td>
<td>A: [search] Formula that connects mass, ...</td>
</tr>
<tr>
<td>Temporal Sequences</td>
<td>19.70</td>
<td>30.22</td>
<td>49.5</td>
<td>88.00</td>
<td>81.8</td>
<td>A: [subquestion] Is X free Yam to Zam?</td>
</tr>
<tr>
<td>Track Shuffled objs.</td>
<td>19.44</td>
<td>36.48</td>
<td>37.67</td>
<td>99.86</td>
<td>36.32</td>
<td>C: Define object pair data struct, swap logic</td>
</tr>
<tr>
<td>Unit Interpretation*</td>
<td>41.2</td>
<td>41.2</td>
<td>53.99</td>
<td>95.0</td>
<td>58.2</td>
<td>A: [add unit] Add the right unit to the answer</td>
</tr>
<tr>
<td>Word Unscrambling*</td>
<td>32.44</td>
<td>33.40</td>
<td>42.70</td>
<td>62.11</td>
<td>40.72</td>
<td>T: lookup permutations in dictionary</td>
</tr>
<tr>
<td>Average</td>
<td>30.2</td>
<td>43.8</td>
<td>56.0</td>
<td>79.85</td>
<td>58.5</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 6: Improving ART and free-form CoT via self-consistency and human-in-the-loop feedback. (*) indicates that human-in-the-loop improvement was done over automatically generated CoT reasoning for these tasks. Feedback for ART includes correcting sub-steps in programs ("C:"), adding additional sub-steps("A:"), and defining new tools("T:"). Note that only five examples were edited for each task.
explicitly reason about them. As with library tasks, we observe that string manipulation tasks like simple text editing, word unscrambling, and repeat copy logic suffer from code generation errors.</p>
<p>As observed in the case of library tasks, ART is better than AutoCoT on almost all tasks (24.6 $\%$ points). Tools are once again called very frequently ( $89 \%$ of instances), and are responsible for a significant fraction of the gains over baselines.</p>
<p>When compared to the best published GPT-3 results, ART performs favorably on average, especially on arithmetic tasks ( $+6.1 \%$ points). As before, it does worse in tasks where good human demonstrations of how to decompose the task itself (provided by Suzgun et al. (2022)) have a big impact. We re-evaluate ART with more human feedback on these tasks in 5.3, but even without that we conclude that ART is competitive on BigBench even when we do not have supervision for decompositions for the task at hand (i.e. there is cross-task generalization).</p>
<p>Other benchmarks To make sure ART does not overfit to BigBench-style tasks, we evaluate performance on additional benchmarks. We report performance on randomly selected tasks from the MMLU benchmark (Hendrycks et al., 2020) in</p>
<p>Table 3, where ART is more effective than all baselines on 5/6 tasks ( +8.5 points better than few-shot baseline on average), despite having no supervision for demonstrations or tool use. MMLU requires extensive world knowledge, and thus most of these tasks benefit the most from the search tool.</p>
<p>In Table 4, we compare ART to a random subset of tasks used to evaluate Toolformer (Schick et al., 2023), a model finetuned to use a variety of tools. The comparison is not exact since Toolformer uses a smaller GPT-J model, but it is informative that ART outperforms Toolformer by a large margin on 5/6 of these tasks. To make sure these gains are not simply a result of model scale, we also use vanilla GPT-3 as a baseline, which yields much worse results than ART on all tasks. Besides improved performance, we note again that ART does not require additional fine-tuning when new tools or new base LLMs are introduced, and also is amenable to further improvement at the cost of compute or human feedback.</p>
<h3>5.3 Improving ART</h3>
<p>Self-consistency Previous work has noted benefits in generating multiple LLM outputs and taking the most frequent answer (a process known as self-consistency), particularly for settings with</p>
<p>multi-step reasoning (Khot et al., 2022; Wang et al., 2022). In Table 5, we present self-consistency results (generating 15 outputs) for ART on a subset of tasks and see that it consistently improves performance, at the cost of extra computation.</p>
<p>Human feedback We also pilot the use of taskspecific feedback in Table 6, by having one of the authors edit 5 random instances of modelgenerated programs that resulted in errors for each task. When editing, we correct errors in sub-steps (denoted as "C:"), adds missing substeps ("A:"), or defines a new tool and demonstrates its use ("T:"). For example, this involved introducing an "add unit" sub-step for the PQA task, and implementing a dictionary lookup function as a tool for the "Word Unscrambling" task (both illustrated by Figure 3).</p>
<p>We also compare human feedback applied to CoT-style reasoning. Suzgun et al. (2022) already provide reference CoT-style reasoning for some tasks. For datasets where human-authored CoT reasoning is unavailable, we correct the output of the automatic CoT baseline, as indicated in Table 6. The same author edits 5 random instances of AutoCoT decompositions that lead to errors on the same tasks, correcting errors in sub-steps or adding new sub-steps. As a reference, the edits included 35% of tokens in the baseline, and 15.7% of tokens in the ART programs. This included correcting substep arguments and outputs in 72% of the chosen tasks and adding additional sub-steps in 44% of the tasks. New tool definitions were added for two tasks — dictionary lookup for word unscrambling and a Prolog engine for formal fallacies.</p>
<p>In both cases, editing programs and adding them as demonstrations leads to significant gains in performance on the task at hand. However, the gain is much more dramatic in ART, leading it to consistently outperform the best published GPT-3 baseline for the task at hand. Further, these corrected programs and tools can be added to the task and tool libraries, and our prior results in Table 3 suggest that they potentially help improve ART on other tasks as well. This pilot indicates that besides being competitive on cross-task generalization, ART is very amenable to task-specific improvement with minimal human intervention. We report similar results in the task library in A.5.</p>
<h2>6 Conclusion</h2>
<p>We introduce ART, a gradient-free approach for automatic multi-step reasoning generation and automatic tool-use for a large black-box language model. Our main contributions include a lightweight grammar to represent multi-step reasoning as a program (with tool calls and arguments), an extensible library of seed tasks for which programs are authored, and a tool library that consists of useful external utilities like search, code generation, and execution. The interpretable reasoning framework also allows humans to improve task decomposition and tool use to boost performance. ART achieves a substantial improvement over few-shot prompting and automatic generation of CoT reasoning on unseen tasks in the BigBench and MMLU benchmarks, and substantially exceeds performance on hand-crafted CoT prompts when human feedback is incorporated. ART also benefits from approaches such as self-consistency, or from new and more powerful LLMs trained for tool use.</p>
<h2>References</h2>
<ul>
<li>Arora et al. (2022) Simran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. 2022. Ask me anything: A simple strategy for prompting language models. <em>arXiv preprint arXiv:2210.02441</em>.</li>
<li>Berrer-Kellner et al. (2022) Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2022. Prompting is programming: A query language for large language models. <em>arXiv preprint arXiv:2212.06094</em>.</li>
<li>Brown et al. (2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. <em>arXiv preprint arXiv:2005.14165</em>.</li>
<li>Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. <em>arXiv preprint arXiv:2107.03374</em>.</li>
<li>Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>arXiv preprint arXiv:2211.12588</em>.</li>
<li>Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. <em>arXiv preprint arXiv:2204.02311</em>.</li>
<li>Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi</li>
</ul>
<p>Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive prompting for decomposing complex questions. arXiv preprint arXiv:2212.04092.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8460-8478, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.</p>
<p>Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search \&amp; generation for dialogue and prompt completion. arXiv preprint arXiv:2203.13224.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<h2>A Appendix</h2>
<h2>A. 1 Task Library</h2>
<p>Library Design We analyzed input-output instances of all 200 tasks in BigBench, filtered out text classification and short answer generation tasks in English, and created a list of reasoning skills that were relevant to solving each task. We do not focus on long text understanding, long text generation, and multi-lingual tasks in this work. We find that most of these tasks rely on a few common skills mentioned below:
Visual Reasoning, Temporal Reasoning, Propositional logic, Natural Logic, Machine Translation, Web Search, Knowledge Base or Database lookup, Recursive sub-question decomposition, Long text
understanding, Database Operations, Algebra and Arithmetic, Code Generation and Editing, Text Tagging/Annotation(linguistic markers), Specialized Search(eg. looking up linguistic knowledge, scientific knowledge etc), String editing, Recursive operations over multiple choices, Topic classification, Evidence extraction, conditional Text Generation/Editing, and Sentence similarity.
In this work, we choose to focus on the five most used skills that cover a significant proportion of BigBench tasks for classification (over 50 of the 91 tasks that remained after filtrating out long-text understanding, generation, and multi-lingual tasks). We randomly select 2-4 tasks from each of these 5 task clusters and author decomposed programs with appropriate tool use for these tasks. This results in a total of 15 tasks that compose the task library.</p>
<ul>
<li>Arithmetic: Elementary MathQA, Grade school math (GSM8K), arithmetic Questions about ratios (Aqua-Rat), Navigate</li>
<li>Code: Auto Debugging, Code Description</li>
<li>Search and question decomposition: Anachronims, Multi-step question answering (Musique), Hindu Knowledge, Known Unknown</li>
<li>Free-form reasoning: Formal fallacies, Hyperbation</li>
<li>String Operations: Kth letter concatenation, Language games, Date understanding</li>
</ul>
<p>Cluster Programs The programs written for tasks in each task cluster are shown in Table 7 for tasks involving string editing and manipulation, in Table 8 for arithmetic and algebra tasks, in Table 10 for code generation, editing and debugging tasks, in Table 9 for tasks benefit from search of world knowledge, and in Table 11 for tasks that benefit from eliciting chain-of-thought reasoning following the prompt "Let's think step-by-step".</p>
<p>Program Format We define a parsing expression grammar (PEG) (shown in Figure 4) that describes the language used to write multi-step reasoning programs. This grammar is designed to parse full programs of the form "Input: ... $Q_{1}$ : ... #1:... $Q_{n}$ : [EOQ] Ans: ". We use the python library parsimoneous ${ }^{4}$ to construct the grammar and parse programs generated by LLMs.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">String Operations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the</td>
</tr>
<tr>
<td style="text-align: left;">task. You can use string operations like splitting, reformatting, editing or merging. You can also use other operations like</td>
</tr>
<tr>
<td style="text-align: left;">arithmetic and logic.</td>
</tr>
<tr>
<td style="text-align: left;">Description: (Date Understanding) Find the required date in MM/DD/YYYY using information about related events and</td>
</tr>
<tr>
<td style="text-align: left;">dates in the input. Clue: First find what day is today.</td>
</tr>
<tr>
<td style="text-align: left;">Input: The deadline is Jun 1, 2021, which is 2 days away from now. What is the date 24 hours later in MM/DD/YYYY?</td>
</tr>
<tr>
<td style="text-align: left;">Q1: [string reformat] Jun 1, 2021 in MM/DD/YYYY</td>
</tr>
<tr>
<td style="text-align: left;">#1: 06/01/2021</td>
</tr>
<tr>
<td style="text-align: left;">Q2: [arithmetic] 06/01/2021 is 2 days away from now. What date is today?</td>
</tr>
<tr>
<td style="text-align: left;">#2: Today is 04/01/2021</td>
</tr>
<tr>
<td style="text-align: left;">Q3: [arithmetic] What date is 24 hours later than today?</td>
</tr>
<tr>
<td style="text-align: left;">#3: 05/01/2021</td>
</tr>
<tr>
<td style="text-align: left;">Q4: [EOQ]</td>
</tr>
<tr>
<td style="text-align: left;">Ans: 05/31/2021</td>
</tr>
<tr>
<td style="text-align: left;">Description: (Language games) Translate English into Pig Latin.</td>
</tr>
<tr>
<td style="text-align: left;">Input: (English) Sami made his way across the bar and hugged Layla.</td>
</tr>
<tr>
<td style="text-align: left;">Q1: [string split] What are the words in "Sami made his way across the bar and hugged Layla."?</td>
</tr>
<tr>
<td style="text-align: left;">#1: ["Sami", "made", "his", "way", "across", "the", "bar", "and", "hugged", "Layla", "."]</td>
</tr>
<tr>
<td style="text-align: left;">Q2: [string edit] Transfer the initial consonant of each word to the end of the word and adding "ay" after it.</td>
</tr>
<tr>
<td style="text-align: left;">#2: ["Amisay", "ademay", "ishay", "ayway", "acrossyay", "ethay", "arbay", "andyay", "uggedhay", "Aylalay", "."]</td>
</tr>
<tr>
<td style="text-align: left;">Q3: [string merge] Concatenate #2 into a full sentence.</td>
</tr>
<tr>
<td style="text-align: left;">#3: Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay.</td>
</tr>
<tr>
<td style="text-align: left;">Q4: [EOQ]</td>
</tr>
<tr>
<td style="text-align: left;">Ans: Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay.</td>
</tr>
<tr>
<td style="text-align: left;">Description: (Kth letter concatenation) Take the letters at position 3 of the words in a list of words and concatenate them</td>
</tr>
<tr>
<td style="text-align: left;">using a space.</td>
</tr>
<tr>
<td style="text-align: left;">Input: Take the letters at position 3 of the words in "Savita Saeed Ramos Sato Yadav" and concatenate them using a space.</td>
</tr>
<tr>
<td style="text-align: left;">Q1: [string split] What are the words in "Savita Saeed Ramos Sato Yadav"?</td>
</tr>
<tr>
<td style="text-align: left;">#1: ["Savita", "Saeed", "Ramos", "Sato", "Yadav"]</td>
</tr>
<tr>
<td style="text-align: left;">Q2: [string index] What is the third letter of words in the list in #1?</td>
</tr>
<tr>
<td style="text-align: left;">#2: ["v", "e", "m", "t", "d"]</td>
</tr>
<tr>
<td style="text-align: left;">Q3: [string merge] Concatenate #2 with spaces</td>
</tr>
<tr>
<td style="text-align: left;">#3: "v e m t d"</td>
</tr>
<tr>
<td style="text-align: left;">Q4: [EOQ]</td>
</tr>
<tr>
<td style="text-align: left;">Ans: v e m t d</td>
</tr>
<tr>
<td style="text-align: left;">Description: \%s</td>
</tr>
<tr>
<td style="text-align: left;">Input: \%s</td>
</tr>
<tr>
<td style="text-align: left;">Q1:</td>
</tr>
</tbody>
</table>
<p>Table 7: Programs in the task library for tasks requiring string manipulation.</p>
<p>Arithmetic
In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task. You can generate python code to solve arithmetic and algebra equations in using functions from sympy.
from sympy import Symbol
from sympy import simplify
import math
from sympy import solve_it
# solve_it(equations, variable): solving the equations and return the variable value.
Description: (Aqua-rat) Solve the following arithmetic problems on ratios and fractions, writing out intermediate arithmetic calculations as python code. Store your result as a variable named 'ans'.
Input: In a flight of 600 km , an aircraft was slowed down due to bad weather. Its average speed for the trip was reduced by $200 \mathrm{~km} / \mathrm{hr}$ and the time of flight increased by 30 minutes. The duration of the flight is: A) 1 hour B) 2 hours C) 3 hours D) 4 hours E) 5 hours
Q1: [generate python code] write python code to solve the problem, using math and sympy.
#1:
duration $=$ Symbol('duration', positive=True)
delay $=30 / 60$
total_disntace $=600$
original_speed $=$ total_disntace $/$ duration
reduced_speed $=$ total_disntace $/($ duration + delay $)$
solution $=$ solve_it(original_speed - reduced_speed - 200, duration)
ans $=$ solution[duration]
print(ans)
Q2: [code execute] Execute the python code in #1 and get the value of "ans"
#2:
1.0
Q1: [compare] Which of the options among A) 1 hour B) 2 hours C) 3 hours D) 4 hours E) 5 hours is most similar to the answer? #3: A
Q4: [EOQ]
Ans: A
Description: (Elementary Math) Solve the following middle-school arithmetic problems, writing out intermediate arithmetic calculations as python code. Store your result as a variable named 'ans'.
Input: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $\$ 2$ per fresh duck egg. How much in dollars does she make every day at the farmers' market?
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as 'ans'
#1:
total_eggs $=16$
eaten_eggs $=3$
baked_eggs $=4$
sold_eggs = total_eggs - eaten_eggs - baked_eggs
dollars_per_egg $=2$
ans $=$ sold_eggs * dollars_per_egg
print(ans)
Q2: [code execute] Execute the python code in #1 and get the value of "ans"
#2: 18
Q3: [EOQ]
Ans: 18
Description: (Grage school Math) Solve the following middle-school arithmetic problems, writing out intermediate arithmetic calculations as python code. Store your result as a variable named 'ans'.
Input: Joseph and Getty went to buy ice creams, they together bought 36 ice creams. On the way back, Joseph ate 12 of the ice creasm, and he has 2 ice creams left now.
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as 'ans'
#1:
num_ice_creams_bought_by_joseph $=2+12$
total_ice_creams $=36$
ans $=$ total_ice_creams - num_ice_creams_bought_by_joseph
print(ans)
Q2: [code execute] Execute the python code in #1 and get the value of "ans"
#2: 22
Q3: [EOQ]
Ans: 22
Description: \%s
Input: \%s
Q1:</p>
<table>
<thead>
<tr>
<th>Search</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task.</td>
<td></td>
</tr>
<tr>
<td>You can use search functions like Google search in one or more of your substeps, if there in insufficient information. Other</td>
<td></td>
</tr>
<tr>
<td>functions like arithmetic and logical operations can also be used.</td>
<td></td>
</tr>
<tr>
<td>Description: (Knwon or Unknwon) Choose the option that best answers the question. If the question does not have a known</td>
<td></td>
</tr>
<tr>
<td>answer, choose "Unknown".</td>
<td></td>
</tr>
<tr>
<td>Input: How many hairs were on Neil Armstrong's head when he landed on the moon?</td>
<td></td>
</tr>
<tr>
<td>choice: Unknown</td>
<td></td>
</tr>
<tr>
<td>choice: Five million</td>
<td></td>
</tr>
<tr>
<td>Q1: [search] How many hairs were on Neil Armstrong's head when he landed on the moon?</td>
<td></td>
</tr>
<tr>
<td>#1:</td>
<td></td>
</tr>
<tr>
<td>Apollo 11 (July 16-24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil</td>
<td></td>
</tr>
<tr>
<td>Armstrong and lunar module pilot Buzz Aldrin.</td>
<td></td>
</tr>
<tr>
<td>Neil Alden Armstrong (August 5, 1930 - August 25, 2012) was an American astronaut and aeronautical engineer who became</td>
<td></td>
</tr>
<tr>
<td>the first person to walk on the Moon.</td>
<td></td>
</tr>
<tr>
<td>Q2: [subquestion] Does the information help answer the question? There could be no definitive answer because the question is</td>
<td></td>
</tr>
<tr>
<td>too specific, about personal details not in public record, because the answer is not yet known, or the question is opinion-based.</td>
<td></td>
</tr>
<tr>
<td>#2: No. The question is too specific</td>
<td></td>
</tr>
<tr>
<td>Q3: [compare] What is the final answer?</td>
<td></td>
</tr>
<tr>
<td>#3: Unknown</td>
<td></td>
</tr>
<tr>
<td>Q4: [EOQ]</td>
<td></td>
</tr>
<tr>
<td>Ans: Unknown</td>
<td></td>
</tr>
<tr>
<td>Description: (Anachronisms) An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper</td>
<td></td>
</tr>
<tr>
<td>time. Does the sentence contain an anachrornism? Answer Yes/No.</td>
<td></td>
</tr>
<tr>
<td>Input: President George H. W. Bush called his generals to the Oval Office at the outset of the Gulf War.</td>
<td></td>
</tr>
<tr>
<td>Q1: [tag] What are the entities in this sentence?</td>
<td></td>
</tr>
<tr>
<td>#1:</td>
<td></td>
</tr>
<tr>
<td>President George H. W. Bush</td>
<td></td>
</tr>
<tr>
<td>Gulf War</td>
<td></td>
</tr>
<tr>
<td>Q2: [search] When was President George H. W. Bush president?</td>
<td></td>
</tr>
<tr>
<td>#2: George H. W. Bush's tenure as the 41st president of the United States began with his inauguration on January 20, 1989,</td>
<td></td>
</tr>
<tr>
<td>and ended on January 20, 1993.</td>
<td></td>
</tr>
<tr>
<td>Q3: [search] When was the Gulf War fought?</td>
<td></td>
</tr>
<tr>
<td>#3: The Gulf War[b] was a 1990-1991 armed campaign waged by a 35-country military coalition in response to the Iraqi</td>
<td></td>
</tr>
<tr>
<td>invasion of Kuwait.</td>
<td></td>
</tr>
<tr>
<td>#4: [subquestion] Could these entities have co-existed based on thier time periods alone?</td>
<td></td>
</tr>
<tr>
<td>Yes. Their time periods intersect.</td>
<td></td>
</tr>
<tr>
<td>Q5: [generate output] Is this an anachronism?</td>
<td></td>
</tr>
<tr>
<td>#5: No</td>
<td></td>
</tr>
<tr>
<td>Q6: [EOQ]</td>
<td></td>
</tr>
<tr>
<td>Ans: No</td>
<td></td>
</tr>
<tr>
<td>Description: (Hindu Knowledge) Answer questions about Hindu mythology by choosing the option that best answers the</td>
<td></td>
</tr>
<tr>
<td>question.</td>
<td></td>
</tr>
<tr>
<td>Input: In the Mahabharata, Karna is cursed to forget the incantations needed to use which weapon?</td>
<td></td>
</tr>
<tr>
<td>choice: Anjalikastra</td>
<td></td>
</tr>
<tr>
<td>choice: Narayanastra</td>
<td></td>
</tr>
<tr>
<td>choice: Agneyastra</td>
<td></td>
</tr>
<tr>
<td>choice: Brahmastra</td>
<td></td>
</tr>
<tr>
<td>Q1: [search] In the Mahabharata, Karna is cursed to forget the incantations needed to use which weapon?</td>
<td></td>
</tr>
<tr>
<td>#1: As a result, he cursed Karna, saying that HIS MARTIAL SKILLS, including the use of BRAHMASTRA, would abandon</td>
<td></td>
</tr>
<tr>
<td>him when he needed them most. Indra, the King of Gods, stung Karna in the form of a bee to get him cursed by Parshuram.</td>
<td></td>
</tr>
<tr>
<td>Karna walked through the woods in despair, feeling dejected by the curse. A skilled \&amp; devoted warrior...</td>
<td></td>
</tr>
<tr>
<td>Q2: [compare] Which option is the answer in #3 most similar to?</td>
<td></td>
</tr>
<tr>
<td>#2: Brahmastra</td>
<td></td>
</tr>
<tr>
<td>Q3: [EOQ]</td>
<td></td>
</tr>
<tr>
<td>Ans: Brahmastra</td>
<td></td>
</tr>
<tr>
<td>Description: \%s</td>
<td></td>
</tr>
<tr>
<td>Input: \%s</td>
<td></td>
</tr>
<tr>
<td>Q1:</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 9: Programs in the task library for tasks requiring search for world knowledge.</p>
<h1>Code Generation and Editing</h1>
<p>In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task. You can use a python code generation and execution function in one or more of your substeps, if required. Other functions like arithmetic and logical operations can also be used.
Description: (Auto Debugging) Debug the following code snippets by finding the answer or the error message.
Input:
...
if $x&lt;5$ :
pass
...
What error does this program surface?
Q1: [execute] Execute the following python code snippet.
if $\mathrm{x}&lt;5$ :
pass
#1:
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
NameError: name ' $x$ ' is not defined
Q2: [generate answer] What is the final error message?
#2: NameError: name ' $x$ ' is not defined
Q3: [EOQ]
Ans: NameError: name ' $x$ ' is not defined
Description: (Code description) Given a python code snippet, choose the option that is the best description of the code snippet.
Input:
Python code:
try:
$\mathrm{n}=\operatorname{int}($ input $())$
$\mathrm{m}=\operatorname{int}($ input $())$
integer_sum $=\operatorname{int}(\mathrm{n})+\operatorname{int}(\mathrm{m})$
print(integer_sum)
except:
print('error')
choice: prints number between 5 and 6
choice: try input and except error
choice: inputs the string 'try'
choice: prints sum of two input numbers only if they are integers otherwise raises error
Q1: [code generate] prints number between 5 and 6
#1:
import random
print(random.uniform(5,6))
Q2: [code generate] try input and except error
#2:
try:
file = open(file_name, "r")
data $=$ file.read()
file.close()
except:
print('error')
Q3: [code generate] inputs the string 'try'
#3: print('try')
Q4: [code generate] prints sum of two input numbers only if they are integers otherwise raises error
#4:
#!/usr/bin/python
a=raw_input("enter first number: ")
b=raw_input("enter second number: ")
try:
sum=int(a)+int(b)
print "sum is: ",sum
except:
print "enter integer values only"
Q5: [compare] Which of the generated code snippets are most like the original one?
#5: prints sum of two input numbers only if they are integers otherwise raises error
Q6: [EOQ]
Ans: prints sum of two input numbers only if they are integers otherwise raises error
Description: \%s
Input: \%s
Q1:</p>
<h1>CoT Reasoning</h1>
<p>In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task. Thinking though the problem explicitly can be one of the substeps you use.
Description: (Sports Understanding) Determine whether an artificially constructed sentence relating to sports is plausible. The final answer should be "yes" or "no".
Input: Is the following sentence plausible? "Santi Cazorla scored a touchdown."
Q1: [think step-by-step]
#1: Let's think step-by-step. Santi Cazorla is a soccer player. Touchdown is part of American football and rugby. So the answer is no.
Q2: [EOQ]
Ans: no
Description: (Hyperbation) Identify correct adjective ordering from the two choices. This involves selecting what would be considered the more inexplicably "intuitive" sentence by a native English speaker.
Input: Which sentence has the correct adjective order:
Options:
(A) repulsive small Brazilian exercise ship
(B) Brazilian repulsive exercise small ship</p>
<p>Q1: [think step-by-step]
#1: Let's think step-by-step. When there is more than one adjective before a noun, the adjectives need to respect the following order before a noun: "[1. opinion] [2. size] [3. age] [4. shape] [5. color] [6. origin] [7. material] [8. purpose] noun".
Option (A): "repulsive small Brazilian exercise ship". (1) "repulsive" falls into the opinion category. (2) "small" falls into the size category. (3) "Brazilian" falls into the origin category. (4) "exercise" falls into the purpose category. Option (A) has the following adjective order: [1. opinion] [2. size] [6. origin] [8. purpose] (or, in numeric terms, 1268 ). Because $1&lt;2&lt;6&lt;8$ is correct, (A) has the correct ordering.
Option (B): "Brazilian repulsive exercise small ship". Option (B) has the following adjective order: [6. origin] [1. opinion] [8. purpose] [2. size] (or, in numeric terms, 6182 ). Because $6&lt;1&lt;8&lt;2$ is not correct, (B) does not have the correct ordering. So the answer is (A).
Q2: [EOQ]
Ans: (A)
Description: (Formal Fallacies) Distinguish deductively valid syllogistic arguments from formal fallacies, paying specific attention to negations.
Input: "It is not always easy to see who is related to whom - and in which ways. The following argument pertains to this question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy."
Is the argument, given the explicitly stated premises, deductively valid or invalid?
Options:</p>
<ul>
<li>valid</li>
<li>invalid</li>
</ul>
<p>Q1: [think step-by-step]
#1:
Let's think step-by-step.
(1) Lesley is a close friend of Fernando: Lesley = friend(Fernando).
(2) Being a close friend of Fernando or a schoolmate of Lowell is sufficient for being a great-grandfather of Leroy: If $\mathrm{X}=$ friend(Fernando) OR SCHOOLMATE(Lowell), then $\mathrm{X}=$ great-grandfather(Leroy).
Hypothesis: Does it follow that Lesley is a great-grandfather of Leroy: Lesley = great-grandfather(Leroy)?
Let's see whether the Hypothesis can be deduced from the arguments (1) and (2) by logical reasoning?
By (1), we have Lesley = friend(Fernando). By (2), we have if Lesley = friend(Fernando), then Lesley = greatgrandfather(Leroy).
So, it is true that Lesley is a great-grandfather of Leroy. So the answer is valid.
Q2: [EOQ]
Ans: valid
Description: (Reasoning about colored objects) Given a collection of colored objects in the text input, answer the question at the end of the input.
Input: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a blue stress ball. What color is the stress ball?
Q1: [think step-by-step]
#1: Let's think step-by-step. According to this question, the color of the stress ball is blue. So the answer is blue.
Q2: [EOQ]
Ans: blue
Description: $\% \mathrm{~s}$
Input: $\% \mathrm{~s}$
Q1: ${ }^{\prime \prime}$
Table 11: Programs in the task library for tasks requiring free-form chain-of-thought style reasoning about logic and linguistics.</p>
<p>A.2 Task Selection</p>
<p>When provided new task description and input instance, ART retrieves N tasks from the task library to constructs a dynamic multi-task prompt. We explore two strategies for task selection.</p>
<p>Task-Cluster based 50 examples used for tuning except in cases with fewer than 100 examples, where we reduce this number to 10.</p>
<p>We iterate over all five task clusters in the library, prompting the LLM with demonstration programs from just one cluster at a time. For example, we only use programs from arithmetic tasks as demonstrations in the prompt in one such iteration. The task cluster with the highest performance on the held-out set of examples (50) is chosen. This strategy requires as many API calls as there are task clusters, and a held-out set of input-output pairs for the new task. Note that no additional supervision is needed for the new task to generate a decomposed program.</p>
<p>LLM-Similarity based The LLM is prompted with pairs of tasks. Some pairs contain two tasks from the same cluster and are labeled "Similar" while some pairs don’t and are labeled "Not similar". Additionally, we also provide reasoning for the decision — "Elementary math QA and GSM8K are related tasks because they both require solving arithmetic word problems". A task in this prompt is represented by its name, an instruction, and a few input-output pairs. We use the prompt in Table 13 to prompt LLMs.</p>
<p>The LLM is prompted for a decision for every library task paired with the new task. We choose the top-N tasks ranked by the ratio of log probabilities of "Similar" to "Not similar". This strategy requires fewer held-out examples but is prone to high variance in performance based on the tasks chosen in every experimental run. For PQA, the most similar tasks chosen based on the LLM-based similarity are anachronisms and GSM8K.</p>
<p>In Table 12, we examine the effect of changing the task selection strategy in ART. Instead of choosing the task cluster with the highest held-out performance over 50 examples, we use the LLMbased similarity score to choose task programs for the prompt. This strategy is worse on average compared to tuning performance on a held-out set and has high variance over several runs where different tasks are chosen by the LLM. Selecting similar tasks that share sub-tasks and tools (without any su- pervision) is still a challenging task for LLMs, and will explore this direction further in future work.</p>
<h3>A.3 Tool Use</h3>
<p>Code Generation We use the Codex (Chen et al., 2021) model for code generation. Argument for code generation is the previous sub-task’s answer sequence ""#i-1:..."" and the sequence generated by the LM after the sub-task query symbol ""Qi:[generate python code]"". When $i=1$, the instance input is used as the first argument. We include the previous answer/input since it often contains information relevant to generating accurate code, like the arithmetic word problem for which code needs to be generated (see Table 8 for examples). Both arguments are provided to Codex as a multi-line python comment, while maintaining their original formatting. To keep the answer variable consistent, we also append an additional instruction: Store the final answer in variable 'ans' and print it. For example:</p>
<p>Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers market daily for \$2 per fresh duck egg. How much in dollars does she make every day at the farmers market?</p>
<p>is used to prompt Codex as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;&quot;</span><span class="err">&quot;</span>
<span class="err">Janet&#39;s ducks lay 16 eggs per day. She</span>
<span class="err">eats three for breakfast every morning</span>
<span class="err">and bakes muffins for her friends every</span>
<span class="err">day with four. She sells the remainder</span>
<span class="err">at the farmers market daily for \$2 per</span>
<span class="err">fresh duck egg. How much in dollars does</span>
<span class="err">    she make every day at the farmers</span>
<span class="err">market?</span>
<span class="err">Write down the arithmetic or algebra</span>
<span class="err">equations as python code, storing the</span>
<span class="err">answer as &#39;ans&#39; and print it.</span>
<span class="s2">&quot;&quot;</span><span class="err">&quot;</span>
</code></pre></div>

<p>Codex generation temperature is set to 0.3 and the maximum length to 500 tokens, with "print(ans)" used as the stopping criterion.</p>
<p>Code Editing We use the Codex (Chen et al., 2021) model for code generation and code editing. Arguments for both include the previous</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Simple Text <br> Editing</th>
<th style="text-align: center;">CS <br> Algorithms</th>
<th style="text-align: center;">Strategy QA</th>
<th style="text-align: center;">Physics <br> Questions</th>
<th style="text-align: center;">Unit <br> Interpretation</th>
<th style="text-align: center;">Reasoning about <br> colored objects</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Best task cluster</td>
<td style="text-align: center;">27.65</td>
<td style="text-align: center;">88.11</td>
<td style="text-align: center;">66.44</td>
<td style="text-align: center;">20.37</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">64.34</td>
</tr>
<tr>
<td style="text-align: left;">LLM-based task sim.</td>
<td style="text-align: center;">38.30</td>
<td style="text-align: center;">83.71</td>
<td style="text-align: center;">60.39</td>
<td style="text-align: center;">14.06</td>
<td style="text-align: center;">43.56</td>
<td style="text-align: center;">62.00</td>
</tr>
</tbody>
</table>
<p>Table 12: Comparing ART results on GPT3 (175B) model with two similar task selection strategies. LLM-based similarity is worse on average compared to just choosing the best task cluster.</p>
<h1>Prompt to LLM for selecting similar tasks</h1>
<p>Give two tasks with their descriptions and examples of inputs and outputs for the tasks, determine if they are similar. Two tasks are similar if require common subtasks like string operations, web search, translation, arithmetic, code execution, etc.</p>
<p>Task1: [Date understanding] Find the required date in MM/DD/YYYY using information about related events and dates in the input. Input: The deadline is Jun 1, 2021, which is 2 days away from now. What is the date 24 hours later in MM/DD/YYYY? The final answer is 05/01/2021.
Task2: [Language Games] Translate English into Pig Latin. Input: English sentence is "Sami made his way across the bar and hugged Layla". The final answer is "Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay." Are these similar? Yes. They both require answering in a spcific string format.</p>
<p>Task1: [K'th letter concatenation] Take the letters at position 3 of the words in a list of words and concatenate them using a space. Input: What are the words in "Savita Saeed Ramos Sato Yadav"? The final answer is "v e m t d".
Task2: [Language Games] Translate English into Pig Latin. Input: English sentence is "Sami made his way across the bar and hugged Layla". The final answer is "Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay." Are these similar? Yes. They both require accessing and manipulating characters in strings.</p>
<p>Task1: [K'th letter concatenation] Take the letters at position 3 of the words in a list of words and concatenate them using a space. Input: What are the words in "Savita Saeed Ramos Sato Yadav"? The final answer is "v e m t d".
Task2: [Known Unknown] Choose the option that best answers the question. If the question does not have a known answer, choose "Unknown". Input: How many hairs were on Neil Armstrong's head when he landed on the moon? The final answer is "Unknown".
Are these similar? No. Task 1 requires manipulating strings and Task 2 requires answering a question by possibly looking up information on the web.</p>
<p>Task1: [Anachronisms] An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time. Does the sentence contain an anachrornism? Input: Kurt Cobain starred in the 1980 television show "Twin Peaks". The final answer is "Yes".
Task2: [Known Unknown] Choose the option that best answers the question. If the question does not have a known answer, choose "Unknown". Input: Where was Mark Twain born? The final answer is Florida, Missouri.
Are these similar? Yes. They both require searching information about entities mentioned in the text, like Kurt Cobain or Mark Twain.</p>
<p>Task1: [Hindu Knowledge] Answer questions about Hindu mythology by choosing the option that best answers the question. Input: In the Mahabharata, Karna is cursed to forget the incantations needed to use which weapon? Choices: Anjalikastra, Narayanastra, Agneyastra, Brahmastra. The final answer is Brahmastra.
Task2: [Code Debugging] Debug the following code snippets by finding the answer or the error message. Input: if $\mathrm{x}&lt;5$ : pass
The final answer is
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
NameError: name 'x' is not defined
Are these similar? No. Task 1 is about asnswering a question and requires searching information about entities mentioned in the text. Task 2 is a question about debugging code and may require a Python interpreter.
Task 1: $\%$ s
Task 2: $\%$ s
Are these similar?</p>
<p>Table 13: Programs in the task library.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: PeG Grammar used to parse ART programs
sub-task's answer sequence " $# i-1: \ldots$ " (or the input if $i=1$ ), and the sequence generated by the LM after the sub-task query symbol "Qi : [generate python code]". The first argument is the code snippet that needs to be edited and the second argument is a multi-line comment in Python used as the instruction for editing/generation. To ensure that subsequent code execution results in the generation of an answer string independent of variable name, the edit instruction is to print the required variable. For example, for the auto debugging task in the task library, the following program snippet:</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Input</span><span class="p">:</span>
<span class="sc">&#39; &#39;</span><span class="p">.</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">])</span>
<span class="sc">&#39; &#39;</span><span class="p">.</span>
<span class="n">What</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">this</span>
<span class="n">program</span><span class="w"> </span><span class="n">executes</span><span class="o">?</span>
<span class="nl">Q1</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">code</span><span class="w"> </span><span class="n">edit</span><span class="p">]</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">print</span>
<span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">x</span>
</code></pre></div>

<p>is used to prompt Codex in edit mode as follows. For code input:
$x=\operatorname{set}([1,1,2,3])$
For edit instruction:
Edit the code to print the value of $x$
Code Execution We run python code in a virtual python environment with arithmetic, symbolic, and scientific computing packages pre-installed. The arguments to code execute include the previous subtask's answer sequence " $# i-1: \ldots$ ", which is the python code snippet that requires executing. If $i=$ 1 , the input contains the code. The other argument is the sequence generated by the LM after the subtask query symbol " $Q i:[$ execute code]" (which is
from sympy.solvers import solve
from sympy import Symbol, Eq, simplify
import math
import numpy as np
import cvxpy as cp
import statistics
def solve_it(equation, variable):
solution=solve(equation, variable, dict=True)
if not solution:
if isinstance(variable, list):
solution=v: None for v in variable
else:
solution=variable: None
return solution
else:
solution = solution[0]
return solution
Table 14: Code prefix appended before a code snippet prior to execution.
simply to the code snippet as a comment). Again, to encourage executable code with consistent variable usage, we also append the sequence "Store your final answer is variable 'ans'" to the comment. The results of the execution call are used to replace the answer sequence generated by the language model. Finally, we prepend a code snippet consisting of useful module and function imports so that function calls external modules like numpy and scipy are executed successfully. This code prefix is shown in Table 14. We use the exec native python function to execute the code snippet and access the 'ans' local variable if it exists.</p>
<p>Knowledge Base lookup This tool is added in the Word Unscrambling task. This function call is used to look up data by keys in a relational knowledge base. For example, we use dictionary lookup for the Word Unscrambling task. The input to this function is again the previous sub-task's answer sequence (if it exists, or the original input is used) and</p>
<p>the sequence generated by the LM after the function name symbol. The first argument is parsed as a python code snippet and interpreted as a list of lookup keys. The second argument is parsed as a code generation prompt which is consequently executed. For example, the first argument of $l=[$ 'yob', 'boy', 'oyb'] and the second argument Check which of these list of words is a word in English. Store the final answer is 'ans' and print it. results in the following code snippet and final answer 'boy':</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">lookup</span><span class="p">(</span><span class="n">word_list</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">enchant</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">enchant</span><span class="o">.</span><span class="n">Dict</span><span class="p">(</span><span class="s2">&quot;en_US&quot;</span><span class="p">)</span>
    <span class="n">valid_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
            <span class="n">valid_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">valid_list</span>
</code></pre></div>

<p>While this is a restricted definition for a general knowledge base lookup or query, we explore how human-in-the-loop feedback can be used to create custom lookup tools.</p>
<p>Prolog Engine This tool is added in the formal fallacies task. This task consits of first-order logic statements stated in natural language, as follows:</p>
<p>To begin with, Bonnie is a schoolmate of Miranda. Moreover, whoever is a
workmate of Aubrey is not a schoolmate of Miranda. All this entails that Bonnie is not a workmate of Aubrey.
Is the argument, given the explicitly stated premises, deductively valid or invalid?</p>
<p>This can be written in Prolog ${ }^{5}$ as:
workmate(X, aubrey) :- + schoolmate(X, miranda).
schoolmate(bonnie, miranda).
?- workmate(bonnie, aubrey).
Humans provide feedback by authoring such prolog statements for a few instances with a new tool symbol "[translate to prolog]". They then author a new tool that calls a python prolog parsing engine to execute the prolog code and determine the binary value of the final expression. This is integrated back into the program.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>A. 4 Baselines</h2>
<p>Few-shot baseline This is the direct prompting baseline where the prompt consists of input-output pairs only and no additional intermediate reasoning steps. Following prior work that reports results with direct prompting (Suzgun et al., 2022; Wei et al., 2022), we use 3 randomly chosen input-output instances. We run direct prompting for both, InstructGPT (text-davinci-002) (Ouyang et al., 2022) and Codex (code-davinci-002) (Chen et al., 2021) and report the higher performance. This follows from (Chung et al., 2022), where they find that Codex models are better at analytical tasks than text models, even with direct prompting.</p>
<p>Auto CoT A baseline that generates automatic CoT-style multi-step reasoning in a free-form natural language (as done in AutoCoT (Zhang et al., 2022)). A randomly selected subset of examples in the dataset is used to prompt the LLM to elicit CoTstyle reasoning (Input + Let's think step-by-step.). Since CoT-style generation is free-form and parsing potential tool use symbols is harder, we don't use tools for this baseline. This baseline specifically measures the effectiveness of a custom query language (and PeG grammar) we use to write programs and parse tool calls; While (Zhang et al., 2022) cluster training examples to provide diverse demonstrations to the LLM, we choose a random selection of 5 examples. A careful selection of demonstration examples may also be used for ART, and we leave an exploration of this choice to future work. We parse the generated CoT-style reasoning to extract the answer string and add the phrase "The final answer i" along with the answer string to the end of the reasoning. This pattern is used for evaluation.</p>
<p>Best GPT-3 Approaches We briefly describe the GPT-3 best results reported in Tables 2 and Tables 3, which correspond to the best GPT-3 results reported in approaches that use multi-step reasoning (like CoT) and tool use, with human supervision for both.</p>
<ul>
<li>(Suzgun et al., 2022): Human-authored CoT reasoning for several tasks in BigBench. A closer inspection of their hand-crafted prompts revealed that they cast BigBench tasks to multiple-choice tasks (selecting between options A,B,C,...), which differs from the more challenging format proposed originally and used in this work. Hence, we modify</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://en.wikipedia.org/wiki/Prolog&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ We compare both strategies in Appendix A. 2&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6691 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6691</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6691</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-e325fe41c8c1d547ccd102ac82be3ec8b23960f2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e325fe41c8c1d547ccd102ac82be3ec8b23960f2" target="_blank">ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, which automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests.</p>
                <p><strong>Paper Abstract:</strong> Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6691.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6691.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large OpenAI language model used in the paper to generate Parsel programs zero-shot and to implement/evaluate solutions for HumanEval; combining GPT-4 with the Parsel pipeline substantially improved code-generation pass rates on HumanEval relative to GPT-4 alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>program synthesis (includes numeric/algorithmic tasks such as numeric conversions and short coding problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language function docstring / problem description translated to Parsel then synthesized to code</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>small to moderate programming tasks (HumanEval-level)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Parsel zero-shot translation (describe Parsel format and indentation/description rules); automatic unit-test generation optionally used; selected implementations by CodeT score; selection heuristics: sample Parsel programs until implementations pass generated tests</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1 (and pass@any) / pass rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pass@1 improved from reported baseline 67% to 85% when using Parsel with generated tests (85.1% reported); pass@any up to 91.5% with up to 8 Parsel programs (8 implementations each) vs. GPT-4 alone 82.3% with 64 direct programs; 93.9% vs. 84.1% when comparing Parsel (16x8) to 128 direct programs</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic interpretability analysis of how GPT-4 processes numeric tokens or arithmetic was reported; analysis focused on end-to-end empirical improvements from the Parsel decomposition and sampling/reranking heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper reports some tasks GPT-4 with Parsel still failed (8 tasks never solved); specific reported failure mode for numeric conversion (decimal_to_binary) where Parsel solutions relied on stateful updates that could not mutate integers â€” a statefulness / mutation semantics mismatch caused failure.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Using Parsel with GPT-4 showed improved pass rates with more Parsel programs and more implementations; authors report probability of passing increases approximately quadratically with the log of number of Parsel programs sampled and number of implementations sampled (R^2 >= 0.99 on explored APPS subset), and Parsel outperforms direct sampling given comparable effective sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6691.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6691.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (OpenAI code language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-specialized language model used in experiments to (a) translate model-produced high-level plans to Parsel and (b) generate function implementations; used as the backbone code LLM in many Parsel evaluations (APPS experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>APPS (competition-level subset) and HumanEval (comparative context)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>algorithmic program synthesis (competition-level problems), general code generation (includes numeric/algorithmic subproblems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problem statement â†’ (few-shot) ask model to propose a step-by-step algorithm â†’ translate algorithm to Parsel â†’ synthesize implementations per function â†’ test using constraints/examples</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>competition-level coding (hard), and HumanEval (easier) in separate experiments</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompts: zero-shot plan generation in some cases, then Codex prompted to translate plans to Parsel (three examples provided for Parsel translation); generation of multiple implementations per function (k implementations) and combinatorial composition</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass rate (pass@n and effective sample-adjusted pass@nÃ—k)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Parsel pipeline using Codex achieved substantially higher pass rates on APPS competition problems than direct Codex sampling; reported example: improvement from 14.5% (baseline prior method) to 25.5% (Parsel with Codex) on competition-level APPS subset (figure cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes into Codex numeric or arithmetic processing were reported; analysis focused on decomposition, combinatorial composition of implementations, and effect of constraints/tests.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Codex and similar code LLMs notably degrade as the number of sequential tokens/steps grows (cited prior observation); failures arise when models must compose many building blocks without modular constraints. Parsel mitigates but does not eliminate failures when functions lack constraints or when SCCs are large (exponential combination growth).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Parsel with Codex benefits from combinatorial composition: with n Parsel programs and k implementations per function, effective candidate programs grow exponentially while generation cost is linear; pass rate improves with more evaluations and number of implementations, with diminishing practical returns limited by rate limits and evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6691.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6691.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaCode</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaCode</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeepMind's competitive-code-oriented language model referenced as a prior baseline; Parsel's results are compared to AlphaCode's performance on APPS competition problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaCode</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer / large code model (reported by AlphaCode authors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>APPS (competition-level subset) comparative baseline</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level program synthesis (algorithmic/algorithm design tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problem statement â†’ direct code generation baseline (compared to Parsel decomposition + synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>competition-level (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct code generation baseline (as reported in AlphaCode comparisons); Parsel compared against AlphaCode and Codex baselines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass rate on APPS competition problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports Parsel outperforms prior results including AlphaCode on APPS competition subset; exact AlphaCode numeric pass rates are not given in the paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal analysis of AlphaCode's arithmetic processing is presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not specifically enumerated for AlphaCode in this paper; general failure modes for code LLMs (difficulty composing many steps, sensitivity to token-level errors) are discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not specifically analyzed here for AlphaCode; Parsel's compositional approach aims to improve effective scaling by reusing modular function implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6691.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6691.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002 (GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3 style model (text-davinci-002) used zero-shot to produce high-level step-by-step plans for APPS problems which were then translated to Parsel; used as the planner in some experiments before Codex translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>APPS (competition-level subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>algorithm design / producing step-by-step plans for program synthesis (includes numeric/algorithmic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problem statement â†’ prompt to 'think step by step' to produce a high-level algorithmic plan â†’ plan translated to Parsel</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>competition-level (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot prompt asking model to propose a high-level solution and explain why it is correct; instruction to 'Think step by step to come up with a clever algorithm' (Kojima-style chain-of-thought cue)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used as planner input to Parsel synthesizer; end-to-end performance measured via final pass rate on APPS when combined with Codex-based implementation and Parsel synthesizer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported as a standalone pass-rate; used as plan generator upstream of Parsel/Codex pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal analysis of numerical/arithmetic processing in text-davinci-002 was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Primary failure modes relate to plan quality affecting downstream synthesis; improving Parsel generation is identified as a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in the paper for arithmetic behavior specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6691.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6691.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen 2.7B (evaluated open model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source code model (2.7B parameters) evaluated by the authors in the same Parsel configuration used for Codex; it performed poorly on the sampled APPS problems in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeGen 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>APPS (competition-level subset) (small random sample)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level program synthesis (algorithmic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problem statement â†’ Parsel generation/implementation pipeline (same configuration as used for Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>competition-level (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Same Parsel pipeline as Codex experiments (APPS configuration, 8x16 sampling configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>number of problems solved / pass rate on sampled APPS problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On a random sample of 25 problems in the same configuration used for Codex, the CodeGen 2.7B model solved 0 problems (no problems solved).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic analysis of numeric processing; authors note open-source models underperform relative to Codex/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Underperforms on complex algorithmic/program-synthesis tasks; authors attribute poor performance to model capacity/data differences compared to closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Authors observe smaller open-source code models underperform on these tasks; no detailed scaling analysis presented beyond the empirical 0/25 result at 2.7B.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6691.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6691.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calculator / external-tool access (Cobbe et al. mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Access to an external calculator (tool use for math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper mentions prior work showing that providing language models access to a calculator (external tool) can improve solving of more complex math word problems; presented as a promising direction to enhance Parsel's ability to handle explicit arithmetic-heavy steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>math word problems (general reference; specific benchmark not given in-text)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math word problems (arithmetic-heavy multi-step calculations)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems; solved via model augmented with an external calculator/tool</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>complex math word problems (harder than purely symbolic examples)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>tool augmentation / external calculator access (model executes calculations via calculator rather than relying solely on internal token-based numeracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improved problem-solving success on math word problems (qualitative claim in paper referencing prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probing reported in this paper about how the model integrates calculator outputs; referenced prior work as evidence that tool access helps arithmetic-heavy reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Implied failure mode addressed: model internal numeric computation/precision and multi-step arithmetic reasoning are weak without external tools; giving calculator access mitigates arithmetic errors</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper; idea presented as a potential future integration to improve arithmetic-heavy intermediate steps in Parsel-generated proofs or programs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Competition-level code generation with alphacode <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Minif2f: a cross-system benchmark for formal olympiad-level mathematics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6691",
    "paper_id": "paper-e325fe41c8c1d547ccd102ac82be3ec8b23960f2",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large OpenAI language model used in the paper to generate Parsel programs zero-shot and to implement/evaluate solutions for HumanEval; combining GPT-4 with the Parsel pipeline substantially improved code-generation pass rates on HumanEval relative to GPT-4 alone.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_family": "decoder-only transformer",
            "model_size": "",
            "training_data_description": "",
            "benchmark_name": "HumanEval",
            "task_type": "program synthesis (includes numeric/algorithmic tasks such as numeric conversions and short coding problems)",
            "problem_format": "natural-language function docstring / problem description translated to Parsel then synthesized to code",
            "difficulty_level": "small to moderate programming tasks (HumanEval-level)",
            "prompting_method": "Parsel zero-shot translation (describe Parsel format and indentation/description rules); automatic unit-test generation optionally used; selected implementations by CodeT score; selection heuristics: sample Parsel programs until implementations pass generated tests",
            "performance_metric": "pass@1 (and pass@any) / pass rate",
            "performance_value": "Pass@1 improved from reported baseline 67% to 85% when using Parsel with generated tests (85.1% reported); pass@any up to 91.5% with up to 8 Parsel programs (8 implementations each) vs. GPT-4 alone 82.3% with 64 direct programs; 93.9% vs. 84.1% when comparing Parsel (16x8) to 128 direct programs",
            "internal_analysis": "No internal mechanistic interpretability analysis of how GPT-4 processes numeric tokens or arithmetic was reported; analysis focused on end-to-end empirical improvements from the Parsel decomposition and sampling/reranking heuristics.",
            "failure_modes": "Paper reports some tasks GPT-4 with Parsel still failed (8 tasks never solved); specific reported failure mode for numeric conversion (decimal_to_binary) where Parsel solutions relied on stateful updates that could not mutate integers â€” a statefulness / mutation semantics mismatch caused failure.",
            "scaling_trend": "Using Parsel with GPT-4 showed improved pass rates with more Parsel programs and more implementations; authors report probability of passing increases approximately quadratically with the log of number of Parsel programs sampled and number of implementations sampled (R^2 &gt;= 0.99 on explored APPS subset), and Parsel outperforms direct sampling given comparable effective sample budgets.",
            "uuid": "e6691.0",
            "source_info": {
                "paper_title": "ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Codex",
            "name_full": "Codex (OpenAI code language model)",
            "brief_description": "A code-specialized language model used in experiments to (a) translate model-produced high-level plans to Parsel and (b) generate function implementations; used as the backbone code LLM in many Parsel evaluations (APPS experiments).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex",
            "model_family": "decoder-only transformer",
            "model_size": "",
            "training_data_description": "",
            "benchmark_name": "APPS (competition-level subset) and HumanEval (comparative context)",
            "task_type": "algorithmic program synthesis (competition-level problems), general code generation (includes numeric/algorithmic subproblems)",
            "problem_format": "natural-language problem statement â†’ (few-shot) ask model to propose a step-by-step algorithm â†’ translate algorithm to Parsel â†’ synthesize implementations per function â†’ test using constraints/examples",
            "difficulty_level": "competition-level coding (hard), and HumanEval (easier) in separate experiments",
            "prompting_method": "Few-shot prompts: zero-shot plan generation in some cases, then Codex prompted to translate plans to Parsel (three examples provided for Parsel translation); generation of multiple implementations per function (k implementations) and combinatorial composition",
            "performance_metric": "pass rate (pass@n and effective sample-adjusted pass@nÃ—k)",
            "performance_value": "Parsel pipeline using Codex achieved substantially higher pass rates on APPS competition problems than direct Codex sampling; reported example: improvement from 14.5% (baseline prior method) to 25.5% (Parsel with Codex) on competition-level APPS subset (figure cited in paper).",
            "internal_analysis": "No mechanistic probes into Codex numeric or arithmetic processing were reported; analysis focused on decomposition, combinatorial composition of implementations, and effect of constraints/tests.",
            "failure_modes": "Codex and similar code LLMs notably degrade as the number of sequential tokens/steps grows (cited prior observation); failures arise when models must compose many building blocks without modular constraints. Parsel mitigates but does not eliminate failures when functions lack constraints or when SCCs are large (exponential combination growth).",
            "scaling_trend": "Parsel with Codex benefits from combinatorial composition: with n Parsel programs and k implementations per function, effective candidate programs grow exponentially while generation cost is linear; pass rate improves with more evaluations and number of implementations, with diminishing practical returns limited by rate limits and evaluation cost.",
            "uuid": "e6691.1",
            "source_info": {
                "paper_title": "ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "AlphaCode",
            "name_full": "AlphaCode",
            "brief_description": "DeepMind's competitive-code-oriented language model referenced as a prior baseline; Parsel's results are compared to AlphaCode's performance on APPS competition problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "AlphaCode",
            "model_family": "decoder-only transformer / large code model (reported by AlphaCode authors)",
            "model_size": "",
            "training_data_description": "",
            "benchmark_name": "APPS (competition-level subset) comparative baseline",
            "task_type": "competition-level program synthesis (algorithmic/algorithm design tasks)",
            "problem_format": "natural-language problem statement â†’ direct code generation baseline (compared to Parsel decomposition + synthesis)",
            "difficulty_level": "competition-level (hard)",
            "prompting_method": "Direct code generation baseline (as reported in AlphaCode comparisons); Parsel compared against AlphaCode and Codex baselines",
            "performance_metric": "pass rate on APPS competition problems",
            "performance_value": "Paper reports Parsel outperforms prior results including AlphaCode on APPS competition subset; exact AlphaCode numeric pass rates are not given in the paper's text.",
            "internal_analysis": "No internal analysis of AlphaCode's arithmetic processing is presented in this paper.",
            "failure_modes": "Not specifically enumerated for AlphaCode in this paper; general failure modes for code LLMs (difficulty composing many steps, sensitivity to token-level errors) are discussed.",
            "scaling_trend": "Not specifically analyzed here for AlphaCode; Parsel's compositional approach aims to improve effective scaling by reusing modular function implementations.",
            "uuid": "e6691.2",
            "source_info": {
                "paper_title": "ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002 (GPT-3 family)",
            "brief_description": "A GPT-3 style model (text-davinci-002) used zero-shot to produce high-level step-by-step plans for APPS problems which were then translated to Parsel; used as the planner in some experiments before Codex translation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_family": "decoder-only transformer",
            "model_size": "",
            "training_data_description": "",
            "benchmark_name": "APPS (competition-level subset)",
            "task_type": "algorithm design / producing step-by-step plans for program synthesis (includes numeric/algorithmic reasoning)",
            "problem_format": "natural-language problem statement â†’ prompt to 'think step by step' to produce a high-level algorithmic plan â†’ plan translated to Parsel",
            "difficulty_level": "competition-level (hard)",
            "prompting_method": "Zero-shot prompt asking model to propose a high-level solution and explain why it is correct; instruction to 'Think step by step to come up with a clever algorithm' (Kojima-style chain-of-thought cue)",
            "performance_metric": "Used as planner input to Parsel synthesizer; end-to-end performance measured via final pass rate on APPS when combined with Codex-based implementation and Parsel synthesizer",
            "performance_value": "Not reported as a standalone pass-rate; used as plan generator upstream of Parsel/Codex pipeline.",
            "internal_analysis": "No internal analysis of numerical/arithmetic processing in text-davinci-002 was reported.",
            "failure_modes": "Primary failure modes relate to plan quality affecting downstream synthesis; improving Parsel generation is identified as a bottleneck.",
            "scaling_trend": "Not analyzed in the paper for arithmetic behavior specifically.",
            "uuid": "e6691.3",
            "source_info": {
                "paper_title": "ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CodeGen-2.7B",
            "name_full": "CodeGen 2.7B (evaluated open model)",
            "brief_description": "An open-source code model (2.7B parameters) evaluated by the authors in the same Parsel configuration used for Codex; it performed poorly on the sampled APPS problems in their experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeGen 2.7B",
            "model_family": "decoder-only transformer",
            "model_size": "2.7B",
            "training_data_description": "",
            "benchmark_name": "APPS (competition-level subset) (small random sample)",
            "task_type": "competition-level program synthesis (algorithmic tasks)",
            "problem_format": "natural-language problem statement â†’ Parsel generation/implementation pipeline (same configuration as used for Codex)",
            "difficulty_level": "competition-level (hard)",
            "prompting_method": "Same Parsel pipeline as Codex experiments (APPS configuration, 8x16 sampling configuration)",
            "performance_metric": "number of problems solved / pass rate on sampled APPS problems",
            "performance_value": "On a random sample of 25 problems in the same configuration used for Codex, the CodeGen 2.7B model solved 0 problems (no problems solved).",
            "internal_analysis": "No mechanistic analysis of numeric processing; authors note open-source models underperform relative to Codex/GPT-4.",
            "failure_modes": "Underperforms on complex algorithmic/program-synthesis tasks; authors attribute poor performance to model capacity/data differences compared to closed models.",
            "scaling_trend": "Authors observe smaller open-source code models underperform on these tasks; no detailed scaling analysis presented beyond the empirical 0/25 result at 2.7B.",
            "uuid": "e6691.4",
            "source_info": {
                "paper_title": "ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Calculator / external-tool access (Cobbe et al. mention)",
            "name_full": "Access to an external calculator (tool use for math word problems)",
            "brief_description": "Paper mentions prior work showing that providing language models access to a calculator (external tool) can improve solving of more complex math word problems; presented as a promising direction to enhance Parsel's ability to handle explicit arithmetic-heavy steps.",
            "citation_title": "Training verifiers to solve math word problems.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_family": "",
            "model_size": "",
            "training_data_description": "",
            "benchmark_name": "math word problems (general reference; specific benchmark not given in-text)",
            "task_type": "math word problems (arithmetic-heavy multi-step calculations)",
            "problem_format": "natural-language math word problems; solved via model augmented with an external calculator/tool",
            "difficulty_level": "complex math word problems (harder than purely symbolic examples)",
            "prompting_method": "tool augmentation / external calculator access (model executes calculations via calculator rather than relying solely on internal token-based numeracy)",
            "performance_metric": "improved problem-solving success on math word problems (qualitative claim in paper referencing prior work)",
            "performance_value": "",
            "internal_analysis": "No mechanistic probing reported in this paper about how the model integrates calculator outputs; referenced prior work as evidence that tool access helps arithmetic-heavy reasoning.",
            "failure_modes": "Implied failure mode addressed: model internal numeric computation/precision and multi-step arithmetic reasoning are weak without external tools; giving calculator access mitigates arithmetic errors",
            "scaling_trend": "Not analyzed in this paper; idea presented as a potential future integration to improve arithmetic-heavy intermediate steps in Parsel-generated proofs or programs.",
            "uuid": "e6691.5",
            "source_info": {
                "paper_title": "ParselðŸ¦†: Algorithmic Reasoning with Language Models by Composing Decompositions",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2
        },
        {
            "paper_title": "Competition-level code generation with alphacode",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "Minif2f: a cross-system benchmark for formal olympiad-level mathematics",
            "rating": 1
        }
    ],
    "cost": 0.015541,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Parsel : Algorithmic Reasoning with Language Models by Composing Decompositions</h1>
<p>Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, Nick Haber<br>Stanford University<br>{ezelikman, qhwang, poesia, ngoodman, nhaber}@stanford.edu</p>
<h4>Abstract</h4>
<p>Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over $75 \%$ higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from $67 \%$ to $85 \%$. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel.</p>
<h2>1 Introduction</h2>
<p>To a language model for code (as for a human), each new token is a new chance to break the program. Chen et al. [12] highlighted this issue in a simple synthetic experiment: when asked to generate a program with a series of simple string transformations, the performance of their code language model, Codex, drops dramatically with the number of steps. As they pointed out, a human who can implement a few building blocks should be able to compose these blocks with arbitrary length.
Unlike other token generators, human programmers have (mostly) learned to break down complex tasks into manageable parts that work alone (i.e., are modular) and work together (i.e., are compositional). And, when human-generated tokens cause functions to break, the functions can ideally be rewritten independently of the rest of the program. In contrast, naively, we expect code language models to generate token sequences that are correct in their entirety. Motivated by this, we study how to leverage language models to decompose problems and assemble their compositional solutions.
We introduce Parsel, a framework that enables the decomposition, implementation, then composition of complex programs from natural language. The process consists of three main steps: 1) A language model generates a Parsel program by decomposing a natural language task into natural language function descriptions. 2) For each function description, a language model generates modular implementations. 3) The Parsel synthesizer then builds up programs by testing minimal, combinatorial groups of implementations against sets of constraints (e.g. input-output examples).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Parsel overview. A human or LLM writes a task decomposition (in Parsel), which is split into its strongly-connected components (SCC), and then the Parsel synthesizer uses a code LLM and a constraint solver to implement and compose each SCC. When all functions have constraints and there is no recursion, each function is its own SCC. We provide a more detailed figure in Appendix G.</p>
<p>Thus, generating and implementing Parsel mirrors a pattern in human reasoning â€“ decomposing an abstract plan until it can be solved automatically [55] â€“ and this compositional structure also benefits language models. We show that LLMs can generate Parsel with only few-shot prompting. Using their solutions as input to the Parsel synthesizer, we outperform prior work on competition-level problems in the APPS dataset [27], including AlphaCode [35] and both versions of Codex [12, 11]. We also show that with GPT-4 [44], not only can we generate Parsel <em>zero-shot</em> just by describing its format, but this allows it to substantially outperform GPT-4 alone on HumanEval [12], with and without generated tests. By automatically generating and implementing Parsel, LLMs also propose robotic plans that are more accurate than in prior work on language models as robotic planners [28]. Broadly, we formulate Parsel as a general-purpose algorithmic reasoning framework.</p>
<h2>2 Methods</h2>
<p>In this section, we provide an overview of the Parsel framework. First, we show how programs can be specified in Parsel. Afterward, we explain how language models can generate Parsel from task descriptions. Finally, we present the Parsel synthesizer that allows us to actually implement Parsel programs into runnable code. Specifically, we further discuss how we handle various challenges in solving the constraints, from recursion to test underspecification. There are several steps to synthesizing a Parsel program, which we explicate in this section. We visualize the details in Fig. 1 and provide a high-level pseudocode in Fig. A.11.</p>
<p>With Parsel, we balance two challenges. On one hand, function implementations must be modular to fully leverage combinatoriality. By requiring function signatures and descriptions to be specified (or generated), we ensure functions can call others without specific implementation information. On the other hand, it must be possible to avoid combinatorial explosions. By factoring the dependency graph into strongly-connected components (SCCs), we ensure the complexity grows exponentially only with the size of the largest strongly-connected component but not the total number of functions.</p>
<h3>2.1 Specifying Parsel Programs</h3>
<p>To formally specify a high-level algorithmic design, we develop a simple intermediate language, also called Parsel. It is designed to be accessible to programmers, code LLMs, and students, as discussed further in Appendix A, and inspired by many works (Section 4). In Parsel, each line contains a function description, a constraint, or a reference to a description. They all obey scoping rules, with some nuances per target language. In short: descriptions specify what a function does in natural language, optionally preceded by a function name and its arguments (if not included, they are inferred); references indicate to the language model that a function should call another function described elsewhere; constraints validate that a function is implemented correctly (if not provided, they may be generated automatically using the language model). We elaborate on syntactic details in Appendix Q. We provide some examples of Parsel programs in Figures 2 and 3. For example, in Figure 2, task_plan(): return a list of strings ... represents a description, while the call to collatz_recursion by recursion_rule is a reference.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure i: Parsel to VirtualHome (robotic planning)</p>
<div class="codehilite"><pre><span></span><code><span class="n">collatz_recursion</span><span class="p">(</span><span class="n">num</span><span class="p">,</span><span class="w"> </span><span class="n">cur_list</span><span class="o">=</span><span class="n">list</span><span class="p">())</span><span class="o">:</span><span class="w"> </span><span class="n">Calls</span>
<span class="w">    </span><span class="n">base_case</span><span class="w"> </span><span class="nf">if</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">otherwise</span><span class="w"> </span><span class="n">recursion_rule</span>
<span class="mi">19</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">19</span><span class="p">,</span><span class="w"> </span><span class="mi">58</span><span class="p">,</span><span class="w"> </span><span class="mi">29</span><span class="p">,</span><span class="w"> </span><span class="mi">88</span><span class="p">,</span><span class="w"> </span><span class="mi">44</span><span class="p">,</span><span class="w"> </span><span class="mi">22</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="mi">34</span><span class="p">,</span><span class="w"> </span><span class="mi">17</span><span class="p">,</span><span class="w"> </span><span class="mi">52</span><span class="p">,</span>
<span class="w">    </span><span class="mi">26</span><span class="p">,</span><span class="w"> </span><span class="mi">18</span><span class="p">,</span><span class="w"> </span><span class="mi">40</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="n">base_case</span><span class="p">(</span><span class="n">num</span><span class="p">,</span><span class="w"> </span><span class="n">cur_list</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">Returns</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">list</span>
<span class="w">        </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">appended</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">it</span>
<span class="w">    </span><span class="n">recursion_rule</span><span class="p">(</span><span class="n">num</span><span class="p">,</span><span class="w"> </span><span class="n">cur_list</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">Add</span><span class="w"> </span><span class="n">num</span><span class="w"> </span><span class="n">to</span>
<span class="w">        </span><span class="n">list</span><span class="p">,</span><span class="w"> </span><span class="n">collatz</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="mi">3</span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nf">if</span><span class="w"> </span><span class="n">odd</span><span class="w"> </span><span class="kr">or</span><span class="w"> </span><span class="n">n</span>
<span class="w">        </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nf">if</span><span class="w"> </span><span class="n">even</span>
<span class="w">        </span><span class="n">collatz_recursion</span>
</code></pre></div>

<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure ii: Parsel to Python
Figure 2: Examples using Parsel to compile programs for VirtualHome [50] procedures and Python. Note the columns on the generated examples are only there to allow them to fit compactly - each program implementation is one contiguous solution. Colors for Parsel code on the left are used to indicate constraints and references. All other Parsel lines shown are definitions.</p>
<h1>2.2 Generating Parsel Programs</h1>
<p>We use a language model to generate Parsel programs from arbitrary natural language task descriptions as a few-shot translation task (or zero-shot with GPT4 [44]). Explicitly writing out a preliminary plan can be helpful, so we first ask the model to (zero-shot) generate a plan in natural language by thinking step-by-step (motivated by Kojima et al. [32]) and then prompt the model to translate the plan to Parsel. These steps correspond to the first and second arrows in Figure 3, respectively. In some domains, in particular for robotic planning, we find that this intermediate step is not necessary (while increasing cost) and can ask the language model to directly translate the task description to a Parsel solution.</p>
<h3>2.3 Implementing Parsel Programs</h3>
<p>Given the Parsel program, a core technical challenge is how to implement it in a modular way (to fully leverage the advantage of decomposition). Here we propose the Parsel synthesizer: At a high level, to implement Parsel programs with the Parsel synthesizer, we first use a language model to generate a set of candidate implementations of each of the functions based on their descriptions and then search over minimal sets of combinations of the functions to find ones that satisfy the provided constraints. The step described in this section corresponds to the rightmost arrow in Figure 3.</p>
<h3>2.3.1 Implementing Functions</h3>
<p>A function is implemented by first aggregating descriptions and function signatures of its (potentially zero) children to form an LLM prompt (for Python, we generate a prompt as if the child functions are imported and use their descriptions as comments). Crucially, this facilitates easily changing child implementations. A code LLM is then queried using the description's text as a docstring and the description's function name and arguments for the signature; full prompts shown in Appendix J.</p>
<h3>2.3.2 Searching Over Function Combinations</h3>
<p>Sequential case. The most straightforward case is when all functions have constraints (e.g., unit tests), and no functions have recursive dependencies (e.g., Fig. A.9). We start by considering this case. This defines a clear topological order on functions so they can be implemented sequentially. In this situation, Parsel implements functions with post-order traversal from function at the root, generating implementations and finding one passing the specified constraints for each function. In other words, without any cycles in the call graph, we can start by implementing the leaf functions first, then their parents, etc., until the program is implemented. However, in practice, many programs have more complex structures and constraints.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Parsel pipeline. Overview of the full Parsel pipeline for an APPS example. Each step is performed by a language model. We include the complete task, sketch, and program in Appendix R.</p>
<p>Recursion. Permitting mutual recursion (e.g. a function $f$ that depends on $g$ while $g$ also calls $f$ ) introduces the need for joint implementation of functions. However, naively considering all possible implementations is intractable for large programs. Specifically, the number of possible implementations of a program with $k$ functions and $n$ implementations per function is $O\left(n^{k}\right)$, exponential in the number of functions. We propose a more efficient solution, inspired by Cocke and Kennedy [16]. We reference the function call graph, identify its strongly-connected components (SCCs), and for each SCC consider all possible sets of function implementations until one set satisfies all constraints. For example, for possible implementations of functions $f, g, h$ forming an SCC of a call graph, with $I(f)$ corresponding to the language-model-generated implementations of $f$, we evaluate uniformly random samples without replacement from $I(f) \times I(g) \times I(h)$.</p>
<p>Functions without constraints. For functions with no constraints, we can conveniently use the same approach as above by reformulating the call graph as a "test dependency graph" and adding an edge from a test-less child to its parent. That is, if a function has no constraints, it depends on its parents to enforce constraints on its implementations. This could also allow us to automatically introduce new children via automatic decomposition without needing to also generate constraints for those children (see Subsection Q.3). Alternatively, we support automatic test generation, prompting the language model to generate tests. For generated tests, we select the best implementation based on CodeT score [11], as some generated tests could be incorrect. In practice, we use the first (dependency) approach for inner functions and the second (test generation) approach when the top-level function has no tests. Note an opposite extreme from the sequential case: if no function or only the top-level function has constraints, we simply implement each function some number of times and then test all combinations.</p>
<h1>3 Experiments</h1>
<p>We explore the ability of Parsel to generate programs for various tasks, including competitive coding, a standard coding benchmark, and robotic task planning. These three categories represent related but distinct kinds of algorithmic reasoning, with varying levels of abstractness and generalizability. Competitive programming represents one of the few benchmarks for evaluating code generation tools that benefits greatly from decomposition. ${ }^{1}$ By evaluating Parsel on this breadth of tasks, we hope to better understand its generality as a framework for algorithmic reasoning.</p>
<h3>3.1 Python Code Generation</h3>
<p>Solving competition-level problems. We evaluated Parsel on the competition-level subset of the APPS [27] dataset as follows: first, we zero-shot prompt GPT-3 (text-davinci-002) with the problem statement, requiring it to first propose a high-level solution and explain why it is correct, asking it to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Left: Competition-level problem pass rate. Comparison of Parsel's pass rate on competition-level APPS problems [27] against direct code generation with Codex [12, 11] and AlphaCode [35] Labels correspond to pass@n and pass@nÃ—k. Sample budget is measured in effective number of complete programs generated by the code LLM. (These measures are further explained in Subsection 3.1.) Right: Pass rate vs number of evaluations. Parsel generates and evaluates many programs with a small inference budget, by combinatorial composition. Though evaluation is cheap compared to generation, we examine to understand the effect of evaluation number. For evaluation budget analysis, we evaluate pass@8Ã—16 on a random 200-problem competition-level APPS subset.</p>
<p>"Think step by step to come up with a clever algorithm" [32]. We then prompt Codex [12] to translate the generated solution into Parsel code, providing three examples of valid code. We then attempt to synthesize the Parsel code and evaluate implementations to measure their pass rate, i.e., the proportion of solved problems. We report "pass@nÃ—k", where n is the number of Parsel (i.e. intermediate, semi-formal) programs generated and k is the number of implementations per function. Prior work has emphasized performance as a function of the LLM sample/inference budget measured in complete programs generated [12, 35]; for Parsel, we use the effective number of complete programs generated from the LLM for comparison, which is nÃ—k. We compare to directly generating solutions from Codex [12, 11] and AlphaCode [35]. We visualize the pass rate in Figure 4, finding that Parsel substantially improves over prior work, from 14.5% to 25.5%. We include full prompts and further evaluations in Appendix M.</p>
<p>Since Parsel will "mix-and-match" function implementations, the number of distinct candidate programs available to test grows exponentially with k, while the generation cost for the components is linear. Generating each program likely requires on the order of petaFLOPs of compute [21], dwarfing the computational cost of testing complete programs. Yet when few functions have constraints (as in this evaluation) or there are complex recursive dependencies, it could become expensive to exhaustively evaluate all possible programs. Thus, we also explore the performance of Parsel as a function of the number of evaluations, as shown in Fig. 4. We find that the performance improves log-linearly with the number of evaluations, justifying spending compute to evaluate all programs implied by generated function implementations.</p>
<p>Ablating the Parsel synthesizer. We perform an ablation to explore the impact of the Parsel intermediate language: we provide the same high-level plans to Codex, but instead of translating them to Parsel and synthesizing programs as described, we translate them to Python directly. We match the code generation budget (effectively complete programs referenced), generating 16 Python implementations per high-level plan on 100 randomly sampled problems, and find that the pass@8Ã—16 performance drops to 6% from 25.5% for the full pipeline. Not only did this approach solve substantially fewer problems, but the problems that it did solve were a strict subset of those which we were able to solve with Parsel. This indicates that for Parsel with Codex, the step-by-step decomposition provided by the high-level plan is not by itself responsible for the observed improvements by Parsel.</p>
<p>HumanEval. We next tested Parsel on HumanEval[12]. Because these problems are substantially simpler, they provide an opportunity to evaluate pass@1 by generating tests automatically. Furthermore, this smaller and simpler set of problems enables us to assess the (much more costly)</p>
<p><sup>2</sup>Sampling the longer solutions resulted hitting a rate limit more frequently, even when using the same number of total tokens per prompt; a full 128-solution comparison on 1000 problems would potentially take months.</p>
<p>GPT-4. We found a significant improvement over the state-of-the-art pass@1 performance-from $67 \%$ to $85 \%$. We found that by describing the Parsel format (see Appendix N for details), noting the role of indentation in providing hierarchy and explaining the format of a description, GPT-4 was able to translate natural language plans into Parsel zero-shot. Moreover, the combination of Parsel and GPT-4 surpassed GPT-4 alone on HumanEval, both with and without generated tests, further emphasizing the importance of leveraging intermediate languages like Parsel for improved code generation performance in complex tasks.</p>
<p>The $85.1 \%$ pass rate with generated tests, evaluated based on the CodeT score [11], was obtained by sampling until three Parsel programs had one implementation passing a generated test and then selecting the one passing the most tests. On average, this required 3.5 Parsel programs per problem, with 8 implementations each (i.e. $&lt;32$ complete implementations on average). Parsel also outperforms GPT-4 with CodeT alone on the same set of tests, and using a comparable 32 samples, which passes $81.1 \%$ of the problems. Additionally, when evaluating performance in terms of pass@any (i.e., not filtering attempts by CodeT), we discovered that by allowing up to 8 Parsel programs (still with 8 implementations each), the pass rate increased to $91.5 \%$. In contrast, when allowing up to 64 directly generated programs, the pass rate was only $82.3 \%$. This difference becomes more pronounced when generating 128 programs ( 128 directly vs. $16 \times 8$ with Parsel), improving from $84.1 \%$ to $93.9 \%$ (See Fig. 5).</p>
<p>While there are still eight tasks that are never solved with or without Parsel, GPT-4 solves eighteen problems with Parsel that it cannot solve alone. For eight of these, the solution has six or more functions (excluding defined but unused functions). Of the problems Parsel solved, the ones GPT-4 could not solve itself required 4.2 functions on average, while those GPT-4 could solve required only 2.8 functions (sampled over 50 problems). We find GPT-4 alone solves two problems pass@any that GPT-4 with Parsel cannot: decimal_to_binary and decode_cyclic. The decode_cyclic solutions match Chen et al. [12]'s Figure 2 (but not the dataset's canonical solution) with minor comment differences, indicating possible contamination. In decimal_to_binary, the generated Parsel solutions often unsuccessfully rely on statefulness: for example, divide_by_2_with_remainder calls calculate_remainder and update_decimal, but update_decimal cannot mutate an integer.</p>
<p>Comparison to a human expert. In our fully automated pipeline, an LLM generates a Parsel program, which Parsel attempts to synthesize into a solution. We also evaluated how well an expert Parsel user would perform by directly writing Parsel and interacting with the Parsel synthesizer. As a case study, one of our authors with significant prior experience in competitive programming was presented with 10 randomly-selected competition-level Codeforces problems from the APPS dataset. The participant successfully solved 5 of the problems within a 6 -hour time frame. In comparison, when GPT-3 generated Parsel solutions to these problems, it had a success rate of 2 out of 10 problems with 8 attempts per problem (of course, in a much shorter time-frame). This result suggests that, given a suitable decomposition, the Parsel synthesizer can effectively generate complete correct solutions to hard problems - thus, a major point for improvement in the fully automated pipeline lies in the first stage (Parsel generation). In other words, improving the ability of models to decompose tasks into Parsel programs is a key bottleneck and is an important direction for future work.</p>
<p>While the participant could solve problems that GPT-3 could not, the participant found some aspects of working with Parsel counterintuitive. For example, one of the most effective ways to provide additional details to ensure a working solution was to write additional tests rather than to clarify the meaning in the function descriptions. Given the stochasticity of language model generation, there was concern that changing the description of a child function could break its parent - however, in practice, this seemed rare. We include the participant's solutions and the associated problems in Appendix P.</p>
<p>Chained components. Chen et al. [12] highlight in their discussion of limitations that language models fail to produce code that requires the simple composition of building blocks. We replicate their experiment with Parsel, using the same docstring and providing the same descriptions as function descriptions. In Fig. 6, we visualize how Parsel solves this. We find that as the number of required components grows even interchanging the parts of two complete programs (1x2) solves more problems than Codex with 16 programs. This illustrates a key point: assuming any independence, the complete set of combinatorial combinations of $n$ implementations of a program's functions is clearly more likely to be correct than $n$ samples from that same set. Moreover, this suggests that a framework like Parsel may support increasingly complex and abstracted programs as language models improve.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Pass rate vs number of chained components. Parsel performance is shown as $1 \mathrm{x} k$ where $k$ is the number of complete programs sampled. Orig and Curr correspond to the original [12] and current Codex results, respectively. Codex (@16) corresponds to the Codex pass rate with 16 programs.</p>
<h1>3.2 Robotic Planning in VirtualHome</h1>
<p>We perform a study on VirtualHome [50] to demonstrate that Parsel can also be used for complex robotic planning. VirtualHome is a simulation environment consisting of households with various interactive objects and agents. There are a small set of permissible actions with a strict grammar - tasks like "paint ceiling" may require multiple levels of decomposition, e.g. finding and collecting specific painting supplies, finding and using a ladder, and using the painting supplies, each requiring further decomposition.</p>
<p>To test the effectiveness of Parsel in this domain, we investigate whether Parsel could generate programs to solve tasks in the VirtualHome environment, while using the environment to provide feedback on whether the plan is executable. Specifically, we use Parsel to generate a Python program that can generate action plans in natural language similar to ones used in Huang et al. [28]. In each specified constraint, the produced natural language action plan is translated to formal VirtualHome instructions with minimal regex matching and tested executability. If the instructions can be successfully executed, they are considered valid - however, a potential enhancement could be the inclusion of object-relational constraints on the subsequent state of the world. We include an example of a Parsel program that successfully executed and decomposed a task in VirtualHome in Figure 2. Note we also used a header describing a valid action plan, shown in Figure A.45.</p>
<p>However, as pointed out by [28], while it is easy to check plan executability, it is harder to check correctness, because there are generally many valid ways of accomplishing realistic tasks. Thus, like Huang et al. [28], in order to evaluate the accuracy of the executable plans, we perform a human study. Specifically, we ran two surveys on Prolific [45] where we asked 20 participants to make five rankings each, for a total of 100 rankings per survey. In one survey, we asked participants to rank a set of solutions by how accurately each solution accomplishes the task while in another we asked about how understandable the solution was. Two versions of distinct Parsel solutions were shown (where multiple executable Parsel solutions were available), one which included the indented function names used to solve the tasks alongside their step-by-step solutions, and one which only included step-by-step output. We compared both Parsel solutions to an also-executable baseline solution, based on Huang et al. [28]. We include more details about the survey format and executability in Appendix O.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Scaling performance. The probability of passing an APPS competition-level problem increases quadratically with respect to the log of the number of Parsel programs sampled (left) and the number of implementations sampled (right).</p>
<p>Humans ranked the Parsel solutions as more accurate than the baseline. In each ranking, both the indented and non-indented Parsel solutions were consistently ranked higher than the baseline. In accuracy, standard Parsel solutions (as well as indented solutions) were identified as more accurate than the baseline solutions in 69% of comparisons, more than twice as often as the baseline. In clarity, the standard Parsel solution was 70% more likely to be preferred over the baseline while the indented Parsel solution was 50% more likely to be preferred compared to the baseline. There was no notable difference between indented and non-indented Parsel solutions in either accuracy or clarity.</p>
<h1>4 Related Works</h1>
<p>Step-by-step problem solving with LLMs. Many works show that step-by-step reasoning benefits LLM performance [51, 54, 42, 60, 37, 34] and correspondingly, that this performance can be improved with guidance and tool use [71, 68, 63, 59, 22]. Acquaviva et al. [2] encouragingly showed that humans, when asked to explain how to solve problems in the Abstract Reasoning Corpus [14], tended to provide step-by-step hierarchical descriptions with many verification steps. Moreover, Wies et al. [61] presents a theoretical argument showing problems that can be learned efficiently if decomposed but require exponentially many examples w.r.t. length if not decomposed.
Program synthesis. Program synthesis is the long-standing challenge of generating programs from high-level specifications [26], such as input-output examples [7, 25] and/or natural language descriptions [52, 65, 19]. Program synthesizers typically search the exponentially large space of programs. Consequently, synthesizing large, complex programs remains an open challenge. Recently, library learning has shown a way to make progress: even complex programs can be short in terms of the right high-level library. In turn, this library can be progressively induced from solutions to simpler synthesis problems. This idea is embodied in DreamCoder [23, 9]. Library learning requires a rich distribution of related tasks so that patterns emerge from solutions to simple problems. Patterns are abstracted into useful library functions, enabling short solutions to more complex problems. Parsel also aims to synthesize complex programs by decomposing them. However, Parsel programs specify decompositions, so a family of related tasks is not required.
LLMs for formal environment multi-step planning. Also encouragingly, several existing works can be expressed in Parsel. For example, Huang et al. [28] and Brohan et al. [10] showed that language models can automatically generate step-by-step algorithms for robotic agents in language. In both cases, the generated language corresponds directly to pre-implemented low-level robotic abilities. This could be expressed by providing a task description and constraints evaluating that the high-level task was completed successfully. In addition, Jiang et al. [30] proposed a framework to generate formal proofs in formal theorem-proving languages from informal proofs by first generating an intermediate natural language proof sketch. This could be expressed in Parsel by generating each sketch step as a function and using formal verification for each lemma as the Parsel validation step.
Programming languages and frameworks incorporating language models. Other works have explored programming languages that incorporate language models. For example, Cheng et al. [13] explored the introduction of a language-model-based evaluation function, which would allow f('North America?', 'U.S.') to automatically return 'yes' by referencing the knowledge of the language model, and showed that they could also generate programs using this tool with a language model and Beurer-Kellner et al. [8] explored a related SQL-style LLM-querying language. In addition,</p>
<p>Dohan et al. [20] presents an inference-focused framework for language models more broadly for probabilistic graphical models composed of language model actions. Unlike LM Cascades [20], we primarily focus on the constrained generation of programs, instead of leveraging language models as functions within a particular program.</p>
<p>Testing code language model outputs. Related works have explored the capacity of assert statements to constrain the generation space of code LLMs on functions [6, 12, 35]. The automatic generation of unit tests in Chen et al. [12] allowed them to filter many programs before performing final evaluations, significantly improving the pass rate per evaluation. Both the number of final evaluations and the sample budget are important, and we find that combining Parsel with automatic test generation improves performance given a fixed sample and evaluation budget. In addition, Merrill et al. [38] proves essential constraints on what can be learned from assertions alone, and more crucially, what cannot.</p>
<h1>5 Discussion and Limitations</h1>
<p>We note that Parsel has important limitations, largely stemming from its dependence on closed LLMs. First, LLMs naturally underperform on languages underrepresented in their training data, like Parsel, affecting the quality and reliability of the generated code, and these closed models do not offer a training API. In addition, reliance on Codex and GPT-4 introduces vulnerabilities due to potential abrupt changes in behavior - since this project's start, access to Codex (which is free) has become much more limited and the much-better GPT-4 is also much more expensive. We unfortunately found a 2.7B CodeGen model [39], evaluated like Codex on APPS, solved no problems. However, we anticipate that improvements in open-source code LLMs will mitigate these issues.</p>
<p>Despite the limitations, elaborated in Appendix B, and the challenges of using a language model to generate a language that it has never seen before, the generated Parsel was able to implement robust algorithmic reasoning. There are a few natural questions that arise from this work. How robust is Parsel to the language used to specify the problem? When solving hard problems, is it better to increase the number of Parsel programs sampled or the number of program implementations sampled? We visualize Parsel's performance on the explored subset of APPS in Figure 8 to try to investigate some of these questions. In general, it appears that the probability of passing increased quadratically ( $R^{2} \geq 0.99$ for all curves) with respect to the log of both the number of Parsel programs sampled and the number of implementations sampled. Clearly, since the pass rate is bounded between 0 and 1, this trend cannot continue indefinitely. However, given the rate limit associated with calls to the Codex API (officially, 40,000 tokens or 20 requests per minute, but in practice, we consistently ran into the rate limit with $\approx 10 \%$ of that usage), we were not able to identify the inflection point. Li et al. [35] indicated the pass rate curves become log-linear, but we cannot guarantee this holds for Parsel.</p>
<p>Finally, we highlight that Parsel is already capable of synthesizing larger, more complex programs. For reference, we include a working Lisp interpreter in Appendix H, based on Norvig [41], which we translated to Parsel with 21 functions. We make sure to include no explicit references to Lisp in the Parsel code and provide a header to tell Parsel to keep track of environments as dictionaries.</p>
<h2>6 Conclusion</h2>
<p>We hope that Parsel provides a broadly useful framework for several groups: for programmers, Parsel should provide a language for robust code generation without the need to evaluate the underlying code; for students, Parsel should allow the teaching of algorithmic reasoning with less emphasis on syntax and more emphasis on problem-solving, similarly to a mathematics curriculum; for language models, Parsel should facilitate hierarchical task decomposition.</p>
<p>Ultimately, as we discuss in detail in Appendix C, Parsel has many natural extensions building on prior work in code synthesis from automatic test case generation [17, 12], reranking solutions by model-judged quality [69], more clever recursive decomposition [49], bootstrapping increasingly complex programs [4, 43, 68], to generating language model prompts as part of a target language [20, 31]. As we discuss in Appendix D, we also envision that Parsel may be useful for theorem proving. Thus, Parsel helps fill the gap between reasoning and execution, providing a new approach to complex, hierarchical reasoning tasks for language models.</p>
<h1>Acknowledgements</h1>
<p>We would like to thank Jesse Mu, Tianyi Zhang, Rose E. Wang, Allen Nie, Ben Prystawski, Xindi Wu, Fan-Yun Sun, Isaac Kauvar, Xi Jia Zhou, John Thickstun, and Shikhar Murty for their helpful feedback, as well as Rishi Bommasani for highlighting highly relevant works. We also thank the Stanford HAI-Google collaboration for their Cloud Credit grant.</p>
<h2>References</h2>
<p>[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. ManÃ©, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. ViÃ©gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
[2] S. Acquaviva, Y. Pu, M. Kryven, T. Sechopoulos, C. Wong, G. E. Ecanow, M. Nye, M. H. Tessler, and J. B. Tenenbaum. Communicating natural programs to humans and machines. NeurIPS, 2022.
[3] C. Anil, Y. Wu, A. J. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. GurAri, E. Dyer, and B. Neyshabur. Exploring length generalization in large language models. In Advances in Neural Information Processing Systems.
[4] T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. Advances in Neural Information Processing Systems, 30, 2017.
[5] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, et al. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868, 2022.
[6] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[7] M. A. Bauer. Programming by examples. Artificial Intelligence, 12(1):1-21, 1979.
[8] L. Beurer-Kellner, M. Fischer, and M. Vechev. Prompting is programming: A query language for large language models. arXiv preprint arXiv:2212.06094, 2022.
[9] M. Bowers, T. X. Olausson, C. Wong, G. Grand, J. B. Tenenbaum, K. Ellis, and A. Solar-Lezama. Top-down synthesis for library learning. POPL, 2023.
[10] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference on Robot Learning, 2022.
[11] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.
[12] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[13] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radev, M. Ostendorf, L. Zettlemoyer, et al. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022.
[14] F. Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
[15] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[16] J. Cocke and K. Kennedy. An algorithm for reduction of operator strength. Communications of the ACM, 20(11):850-856, 1977.
[17] E. Daka and G. Fraser. A survey on unit testing practices and problems. In 2014 IEEE 25th International Symposium on Software Reliability Engineering, pages 201-211. IEEE, 2014.</p>
<p>[18] P. Denny, V. Kumar, and N. Giacaman. Conversing with copilot: Exploring prompt engineering for solving cs1 problems using natural language. SIGCSE, 2022.
[19] A. Desai, S. Gulwani, V. Hingorani, N. Jain, A. Karkare, M. Marron, and S. Roy. Program synthesis using natural language. In Proceedings of the 38th International Conference on Software Engineering, pages 345-356, 2016.
[20] D. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous, J. Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.
[21] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547-5569. PMLR, 2022.
[22] D. Dua, S. Gupta, S. Singh, and M. Gardner. Successive prompting for decomposing complex questions. arXiv preprint arXiv:2212.04092, 2022.
[23] K. Ellis, C. Wong, M. Nye, M. SablÃ©-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B. Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wakesleep library learning. In Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, pages 835-850, 2021.
[24] M. Games. The fantastic combinations of john conway's new solitaire game "life" by martin gardner. Scientific American, 223:120-123, 1970.
[25] S. Gulwani. Programming by examples. Dependable Software Systems Engineering, 45(137): $3-15,2016$.
[26] S. Gulwani, O. Polozov, R. Singh, et al. Program synthesis. Foundations and TrendsÂ® in Programming Languages, 4(1-2):1-119, 2017.
[27] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, et al. Measuring coding challenge competence with apps. Advances in Neural Information Processing Systems, 2021.
[28] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.
[29] B. Ibarz, V. Kurin, G. Papamakarios, K. Nikiforou, M. Bennani, R. CsordÃ¡s, A. Dudzik, M. BoÅ¡njak, A. Vitvitskyi, Y. Rubanova, et al. A generalist neural algorithmic learner. LoG, 2022.
[30] A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.
[31] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts, and M. Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022.
[32] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
[33] S. Kulal, P. Pasupat, K. Chandra, M. Lee, O. Padon, A. Aiken, and P. S. Liang. Spoc: Searchbased pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019.
[34] A. K. Lampinen, I. Dasgupta, S. C. Chan, K. Matthewson, M. H. Tessler, A. Creswell, J. L. McClelland, J. X. Wang, and F. Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.
[35] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):1092-1097, 2022.
[36] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.
[37] A. MarasoviÄ‡, I. Beltagy, D. Downey, and M. E. Peters. Few-shot self-rationalization with natural language prompts. arXiv preprint arXiv:2111.08284, 2021.</p>
<p>[38] W. Merrill, Y. Goldberg, R. Schwartz, and N. A. Smith. Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? Transactions of the Association for Computational Linguistics, 9:1047-1060, 2021.
[39] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.
[40] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. A conversational paradigm for program synthesis. arXiv preprint arXiv:2203.13474, 2022.
[41] P. Norvig. How to write a Lisp interpreter (in Python), 2010. URL http://norvig.com/ lispy.html.
[42] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[43] A. Odena, K. Shi, D. Bieber, R. Singh, C. Sutton, and H. Dai. Bustle: Bottom-up program synthesis through learning-guided exploration. ICLR, 2021.
[44] OpenAI. Gpt-4 technical report. arXiv, 2023.
[45] S. Palan and C. Schitter. Prolific. ac-a subject pool for online experiments. Journal of Behavioral and Experimental Finance, 17:22-27, 2018.
[46] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.
[47] N. Perry, M. Srivastava, D. Kumar, and D. Boneh. Do users write more insecure code with ai assistants? arXiv preprint arXiv:2211.03622, 2022.
[48] G. Poesia, A. Polozov, V. Le, A. Tiwari, G. Soares, C. Meek, and S. Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations, 2022.
[49] S. Polu and I. Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.
[50] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8494-8502, 2018.
[51] N. F. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself! leveraging language models for commonsense reasoning. ACL, 2019.
[52] M. Raza, S. Gulwani, and N. Milic-Frayling. Compositional program synthesis from natural language and examples. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.
[53] G. Sandoval, H. Pearce, T. Nys, R. Karri, B. Dolan-Gavitt, and S. Garg. Security implications of large language model code assistants: A user study. arXiv preprint arXiv:2208.09727, 2022.
[54] V. Shwartz, P. West, R. L. Bras, C. Bhagavatula, and Y. Choi. Unsupervised commonsense question answering with self-talk. EMNLP 2020, 2020.
[55] H. A. Simon and A. Newell. Human problem solving: The state of the theory in 1970. American psychologist, 26(2):145, 1971.
[56] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.
[57] A. StuhlmÃ¼ller, J. Reppert, and L. Stebbing. Factored cognition primer. https://primer. ought.org, 2022.
[58] M. Tabachnyk and S. Nikolov. Ml-enhanced code completion improves developer productivity, 2022.
[59] J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</p>
<p>[60] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[61] N. Wies, Y. Levine, and A. Shashua. Sub-task decomposition enables learning in sequence to sequence tasks. arXiv preprint arXiv:2204.02892, 2022.
[62] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1-10, 2022.
[63] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[64] A. Ye, C. Cui, T. Shi, and M. O. Riedl. Neural story planning, 2022. URL https://arxiv. org/abs/2212.08718.
[65] P. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. ACL, 2017.
[66] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In 2018 IEEE/ACM 15th international conference on mining software repositories (MSR), pages 476-486. IEEE, 2018.
[67] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta, H. Michalewski, A. Polozov, and C. Sutton. Natural language to code generation in interactive data science notebooks, 2022. URL https://arxiv.org/abs/2212.09248.
[68] E. Zelikman, Y. Wu, J. Mu, and N. Goodman. STar: Bootstrapping reasoning with reasoning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_3ELRdg2sgI.
[69] T. Zhang, T. Yu, T. B. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. I. Wang. Coder reviewer reranking for code generation. arXiv preprint arXiv:2211.16490, 2022.
[70] K. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. ICLR, 2022.
[71] D. Zhou, N. SchÃ¤rli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.
[72] H. Zhou, A. Nova, H. Larochelle, A. Courville, B. Neyshabur, and H. Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.</p>
<div class="codehilite"><pre><span></span><code><span class="n">select_airport_cities</span><span class="p">(</span><span class="n">city_road_cost</span><span class="p">,</span><span class="w"> </span><span class="n">city_airport_cost</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">representing</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">cost</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">road</span><span class="w"> </span><span class="kr">between</span><span class="w"> </span><span class="kr">any</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">cities</span><span class="p">,</span><span class="w"> </span><span class="kr">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">representing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="kr">of</span>
<span class="w">    </span><span class="n">building</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">airport</span><span class="w"> </span><span class="kr">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">city</span><span class="w"> </span><span class="p">(</span><span class="n">where</span><span class="w"> </span><span class="kr">any</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">cities</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">airports</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">connected</span><span class="p">),</span><span class="w"> </span><span class="kr">return</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">cities</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">airports</span><span class="w"> </span><span class="n">built</span><span class="w"> </span><span class="kr">in</span><span class="w"> </span><span class="n">them</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">minimize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">cost</span>
<span class="w">    </span><span class="kr">of</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">roads</span><span class="w"> </span><span class="kr">and</span><span class="w"> </span><span class="n">airports</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="kr">all</span><span class="w"> </span><span class="n">cities</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">connected</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span>
<span class="w">    </span><span class="nf">sorted</span><span class="w"> </span><span class="kr">in</span><span class="w"> </span><span class="n">ascending</span><span class="w"> </span><span class="n">order</span><span class="p">.</span>
<span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[]</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">11</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="n">sky_city_cost</span><span class="p">(</span><span class="n">city_road_cost</span><span class="p">,</span><span class="w"> </span><span class="n">city_airport_cost</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">lists</span><span class="w"> </span><span class="n">representing</span><span class="w"> </span><span class="n">the</span>
<span class="w">        </span><span class="n">cost</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">building</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">road</span><span class="w"> </span><span class="kr">between</span><span class="w"> </span><span class="kr">any</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">cities</span><span class="p">,</span><span class="w"> </span><span class="kr">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">representing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="kr">of</span>
<span class="w">        </span><span class="n">building</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">airport</span><span class="w"> </span><span class="kr">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">city</span><span class="p">,</span><span class="w"> </span><span class="kr">return</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="n">corresponding</span>
<span class="w">        </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">sky</span><span class="p">.</span>
<span class="w">    </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>
<span class="w">    </span><span class="n">minimum_spanning_tree</span><span class="p">(</span><span class="n">cost_matrix</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">lists</span><span class="w"> </span><span class="n">representing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">cost</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">each</span>
<span class="w">        </span><span class="n">edge</span><span class="p">,</span><span class="w"> </span><span class="kr">return</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">adjacency</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">corresponding</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">minimum</span><span class="w"> </span><span class="n">spanning</span><span class="w"> </span><span class="kr">true</span><span class="p">.</span>
<span class="w">    </span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span><span class="w"> </span><span class="o">-&gt;</span>
<span class="w">        </span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>
<span class="w">    </span><span class="n">final_node_connectors</span><span class="p">(</span><span class="n">adjacency_matrix</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">lists</span><span class="w"> </span><span class="n">representing</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">adjacency</span>
<span class="w">        </span><span class="n">matrix</span><span class="p">,</span><span class="w"> </span><span class="kr">return</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="kr">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">nodes</span><span class="w"> </span><span class="n">connected</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">node</span><span class="p">.</span><span class="w"> </span><span class="n">However</span><span class="p">,</span><span class="w"> </span><span class="nf">if</span><span class="w"> </span><span class="kr">only</span><span class="w"> </span><span class="n">one</span>
<span class="w">        </span><span class="n">node</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">connected</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">node</span><span class="p">,</span><span class="w"> </span><span class="kr">return</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">list</span><span class="p">.</span>
<span class="w">    </span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div>

<p>Figure A.9: A potential programming assignment focused on problem-solving rather than implementation. The top-level function and asserts would be the assigned problem (which Codex [12] does not seem to be able to solve directly), while the other functions would be the student solution.</p>
<h1>A Implications</h1>
<p>Parsel is a natural language compiler framework that bridges the gap between natural language and programming language by allowing programmers to write high-level algorithmic designs in natural language and automatically compiling them into valid code. This has potential benefits for programmers, students, and code language models.</p>
<h2>A. 1 For Programmers</h2>
<h2>A.1.1 Current Limitations</h2>
<p>First, programming generation language models like Codex continue to be constrained primarily to individual functions, rarely exceeding a few dozen lines in practice [12, 58]. This is still a dramatic shift from foundational earlier works, which focused on the association between one line of natural language pseudocode with one line of code [33] or a line of text to a StackOverflow snippet [66]. Yet, these models perform worse the more unusual the desired functions are, and recent research suggests that people using these language models are more likely to introduce buggy code [47], although this is not yet conclusive [53].</p>
<h2>A.1.2 Potential Benefits</h2>
<p>On the other hand, results from Google and others indicate that professionals can write code more efficiently with large language models, and the benefits will likely only improve as they improve [58]. Since Parsel requires constraints that ensure functions behave as expected, this should encourage bug-free programs and avoid the need for manually checking that specific underlying functions are correct. Furthermore, a function written in Parsel is likely to be more resilient to breaking changes in the target language, especially syntactic changes (e.g. Python2 to Python3). In addition, a natural extension would draw on work on automatic unit testing [17] to suggest additional constraints where behavior is ambiguous between implementations of a function.</p>
<h1>A. 2 For Students</h1>
<h2>A.2.1 Current Limitations</h2>
<p>In addition, these language models pose serious challenges for programming pedagogy - existing introductory programming classes rely extensively on teaching syntax and how to implement algorithms over how to solve problems with them. Free language model-based tools like Copilot can essentially solve many of these introductory assignments directly, function by function. Those which cannot be solved currently will be increasingly solved [18].</p>
<h2>A.2.2 Potential Benefits</h2>
<p>Many students currently introduced to programming struggle with learning syntax and debugging unclear compiler or interpreter errors. However, abstracting away these details with a natural-language coding language will likely make learning to code more accessible to students who are just beginning to code. In addition, stepping away from implementation-focused assignments will allow a focus on higher-level problem-solving assignments earlier. These will allow for assignments that are more like those in mathematics. For example, for a problem like Figure A.9, instead of choosing between requiring students to manually implement a problem-solving focused question like the top-level description of, or requiring teaching assistants to manually evaluate the reasoning for correctness, one could ask them to implement a solution in Parsel.</p>
<h2>A. 3 For Code Language Models</h2>
<h2>A.3.1 Current Limitations</h2>
<p>Traditional programming languages result in some unique challenges for language models. For example, unlike natural languages, traditional programming languages are far less robust to slight variations in wording. In addition, traditional programming languages require many tokens for syntactic details and in some cases, may take many lines to express what can be expressed far more simply in language. For example, referring to a shortest-path algorithm or Conway's game of life takes far fewer tokens than actually implementing them. However, even with fairly nonstandard problems, LLMs have shown remarkable algorithmic generalization ability [36, 62, 3, 72]. One alternative that has been explored is conversational code generation [40, 67]. However, these approaches have primarily focused on highly imperative programming structures. Moreover, they still require having the full program in context and do not clearly generalize to complex hierarchical programs with many functions.</p>
<h2>A.3.2 Potential Benefits</h2>
<p>Parsel allows code language models to stay closer to natural language when generating code, which corresponds more closely to their primary source of training data. Moreover, it allows complex but standard methods to be described concisely, requiring fewer tokens to generate. One exciting additional benefit is the potential to generate solutions recursively: if the Parsel compiler is unable to find a solution for a set of functions, it should be possible to prompt the model to define new helper functions. In fact, we find that often the model attempts to reference undefined auxiliary functions when defining complex functions (e.g. "count_living_neighbors(grid, i, j)" in Conway's game of life), and as a result support an optional argument where the model can attempt to resolve NameErrors automatically by attempting to implement functions.</p>
<h2>B Limitations</h2>
<p>There are several limitations to the current implementation of Parsel. First, Parsel relies on a code LLM to generate implementations of individual functions, and the quality of these implementations can vary depending on the specific model used and the complexity of the function descriptions. In particular, Parsel may struggle to generate correct code for individual functions with complex behavior (i.e. functions that Codex cannot implement). However, this can be mitigated by decomposing the complex functions into simpler ones that can be implemented more easily.</p>
<p>The current implementation of Parsel may struggle to generate correct code when there are many functions with complex dependencies or without constraints. This is because the number of implementation combinations to consider grows exponentially with the size of the largest strongly connected components. As discussed, this can limit Parsel's performance on some programs. However, approaches like Chen et al. [11] may be able to mitigate this.</p>
<p>Code LLMs, unfortunately, do not perform well on languages underrepresented in their training data with few examples to learn from, LLMs may struggle to generate correct code in these languages [5]. However, some LLMs can learn new languages in context, allowing them to generate code in languages not in their training data [5]. These limitations can impact the quality and reliability of the code generated with Parsel. In addition, because code LLMs have never been trained on Parsel, this harms their ability to generate it. While we could wait for Parsel to gain widespread adoption, it should also be possible to translate many existing codebases to Parsel. We include a proof-of-concept backtranslation/decompilation study in Appendix K.</p>
<p>In addition, the best open-source code LLMs currently available e.g. PolyCoder [62] substantially underperform Codex, while Codex is competitive with other traditional LLMs on reasoning tasks [36]. However, this dependence on closed models creates a vulnerability, as the providers of closed LLMs can change behavior (e.g. rate limits or model implementations) without warning. Indeed, between the time we started working on Parsel and this version of the paper, OpenAI ended widespread access to Codex, now available only by request.</p>
<p>Because of this, we evaluated a 2.7B CodeGen model from Nijkamp et al. [39] with Parsel in the same configuration we used when evaluating APPS on Codex (in the 8x16 configuration). We found that it could solve none of the random 25 problems which we evaluated it on. However, despite these limitations, the current Parsel implementation has shown promising results in generating correct code for a variety of functions and languages. Many limitations will likely be ameliorated as code LLMs improve.</p>
<h1>C Future Work</h1>
<p>In the future, we hope to more deeply integrate automatic unit test generation, especially in combination with user-provided tests [17, 11]. One method would be to identify edge cases and check whether the set of functions that successfully solve all existing tests disagree on any new tests. This could permit automatic decomposition without exponential growth in implementation combinations. Techniques like those proposed in Zhang et al. [69], which would allow us to rerank a set of solutions, could also allow us to search the combinatorial space of solutions more quickly. Relatedly, for the robotic task planning, incorporating asserts at the execution level (e.g. checking whether the agent is close to the microwave, as in Singh et al. [56]) is a promising research direction. Furthermore, evaluating the examples in this paper, we found that using the minimum CodeT score across all generated functions was a consistently effective heuristic to identify good sets of functions. However, generating unit tests for all functions when generating Parsel programs instead of generating unit tests for a shared top-level function increases the inference cost from linear in the number of tasks to also being linear in the number of functions and Parsel programs generated. Finding a way to balance this tradeoff would likely be valuable.</p>
<p>In addition, we plan to incorporate ways of varying the "confidence threshold" of the language model. Ensuring that the descriptions are straightforward and unambiguous is important for more critical programs and parts of programs. In addition, when teaching students simpler concepts, requiring them to decompose the task further may be useful.</p>
<p>We would like to integrate value functions to allow decomposition to be done more methodically where no verification is possible. Specifically, automatically decomposing all functions that have not yet been implemented in an SCC is suboptimal and could be improved with a model of expected improvement due to expansion, as done for proof expansion in Polu and Sutskever [49]. In addition, when decomposing functions, we would like to permit the model to reference already-defined functions (rather than to just define new ones). We might even use the code language model to determine which function to evaluate next. Further, we aim to support more general reward functions for function implementations where multiple may be valid but we rank implementations based on a desired feature. These "soft" constraints may also allow new Parsel uses, e.g. planning stories in natural language [64].</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure A.10: Parsel to Lean (theorem proving)</p>
<p>Finally, we hope it would be possible to use Parsel as a framework for bootstrapping increasingly complex program generation (e.g. Anthony et al. [4], Zelikman et al. [68], Odena et al. [43]). That is, by 1) generating Parsel examples from a purely natural language specification and then reinforcing those which successfully compile, and 2) by reinforcing the model with each successfully compiled component, we would likely be able to iteratively improve performance with an arbitrarily large dataset of examples.
Another feature that would be valuable would be the ability to incorporate multiple base tools with different kinds of specialized models, inspired by Ibarz et al. [29] and Dohan et al. [20]. That is, it would be valuable to allow a model to determine which target language to use, possibly combining them. For example, for large parts of the Tensorflow and PyTorch libraries, while their interfaces are written in Python, they depend heavily on large C++ codebases [46, 1]. Relatedly, Cobbe et al. [15] showed that giving language models access to a calculator allowed them to solve more complex math word problems. This, combined with the observation that Parsel could also compile programs by generating language model prompts to be used as part of the program, may potentially allow the automatic generation of task-specific language model cascades [20].
Another noteworthy addition would be the integration of Synchromesh [48], ensuring that each new word or token generated by the model is actually possible within the grammar of the given formal language and does not violate other semantic constraints.
Ultimately, we hope that this specification for Parsel is a jumping-off point for a new way of thinking about programming and reasoning.</p>
<h1>D Theorem Proving in Lean</h1>
<p>With the same framework, we can generate proofs in formal theorem-proving languages such as Lean, as in Figure A.10. We include the translated version in the appendix. Note a nuance of Lean and theorem-proving languages is that the ability to run Lean on proof with no errors/warnings indicates the proof is correct (but is not a guarantee that the proof statement matches our claim in language). Thus, each function in a Lean Parsel proof has an "implicit constraint." This makes it straightforward to identify which informal parts of a proof are most difficult to explicate. Generally, we believe Parsel can be a powerful tool for theorem proving.
Yet, we observed important challenges in this context, which we believe are avenues for future work and can be resolved. For example, in datasets such as MiniF2F [70], many proofs require explicit calculations in intermediate steps. That is, many proofs are similar to "Find the minimum value of $\frac{9 x^{2} \sin ^{2} x+4}{4 \sin x}$ for $0&lt;x&lt;\pi$. Show that it is 012 ." (from the informal MiniF2F introduced by Jiang et al. [30]). We believe that a dataset of proof statements (in natural and formal language), requiring complex proofs that are more abstract and less dependent on explicit calculations would allow us to better measure progress towards solving difficult theorems - we leave this to future work.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">parsel</span><span class="p">(</span><span class="nx">program</span><span class="p">,</span><span class="w"> </span><span class="nx">target_language</span><span class="p">):</span><span class="w"> </span><span class="nx">synthesize</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="nx">specifying</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">Parsel</span><span class="w"> </span><span class="nx">program</span><span class="p">.</span>
<span class="nx">parse_program</span><span class="p">(</span><span class="nx">program</span><span class="p">):</span><span class="w"> </span><span class="nx">parse</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Parsel</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">call</span><span class="w"> </span><span class="nx">graph</span>
<span class="w">    </span><span class="nx">create_root_node</span><span class="p">():</span><span class="w"> </span><span class="nx">create</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">root</span><span class="w"> </span><span class="nx">node</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">node</span><span class="p">,</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">constraints</span>
<span class="w">    </span><span class="nx">parse_line</span><span class="p">(</span><span class="nx">line</span><span class="p">,</span><span class="w"> </span><span class="nx">current_node</span><span class="p">,</span><span class="w"> </span><span class="nx">current_indent</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">function_graph</span><span class="p">:</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">indentation</span><span class="p">,</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">node</span>
<span class="w">        </span><span class="nx">to</span><span class="w"> </span><span class="nx">its</span><span class="w"> </span><span class="nx">parents</span><span class="p">.</span><span class="w"> </span><span class="k">then</span><span class="p">,</span><span class="w"> </span><span class="nx">parse</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">definition</span><span class="p">,</span><span class="w"> </span><span class="nx">reference</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="kd">constraint</span><span class="p">.</span>
<span class="w">    </span><span class="nx">parse_definition</span><span class="p">(</span><span class="nx">line</span><span class="p">):</span><span class="w"> </span><span class="nx">create</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">node</span><span class="p">,</span><span class="w"> </span><span class="nx">make</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">child</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">node</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">parent</span><span class="p">,</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">assign</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">as</span>
<span class="w">        </span><span class="nx">current</span><span class="w"> </span><span class="nx">node</span><span class="p">.</span>
<span class="w">    </span><span class="nx">parse_reference</span><span class="p">(</span><span class="nx">line</span><span class="p">):</span><span class="w"> </span><span class="nx">add</span><span class="w"> </span><span class="nx">reference</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">child</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">node</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nx">reference</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">ancestor</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">direct</span><span class="w"> </span><span class="nx">child</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">an</span>
<span class="w">        </span><span class="nx">ancestor</span>
<span class="w">    </span><span class="nx">parse_constraint</span><span class="p">(</span><span class="nx">line</span><span class="p">):</span><span class="w"> </span><span class="nx">add</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="kd">constraint</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">node</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">constraints</span><span class="p">.</span>
<span class="nx">get_dependency_graph</span><span class="p">(</span><span class="nx">function_graph</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">dependency_graph</span><span class="p">:</span><span class="w"> </span><span class="nx">taking</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">graph</span><span class="p">,</span><span class="w"> </span><span class="nx">create</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">copy</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">nodes</span><span class="w"> </span><span class="nx">without</span>
<span class="w">        </span><span class="nx">asserts</span><span class="w"> </span><span class="nx">also</span><span class="w"> </span><span class="nx">depend</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">their</span><span class="w"> </span><span class="nx">parents</span><span class="w"> </span><span class="nx">unless</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">implicitly</span><span class="w"> </span><span class="nx">tests</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">functions</span><span class="p">.</span>
<span class="nx">identify_strongly_connected_components</span><span class="p">(</span><span class="nx">dependency_graph</span><span class="p">):</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nx">BCCs</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">dependency</span><span class="w"> </span><span class="nx">graph</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">edges</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">the</span>
<span class="w">        </span><span class="nx">BCCs</span><span class="p">.</span>
<span class="nx">synthesize_scc</span><span class="p">(</span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="nx">scc_graph</span><span class="p">):</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="nx">solving</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">given</span><span class="w"> </span><span class="nx">BCC</span><span class="p">,</span><span class="w"> </span><span class="nx">starting</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">BCC</span><span class="w"> </span><span class="nx">dependencies</span><span class="p">,</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nx">generating</span><span class="w"> </span><span class="nx">possible</span><span class="w"> </span><span class="nx">implementations</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">BCC</span><span class="w"> </span><span class="nx">functions</span><span class="p">,</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">finding</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="nx">combination</span><span class="w"> </span><span class="nx">satisfying</span><span class="w"> </span><span class="nx">the</span>
<span class="w">        </span><span class="nx">functions</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">constraints</span>
<span class="nx">synthesize_children</span><span class="p">(</span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="nx">scc_graph</span><span class="p">):</span><span class="w"> </span><span class="nx">synthesize</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">BCCs</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">BCC</span><span class="w"> </span><span class="nx">depends</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">add</span><span class="w"> </span><span class="nx">them</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="kt">string</span><span class="p">.</span>
<span class="nx">synthesize_scc</span>
<span class="nx">generate</span><span class="w"> </span><span class="nx">implementations</span><span class="p">(</span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">children_implementation_str</span><span class="p">):</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">BCC</span><span class="p">,</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">to</span>
<span class="w">        </span><span class="nx">generate</span><span class="w"> </span><span class="nx">n</span><span class="w"> </span><span class="nx">implementations</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">starting</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">BCC</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">children</span><span class="p">.</span>
<span class="nx">solve_constraints</span><span class="p">(</span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="kd">fn</span><span class="w"> </span><span class="nx">implementations</span><span class="p">):</span><span class="w"> </span><span class="nx">taking</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">provided</span><span class="w"> </span><span class="nx">constraints</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluate</span><span class="w"> </span><span class="nx">a</span>
<span class="w">        </span><span class="nx">shuffled</span><span class="w"> </span><span class="nx">list</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">direct</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">implementations</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">constraints</span><span class="w"> </span><span class="nx">until</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">passes</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">them</span>
<span class="w">    </span><span class="nx">direct_product</span><span class="w"> </span><span class="nx">implementations</span><span class="p">(</span><span class="kd">fn</span><span class="w"> </span><span class="nx">implementations</span><span class="p">):</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">direct</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">list</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">lists</span><span class="w"> </span><span class="nx">of</span>
<span class="w">        </span><span class="kd">fn</span><span class="w"> </span><span class="nx">implementations</span>
<span class="nx">generate_constraints</span><span class="p">(</span><span class="kd">fn</span><span class="w"> </span><span class="nx">node</span><span class="p">):</span><span class="w"> </span><span class="nx">translate</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">constraints</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="kt">string</span><span class="w"> </span><span class="nx">idiomatic</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">target</span>
<span class="w">        </span><span class="nx">language</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">add</span><span class="w"> </span><span class="nx">these</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">list</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">combined</span><span class="w"> </span><span class="nx">implementations</span>
<span class="nx">eval_str</span><span class="p">(</span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="nx">implementation_str</span><span class="p">):</span><span class="w"> </span><span class="nx">evaluate</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">implementation</span><span class="w"> </span><span class="nx">including</span><span class="w"> </span><span class="nx">constraints</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">target</span><span class="o">-</span><span class="nx">language</span>
<span class="w">        </span><span class="nx">executor</span>
<span class="nx">on_fail</span><span class="p">(</span><span class="nx">scc</span><span class="p">,</span><span class="w"> </span><span class="nx">scc_graph</span><span class="p">):</span><span class="w"> </span><span class="nx">raise</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">error</span><span class="w"> </span><span class="nx">highlighting</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">scc</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="nx">could</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">synthesized</span>
</code></pre></div>

<p>Figure A.11: Pseudocode in the style of Parsel describing how Parsel synthesizes programs. A detailed version including automatic decomposition and automatic infilling is in Figure A. 12 of Appendix F. Constraints are left out for clarity - e.g. one could define a test function and validate the compilability (or lack thereof) of a set of reference Parsel programs.</p>
<h1>E Optimizations</h1>
<h2>E. 1 Caching</h2>
<p>We cache responses from the language model with respect to the prompt and language model decoding parameters 1) to reduce the number of queries necessary and 2) to keep the programs generated mostly stable (i.e. a working function should continue working unless it or its children change). To this end, when the number of desired implementations increases for a pre-existing query with all other arguments fixed (temperature, number of decoding tokens, etc), we append the additional ones to those already generated.</p>
<h2>E. 2 Automatic Function Infilling</h2>
<p>Sometimes, a function generated by a language model may call a function that is not yet implemented. In this case, we can (optionally) attempt to automatically generate and implement it based on its usage. The function is then incorporated into the call graph as a unit-test-less child of the function which calls it. To avoid infinite recursion and inefficient use of language model quota, we limit the number of times that this process can be applied to a function.</p>
<h2>E. 3 Multiprocessing</h2>
<p>We use multiprocessing with a user-specified timeout to test many implementation sets in parallel to allow for many fast solutions to be tested alongside slower solutions ${ }^{3}$.</p>
<h2>F Parsel Pseudocode</h2>
<p>We include a longer-form Parsel pseudocode in the style of Parsel. Note this pseudocode does not include backtranslation.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure A.12: Longer pseudocode of Parsel, including automatic infilling and automatic decomposition.</p>
<h1>G Parsel Overview (Detailed)</h1>
<p>We include a more detailed figure outlining Parsel.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure A.13: Parsel overview (detailed).</p>
<h2>H Lisp Interpreter</h2>
<p>We include the Parsel code for a minimal Lisp interpreter.</p>
<p>1 An env is a dictionary of {"var':val} pairs, with a link to its outer environment in env[']_outer'].</p>
<p>2 A procedure is a lambda expression, with parms, body, and env which calls eval_exp on the body.
3 #<em>#</em>#
4 evaluate_program(program): Initialize a standard environment. Parse and evaluate a list of expressions, returning the final result.
5 ['(define square (lambda (r) (<em> r r)))', '(square 3)'] -&gt; 9
6 get_env(parms, args, env=None): Return a new env inside env with parms mapped to their corresponding args, and env as the new env's outer env.
7 [], [] -&gt; {'_outer': None}
8 ['a'], [1] -&gt; {'a': 1, '_outer': None}
9 standard_env(includes=['math','ops','simple_math']): An environment with some Scheme standard procedures. Start with an environment and update it with standard functions.
10 [] -&gt; {'_outer': None}
11 get_math(): Get a dictionary mapping math library function names to their functions.
12 get_ops(): Get a dictionary mapping operator symbols to their functions: , ,, </em>, /, &gt;, &lt;, &gt;=, &lt;=, =.
13 get_simple_math(): Get a dictionary mapping 'abs', 'min', 'max', 'not', 'round' to their functions.
14 apply_fn_dict_key(fn_dict_generator, key, args_list): Return the value of fn_dict_generator()<a href="*args_list">key</a> in standard_env.
15 get_math, 'sqrt', [4] -&gt; 2.0
16 get_ops, '+', [1, 2] -&gt; 3
17 get_simple_math, '###', [-1] -&gt; 1
18 get_math
19 get_ops
20 get_simple_math
21 parse_and_update(expression, env): Parse an expression, return the result.
22 '('+ 1 (<em> 2 3))', {'+': (lambda x, y: x + y), '</em>': (lambda x, y: x * y), '_outer': None} -&gt; 7
23 eval_exp(x, env): Evaluate an expression in an environment and return the result. Check if x is a list, a string, or neither, and call the corresponding function.
24 1, {'_outer': None} -&gt; 1
25 find(env, var): Find the value of var in the innermost env where var appears.
26 {'a':4, '_outer':None}, 'a' -&gt; 4
27 {"_outer':{'a':4, '_outer':None}}, 'a' -&gt; 4
28 {'a':3, '_outer':{'a':4, '_outer':None}}, 'a' -&gt; 3
29 string_case(x, env): Return find(env, x).
30 'a', {'a':4, '_outer':None} -&gt; 4
31 find
32 list_case(x, env): Handle the function specified by the first value of x. Handle the first value of x being quote, if, define, set!, lambda, or otherwise. Return the result.
33 ['quote', 'a'], {'_outer': None} -&gt; 'a'
34 ['if', True, 1, 2], {'_outer': None} -&gt; 1
35 ['define', 'a', 1], {'_outer': None} -&gt; None
36 get_procedure(parms, body, env): Return a procedure which evaluates body in a new environment with parms bound to the args passed to the procedure (in the same order as parms).
37 eval_procedure(parms, body, env, args): Gets a procedure and returns the result of evaluating proc(<em>args) in env. Should not be called directly.
38 ['r'], ['</em>', 'pi', ['<em>', 'r', 'r']], {'</em>': (lambda x, y: x * y), 'pi': 3, '_outer': None}, [1] -&gt; 3
39 get_procedure
40 get_env
41 eval_exp
42 otherwise_case(x, env): Get the procedure by evaluating the first value of x. Then, evaluate the arguments and apply the procedure to them. Return the result.
43 ['<em>', 1, 2], {'</em>': (lambda x, y: x + y), '_outer': None} -&gt; 3
44 eval_exp
45 eval_exp
46 not_list_case(x, env): Return x
47 1, {} -&gt; 1
48 parse(program): Read a Scheme expression from a string.
49 '(1 + (2 * 3))' -&gt; [1, '<em>', [2, '</em>', 3]]
50 tokenize(x): Convert a string into a list of tokens, including parens.
51 "1 + 2" -&gt; ['1', '+', '2']
52 "1 + (2 * 3)" -&gt; ['1', '+', '(', '2', '<em>', '3', ')']
read_from_tokens(tokens): Translate tokens to their corresponding atoms, using parentheses for nesting lists.
53 ['(', '1', '+', '(', '2', '</em>', '3', ')', ')'] -&gt; [1, '<em>', [2, '</em>', 3]]
54 atom(token): Numbers become numbers; every other token is a string.
55 "1" -&gt; 1
56 "a" -&gt; "a"
57 "1.2" -&gt; 1.2
58 nested_list_to_str(exp): Convert a nested list into a string with nesting represented by parentheses.
59 1 -&gt; "1"
60 [1, '<em>', [2, '</em>', 3]] -&gt; "(1 + (2 * 3))"</p>
<p>Figure A.14: Full Lisp interpreter implementation in Parsel, including constraints.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ As anticipating the number of steps that a solution will take universally is a version of the halting problem and thus intractable.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2487 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2487</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2487</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-248266468</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2204.09513v1.pdf" target="_blank">Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics</a></p>
                <p><strong>Paper Abstract:</strong> Calibration of highly dynamic multi-physics manufacturing processes such as electrohydrodynamics-based additive manufacturing (AM) technologies (E-jet printing) is still performed by labor-intensive trial-and-error practices. Such practices have hindered the broad adoption of these technologies, demanding a new paradigm of self-calibrating E-jet printing machines. Here we develop an end-to-end physics-informed Bayesian learning framework (GPJet) which can learn the jet process dynamics with minimum experimental cost. GPJet consists of three modules: the machine vision module, the physics-based modeling module, and the machine learning (ML) module. GPJet was tested on a virtual E-jet printing machine with in-process jet monitoring capabilities. Our results show that the Machine Vision module can extract high-fidelity jet features in real-time from video data using an automated parallelized computer vision workflow. The Machine Vision module, combined with the Physics-based modeling module, can also act as closed-loop sensory feedback to the Machine Learning module of high- and low-fidelity data. This work extends the application of intelligent AM machines to more complex working conditions while reducing cost and increasing computational efficiency.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2487.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2487.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPJet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPJet: Physics-Informed Bayesian Learning Pipeline for E-jet Printing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end physics-informed probabilistic machine learning framework integrating real-time machine vision, a fast physics-based multiphysics model, and Gaussian process-based ML (single- and multi-fidelity) to actively learn and autonomously calibrate electrohydrodynamic jet printing (MEW) processes while minimizing expensive high-fidelity experiments and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPJet</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPJet is a closed-loop pipeline composed of three tightly integrated modules: (a) a parallelized Machine Vision module that extracts high-fidelity jet features (jet radius profile, lag distance, velocities, angles) from video frames in real time; (b) a Physics-based Modeling module providing low-fidelity, cheap multi-physics predictions (1D thin-filament electrohydrodynamics + geometric 'fluid-mechanical sewing machine' model) of the same engineered features; and (c) a Machine Learning module that uses Gaussian Process Regression (single- and multi-fidelity formulations) together with active learning / Bayesian optimization acquisition rules to decide which additional (expensive) high-fidelity observations to acquire in order to learn the mapping from inputs (e.g., tip-to-collector position, speed ratio) to outputs (jet radius, lag distance). The pipeline deploys uncertainty-aware acquisition functions (pure-exploration variance-based and an exploration–exploitation BO strategy) and multi-fidelity kernels (linear autoregressive model between fidelities) to trade cheap low-fidelity evaluations for fewer costly high-fidelity experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Additive manufacturing / experimental calibration of electrohydrodynamic jet printing (melt electrowriting); generally applicable to experimental materials-process optimization and autonomous calibration of manufacturing processes.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Resource allocation is driven by an uncertainty-aware active learning loop: (1) build/fit GP(s) to currently available data (high-fidelity observations from machine vision optionally augmented by low-fidelity physics model outputs); (2) compute acquisition function over candidate inputs (points along jet length for radius, or speed-ratio for lag distance); (3) select next measurement(s) by maximizing the acquisition (pure-exploration using posterior variance for fast coverage or acquisition functions that trade mean and variance in Bayesian optimization for finding optima); (4) perform the (costly) high-fidelity measurement via machine vision at selected input(s) and update the models. Multi-fidelity GPR is used to allocate budget away from high-cost experiments by leveraging many cheap low-fidelity model evaluations and a small number of high-fidelity experiments, and parallelized vision processing is used to satisfy real-time computational budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured in wall-clock processing time per video frame for machine vision (seconds per frame; e.g., sequential 0.033 s/frame, concurrent small improvement, parallel implementation 0.014 s/frame), and in number of high-fidelity observations (count of expensive experimental measurements); also implicitly number of iterations and total active-learning wall-clock time (e.g., ~0.5 s per iteration, total ~3 s for an active learning run reported).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Posterior variance (GP predictive variance) used directly as an exploration acquisition; predictive posterior mean and variance used for acquisition functions; model error is tracked with RMSE against held-out experimental ground truth; uncertainty quantified with 95% confidence intervals and Mean Confidence Interval Width (MCIW); Bayesian Optimization performance measured with Minimum Regret metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Two acquisition modes are used: (a) pure exploration: acquisition ∝ posterior variance (select points with highest predictive uncertainty) for fast function-space coverage; (b) exploration–exploitation (Bayesian Optimization) when optimizing an objective (e.g., minimize lag distance): acquisition functions that depend on both GP mean and variance (the paper describes the BO 'spirit' and uses acquisition that trades mean and variance; specifics of e.g. EI are not enumerated, but the procedure maximizes an acquisition balancing uncertainty and predicted performance). Initial points are seeded randomly or from user-specified initial training points; iterations continue until termination criteria (max iterations, uncertainty threshold, or target reached).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting heuristic is implemented beyond uncertainty-driven sampling; exploration via posterior variance implicitly encourages sampling across diverse/high-uncertainty regions, but there is no separate explicit diversity-regularizer or batch-diversification mechanism described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed/limited number of expensive high-fidelity experiments (counts), real-time per-frame processing time (wall-clock), and maximum iteration/termination criteria (iteration budget or uncertainty thresholds).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget constraints by (1) multi-fidelity modeling that substitutes many cheap low-fidelity simulations for a reduced set of expensive high-fidelity measurements, (2) uncertainty-driven active learning to focus the limited high-fidelity budget where it reduces predictive uncertainty most (pure exploration) or where objective improvement is likely (BO), and (3) parallelization (multi-threading + multiprocessing) of vision tasks to meet real-time per-frame computation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>No general 'breakthrough' metric is defined; in application-specific tasks, breakthroughs are framed as achieving optimization objectives (e.g., minimum lag distance) or accurate function approximation with minimal high-fidelity data. Performance in finding optima is quantified by Minimum Regret metric; quality of learned models is measured by RMSE and confidence interval widths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include: machine vision processing time per frame (sequential ~0.033 s/frame -> parallel ~0.014 s/frame), GP predictive accuracy metrics (RMSE reported in supplementary figures but specific numbers not in main text), confidence quantified by 95% CI and Mean Confidence Interval Width (MCIW), active-learning iteration time ~0.5 s and total learning time ~3 s for reported scenarios, number of high-fidelity observations used in case studies (examples: passive GP fit with n=10 high-fidelity observations; multi-fidelity GPR scenarios with n=6 or n=7 high-fidelity observations plus 32 low-fidelity points), BO convergence to near-optimal speed-ratio in 2–3 iterations in the virtual MEW experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Simple single-fidelity Gaussian Process Regression (passive learning using only high-fidelity data), offline Design-of-Experiments (exhaustive grid search / Cartesian grid DoE referenced in introduction), and random initial sampling for active scenarios are used as baselines or comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Multi-fidelity GPR + active learning outperforms single-fidelity GP: achieves comparable or better predictive accuracy using fewer high-fidelity observations (example: multi-fidelity with n=6 HF + 32 LF improves over single-fidelity GP with the same number of HF points). Active learning on the multi-fidelity model is reported as 'significantly faster' with tighter confidence intervals than the simple GP active learning; BO finds the minimum-lag speed ratio in 2–3 iterations compared to slower passive calibration or grid search. Exact numeric RMSE reductions vs baseline are reported in supplementary figures (not all values tabulated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Concrete reported improvements include: vision pipeline wall-clock reduction ≈57.6% (0.033 s -> 0.014 s per frame); reduction in required high-fidelity observations (demonstrated reduction from n=10 HF to n=6–7 HF when informed by low-fidelity physics while retaining predictive accuracy—≈30–40% fewer expensive measurements in those cases); active-learning wall-clock learning in seconds (~3 s) for the small virtual experiments. The paper describes 'significantly faster' learning and 'more confident' predictions for MFD vs single-fidelity but does not give a single combined efficiency percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper analyzes several tradeoffs: (1) Data quantity and placement vs predictive accuracy: more high-fidelity observations and their locations reduce RMSE and uncertainty (shown in comparisons n=5 vs n=10 and n=4 vs n=12 cases). (2) Cheap low-fidelity evaluations vs expensive high-fidelity experiments: introducing physics-based low-fidelity data via multi-fidelity GPR reduces the number of costly high-fidelity observations required. (3) Computational processing time vs information throughput: parallelization/multiprocessing trades hardware resources for reduced per-frame latency to meet real-time constraints. (4) Exploration vs exploitation: pure exploration (variance acquisition) accelerates global function learning, whereas BO balances exploitation to find optima quickly. The results show that including physics priors (multi-fidelity) tightens priors and reduces required experimental budget, but low-fidelity models can underpredict in some regions (Taylor cone), requiring some targeted high-fidelity sampling there.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key conclusions: (a) Physics-informed priors via multi-fidelity modeling are effective at reallocating experimental budget away from many expensive high-fidelity measurements toward many cheap low-fidelity evaluations plus a small set of strategically chosen high-fidelity points; (b) uncertainty-driven acquisition (posterior variance) quickly guides expensive measurements to regions of greatest information value, improving learning speed; (c) for optimization tasks (lag-distance minimization) BO rapidly finds near-optimal operating points (speed ratio ≈1) in very few experiments; (d) computational parallelization is necessary to meet real-time sensory budgets and thus constrains experimental allocation decisions (must choose experiments compatible with processing latency). Recommendations: combine multi-fidelity modeling, uncertainty-aware acquisition, and computational parallelization to optimize the tradeoff between experimental cost and information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2487.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2487.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity GPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-fidelity Gaussian Process Regression (autoregressive/recursive formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic surrogate modeling method that combines cheap low-fidelity model outputs and sparse expensive high-fidelity observations in a hierarchical GP framework (linear autoregressive relationship) to produce improved predictions and uncertainty quantification while reducing the required number of costly experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-fidelity Gaussian Process Regression (recursive AR1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper uses a two-level multi-fidelity GP where the high-fidelity function is modeled as a linear scaling of the low-fidelity function plus a GP-modeled discrepancy term (h_H(x) = rho * h_L(x) + delta(x)). A numerically efficient recursive inference scheme is used with nested training sets (higher-fidelity inputs are a subset of lower-fidelity inputs). The approach yields predictive posterior mean and variance that incorporate both data sources, enabling uncertainty-aware active learning and acquisition that considers both fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Surrogate modeling for experimental design and learning of manufacturing/process physics (jet radius profile prediction across nozzle-to-collector distance), general materials and multiphysics modeling applications.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experimental budget by leveraging many low-cost low-fidelity model evaluations (32 LF points in tests) and selecting a small set of high-fidelity observations (e.g., n=6 or n=7) chosen by active learning; training inputs are nested so LF grid can guide HF selection. The multi-fidelity kernel encodes correlation (scaling factor) so model effectively uses LF information to reduce HF sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of low-fidelity model evaluations (cheap numerical ODE solves) vs counts of high-fidelity experimental observations; and wall-clock per-evaluation cost (LF much cheaper than HF video acquisition+processing).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP posterior variance and predictive improvement when adding HF points; RMSE and MCIW are used to evaluate information gained per HF experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition driven by GP posterior variance for pure exploration; in BO scenarios the multi-fidelity GP provides predictive mean and variance used in acquisition that balances exploration and exploitation. The paper uses the multi-fidelity posterior to select HF measurements that maximize information (uncertainty reduction) per cost.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond sampling at high-uncertainty regions; nested LF grid combined with variance-based acquisition implicitly spreads HF samples to informative diverse regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited budget of high-fidelity experimental observations (count), unlimited/cheap LF evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimizes under budget by using a fixed dense LF grid and actively selecting a small number of HF sample locations that most reduce HF predictive uncertainty using the MF-GP posterior; nested training structure allows efficient recursive inference.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not defined — task-specific objectives (e.g., accurate radius profile or minimized lag distance) are used as practical high-impact outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Case examples: multi-fidelity fits with n=6 HF + 32 LF or n=7 HF + 32 LF achieved better predictive accuracy and narrower uncertainty than single-fidelity GP fitted with the same number of HF points; particular failure region noted in Taylor cone where LF model underpredicts and additional HF point improved fit.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against single-fidelity (simple) Gaussian Process Regression trained on HF data only with same HF counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Multi-fidelity GPR gives more accurate predictions and tighter confidence intervals for the same HF budget; active learning with MFD converges faster (lower RMSE and MCIW) than simple GP (detailed numeric comparisons in Supplementary Information).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Demonstrated reduction in required HF observations for comparable accuracy (examples: similar performance with n=6–7 HF with MFD vs n≈10 HF for single-fidelity GP in certain radius-profile tasks; this implies ~30–40% fewer expensive HF measurements in reported cases).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors highlight that LF physics models can underperform in some localized regions (Taylor cone), so blindly relying on LF can bias predictions; thus MF strategy must include HF sampling in areas where LF discrepancy is expected. The analysis shows LF data constrains priors and reduces HF requirements but requires careful placement of the remaining HF budget.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Best allocation is to use a dense LF grid to capture large-scale trends and then allocate a small number of HF experiments targeted (via uncertainty acquisition) to regions where LF models are less reliable or uncertainty remains high, yielding efficient reduction in HF experimental cost while preserving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2487.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2487.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Learning / BO (GP-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty-aware Active Learning and Bayesian Optimization using Gaussian Processes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active data acquisition and optimization strategy that uses GP predictive mean and variance to form acquisition functions (pure-exploration variance-based and BO-style exploration–exploitation) to sequentially select costly experiments that maximize information gain or achieve optimization objectives (e.g., minimize lag distance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-driven Active Learning and Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The paper uses Gaussian Processes as surrogates and defines acquisition functions that depend on the GP predictive mean and variance. For function approximation/exploration tasks they use a purely exploratory acquisition (maximize posterior variance) to select next measurement points along the jet length; for objective optimization (minimize lag distance) they use a BO loop that balances exploration and exploitation (acquisitions that consider predicted mean and uncertainty) until termination criteria (max iterations or uncertainty threshold) are met. Acquisition maximization is performed at each iteration to pick the next experimental setting, which is then measured by the machine vision module and used to update the GP.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design and optimization in additive manufacturing process calibration (MEW); generic for autonomous experimentation and materials discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential allocation of experimental budget to inputs that maximize acquisition value: for exploration tasks, select inputs with highest predictive uncertainty (variance) to maximize information gain per experiment; for optimization tasks, select inputs balancing predicted improvement and uncertainty to quickly find minima (or maxima). Initial points are seeded (random or user-defined); termination is by iteration budget or uncertainty/objective thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-iteration wall-clock time (GP fit + acquisition evaluation + measurement), reported iteration time ~0.5 s in virtual runs; total active learning wall-clock time reported ~3 s for example runs. Also HF experiment count is a cost metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Posterior variance (used explicitly), RMSE reduction over iterations, Mean Confidence Interval Width (MCIW) as an uncertainty metric, and Minimum Regret when optimizing an objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Two concrete mechanisms: (1) pure-exploration acquisition = maximize GP posterior variance (sampling uncertain regions for diversity/coverage), (2) BO-style acquisition that depends on GP mean and variance to balance exploiting regions predicted to be optimal and exploring uncertain regions. The choice of acquisition is task-dependent: pure exploration for accurate function mapping, BO for finding optima (lag-distance).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit diversity arises from variance-driven selection which favors unexplored/high-uncertainty regions; no explicit additional diversity penalty or batch diversity algorithm is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed iteration budget or HF sample budget, and real-time computational budget for per-iteration processing.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Active learning focuses HF experiments on maximal information/utility locations; uses early stopping or termination criteria (max iterations, uncertainty threshold, or objective convergence) to respect budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Operationalized as successful optimization outcomes (e.g., reaching minimal lag distance) or rapid function approximation with few HF points; performance measured via Minimum Regret for optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: active-learning iterations last ~0.5 s; total learning runs ~3 s; active learning reached accurate function approximations in a few iterations for both radius profile and lag-distance tasks; BO reached near-optimal speed ratio (≈1) in 2–3 iterations in the virtual experiments. RMSE and MCIW decreased over iterations (plotted in supplementary figures).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against passive learning (fitting GP to a fixed HF dataset), single-fidelity GP active learning, and offline grid/DoE approaches referenced in introduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Active learning with multi-fidelity models converges faster and yields more confident predictions than passive or single-fidelity active learning; BO finds optima in far fewer iterations than exhaustive search (example: optimum speed ratio found in 2–3 iterations vs exhaustive or grid search).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical examples: objective found in ~2–3 HF measurements; function approximations improved substantially with only a few HF observations (n on the order of 4–6) when guided by active learning vs requiring >10 HF points in naive passive sampling for similar accuracy in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses exploration vs exploitation tradeoff: pure variance acquisition provides rapid coverage (useful when mapping unknown functions) but does not directly optimize objectives; BO with combined mean/variance accelerates objective discovery. The authors also discuss tradeoffs between relying on cheap LF predictions (bias risk) and spending HF budget to correct LF model errors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For mapping tasks, a variance-driven exploration strategy plus physics-informed priors (multi-fidelity) yields the best allocation of limited HF budget; for optimization tasks BO-style acquisition rapidly allocates budget to promising regions and converges to optima in few experiments. Combining multi-fidelity surrogates with uncertainty-based acquisition is recommended to minimize HF experiments while maintaining accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2487.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2487.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parallelized Machine Vision Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallelized Edge- and Object-based Computer Vision Jet Metrology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational workflow for real-time extraction of high-fidelity jet features from high-frame-rate video that uses task-wise profiling, multi-threading for I/O-bound tasks and multiprocessing for CPU-bound tasks to meet strict per-frame latency budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parallelized Machine Vision Workflow (edge- and object-based feature extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The vision module decomposes frame processing into tasks (read frame, color invert, edge-based feature extraction, object-based detection, show/save output). Profiling identified CPU-bound tasks (edge-based feature extraction) and I/O-bound tasks; multi-threading is applied to I/O-bound tasks and multiprocessing to CPU-bound tasks to exploit multiple CPU cores; this reduced per-frame processing time from ~0.033 s (sequential) to ~0.014 s (parallel), meeting the real-time requirement imposed by 50 fps camera input (0.02 s/frame). The vision output provides the high-fidelity observations (jet radius profiles and lag distances) that are the costly experimental measurements in the active learning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Real-time experimental metrology for additive manufacturing and any experiment requiring in-situ video-based feature extraction under tight latency budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Computational resources (threads/processes, CPU cores) are allocated per task according to their bound type: I/O-bound tasks use multi-threading to hide I/O latency; CPU-bound tasks use multiprocessing to distribute workload across cores. Stride parameter (sampling density along z-axis) trades computational cost vs resolution of extracted features.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock processing time per frame (seconds per frame) and CPU core count available; also a stride parameter that changes per-frame compute by changing number of processed rows.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Higher spatial sampling (smaller stride) yields more detailed feature vectors (more informative HF observations), evaluated indirectly by downstream GP predictive performance (RMSE, CI width).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not an active experimental allocation mechanism per se, but the stride and region-of-interest cropping settings are design knobs that trade computational cost for measurement information; users can choose denser sampling where model uncertainty is high (implied integration with active learning loop though not fully automated in current work).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Real-time per-frame latency budget (must be <= frame interval, 0.02 s), and overall computation resource constraints (number of CPU cores).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Profiling and task-specific parallelization to meet latency; cropping region-of-interest and adjusting stride reduce computational load; multiprocessing limited by number of physical CPU cores.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Processing time per frame: sequential 0.033 s -> concurrent (multithreading) modest improvement (~0.028 s) -> parallel (multithreading + multiprocessing) 0.014 s per frame. Achieved real-time performance below camera inter-frame interval (0.02 s).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Sequential (single-thread) implementation and concurrent multithreaded implementation used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Parallel implementation reduced per-frame latency by ~57.6% relative to sequential processing (0.033 -> 0.014 s).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Per-frame processing time reduction of ~57.6%; enables real-time HF observation acquisition at 50 fps. Also, using stride and ROI cropping reduces computational cost at the expense of spatial resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Tradeoff between spatial/temporal resolution of extracted features and per-frame processing latency; aggressive parallelization improves latency but is bounded by available CPU cores; stride/ROI are tunable to balance information per frame vs processing cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>To meet real-time constraints while maximizing information per frame: profile tasks, apply multi-threading to I/O-bound tasks and multiprocessing to CPU-bound tasks, and tune stride/ROI to allocate computation to the most informative spatial rows. Implicit recommendation to integrate stride selection with active learning uncertainty to prioritize resolution where the model needs information most.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On-the-fly closed-loop materials discovery via Bayesian active learning <em>(Rating: 2)</em></li>
                <li>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling <em>(Rating: 2)</em></li>
                <li>A fast multifidelity method with uncertainty quantification for complex data correlations: Application to vortex-induced vibrations of marine risers <em>(Rating: 2)</em></li>
                <li>Gaussian processes for autonomous data acquisition at large-scale synchrotron and neutron facilities <em>(Rating: 2)</em></li>
                <li>Autonomous experimentation systems for materials development: A community perspective <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2487",
    "paper_id": "paper-248266468",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "GPJet",
            "name_full": "GPJet: Physics-Informed Bayesian Learning Pipeline for E-jet Printing",
            "brief_description": "An end-to-end physics-informed probabilistic machine learning framework integrating real-time machine vision, a fast physics-based multiphysics model, and Gaussian process-based ML (single- and multi-fidelity) to actively learn and autonomously calibrate electrohydrodynamic jet printing (MEW) processes while minimizing expensive high-fidelity experiments and computational cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPJet",
            "system_description": "GPJet is a closed-loop pipeline composed of three tightly integrated modules: (a) a parallelized Machine Vision module that extracts high-fidelity jet features (jet radius profile, lag distance, velocities, angles) from video frames in real time; (b) a Physics-based Modeling module providing low-fidelity, cheap multi-physics predictions (1D thin-filament electrohydrodynamics + geometric 'fluid-mechanical sewing machine' model) of the same engineered features; and (c) a Machine Learning module that uses Gaussian Process Regression (single- and multi-fidelity formulations) together with active learning / Bayesian optimization acquisition rules to decide which additional (expensive) high-fidelity observations to acquire in order to learn the mapping from inputs (e.g., tip-to-collector position, speed ratio) to outputs (jet radius, lag distance). The pipeline deploys uncertainty-aware acquisition functions (pure-exploration variance-based and an exploration–exploitation BO strategy) and multi-fidelity kernels (linear autoregressive model between fidelities) to trade cheap low-fidelity evaluations for fewer costly high-fidelity experiments.",
            "application_domain": "Additive manufacturing / experimental calibration of electrohydrodynamic jet printing (melt electrowriting); generally applicable to experimental materials-process optimization and autonomous calibration of manufacturing processes.",
            "resource_allocation_strategy": "Resource allocation is driven by an uncertainty-aware active learning loop: (1) build/fit GP(s) to currently available data (high-fidelity observations from machine vision optionally augmented by low-fidelity physics model outputs); (2) compute acquisition function over candidate inputs (points along jet length for radius, or speed-ratio for lag distance); (3) select next measurement(s) by maximizing the acquisition (pure-exploration using posterior variance for fast coverage or acquisition functions that trade mean and variance in Bayesian optimization for finding optima); (4) perform the (costly) high-fidelity measurement via machine vision at selected input(s) and update the models. Multi-fidelity GPR is used to allocate budget away from high-cost experiments by leveraging many cheap low-fidelity model evaluations and a small number of high-fidelity experiments, and parallelized vision processing is used to satisfy real-time computational budgets.",
            "computational_cost_metric": "Measured in wall-clock processing time per video frame for machine vision (seconds per frame; e.g., sequential 0.033 s/frame, concurrent small improvement, parallel implementation 0.014 s/frame), and in number of high-fidelity observations (count of expensive experimental measurements); also implicitly number of iterations and total active-learning wall-clock time (e.g., ~0.5 s per iteration, total ~3 s for an active learning run reported).",
            "information_gain_metric": "Posterior variance (GP predictive variance) used directly as an exploration acquisition; predictive posterior mean and variance used for acquisition functions; model error is tracked with RMSE against held-out experimental ground truth; uncertainty quantified with 95% confidence intervals and Mean Confidence Interval Width (MCIW); Bayesian Optimization performance measured with Minimum Regret metric.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Two acquisition modes are used: (a) pure exploration: acquisition ∝ posterior variance (select points with highest predictive uncertainty) for fast function-space coverage; (b) exploration–exploitation (Bayesian Optimization) when optimizing an objective (e.g., minimize lag distance): acquisition functions that depend on both GP mean and variance (the paper describes the BO 'spirit' and uses acquisition that trades mean and variance; specifics of e.g. EI are not enumerated, but the procedure maximizes an acquisition balancing uncertainty and predicted performance). Initial points are seeded randomly or from user-specified initial training points; iterations continue until termination criteria (max iterations, uncertainty threshold, or target reached).",
            "diversity_mechanism": "No explicit diversity-promoting heuristic is implemented beyond uncertainty-driven sampling; exploration via posterior variance implicitly encourages sampling across diverse/high-uncertainty regions, but there is no separate explicit diversity-regularizer or batch-diversification mechanism described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed/limited number of expensive high-fidelity experiments (counts), real-time per-frame processing time (wall-clock), and maximum iteration/termination criteria (iteration budget or uncertainty thresholds).",
            "budget_constraint_handling": "Handles budget constraints by (1) multi-fidelity modeling that substitutes many cheap low-fidelity simulations for a reduced set of expensive high-fidelity measurements, (2) uncertainty-driven active learning to focus the limited high-fidelity budget where it reduces predictive uncertainty most (pure exploration) or where objective improvement is likely (BO), and (3) parallelization (multi-threading + multiprocessing) of vision tasks to meet real-time per-frame computation budgets.",
            "breakthrough_discovery_metric": "No general 'breakthrough' metric is defined; in application-specific tasks, breakthroughs are framed as achieving optimization objectives (e.g., minimum lag distance) or accurate function approximation with minimal high-fidelity data. Performance in finding optima is quantified by Minimum Regret metric; quality of learned models is measured by RMSE and confidence interval widths.",
            "performance_metrics": "Reported metrics include: machine vision processing time per frame (sequential ~0.033 s/frame -&gt; parallel ~0.014 s/frame), GP predictive accuracy metrics (RMSE reported in supplementary figures but specific numbers not in main text), confidence quantified by 95% CI and Mean Confidence Interval Width (MCIW), active-learning iteration time ~0.5 s and total learning time ~3 s for reported scenarios, number of high-fidelity observations used in case studies (examples: passive GP fit with n=10 high-fidelity observations; multi-fidelity GPR scenarios with n=6 or n=7 high-fidelity observations plus 32 low-fidelity points), BO convergence to near-optimal speed-ratio in 2–3 iterations in the virtual MEW experiments.",
            "comparison_baseline": "Simple single-fidelity Gaussian Process Regression (passive learning using only high-fidelity data), offline Design-of-Experiments (exhaustive grid search / Cartesian grid DoE referenced in introduction), and random initial sampling for active scenarios are used as baselines or comparisons.",
            "performance_vs_baseline": "Multi-fidelity GPR + active learning outperforms single-fidelity GP: achieves comparable or better predictive accuracy using fewer high-fidelity observations (example: multi-fidelity with n=6 HF + 32 LF improves over single-fidelity GP with the same number of HF points). Active learning on the multi-fidelity model is reported as 'significantly faster' with tighter confidence intervals than the simple GP active learning; BO finds the minimum-lag speed ratio in 2–3 iterations compared to slower passive calibration or grid search. Exact numeric RMSE reductions vs baseline are reported in supplementary figures (not all values tabulated in main text).",
            "efficiency_gain": "Concrete reported improvements include: vision pipeline wall-clock reduction ≈57.6% (0.033 s -&gt; 0.014 s per frame); reduction in required high-fidelity observations (demonstrated reduction from n=10 HF to n=6–7 HF when informed by low-fidelity physics while retaining predictive accuracy—≈30–40% fewer expensive measurements in those cases); active-learning wall-clock learning in seconds (~3 s) for the small virtual experiments. The paper describes 'significantly faster' learning and 'more confident' predictions for MFD vs single-fidelity but does not give a single combined efficiency percentage.",
            "tradeoff_analysis": "The paper analyzes several tradeoffs: (1) Data quantity and placement vs predictive accuracy: more high-fidelity observations and their locations reduce RMSE and uncertainty (shown in comparisons n=5 vs n=10 and n=4 vs n=12 cases). (2) Cheap low-fidelity evaluations vs expensive high-fidelity experiments: introducing physics-based low-fidelity data via multi-fidelity GPR reduces the number of costly high-fidelity observations required. (3) Computational processing time vs information throughput: parallelization/multiprocessing trades hardware resources for reduced per-frame latency to meet real-time constraints. (4) Exploration vs exploitation: pure exploration (variance acquisition) accelerates global function learning, whereas BO balances exploitation to find optima quickly. The results show that including physics priors (multi-fidelity) tightens priors and reduces required experimental budget, but low-fidelity models can underpredict in some regions (Taylor cone), requiring some targeted high-fidelity sampling there.",
            "optimal_allocation_findings": "Key conclusions: (a) Physics-informed priors via multi-fidelity modeling are effective at reallocating experimental budget away from many expensive high-fidelity measurements toward many cheap low-fidelity evaluations plus a small set of strategically chosen high-fidelity points; (b) uncertainty-driven acquisition (posterior variance) quickly guides expensive measurements to regions of greatest information value, improving learning speed; (c) for optimization tasks (lag-distance minimization) BO rapidly finds near-optimal operating points (speed ratio ≈1) in very few experiments; (d) computational parallelization is necessary to meet real-time sensory budgets and thus constrains experimental allocation decisions (must choose experiments compatible with processing latency). Recommendations: combine multi-fidelity modeling, uncertainty-aware acquisition, and computational parallelization to optimize the tradeoff between experimental cost and information gain.",
            "uuid": "e2487.0",
            "source_info": {
                "paper_title": "Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Multi-fidelity GPR",
            "name_full": "Multi-fidelity Gaussian Process Regression (autoregressive/recursive formulation)",
            "brief_description": "A probabilistic surrogate modeling method that combines cheap low-fidelity model outputs and sparse expensive high-fidelity observations in a hierarchical GP framework (linear autoregressive relationship) to produce improved predictions and uncertainty quantification while reducing the required number of costly experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Multi-fidelity Gaussian Process Regression (recursive AR1)",
            "system_description": "The paper uses a two-level multi-fidelity GP where the high-fidelity function is modeled as a linear scaling of the low-fidelity function plus a GP-modeled discrepancy term (h_H(x) = rho * h_L(x) + delta(x)). A numerically efficient recursive inference scheme is used with nested training sets (higher-fidelity inputs are a subset of lower-fidelity inputs). The approach yields predictive posterior mean and variance that incorporate both data sources, enabling uncertainty-aware active learning and acquisition that considers both fidelities.",
            "application_domain": "Surrogate modeling for experimental design and learning of manufacturing/process physics (jet radius profile prediction across nozzle-to-collector distance), general materials and multiphysics modeling applications.",
            "resource_allocation_strategy": "Allocates experimental budget by leveraging many low-cost low-fidelity model evaluations (32 LF points in tests) and selecting a small set of high-fidelity observations (e.g., n=6 or n=7) chosen by active learning; training inputs are nested so LF grid can guide HF selection. The multi-fidelity kernel encodes correlation (scaling factor) so model effectively uses LF information to reduce HF sampling.",
            "computational_cost_metric": "Counts of low-fidelity model evaluations (cheap numerical ODE solves) vs counts of high-fidelity experimental observations; and wall-clock per-evaluation cost (LF much cheaper than HF video acquisition+processing).",
            "information_gain_metric": "GP posterior variance and predictive improvement when adding HF points; RMSE and MCIW are used to evaluate information gained per HF experiment.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition driven by GP posterior variance for pure exploration; in BO scenarios the multi-fidelity GP provides predictive mean and variance used in acquisition that balances exploration and exploitation. The paper uses the multi-fidelity posterior to select HF measurements that maximize information (uncertainty reduction) per cost.",
            "diversity_mechanism": "No explicit diversity mechanism beyond sampling at high-uncertainty regions; nested LF grid combined with variance-based acquisition implicitly spreads HF samples to informative diverse regions.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Limited budget of high-fidelity experimental observations (count), unlimited/cheap LF evaluations.",
            "budget_constraint_handling": "Optimizes under budget by using a fixed dense LF grid and actively selecting a small number of HF sample locations that most reduce HF predictive uncertainty using the MF-GP posterior; nested training structure allows efficient recursive inference.",
            "breakthrough_discovery_metric": "Not defined — task-specific objectives (e.g., accurate radius profile or minimized lag distance) are used as practical high-impact outcomes.",
            "performance_metrics": "Case examples: multi-fidelity fits with n=6 HF + 32 LF or n=7 HF + 32 LF achieved better predictive accuracy and narrower uncertainty than single-fidelity GP fitted with the same number of HF points; particular failure region noted in Taylor cone where LF model underpredicts and additional HF point improved fit.",
            "comparison_baseline": "Compared against single-fidelity (simple) Gaussian Process Regression trained on HF data only with same HF counts.",
            "performance_vs_baseline": "Multi-fidelity GPR gives more accurate predictions and tighter confidence intervals for the same HF budget; active learning with MFD converges faster (lower RMSE and MCIW) than simple GP (detailed numeric comparisons in Supplementary Information).",
            "efficiency_gain": "Demonstrated reduction in required HF observations for comparable accuracy (examples: similar performance with n=6–7 HF with MFD vs n≈10 HF for single-fidelity GP in certain radius-profile tasks; this implies ~30–40% fewer expensive HF measurements in reported cases).",
            "tradeoff_analysis": "Authors highlight that LF physics models can underperform in some localized regions (Taylor cone), so blindly relying on LF can bias predictions; thus MF strategy must include HF sampling in areas where LF discrepancy is expected. The analysis shows LF data constrains priors and reduces HF requirements but requires careful placement of the remaining HF budget.",
            "optimal_allocation_findings": "Best allocation is to use a dense LF grid to capture large-scale trends and then allocate a small number of HF experiments targeted (via uncertainty acquisition) to regions where LF models are less reliable or uncertainty remains high, yielding efficient reduction in HF experimental cost while preserving accuracy.",
            "uuid": "e2487.1",
            "source_info": {
                "paper_title": "Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Active Learning / BO (GP-driven)",
            "name_full": "Uncertainty-aware Active Learning and Bayesian Optimization using Gaussian Processes",
            "brief_description": "An active data acquisition and optimization strategy that uses GP predictive mean and variance to form acquisition functions (pure-exploration variance-based and BO-style exploration–exploitation) to sequentially select costly experiments that maximize information gain or achieve optimization objectives (e.g., minimize lag distance).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GP-driven Active Learning and Bayesian Optimization",
            "system_description": "The paper uses Gaussian Processes as surrogates and defines acquisition functions that depend on the GP predictive mean and variance. For function approximation/exploration tasks they use a purely exploratory acquisition (maximize posterior variance) to select next measurement points along the jet length; for objective optimization (minimize lag distance) they use a BO loop that balances exploration and exploitation (acquisitions that consider predicted mean and uncertainty) until termination criteria (max iterations or uncertainty threshold) are met. Acquisition maximization is performed at each iteration to pick the next experimental setting, which is then measured by the machine vision module and used to update the GP.",
            "application_domain": "Experimental design and optimization in additive manufacturing process calibration (MEW); generic for autonomous experimentation and materials discovery workflows.",
            "resource_allocation_strategy": "Sequential allocation of experimental budget to inputs that maximize acquisition value: for exploration tasks, select inputs with highest predictive uncertainty (variance) to maximize information gain per experiment; for optimization tasks, select inputs balancing predicted improvement and uncertainty to quickly find minima (or maxima). Initial points are seeded (random or user-defined); termination is by iteration budget or uncertainty/objective thresholds.",
            "computational_cost_metric": "Per-iteration wall-clock time (GP fit + acquisition evaluation + measurement), reported iteration time ~0.5 s in virtual runs; total active learning wall-clock time reported ~3 s for example runs. Also HF experiment count is a cost metric.",
            "information_gain_metric": "Posterior variance (used explicitly), RMSE reduction over iterations, Mean Confidence Interval Width (MCIW) as an uncertainty metric, and Minimum Regret when optimizing an objective.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Two concrete mechanisms: (1) pure-exploration acquisition = maximize GP posterior variance (sampling uncertain regions for diversity/coverage), (2) BO-style acquisition that depends on GP mean and variance to balance exploiting regions predicted to be optimal and exploring uncertain regions. The choice of acquisition is task-dependent: pure exploration for accurate function mapping, BO for finding optima (lag-distance).",
            "diversity_mechanism": "Implicit diversity arises from variance-driven selection which favors unexplored/high-uncertainty regions; no explicit additional diversity penalty or batch diversity algorithm is described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed iteration budget or HF sample budget, and real-time computational budget for per-iteration processing.",
            "budget_constraint_handling": "Active learning focuses HF experiments on maximal information/utility locations; uses early stopping or termination criteria (max iterations, uncertainty threshold, or objective convergence) to respect budgets.",
            "breakthrough_discovery_metric": "Operationalized as successful optimization outcomes (e.g., reaching minimal lag distance) or rapid function approximation with few HF points; performance measured via Minimum Regret for optimization tasks.",
            "performance_metrics": "Reported: active-learning iterations last ~0.5 s; total learning runs ~3 s; active learning reached accurate function approximations in a few iterations for both radius profile and lag-distance tasks; BO reached near-optimal speed ratio (≈1) in 2–3 iterations in the virtual experiments. RMSE and MCIW decreased over iterations (plotted in supplementary figures).",
            "comparison_baseline": "Compared against passive learning (fitting GP to a fixed HF dataset), single-fidelity GP active learning, and offline grid/DoE approaches referenced in introduction.",
            "performance_vs_baseline": "Active learning with multi-fidelity models converges faster and yields more confident predictions than passive or single-fidelity active learning; BO finds optima in far fewer iterations than exhaustive search (example: optimum speed ratio found in 2–3 iterations vs exhaustive or grid search).",
            "efficiency_gain": "Empirical examples: objective found in ~2–3 HF measurements; function approximations improved substantially with only a few HF observations (n on the order of 4–6) when guided by active learning vs requiring &gt;10 HF points in naive passive sampling for similar accuracy in some tasks.",
            "tradeoff_analysis": "Paper discusses exploration vs exploitation tradeoff: pure variance acquisition provides rapid coverage (useful when mapping unknown functions) but does not directly optimize objectives; BO with combined mean/variance accelerates objective discovery. The authors also discuss tradeoffs between relying on cheap LF predictions (bias risk) and spending HF budget to correct LF model errors.",
            "optimal_allocation_findings": "For mapping tasks, a variance-driven exploration strategy plus physics-informed priors (multi-fidelity) yields the best allocation of limited HF budget; for optimization tasks BO-style acquisition rapidly allocates budget to promising regions and converges to optima in few experiments. Combining multi-fidelity surrogates with uncertainty-based acquisition is recommended to minimize HF experiments while maintaining accuracy.",
            "uuid": "e2487.2",
            "source_info": {
                "paper_title": "Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Parallelized Machine Vision Workflow",
            "name_full": "Parallelized Edge- and Object-based Computer Vision Jet Metrology",
            "brief_description": "A computational workflow for real-time extraction of high-fidelity jet features from high-frame-rate video that uses task-wise profiling, multi-threading for I/O-bound tasks and multiprocessing for CPU-bound tasks to meet strict per-frame latency budgets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Parallelized Machine Vision Workflow (edge- and object-based feature extraction)",
            "system_description": "The vision module decomposes frame processing into tasks (read frame, color invert, edge-based feature extraction, object-based detection, show/save output). Profiling identified CPU-bound tasks (edge-based feature extraction) and I/O-bound tasks; multi-threading is applied to I/O-bound tasks and multiprocessing to CPU-bound tasks to exploit multiple CPU cores; this reduced per-frame processing time from ~0.033 s (sequential) to ~0.014 s (parallel), meeting the real-time requirement imposed by 50 fps camera input (0.02 s/frame). The vision output provides the high-fidelity observations (jet radius profiles and lag distances) that are the costly experimental measurements in the active learning loop.",
            "application_domain": "Real-time experimental metrology for additive manufacturing and any experiment requiring in-situ video-based feature extraction under tight latency budgets.",
            "resource_allocation_strategy": "Computational resources (threads/processes, CPU cores) are allocated per task according to their bound type: I/O-bound tasks use multi-threading to hide I/O latency; CPU-bound tasks use multiprocessing to distribute workload across cores. Stride parameter (sampling density along z-axis) trades computational cost vs resolution of extracted features.",
            "computational_cost_metric": "Wall-clock processing time per frame (seconds per frame) and CPU core count available; also a stride parameter that changes per-frame compute by changing number of processed rows.",
            "information_gain_metric": "Higher spatial sampling (smaller stride) yields more detailed feature vectors (more informative HF observations), evaluated indirectly by downstream GP predictive performance (RMSE, CI width).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Not an active experimental allocation mechanism per se, but the stride and region-of-interest cropping settings are design knobs that trade computational cost for measurement information; users can choose denser sampling where model uncertainty is high (implied integration with active learning loop though not fully automated in current work).",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Real-time per-frame latency budget (must be &lt;= frame interval, 0.02 s), and overall computation resource constraints (number of CPU cores).",
            "budget_constraint_handling": "Profiling and task-specific parallelization to meet latency; cropping region-of-interest and adjusting stride reduce computational load; multiprocessing limited by number of physical CPU cores.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Processing time per frame: sequential 0.033 s -&gt; concurrent (multithreading) modest improvement (~0.028 s) -&gt; parallel (multithreading + multiprocessing) 0.014 s per frame. Achieved real-time performance below camera inter-frame interval (0.02 s).",
            "comparison_baseline": "Sequential (single-thread) implementation and concurrent multithreaded implementation used as baselines.",
            "performance_vs_baseline": "Parallel implementation reduced per-frame latency by ~57.6% relative to sequential processing (0.033 -&gt; 0.014 s).",
            "efficiency_gain": "Per-frame processing time reduction of ~57.6%; enables real-time HF observation acquisition at 50 fps. Also, using stride and ROI cropping reduces computational cost at the expense of spatial resolution.",
            "tradeoff_analysis": "Tradeoff between spatial/temporal resolution of extracted features and per-frame processing latency; aggressive parallelization improves latency but is bounded by available CPU cores; stride/ROI are tunable to balance information per frame vs processing cost.",
            "optimal_allocation_findings": "To meet real-time constraints while maximizing information per frame: profile tasks, apply multi-threading to I/O-bound tasks and multiprocessing to CPU-bound tasks, and tune stride/ROI to allocate computation to the most informative spatial rows. Implicit recommendation to integrate stride selection with active learning uncertainty to prioritize resolution where the model needs information most.",
            "uuid": "e2487.3",
            "source_info": {
                "paper_title": "Physics-Informed Bayesian learning of electrohydrodynamic polymer jet printing dynamics",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On-the-fly closed-loop materials discovery via Bayesian active learning",
            "rating": 2,
            "sanitized_title": "onthefly_closedloop_materials_discovery_via_bayesian_active_learning"
        },
        {
            "paper_title": "Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling",
            "rating": 2,
            "sanitized_title": "nonlinear_information_fusion_algorithms_for_dataefficient_multifidelity_modelling"
        },
        {
            "paper_title": "A fast multifidelity method with uncertainty quantification for complex data correlations: Application to vortex-induced vibrations of marine risers",
            "rating": 2,
            "sanitized_title": "a_fast_multifidelity_method_with_uncertainty_quantification_for_complex_data_correlations_application_to_vortexinduced_vibrations_of_marine_risers"
        },
        {
            "paper_title": "Gaussian processes for autonomous data acquisition at large-scale synchrotron and neutron facilities",
            "rating": 2,
            "sanitized_title": "gaussian_processes_for_autonomous_data_acquisition_at_largescale_synchrotron_and_neutron_facilities"
        },
        {
            "paper_title": "Autonomous experimentation systems for materials development: A community perspective",
            "rating": 1,
            "sanitized_title": "autonomous_experimentation_systems_for_materials_development_a_community_perspective"
        }
    ],
    "cost": 0.019782249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Physics-Informed Bayesian Learning of Electrohydrodynamic Polymer Jet Printing Dynamics</p>
<p>Athanasios Oikonomou 
Mechanical Engineering
University of Patras
PatrasGreece</p>
<p>National Centre for Scientific Research Demokritos
Agia Paraskevi1534AtticaGreece</p>
<p>Superlabs ΑΜΚΕ
15124Marousi, AtticaGreece</p>
<p>Theodoros Loutas 
Mechanical Engineering
University of Patras
PatrasGreece</p>
<p>Dixia Fan 
Westlake University
HangzhouChina</p>
<p>Alysia Garmulewicz 
Faculty of Economics and Administration
University of Santiago
SantiagoChile</p>
<p>George Nounesis 
National Centre for Scientific Research Demokritos
Agia Paraskevi1534AtticaGreece</p>
<p>Santanu Chaudhuri 
Civil, Materials, and Environmental Engineering Department
University of Illinois at Chicago
60607ILUnited States</p>
<p>Argonne National Laboratory
60439LemontILUnited States</p>
<p>Filippos Tourlomousis 
National Centre for Scientific Research Demokritos
Agia Paraskevi1534AtticaGreece</p>
<p>Superlabs ΑΜΚΕ
15124Marousi, AtticaGreece</p>
<p>Biological Lattice Industries Corp
02108BostonMAUnited States</p>
<p>Physics-Informed Bayesian Learning of Electrohydrodynamic Polymer Jet Printing Dynamics
1E0D0C8D248D20444B6395554A7B8E19
Calibration of highly dynamic multi-physics manufacturing processes such as electrohydrodynamics-based additive manufacturing (AM) technologies (E-jet printing) is still performed by labor-intensive trial-and-error practices.These practices have hindered the broad adoption of these technologies, demanding a new paradigm of self-calibrating E-jet printing machines.To address this need, we developed GPJet, an end-to-end physics-informed Bayesian learning framework, and tested it on a virtual E-jet printing machine with in-process jet monitoring capabilities.GPJet consists of three modules: a) the Machine Vision module, b) the Physics-Based Modeling Module, and c) the Machine Learning (ML) module.We demonstrate that the Machine Vision module can extract high-fidelity jet features in real-time from video data using an automated parallelized computer vision workflow.In addition, we show that the Machine Vision module, combined with the Physics-based modeling module, can act as closed-loop sensory feedback to the Machine Learning module of high-and low-fidelity data.Powered by our datacentric approach, we demonstrate that the online ML planner can actively learn the jet process dynamics using video and physics with minimum experimental cost.GPJet brings us one step closer to realizing the vision of intelligent AM machines that can efficiently search complex process-structure-property landscapes and create optimized material solutions for a wide range of applications at a fraction of the cost and speed.</p>
<p>Introduction</p>
<p>The programmable assembly of functional inks in two-and three-dimensions using computer numerically controlled (CNC) machines coupled with printing technologies has revolutionized the design and fabrication of physical objects.Extrusion-based additive manufacturing (AM) technologies, often referred to as direct ink writing or 3D printing, are transforming fields such as healthcare, robotics, electronics, and sustainability 1,2 .While the potential of 3D printing is celebrated very often in scientific journals and the media, there is a "secret" that practitioners and companies of 3D printing do not stress out.This under-reported reality entails the extensive experimentation and manual labor required for tuning process parameters that are high in number and often inter-dependent, to achieve process stability and reproducible outcomes 3 .Every time a new material needs to be processed, or ambient conditions vary, practitioners follow trial and error approaches for printing process calibration.These calibration practices have led to the creation of experienced "super users" at the expense of an enormous degree of individual process engineering.</p>
<p>Electro-hydrodynamics-based AM technologies, also known as E-jet printing technologies, are notable examples of extrusion-based AM technologies that have been facing such challenges due to their complex multi-physics and highly dynamic nature 4,5 .During E-jet printing, a polymer solution or a polymer melt is extruded through a charged needle tip towards a grounded collector.As soon as the electrostatic stresses overcome the polymer material's viscoelastic and surface tension stresses, a cone-jet is formed in the free flow regime (Figure 1a).An instabilities area, whose length depends on the nature of the polymer, follows the cone-jet regime.Focusing on the polymer melt case, where the instabilities area is closer to the collector (Figure 1a), a translational stage can be employed to write high-resolution fibers (Figure 1b), a process known as melt electrowriting (MEW).With this capability, MEW has been established as an emerging high-resolution AM technology for fabricating architected biomaterial scaffolds, opening new tissue engineering avenues.MEW has undergone ten years of process optimization studies since its first inception in the literature 6,7 .Tunable fiber diameter and patterning fidelity are critical scaffold attributes for biological outcomes and efficacy.These can be optimized by tuning five inter-dependent usercontrolled process parameters assuming stable ambient environmental conditions (temperature and humidity): a) the applied voltage at the needle tip, b) the extrusion volumetric flowrate, c) the temperature at the syringe, d) the collector speed, and needle tip to collector distance.Considering the dynamic range of each process variable in combination with the highly sensitive spatial and time scales of the process in the micron range, one quickly realizes why it took 10 years for process optimization with the vast majority of these studies using one specific material i.e. polycaprolactone (PCL).</p>
<p>Earlier studies achieved printing fidelity with MEW using an approach based on intuition, i.e., manually selecting values for the critical process parameters, performing post-printing fidelity measurements, assessing trends and patterns in data, and selecting process parameter settings for follow-up experimentation.Later studies focused on understanding the previously identified printing regimes with respect to the physics and the dynamics of the process [8][9][10] .A recent study systematically approached the calibration process by exploring the parameter space using a Design-of-Experiments approach in a simple Cartesian grid defined by the number of independent process parameters 11 .In this study, computer vision was employed to image the jet in the freeflow regime as a function of various process parameter conditions in a high throughput manner 11 .The generated dataset was then assessed offline to identify high fidelity printability regimes 11 .However, selecting an exploration strategy implies picking a resolution without knowing the model function.To address that, the resolution is often chosen high, aiming for an exhaustive search to avoid inaccuracies.With the high dimensionality of the parameter space, this brute force data collection method quickly fails to explore the space efficiently and becomes prone to bias.</p>
<p>The challenges mentioned before, combined with the demand for increasingly complex and reproducible products, warrant a new paradigm for E-jet printing machines.In this paradigm, rigid machines calibrated by trial-and-error practices are replaced by "intelligent" autonomous machines capable of adapting and learning process dynamics with minimum experimental cost.Artificial intelligence and Machine Learning (ML) are transforming many areas of experimental science in this direction.However, advances in manufacturing science are mainly driven by expensive physics-based simulations that cannot resolve all scales and, more recently, by data-hungry neural networks trained offline with in-process monitoring datasets for defect detection and process performance prediction on various AM platform technologies 12 .</p>
<p>To address these challenges, we adopt an approach inspired by the operating principles behind autonomous materials experimentation platforms, also known as research robots [13][14][15] and from the field of physics-informed machine learning 16,17 .Research robots demonstrate closed-loop control through online learning from prior experiments, planning and execution of new experiments.Physics-informed machine learning lays the foundations for integrating data with domain knowledge in the form of mathematical models to allow efficient simulations of highly multi- physics phenomena.The underlying framework of research robots provides a systematic datadriven approach for the identification of the best follow-up experiments to optimize unknown functions.The functions are approximated by Gaussian Process Regression (GPR), which is a robust statistical, nonparametric technique both for function approximation and uncertainty quantification 18,19 .During the Bayesian optimization loop, an acquisition function balances the utilization of experiments that explore the unknown function with experiments that exploit prior knowledge by considering the quantified uncertainty after each function approximation step 20 .Efficiency with respect to the utilization of experimental resources could be further improved by augmenting the surrogate model with prior domain knowledge following a multi-fidelity modeling approach [21][22][23] .The success of this approach has been documented in the field of computational science by using simple and potentially inaccurate models that carry a low computational cost to achieve predictive accuracy on a small set of high-fidelity observations obtained from accurate models that carry a high computational cost.</p>
<p>Automated materials experimentation systems driven by Bayesian optimization active learning frameworks have demonstrated remarkable performance in autonomously searching the vast synthesis-process-structure-property landscape resulting to the accelerated discovery of advanced materials for a wide variety of applications 20,[24][25][26][27] including AM 28 .However, their application not only towards the calibration of E-jet printing processes, but also in general AM technologies remains significantly underexploited.In one study concerning E-jet printing of substrates with micron-scale topographical features, the authors demonstrated a research robot, whose planner is informed by an in-line nano-surface metrology tool and actively learns to tune the extrusion rate until it achieves a predefined topographical feature 29 .In another study about direct ink writing of paste materials, the authors demonstrated an autonomous 3D printer, whose planner is informed by machine vision cameras and adaptively searches the space of four process parameters to print single struts with geometrical features that match user-defined specifications 30 .</p>
<p>In this paper, we develop and demonstrate GPJet, an end-to-end physics-informed probabilistic machine learning framework that sets the basis for the next generation of self-calibrating E-jet printing machines.We construct a virtual MEW machine using a previously published video dataset acquired by a conventional camera that performs in situ jet monitoring under various process conditions, and we demonstrate that GPJet is capable of:</p>
<p>• high-fidelity jet feature extraction in real-time from video data using a parallelized computer vision algorithmic workflow that is systematically profiled under various implementations, • low-fidelity jet feature extraction from "cheap" physics-based models describing the evolution of the jet across the free-flow regime and the deposition dynamics of a gravity-driven viscous thread onto a moving surface known as the "fluid-mechanical sewing machine" and • learning the process dynamics with minimum experimental cost as described by the required number of high-fidelity data.This is supported by performance tests comparing offline and online calibration scenarios revealing that the online ML planner can effectively learn the jet evolution in the free-flow regime much more efficiently when it is informed by physics and based on that to adaptively tune the translational speed of the collector for minimum jet lag distance.In that case, the ML planner follows a decision-making strategy revealing the universality of the fluid mechanical sewing machine model in predicting the deposition dynamics of any printing process of viscous jets no matter what the nature of the jet driving force is.</p>
<p>This paper is organized as follows: First, we introduce GPJet, the physics-informed machine learning pipeline modules that we developed.Then, we describe the curated dataset we used to demonstrate the pipeline's utility and performance using MEW as a testbed.Third, we describe the results of our study, starting with the real-time jet feature extraction and how these are leveraged by the ML planner within GPJet for batch and online learning process dynamics using data and physics.Then, we discuss the advantages of GPJet as a platform whose utility can be generalized for any AM process and its current limitations.Lastly, we conclude with the current work in progress to realize fully autonomous closed-loop ML-driven E-jet printing manufacturing platforms.</p>
<p>GPJet: The Physics-Informed Machine Learning Pipeline</p>
<p>To demonstrate the ability of learning the dynamics of E-Jet printing processes in a data-driven fashion, we employ a pipeline-based approach that is depicted in Figure 2. The approach is composed of three modules, namely: the Machine Vision module, the Physics-based Modeling Module, and the Machine Learning Module.In GPJet, features that are representative of the printing process dynamics, are extracted by the Machine Vision module and the Physics-based modeling module.In the context of this paper, high-fidelity observations are referred to the jet features extracted experimentally, and low-fidelity observations are referred to the same jet features as predicted by a low-cost numerical model that is a good approximation of the reality.</p>
<p>As a first step, jet features are engineered and extracted in real-time using an algorithmic computer vision workflow taking as an input time-series video data (see Methods for details).The Machine Vision module allows us to probe and measure the jet dynamics, a capability hereafter denoted as jet metrology.The jet metrology serves as a feature extraction step of high-fidelity observations corresponding to the jet radius profile (  [mm]) and the jet lag distance (  [mm]), which are then fed into the Machine Learning module that can perform various Bayesian-based batch and online learning tasks (see Methods for details).The Machine Learning module can be further informed by low-fidelity observations, a capability hereafter denoted as Multi-fidelity modeling.The low-fidelity observations are obtained by the Physics-based modeling module and correspond to the same engineered features that are extracted experimentally by the Machine Vision module (  [mm] and (  [mm]).</p>
<p>Collectively, the GPJet pipeline offers a range of unique capabilities ranging from real-time feature extraction using computer vision to physics-informed machine learning capabilities that aim to minimize experimental cost without sacrificing accuracy and robustness.</p>
<p>Dataset</p>
<p>To demonstrate the utility and performance of the GPJet pipeline, we curated a dataset that emulates a virtual E-jet printing machine with a dynamic range of 12 user-controlled machine settings.The dataset is depicted in Table 1 and is created based on previously published timeseries video data 10 .Specifically, the raw data is acquired by a conventional camera with 50 fps and a field of view spanning the area between the needle tip and the grounded collector of a melt electro-writing (MEW) system.A detailed explanation of the raw data, the preprocessing procedure derive the final curated dataset can be found in Supplementary Note.MEW constitutes an ideal testbed for demonstrating the capabilities and the flexibility of our GPJet framework.The highly dynamic nature of the process and the multiple user-controlled independent process parameters, pose several challenges that we demonstrate both in an offline and an online selfcalibrating machine scenario.</p>
<p>Results</p>
<p>Learning Jet Dynamics from Videos</p>
<p>As a first goal we set out to tackle the challenge of real-time process monitoring and jet metrology.To demonstrate the highly dynamic nature of the process, we plot overlaid video frames showing the jet hitting a stationary collector (Figure 3a).We chose to plot frames with a time step equal to 0.2 sec since the electrostatic nature of the process and the viscoelasticity of the molten jet cause instabilities of a significantly smaller time scale (~0.02 sec) and result in jet topologies that are indistinguishable with a naked eye.This number provided a starting point for setting a goal related to the computational efficiency of the machine vision module for real-time performance.Since the camera acquisition time was equal to 0.02 sec (50 fps), we proceeded with the goal to maintain computational processing time equal or smaller than that.</p>
<p>To accomplish this, we started by dividing the computer vision workflow in specific algorithmic tasks and implemented a sequential code version.We continued by systematically profiling the code, identifying the computationally expensive tasks, and then gradually parallelizing the code to reduce computational processing time.This approach led to three different code implementations of the machine vision module: a) the sequential, b) the concurrent and the c) parallel, with the last one achieving real-time performance.The results of the profiling experiments are shown in Figure 3b, where all the tasks are plotted along with their respective processing time for the three different code implementations.</p>
<p>Specifically, the machine vision tasks per frame are the following:   Profiling the sequential code version reveals that an average time of 0.033 sec. is needed to perform the whole machine vision workflow per frame with the most expensive task being the one that performs edge-based feature extraction across the jet length (Figure 3c).To alleviate this source of computational cost, we employed a multi-threading strategy for the concurrent code version that led to a modest improvement of 0.005 sec.</p>
<p>Multi-threading is implementing software to perform two or more tasks in a concurrent manner within the same application.Multi-threading employs multiple threads to perform each task with no limitation in the number of threads that can be used [10].We learned that multithreading on one hand can reduce processing time of I/O bound tasks almost to zero, but on the other hand does not improve processing time of CPU bound tasks, such as Task 3 and Task 4, which are the most expensive.</p>
<p>To further reduce processing time, we augmented the concurrent version with a multiprocessing strategy that led to the parallel code version.Multi-processing systems have multiple processors running at the same time.Therefore, different tasks of an application can be run in different processors in a parallel manner.This capability considerably accelerates program performance.The limitation of this strategy is related to the fact that the number of processes that can be employed must be less or equal to the number of processors (CPU cores) of the device [10].Finally, by employing multi-threading for I/O bound tasks (Task 1, Task 5 and Task 6) and multiprocessing for CPU bound tasks (Task 3, Task 4), we were able to achieve real-time process monitoring and jet metrology with processing time up to 0.014 sec.</p>
<p>Instrumented with the capability to perform jet feature extraction in real-time, we then focused on quantifying process dynamics relevant features.With the edge-based feature extraction algorithm, which is described in detail in sub-section 6.2 under the Methods section, we were able to measure the jet diameter profile, the area of the whole jet, the angle between the vertical line that connects the nozzle tip with the collector, and different points across the length of the jet profile and finally the translational jet speed at different points across the length of the jet profile.The high content spatio-temporal results are plotted in Figure 1 of the Supplementary Information demonstrating the breadth of information of the machine vision module and the fact that the jet point right above the collector undergoes a highly fluctuating behavior that will directly affect the printing quality.</p>
<p>We present the jet metrology results for two distinct phases during the printing process in Figure 4ai-ii and Figure 4bi-ii focusing on the jet point right above the collector, hereafter denoted as point of interest.With the object-based feature extraction algorithm which is described in detail in sub-section 5.2 under the Methods section, we were able to detect key objects in the field of view such the needle tip, the Taylor cone, which is defined as the jet area between the needle tip outlet and the jet point 2Ro away from the needle tip, the remaining jet and the collector.In this way, we were able to measure the Lag distance, defined as the distance between the point of interest and the projection of the middle point of the nozzle tip outlet to the collector.All detected objects are denoted graphically in Figure 3d, which shows the video output after Task 4 during the computer vision workflow.As a next step, we asked how we could leverage the extracted features to learn the dynamics of the process in the most efficient data-driven way, with respect to both experimental and computational cost.To address this question, we developed several Bayesian learning techniques, Gaussian process regression (GPR) is a robust statistical, non-parametric technique for function approximation with kernel machines.GPR provides the important advantages of uncertainty quantification, the ability to perform well with small datasets and the capability to easily include domain-aware physics-based models in the deployed kernels.⁄ ) jet radius observation data (n=5) obtained from the computer vision metrology module of the GPJet framework at specific z axis coordinates along the normalized jet length (   ⁄ ).b) fitting normalized jet radius using a higher number of observation data (n=10) compared to the previous case (a).c) fitting lag distance (  ) observation data (n=3) obtained from the computer vision metrology module of the GPJet framework for specific speed ratios (  /  ).d) fitting lag distance using all available observation data (n=12).For non-normalized quantities units are in SI.Filled contours represent uncertainty bounds (95% confidence intervals (CIs)).</p>
<p>To learn how the jet radius profile evolves over the tip to collector distance, we chose radial basis functions (RBF) as the kernel approximator and performed GPR.We trained the model under two different scenarios with n=5 observations and n=10 observations chosen at equally spaced points along the jet length for the 1 st and 2 nd scenario, respectively.It is important to mention that the machine vision module provides n = 93 observations along the jet length.The results are shown in Figure 5a and Figure 5b for the two different training scenarios.GPs can approximate the jet radius profile evolution with just n = 10 observations showcasing the efficiency of our data-driven approach with respect to computational cost.</p>
<p>To learn the function describing the relationship between the Lag distance and the ratio of the collector speed over the jet speed at the point of interest, we employ the same modeling strategy as before.Similarly, we set up two different training scenarios with n=4 observations and n=12 observations, respectively.Please note here that the number of high-fidelity observations at our disposal is constrained by our previously published experimental dataset (see sub-section 5.1 under the Methods section), where videos were acquired only at 12 different speed ratio settings.The results are shown in Figure 5c and Figure 5d for the two different training scenarios.While in the 1 st training scenario, GPR provides a smooth function approximation, the prediction's error from the experimental ground truth quantified by the Root Mean Square Error (RMSE), is significantly higher compared to the 2 nd training scenario (see Figure 3-b-d</p>
<p>in Supplementary Information).</p>
<p>As a result, the function describing the relationship under question, is hard to approximate due to the high variability of the Lag distance caused by the jet instabilities close to the collector.</p>
<p>Collectively, our machine vision module informing the GPR capabilities of the machine learning module with high-fidelity observations demonstrates that we can learn the dynamics of the process.Specifically, GPJet demonstrates excellent performance with respect to the prediction of jet radius profile evolution for a small amount of high-fidelity observations n = 10.Furthermore, GPJet demonstrates very good performance for the available number of high-fidelity observations with respect to the Lag distance behavior at different collector speed settings.</p>
<p>Learning Jet Dynamics from Videos &amp; Physics</p>
<p>As a next step, we focused on exploring how we could further reduce the number of highfidelity observations without losing the predictive capability of GPR with respect to the jet radius profile evolution.To accomplish that, we augmented the high-fidelity observations obtained by the machine vision module with low-fidelity observations obtained in a principled manner by a multi-physics model.The multi-physics model captures the electro-hydrodynamics, the heat transfer and viscoelastic constitutive material behavior of the molten jet in 1D across the needle tip to collector distance.The mathematical formulation and numeric implementation of the model are described in detail in sub-section 5.3 under the Methods sections.</p>
<p>We set up our data-driven scheme with two fidelities corresponding to two different kernel machines integrated in one multi-fidelity kernel, in which the correlation between the two kernels is encoded as a linear relationship.In other words, we constrain the prior knowledge during GPR with physics-relevant knowledge, resulting to a physics-informed posterior prediction that requires much less high-fidelity observations.</p>
<p>We trained the multi-fidelity model under two different scenarios with n=6 high-fidelity observations and n=7 high-fidelity observations, respectively.For both scenarios the number of low-fidelity observations was kept to a number equal to 32 and equally spaced points across the jet length.For the 1 st scenario n=6 equally spaced points were chosen across the jet length depicted in the jet schematic of the Figure 6a (upper-left).The results are shown in Figure 6a-i and Figure 6a-ii.In Figure 6a-i, we plot the multi-fidelity GPR predictions for the low and high-fidelity observations respectively.In Figure 6a-ii, we plot the predictions of the multi-fidelity GPR in high-fidelity observations together with the predictions of a simple GP in high-fidelity observations.Both plots demonstrate that we can learn the jet radius profile much better using two different fidelities compared to using only one fidelity for the same number of high-fidelity observations.Our results, point out that we lose predictive accuracy for the Taylor cone area (below the needle tip outlet).This phenomenon was expected due to that the fact that similar behavior was observed when the multi-physics model was tested and informed the strategy of the 2 nd scenario, where we chose 7 high-fidelity observations with the additional point being in the Taylor cone area.The results are shown in Figure 6b-i and Figure 6b-ii demonstrating that we have managed to further reduce the required number of high-fidelity observations that need to be extracted by the machine vision module without compromising the predictive accuracy.</p>
<p>Active Learning of Jet Dynamics</p>
<p>Up to now, we demonstrated that GPJet, is a robust tool for passive learning of jet dynamics.By "passive", we mean that given a high-fidelity dataset provided by the Machine Vision module and augmented by low-fidelity data provided by the Physics-based module, the GPR capabilities of the Machine Learning module can model the function that mathematically represents the relation between the jet radius and the needle tip to collector distance.In addition to that, we employed the same strategy without low fidelity data, to model the function describing the highly dynamic relationship between the Lag distance and the ratio of the collector speed and the jet velocity at the point of interest.</p>
<p>In this section, we asked the questions of whether we could actively choose data points across jet length for which to observe the outputs to accurately model the underlying function describing the jet dynamics with respect to the extracted jet features.To accomplish that, we deploy a virtual MEW machine, whose dynamic range is defined by the available dataset, and we run simulation experiments to demonstrate if we can learn the underlying functions in an active manner as quickly and accurately as possible.</p>
<p>To accomplish that, we set up an exploration scenario, a set-up closely related to optimal experimental design scenarios as it equates to adaptively selecting the input spatial points across the jet length based on what is already known about the function describing the jet radius profile and where knowledge can be improved.We run active learning in both the multi-fidelity GP and simple GP for the jet radius profile evolution.The results are shown in Figure 7.To systematically, compare the performance of the two different models, we chose the same initial training points (Figure7a-i and Figure7b-i) and the same number of iterations during each training phase.For each iteration (Figure 7a(i-vi) and Figure 7b(i-vi)), we graphically show, on the processed video frame the adaptively selected point across the jet length and below that the modeling results.The adaptive selection is based on a purely exploratory acquisition function that steers the point selection towards the area of least knowledge quantified by the uncertainty output of the modeling step.The results demonstrate that we can learn actively and in a purely exploratory scenario accurately and fast the underlying function.Each iteration phase for the multi-fidelity (MFD) GPs and simple GPs lasts around ~ 0.5 seconds leading to a total learning time equal to 3 sec.Lastly, we extract performance metrics to compare the active learning between the multi-fidelity and simple GP model (see Figure 2-a-c in Supplementary Information).The results demonstrate that active learning on the MFD model is significantly faster (Figure 2-a-c) with more confident predictions since the model's prior assumptions are constrained by domain-aware data.</p>
<p>Then, we employ the same strategy to actively learn the function describing the relation between the Lag distance and the speed ratio (put symbol) in an exploration scenario.The results are shown in Figure 8.The virtual MEW machine performs remarkably well in the prescribed experimental simulation.It starts by randomly selecting one speed ratio equal to 5 (see Figure 8a) and after 4 additional iterations (see Figure 8-a-b-c-d), the underlying function is quite effectively approximated.Performance metrics (see Figure 3-b-d in Supplementary Information) demonstrate that the underlying function can be learned fast in an active manner ) and low fidelity model data obtained from the computer vision metrology module of the GPJet framework and from the multi-physics model, respectively, at specific z axis coordinates along the normalized jet length (   ⁄ ) ( i -vi denote the iterations of the active learning algorithm until it meets its termination criteria).B) exploring the design space using Active Learning to fit a Gaussian Process to normalized high fidelity observation data (red color) of jet radius (    ⁄ ) obtained from the computer vision metrology module of the GPJet framework at specific z axis coordinates along the normalized jet length (   ⁄ ) ( i -vi denote the iterations of the active learning algorithm until it meets its termination criteria).and provide predictions with higher confidence compared to the passive learning approach and  Performing Bayesian Optimization to find the minimum lag-distance (L j ) by fitting a Gaussian Process Model to lag distance (L j ) observation data obtained from the computer vision metrology module of the GPJet framework for specific speed ratios (U c /V jm ).a-c) Iterations of the Bayesian optimization algorithm until it meets termination criteria.d) For speed ratios less than one (U c /V jm &lt; 1) the process is unstable, no straight line is formed, instead the translated coiling, alternating loops, W patterns and meanders patterns are formed, therefore no lag distance (L j ) observation data can be obtained from the computer vision metrology module of the GPJet framework.</p>
<p>Finally, we set out to address the following question.Can the virtual MEW machine find the speed ratio corresponding to the minimum Lag distance in an autonomous way?Autonomy in this paper, refers to the machine's ability to self-drive measurements of an experiment.Some initial parameters, such as the parameters to explore and their corresponding ranges constrained by the dataset, is defined by the user a priori.Instead of us learning the relation between the Lag distance and the speed ratio and afterwards calibrating the machine hyperparameters, we aim to demonstrate a self-calibrating scenario.To achieve that we employ an exploitation-exploration stategy in the spirit of Bayesian Optimization (BO) Lookman (2019) .It is called exploration-exploitation as scenarios where the output of the underlying function must be optimized require us to both sample uncertain areas to acquire more knowledge about the function (exploration) as well as sampling input points that are likely to produce extremum outputs given the current knowledge of the function (exploitation).The virtual MEW machine performs remarkably well in the prescribed experimental simulation.It starts again by randomly selecting a speed ratio equal to (see Figure 9-a) and after 2 additional iterations (see Figure 9-a-c) the speed ratio corresponding to the minimum Lag distance has been reached.This speed ratio is close to 1, as expected from the mechanical sewing machine model, which is described in detail in sub-section 6.4 under the Methods section.BO validates the initial hypothesis formed by universality about the mechanical sewing machine model.</p>
<p>Conclusions</p>
<p>In this work, we demonstrate GPJet, an end-to-end physics-informed probabilistic machine learning framework that sets the basis for the next generation of self-calibrating E-jet printing machines.We construct a virtual MEW machine using a previously published video dataset acquired by a conventional camera that performs in situ jet monitoring under various process conditions, and we demonstrate that GPJet is capable of: • high-fidelity jet feature extraction in real-time from video data using a parallelized computer vision algorithmic workflow that is systematically profiled under various implementations, • low-fidelity jet feature extraction from fast physics-based models describing the evolution of the jet across the free-flow regime and the deposition dynamics of a gravity-driven viscous thread onto a moving surface known as the "fluid-mechanical sewing machine" and</p>
<p>• learning the process dynamics with minimum experimental cost as described by the required number of high-fidelity data.Two case studies were performed, one regarding the jet diameter profile and the other regarding the lag distance.We have proven, through Gaussian Process Regression, that for an offline learning strategy, the number of data and their respective position in the design space are crucial for the quality and the confidence of the predictions in both cases.Also, in the case of the jet radius profile, coupling high-fidelity data, provided from the machine vision module, with low-fidelity data, provided from the multi-physics numerical model, through multi-fidelity Gaussian Process Regression, can provide better and more confident predictions, while using less high-fidelity observations.Such results lead to computational cost reduction, since jet diameter needs to be evaluated in less points across the nozzle to bed distance, and hence to even faster video processing times.Then an online learning strategy was utilized.Bayesian Optimization algorithms were employed to actively learn the jet diameter profile, using the minimum experimental cost, with and without multi-fidelity modeling.Once more ML planner can effectively learn the jet evolution in the freeflow regime much more efficiently when it is informed by physics.Same strategy is employed.</p>
<p>Methods</p>
<p>Machine Vision Module</p>
<p>Jet Metrology.For the implementation of the Jet Metrology algorithm, Python 3.8 was used, along with the python bindings of the OpenCV library, which enables us to read and process video data.The Jet Metrology algorithm consists of two sub-algorithms.The first is the Object Segmentation and Detection algorithm.The second is the Feature Extraction algorithm.</p>
<p>The first sub-algorithm segments the needle tip, the Taylor cone, the jet and the deposited fiber `on the collector.In addition to that, the algorithm attempts to find the jet's deposition point on the collector.Finally, the segmented objects of interest are plotted for the user to visually inspect the output and assess the performance of the algorithm.To detect the objects of interest in each video frame we use the very much alike meanshift 31 and camshift 32</p>
<p>algorithms</p>
<p>The meanshift algorithm is based on a statistical concept directly related to clustering.Similar to other clustering algorithms, the meanshift algorithm scans the whole frame for high concentration of pixels of the same color.The main difference between the meanshift and the camshift algorithms is that the camshift algorithm has the capability to adjust, so that the tracking box can change its size and direction, to better correlate with the movements of the tracked object.The meanshift and camshift algorithm are useful tools to employ for object tracking.Also, unlike neural networks and other machine learning methods for object detection, these algorithms can be immediately implemented and deployed unsupervised, i.e. without the need to train a model with numerous labeled images.Instead, the algorithm takes as an input the initial color of the object, that needs to be detected, and then it tracks it throughout the rest of the video.On the other hand, using color as a primary method of identification, neither of the two algorithms can identify objects based on specific shapes and features, which makes them significantly less powerful than other methods.Furthermore, objects varying in color on a large scale and complex or noisy backgrounds can make object detection and tracking problematic.As a result, the meanshift and camshift algorithms work best under controlled environments.</p>
<p>The first step is to reverse the image colors so that the objects of interest are white and the background black.The next step is to apply a multi-color mask to segment them, and then to change the image color-space from BGR to HSV.Finally, the meanshift algorithm is applied to detect the needle and the Taylor cone, since there is barely any significant movement to them, as well as the camshift algorithm to detect and track the jet.</p>
<p>To find the deposition point, the algorithm needs to know the collector's position.Then, it creates a window around the collector, crops the region of interest from the frame and processes that instead of the whole frame.The built-in function used to find the deposition point is the cv2.goodFeaturesToTrack.This function finds the most prominent corner in our region of interest by calculating its eigen-values, as described in 33 .</p>
<p>Finally, by subtracting the deposition point from the nozzle's position (center of blue rectangle in Figure 2c), we get the lag distance, which is depicted with a two-way orange arrow in Figure 3.</p>
<p>The second sub-algorithm is the one responsible for extracting all the jet features that are relevant to the process dynamics.These features are the diameter, areas, and angles of the jet as we move along the z-axis.Another important feature is the velocity of each jet's point along the xaxis relatively to the nozzle's position.To get all those features we follow a straightforward procedure.The algorithm takes three inputs, the first is the current video frame.The second input is the calibration factor (), which is a correlation between distance units (mm) and pixels.The last one is the stride.The stride indicates every how many pixels along the z-axis we perform computations.Using too small a stride would lead to more precise calculations but would tremendously increase the computation time.On the other hand, using too large a stride would lead to shorter computation times but at a risk to lose important information.</p>
<p>The first step is to change the frame's color-space from RGB scale to grayscale, so that the Canny edge detection algorithm 34 can be applied.The parameters of the Canny edge detector are [threshold_1, threshold_2] and were specified to 150 and 255 in a semi-automatic way, using trackbars while performing edge detection to other video samples.After performing Canny edge detection, we read the first row of pixels in our canny-frame, which now is an array of 0 and 255.If Canny algorithm has been implemented correctly when we read this row of pixels from left to right, the first time we encounter a 255 should be the left edge () of our jet.Likewise, the first time we encounter a 255 while reading the row of pixels from right to the left, should be the right edge () of our jet.By subtracting those two pixels' indices and multiplying with the calibration factor we get the diameter of the jet at this position in the z-axis, which is equal to 2   .
2 𝑅𝑅 𝑗𝑗 = (𝑏𝑏𝑠𝑠 − 𝑙𝑙𝑠𝑠)𝑠𝑠𝑐𝑐(1)𝑏𝑏 𝑏𝑏𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑗𝑗𝑏𝑏𝑏𝑏 = 𝑏𝑏𝑠𝑠(2)
Those indexes are also stored in two variables (  ,   ) so that they can be used to calculate the jet angles as we move down the z-axis.</p>
<p>Then we repeat the procedure for every 'stride' rows.After finding the left () and right () edges and calculating the diameter, the area and angles can be calculated as:
𝐴𝐴 𝑗𝑗 = ���𝑏𝑏𝑠𝑠 𝑝𝑝𝑏𝑏𝑝𝑝𝑝𝑝𝑝𝑝𝑜𝑜𝑏𝑏𝑝𝑝 − 𝑙𝑙𝑠𝑠 𝑝𝑝𝑏𝑏𝑝𝑝𝑝𝑝𝑝𝑝𝑜𝑜𝑏𝑏𝑝𝑝 � + (𝑏𝑏𝑠𝑠 − 𝑙𝑙𝑠𝑠)� * 𝑠𝑠𝑡𝑡𝑏𝑏𝑠𝑠𝑠𝑠𝑠𝑠� 𝑠𝑠𝑐𝑐 2(3)𝜃𝜃 𝑗𝑗𝑗𝑗 = arctan((𝑙𝑙𝑠𝑠 − 𝑙𝑙𝑠𝑠 𝑝𝑝𝑏𝑏𝑝𝑝𝑝𝑝𝑝𝑝𝑜𝑜𝑏𝑏𝑝𝑝 )/𝑠𝑠𝑡𝑡𝑏𝑏𝑠𝑠𝑠𝑠𝑠𝑠)(4)𝜃𝜃 𝑗𝑗𝑏𝑏 = arctan((𝑏𝑏𝑠𝑠 − 𝑏𝑏𝑠𝑠 𝑝𝑝𝑏𝑏𝑝𝑝𝑝𝑝𝑝𝑝𝑜𝑜𝑏𝑏𝑝𝑝 )/𝑠𝑠𝑡𝑡𝑏𝑏𝑠𝑠𝑠𝑠𝑠𝑠)(5)
The   ,   are then updated with the ,  values.After accessing all frame's rows, the algorithm returns arrays containing all the quantified Diameters, Areas, Right Boundaries, Angles left and Angles right.The same procedure is applied to all frames.Right Boundaries are important because by subtracting the right edges of two consecutive frames we can calculate the jet's velocity (  ) on the x-axis.</p>
<p>Physics-based Modeling Module</p>
<p>Multiphysics Model.The importance of accurately extracting jet properties is signified by several studies on predicting the jet stable region diameter, through mathematical modeling.Zhmayev et al. proposed a model by fully coupling the conservation of mass, momentum, charge and energy equations with a constitutive model and the electric field equations at the steady state 31 .Similar to most models, they utilize the thin filament approximation to obtain a simpler and more tractable solution.This assumption is possible by appropriately averaging the model variables across the radial direction.In addition, the charge and electric field equations are simplified, under the assumption of low electrical conductivity, as compared to the governing equations for isothermal simulations presented by Carroll and Joo 32 .The conservation of energy relation and a non-isothermal constitutive model were added to extend to non-isothermal situations.The resulting governing equations after being nondimensionalized are as follows (see Supplementary Information for Nomenclature):
Continuity: 𝑅𝑅 𝑗𝑗 2 𝑉𝑉 𝑗𝑗 = 1(6)
Momentum:
𝑅𝑅𝑠𝑠𝑉𝑉 𝑗𝑗 𝑉𝑉 𝑗𝑗 ′ = 𝐵𝐵𝐵𝐵 + �𝑅𝑅 𝑗𝑗 2 (𝜏𝜏 𝑧𝑧𝑧𝑧 − 𝜏𝜏 𝑏𝑏𝑏𝑏 )� ′ 𝑅𝑅 𝑗𝑗 2 + 𝑅𝑅 𝑗𝑗 ′ 𝐶𝐶𝑏𝑏𝑅𝑅 𝑗𝑗 2 + 𝐹𝐹 𝑝𝑝 �𝜎𝜎𝜎𝜎 ′ + 𝛽𝛽 𝛦𝛦 𝛦𝛦 𝑡𝑡 𝐸𝐸 𝑡𝑡 ′ + 2𝜎𝜎𝛦𝛦 𝑡𝑡 𝑅𝑅 𝑗𝑗 �(7)
Charge:
𝜎𝜎 = 𝑅𝑅(8)
Electric field:
𝐸𝐸 𝑡𝑡 = 1 �1 + 2𝑍𝑍 − 𝑍𝑍 2 𝑥𝑥 � � � 1 + �𝑅𝑅 𝑗𝑗 ′ � 2 � 𝐸𝐸′ 𝑡𝑡 = −2 + 2𝑍𝑍/𝑥𝑥 �1 + 2𝑍𝑍 − 𝑍𝑍 2 𝑥𝑥 � � � 1 + �𝑅𝑅 𝑗𝑗 ′ � 2 �(9)
Energy:
𝑃𝑃𝑠𝑠𝑉𝑉 𝑗𝑗 𝛩𝛩 ′ = 𝑁𝑁𝑏𝑏𝑉𝑉 𝑗𝑗 ′ (𝜏𝜏 𝑧𝑧𝑧𝑧 − 𝜏𝜏 𝑏𝑏𝑏𝑏 ) − 2𝐵𝐵𝑠𝑠 𝐿𝐿 (𝛩𝛩 − 𝛩𝛩 ∞ ) 𝑅𝑅 𝑗𝑗(10)
Constitutive:
𝜏𝜏 𝑧𝑧𝑧𝑧 = 𝜏𝜏 𝑝𝑝,𝑧𝑧𝑧𝑧 + 2𝛽𝛽𝑐𝑐(𝛩𝛩)𝑉𝑉 𝑗𝑗 ′ 𝜏𝜏 𝑏𝑏𝑏𝑏 = 𝜏𝜏 𝑝𝑝,
The system of equations can be reduced to a set of five coupled first order ordinary differential equations (ODEs).Boundary Conditions are required, in order to proceed towards the numerical solution.The model was implemented in Python.While true properties and parameters of the material are not provided the ones used in [13] for PCL were used.As also referred in [12], [13] the model slightly underpredicts the jet radius while in the Taylor cone area, but when the jet is stabilized, it accurately predicts it's radius.Knowing this, even if the volumetric flowrate (Q) is not provided with the dataset, a Particle Swarm Optimization (PSO) algorithm was also implemented to find the Q for which the predicted jet's radius better fits the computer vision observations.Geometrical Model.Lag distance is a highly important parameter regarding the quality of the process outcome.Specifically, for some collector speeds, the jet falls onto the moving collector in a way reminiscent of a sewing machine, generating a rich variety of periodic patterns, such as meanders, W patterns, alternating loops and translated coiling (see Figure 9d).P. T. Brun et al. 33 proposed a quasistatic geometrical model, consisting of three coupled ordinary differential equations for the radial deflection, the orientation and the curvature of the path of the jet's contact point with the collector, capable of reconstructing the patterns observed experimentally while successfully calculated the bifurcation threshold of different patterns.They also evidenced that the jet/collector velocity ratio (  /  ) was the key factor for pattern variation.
𝜏𝜏 𝑝𝑝,𝑧𝑧𝑧𝑧 | 𝑍𝑍=0 = 2(1 − 𝛽𝛽)𝑐𝑐(𝛩𝛩)𝑉𝑉 𝑗𝑗 ′ (12) 𝜏𝜏 𝑝𝑝,𝑏𝑏𝑏𝑏 | 𝑍𝑍=0 = −(1 − 𝛽𝛽)𝑐𝑐(𝛩𝛩)𝑉𝑉 𝑗𝑗 ′ (13) 𝛩𝛩| 𝑍𝑍=0 = 0 (14) 𝑅𝑅| 𝑍𝑍=0 = 1 (15) ⎣ ⎢ ⎢ ⎡ 6 𝑅𝑅 𝑗𝑗 4 �𝑅𝑅 𝑗𝑗 ′ � 2 + � 1 𝐶𝐶𝑏𝑏𝑅𝑅 𝑗𝑗 2 + 𝐹𝐹𝑠𝑠𝑅𝑅 𝑗𝑗 � 𝑅𝑅 𝑗𝑗 ′ + 2𝐹𝐹𝑠𝑠 � 1 + �𝑅𝑅 𝑗𝑗 ′ � 2 ⎝ ⎛ 1 − 𝛽𝛽 𝛦𝛦 � 1 + �𝑅𝑅 𝑗𝑗 ′ � 2 ⎠ ⎞ ⎦ ⎥ ⎥ ⎤ 𝑧𝑧=0 = 0(16)
According to this geometrical model, the deposited trace on the collector is a combination of the obit of the contact point (when collector's speed is equal to zero   = 0, the jet creates coiling patters with radius   ) and the movement of the collector.
𝒒𝒒(𝑠𝑠, 𝑡𝑡) = 𝒓𝒓(𝑠𝑠) + 𝑈𝑈 𝑐𝑐 �𝑡𝑡 − 𝑠𝑠 𝑉𝑉 𝑗𝑗𝑗𝑗 � 𝑠𝑠 𝑗𝑗 ,(17)
where (, ) is the deposited trace,  is the arc-length,  is time, () is the contact point at time /  ,   is the direction of the collector's speed,  − /  is the time that the contact point moves together with the collector.Differentiating (, ) and moving from Cartesian to Polar coordinates (,  denote the polar coordinates of the contact point ()), and considering the curvature  ′ at the bottom of the jet, we get the system of ODEs:
𝑏𝑏 ′ = cos(𝜃𝜃 − 𝜓𝜓) + 𝑈𝑈 𝑐𝑐 𝑉𝑉 𝑗𝑗𝑗𝑗 𝑠𝑠𝐵𝐵𝑠𝑠𝜓𝜓 (18)𝜓𝜓 ′ = 1 𝑏𝑏 �sin(𝜃𝜃 − 𝜓𝜓) − 𝑈𝑈 𝑐𝑐 𝑉𝑉 𝑗𝑗𝑗𝑗 𝑠𝑠𝑠𝑠𝑠𝑠𝜓𝜓� (19)𝜃𝜃 ′ = 1 𝑅𝑅 𝑐𝑐 � 𝑏𝑏 𝑅𝑅 𝑐𝑐 �1 + 0.715 2 cos(𝜃𝜃 − 𝜓𝜓) 1 − 0.715 cos(𝜃𝜃 − 𝜓𝜓) 𝑏𝑏� sin(𝜃𝜃 − 𝜓𝜓)(20)
This geometrical model was implemented in Python and by varying the dimensionless parameter   /  from 0 to 1 as suggested 30 , the orbit and the deposited trace can be reconstructed.Verifying the results from 30 , the critical velocity at which the straight pattern appears is   =   , which means   /  = 1.for speed ratios 0 &lt;   /  &lt; 1 the process is highly unstable, forming the translated coiling, alternating loops, W patterns and meanders when the speed ratios are 0.23, 0.48, 0.64, 0.83, respectively.</p>
<p>Machine Learning Module</p>
<p>Gaussian Process Regression.Gaussian Process Regression is a non-parametric stochastic process with strong probabilistic establishment 35 .GPR is a supervised machine learning technique, which predicts a probability distribution based on Bayesian theory unlike other machine learning algorithms that give deterministic predictions.The idea behind GPR is that the posterior probability can be modified based on a prior probability, given a new observation.Those characteristics allow the uncertainty quantification of each point prediction.Assuming there is a dataset available, consisting of input-output pairs of observations  = {  ,   } = (, ),  = 1, 2, … ,  that are generated by an unknown model function   = (), ℝ  (23) GPR aims to learn the mapping between the set of input variables and the unknown model (), given the set of observations .To map this correlation () is typically assigned a GP prior.</p>
<p>Gaussian Processes (GPs) are powerful modelling frameworks incorporating a variety of kernels.A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution 35 .</p>
<p>~ �(), (,  ′ ; )� (24) where  is a kernel function with a set of trainable hyperparameters .The kernel defines a symmetric-positive covariance matrix   = �  ,   ; �, ℝ  , which reflects the prior available knowledge on the function to be approximated.Furthermore, kernel's eigenvalues define a reproducing kernel Hilbert space, that determines the class of functions within approximation capacity of the predictive GP posterior mean.Hyper-parameters  are trained by maximizing the marginal log-likelihood of the model 35 .</p>
<p>Assuming a Gaussian likelihood and using the Sherman-Morrison-Woodbury formula the expression for the posterior distribution (|, ) is tractable and can be used to perform prediction given a new output  +1 for a new input  +1 .</p>
<p>(𝑐𝑐 𝑏𝑏+1 |𝑦𝑦 1:𝑏𝑏 , 𝑥𝑥 1:𝑏𝑏 , 𝑥𝑥 𝑏𝑏+1 ) = 𝒩𝒩�𝑐𝑐 𝑏𝑏+1 �𝜇𝜇 𝑏𝑏 (𝑥𝑥 𝑏𝑏+1 ), 𝜎𝜎 𝑏𝑏 2 (𝑥𝑥 𝑏𝑏+1 )�(26)
  ( +1 ) =  +1  −1  1:
𝜎𝜎 𝑏𝑏 2 (𝑥𝑥 𝑏𝑏+1 ) = 𝑘𝑘(𝑥𝑥 𝑏𝑏+1 , 𝑥𝑥 𝑏𝑏+1 ) − 𝑘𝑘 𝑏𝑏+1 𝐾𝐾 −1 𝑘𝑘 𝑏𝑏+1 𝑇𝑇(27)
) where  +1 = [( +1 ,  1 ), … , ( +1 ,   )].As referenced before prediction consists of a mean, computed using the posterior mean  * , and an uncertainty term, computed using the posterior variance  * 2 .</p>
<p>Multi-fidelity Modeling.The GPR framework, presented above, can be extended to construct probabilistic models able to consider numerous information sources of different fidelity levels [24].Supposing that s levels of information source are available, the input, output data pairs can be organized by increasing fidelity as   = {  ,   },  = 1, 2, … , .So,   denotes the output of the most accurate and expensive to evaluate model, while  1 denotes the output of the cheapest and least accurate model to evaluate.Assuming that only two models are available, a high-fidelity model and a low fidelity model, the high-fidelity model can be defined as a scaled sum of the low fidelity model plus an error term:</p>
<p>ℎℎ () =   () +   () (29) where  is a scaling constant quantifying the correlation between the two models and   () denotes another GP which models the error.</p>
<p>A numerically efficient recursive inference scheme can then be constructed, by replacing the GP prior   () with the GP posterior     +1 () of the previous inference level, while assuming that the corresponding experimental design sets {D 1 , D 2 , . . ., D s } have a nested structure.This implies that the training inputs of higher fidelity model needs to be a subset of the training inputs of the low fidelity model.This scheme is matching totally the Gaussian posterior distribution predicted by the fully coupled scheme, only now the inference problem is decoupled into two GPR problems, yielding the multi-fidelity posterior distribution  � ℎℎ � ℎℎ ,  ℎℎ ,     +1 � with a predictive mean and variance at each level 18 .</p>
<p>𝜇𝜇 𝑗𝑗𝑜𝑜𝑙𝑙 (𝑥𝑥</p>
<p>where  ℎℎ ,   denote the number of training points from the high and low fidelity models, respectively.</p>
<p>Active Learning.Let's assume again that  observations are available {  ,   },  = 1, … ,  where   = (  ) and the next point to be evaluated ( +1 ,  +1 ) needs to be considered.The question that arises is if there is a more informed way to pick those points when evaluation is expensive to perform, rather than random picking.This is achieved through an acquisition function (•).The role of the acquisition function is to guide the search for the optimum.They are defined in a way such that high acquisition values correspond to a potential optimum of the unknown model , large prediction uncertainty or a combination of those.Maximizing the acquisition function is used to select the next point to evaluate the function at.Consequently, the goal is to sample  sequentially at   (|).</p>
<p>Every acquisition function depends on ,  2 or a combination of both.The scale at which it depends on each one of those defines the exploration-exploitation tradeoff.When exploring, points where the GP variance is large should be chosen.When exploiting, points where the GP mean is closest to the extremum should be chosen.Many acquisition functions are available, some of them are: For Video S2 the air pressure feeding the nozzle was set to 2.4 bar and the distance between nozzle and collector was set to 4.5mm with a standard deviation 0.1mm.Collector's speeds tested in Video S2 were 292.5 mm s -1 , 520 mm s -1 , 1300 mm s -1 and 4420 mm s -1 .</p>
<p>First, the videos were split based on the collector speed setting.Second, the video frames were cropped to remove redundant pixels that would result to increased processing time.For real time video processing the user would need to specify the region of interest in the frame, so as to crop and dispose needless information, as well as the position of the nozzle, the collector, and a factor, which represents the length of the Taylor cone depending on the nozzle's diameter.</p>
<p>Supporting Figures</p>
<p>Figure 1 :
1
Figure 1: Electrohydrodynamic Jet Printing Process.Solution electrospinning (SES) vs. melt electrospinning (MES).The main differentiating feature between the two processes is the extent of the jet instabilities that arise from the electrostatic forces acting at the polymer jet-air interface.For MES, the chaotic jet regime is limited close to the grounded collector plate due to the high viscosity and dielectric properties of the pure polymer melt.b Direct melt electrowriting (MEW) and its operating principle.</p>
<p>Figure 2 :
2
Figure 2: The GPJet Pipeline Framework.A Physics-informed Bayesian Machine Learning framework comprised by three different modules: a) the Machine Vision module, which takes as an input timeseries video focusing on the polymer jet in the free flow regime and performs extraction of high-fidelity jet features in real-time based on an automated image processing workflow b) the Physics-based Modeling Module, which and c) the Machine Learning module, which takes as an input high fidelity experimental data from the Machine Vision module and low fidelity modeling data from the and performs a series of data-driven tasks to learn the jet dynamics.</p>
<p>Task 1 :
1
Read new video frame.Task 2: Process the frame to reverse background color.Task 3: Edge-based feature extraction and data storage.</p>
<p>Figure 3 :
3
Figure 3: Machine Vision Module.a) Process dynamics and its scale.b) Profiling experiments for different code implementations.c) Edge-based feature extraction methodology (Task 3 in Figure 3b).d) Object-based feature extraction methodology (Task 4 in Figure3b).</p>
<p>Task 4 :
4
Object-based feature extraction and data storage.Task 5: Show processed video output.Task 6: Save video output.</p>
<p>Figure 4 :
4
Figure 4: Jet Metrology with the Machine Vision Module.a) The extracted features during the deceleration-acceleration phase of the printing process.i) Overlayed video frames demonstrating the dynamics during the deceleration-acceleration phase and normalized jet length point of interest (   = 17.5 ⁄ ) denoted with red color.ii) Normalized jet radius (    ⁄ ), Normalized jet area (    ⁄ ), Normalized jet angles (/90  ) and Normalized jet velocity (  /  ) at the denoted point of interest plotted against the normalized time (   ⁄ ) during the deceleration-acceleration phase.iii) Jet lag distance (  ) plotted against the normalized time (   ⁄ ) during the deceleration-acceleration phase.b) The extracted features during the steady speed phase pf the printing process.i) Overlayed video frames demonstrating the dynamics during the steady speed phase.ii) Normalized jet radius (    ⁄ ), Normalized jet area (    ⁄ ), Normalized jet angles (/90  ) and Normalized jet velocity (  /  ) at the denoted point of interest plotted against the normalized time (   ⁄ ) during the steady speed phase.iii) Jet lag distance (  ) plotted against the normalized time (   ⁄ ) during the steady speed phase.</p>
<p>Figure 5 :
5
Figure 5: Results of Gaussian Process Modeling Regression Tasks.a) fitting normalized (   ⁄ ) jet radius observation data (n=5) obtained from the computer vision metrology module of the GPJet framework at specific z axis coordinates along the normalized jet length (   ⁄ ).b) fitting normalized jet radius using a higher number of observation data (n=10) compared to the previous case (a).c) fitting lag distance (  ) observation data (n=3) obtained from the computer vision metrology module of the GPJet framework for specific speed ratios (  /  ).d) fitting lag distance using all available observation data (n=12).For non-normalized quantities units are in SI.Filled contours represent uncertainty bounds (95% confidence intervals (CIs)).</p>
<p>Figure 6 :
6
Figure 6: Results of Multi-fidelity Modeling Regression Tasks.a) fitting normalized high fidelity observation data (n=6, red color) of jet radius (    ⁄) and low fidelity model data obtained from the computer vision metrology module of the GPJet framework and from the multi-physics model, respectively, at specific z axis coordinates along the normalized jet length (   ⁄ ) and comparing the results with a simple GP fit using the same number of high fidelity observation data.b) fitting a higher number of normalized high fidelity observation data (n=7, red color) of jet radius (    ⁄ ) and low fidelity model data obtained from the computer vision metrology module of the GPJet framework and from the multi-physics model, respectively, at specific z axis coordinates along the normalized jet length (   ⁄ ) and comparing the results with a simple GP fit using the same number of high fidelity observation data.</p>
<p>Figure 7 :
7
Figure 7: Results of Active Learning process on Multifidelity Modeling Task.a) exploring the design space using Active Learning to fit a Multifidelity Gaussian Process to normalized high fidelity observation data (red color) of jet radius (    ⁄) and low fidelity model data obtained from the computer vision metrology module of the GPJet framework and from the multi-physics model, respectively, at specific z axis coordinates along the normalized jet length (   ⁄ ) ( i -vi denote the iterations of the active learning algorithm until it meets its termination criteria).B) exploring the design space using Active Learning to fit a Gaussian Process to normalized high fidelity observation data (red color) of jet radius (    ⁄ ) obtained from the computer vision metrology module of the GPJet framework at specific z axis coordinates along the normalized jet length (   ⁄ ) ( i -vi denote the iterations of the active learning algorithm until it meets its termination criteria).</p>
<p>Figure 8 :
8
Figure 8: Results of Exploring the Design Space Task.Exploring the design space using active learning to fit a Gaussian Process Model to lag distance (  ) observation data obtained from the computer vision metrology module of the GPJet framework for specific speed ratios (  /  ).a-d) Iterations of the active learning algorithm until it meets termination criteria.</p>
<p>Figure 9 :
9
Figure 9: Results of Bayesian Optimization Task.Performing Bayesian Optimization to find the minimum lag-distance (L j ) by fitting a Gaussian Process Model to lag distance (L j ) observation data obtained from the computer vision metrology module of the GPJet framework for specific speed ratios (U c /V jm ).a-c) Iterations of the Bayesian optimization algorithm until it meets termination criteria.d) For speed ratios less than one (U c /V jm &lt; 1) the process is unstable, no straight line is formed, instead the translated coiling, alternating loops, W patterns and meanders patterns are formed, therefore no lag distance (L j ) observation data can be obtained from the computer vision metrology module of the GPJet framework.</p>
<p>( 21 )
21
() can be completely estimated by a mean () and a covariance function (, ′).() = [()] (22) (, ′) = [�() − ()��( ′ ) − ( ′ )�]</p>
<p>Figure 1 :
1
Figure 1: Features Extracted from Computer Vision Module.a) Normalized jet radius (    ⁄ ) obtained from the computer vision metrology module of the GPJet framework plotted against the normalized jet length (   ⁄ ) and the normalized time (   ⁄ ).b) Normalized jet area (    ⁄ ) obtained from the computer vision metrology module of the GPJet framework plotted against the normalized jet length (   ⁄ ) and the normalized time (   ⁄ ).c) Jet angles () obtained from the computer vision metrology module of the GPJet framework plotted against the normalized jet length (   ⁄ ) and the normalized time (   ⁄ ).d) Jet velocities (  ) obtained from the computer vision metrology module of the GPJet framework plotted against the normalized jet length (   ⁄ ) and the normalized time (   ⁄ ).</p>
<p>Figure 2 :
2
Figure 2: Performance Metrics Evolution for Active Learning Tasks.a) Root Mean Squared Error (RMSE) evolution after each iteration, regarding the normalized jet radius (    ⁄ ).b) Root Mean Squared Error (RMSE) evolution after each iteration, regarding the lag distance (  ).c) Mean Confidence Interval Width (MCIW) evolution after each iteration, regarding the normalized jet radius (    ⁄ ).d) Mean Confidence Interval Width (MCIW) evolution after each iteration, regarding the lag distance (  ).</p>
<p>Figure 3 :
3
Figure 3: Collective Performance Metrics for Regression and Active Learning Tasks.a) Root Mean Squared Error (RMSE) for every case, regarding the normalized jet radius (    ⁄ ).b) Root Mean Squared Error (RMSE) performance metric, for every case, regarding the lag distance   .c) Mean Confidence Interval Width (MCIW) performance metric, for every case, regarding the normalized jet radius (    ⁄ ).d) Mean Confidence Interval Width (MCIW) performance metric, for every case, regarding the lag distance (  ).</p>
<p>Figure 4 :
4
Figure 4: Performance Metrics.a) Minimum Regret Performance Metric evolution, after each iteration, regarding the Bayesian Optimization Task to find the minimum lag-distance (  ).b) Minimum Regret Performance Metric evolution, after each iteration, regarding the Active Learning Task to explore the design space of lag-distance (  ) for specific speed ratios (  /  ).</p>
<p>Table 1 :
1
Overview of curated dataset that emulates an E-jet printing machine.</p>
<p>Machine Setting Air pressure 𝑝𝑝 [𝑏𝑏𝑏𝑏𝑏𝑏] Tip to Collector Distance 𝑍𝑍 [𝑚𝑚𝑚𝑚] Collector Speed 𝑈𝑈 𝑐𝑐 [𝑚𝑚𝑚𝑚/𝑠𝑠] Number of frames Duration [𝑠𝑠𝑠𝑠𝑠𝑠]
11.23.5191.2134126.8221.23.5212.5167233.4431.23.5255143728.7441.23.5340134326.8651.23.551064812.9661.23.585061312.2671.23.515304579.1481.23.528904018.0292.44.5292.5110822.16102.44.552080216.04112.44.5130081216.24122.44.544202845.68See Supplementary Note for data source and pre-processing.</p>
<p>Table 2 : Material properties of PCL
2Property</p>
<p>Table 3 : Typical values of dimensionless parameters used for PCL
3ParameterValue𝐵𝐵𝑠𝑠0.424𝐷𝐷𝑠𝑠1.14𝐵𝐵𝐵𝐵0𝑅𝑅𝑠𝑠5.785  *  10 −6𝐶𝐶𝑏𝑏1048.276𝑁𝑁𝑏𝑏0.446𝐹𝐹𝑠𝑠0.0254𝐷𝐷21.283𝑃𝑃𝑠𝑠105.209𝑃𝑃𝑠𝑠 𝑐𝑐0.1122</p>
<p>+1 ) =   +    +1   −1 �  1:  −   � (30)  ℎℎ (  ℎℎ ) =   �  ℎℎ +1 � +     2 �   +1 � = (   +1 ,    +1 ) −    +1   −1    +1
(31)+ 𝑘𝑘 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ+1 𝐾𝐾 ℎ𝑝𝑝𝑖𝑖ℎ −1 �𝑦𝑦 ℎ𝑝𝑝𝑖𝑖ℎ 1:𝑛𝑛 ℎ𝑖𝑖𝑖𝑖ℎ− 𝜌𝜌𝜇𝜇 𝑗𝑗𝑜𝑜𝑙𝑙 �𝑥𝑥 ℎ𝑝𝑝𝑖𝑖ℎ 1:𝑛𝑛 ℎ𝑖𝑖𝑖𝑖ℎ� − 𝜇𝜇 𝑝𝑝𝑏𝑏𝑏𝑏 �𝑇𝑇(32)𝜎𝜎 ℎ𝑝𝑝𝑖𝑖ℎ 2�𝑥𝑥 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ +1 �2 = 𝜌𝜌 2 𝜎𝜎 𝑗𝑗𝑜𝑜𝑙𝑙 𝑛𝑛 𝑙𝑙𝑙𝑙𝑙𝑙 +1 − 𝑘𝑘 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ +1 𝐾𝐾 ℎ𝑝𝑝𝑖𝑖ℎ �𝑥𝑥 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ +1 � + 𝑘𝑘 �𝑥𝑥 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ +1 , 𝑥𝑥 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ +1 � −1 𝑘𝑘 𝑏𝑏 ℎ𝑖𝑖𝑖𝑖ℎ 𝑇𝑇</p>
<p>Table 4 : Types of acquisition functions for Active Learning scheme Variance 𝜎𝜎
4
2()
Purely exploration,makes sure, that welearn the function𝑐𝑐 everywhere on xto a similar level ofabsolute error.
Jet's trace on the collector  The contact point  </p>
<p>After sampling  +1 and evaluating  +1 , GP regression is performed to fit to the new point as well.Then the process repeats itself until termination criteria are met, such as a maximum number of iterations, a minimum or maximum value is reached, or uncertainty is below an allowed value.-SupplementaryRegretThe Minimum Regret Performance Metric quantifies how well a method works at finding the optimum.-DatasetVideo S1 and Video S2 published by Hrynevich et al.10were chosen as the source of the dataset used in this paper.A Sony Alpha 7 (Sony Corp. Japan) digital camera was used with a Nikon ED 200 mm lens (Nikon Corp. Japan).1080 p resolution videos of the nozzle, jet and collector were taken at 50 frames per second.Process hyperparameters were set to 8 m s -2 and 500 m s -3 maximal stage acceleration and jerk, a 22G nozzle was used, polymer temperature was set to 87 o C and the voltage to the collector was set to -1.5kV, while the voltage to the nozzle was set to +5.75kV.For Video S1 the air pressure feeding the nozzle was set to 1.2 bar and the distance between nozzle and collector was set to 3.5mm with a standard deviation 0.1mm.Collector's speeds tested in Video S1 were 191.25 mm s -1 , 212.5 mm s -1 , 255 mm s -1 , 340 mm s -1 , 510 mm s -1 , 850 mm s -1 , 1530 mm s -1 and 2890 mm s -1 .
Printing soft matter in three dimensions. R L Truby, J A Lewis, Nature. 5402016</p>
<p>Three-dimensional printed electronics. J A Lewis, B Y Ahn, Nature. 518</p>
<p>Process-Structure-Properties in Polymer Additive Manufacturing via Material Extrusion: A Review. G D Goh, Crit Rev Solid State. 452019</p>
<p>High-resolution electrohydrodynamic jet printing. J.-U Park, Nat Mater. 62007</p>
<p>Mechanisms, Capabilities, and Applications of High-Resolution Electrohydrodynamic Jet Printing. M S Onses, E Sutanto, P M Ferreira, A G Alleyne, J A Rogers, Small. 112015</p>
<p>Direct Writing By Way of Melt Electrospinning. T D Brown, P D Dalton, D W Hutmacher, Adv Mater. 232011</p>
<p>The Next Frontier in Melt Electrospinning: Taming the Jet. T M Robinson, D W Hutmacher, P D Dalton, Adv Funct Mater. 2919046642019</p>
<p>Melt Electrospinning Writing Process Guided by a "Printability Number. F Tourlomousis, H Ding, D M Kalyon, R C Chang, J Manuf Sci Eng. 139810042017</p>
<p>Fibre pulsing during melt electrospinning writing. G Hochleitner, Bionanomaterials. 172016</p>
<p>Accurate Prediction of Melt Electrowritten Laydown Patterns from Simple Geometrical Considerations. A Hrynevich, I Liashenko, P D Dalton, Adv Mater Technologies. 52000772. 2020</p>
<p>Printomics: the high-throughput analysis of printing parameters applied to melt electrowriting. F M Wunner, Biofabrication. 11250042019</p>
<p>Research and Application of Machine Learning for Additive Manufacturing. J Qin, </p>
<p>. Addit Manuf. 521026912022</p>
<p>Autonomous experimentation systems for materials development: A community perspective. E Stach, Matter. 42021</p>
<p>A robotic Intelligent Towing Tank for learning complex fluid-structure dynamics. D Fan, Sci Robotics. 42019</p>
<p>The Automation of Science. R D King, Science. 3242009</p>
<p>Physics-informed machine learning. G E Karniadakis, Nat Rev Phys. 32021</p>
<p>A fast multifidelity method with uncertainty quantification for complex data correlations: Application to vortex-induced vibrations of marine risers. X Meng, Z Wang, D Fan, M S Triantafyllou, G E Karniadakis, Comput Method Appl M. 3861142122021</p>
<p>Engineering Design via Surrogate Modelling. D A I J Forrester, D A Sóbester, P A J Keane, 10.1002/97804707708012019</p>
<p>C E Rasmussen, C K I Williams, Gaussian Processes for Machine Learning. 2006</p>
<p>On-the-fly closed-loop materials discovery via Bayesian active learning. A G Kusne, Nat Commun. 1159662020</p>
<p>Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. P Perdikaris, M Raissi, A Damianou, N D Lawrence, G E Karniadakis, Proc Royal Soc Math Phys Eng Sci. 473201607512017</p>
<p>Multi-fidelity modelling of mixed convection based on experimental correlations and numerical simulations. H Babaee, P Perdikaris, C Chryssostomidis, G E Karniadakis, J Fluid Mech. 8092016</p>
<p>Multi-fidelity Gaussian process regression for prediction of random fields. L Parussini, D Venturi, P Perdikaris, G E Karniadakis, J Comput Phys. 3362017</p>
<p>Gaussian processes for autonomous data acquisition at large-scale synchrotron and neutron facilities. M M Noack, Nat Rev Phys. 32021</p>
<p>Accelerating the discovery of materials for clean energy in the era of smart automation. D P Tabor, Nat Rev Mater. 32018</p>
<p>The machine learning revolution in materials?. K G Reyes, B Maruyama, Mrs Bull. 442019</p>
<p>Autonomy in materials research: a case study in carbon nanotube growth. P Nikolaev, Npj Comput Mater. 2160312016</p>
<p>A Bayesian experimental autonomous researcher for mechanical design. A E Gongora, Sci Adv. 617082020</p>
<p>Application of robust monotonically convergent spatial iterative learning control to microscale additive manufacturing. Z Wang, C P Pannier, K Barton, D J Hoelzle, Mechatronics. 562018</p>
<p>Toward autonomous additive manufacturing: Bayesian optimization on a 3D printer. J R Deneault, Mrs Bull. 462021</p>
<p>Modeling of non-isothermal polymer jets in melt electrospinning. E Zhmayev, H Zhou, Y L Joo, J Non-newton Fluid. 1532008</p>
<p>Electrospinning of viscoelastic Boger fluids: Modeling and experiments. C P Carroll, Y L Joo, Phys Fluids. 18531022006</p>
<p>Liquid Ropes: A Geometrical Model for Thin Viscous Jet Instabilities. P.-T Brun, B Audoly, N M Ribe, T S Eaves, J R Lister, Phys Rev Lett. 1141745012015</p>            </div>
        </div>

    </div>
</body>
</html>
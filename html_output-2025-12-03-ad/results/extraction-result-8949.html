<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8949 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8949</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8949</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-66d9550a51cb493aaeeae72d396b0bdb1ca47fe9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/66d9550a51cb493aaeeae72d396b0bdb1ca47fe9" target="_blank">Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work describes the set of good provers and shows that optimizing process rewards from such provers improves exploration during test-time search and online RL and shows that weak prover policies can substantially improve a stronger base policy.</p>
                <p><strong>Paper Abstract:</strong> A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask:"How should we design process rewards?". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is $>8\%$ more accurate, and $1.5-5\times$ more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with $5-6\times$ gain in sample efficiency, and $>6\%$ gain in accuracy, over ORMs.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8949.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8949.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process Advantage Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A trained verifier that predicts per-step advantages A^mu (change in prover-policy success probability before vs after a step) under a chosen prover policy mu, used as dense step-level supervision combined with outcome/Q rewards to iteratively improve generation via beam search re-ranking and as dense rewards in RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2B, Gemma-9B, Gemma-27B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma family LLMs of sizes 2B, 9B, 27B finetuned on the MATH dataset (SFT checkpoints used as base policies and provers in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Process Advantage Verification (PAV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Compute A^mu(s,a)=Q^mu(s,a)-V^mu(s) (advantage under a prover mu) via rollouts from mu (Monte Carlo estimates) and train an LLM verifier to predict A^mu for prefixes; combine predicted A^mu with Q^pi (outcome/value) into an effective per-step reward Q^pi + alpha * A^mu for beam re-ranking or RL. Used at test time to iteratively prune/expand beams (per-step selection) and at training time as dense per-step rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (MATH benchmark) and synthetic didactic planted-subsequence</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve multi-step math problems where final-answer regex check (Rex) assesses correctness; didactic task involves producing a planted subsequence in a sequence generation MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Beam-search with PAVs: >8% absolute higher accuracy vs ORMs and 1.5–5x compute-efficiency (10x compute-efficiency for Gemma-2B/9B, 5x for Gemma-27B, when matching best-of-128 ORM baseline). PAV-as-ORM at N=128 gave ~4% gain over ORMs in one comparison. As dense rewards in RL (PAV-RL), yields >7% higher test accuracy than ORM-RL and improves RFT policy by +11% (2B) and +15% (9B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (ORM best-of-N): best-of-128 with ORM used as baseline; ORMs achieve substantially lower accuracy (PAV improves >8% absolute). ORM-RL (only outcome rewards) is >7% worse than PAV-RL and is much less sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External verifier module (finetuned LLM) trained to predict Monte-Carlo estimated A^mu; used to score each candidate step during iterative beam search (re-ranking/pruning) or to provide dense per-step rewards during online RL.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative experimental evidence across Gemma-2B/9B/27B: beam search with PAVs gave >8% higher accuracy and 1.5–10x better compute-efficiency vs best-of-N re-ranking with ORMs; PAV-RL achieved >7% higher final test accuracy than ORM-RL and ~6x sample-efficiency improvement (training converges ~6x faster). Didactic synthetic experiments also show 10x sample-efficiency improvement using effective reward Q^pi + alpha * A^mu vs outcome-only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PAVs rely on the choice of prover mu: overly-strong provers (mu that succeeds from almost any prefix) produce A^mu≈0 and fail to distinguish steps, making PAV uninformative; overly-weak provers likewise produce near-zero advantages. Setting mu = pi (prover identical to base) yields negligible additional gradient (no net benefit). Learned PAVs have fitting/estimation errors which upper-bound performance; paper notes inability to automatically adapt/update provers is an open limitation. Empirically, using Q^mu from overly strong provers during RL produced degenerate policies that mostly rephrase the question (reported in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared against: ORMs (outcome-only verifiers used for best-of-N), PRMs/Q-value predictors (Q^pi per-prefix value verifiers used in prior work and as ablation), and PAV-as-ORM; PAVs outperformed Q^pi-based PRMs in exploration and compute-efficiency and outperformed ORMs in both beam-search accuracy and RL sample efficiency. PAVs combine outcome(Q^pi) and advantage(A^mu) for an improved explore/exploit tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations include varying the prover mu (Best-of-K with K in {1,2,4,8,16,32}): medium K (Bo4) generally best—too small or too large K degrades performance (large K→A^mu≈0). Ablation on PAV training data: tradeoff n_cov (seed rollouts) vs n_mc (Monte-Carlo rollouts) — low-budget regimes favor coverage (n_cov > n_mc); larger budgets benefit higher n_mc to reduce Q estimate noise. Alpha (weighting of A^mu) tuned per base model (0.5 for 2B/9B, 0.2 for 27B) and improvements robust across [0.2,0.6].</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8949.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8949.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAV-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning with Process Advantage Verifiers (PAV-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Online RL algorithm that augments the outcome RL objective with dense per-step advantages predicted by a PAV (effective reward = Q^pi + alpha * A^mu), producing faster and higher-quality policy improvements compared to outcome-only RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2B, Gemma-9B (SFT/RFT initialized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma-2B and Gemma-9B SFT policies further optimized with rejection finetuning (RFT) used as RL initializations; RL updates applied to same architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>PAV-augmented RL (PAV-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During online RL, use predicted per-step advantage A^mu from a fixed PAV (trained under prover mu) and add it to Q^pi to form dense step rewards; update policy with policy-gradient-like updates using the effective reward, thereby iteratively improving the model's generation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step math problems evaluated by final answer correctness (Rex). RL objective maximizes expected Rex; PAV-RL augments with per-step advantage supervision to improve credit assignment and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PAV-RL yields >7% higher test accuracy than ORM-RL; improves RFT policy by +11% (Gemma-2B) and +15% (Gemma-9B). PAV-RL is ~6x more sample-efficient than ORM-RL (converges ~6x faster in training iterations). Also yields higher Pass@N across N<=128 by >7% (2B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>ORM-RL (outcome-only RL) yields substantially lower accuracy (PAV-RL >7% better) and requires ~6x more environment samples/iterations to reach comparable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External learned verifier (PAV) provides per-step dense reward signals during RL training (no special internal module beyond standard policy gradient framework). PAVs were trained offline using Monte-Carlo rollouts under prover mu.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical RL runs on Gemma-2B/9B show faster learning curves (Figures reported): PAV-RL achieves higher test accuracy in fewer iterations (6x sample-efficiency) and higher final accuracy than ORM-RL; Pass@N curves improved and PAV-RL discovers solutions that Best-of-256 from SFT cannot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PAV-RL performance depends on choice of prover mu used to train the PAV; fixed PAVs trained on certain provers remain useful but the paper did not run experiments where the prover is updated online (left as future work). Fitting errors in learned PAVs limit RL gains; strong provers as mu can lead to near-zero advantages and thus no benefit. ORM-RL can collapse to low-entropy (non-diverse) policies — contrasted as a failure mode of outcome-only RL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to ORM-RL (outcome-only), PAV-RL is quantitatively superior in sample-efficiency and final accuracy; compared to methods that use Q^pi PRMs as dense rewards (prior works), PAV-RL yields substantially larger gains (prior PRM-RL reported only 1–2% gains).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Experiments tested two fixed provers for PAV training (2B SFT and 9B SFT); surprisingly, a weaker prover (2B) served as the best prover for both 2B and 9B RL runs in some cases, corroborating theory that complementary (not strictly stronger) provers can be best. No experiments performed for dynamically updating prover during RL (left to future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8949.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8949.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-PRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process Reward Models using Q^pi (value-function PRMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Process reward models that assign step-level scores equal to the estimated future success probability (Q^pi) under the base policy, trained from Monte-Carlo rollouts — previously-proposed automated PRM approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2B, Gemma-9B, Gemma-27B (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Q^pi PRMs are trained LLM verifiers that predict Monte-Carlo estimates of the base policy's value Q^pi(s,a) for each prefix; used here as baseline PRMs for beam search and as components in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Value-function-based PRM (Q^pi)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Estimate Q^pi(s,a) = expected final success when continuing to sample from base policy pi conditioned on prefix; train verifier on (s,a,Q_mc) pairs (Monte Carlo labeled) and use per-step Q estimates to re-rank/prune beams or as dense rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (MATH) and beam-search re-ranking</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-step math problems; Q^pi PRMs used to guide beam search and re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Beam-search/re-ranking using Q^pi PRM yields modest gains over naive sampling but is less effective than PAV-led effective reward; PAVs achieve ~8x compute-efficiency improvement over beam search using Q^pi in the paper's evaluations (and PAV-as-ORM gave a ~4% gain over ORMs at N=128 in one experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline best-of-N with ORM or plain sampling: Q^pi PRMs sometimes improve selection but suffer from exploiting high-Q prefixes and less exploration compared to advantage-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External verifier predicting Q^pi trained from Monte-Carlo rollouts (supervised regression/cross-entropy on labeled Q estimates), used iteratively in beam search to score prefixes.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The paper shows that Q^pi as a process reward tends to retain high-Q states and can hurt exploration; empirical comparisons show PAV-based effective reward outperforms Q^pi-only PRMs in pass@N and compute-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Q^pi mixes action evaluation with the promise of the previous state and can cause beam search to exploit already-high-Q states instead of exploring promising actions; Q^pi from a very strong prover mu can assign unearned bonuses to trivial actions and leads to A^mu≈0 when used for advantage estimation; prior PRM-RL work using Q^pi reported only tiny gains (1–2%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against PAV (advantage-based) and ORMs: Q^pi PRMs are less effective at promoting exploration and compute efficiency; Snell et al. used Q^pi for beam search in prior work but PAVs outperform it.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper includes experiments showing Q^pi-only beam search (Snell et al.) is outperformed by PAVs and that combining Q^pi with A^mu (effective reward) yields better exploration-exploitation tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8949.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8949.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-K (BoK)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-of-K sampling/re-ranking policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time search method that samples K complete responses from the base policy and returns the one with the highest verifier score (commonly used with ORMs); here also used to define prover policies (BoK(pi)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2B, Gemma-9B, Gemma-27B (used as base policies and to construct BoK provers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BoK policies are derived procedurally from a base LLM policy by sampling K rollouts and selecting the best according to an ORM (or other verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-many-then-select (Best-of-K)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate K full candidate solutions from the base policy and select the highest-scoring one by a verifier (commonly an ORM); in this paper Best-of-K variants (Bo2..Bo32) are used as prover policies mu to compute advantages A^mu for PAV training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (MATH) and used in prover selection experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as both a baseline test-time selection method and as a class of provers to compute per-step advantages (BoK(pi)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>As provers, BoK with moderate K (e.g., Bo4) produced the best PAV-derived improvements; too large K (e.g., Bo32) led to A^mu≈0 and worse outcomes. BoK as the direct test-time selection baseline (Best-of-N with ORM) is the comparison target: PAV beam search matches or exceeds best-of-N oracle with far less compute.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Plain Best-of-N with an ORM re-ranker is the baseline; PAV-based beam search outperforms best-of-N (ORM) in accuracy and compute efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Generate multiple full responses then select using a verifier (non-iterative selection at the end); when used as prover, BoK is used to generate rollouts under mu for advantage estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical ablations over K show Bo4 was often the most effective prover for inducing useful advantages and downstream gains; theoretical analysis (Remark 3.1) argues BoK increases distinguishability for moderate K.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large K makes BoK trivially strong leading to near-uniform high Q^mu across prefixes and thus A^mu≈0, which fails to provide useful per-step signals; BoK strength alone is not sufficient—complementarity with pi matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8949.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8949.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>On-the-fly MC Advantage Estimation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>On-the-fly Monte-Carlo rollout advantage estimation (as in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method referenced in related work that estimates advantages A^pi online by performing Monte-Carlo rollouts from the current policy during RL to compute per-step advantages without training a PAV.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>On-the-fly Monte-Carlo advantage estimation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Estimate step-level advantages by running Monte-Carlo rollouts during RL from the current policy and computing empirical Q/V differences (used in concurrent work Kazemnejad et al. as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced in context of RL for reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to provide per-step advantage estimates for training without an offline verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Mentioned: concurrent work (Kazemnejad et al.) uses on-the-fly advantage estimation; paper argues gains from on-policy A^pi are smaller than using an appropriate distinct prover mu and PAVs, but no direct numeric comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Compute advantages online via Monte-Carlo rollouts during RL (no separate verifier model).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites concurrent work that uses on-the-fly A^pi but argues theoretically and empirically that distinct-prover advantages (A^mu) provide bigger gains; no direct experiments in this paper comparing on-the-fly A^pi vs PAVs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Setting mu=pi (on-policy advantages) leads to near-zero additional gradient benefit in the effective reward and thus little improvement over outcome-only RL, according to the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's verify step by step. <em>(Rating: 2)</em></li>
                <li>Improve mathematical reasoning in language models by automated process supervision. <em>(Rating: 2)</em></li>
                <li>Scaling llm test-time compute optimally can be more effective than scaling model parameters. <em>(Rating: 2)</em></li>
                <li>Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. <em>(Rating: 2)</em></li>
                <li>Self-explore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8949",
    "paper_id": "paper-66d9550a51cb493aaeeae72d396b0bdb1ca47fe9",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "PAV",
            "name_full": "Process Advantage Verifier",
            "brief_description": "A trained verifier that predicts per-step advantages A^mu (change in prover-policy success probability before vs after a step) under a chosen prover policy mu, used as dense step-level supervision combined with outcome/Q rewards to iteratively improve generation via beam search re-ranking and as dense rewards in RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2B, Gemma-9B, Gemma-27B",
            "model_description": "Gemma family LLMs of sizes 2B, 9B, 27B finetuned on the MATH dataset (SFT checkpoints used as base policies and provers in experiments).",
            "reflection_method_name": "Process Advantage Verification (PAV)",
            "reflection_method_description": "Compute A^mu(s,a)=Q^mu(s,a)-V^mu(s) (advantage under a prover mu) via rollouts from mu (Monte Carlo estimates) and train an LLM verifier to predict A^mu for prefixes; combine predicted A^mu with Q^pi (outcome/value) into an effective per-step reward Q^pi + alpha * A^mu for beam re-ranking or RL. Used at test time to iteratively prune/expand beams (per-step selection) and at training time as dense per-step rewards.",
            "task_name": "Mathematical reasoning (MATH benchmark) and synthetic didactic planted-subsequence",
            "task_description": "Solve multi-step math problems where final-answer regex check (Rex) assesses correctness; didactic task involves producing a planted subsequence in a sequence generation MDP.",
            "performance_with_reflection": "Beam-search with PAVs: &gt;8% absolute higher accuracy vs ORMs and 1.5–5x compute-efficiency (10x compute-efficiency for Gemma-2B/9B, 5x for Gemma-27B, when matching best-of-128 ORM baseline). PAV-as-ORM at N=128 gave ~4% gain over ORMs in one comparison. As dense rewards in RL (PAV-RL), yields &gt;7% higher test accuracy than ORM-RL and improves RFT policy by +11% (2B) and +15% (9B).",
            "performance_without_reflection": "Baseline (ORM best-of-N): best-of-128 with ORM used as baseline; ORMs achieve substantially lower accuracy (PAV improves &gt;8% absolute). ORM-RL (only outcome rewards) is &gt;7% worse than PAV-RL and is much less sample-efficient.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External verifier module (finetuned LLM) trained to predict Monte-Carlo estimated A^mu; used to score each candidate step during iterative beam search (re-ranking/pruning) or to provide dense per-step rewards during online RL.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative experimental evidence across Gemma-2B/9B/27B: beam search with PAVs gave &gt;8% higher accuracy and 1.5–10x better compute-efficiency vs best-of-N re-ranking with ORMs; PAV-RL achieved &gt;7% higher final test accuracy than ORM-RL and ~6x sample-efficiency improvement (training converges ~6x faster). Didactic synthetic experiments also show 10x sample-efficiency improvement using effective reward Q^pi + alpha * A^mu vs outcome-only.",
            "limitations_or_failure_cases": "PAVs rely on the choice of prover mu: overly-strong provers (mu that succeeds from almost any prefix) produce A^mu≈0 and fail to distinguish steps, making PAV uninformative; overly-weak provers likewise produce near-zero advantages. Setting mu = pi (prover identical to base) yields negligible additional gradient (no net benefit). Learned PAVs have fitting/estimation errors which upper-bound performance; paper notes inability to automatically adapt/update provers is an open limitation. Empirically, using Q^mu from overly strong provers during RL produced degenerate policies that mostly rephrase the question (reported in appendix).",
            "comparison_to_other_methods": "Directly compared against: ORMs (outcome-only verifiers used for best-of-N), PRMs/Q-value predictors (Q^pi per-prefix value verifiers used in prior work and as ablation), and PAV-as-ORM; PAVs outperformed Q^pi-based PRMs in exploration and compute-efficiency and outperformed ORMs in both beam-search accuracy and RL sample efficiency. PAVs combine outcome(Q^pi) and advantage(A^mu) for an improved explore/exploit tradeoff.",
            "ablation_study_results": "Ablations include varying the prover mu (Best-of-K with K in {1,2,4,8,16,32}): medium K (Bo4) generally best—too small or too large K degrades performance (large K→A^mu≈0). Ablation on PAV training data: tradeoff n_cov (seed rollouts) vs n_mc (Monte-Carlo rollouts) — low-budget regimes favor coverage (n_cov &gt; n_mc); larger budgets benefit higher n_mc to reduce Q estimate noise. Alpha (weighting of A^mu) tuned per base model (0.5 for 2B/9B, 0.2 for 27B) and improvements robust across [0.2,0.6].",
            "uuid": "e8949.0",
            "source_info": {
                "paper_title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PAV-RL",
            "name_full": "Reinforcement Learning with Process Advantage Verifiers (PAV-RL)",
            "brief_description": "Online RL algorithm that augments the outcome RL objective with dense per-step advantages predicted by a PAV (effective reward = Q^pi + alpha * A^mu), producing faster and higher-quality policy improvements compared to outcome-only RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2B, Gemma-9B (SFT/RFT initialized)",
            "model_description": "Gemma-2B and Gemma-9B SFT policies further optimized with rejection finetuning (RFT) used as RL initializations; RL updates applied to same architectures.",
            "reflection_method_name": "PAV-augmented RL (PAV-RL)",
            "reflection_method_description": "During online RL, use predicted per-step advantage A^mu from a fixed PAV (trained under prover mu) and add it to Q^pi to form dense step rewards; update policy with policy-gradient-like updates using the effective reward, thereby iteratively improving the model's generation behavior.",
            "task_name": "Mathematical reasoning (MATH)",
            "task_description": "Multi-step math problems evaluated by final answer correctness (Rex). RL objective maximizes expected Rex; PAV-RL augments with per-step advantage supervision to improve credit assignment and exploration.",
            "performance_with_reflection": "PAV-RL yields &gt;7% higher test accuracy than ORM-RL; improves RFT policy by +11% (Gemma-2B) and +15% (Gemma-9B). PAV-RL is ~6x more sample-efficient than ORM-RL (converges ~6x faster in training iterations). Also yields higher Pass@N across N&lt;=128 by &gt;7% (2B).",
            "performance_without_reflection": "ORM-RL (outcome-only RL) yields substantially lower accuracy (PAV-RL &gt;7% better) and requires ~6x more environment samples/iterations to reach comparable performance.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External learned verifier (PAV) provides per-step dense reward signals during RL training (no special internal module beyond standard policy gradient framework). PAVs were trained offline using Monte-Carlo rollouts under prover mu.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical RL runs on Gemma-2B/9B show faster learning curves (Figures reported): PAV-RL achieves higher test accuracy in fewer iterations (6x sample-efficiency) and higher final accuracy than ORM-RL; Pass@N curves improved and PAV-RL discovers solutions that Best-of-256 from SFT cannot.",
            "limitations_or_failure_cases": "PAV-RL performance depends on choice of prover mu used to train the PAV; fixed PAVs trained on certain provers remain useful but the paper did not run experiments where the prover is updated online (left as future work). Fitting errors in learned PAVs limit RL gains; strong provers as mu can lead to near-zero advantages and thus no benefit. ORM-RL can collapse to low-entropy (non-diverse) policies — contrasted as a failure mode of outcome-only RL.",
            "comparison_to_other_methods": "Compared to ORM-RL (outcome-only), PAV-RL is quantitatively superior in sample-efficiency and final accuracy; compared to methods that use Q^pi PRMs as dense rewards (prior works), PAV-RL yields substantially larger gains (prior PRM-RL reported only 1–2% gains).",
            "ablation_study_results": "Experiments tested two fixed provers for PAV training (2B SFT and 9B SFT); surprisingly, a weaker prover (2B) served as the best prover for both 2B and 9B RL runs in some cases, corroborating theory that complementary (not strictly stronger) provers can be best. No experiments performed for dynamically updating prover during RL (left to future work).",
            "uuid": "e8949.1",
            "source_info": {
                "paper_title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Q-PRM",
            "name_full": "Process Reward Models using Q^pi (value-function PRMs)",
            "brief_description": "Process reward models that assign step-level scores equal to the estimated future success probability (Q^pi) under the base policy, trained from Monte-Carlo rollouts — previously-proposed automated PRM approach.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2B, Gemma-9B, Gemma-27B (as baselines)",
            "model_description": "Q^pi PRMs are trained LLM verifiers that predict Monte-Carlo estimates of the base policy's value Q^pi(s,a) for each prefix; used here as baseline PRMs for beam search and as components in comparisons.",
            "reflection_method_name": "Value-function-based PRM (Q^pi)",
            "reflection_method_description": "Estimate Q^pi(s,a) = expected final success when continuing to sample from base policy pi conditioned on prefix; train verifier on (s,a,Q_mc) pairs (Monte Carlo labeled) and use per-step Q estimates to re-rank/prune beams or as dense rewards.",
            "task_name": "Mathematical reasoning (MATH) and beam-search re-ranking",
            "task_description": "Same multi-step math problems; Q^pi PRMs used to guide beam search and re-ranking.",
            "performance_with_reflection": "Beam-search/re-ranking using Q^pi PRM yields modest gains over naive sampling but is less effective than PAV-led effective reward; PAVs achieve ~8x compute-efficiency improvement over beam search using Q^pi in the paper's evaluations (and PAV-as-ORM gave a ~4% gain over ORMs at N=128 in one experiment).",
            "performance_without_reflection": "Baseline best-of-N with ORM or plain sampling: Q^pi PRMs sometimes improve selection but suffer from exploiting high-Q prefixes and less exploration compared to advantage-based scoring.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External verifier predicting Q^pi trained from Monte-Carlo rollouts (supervised regression/cross-entropy on labeled Q estimates), used iteratively in beam search to score prefixes.",
            "number_of_iterations": null,
            "evidence_for_improvement": "The paper shows that Q^pi as a process reward tends to retain high-Q states and can hurt exploration; empirical comparisons show PAV-based effective reward outperforms Q^pi-only PRMs in pass@N and compute-efficiency.",
            "limitations_or_failure_cases": "Q^pi mixes action evaluation with the promise of the previous state and can cause beam search to exploit already-high-Q states instead of exploring promising actions; Q^pi from a very strong prover mu can assign unearned bonuses to trivial actions and leads to A^mu≈0 when used for advantage estimation; prior PRM-RL work using Q^pi reported only tiny gains (1–2%).",
            "comparison_to_other_methods": "Compared against PAV (advantage-based) and ORMs: Q^pi PRMs are less effective at promoting exploration and compute efficiency; Snell et al. used Q^pi for beam search in prior work but PAVs outperform it.",
            "ablation_study_results": "Paper includes experiments showing Q^pi-only beam search (Snell et al.) is outperformed by PAVs and that combining Q^pi with A^mu (effective reward) yields better exploration-exploitation tradeoff.",
            "uuid": "e8949.2",
            "source_info": {
                "paper_title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Best-of-K (BoK)",
            "name_full": "Best-of-K sampling/re-ranking policy",
            "brief_description": "A test-time search method that samples K complete responses from the base policy and returns the one with the highest verifier score (commonly used with ORMs); here also used to define prover policies (BoK(pi)).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2B, Gemma-9B, Gemma-27B (used as base policies and to construct BoK provers)",
            "model_description": "BoK policies are derived procedurally from a base LLM policy by sampling K rollouts and selecting the best according to an ORM (or other verifier).",
            "reflection_method_name": "Generate-many-then-select (Best-of-K)",
            "reflection_method_description": "Generate K full candidate solutions from the base policy and select the highest-scoring one by a verifier (commonly an ORM); in this paper Best-of-K variants (Bo2..Bo32) are used as prover policies mu to compute advantages A^mu for PAV training.",
            "task_name": "Mathematical reasoning (MATH) and used in prover selection experiments",
            "task_description": "Used as both a baseline test-time selection method and as a class of provers to compute per-step advantages (BoK(pi)).",
            "performance_with_reflection": "As provers, BoK with moderate K (e.g., Bo4) produced the best PAV-derived improvements; too large K (e.g., Bo32) led to A^mu≈0 and worse outcomes. BoK as the direct test-time selection baseline (Best-of-N with ORM) is the comparison target: PAV beam search matches or exceeds best-of-N oracle with far less compute.",
            "performance_without_reflection": "Plain Best-of-N with an ORM re-ranker is the baseline; PAV-based beam search outperforms best-of-N (ORM) in accuracy and compute efficiency.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Generate multiple full responses then select using a verifier (non-iterative selection at the end); when used as prover, BoK is used to generate rollouts under mu for advantage estimation.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Empirical ablations over K show Bo4 was often the most effective prover for inducing useful advantages and downstream gains; theoretical analysis (Remark 3.1) argues BoK increases distinguishability for moderate K.",
            "limitations_or_failure_cases": "Large K makes BoK trivially strong leading to near-uniform high Q^mu across prefixes and thus A^mu≈0, which fails to provide useful per-step signals; BoK strength alone is not sufficient—complementarity with pi matters.",
            "uuid": "e8949.3",
            "source_info": {
                "paper_title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "On-the-fly MC Advantage Estimation",
            "name_full": "On-the-fly Monte-Carlo rollout advantage estimation (as in related work)",
            "brief_description": "A method referenced in related work that estimates advantages A^pi online by performing Monte-Carlo rollouts from the current policy during RL to compute per-step advantages without training a PAV.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "On-the-fly Monte-Carlo advantage estimation",
            "reflection_method_description": "Estimate step-level advantages by running Monte-Carlo rollouts during RL from the current policy and computing empirical Q/V differences (used in concurrent work Kazemnejad et al. as cited).",
            "task_name": "Referenced in context of RL for reasoning tasks",
            "task_description": "Used to provide per-step advantage estimates for training without an offline verifier.",
            "performance_with_reflection": "Mentioned: concurrent work (Kazemnejad et al.) uses on-the-fly advantage estimation; paper argues gains from on-policy A^pi are smaller than using an appropriate distinct prover mu and PAVs, but no direct numeric comparison in this paper.",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Compute advantages online via Monte-Carlo rollouts during RL (no separate verifier model).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites concurrent work that uses on-the-fly A^pi but argues theoretically and empirically that distinct-prover advantages (A^mu) provide bigger gains; no direct experiments in this paper comparing on-the-fly A^pi vs PAVs.",
            "limitations_or_failure_cases": "Setting mu=pi (on-policy advantages) leads to near-zero additional gradient benefit in the effective reward and thus little improvement over outcome-only RL, according to the paper's analysis.",
            "uuid": "e8949.4",
            "source_info": {
                "paper_title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's verify step by step.",
            "rating": 2
        },
        {
            "paper_title": "Improve mathematical reasoning in language models by automated process supervision.",
            "rating": 2
        },
        {
            "paper_title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters.",
            "rating": 2
        },
        {
            "paper_title": "Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment.",
            "rating": 2
        },
        {
            "paper_title": "Self-explore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards.",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 1
        }
    ],
    "cost": 0.019105749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning</h1>
<p>Amrith Setlur ${ }^{1,3, <em>}$, Chirag Nagpal ${ }^{1, </em>}$, Adam Fisch ${ }^{2}$, Xinyang Geng ${ }^{2}$, Jacob Eisenstein ${ }^{2}$, Rishabh Agarwal ${ }^{2}$, Alekh Agarwal ${ }^{1}$, Jonathan Berant ${ }^{1,2}$ and Aviral Kumar ${ }^{1,2,3}$<br>${ }^{1}$ Google Research, ${ }^{2}$ Google DeepMind, ${ }^{3}$ Carnegie Mellon University, ${ }^{*}$ Equal contribution, ${ }^{1}$ Equal advising</p>
<h4>Abstract</h4>
<p>A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask: "How should we design process rewards?". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is $&gt;8 \%$ more accurate, and $1.5-5 \times$ more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with $5-6 \times$ gain in sample efficiency, and $&gt;6 \%$ gain in accuracy, over ORMs.</p>
<h2>1. Introduction</h2>
<p>Trained reward models or verifiers are often used to improve math reasoning in large language models, either by re-ranking solutions at test-time (Collins, 2000) or via reinforcement learning (RL) (Uesato et al., 2022). Typically, verifiers are trained to predict the outcome of an entire reasoning trace, often referred to as outcome reward models (ORM) (Cobbe et al., 2021b; Hosseini et al., 2024). However, ORMs only provide a sparse signal of correctness, which can be hard to learn from and inefficient to search against. This challenge is alleviated by fine-grained supervision, in theory. For reasoning, prior works train process reward models (PRMs) that assign intermediate rewards after each step of search (Snell et al., 2024) or during RL. While Lightman et al. (2023) obtains PRM annotations from human raters, this approach is not scalable. More recent works (Luo et al., 2024; Wang et al., 2024) train PRMs to predict automatically-generated annotations that estimate future success of solving the problem, akin to value functions in RL. So far, automated PRMs, especially as dense rewards in RL, only improve by 1-2\% over ORMs (Shao et al., 2024), raising serious doubts over their utility.</p>
<p>To resolve these uncertainties, in this paper, we train PRMs with automated annotations, such that optimizing the dense rewards from trained PRMs can improve a base policy compute- and sampleefficiently, during test-time search and online RL. For this, we first ask: (i) what should the per-step process rewards measure, and (ii) what kind of automated data collection strategy should we use to train PRMs that predict this measure. For (i), conventional belief (Lightman et al., 2023; Uesato et al., 2022)</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Process advantage verifiers (PAV): Process reward for a step is defined as progress (advantage) under the prover policy, i.e., change in prover policy's success rate before and after the step. (a): The base policy samples both correct (1) and incorrect (2) steps but struggles to succeed from either. A strong prover policy completes the solution from both steps, and is unable to adequately reflect progress made by (1) and (2) (both scored 0.0). Conversely, a complementary prover policy distinguishes (1, (2) more prominently (only succeeds from (1). (b,c): Compared to ORMs, PAVs are 5x more compute efficient, $10 \%$ more accurate in test-time search, and 6x more sample efficient, $7 \%$ more accurate for online reinforcement learning (RL).
has been to measure mathematical correctness or relevance of steps. But, it is unclear if this supervision yields the most improvement in the base policy (e.g., a policy may need to generate simpler, repetitive, and even incorrect steps to explore and discover the final answer during test-time search and RL). Our key insight is that per-step, process rewards that measure a notion of progress: change in the likelihood of arriving at a correct final answer before and after taking the step, are effective, for both test-time beam search and online RL. Reinforcing steps that make progress regardless of whether they appear in a correct or incorrect trace diversifies the exploration of possible answers at initial steps, which is crucial when the approach to solve a problem is not clear. Formally, such rewards correspond to per-step advantages of steps from the RL literature (Sutton and Barto, 2018). We empirically show that using advantages in addition to ORM rewards outperforms the typical use of future probabilities of success or $Q$-values (Wang et al., 2024) for both search and RL. This is because, when given a combinatorial space of responses, under bounded computational and sampling constraints, $Q$-values mainly "exploit" states whereas advantages also "explore" steps that make the most progress towards the final answer (Fig. 2).</p>
<p>To answer (ii), we first note that advantages under a poor base policy are $\approx 0$ on most steps, and thus will not be informative for search or RL. In addition, regardless of the strength of the base policy, using its own per-step advantages as process rewards in RL will result in base policy updates equivalent to only</p>
<p>using outcome rewards for RL (since a standard policy gradient algorithm already computes advantages). Hence, we propose to use advantages estimated via rollouts under a different prover policy as process rewards (Fig. 1(a)). How should we choose this prover policy? A natural guess would be to use a very capable prover. However, we show advantages under an overly capable prover policy, that can succeed from any step, fail to distinguish good and bad steps. A similar argument holds for very weak provers.</p>
<p>In theory, we formalize this intuition to define good provers as policies that are complementary to the base policy (i.e., policies with advantages that can contrast steps produced by the base policy sufficiently), while still producing step-level advantages correlated with those of the base policy. For e.g., for Best-of- $K$ policies (Nakano et al., 2021) corresponding to a base policy, we empirically find that provers corresponding to $K&gt;1$ (but not too large) are more capable at improving the base policy. Contrary to intuition, the set of complementary provers also contains policies that are worse than the base policy. To predict the advantages of such provers we train dense verifiers, called process advantage verifiers (PAVs), that accelerate sample and compute efficiency of RL and search.</p>
<p>With the conceptual design of PAVs in place, we prescribe practical workflows for training PAVs and demonstrate their efficacy on a series of 2B, 9B, and 27B Gemma2 models (Gemma Team et al., 2024). PAV training data is gathered by sampling "seed" solution traces from the prover and partial rollouts from the same to estimate the $Q$-value at each prefix of the seed trace. Our workflow prescribes favorable ratios for seed and partial rollouts. Our first set of empirical results show that for an equal budget on test-time compute, beam search against trained PAVs is $&gt;8 \%$ better in accuracy, and $\mathbf{1 . 5 - 5 \times}$ more compute efficient compared to re-ranking complete traces against an ORM (Fig. 1(b)). Dense rewards from PAVs improve the efficiency of step-level exploration during search by pruning the combinatorial space of solutions aggressively and honing in on a diverse set of possible sequences. Finally, we demonstrate for the first time, that using PAVs as dense rewards in RL scales up data efficiency by $6 \times$ compared to only using outcome rewards (Fig. 1(c)). Moreover, base policies trained with PAVs also achieve $8 \times$ better Pass $@ N$ performance (probability of sampling the correct solution in $N$ attempts), and consequently afford a higher ceiling on the performance of any test-time re-ranker. Finally, running RL with PAVs discovers solutions to hard problems that sampling from the SFT policy with a very large budget can't solve.</p>
<h1>2. Preliminaries, Definitions, and Notation</h1>
<p>Following protocols from Lightman et al. (2023); Uesato et al. (2022), a reasoning trace from an LLM consists of multiple logical steps separated by a demarcation token. An outcome reward model (ORM) is a trained verifier that assigns a numerical score after the last step of the trace, and a process reward model (PRM) is a trained verifier that scores each step of the trace individually.</p>
<p>Problem setup and notation. Given a math problem $\boldsymbol{x} \in \mathcal{X}$, our goal is to improve a base policy $\pi$ that samples a response $\boldsymbol{y} \sim \pi(\cdot \mid \boldsymbol{x})$ in the set $\mathcal{Y}$. A response $\boldsymbol{y}$ consists of multiple reasoning steps (maximum $H$ ), separated by a delimiter ('next line' in our case), i.e., $\boldsymbol{y}=\left(a_{1}, a_{2}, \ldots, a_{H}\right)$. Since sampling is auto-regressive, we can view each step as an action taken by the agent $\pi$ in a Markov decision process (MDP) with deterministic dynamics. Specifically, we treat the prefix $\left(\boldsymbol{x}, a_{1}, \ldots, a_{h-1}\right)$ as the current state $\boldsymbol{s}<em h="h">{h}$ and next step $a</em>} \sim \pi(\cdot \mid \boldsymbol{x})$ as the action taken by $\pi$ at $\boldsymbol{s<em h_1="h+1">{h}$, resulting in the next state $\boldsymbol{s}</em>}$. For problem $\boldsymbol{x}$, with ground-truth response $\boldsymbol{y<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{x}}^{\star}$, we can evaluate the accuracy of $\pi$ by running a regular expression match on the final answer (Hendrycks et al., 2021): $\operatorname{Rex}\left(\boldsymbol{y}, \boldsymbol{y}</em>}}^{\star}\right) \mapsto{0,1}$, i.e., accuracy is given by $\mathbb{E<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{y} \sim \pi(\cdot \mid \boldsymbol{x})}\left[\operatorname{Rex}\left(\boldsymbol{y}, \boldsymbol{y}</em>}}^{\star}\right)\right]$. Now, given a dataset $\mathcal{D}=\left{\left(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">{i}, \boldsymbol{y}</em>$ to train ORMs and PRMs.}}^{\star}\right)\right}_{i}$ of problem-solution pairs, the main goal is to learn a good base policy by optimizing this outcome reward on $\mathcal{D}$. Next, we see how we can leverage the final answer verifier Rex available on $\mathcal{D</p>
<p>Outcome reward model (ORM). Given a response $y$, an ORM estimates the ground-truth correctness $\operatorname{Rex}\left(y, y_{x}^{<em>}\right)$. To train such a model we first take problems in $\mathcal{D}$, and collect training data of the form $\left{\left(x, y \sim \pi(\cdot \mid x), \operatorname{Rex}\left(y, y_{x}^{</em>}\right)\right)\right}$. Then we train an ORM that takes as input a problem-response pair $(x, y)$ and predicts $\operatorname{Rex}\left(y, y_{x}^{<em>}\right)$. At test time, when $y_{x}^{</em>}$ is unknown, the ORM is used to score candidate solutions revealed by test-time search. Given a base policy $\pi$, a Best-of- $K$ policy: $\operatorname{BoK}(\pi)$, is a policy that samples $K$ responses from $\pi$, scores them against an ORM, and returns the one with the highest score. Whenever the ORM matches Rex, the performance of $\operatorname{BoK}(\pi)$ is referred to as Pass @K. Furthermore, when the likelihood of $\pi$ solving problem $\boldsymbol{x}$ is $p_{\boldsymbol{x}}$, then for $\operatorname{BoK}(\pi)$ this likelihood is given by the expression: $1-\left(1-p_{x}\right)^{K}$. In general, this is larger than $p_{x}$, making $\operatorname{BoK}(\pi)$ stronger than $\pi$ for $K&gt;1$.
Standard process reward models (PRMs). A PRM scores every step $a_{h}$ in a multi-step response $y \sim \pi$ (e.g., in Lightman et al. (2023) PRMs are trained to score correct steps over incorrect and irrelevant ones). But, unlike ORMs, which only require Rex for data collection, PRM training data requires expensive step-level human annotations. Prior works (Luo et al., 2024; Wang et al., 2024) attempted to scale process rewards automatically by sampling from the model to provide a heuristic understanding of when a step is actually correct. In particular, they evaluate a prefix by computing the expected future accuracy of multiple completions sampled from $\pi$, after conditioning on the prefix, i.e., value function $Q^{\pi}$ (Eq. 1) from RL. Similarly, we define $V^{\pi}\left(s_{h}\right):=\mathbb{E}<em h="h">{a</em>$.} \sim \pi\left(\cdot \mid s_{h}\right)} Q^{\pi}\left(s_{h}, a_{h}\right)$ as value of state $s_{h}$. These works use $Q^{\pi}$ as the PRM that assigns a score of $Q^{\pi}\left(s_{h}, a_{h}\right)$ to the action $a_{h}$, at state $s_{h</p>
<p>$$
Q^{\pi}\left(\underbrace{\left(x, a_{1}, \ldots, a_{h-1}\right)}<em h="h">{\text {state } s</em>}}, \underbrace{a_{h}<em h="h">{\text {action } a</em>}}\right)=\underbrace{\mathbb{E<em h_1="h+1">{a</em>
$$}, \ldots, a_{H} \sim \pi\left(\cdot \mid s_{h}, a_{h}\right)}\left[\operatorname{Rex}\left(\left(a_{1}, \ldots, a_{H}\right), y_{x}^{*}\right)\right]}_{\text {likelihood of future success }</p>
<p>Using PRMs for beam search at test-time. Given a PRM, a natural way to spend test-time compute is to use it as a step-level re-ranker within a beam search procedure (Snell et al., 2024). For each problem, at step 0 , a beam of maximum width $B$, is initialized with a single state consisting of just the problem. At step $h$, a beam contains partial responses unrolled till a set of states or prefixes $\left{s_{i}\right}<em i="i">{i=1}^{B}$. From each state $s</em>\right}}$ in this set, $C$ independent actions or steps $\left{a_{i, j<em i="i">{i=1}^{C}$ are sampled from $\pi\left(\cdot \mid s</em>$ ), and only the states corresponding to the top $B$ values are retained in the beam for the next step.}\right)$, each of which leads to a new state. Process rewards from PRMs assign a score to every new state ( $s_{i}, a_{i, j</p>
<h1>3. How Should we Define Process Rewards and Why?</h1>
<p>Ultimately, we are interested in test-time search and RL methods that can most efficiently and reliably discover solution traces with the correct final answer, thus maximizing Rex. To this end, process rewards should serve as step-level supervision to indirectly maximize outcome-level Rex. Our position contrasts with conventional belief that process rewards should mainly evaluate mathematical correctness or relevance of individual steps (Lightman et al., 2023; Uesato et al., 2022), since LLMs might need to generate trivial or repetitive intermediate steps in order to discover a trace with the correct final answer. With this insight, in this section we approach the design of dense automated step rewards as a form of supervision to be used in conjunction with sparse outcome rewards to improve the base policy.</p>
<p>In an MDP, a starting point to design step-level dense feedback that is eventually meant to optimize a sparse outcome reward Rex is to consider the notion of a potential function ( Ng et al., 1999): in our case, this is a function that summarizes the difference between some statistic of the policy at the future state and the same statistic computed at the current state. By appealing to this framework, in Sec. 3.1, we</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Issues with using Q-values as process rewards: (a): Unlike $A^{\pi}, Q^{\pi}$ mixes action evaluation with the $Q$-value of the previous state. Beam search with $Q^{\pi}$ exploits high-likelihood states, while adding $A^{\pi}$ (e.g., $Q^{\pi}+a A^{\pi}$ in Eq. 5) aids in exploring states reached by making actions that induce progress, i.e., increase likelihood of success. (b): $Q^{\mu}$ from a strong prover $\mu$ can assign unmerited bonuses to trivial actions.
show that advantages - not value functions (Luo et al., 2024; Wang et al., 2024) - that measure a notion of "progress" at each new step are more appropriate for use as dense rewards in search and RL (primarily for exploration). Then in Secs. 3.3 and 3.4, we show that this progress or advantage vakue is measured best under a policy $\mu$, different from the base policy $\pi$. We call this policy $\mu$, the prover policy.</p>
<h1>3.1. Process Rewards Should be Advantages, Not Value Functions</h1>
<p>To understand the relationship to potential functions, we first study test-time beam search, and present some challenges with the reward design of Snell et al. (2024), that uses value function $Q^{\pi}(\boldsymbol{s}, a)$ of the base policy $\pi$ to reward action $a$ at state $s$. Consider the example in Fig. 2(a), where from the 2 states in the beam, we sample 3 actions. If we pick next states purely based on highest values of $Q^{\pi}$, we would be comparing steps sampled from different states (e.g., $a_{1,1}$ vs. $a_{2,1}$ ) against each other. Clearly, a reduction in expected final outcome, i.e., $Q^{\pi}\left(s_{1}, a_{1,1}\right)-V^{\pi}\left(s_{1}\right)$, means that $a_{1,1}$ by itself has a negative effect of -0.05 on the probability of success from $s_{1}$, whereas $a_{2,1}$ has a positive effect of +0.20 from $s_{2}$. However, expanding the beam based on absolute values of $Q^{\pi}$ retains the action that makes negative progress, and removes state $s_{2}$ from the beam (as beam size is 2 ). In other words, $Q^{\pi}$ fails to decouple the "evaluation" of an action (step), from the "promise" shown by the previous state. This will not be an issue for every problem, and particularly not when the beam capacity is unbounded, but under finite computational and sampling constraints, using $Q^{\pi}$ might retain states with potentially unfavorable steps that hurt the overall likelihood of success. If we could also also utilize the progress made by the previous step along with the likelihood of success $Q^{\pi}$ when deciding what to retain in the beam, then we can address this tradeoff.</p>
<p>How can we measure the "progress" made by a step? One approach is to consider the relative increase/decrease in the likelihood of success, before and after the step. This notion is formalized by the advantage (Eq. 2) of a step under policy $\pi$. Furthermore, since advantages can attach either positive or negative values to a step, training the base policy against advantages supervises the base policy when it generates a step that makes progress (where $A^{\pi}&gt;0$ ), and also when it fails to produce one, employing a "negative gradient" that speeds up RL training (Tajwar et al., 2024).</p>
<p>$$
A^{\pi}\left(s_{h}, a_{h}\right):=Q^{\pi}\left(s_{h}, a_{h}\right)-V^{\pi}\left(s_{h}\right)=Q^{\pi}\left(s_{h}, a_{h}\right)-Q^{\pi}\left(s_{h-1}, a_{h-1}\right)
$$</p>
<p>Recall that since we view process rewards as potential functions in the MDP, they can be computed under</p>
<p>any policy $\mu$, which can be the base policy. However, in the above example, reasons for which $Q^{\pi}$ is a seemingly unfit choice for process rewards also apply to $Q^{\mu}$. Nevertheless, we can possibly use advantage under $\mu: A^{\mu}$, which measures the progress made by a step to improve the likelihood of success under $\mu$. In that case, how should we choose this policy $\mu$, that we call the prover policy, and should it be necessarily different from base policy $\pi$ ? Before diving into the choice of $\mu$, we discuss a more pertinent question: how should we use $A^{\mu}$ in conjunction with outcome rewards for improving the base policy $\pi$ ? We will then formally reason about the choice of $\mu$ in Secs. 3.3 and 3.4.</p>
<h1>3.2. Our Approach: Process Advantage Verifiers (PAV)</h1>
<p>For building an approach that uses process rewards $A^{\mu}$ together with the outcome reward Rex to improve the base policy $\pi$, we situate ourselves in the context of improving $\pi$ with online RL. If all we had was access to Rex on $\mathcal{D}$, the standard RL objective is given by:</p>
<p>$$
\ell_{\mathrm{ORM}-\mathrm{RL}}(\pi):=\mathbb{E}<em 1="1">{x \sim \mathcal{D},\left(a</em>\right)\right]
$$}, \ldots, a_{H}\right) \sim \pi(\cdot \mid x)}\left[\operatorname{Rex}\left(\left(x, a_{1}, \ldots, a_{H}\right), y_{x}^{*</p>
<p>Inspired by how reward bonuses (and potential functions) are additive (Bellemare et al., 2016; Ng et al., 1999), one way to use process rewards $A^{\mu}$ is to combine it with the standard RL objective as:</p>
<p>$$
\ell_{\mathrm{PAV}-\mathrm{RL}}^{\pi^{\prime}}(\pi):=\ell_{\mathrm{ORM}-\mathrm{RL}}(\pi)+\alpha \cdot \sum_{h=1}^{H} \mathbb{E}<em h="h">{s</em>} \sim d_{h}^{\pi^{\prime}}} \mathbb{E<em h="h">{a</em>\right)\right]
$$} \sim \pi\left(\cdot \mid s_{h}\right)}\left[A^{\mu}\left(s_{h}, a_{h</p>
<p>The term in red is the difference in likelihoods of success of the prover $\mu$, summed over consecutive steps (a notion of progress). Here, $d_{h}^{\pi^{\prime}}$ denotes the distribution over states at step $h$, visited by the old policy $\pi^{\prime}$ (policy at previous iterate). Following policy gradient derivations (Williams, 1992):</p>
<p>$$
\left.\nabla_{\pi} \ell_{\mathrm{PAV}-\mathrm{RL}}^{\pi^{\prime}}(\pi)\right|<em h="1">{\pi^{\prime}=\pi}=\sum</em>
$$}^{H} \nabla_{\pi} \log \pi\left(a_{h} \mid s_{h}\right) \cdot \underbrace{\left(Q^{\pi}\left(s_{h}, a_{h}\right)+\alpha \cdot A^{\mu}\left(s_{h}, a_{h}\right)\right)}_{\text {effective reward }</p>
<p>At a glance, we can view $Q^{\pi}\left(s_{h}, a_{h}\right)+\alpha A^{\mu}\left(s_{h}, a_{h}\right)$ as the effective reward for step $a_{h}$ when scored against a combination of the outcome evaluation Rex, i.e., $Q^{\pi}$, and process rewards $A^{\mu}$. Thus, we can optimize Eq. 4 indirectly via (a) running beam-search against the effective reward; or (b) online RL where the policy gradients are given by Eq. 5. For either of these, we need access to verifiers that are trained to predict the advantage $A^{\mu}\left(s_{h}, a_{h}\right)$ under the prover. We refer to these verifiers as process advantage verifiers (PAVs). In Sec. 4.2 we describe how to train PAVs, but now we use the above formulation to reason about how to choose prover $\mu$ that is most effective at improving base $\pi$.</p>
<p>We also remark that the term in red resembles prior work on imitation learning via policy optimization (Ross and Bagnell, 2014; Sun et al., 2017), where the main aim is to learn a policy $\pi$ that imitates the prover $\mu$, or to improve upon it to some extent. Of course, this is limiting since our goal is to not just take actions that perform at a similar level as $\mu$, but to improve the base policy even further, and using a combination of $Q^{\pi}$ and $A^{\mu}$ is critical towards this goal.</p>
<p>How should we choose the prover $\mu$ ? Perhaps a natural starting point is to set the prover to be identical to the base policy, i.e., $\mu=\pi$, which produces process rewards that prior works have considered Shao et al. (2024). However, setting $A^{\pi}=A^{\mu}$ in Eq. 5 results in exactly the same policy gradient update as only optimizing outcome evaluation Rex. Moreover, for a poor base policy $\pi$, where $Q^{\pi} \approx 0$ on most states,</p>
<p>the term $A^{\pi}$ would also be $\approx 0$, and hence running beam search with the effective rewards would not be informative at all. Hence, a better approach is to use a different prover policy, but a very weak prover $\mu$ will likely run into similar issues as a poor base policy. We could instead use a very capable prover $\mu$, but unfortunately even this may not be any better than optimizing only the outcome reward either. To see why, consider a scenario where $\pi$ 's response contains an intermediate step that does not help make progress towards the solution (e.g., $\pi$ simply restates the question, see Fig. 2(b)). Here, $Q^{\mu}$ for a capable prover before and after this irrelevant step will be identical since $\mu$ can succeed from either step. This means that $\mu$ fails to distinguish steps, resulting in $A^{\mu} \approx 0$ in most cases. Training with this process reward during RL will then lead to gradients that are equivalent to those observed when purely optimizing $\ell_{\text {ORM-RL }}$. In fact, empirically, we observe that online RL with $Q^{\mu}$ from strong provers leads to polices that only produce re-phrasings of the question (App. G) and do not succeed at solving the question. Clearly, any policy different from the base policy cannot serve as a prover. So, how do we identify a set of good provers? Can they indeed be weaker than the base policy? We answer next.</p>
<h1>Takeaway: What should process rewards measure during test-time search and online RL?</h1>
<ul>
<li>Process rewards should correspond to progress, or advantage, as opposed to absolute $Q$-values, for a better explore-exploit tradeoff during beam search and online RL.</li>
<li>Advantages should be computed using a prover policy, different from the base policy.</li>
</ul>
<h3>3.3. Analysis in a Didactic Setting: Learning a Planted Sub-sequence</h3>
<p>In this section, we aim to characterize prover policies that are effective in improving the base policy. To do so, we first introduce a didactic example, representative of real reasoning scenarios to illustrate the main intuition. Then, we will formalize these intuitions in the form of theoretical results.</p>
<p>Didactic example setup. Given an unknown sub-sequence $\boldsymbol{y}^{\star}$ consisting of tokens from vocabulary $\mathcal{V}:={1,2, \ldots, 15}$, we train a policy $\pi$ to produce a response which contains this sub-sequence. The task completion reward is terminal and sparse, i.e., $r\left(\boldsymbol{y}, \boldsymbol{y}^{\star}\right)=1$ for a $\boldsymbol{y}$ if and only if $\boldsymbol{y}^{\star}$ appears in $\boldsymbol{y}$. By design, the reward $r\left(\boldsymbol{y}, \boldsymbol{y}^{\star}\right)$ resembles outcome reward $\operatorname{Rex}\left(\boldsymbol{y}, \boldsymbol{y}_{\boldsymbol{y}}^{\star}\right)$ in Sec. 2. The prover policy $\mu$ is a procedural policy, parameterized by a scalar $\gamma&gt;0$ (details in App. B). As $\gamma$ increases, the performance of $\mu$ improves and $\rightarrow 1$ as $\gamma \rightarrow \infty$. For simplicity, we assume oracle access to ground-truth $A^{\mu}$ and $Q^{\pi}$, and alleviate errors from learned verifiers approximating these values.
(1) RL with effective reward $Q^{\pi}+\alpha A^{\mu}$ is $10 \times$ more sample-efficient than only outcome reward. In Fig. 3(a), we first note that training $\pi$ with this effective reward under a prover $\mu$ with strength $\gamma=10$, produces optimal performance ( $100 \%$ accuracy) in 350 iterations, despite starting from a mediocre initialization for $\pi(\gamma=5.0)$. Training with only outcome reward is ineffective. More importantly, in Fig. 3(b), we note that effective rewards only help for a set of provers, in $\gamma \in[8.0,15.0]$. Outside this range, we observed advantages $A^{\mu}$ were close to 0 on most states, either because $\mu$ was poor (small $\gamma$ ) and was unable to generate $\boldsymbol{y}^{\star}$ even when $\pi$ got the sequence partially correct, or because $\mu$ was strong (large $\gamma$ ) that it generated $\boldsymbol{y}^{*}$ with almost equal likelihood from all prefixes.
(2) Effective reward improves Pass @N by $5 \times$ over only outcome reward. We report the "Pass @N" performance in Fig. 3(c), which measures the maximum reward $r$ across $N$ traces sampled i.i.d. from $\pi$ and hence, represents the ceiling on the performance of any test-time search method that picks a single response from multiple draws (e.g., as in Best-of-N). For a policy trained with the effective reward for 100 iterations, the Pass @N performance grows $5 \times$ faster with $N$, compared to the policy trained with only</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Results for our didactic analysis: (a): We train base policy via RL with either effective reward $Q^{\pi}+\alpha A^{\mu}$, or the typical $Q^{\pi}$ (computed via Monte-Carlo sampling). (b): We vary the strength $\gamma$ of the prover $\mu$ used to compute advantages $A^{\mu}$ in the effective reward, and plot the base policy accuracy averaged over the RL run. (c): We plot the max score out of $N$ responses (Pass @N) sampled i.i.d. from an undertrained base policy (iter 100).
the outcome reward. Due to only sparse feedback, the latter policy does not learn to sample partially correct $\boldsymbol{y}^{\star}$, whereas a policy trained with the effective reward produces partially correct $\boldsymbol{y}^{\star}$, and is able to sample the complete $\boldsymbol{y}^{\star}$ with higher likelihood during Pass @N.</p>
<h1>Takeaway: Online RL with process rewards from different prover policies.</h1>
<p>Effective rewards $Q^{\pi}+\alpha A^{\mu}$ from prover $\mu$ : (i) improve sample efficiency of online RL, and (ii) yield policies with better Pass @N performance, over using only outcome rewards. But, advantages of very capable or poor $\mu$ do not improve base policy beyond outcome rewards.</p>
<h3>3.4. Theory: Provers Complementary to the Base Policy Boost Improvement</h3>
<p>From our didactic analysis, it is clear that process rewards $A^{\mu}$ under different provers $\mu$ disparately affect the base policy that optimizes $Q^{\pi}+\alpha A^{\mu}$ via online RL. We now present a formal analysis of why this happens and characterize a class of provers that can guarantee non-trivial improvements to the base policy. For simplicity, we assume oracle access to $Q^{\pi}, A^{\mu}$ at every state-action pair $\left(s_{h}, a_{h}\right)$ and prove our result in the tabular RL setting, where the policy class is parameterized using the softmax parameterization in Agarwal et al. (2021). Proofs for this section are in App. F.</p>
<p>Main intuitions. We expect a prover $\mu$ to improve a base policy $\pi$ only when $\boldsymbol{\mu}$ is able to distinguish different actions taken by $\pi$, by attaining sufficiently varying advantage values $A^{\mu}\left(\boldsymbol{s}<em h="h">{h}, a\right)$ for actions $a$ at state $\boldsymbol{s}</em>}$. This can be formalized under the notion of sufficiently large variance across actions, $\mathbb{V<em h="h">{a \sim \pi}\left[A^{\mu}\left(s</em>$ are reasonably aligned on their assessment of steps from $\pi$.}, a\right)\right]$. In that case, can we simply use a policy with large advantage variance under any measure? No, because when the prover $\mu$ ranks actions at a given state very differently compared to the base policy $\pi$ (e.g., if $A^{\mu}$ and $A^{\pi}$ are opposite), then effective rewards $Q^{\mu}+\alpha A^{\pi}$ will be less reliable due to conflicting learning signals. Thus, we want $\mathbb{E}_{\pi}\left[\left\langle A^{\mu}, A^{\pi}\right\rangle\right]$ to not be too negative, so that $\boldsymbol{\mu}$ and $\boldsymbol{\pi</p>
<p>In Theorem 3.1, we present our result on policy improvement where the base policy is updated with natural policy gradient (Kakade, 2001a): $\pi_{t+1}\left(a \mid \boldsymbol{s}<em h="h">{h}\right) \propto \exp \left(\gamma \cdot\left(Q^{\pi}\left(\boldsymbol{s}</em>, a\right)\right)\right)$. We note that in this idealized update rule, swapping $Q$ values (of $\mu$ or $\pi$ ) with advantages does not affect the update since we assume access to all possible actions when running the update. Nonetheless, despite this simplifying assumption, the analysis is able to uncover good choices for the prover policy $\mu$ for computing process}, a\right)+A^{\mu}\left(\boldsymbol{s}_{h</p>
<p>reward $A^{\mu}$, and is orthogonal to the design consideration of advantages or $Q$-values as process rewards that we have discussed so far in this paper. Theorem 3.1 formalizes our intuition by showing that policy improvement at iteration $t$, grows as the variance in $A^{\mu}$ values increases (higher distinguishability) and reduces when $A^{\mu}$ and $A^{\pi}$ become extremely misaligned. This will then allow us to discuss a special case for the case of Best-of-K policies as provers as an immediate corollary.</p>
<p>Theorem 3.1 (Lower bound on policy improvement; informal). For base policy iterate $\pi_{t}$, after one step of policy update, with learning rate $\gamma \ll 1$, the improvement over a distribution of states $\rho$ :</p>
<p>$$
\mathbb{E}<em t_1="t+1">{s \sim \rho}\left[V^{\pi</em>}}(s)-V^{\pi_{t}}(s)\right] \geq \gamma \cdot \underbrace{\mathbb{E<em _pi__t="\pi_{t" _sim="\sim" a="a">{s \sim \rho} \mathbb{V}</em>}}\left[A^{\mu}(s, a)\right]<em _rho="\rho" _sim="\sim" s="s">{\text {distinguishability from } \mu}+\gamma \cdot \underbrace{\mathbb{E}</em>} \mathbb{E<em t="t">{a \sim \pi</em>}}\left[A^{\mu}(s, a) A^{\pi_{t}}(s, a)\right]<em t="t">{\text {alignment between } \pi</em>
$$} \text { and } \mu</p>
<p>It may seem that the base policy $\pi$ can only learn from an improved prover $\mu$, but our result shows that a weak prover can also amplify a stronger base policy, since a weak prover $\mu$ may have a lower average of $Q^{\mu}$ under its own measure, but still have higher variance across $Q^{\mu}$ (compared to $Q^{\pi}$ ) when evaluated under $\pi$ (see Proposition F. 1 in App. F. 5 for formal discussion). This tells us that rewarding progress under a prover is different from typical knowledge distillation or imitation learning algorithms (Hinton, 2015; Rusu et al., 2015) that in most cases remain upper bounded by the performance of the stronger teacher. So provers cannot be characterized purely by strength, what is a class of provers that is a reasonable starting point if we were to improve any base policy $\pi$ ?
The policy class of "Best-of-K" (computed over base policies) contain complementary provers. A good starting point to identify good provers for a base policy $\pi$, is the class of Best-of-K policies or $\operatorname{BoK}(\pi)$. Recall from Sec. 2 that the performance of $\operatorname{BoK}(\pi)$ increases monotonically with $K$. Applying Theorem 3.1 to this class, we arrive at Remark 3.1 that recommends using $\operatorname{BoK}(\pi)$ with $K&gt;1$ as a prover policy for a poor base policy $\pi$. However, $K$ cannot be too large always since when $Q^{\pi}(s, a) \approx 1$, increasing $K$ too much can hurt distinguishability of different steps at that state. In the next section, we empirically note that the policies in the class of $\operatorname{BoK}(\pi)$ indeed induce different performance gains when used as prover policies, and we find Bo4 to be a good choice for test-time search over most base policies.</p>
<p>Remark 3.1. When $Q^{\pi}(s, a)=O(1 / K), \forall s, a$, using $\operatorname{BoK}(\pi)$ as a prover for base $\pi$ improves distinguishability (and improvement) by $\Omega\left(K^{2}\right)$, and make alignment worse at most by $O(K)$.</p>
<p>Takeaway: Formal characterization of good prover policies that improve the base policy.
Provers with advantages that can distinguish actions taken by the base policy (more strongly than the base policy itself) but are not too misaligned from the base, boost improvements on each update of the base policy. We call such policies complementary provers. $\operatorname{BoK}(\pi)$ for any base policy $\pi$ for $K&gt;1$ can provide a good starting choice of prover policies.</p>
<h1>4. Results: Scaling Test-Time Compute with PAVs</h1>
<p>Now, we study how process verifiers can scale up test-time compute. While our derivations from Sec. 3.2 were with RL, we can also use the effective reward $Q^{\pi}\left(s_{h}, a_{h}\right)+\alpha \cdot A^{\mu}\left(s_{h}, a_{h}\right)$ for running beam search over intermediate steps sampled from base policy $\pi$. To do so, we train a process advantage verifier to predict $A^{\mu}$, along with a process reward model $Q^{\pi}$. PAV training is done using procedures discussed in Sec. 4.2. While the candidates of the beam are selected using a combination of both the PAV and the PRM $Q^{\pi}$,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 | For test-time search, PAVs are $8-10 \%$ more accurate and $1.5-5 \times$ more compute efficient over ORMs: On samples from (a) Gemma-2B , (b) 9B , and (c) 27B SFT policies, we run test-time beam search with the estimate of effective reward $Q^{\pi}+\alpha A^{\mu}$ (PAV), where $\mu$ is the Bo4 $(\pi)$ policy. We compare beam search performance with best-of-N, re-ranking with a trained outcome verifier (ORM), or the oracle Rex (Pass @N).
the final candidate is selected using the outcome reward prediction from $Q^{\pi}$ itself (i.e., we repurpose the PRM representing $Q^{\pi}$ as an ORM). For clarity, we abuse notation and refer to the estimated effective reward (ORM $+\alpha$ PAV) as PAV directly.</p>
<p>Setup. We finetune Gemma 2B, 9B, and 27B (Gemma Team et al., 2024) on MATH (Hendrycks et al., 2021) via supervised fine-tuning (SFT) to get three base policies. The set of provers consists of the three base SFT policies themselves as well as their best-of-K policies for different values of $K \in\left{2^{0}, \ldots, 2^{5}\right}$. Additional details for the experiments in this section are in App. C.</p>
<h1>4.1. PAVs Scale Test-Time Compute by $5-10 \times$ Over ORMs</h1>
<p>Result 1: PAVs are more compute efficient than ORMs. In Fig. 4, we plot the performance of beam search with PAVs for different sizes of the beam $N$, and compare it with best-of- $N$ using ORMs, i.e., sampling $N$ complete solutions from the base policy and returning the one with the highest ORM score. To compare PAVs and ORMs, we evaluate the compute efficiency of PAVs over ORMs, given by the ratio of total compute needed by PAVs to obtain the same performance as running best-of-128 with ORM. Even when accounting for the fact that running beam search with PAVs does require additional compute per solution trace (since each element in the beam samples $C=3$ next steps, before scoring and pruning the beam), PAVs are able to scale the compute efficiency by $\mathbf{1 0 \times}$ over ORMs for Gemma-2B, 9B base models, and by $5 \times$ for Gemma-27B model. We use $\operatorname{BoK}(\pi)$ with $K=4$ as the prover policy for all base policies $\pi$.</p>
<p>We also compare performance with beam search using process verifiers that only predict $Q^{\pi}$, and best-of-N where the ORM is replaced with PAV (PAV-as-ORM). At $N=128$, similar to Luo et al. (2024), we note a similar gain of $4 \%$ for "PAV-as-ORM" Fig. 5(a) over only ORMs, for base Gemma-9B $\pi$. When comparing beam search with $Q^{\pi}$ (Snell et al., 2024), we find that PAVs scale compute efficiency by $8 \times$. Evidently, advantages from the prover in the effective reward positively impact the beam search. Why does $A^{\mu}$ help, and for what choice of the prover $\mu$ ?</p>
<p>Result 2: Beam search with too weak/strong provers is sub-optimal. In Fig. 5(b), for the setting when the base policy $\pi$ is a Gemma-2B SFT model, we compare beam search with PAVs where the provers are given by $\operatorname{BoK}(\pi)$, for different values of $K$. Recall that as $K$ increases, $\operatorname{BoK}(\pi)$ becomes stronger. Corroborating our analysis in Sec. 3.4, our results show that neither too weak (Bo2) or too strong (Bo32) provers perform best. Instead, across all values of $N$, we find Bo4 to be dominant. The advantage values</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | Comparing PAVs with search baselines and ablating over the prover policy: (a): We compare beam search over Gemma 9B SFT, using either effective reward (PAV), or $Q^{\pi}$ (Snell et al., 2024), and report best-of-N performance where the re-ranker is either the ORM or PAV-as-ORM. (b): For the base Gemma 2B SFT policy, we run beam search with the effective reward where the prover is $\operatorname{BoK}(\pi)$ for different values of $K$. In both $(a),(b)$ the x-axis scales the size of the beam or $N$ for best-of-N. (c): For each base policy in the set: Gemma 2B, 9B, 27B policies, we run beam search with PAVs (beam size of 16) where the prover is another policy from the same set.
$A^{\mu} \approx 0$ on all steps for very large $K$, since $Q^{\mu}\left(\boldsymbol{s}<em h="h">{h}, a</em>}\right)=1-\left(1-Q^{\pi}\left(\boldsymbol{s<em h="h">{h}, a</em> \rightarrow 1$ on all steps, as we increase $K$. Hence, in order to succeed we need an intermediate-level prover policy.}\right)\right)^{K</p>
<p>We make similar observations in Figure 5(c) where we use the three base policies (Gemma 2B/9B/27B) as provers for training PAVs. In this scenario, we evaluate beam search with PAVs at $N=16$ on top of different base policies. We find that for the 2B and 9B base models, the 9B and 27B provers are most effective respectively, whereas for the 27B model, surprisingly a weaker 9B policy is more effective than the stronger 27B model. The weaker model presumably offers a complementary signal that distinguishes between different actions taken by 27B, aligning with our theoretical observations in Sec. 3.4.</p>
<p>Result 3: Advantages from the prover policy enable exploration. As discussed in Sec. 3.1, advantage $A^{\mu}$ measures the progress made by an action agnostic of the value of the previous state, where as $Q^{\pi}$ measures the promise of a particular state. Given a finite capacity beam, our effective reward (Eq. 5), which linearly combines $Q^{\pi}$ and $A^{\mu}$ induces a better tradeoff between exploring new prefixes (states) from where progress can be made and exploiting currently known prefixes with high Q-values. Exploration at
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 | (a): Beam search with PAVs improves exploration efficiency (higher Pass@N), over typical PRMs. (b): Performance of beam search over Gemma 9B SFT for PAVs trained on datasets with different $n_{\mathrm{mc}} / n_{\mathrm{cov}}$.</p>
<p>initial steps is critical to ensure that the beam at later steps covers diverse partial rollouts each with a high likelihood of producing the correct answer. Thus over-committing to the beam with actions from the same state, regardless of the progress made by each can prove to be sub-optimal over a selection strategy that balances rewarding previous actions $A^{\mu}$ and current states $Q^{\pi}$. Indeed, we observe in Fig. 6(a), beam search with PAV enhances pass@N performance vs. beam search with $Q^{\pi}$ and i.i.d. sampling.</p>
<h1>Takeaways: Scaling test-time compute with process advantage verifiers.</h1>
<ul>
<li>Beam search with PAVs boosts accuracy by $&gt;8 \%$ \&amp; compute efficiency by $1.5-5 \mathrm{x}$ over ORMs.</li>
<li>Utilizing Best-of-K policies (corresponding to the base policy) as provers induce better exploration to maximize outcome reward. Optimal provers for a base policy appear at $K&gt;1$.</li>
</ul>
<h3>4.2. How to Collect Data to Train PAVs?: PAV Training Data Scaling Laws</h3>
<p>We now describe the procedure for training outcome verifiers and PAVs. We can learn to predict $Q^{\pi}$ for a policy $\pi$ (similar for $Q^{\mu}$ ) by finetuning LLMs with a cross-entropy loss on the following data with triplets $\left(\boldsymbol{s}, a, Q_{\mathrm{mc}}^{\pi}(\boldsymbol{s}, a)\right)$. To collect this data, we first sample $n_{\mathrm{cov}}$ "seed" rollouts from the base or prover policy respectively for ORM and PAVs, to promote coverage over prefixes and steps. Then we sample $n_{\mathrm{mc}}$ additional rollouts, conditioned on each prefix in the seed rollout to compute the Monte-Carlo estimate of $Q^{\pi}$ at each prefix. In Fig. 6(b) we plot the beam search performance of PAVs trained with different ratios of $n_{\mathrm{mc}} / n_{\mathrm{cov}}$, as we scale the total dataset size. Here, the beam size is fixed to 128 and the base policy is the Gemma 9B SFT policy and prover is Bo4 policy. We find that under low sampling budgets, optimizing for coverage $\left(n_{\mathrm{cov}}&gt;n_{\mathrm{mc}}\right)$ is better for performance, and when budget is higher, reducing label noise in $Q_{\mathrm{mc}}^{\pi}$ by setting $n_{\mathrm{mc}}&gt;n_{\mathrm{cov}}$ gets us more improvements. In addition, we also spend some initial sampling budget is spent to identify "high value" states where $Q^{\pi}$ is larger than a threshold, and identify the first step with low $Q^{\pi}$ on an incorrect partial rollout from this state. We found this strategy to scale better with dataset size, as we discuss in App. D.</p>
<h2>5. Results: Scaling Dense-Reward RL with PAVs</h2>
<p>We can also use PAVs to train policies via online reinforcement learning (RL), by using the effective reward $Q^{\pi}+\alpha A^{\mu}$ as dense, per-step rewards. We compare the sample efficiency of PAV-RL (i.e., $\ell_{\text {PAV-RL }}$ in Eq. 4) with standard ORM-RL (i.e., $\ell_{\text {ORM-RL }}$ in Eq. 3) on Gemma 2B and Gemma 9B SFT models, which are further optimized via rejection finetuning (RFT) (Yuan et al., 2023), before using them to initialize RL. To our knowledge, no prior work has successfully demonstrated the use of dense per-step feedback with a process reward model for RL, and we present the first significant set of results establishing the efficacy of this approach. We show that PAV-RL is much more sample-efficient, and enjoys a higher ceiling on the performance of any test-time re-ranker. Additional details for the experiments are in App. E.</p>
<p>Result 1: PAV-RL is $&gt;7 \%$ better than ORM-RL in test accuracy, and $6 \times$ sample efficient. In Fig. 7(a), we report the test accuracies of Gemma 2B and 9B models trained with SFT, RFT, ORM-RL and PAV-RL. PAV-RL improves the RFT policy by $11 \%$ for 2 B , and $15 \%$ for 9 B , with $&gt;7 \%$ gain over ORM-RL in both cases. Not only do the effective rewards from PAV improve the raw accuracy after RL, this higher accuracy is attained $6 \times$ faster (see Fig. 7(b)) for the 2B run and similarly for the 9B RL run (Fig. 7(c)). For both 2B and 9B, RL runs, we experiment with two options for the prover policy: (i) 2B SFT policy; and (ii) 9B SFT policy. While both of these provers rapidly become weaker than the base policy within a few gradient steps of RL, a fixed PAV trained with each of these provers is able to still sustain performance gains in RL. More interestingly, we find that the 2B SFT policy serves as the best choice of the prover for both 2B and</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure $7 \mid$ PAVs as dense rewards in RL improve sample efficiency compared to ORMs, along with gains on raw accuracy: (a) We report the performance of a base policy trained using RL with effective rewards (PAV-RL), or only outcome rewards (ORM-RL), and baselines SFT, RFT. (b,c): Across training iterations, we report the test performance of policies trained with PAV-RL and ORM-RL, on Gemma 2B and 9B SFT base policies.</p>
<p>9B policies. This observation that a weak prover can still improve the base policy corroborates our results in the didactic setup and our analysis in Sec. 3.4. While we were not able to run experiments where the prover policy is dynamically updated on the fly, we believe that updating the prover through the process of RL training should only amplify these benefits.</p>
<p>Result 2: PAV-RL achieves higher performance ceiling on test-time re-ranking. In Fig. 8(a), for Gemma 2B, we plot the Pass @N performance for each method, and find (i) Pass @N is higher ( $&gt;7 \%$ ) for PAV-RL, compared to ORM-RL, for any $N \leq 128$; and (ii) the rate at which Pass @N improves for PAV-RL is higher than ORM-RL. Both trends are consistent with our observations on the didactic example in Sec. 3.3. Notably, for $N \geq 64$, ORM-RL is worse than the SFT policy, perhaps due to lower entropy over the distribution at the next step resulting in non-diverse candidates. Why does PAV-RL produce diverse candidates, and does not suffer from the low diversity problem in ORM-RL? We answer this with a key insight on how the primary benefit of PAVs is to promote efficient exploration.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 | (a): For the policies trained in (a) we report the best-of-N performance where the oracle reward Rex is used to rank $N$ candidates sampled from the base policy (Pass @N). (b): Amongst hard problems that remain unsolved by Best-of-256 over the base SFT policy, we check how many are solved by Best-of-N over PAV-RL or ORM-RL. PAV-RL is able to solve a substantially more problems than what ORM-RL was able to solve.</p>
<p>Result 3: PAVs improve exploration and discover correct solutions to novel problems. An outcome reward model rewards downweight all steps in an incorrect rollout equally during RL, whereas the effective reward $Q^{\pi}+\alpha A^{\mu}$ in PAVs, up-weights steps that make progress under the prover, even when the complete rollout is incorrect. This increases the coverage over individual steps that can improve the likelihood of the base policy to succeed (since the prover policy is not too misaligned with the base policy). These can now be proposed by the base policy at a given prefix. This mechanism for exploration is analogous to test-time search we discussed in Sec. 4.1. Hence, the directed supervision from PAVs improves sample-efficiency throughout the course of training (Fig. 7(c)). In fact, we also find that combining the PAV-RL policy with test-time beam search is able to solve a substantially larger number of new problems within smaller compute budgets $(N=16,32)$ that the SFT policy cannot solve with a much larger budget $N=256$ (Fig. 8(b)).</p>
<h1>Takeaway: RL with process advantage verifiers (PAVs) as dense rewards</h1>
<ul>
<li>Using trained PAVs as dense rewards in RL boosts scales sample efficiency by $5-6 \times$, compared to only using sparse ORM rewards, and results in policies with a higher Pass @N performance.</li>
<li>Advantages from a complementary prover policy improves the sample efficiency of exploration in RL, and produces policies that can discover solutions to hard novel questions.</li>
</ul>
<h2>6. Related Work</h2>
<p>We briefly discuss some key related works here, and leave the detailed discussion for App. A. To address issues of sparse feedback in ORMs (Cobbe et al., 2021b), recent works (Lightman et al., 2023; Uesato et al., 2022) trained process reward models (PRMs) to densely predict incorrect steps in a multi-step reasoning trace. Since human data collection for process labels is not scalable enough, recent work (Luo et al., 2024; Wang et al., 2024) used automated supervision to annotate steps with $Q$ values under the base policy, i.e., the PRMs score a step with the likelihood of future success, when continuing to sample from the step. While $Q$-value PRMs in Lightman et al. (2023); Luo et al. (2024) were mainly used as verifiers for re-ranking, Snell et al. (2024) used them for test-time beam search. Shao et al. (2024) uses PRMs for RL but found a gain of only $1-2 \%$ with PRMs. In our work, we question solely relying on $Q$-values or advantages of the base policy, and find that measuring progress (i.e., advantages) under a different prover policy can amplify exploration, thus boosting test-time search and RL. To our knowledge, we are the first to show substantial gains in compute and sample efficiency with PRMs. Our methodology for data collection is similar to (Hwang et al., 2024; Setlur et al., 2024) (i.e., identify "first pits" in reasoning traces), these works only use it to collect preference pairs. Beyond all of these, we also characterize which policy to use for computing advantages.</p>
<p>Concurrently to us, akin to the methodology in Hwang et al. (2024); Setlur et al. (2024), Kazemnejad et al. (2024) optimize the base policy $\pi$ with online RL, where the dense step-level rewards correspond to advantages $A^{\pi}$ under the base policy $\pi$ itself. This is a special case of our setting, where the prover policy $\mu=\pi$, but as we note in Sec. 3.2, setting $\mu=\pi$ in our effective reward (Eq. 5) results in exactly the same policy gradient updates as only optimizing the outcome reward. Since Kazemnejad et al. (2024) use "on-the-fly" Monte-Carlo rollout estimation to estimate advantages, they are able to avoid estimation errors in the process reward model. Nonetheless, our theoretical result and the didactic example (both of which assume access to perfect advantage estimates) show that gains from this approach are significantly smaller than using an appropriate prover policy, which is distinct from the base policy.</p>
<h1>7. Discussion and Conclusion</h1>
<p>We began our exposition with the following question: how to define process rewards such that optimizing the base policy against process rewards ultimately improves the outcome level correctness of the final answer? Our key finding is that process rewards defined as advantages of a prover policy, distinct from the base policy improve the efficiency of exploration for steps sampled from the base policy during test-time search and online RL. This improved exploration in turn leads to discovery of better solutions, resulting in a higher accuracy on the math reasoning task. We also formally characterized the set of good prover policies as policies with step-level advantages that meaningfully contrast steps generated by the base policy, while still producing step-level advantages that are aligned with the base policy. Having trained process advantage verifiers (PAVs) to predict advantages under the prover policy, we empirically observed that test-time search against the trained PAVs improve the compute-efficiency of search by $1.5-5 \times$, and accuracy of search by over $8 \%$ compared to running best-of- $N$ against an ORM. Next, we present one of the significant results that validate the use of dense supervision when optimizing the base policy with online RL. Specifically, we show that dense online RL with rewards from our trained PAVs, improves sample efficiency of online RL by $5-6 \times$, and results in an accuracy gain of over $6 \%$.</p>
<p>Limitations. Despite the promise of our results, there are several limitations to our work that present important avenues for future research. First, while we can easily compute the right hand side of our result in Theorem 3.1 to understand whether a given prover policy will improve a fixed base policy, it is unclear how to automatically design a flexible class of optimal (or very good) prover policies for a sequence of base policy iterates. Perhaps simultaneously optimizing the prover and the base policy (in a two-player game) might provide for an approach to obtain the best prover during RL, but this is largely an open question. Second, since inevitably learning a process advantage verifier (PAV) will incur fitting errors and this upper bounds peformance of our method. Fitting errors can be circumvented if our approach if we can simply run rollouts from prover policies during online RL or search to estimate advantages without training verifiers, and extending our approach to this setup is a good avenue for future work.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to thank Charlie Snell, Yi Su, Katherine Heller, and Virginia Smith for feedback on an earlier version of this paper. We also thank Ahmad Beirami, Sergey Levine, Victor Veitch, Idan Shenfeld, Arian Hosseini, Stephen Pfohl, Xiangyu Qi, Tianhe Yu, and Christina Baek for technical discussions. AS and CN also thank Preston Robinette, Sho Kannan, Tianze Shi, Diana Mincu, Hritik Bansal, and Liangchen Luo for code, infrastructure and data analytics support.</p>
<h2>References</h2>
<p>A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1-76, 2021.
T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017.
M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages $1471-1479,2016$.</p>
<p>X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.
J. D. Chang, K. Brantley, R. Ramamurthy, D. Misra, and W. Sun. Learning to generate better than your llm. arXiv preprint arXiv:2306.11816, 2023.
K.-W. Chang, A. Krishnamurthy, A. Agarwal, H. Daumé III, and J. Langford. Learning to search better than your teacher. In International Conference on Machine Learning, pages 2058-2066. PMLR, 2015.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021a.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b.
M. Collins. Discriminative reranking for natural language parsing. In Proceedings of the International Conference on Machine Learning, 2000.</p>
<p>Gemma Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
M. Germain, K. Gregor, I. Murray, and H. Larochelle. Made: Masked autoencoder for distribution estimation. In International conference on machine learning, pages 881-889. PMLR, 2015.
A. Havrilla, Y. Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.
G. Hinton. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
A. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.
H. Hwang, D. Kim, S. Kim, S. Ye, and M. Seo. Self-explore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards. arXiv preprint arXiv:2404.10346, 2024.
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 267-274, 2002.
S. M. Kakade. A natural policy gradient. In Advances in neural information processing systems, volume 14. Advances in neural information processing systems, 2001a.
S. M. Kakade. A natural policy gradient. Advances in neural information processing systems, 14, 2001b.
A. Kazemnejad, M. Aghajohari, E. Portelance, A. Sordoni, S. Reddy, A. Courville, and N. L. Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024.</p>
<p>H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.
L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu, Y. Zhu, L. Meng, J. Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024.
Q. Ma, H. Zhou, T. Liu, J. Yuan, P. Liu, Y. You, and H. Yang. Let's reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint arXiv:2310.10080, 2023.
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pages 278-287, 1999.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.
S. Ross and J. A. Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.
A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.
A. Setlur, S. Garg, X. Geng, N. Garg, V. Smith, and A. Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532, 2024.
Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
A. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. Parisi, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023a.
I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530. IEEE, 2023b.
C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.
W. Sun, A. Venkatraman, G. J. Gordon, B. Boots, and J. A. Bagnell. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In International conference on machine learning, pages 3309-3318. PMLR, 2017.</p>
<p>R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. The MIT Press, second edition, 2018.
R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.
F. Tajwar, A. Singh, A. Sharma, R. Rafailov, J. Schneider, T. Xie, S. Ermon, C. Finn, and A. Kumar. Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data, 2024.
J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.
P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.
Y. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024.
F. Yu, A. Gao, and B. Wang. Outcome-supervised verifiers for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023.
Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.
E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.
L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024.</p>
<h1>Appendices</h1>
<h2>A. Additional Related Work</h2>
<p>In this section, we highlight works from four relevant streams, expanding on discussion in Section 6. First, we look at works that train verifiers to provide outcome level feedback (Cobbe et al., 2021b; Hosseini et al., 2024; Singh et al., 2023b; Zelikman et al., 2022) on the correctness of the full response (ORM). Here, the trained ORMs are mainly used for test-time search (best-of- $N$ ). Next, we look at works that alleviate issues with sparse feedback in ORMs, and instead train process reward models (PRMs), that can perform credit assignment. PRMs are trained either through human annotations (Lightman et al., 2023; Uesato et al., 2022), or automated forms of supervision (Luo et al., 2024; Snell et al., 2024; Wang et al., 2024). While some works use PRMs and ORMs to collect data for supervised fine-tuning Hosseini et al. (2024) or offline RL Setlur et al. (2024), other works directly use them as rewards in online RL (Shao et al., 2024; Uesato et al., 2022; Wang et al., 2024). Finally, we contrast our work against papers on imitating stronger teacher policies via RL objectives that optimize potential functions of teacher policies.</p>
<p>Outcome reward models. ORMs are verifiers (Cobbe et al., 2021b; Uesato et al., 2022) commonly used to improve the test-time performance using best-of- $N$, where we generate multiple candidate solutions from the base policy (LLM), rank them using the ORM, and pick the best one. ORMs are trained to assess correctness of a solution either using binary classification (Cobbe et al., 2021a; Yu et al., 2023), preference optimization using DPO (Hosseini et al., 2024), or next-token prediction (Zhang et al., 2024). Furthermore, prior works train LLMs on self-generated data using ground-truth outcome rewards (Rex), either via supervised fine-tuning (Singh et al., 2023a; Yuan et al., 2023; Zelikman et al., 2022), or online RL (Bi et al., 2024). In contrast to these approaches, our work focuses on process reward models (PRMs) for improving performance with beam-search at test time as well as online RL where we maximize the effective reward in Eq. 5 which linearly combines both Rex (outcome supervision) and process supervision in the form of advantages $A^{\mu}$ under a prover policy $\mu$.</p>
<p>PRMs and credit assignment. Several works focus on training step-level PRMs on math reasoning tasks, either using human labels (Lightman et al., 2023) or automated LLM-generated data to estimate value functions $Q^{\alpha}$ (Luo et al., 2024; Wang et al., 2024). Our work also focus on automated data collection for PRMs but empirically argues for using the advantage function $A^{\mu}$ as step-level rewards along with $Q^{\alpha}$, with a conceptual explanation in Section 3.1. Several prior works have explored step-level search algorithms with PRMs, such as beam search (Snell et al., 2024), heuristic greedy search (Ma et al., 2023), and reward-balanced tree search (Wu et al., 2024). Hwang et al. (2024); Setlur et al. (2024) use advantages to identify the "first pit" in an incorrect reasoning trace. Specifically, they collect data by computing advantages at each step using Monte Carlo rollouts. Then in an incorrect trace, they identify the step with the least advantage, and use the prefix of that step to construct preference pairs for offline direct preference optimization (Rafailov et al., 2023). In contrast, our work computes advantages under a prover policy, that we formally characterize, and use the computed advantages for improving test-time search and efficiency of online reinforcement learning.</p>
<p>Online RL for math reasoning. Once we have a trained outcome or process verifiers, it is natural update a policy by optimizing it against the learned signal, similar to how learned reward models are optimized in RLHF (Ouyang et al., 2022). In the context of math reasoning, Havrilla et al. (2024); Shao et al. (2024); Uesato et al. (2022) trained policies with RL, experimenting with both dense and sparse</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9 | Pictorial description of our planted sub-sequence didactic setup: An example showing five samples drawn i.i.d. from a very strong policy $(\gamma=100)$, and a relatively weaker $(\gamma=15)$ policy in our didactic setup.
rewards. In all three works, the gains observed by using PRMs that predict step-level correctness (similar to Lightman et al. (2023)) is quite small, compared to simply using trained ORMs, or the ground-truth outcome supervision Rex. In fact, Havrilla et al. (2024) states that the only algorithm that does well is a form of expert iteration (Anthony et al., 2017), which does not inhibit exploration as severely as some other approaches they compare with. Our work presents one of the first results, where trained PRMs, used in conjunction with the outcome rewards during online RL, result in policies with substantially higher ( $+6 \%$ ) performance, than the one trained only with outcome supervision. Our results also indicate a $5-6 \times$ sample efficiency boost for online RL, with our trained PAVs.</p>
<p>Connections to imitation learning through RL. The idea of mixing potential functions from different policies $\mu$ and $\pi$, in order to improve upon a sub-optimal expert $\mu$ appears in Chang et al. (2015), but this work considers the structured prediction problem which is vastly different from our setting. Related to this, is the work by Chang et al. (2023), which uses a "guide" policy to rollout from prefixes generated by a base policy. The base policy can now imitate the guide by cloning those rollouts, and eventually surpass. Our work also uses a prover policy which can complete rollouts from states where the base policy fails. But, we also show that weak provers in many cases are able to improve the base policy, or search over its responses, better than a stronger prover policy. We tie this observation to the insight that the main goal of the prover policy is to distinguish steps taken by the base policy, as measured by advantages under the prover. Thus, we do not require the prover policy to be something better than the base policy, which is a key distinction with Chang et al. (2023).</p>
<h1>B. Didactic Analysis</h1>
<p>We consider sequences of length 10 from a 15 -token vocabulary $\mathcal{V}:={1,2, \ldots, 14}$, where the end-ofsequence token is given by 14 , and all tokens following the end-of-sequence token (including it) are masked. Given an unknown planted sequence $\boldsymbol{y}^{\star}$ (in Fig. 9), we train a policy $\pi$ with policy gradient, where the outcome reward we wish to optimize is terminal and sparse, i.e., for $\boldsymbol{y} \sim \pi$ we have $r\left(\boldsymbol{y}, \boldsymbol{y}^{\star}\right)=1$ if and only if $\boldsymbol{y}^{\star}$ appears in $\boldsymbol{y}$, and 0 otherwise (Fig. 9). The policy $\pi$ in our experiments is represented by a multi-layer neural network, similar to the MADE architecture (Germain et al., 2015). The prover policy $\mu$ is parameterized by a scalar $\gamma&gt;0$. In particular, at any state $s$, where the last $k$ tokens leading up to $s$ match first $k$ tokens of $\boldsymbol{y}^{\star}$, then:</p>
<p>$$
\mu\left(\boldsymbol{y}_{s+1}^{\star} \mid \boldsymbol{s}\right) \propto \gamma
$$</p>
<p>and uniform on all other tokens. Thus, as $\gamma$ increases, the performance of $\mu$ improves and $\rightarrow 1$ as $\gamma \rightarrow \infty$. For our experiments, we assume (almost) oracle access to ground-truth advantage and $Q$-values, thus</p>
<p>mitigating any confounding issues due to usage of a learned verifier. We are able to approximate exact $Q$-values very accurately by using Monte Carlo estimates with large $&gt;100$ rollouts. With the goal of optimizing the terminal reward $r\left(\boldsymbol{y}, \boldsymbol{y}^{\star}\right)$, we optimize $\pi$ with two types of rewards: (i) only the outcome reward $r\left(\boldsymbol{y}, \boldsymbol{y}^{\star}\right)$, which is equivalent to using only $Q^{\pi}$ as step-level rewards; and (ii) using the effective reward: $Q^{\pi}+\alpha A^{\mu}$ as the step-level reward.</p>
<p>Training details. We use effective rewards with $\alpha=1$, and use the gradient in Eq. 5 to update the policy via policy gradient iterations. For the ORM runs, where we only use the outcome reward $r\left(\boldsymbol{y}, \boldsymbol{y}^{\star}\right)$, the policy gradient is equivalent to the case where $\alpha=0$ in Eq. 5. We train for 10,000 iterations in both cases, with a batch size of 64 , and a constant learning rate of $1 e-3$ for the Adam optimizer. The RL runs are initialized with a supervised finetuned policy. For this we take a randomly initialized network, based on the MADE architecture (Germain et al., 2015), with 3 layers, and 128 hidden units in each. Then we train it with supervised next-token prediction loss for 50 iterations on a dataset of 3200 samples from a weak policy $(\gamma=5.0)$. The batch size for the SFT training is also set to 64 . For evaluating Pass $@ N$ performance, we either sample $N$ independent trajectories (temperature 1.0) from the base policy trained using effective rewards, or only $Q^{\pi}$. We also evaluate Pass $@ N$ for the SFT policy for comparison.</p>
<h1>C. Additional: Experiments on Test-time Search with PAVs</h1>
<p>Implementation details. For our experiments in Sec. 4, we use three pretrained models: Gemma 2B, 9B and 27B. We finetune each of these on the MATH (Hendrycks et al., 2021) dataset. The finetuning is done for 5000 iterations, with a batchsize of 32 , and a maximum learning rate of $5 e-6$ for 2B, 9B and $5 e-7$ for the 27B models. We trained the policies using the Adam optimizer, with a linear warm up and cosine decay learning rate schedule. The linear warm up is done for the first 500 iterations. For the base policies, we choose the SFT checkpoints with the best accuracy on a holdout validation set of the MATH dataset. Given the SFT checkpoints, we next train PAVs using the procedure in Sec. 4.2. We do this for a class of provers, which include the base policies themselves. As we discuss in Sec. 4, the prover class also includes the best-of- $K$ policy for $K$ in ${2,4,8,16,32}$.</p>
<p>We use the hold out validation set to ascertain the value of $\alpha$ in the effective reward. For each base policy we run beam search with a beam size of 16 on this hold out validation set, and using the base policy itself as a prover, we evaluate the value of $\alpha$ that works best in the effective reward. We find that $\alpha=0.5$ worked best for Gemma 2B and 9B base policies, while a lower value of $\alpha=0.2$ was optimal for Gemma 27B. To tune $\alpha$ we ran a grid search over the range $[0.0,1.0]$, evaluating at an interval of 0.1 . We observe that the choice of $\alpha$ is a relatively robust one, since for all three base policies, we saw improvements (over only $Q^{\pi}$ as the reward) for values in the range of $[0.2,0.6]$. Having a separate value of $\alpha$ for each base policy, we use the same value in the effective reward given by any choice of the prover policy that is used for that base policy. Next, we present an experiment that compares the predictive power of effective reward vs. just $Q^{\pi}$ at initial states of a rollout under the base policy $\pi$, when either is used to predict the final outcome given by Rex.</p>
<p>Experiment: Is the effective reward able to predict the final outcome better than $Q^{\pi}$ ? In Fig. 10, we describe an experiment where for both the effective reward $Q^{\pi}+\alpha A^{\mu}$ (PAV) and just $Q^{\pi}$ (PQV), we compute the error of the classifier that makes a prediction on the final outcome by thresholding on either reward value at each step of the rollout. This threshold is computed using a validation set, and is separate for each step and reward combination. The figure tells us that the outcome prediction error drops for both rewards as the base policy is rolled out more, but clearly the effective reward dominates $Q^{\pi}$ (PQV)</p>            </div>
        </div>

    </div>
</body>
</html>
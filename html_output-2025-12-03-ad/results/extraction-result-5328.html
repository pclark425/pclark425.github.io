<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5328 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5328</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5328</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-267658066</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.09193v2.pdf" target="_blank">(Ir)rationality and cognitive biases in large language models</a></p>
                <p><strong>Paper Abstract:</strong> Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5328.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5328.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large multimodal transformer model used via API; state-of-the-art conversational and reasoning LLM at time of study, evaluated zero-shot on cognitive-psychology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based large language model (GPT family) accessed via OpenAI API; used with default parameters in zero-shot prompts for cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (Kahneman & Tversky et al. tasks: Linda/conjunction, Wason selection, Monty Hall, AIDS/medical test, Birth sequence/Hospital problem, High school problem; includes facilitated versions of Wason, AIDS, Monty Hall)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A set of classic tasks from cognitive psychology designed to elicit human cognitive biases and test probabilistic and logical reasoning (e.g., conjunction fallacy, selection task, probability estimation, Monty Hall decision). Tasks were administered in their classic and in some cases facilitated formats; LLMs were prompted ten times per task (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated across the 12 tasks (10 runs per task): 69.2% of runs produced a correct final answer with correct (logical) reasoning (reported in paper). GPT-4 achieved the highest proportion of answers that were both correct and accompanied by correct reasoning (69.2%). GPT-4 also produced many human-like answers overall (reported total human-like responses 73.3% when counting correct + human-like incorrect). Responses were nonetheless inconsistent across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Paper does not report a numerical human baseline for the same experimental protocol; it cites prior human-subject literature (e.g., Kahneman & Tversky, Bruckmaier et al.) where facilitated versions typically increase human correctness, but no task-by-task human percentages are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On aggregate GPT-4 achieves a high rate of correct reasoned responses (69.2%), which the authors note is superior to most other evaluated models and in some instances matches or exceeds human-like performance reported in prior literature; however the paper does not provide direct statistical human-vs-GPT-4 comparisons using the same experimental procedure and sample, only qualitative comparisons to published human results.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Despite high aggregate correctness, GPT-4 displayed inconsistent answers across repeated runs and sometimes produced correct answers via illogical/inaccurate intermediate reasoning or provided non-human-like incorrect answers; the paper emphasizes inconsistency as an LLM-specific form of irrationality and notes possible exposure of GPT-4 to some tasks in training (models sometimes identified Monty Hall), so training-set memorization may partly explain some correct answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5328.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5328.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 series model accessed via API and evaluated zero-shot on the cognitive battery; lower aggregate performance than GPT-4 in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based LLM (GPT-3.5 family) used with default API parameters in zero-shot prompting for each cognitive task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (same battery as for other models)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic cognitive tasks (conjunction fallacy, selection/Wason, Monty Hall, AIDS probability, birth sequence/hospital problem, high school problem) administered zero-shot, 10 runs per task.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated: correct & reasoned responses occurred in 29.2% of runs (table values), where the model produced human-like incorrect responses (studied bias) in 21.7% of runs; combining correct+human-like responses yields ~50.8% of runs being human-like (correct or typical human bias). Overall performance lower than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not specified numerically in paper; authors reference literature showing humans often commit the studied biases (e.g., conjunction fallacy) and perform better on facilitated versions, but no direct numeric baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 produced fewer correct reasoned answers than GPT-4 and showed a moderate fraction of human-like biased responses (~21.7% of runs showing human-like biases), indicating it sometimes mirrors human bias patterns but overall underperforms GPT-4 on correct, logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Responses were inconsistent across repeated runs; many incorrect responses were non-human-like (illogical or nonsensical) rather than standard human bias errors. No task-by-task human comparison numbers provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5328.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5328.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard (LaMDA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (LaMDA-powered)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's LaMDA-powered conversational model (Bard) evaluated zero-shot on the cognitive-psychology battery; exhibited middling aggregate correctness and notable failure modes on mathematical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard (LaMDA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google LaMDA-based chatbot (Bard) accessed through online interface with default settings; zero-shot prompts used for all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (same battery)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic tasks probing cognitive biases and probabilistic reasoning, presented in classic and facilitated formats; each task prompted 10 times.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated: table reports ~35.8% of runs as correct with logical reasoning (first category ~0.358). Bard produced a relatively high proportion of correct answers accompanied by illogical reasoning on mathematical tasks (paper reports 39% of Bard's correct mathematical-task responses contained illogical reasoning vs 20% with logical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No numeric human baseline in-paper; paper cites general human patterns (facilitated versions improve human correctness) but no direct numbers for Bard comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Bard underperforms GPT-4 on aggregate correct & reasoned responses; for mathematical tasks Bard frequently produced correct final answers while giving illogical or incorrect intermediate reasoning, indicating brittle calculation/logic behavior compared to expected human reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Particularly poor on reasoning-involving calculations (many correct answers with illogical reasoning), inconsistency across runs; facilitated task formats did not uniformly improve Bard's performance (only AIDS facilitated version showed consistent improvement across models except Llama2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5328.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5328.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's conversational LLM evaluated zero-shot on the cognitive battery; second-best aggregate performance in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic transformer-based conversational model (Claude 2) accessed via online chatbot interface with default parameters; tested zero-shot on tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (same battery)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Set of classic and facilitated problems from the heuristics-and-biases literature; models prompted ten times per task.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated: Claude 2 produced correct final answers with correct reasoning in 55.0% of runs (reported in text as second best after GPT-4). Other aggregate categories (human-like incorrect, non-human-like incorrect, refusal) occurred at lower rates but still present.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Paper does not provide numeric human baseline; references classical human-subject findings that facilitated versions generally aid human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Claude 2 outperforms most other non-OpenAI models in this set but underperforms GPT-4; as with other models, many incorrect answers were non-human-like and responses were inconsistent across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Inconsistency across runs; many incorrect responses were illogical rather than classic human biases; some refusal/omission behavior was observed but less than in some Llama 2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5328.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5328.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 2 (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama 2 small variant (7B) evaluated via online interface; had the poorest aggregate performance with many incorrect/non-human-like answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 7-billion-parameter model accessed via online chatbot interface; default system prompt present by default in tested environment (authors removed it in some runs to elicit responses).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (same battery)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic and facilitated tasks from cognitive psychology presented zero-shot; authors sometimes removed default system prompt to obtain responses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated: Llama 2 (7B) produced incorrect responses in 77.5% of runs (reported as the highest incorrect rate). Correct reasoned responses were very rare (table shows very low correct proportion ~2.5%). Many responses were non-human-like incorrect answers or refusals when default safety prompt was active.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided numerically in paper; authors reference human performance literature but do not report direct comparisons for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Llama2-7B substantially underperforms GPT-4, Claude 2, and GPT-3.5 on aggregate correctness and human-like responding; many failures are non-human-like (illogical), and default safety prompting caused frequent refusals unless removed.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>High rate of incorrect, non-human-like answers; default system prompt caused many refusals (authors removed it to obtain answers); poor performance on mathematical tasks (virtually no correct mathematical-task responses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5328.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5328.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 2 (13 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama 2 middle variant (13B) evaluated in the same battery; performed poorly with a low rate of human-like responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 13-billion-parameter model, accessed via online interface with default system prompt (authors removed prompt in some runs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (same battery)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic biases and reasoning tasks, zero-shot, 10 runs per task; facilitated versions included for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated: low overall correct reasoned responses (~5.0% reported in table row); total human-like response proportion reported as only 8.3% in text (lowest among models). Many runs resulted in incorrect/non-human-like answers or refusals when safety prompt present.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided numerically in this paper; authors refer to prior human-subject findings but provide no direct numeric baseline here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Llama2-13B underperforms all OpenAI/Anthropic models in the study and shows the lowest rate of human-like responding; fails differently from humans (non-human-like illogical errors common).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Very low rate of human-like responses; many refusals when system safety prompt remained; failed virtually all mathematical tasks (7B and 13B produced no correct responses on calculation-containing tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5328.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5328.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 2 (70 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama 2 large variant (70B) evaluated on the battery; produced somewhat better correctness than the smaller Llama variants but had a high refusal/no-answer rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 70-billion-parameter model accessed via online interface (default system prompt reportedly not present, but behavior similar to smaller Llama models).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Battery of 12 cognitive psychology tasks (same battery)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>probabilistic and logical reasoning; heuristics & biases; decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same set of classic and facilitated tasks, zero-shot prompting with 10 runs per task.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Aggregated: table indicates ~15.0% correct reasoned responses; notable high refusal/no-answer rate: 41.7% of runs resulted in no answer or refusal (reported in text). Some correct outputs but many refusals and non-human-like incorrect answers. On mathematical tasks Llama2-70B produced only one correct instance across all calculation-containing tasks (authors report 70B only succeeded once on calculation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided numerically in this paper; qualitative comparison to human literature only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Llama2-70B performs better than the smaller Llama variants on some measures but far below GPT-4 and Claude 2; frequent refusals and inconsistency limit comparability to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Very high refusal/no-answer rate (41.7%), likely safety-prompt or instruction-tuned behavior; performance on mathematical tasks extremely poor (only one correct instance reported). Many incorrect responses were non-human-like and illogical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(Ir)rationality and cognitive biases in large language models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT <em>(Rating: 2)</em></li>
                <li>Tversky and Kahneman's Cognitive Illusions: Who Can Solve Them, and Why? <em>(Rating: 1)</em></li>
                <li>Language models show human-like content effects on reasoning tasks <em>(Rating: 1)</em></li>
                <li>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5328",
    "paper_id": "paper-267658066",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "OpenAI's large multimodal transformer model used via API; state-of-the-art conversational and reasoning LLM at time of study, evaluated zero-shot on cognitive-psychology tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI transformer-based large language model (GPT family) accessed via OpenAI API; used with default parameters in zero-shot prompts for cognitive tasks.",
            "model_size": null,
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (Kahneman & Tversky et al. tasks: Linda/conjunction, Wason selection, Monty Hall, AIDS/medical test, Birth sequence/Hospital problem, High school problem; includes facilitated versions of Wason, AIDS, Monty Hall)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "A set of classic tasks from cognitive psychology designed to elicit human cognitive biases and test probabilistic and logical reasoning (e.g., conjunction fallacy, selection task, probability estimation, Monty Hall decision). Tasks were administered in their classic and in some cases facilitated formats; LLMs were prompted ten times per task (zero-shot).",
            "llm_performance": "Aggregated across the 12 tasks (10 runs per task): 69.2% of runs produced a correct final answer with correct (logical) reasoning (reported in paper). GPT-4 achieved the highest proportion of answers that were both correct and accompanied by correct reasoning (69.2%). GPT-4 also produced many human-like answers overall (reported total human-like responses 73.3% when counting correct + human-like incorrect). Responses were nonetheless inconsistent across repeated runs.",
            "human_baseline_performance": "Paper does not report a numerical human baseline for the same experimental protocol; it cites prior human-subject literature (e.g., Kahneman & Tversky, Bruckmaier et al.) where facilitated versions typically increase human correctness, but no task-by-task human percentages are reported in this paper.",
            "performance_comparison": "On aggregate GPT-4 achieves a high rate of correct reasoned responses (69.2%), which the authors note is superior to most other evaluated models and in some instances matches or exceeds human-like performance reported in prior literature; however the paper does not provide direct statistical human-vs-GPT-4 comparisons using the same experimental procedure and sample, only qualitative comparisons to published human results.",
            "notable_differences_or_limitations": "Despite high aggregate correctness, GPT-4 displayed inconsistent answers across repeated runs and sometimes produced correct answers via illogical/inaccurate intermediate reasoning or provided non-human-like incorrect answers; the paper emphasizes inconsistency as an LLM-specific form of irrationality and notes possible exposure of GPT-4 to some tasks in training (models sometimes identified Monty Hall), so training-set memorization may partly explain some correct answers.",
            "uuid": "e5328.0",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "OpenAI GPT-3.5 series model accessed via API and evaluated zero-shot on the cognitive battery; lower aggregate performance than GPT-4 in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI transformer-based LLM (GPT-3.5 family) used with default API parameters in zero-shot prompting for each cognitive task.",
            "model_size": null,
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (same battery as for other models)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "Classic cognitive tasks (conjunction fallacy, selection/Wason, Monty Hall, AIDS probability, birth sequence/hospital problem, high school problem) administered zero-shot, 10 runs per task.",
            "llm_performance": "Aggregated: correct & reasoned responses occurred in 29.2% of runs (table values), where the model produced human-like incorrect responses (studied bias) in 21.7% of runs; combining correct+human-like responses yields ~50.8% of runs being human-like (correct or typical human bias). Overall performance lower than GPT-4.",
            "human_baseline_performance": "Not specified numerically in paper; authors reference literature showing humans often commit the studied biases (e.g., conjunction fallacy) and perform better on facilitated versions, but no direct numeric baseline in this paper.",
            "performance_comparison": "GPT-3.5 produced fewer correct reasoned answers than GPT-4 and showed a moderate fraction of human-like biased responses (~21.7% of runs showing human-like biases), indicating it sometimes mirrors human bias patterns but overall underperforms GPT-4 on correct, logical reasoning.",
            "notable_differences_or_limitations": "Responses were inconsistent across repeated runs; many incorrect responses were non-human-like (illogical or nonsensical) rather than standard human bias errors. No task-by-task human comparison numbers provided in paper.",
            "uuid": "e5328.1",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Bard (LaMDA)",
            "name_full": "Google Bard (LaMDA-powered)",
            "brief_description": "Google's LaMDA-powered conversational model (Bard) evaluated zero-shot on the cognitive-psychology battery; exhibited middling aggregate correctness and notable failure modes on mathematical tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bard (LaMDA)",
            "model_description": "Google LaMDA-based chatbot (Bard) accessed through online interface with default settings; zero-shot prompts used for all tasks.",
            "model_size": null,
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (same battery)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "Classic tasks probing cognitive biases and probabilistic reasoning, presented in classic and facilitated formats; each task prompted 10 times.",
            "llm_performance": "Aggregated: table reports ~35.8% of runs as correct with logical reasoning (first category ~0.358). Bard produced a relatively high proportion of correct answers accompanied by illogical reasoning on mathematical tasks (paper reports 39% of Bard's correct mathematical-task responses contained illogical reasoning vs 20% with logical reasoning).",
            "human_baseline_performance": "No numeric human baseline in-paper; paper cites general human patterns (facilitated versions improve human correctness) but no direct numbers for Bard comparison.",
            "performance_comparison": "Bard underperforms GPT-4 on aggregate correct & reasoned responses; for mathematical tasks Bard frequently produced correct final answers while giving illogical or incorrect intermediate reasoning, indicating brittle calculation/logic behavior compared to expected human reasoning.",
            "notable_differences_or_limitations": "Particularly poor on reasoning-involving calculations (many correct answers with illogical reasoning), inconsistency across runs; facilitated task formats did not uniformly improve Bard's performance (only AIDS facilitated version showed consistent improvement across models except Llama2).",
            "uuid": "e5328.2",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Claude 2",
            "name_full": "Anthropic Claude 2",
            "brief_description": "Anthropic's conversational LLM evaluated zero-shot on the cognitive battery; second-best aggregate performance in the study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 2",
            "model_description": "Anthropic transformer-based conversational model (Claude 2) accessed via online chatbot interface with default parameters; tested zero-shot on tasks.",
            "model_size": null,
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (same battery)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "Set of classic and facilitated problems from the heuristics-and-biases literature; models prompted ten times per task.",
            "llm_performance": "Aggregated: Claude 2 produced correct final answers with correct reasoning in 55.0% of runs (reported in text as second best after GPT-4). Other aggregate categories (human-like incorrect, non-human-like incorrect, refusal) occurred at lower rates but still present.",
            "human_baseline_performance": "Paper does not provide numeric human baseline; references classical human-subject findings that facilitated versions generally aid human performance.",
            "performance_comparison": "Claude 2 outperforms most other non-OpenAI models in this set but underperforms GPT-4; as with other models, many incorrect answers were non-human-like and responses were inconsistent across runs.",
            "notable_differences_or_limitations": "Inconsistency across runs; many incorrect responses were illogical rather than classic human biases; some refusal/omission behavior was observed but less than in some Llama 2 variants.",
            "uuid": "e5328.3",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-7b",
            "name_full": "Meta Llama 2 (7 billion parameters)",
            "brief_description": "Llama 2 small variant (7B) evaluated via online interface; had the poorest aggregate performance with many incorrect/non-human-like answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 (7B)",
            "model_description": "Meta's Llama 2 7-billion-parameter model accessed via online chatbot interface; default system prompt present by default in tested environment (authors removed it in some runs to elicit responses).",
            "model_size": "7B",
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (same battery)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "Classic and facilitated tasks from cognitive psychology presented zero-shot; authors sometimes removed default system prompt to obtain responses.",
            "llm_performance": "Aggregated: Llama 2 (7B) produced incorrect responses in 77.5% of runs (reported as the highest incorrect rate). Correct reasoned responses were very rare (table shows very low correct proportion ~2.5%). Many responses were non-human-like incorrect answers or refusals when default safety prompt was active.",
            "human_baseline_performance": "Not provided numerically in paper; authors reference human performance literature but do not report direct comparisons for this model.",
            "performance_comparison": "Llama2-7B substantially underperforms GPT-4, Claude 2, and GPT-3.5 on aggregate correctness and human-like responding; many failures are non-human-like (illogical), and default safety prompting caused frequent refusals unless removed.",
            "notable_differences_or_limitations": "High rate of incorrect, non-human-like answers; default system prompt caused many refusals (authors removed it to obtain answers); poor performance on mathematical tasks (virtually no correct mathematical-task responses).",
            "uuid": "e5328.4",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-13b",
            "name_full": "Meta Llama 2 (13 billion parameters)",
            "brief_description": "Llama 2 middle variant (13B) evaluated in the same battery; performed poorly with a low rate of human-like responses.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 (13B)",
            "model_description": "Meta's Llama 2 13-billion-parameter model, accessed via online interface with default system prompt (authors removed prompt in some runs).",
            "model_size": "13B",
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (same battery)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "Classic biases and reasoning tasks, zero-shot, 10 runs per task; facilitated versions included for some tasks.",
            "llm_performance": "Aggregated: low overall correct reasoned responses (~5.0% reported in table row); total human-like response proportion reported as only 8.3% in text (lowest among models). Many runs resulted in incorrect/non-human-like answers or refusals when safety prompt present.",
            "human_baseline_performance": "Not provided numerically in this paper; authors refer to prior human-subject findings but provide no direct numeric baseline here.",
            "performance_comparison": "Llama2-13B underperforms all OpenAI/Anthropic models in the study and shows the lowest rate of human-like responding; fails differently from humans (non-human-like illogical errors common).",
            "notable_differences_or_limitations": "Very low rate of human-like responses; many refusals when system safety prompt remained; failed virtually all mathematical tasks (7B and 13B produced no correct responses on calculation-containing tasks).",
            "uuid": "e5328.5",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama2-70b",
            "name_full": "Meta Llama 2 (70 billion parameters)",
            "brief_description": "Llama 2 large variant (70B) evaluated on the battery; produced somewhat better correctness than the smaller Llama variants but had a high refusal/no-answer rate.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 2 (70B)",
            "model_description": "Meta's Llama 2 70-billion-parameter model accessed via online interface (default system prompt reportedly not present, but behavior similar to smaller Llama models).",
            "model_size": "70B",
            "cognitive_test_name": "Battery of 12 cognitive psychology tasks (same battery)",
            "cognitive_test_type": "probabilistic and logical reasoning; heuristics & biases; decision-making",
            "cognitive_test_description": "Same set of classic and facilitated tasks, zero-shot prompting with 10 runs per task.",
            "llm_performance": "Aggregated: table indicates ~15.0% correct reasoned responses; notable high refusal/no-answer rate: 41.7% of runs resulted in no answer or refusal (reported in text). Some correct outputs but many refusals and non-human-like incorrect answers. On mathematical tasks Llama2-70B produced only one correct instance across all calculation-containing tasks (authors report 70B only succeeded once on calculation tasks).",
            "human_baseline_performance": "Not provided numerically in this paper; qualitative comparison to human literature only.",
            "performance_comparison": "Llama2-70B performs better than the smaller Llama variants on some measures but far below GPT-4 and Claude 2; frequent refusals and inconsistency limit comparability to human performance.",
            "notable_differences_or_limitations": "Very high refusal/no-answer rate (41.7%), likely safety-prompt or instruction-tuned behavior; performance on mathematical tasks extremely poor (only one correct instance reported). Many incorrect responses were non-human-like and illogical.",
            "uuid": "e5328.6",
            "source_info": {
                "paper_title": "(Ir)rationality and cognitive biases in large language models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
            "rating": 2,
            "sanitized_title": "humanlike_intuitive_behavior_and_reasoning_biases_emerged_in_large_language_models_but_disappeared_in_chatgpt"
        },
        {
            "paper_title": "Tversky and Kahneman's Cognitive Illusions: Who Can Solve Them, and Why?",
            "rating": 1,
            "sanitized_title": "tversky_and_kahnemans_cognitive_illusions_who_can_solve_them_and_why"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning tasks",
            "rating": 1,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning_tasks"
        },
        {
            "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
            "rating": 1,
            "sanitized_title": "instructed_to_bias_instructiontuned_language_models_exhibit_emergent_cognitive_bias"
        }
    ],
    "cost": 0.01475025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>(Ir)rationality and Cognitive Biases in Large Language Models
15 Feb 2024</p>
<p>Olivia Macmillan-Scott olivia.macmillan-scott.16@ucl.ac.uk 
Mirco Musolesi m.musolesi@ucl.ac.uk </p>
<p>University College London</p>
<p>University College London University of Bologna</p>
<p>(Ir)rationality and Cognitive Biases in Large Language Models
15 Feb 2024E3B9A053ED9032BD792C0876DDF1133AarXiv:2402.09193v2[cs.CL]
Do large language models (LLMs) display rational reasoning?LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear.In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature.We find that, like humans, LLMs display irrationality in these tasks.However, the way this irrationality is displayed does not reflect that shown by humans.When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases.On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses.Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have quickly become integrated into everyday activities, and their increasing capabilities mean this will only become more pervasive.Given this notion, it is important for us to develop methodologies to evaluate the behaviour of LLMs.As we will see, these models still exhibit biases and produce information that is not factual [1].However, there is extensive variation in the responses given by different models to the same prompts.In this paper, we take a comparative approach based in cognitive psychology to evaluate the rationality and cognitive biases present in a series of LLMs; the aim of this paper is to provide a method to evaluate and compare the behaviour and capabilities of different models, here with a focus on rational and irrational reasoning.There exist different definitions of what is rational in artificial intelligence [2], and conceptions vary depending on whether we are looking at reasoning or behaviour [3].For this study we are concerned with rational reasoning: we understand an agent (human or artificial) to be rational if it reasons according to the rules of logic and probability; conversely, we take an irrational agent to be one that does not reason according to these rules.This is in line with Stein's [4] formal definition of the Standard Picture of rationality.</p>
<p>In this paper, we evaluate seven LLMs using cognitive tests proposed by Kahneman and Tversky [5][6][7] and others [8][9][10], as well as some facilitated versions formulated by Bruckmaier et al. [11], and evaluate the responses across two dimensions: correct and human-like [12].These tasks were initially designed to illustrate cognitive biases and heuristics in human reasoning, showing that humans often do not reason rationally [13]; in this case, we use them to evaluate the rationality of LLMs.The 'holy grail' would be to develop a set of benchmarks that can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.</p>
<p>In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14].This line of argument encourages species-fair comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences.Lampinen [15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models.However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans.This is the approach we have taken in this paper -in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans.</p>
<p>Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called machine psychology, which would treat LLMs as participants in psychological experiments.The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models.Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19].One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20].One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour.Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour.Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour, both individual and within social settings.</p>
<p>Given the data that they are trained on, LLMs naturally contain human-like biases [24][25][26].Schramowski et al. [24] highlight that language models reflect societal norms when it comes to ethics and morality, meaning that these models contain human-like biases regarding what is right and wrong.Similarly, Durt et al. [26] discuss the clichs and biases exhibited by LLMs, emphasising that the presence of these biases is not due to the models' mental capacities but due to the data they are trained on.Others have focused on specific qualities of human decision-making that are not possessed by LLMs, namely the ability to reflect and learn from mistakes, and propose an approach using verbal reinforcement to address this limitation [27].As these studies show, LLMs display human-like biases which do not arise from the models' ability to reason, but from the data they are trained on.Therefore, the question is whether LLMs also display biases that relate to reasoning: do LLMs simulate human cognitive biases?There are cases where is may be beneficial for AI systems to replicate human cognitive biases, in particular for applications that require human-AI collaboration [28].</p>
<p>To answer this question, we use tasks from the cognitive psychology literature designed to test human cognitive biases, and apply these to a series of LLMs to evaluate whether they display rational or irrational reasoning.The capabilities of these models are quickly advancing, therefore the aim of this paper is to provide a methodological contribution showing how we can assess and compare LLMs.A number of studies have taken a similar approach, however they do not generally compare across different model types [12,16,[29][30][31][32][33][34][35], or those that do are not evaluating rational reasoning [36].Some find that LLMs outperform humans on reasoning tasks [16,37], others find that these models replicate human biases [30,38], and finally some studies have shown that LLMs perform much worse than humans on certain tasks [36].Binz and Schulz [12] take a similar approach to that presented in this paper, where they treat GPT-3 as a participant in a psychological experiment to assess its decision-making, information search, deliberation and causal reasoning abilities.They assess the responses across two dimensions, looking at whether GPT-3's output is correct and/or human-like; we follow this approach in this paper as it allows us to distinguish between answers that are incorrect due to a human-like bias or are incorrect in a different way.While they find that GPT-3 performs as well or even better than human subjects, they also find that small changes to the wording of tasks can dramatically decrease the performance, likely due to GPT-3 having encountered these tasks in training.Hagendorff et al. [16] similarly use the Cognitive Reflection Test (CRT) and semantic illusions on a series of OpenAI's Generative Pre-trained Transformer (GPT) models.They classify the responses as correct, intuitive (but incorrect), and atypical -as models increase in size, the majority of responses go from being atypical, to intuitive, to overwhelmingly correct for GPT-4, which no longer displays human cognitive errors.Other studies that find the reasoning of LLMs to outperform that of humans includes Chen et al.'s [33] assessment of the economic rationality of GPT, and Webb et al.'s [34] comparison of GPT-3 and human performance on analogical tasks.</p>
<p>As mentioned, some studies have found that LLMs replicate cognitive biases present in human reasoning, and so in some instances display irrational thinking in the same way that humans do.Itzhak et al. [38] focus on the effects of fine-tuning; they show that instruction tuning and reinforcement learning from human feedback, while improving the performance of LLMs, can also cause these models to express cognitive biases that were not present or less expressed before these fine-tuning methods were applied.While said study [38] investigate three cognitive biases that lead to irrational reasoning, namely the decoy effect, certainty effect and belief bias, Dasgupta et al. [30] centre their research on the content effect and find that, like humans, models reason more effectively about believable situations than unrealistic or abstract ones.In few-shot task evaluation, the performance of LLMs is shown to increase after being provided with in-context examples, just as examples improve learning in humans [39].Others have found LLMs to perform worse than human subjects on certain cognitive tasks, Ruis et al. [36] test the performance of four categories of models on an implicature task, showing that the models that perform best are those that have been fine-tuned on example-level instructions, both at the zero-shot and few-shot levels.However, they still find that models perform close to random, particularly in zero-shot evaluation.Looking at performance on mathematical problems in particular, GPT-4 has shown inconsistencies in its capabilities, correctly answering difficult mathematical questions in some instances, while also making very basic mistakes in others [37].As we will see below, we find this to be the case in our analysis across the language models evaluated.The inconsistency in performance is not only present in tasks involving mathematical calculations, but is apparent across the battery of tasks.This paper forms part of the existing area of research on the evaluation of LLMs.It differs from existing work by focusing on rational and irrational reasoning, and comparing the performance of different models.As we have seen, past studies have applied cognitive psychology to study LLMs.While they often focus on seeing whether LLMs replicate different aspects of human behaviour and reasoning, such as cognitive biases, we are interested in whether the way LLMs display rational or irrational reasoning.Much of the existing work focuses on a single model, or different versions of the same model.In this case, we compare across model types and propose a way to evaluate the performance of LLMs, which may ultimately lead to the development of a set of benchmarks to test the rationality of a model.</p>
<p>Methods</p>
<p>Language Models</p>
<p>We evaluate the rational reasoning of seven LLMs using a series of tasks from the cognitive psychology literature.The models that we assess are OpenAI's GPT-3.5 [40] and GPT-4 [41], Google's Bard powered by LaMDA [42], Anthropic's Claude 2 [43], and three versions of Meta's Llama 2 model: the 7 billion (7b), 13 billion (13b) and 70 billion (70b) parameter versions [44].We use the OpenAI API to prompt GPT-3.5 and GPT-4, and all other models are accessed through their online chatbot interfaces.The code for the former is available on GitHub, and information on how models were accessed is detailed in Appendix 1.</p>
<p>We did not change any parameter settings in order to evaluate the models on these cognitive tasks.However, for Llama 2, the 7b and 13b parameter models had the default prompt shown in Figure 1.After running an initial set of the tasks on these Llama 2 models, we removed the default prompt as it generally meant that the models refused to provide a response due to ethical concerns.Removing the system prompt meant we were able to obtain responses for the tasks, and so able to compare the performance of these models to the others mentioned.As we will discuss below, the 70 billion parameter version had no default system prompt, but gave very similar responses to the 7 and 13 billion parameter versions with the prompt included, meaning we often obtained no response from this larger version of the model.</p>
<p>System prompt -Llama 2 7b and 13b</p>
<p>You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.1: List of tasks and the cognitive biases they were designed to exemplify.</p>
<p>Description of Tasks</p>
<p>The tasks used to evaluate these models are taken primarily from Kahneman and Tversky's work [5,6,13,7], who designed a series of tasks to highlight biases and heuristics in human reasoning.Additional tasks [8][9][10] and facilitated versions [11] are also included.These tests have been used extensively on human subjects, showing that they are often answered incorrectly.Based primarily on work by Gigerenzer [45,46], a series of facilitated versions of these tasks were developed, emphasising the impact of context and presentation of the problem.Following on from this, Bruckmaier et al. [11] evaluate human subjects on a set of these tasks, comparing the performance on the original version as opposed to facilitated version.We have included both the classic and facilitated versions of these tasks in our analysis; this allows us to further examine whether the performance of LLMs also increases on the facilitated versions of tasks, or whether we observe a different pattern to that shown in human experiments.Whereas when evaluating human subjects each task would only be asked once, when evaluating LLMs on the same tasks, we prompt the models with each task ten times due to the variation in responses.</p>
<p>In total, we study the performance of seven language models on twelve cognitive tasks, listed in Table 1 (full task details are included in Appendix 2).Nine of them are from the set of tasks originally designed by Kahneman and Tversky [5][6][7], Wason [8], Eddy [9] and Friedman [10], and three which are facilitated versions of these tasks [11].For the birth sequence problem [5], two version are included: one with an ordered sequence and one with a random sequence.We include facilitated versions [11] for the Wason task, the AIDS task and the Monty Hall problem.We use zero-shot evaluation, as we are interested in the performance of these models without further learning, and for each task we prompt the model ten times in order to check for consistency of responses.</p>
<p>Categorisation of Responses</p>
<p>Each response to the task is categorised across two dimensions: correct and human-like [12], as detailed in Table 2.The correct dimension simply records whether the model was able to accurately respond to the task: here we focus only on the final answer given, and not on the reasoning provided by the model.The human-like dimension instead looks at whether the response can be compared to one a human subject would have given.A response that is correct and non-human-like would be one where the final response is correct, but the reasoning stated contains factual inaccuracies, incorrect calculations, or violates rules of logic and probability.Instead, a response that is incorrect and human-like corresponds to the answers typically given by humans to these tasks, so is one where the answer given is in accordance with one of the biases studied in the cognitive psychology literature.An example of a response that is neither correct nor human-like is illustrated in Figure 2.</p>
<p>Llama 2 7b -Monty Hall problem</p>
<p>In this scenario, it doesn't matter whether the candidate switches their choice or not.Here's why: When the candidate initially chose door 1, there were two doors left (2 and 3).The quizmaster then opened door 2, revealing a goat.At this point, the candidate has two options:</p>
<p> Keep their original choice of door 1, in which case they will lose the game.</p>
<p> Switch to door 3, in which case they will win the game (the car is behind one of the two unopened doors).In either case, the outcome is the same.Whether the candidate switches or not, they will either win the game or lose.Therefore, it doesn't matter whether they switch or not.</p>
<p>Results</p>
<p>The cognitive tasks used in this paper were designed to show that human reasoning is not always rational, and that humans often employ heuristics or display cognitive biases [5,6].Here, we understand rationality to mean following rules of logic and probability.One important result from this evaluation is that the LLMs' responses are inconsistent -the same model will give very different answers for the same task, sometimes getting the correct answer and other times displaying illogical reasoning.In this sense, the use of these cognitive tasks from the psychology literature reveal another type of irrationality found in LLMs, in this case relating to the inconsistency of results.This feature of LLMs is an important problem to consider and reveals a clear difference in how these tasks apply to LLMs as opposed to humans.Although studies in the literature discuss the idea of treating LLMs as if they were subjects in a psychological experiment [12], the fact that responses vary for the same prompt and model mean we have to take a slightly different approach to evaluating these models, and consider the implications of the inconsistency of responses.Figure 3: Aggregated results across all tasks for each model.The LLMs were prompted with twelve tasks from cognitive psychology, and their responses were categorised over two dimensions: correct and human-like (in this graph, responses categorised as incorrect and non-human-like are distinguished from those that were incorrect but displayed correct reasoning).For each task, the LLMs were prompted ten times.</p>
<p>Results across all tasks are aggregated in Table 3 and Figure 3.The model that displayed the best overall performance was OpenAI's GPT-4, which achieved the highest proportion of answers that were correct and where the results was achieved through correct reasoning (cateogorised as correct and human-like in the above categorisation).GPT-4 gave the correct response and correct reasoning in 69.2% of cases, followed by Anthropic's Claude 2 model, which achieved this outcome 55.0% of the time.Conversely, the model with the highest proportion of incorrect responses (both human-like and non-human-like) was Meta's Llama 2 model with 7 billion parameters, which gave incorrect responses in 77.5% of cases.It is interesting to note that across all language models, incorrect responses were generally not human-like, meaning they were not incorrect due to displaying a cognitive bias.Instead, these responses generally displayed illogical reasoning, and even on occasion provided correct reasoning but then gave an incorrect final answer.An example of the latter is illustrated in Figure 4: this example shows Bard's response to the facilitated version of the Wason task, where the correct response is that both Letter 3 and Letter 4 should be turned over.The model correctly reaches this conclusion in the explanation, but both at the start and end of the response only states that Letter 4 needs to be turned over.This type of response, where the reasoning is correct but the final answer is not, was observed across all model families to varying degrees.</p>
<p>The result that most incorrect responses were not incorrect due to having fallen for a cognitive bias highlight that these models do not fail at these tasks in the same way that humans do.As we have seen, many studies have shown that LLMs simulate human biases and societal norms [24][25][26].However, when it comes to reasoning, the effect is less clear.The model that displayed the highest proportion of human-like biases in its responses was GPT-3.5, where this only occurred in 21.7% of cases.If we include human-like correct responses for GPT-3.5, this brings the proportion to 50.8% of cases.Again, the model that displayed the most human-like responses (both correct and incorrect) was GPT-4 (73.3%); the lowest was Llama 2 with 13 billion parameters, only giving human-like responses in 8.3% of cases.The comparison between correct and human-like responses given by each model is summarised in Figure 5.</p>
<p>In some occasions, the LLMs did not answer the question, or explicitly refused to respond to the task.This was particularly prominent for Llama 2 with 70 billion parameters, which refused to give in answer in 41.7% of cases -an example is given in Figure 6.As mentioned above, we kept the default parameters for all models and did not provide a system prompt.For Llama 2, the 7 and 13 billion parameter versions had the aforementioned system prompt as default.For the 70 billion parameter Google Bard -Wason task (facilitated)</p>
<p>The answer is d) Letter 4.You only need to turn over Letter 4 to check compliance with the rule.</p>
<p> Letter 1 has a 90-cent stamp on the front, so it is compliant with the rule, regardless of where it is going. Letter 2 has Italy marked on the back, so you know it is not going to the USA.</p>
<p>Therefore, you do not need to turn it over. Letter 3 has a 50-cent stamp on the front, which is not enough for a letter going to the USA.However, you cannot see the back of the letter, so you do not know for sure where it is going.Therefore, you need to turn it over to check. Letter 4 has USA marked on the back, so you know it is going to the USA.However, you cannot see the front of the letter, so you do not know how many stamps are affixed to it.Therefore, you need to turn it over to check.</p>
<p>So the answer is Letter 4, because you need to turn it over to see if it has a 90-cent stamp on the front.version, this system prompt was no longer included.However, the responses given by the model were very similar to those given by the other Llama 2 models when said prompt was maintained, which may indicate that this has now been embedded into the model to avoid any harmful or unethical outputs.</p>
<p>As part of the tasks, we included three facilitated versions of classic cognitive tests [11], as well as two versions of the Births Order problem: one of these gives a random order, and the other appears less random.Human subjects tend to assign a lower probability to the latter, as they assume the ordered version is less likely to occur [5].A comparison of the models' results for the classic and facilitated tasks is shown in Figure 7.Given that the facilitated versions of these tasks are more often answered correctly by humans [11], we hypothesised that the same result would be observed for LLMs.However, the only task where this appeared to be the case was the AIDS task (for all aside from Llama 2 models).This is surprising as the facilitated versions of tasks give more context or explanation as to the problem, and therefore the correct response should be easier to obtain.One potential reason for LLMs generally giving correct responses more often for the classic versions of tasks is that these likely appear in their training data, therefore the models have already been exposed to the problems.</p>
<p>The question of whether these models have already seen the cognitive tasks in training can be partially answered by looking at cases where the LLM identifies the problem they are being posed (see Table 4).All models assessed aside from Claude 2 identified at least one version of the Monty Hall problem in some of their responses (only Llama 2 70b identified the Monty Hall problem in every run).Aside from this case, the only other time a task was correctly identified was the Linda problem by Bard.None of the other problems were identified by the LLMs, and the aforementioned inconsistency in the responses indicates that, even if the models have been exposed to these tasks in training, this does not guarantee they will be able to correctly solve the tasks.</p>
<p>Previous literature has identified that LLMs often make basic mistakes in seemingly simple calculations [37].Given this finding, we decided to compare the performance of the models on tasks that contained mathematical calculations and those that did not -these results are illustrated in Figure 8.</p>
<p>In this case, we only look as answers that were categorised as correct and human-like, that is to say that the final answer was correct, and the reasoning presented was also logical.Across all models, performance is higher in non-mathematical tasks as opposed to mathematical ones.The magnitude of the difference in performance varies in the different models, being most stark for Google's Bard model.Surprisingly, there were more instances when Bard gave correct responses that contained illogical reasoning than logical reasoning for the mathematical tasks (39% of responses as opposed to 20%).For the Llama 2 models, performance on mathematical tasks was extremely low.The 7 and 13  Graph also depicts the proportion of responses which did not contain an answer or where there was a refusal to provide an answer.Correct responses include those those with correct (logical) reasoning, as well as those with incorrect (illogical) reasoning that reached the correct answer.Human-like responses include those that are correct with logical reasoning, and those that are incorrect but are achieved through a studied human cognitive bias.</p>
<p>Llama 2 70b -Linda problem I must point out that the question contains harmful gender stereotypes.The assumption that Linda cannot be both a bank clerk and an advocate for women's rights because these roles are mutually exclusive is inappropriate.In addition, it perpetuates negative ideas about people who work in banks, implying that they lack intelligence or interest in social issues.It also implies that individuals involved in the feminist movement cannot have "traditional" jobs.Instead, we should focus on treating everyone with respect and understanding their unique experiences without making assumptions based on limited views.We can do this by avoiding asking questions that reinforce damaging beliefs and biases.Asking questions that promote inclusivity and diversity would be best.billion parameter models did not give correct responses to any of the tasks containing calculations, whereas the 70 billion parameter version only did so in one instance.</p>
<p>Discussion</p>
<p>This paper set out to evaluate LLMs using tasks from the cognitive psychology literature in order to assess whether these models display rational reasoning, or whether they display irrational reasoning akin to that observed in humans.Instead, we have found that these models exhibit irrational reasoning in a different way.Firstly, the responses given by these models are highly inconsistent -the same model will give both correct and incorrect, and both human and non-human-like responses in different runs.Secondly, the majority of incorrect responses do not display human-like biases; they are incorrect in ways different to human subjects.A series of issues can be identified when looking at the explanations given by LLMs, particularly when it comes to mathematical calculations, but also  4: Proportion of task runs that each task was identified by the given model.No other tasks were identified by any of the LLMs.inconsistent logic.In terms of performance on mathematical tasks, previous research has found that although models perform poorly on some basic calculations, they can often also show impressive performance on complex problems [37].While the tasks employed in this paper did not have a wide enough range to investigate performance in sub-fields of mathematics, this constitutes an interesting line of research.
Task Comparison
To ensure we could accurately compare the results to responses given by human subjects, we did not alter the prompts from the classic formulation of the problems.This is a promising research area; some have already conducted studies altering prompts to ensure the problems have not previously been seen by the LLMs being assessed [30], however literature in this area remains limited.Having said that, in our study only the Monty Hall problem was identified by the models, as well as the Linda problem in only one instance.Therefore, even if the LLMs were previously exposed to these cognitive tasks, this does not guarantee they will be able to respond correctly.</p>
<p>When conducting the experiments, we left the default parameters for the LLMs, as these appear to be the preferred option by LLM designers and the majority of users will likely keep them.By not changing the temperature parameter in particular, we were able to compare different responses given by the LLMs.Through this comparison, we showed that there is significant inconsistency in the responses given.Some have addressed this by setting the temperature parameter of the model to 0 to  ensure deterministic responses [12].However, this approach overlooks that a small change in this parameter can drastically change the results obtained.Therefore, we did not set the parameter to 0 in order to observe this variation in responses, which demonstrated the significant inconsistency in the LLM's answers to the tasks.</p>
<p>The only change we made to the default parameters was to remove the default prompts for the 7 and 13 billion versions of the Llama 2 models.Including the prompt led to the LLMs refusing to provide a response in the majority of cases, whereas without it we were able to obtain and analyse results.The 70b version of the model appears to have this prompt embedded, which led to many of the tasks not being answered.Rttger et al. [47] claim that in some cases we may have gone too far in trying to prevent models from responding to unsafe prompts and refer to exaggerated safety, showing that LLMs refuse to comply with safe requests when they include language that is sensitive or may be included in unsafe prompts.</p>
<p>Conclusion</p>
<p>Despite the widespread adoption of LLMs, researchers are still developing methods to assess and evaluate their capabilities.In this paper, we treat these models as if they are participants in cognitive experiments, an approach that has been taken in a growing body of literature.In so doing, we analyse the reasoning they display.We have shown that the seven models assessed in this paper show a different type of irrationality to that displayed by humans; this irrationality is observed across two dimensions.First, the responses given by the LLMs often display incorrect reasoning that differs from cognitive biases observed in humans.This may mean errors in calculations, or violations to rules of logic and probability, or simple factual inaccuracies.Second, the inconsistency of responses reveals another form of irrationality -there is significant variation in the responses given by a single model for the same task.This has implications for potential uses of these models in critical applications and scenarios, such as diplomacy [48,49] or medicine [50].Therefore, the work presented here can serve as a starting point for dealing with safety aspects of LLMs with respect to rational reasoning.This paper provides a methodological contribution to show how the rational reasoning abilities of these types of models can be assessed and compared.The proposed methodology has potential wider applications in studying cognitive abilities of LLMs.These tasks were originally designed for human reasoning, and given that LLMs attempt to simulate human-like language, using these tasks allows us to evaluate whether this is the case.</p>
<p>AIDS task</p>
<p>The probability that someone is infected with HIV is 0.01%.The test recognizes HIV virus with 100% probability if it is present.So, the test is positive.The probability of getting a positive test result when you don't really have the virus is only 0.01%.The test result for your friend is positive.The probability that your friend is infected with the HIV virus is therefore: ___ % [11] adapted from [9] AIDS task (facilitated)</p>
<p>This task involves an assessment of the results of the AIDS test.It is known that HIV can cause AIDS.Now imagine the following: A friend of yours gave blood at the hospital.It will then be checked to see if HIV is present in the blood.The test result is positive.How likely is it that your friend is actually infected with the HIV?To answer this question, you will need the following information: Out of 10,000 people, 1 person is infected with HIV.If the person is infected with the HIV, the test detects HIV.So the test is positive.Only 1 of the 9,999 people who are not infected with HIV have a positive test.</p>
<p>The test result for your friend is positive.How many people who have received a positive test result are actually infected with HIV? ___ from ___.</p>
<p>[11]</p>
<p>Hospital problem</p>
<p>In hospital A about 100 children are born per month.In hospital B about 10 children are born per month.The probability of the birth of a boy or a girl is about 50 percent each.Which of the following statements is right, which is wrong?</p>
<p>The probability that once in a month more than 60 percent of boys will be born is. . .(a) . . .larger in hospital A (b) . . .larger in hospital B (c) . . .equally big in both hospitals [11], adapted from [5,6] Monty Hall problem A candidate on a quiz show can choose one of three doors.Behind one of the doors is the main prize, a car.Behind the other two doors, there are two goats.The rules of the game are now as follows: The quizmaster knows behind which of the doors the car and the goats are.After the candidate has chosen one of the doors, it remains locked for the time being.</p>
<p>The quizmaster then opens one of the other two doors.He always opens a door with a goat behind it.Imagine that the candidate chooses door 1.Instead of opening this door, the quizmaster opens another door, behind which there is a goat.He now offers the candidate the option of switching his choice to the last unopened door.Should the candidate switch to the door or not?[10,11]</p>
<p>Figure 1 :
1
Figure 1: Default system prompt for Llama 2 7b and 13b.</p>
<p>Figure 2 :
2
Figure 2: Example response to the Monty Hall problem by Llama 2 7b (emphasis added).</p>
<p>Figure 4 :
4
Figure 4: Example response to the Wason task (facilitated) by Bard (emphasis added).</p>
<p>Figure 5 :
5
Figure5: Proportion of correct vs human-like responses across all tasks for each language model.Graph also depicts the proportion of responses which did not contain an answer or where there was a refusal to provide an answer.Correct responses include those those with correct (logical) reasoning, as well as those with incorrect (illogical) reasoning that reached the correct answer.Human-like responses include those that are correct with logical reasoning, and those that are incorrect but are achieved through a studied human cognitive bias.</p>
<p>Figure 6 :
6
Figure 6: Example response to the Linda problem by Llama 2 70b.</p>
<p>Figure 7 :
7
Figure 7: Result comparison for tasks that had two versions.For the Wason task, AIDS task and Monty Hall problem, the second set of results corresponds to the facilitated version.For the birth order problem, the second set of results corresponds to the version with a random order.For all four tasks, the second set of results (shown on the right) correspond to the task that human participants more often get right.Aside from the AIDS task, none of the tasks mimic this pattern.</p>
<p>Figure 8 :
8
Figure 8: Proportion of correct and human-like responses (includes only responses with logical reasoning) in mathematical vs. non-mathematical tasks.</p>
<p>the following rule: If there is a vowel on one side of the card, there is an even number on the other side.You see four cards now: cards must in any case be turned over to check the rule?(In other words: which cards could violate the rule above?) are working for the post office.You are responsible for checking whether the right stamp is affixed to a letter.The following rule applies: If a letter is sent to the USA, at least one 90-cent stamp must be affixed to it.There are four letters in front of you, of which you can see either the front or the back.(a) Letter 1: 90-cent stamp on the front (b) Letter 2: Italy marked on the back (c) Letter 3: 50-cents stamp on the front (d) Letter 4: USA marked on the back Which of the letters do you have to turn over in any case if you want to check compliance with this rule?</p>
<p>Table 2 :
2
Categorisation of responses.
CorrectIncorrectHuman-likeCorrect (logical) reasoningStudied biasOther responseNon-human-likeIncorrect (illogical) reasoning(including correct reasoning butincorrect response)CorrectCorrectIncorrectIncorrectIncorrectNo answer(R)(IR)(H)(NH)(CR)GPT-3.50.2920.0420.2170.4080.0330.008GPT-40.6920.1170.0420.1420.0080.000Bard0.3580.2330.0830.1920.1330.000Claude 20.5500.1000.1250.1080.1080.008Llama 2 7b0.0250.1920.1670.6080.0000.008Llama 2 13b0.0500.1920.0330.7000.0000.025Llama 2 70b0.1500.0500.0000.3330.0500.417</p>
<p>Table 3 :
3
Aggregated results.R: reasoned, IR: incorrect reasoning, H: human-like, NH: non-humanlike, CR: correct reasoning.Both Incorrect (NH) and Incorrect (CR) belong to the incorrect &amp; non-human-like categorisation.</p>
<p>Appendix 1 LLM Prompting 1.1 GPT-3.5 and GPT-4To prompt the OpenAI models, GPT-3.5 and GPT-4, we used the OpenAI API.The code for replication can be found in the following GitHub repository: https://github.com/oliviams/LLM_Rationality.All other modelsFor all models aside from the OpenAI ones we accessed them through their online chatbot interfaces.All default parameter settings were kept, aside from the following default prompt that is included for the 7 and 13 billion parameter versions of Llama 2: System prompt -Llama 2 7b and 13b You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.In order to ensure that the previous prompts were not affecting the following responses, the models were reinitialised at each instance rather than adding the task prompt to the current chat.The models were accessed via the following links within the dates 29 th August -22 nd September 2023: In how many of these three constellations should the candidate switch from his first choice to the remaining unopened door?In ___ of 3 constellations.What should the candidate therefore do?Stay or switch?[11]Linda problem Linda is 31 years old, single, very intelligent, and speaks her mind openly.She studied philosophy.During her studies, she dealt extensively with questions of equality and social justice and participated in anti-nuclear demonstrations.Now order the following statements about Linda according to how likely they are.Which statement is more likely?(a) Linda is a bank clerk.(b) Linda is active in the feminist movement.(c) Linda is a bank clerk and is active in the feminist movement.[Birth sequence problem (random)All families with six children in a city were surveyed.In seventy-two families, the exact order of births of boys (B) and girls (G) was GBGBBG.What is your estimate of the number of families surveyed in which the exact order of births was BGBBBB?[5]Birth sequence problem (ordered)All families with six children in a city were surveyed.In seventy-two families, the exact order of births of boys (B) and girls (G) was GBGBBG.What is your estimate of the number of families surveyed in which the exact order of births was BBBGGG?[5]High school problemThere are two programs in a high school.In many rounds of the game, will there be more results of type I or of type II?[5]Table5: Task Description
Language Model Behavior: A Comprehensive Survey. Tyler A Chang, Benjamin K Bergen, Computational Linguistics. 2023</p>
<p>Rationality and Intelligence: A Brief Update. Stuart Russell, Fundamental Issues of Artificial Intelligence. Vincent Mller, Springer2016</p>
<p>Olivia Macmillan, - Scott, Mirco Musolesi, arXiv preprint: 2311.17165Ir)rationality in AI: State of the Art, Research Challenges and Open Questions. 2023</p>
<p>Edward Stein, Without Good Reason: The Rationality Debate in Philosophy and Cognitive Science. Clarendon Press1996</p>
<p>Subjective probability: A judgment of representativeness. Daniel Kahneman, Amos Tversky, Cognitive Psychology. 331972</p>
<p>Judgment under Uncertainty: Heuristics and Biases. Amos Tversky, Daniel Kahneman, Science. 18541571974</p>
<p>Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment. Amos Tversky, Daniel Kahneman, Psychological Review. 9041983</p>
<p>C Peter, Wason, New Horizons in Psychology. B Foss, Penguin Books1966</p>
<p>Probabilistic reasoning in clinical medicine: Problems and opportunities. David M Eddy, Judgment under Uncertainty: Heuristics and Biases. Paul Daniel Kahneman, Amos Slovic, Tversky, Cambridge University Press1982</p>
<p>Monty Hall's Three Doors: Construction and Deconstruction of a Choice Anomaly. Daniel Friedman, The American Economic Review. 8841998</p>
<p>Tversky and Kahneman's Cognitive Illusions: Who Can Solve Them, and Why?. Georg Bruckmaier, Stefan Krauss, Karin Binder, Sven Hilbert, Martin Brunner, Frontiers in Psychology. 122021</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>The psychology of preferences. Daniel Kahneman, Amos Tversky, Scientific American. 24611982</p>
<p>Performance vs. competence in human-machine comparisons. Proceedings of the National Academy of Sciences. 117432020Chaz Firestone</p>
<p>Can language models handle recursively nested grammatical structures? a case study on comparing models and humans. Andrew K Lampinen, arXiv preprint: 2210.153032023</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, Nature Computational Science. 3102023</p>
<p>Can AI language models replace human participants?. Danica Dillion, Niket Tandon, Yuling Gu, Kurt Gray, Trends in Cognitive Sciences. 2772023</p>
<p>AI language models cannot replace human research participants. Jacqueline Harding, N G William D'alessandro, Robert Laskowski, Long, 2023AI &amp; Society</p>
<p>Machine behaviour. Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-Franois Bonnefon, Cynthia Breazeal, Jacob Crandall, Nicholas Christakis, Iain Couzin, Matthew Jackson, Nature. 5682019</p>
<p>Whose Opinions Do Language Models Reflect?. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, arXiv preprint: 2303.175482023</p>
<p>Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata, arXiv preprint: 2305.14930Context Impersonation Reveals Large Language Models' Strengths and Biases. 2023</p>
<p>Turning large language models into cognitive models. Marcel Binz, Eric Schulz, arXiv preprint: 2306.039172023</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23). the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Large pre-trained language models contain human-like biases of what is right and wrong to do. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, Kristian Kersting, Nature Machine Intelligence. 432022</p>
<p>Large language models show human-like content biases in transmission chain experiments. Alberto Acerbi, Joseph M Stubbersfield, Proceedings of the National Academy of Sciences. 12044e23137901202023</p>
<p>Large Language Models and the Patterns of Human Language Use: An Alternative View of the Relation of AI to Understanding and Sentience. Christoph Durt, Tom Froese, Thomas Fuchs, 2023Preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv preprint: 2303.113662023</p>
<p>Aditya Gulati, Miguel Angel Lozano, Bruno Lepri, Nuria Oliver, arXiv preprint: 2210.01122BIASeD: Bringing Irrationality into Automated System Design. 2023</p>
<p>Sotiris Lamprinidis, arXiv preprint: 2307.11787LLM Cognitive Judgements Differ From Human. 2023</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Hannah R Chan, Antonia Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv preprint: 2207.070512023</p>
<p>Bart Holterman, Kees Van Deemter, arXiv preprint: 2305.14020Does ChatGPT have Theory of Mind. 2023</p>
<p>Exploring the Intersection of Rationality, Reality, and Theory of Mind in AI Reasoning: An Analysis of GPT-4's Responses to Paradoxes and ToM Tests. Lucas Freund, 2023Preprint</p>
<p>Yiting Chen, Tracy Xiao Liu, You Shan, Songfa Zhong, arXiv preprint: 2305.12763The Emergence of Economic Rationality of GPT. 2023</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 72023</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome Han, Keith J Ransom, Andrew Perfors, Charles Kemp, Cognitive Systems Research. 831011552024</p>
<p>The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs. Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktschel, Edward Grefenstette, Proceedings of the 37th Conference on Neural Information Processing Systems. the 37th Conference on Neural Information Processing Systems2023</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv preprint: 2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov, arXiv preprint: 2308.002252023</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, Findings of the Association for Computational Linguistics: EMNLP-22. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish202033</p>
<p>GPT-4 Technical Report. 2023OpenAIOpenAITechnical report</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak2022Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. LaMDA: Language Models for Dialog Applications. arXiv preprint: 2201.08239</p>
<p>Model Card and Evaluations for Claude Models. Anthropic, 2023AnthropicTechnical report</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint: 2307.09288</p>
<p>The bounded rationality of probabilistic mental models. Gerd Gigerenzer, Rationality: Psychological and philosophical perspectives. K I Manktelow, D E Over, Taylor &amp; Frances/Routledge1993</p>
<p>Reasoning the fast and frugal way: models of bounded rationality. Gerd Gigerenzer, Daniel Goldstein, Psychological Review. 1031996</p>
<p>Paul Rttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, Dirk Hovy, arXiv preprint: 2308.01263XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. 2023</p>
<p>Escalation Risks from Language Models in Military and Diplomatic Decision-Making. Juan-Pablo Rivera, Gabriel Mukobi, Anka Reuel, Max Lamparth, Chandler Smith, Jacquelyn Schneider, arXiv preprint: 2401.034082024</p>
<p>Andrew Moore, How AI Could Revolutionize Diplomacy. Foreign Policy. 2023. February 9th, 2024</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature Medicine. 292023</p>            </div>
        </div>

    </div>
</body>
</html>
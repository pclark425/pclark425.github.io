<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-678 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-678</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-678</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-5baa4a4b53deb79fec20bb1c131cdc7bf1a202f6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5baa4a4b53deb79fec20bb1c131cdc7bf1a202f6" target="_blank">Grounding Data Science Code Generation with Input-Output Specifications</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GIFT4Code is proposed, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications that leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal to facilitate instruction fine-tuning.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000. The results demonstrate a significant improvement in the LLM's ability to generate code that is not only executable but also accurately aligned with user specifications, substantially improving the quality of code generation for complex data science tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e678.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e678.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL-code misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural Language Intent vs Generated Code Misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed mismatch where a code LLM produces plausible, executable code that does not satisfy the developer's natural-language intent because the NL intent is ambiguous or underspecified for complex data-science tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code LLM for data-science notebook completion</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A PaLM-based decoder-only code language model (62B) used to translate developer NL intents (often short, interactive prompts) plus programmatic context into Python code manipulating complex objects (pandas DataFrames, tensors) in notebooks.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer NL intent (short interactive prompts / question-style)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Jupyter notebook Python code / completion snippets</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / incomplete specification (NL intent underspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Many real-world data-science intents are too concise or ambiguous to uniquely define the correct transformation or aggregation; the model therefore generates code that is syntactically valid and executable but implements a different interpretation than the user intended (examples: failing to group by a requested column, or aggregating wrong columns).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>problem specification / code generation stage (mapping NL intent to program semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Execution of generated code and comparing execution outputs to intended I/O behavior (annotated program or human-intent); qualitative inspection of failure examples (e.g., Fig.2) showing generated code not matching intent.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>pass@k metrics (pass@5/pass@20 for ARCADE; pass@1 for DS-1000) measured before/after remediation; functional-equivalence heuristics on execution outputs comparing predicted program output to annotated reference; qualitative error-type counts (schema errors, syntax errors) and execution rate.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantially lowers semantic correctness (pass@k). Example: base code LLM pass@20 on ARCADE (Full Context) = 37.47% → GIFT4CODE (no spec synthetic FT) = 46.94% → with I/O Summary fine-tuning = 55.47%, demonstrating that misalignment materially reduces task success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in data-science benchmarks: ARCADE reports >50% of problems as under-specified; synthetic code sampling showed ~60% of raw model samples were executable but many did not match intent.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or concise NL intents combined with lack of training data that pairs intents with precise I/O constraints; models are not sufficiently grounded to program execution states.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>GIFT4CODE: generate synthetic NL intents + code, execute candidates to derive execution-based I/O specifications (TypeDesc, I/O Examples, I/O Summaries), then instruction-fine-tune the code LLM on examples where intents are augmented with these I/O specs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: fine-tuning with execution-derived I/O summaries produced the largest gains (e.g., pass@20 Full Context from 46.94% → 55.47% when using I/O Summary in training); synthetic fine-tuning without specs also produced large gains relative to base.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / code generation (data-science notebooks)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Data Science Code Generation with Input-Output Specifications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e678.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e678.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>I/O spec non-adherence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failure to Follow Input-Output Specifications</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code LLMs frequently generate code that violates explicit I/O specifications appended to the NL intent — producing outputs that do not match provided I/O examples, types, or NL I/O summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NL + I/O specification conditioned code generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generation pipeline where the model receives an NL intent plus additional I/O constraints (variable type descriptions, concrete I/O examples, or LLM-produced NL I/O summaries) and returns Python code expected to satisfy those constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>I/O specifications (TypeDesc, concrete I/O examples, LLM-generated I/O summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>notebook Python snippet / program solution</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/ignored specification (model disregards or misuses specifications)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Despite providing I/O specs, vanilla code LLMs often ignore or misinterpret these constraints and produce code that fails to satisfy them (e.g., calling non-existing columns, returning tables with different schema/layout than specified).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>schema understanding / variable referencing / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Execute generated code and trace input/output variables; compare outputs to I/O examples or to the salient columns/types described in the I/O summary; observe runtime errors (KeyError) or semantic mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Error-type counts (schema errors, KeyErrors), execution rate, and pass@k; authors report reduced schema-understanding errors when I/O specs are present but also changes in syntax-error frequency (Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>When specifications are ignored, user effort increases (developers must fix outputs); pass@k remains low for models that fail to utilize specs. Few-shot prompting with specs reduced schema errors but increased syntax errors; instruction fine-tuning with execution-derived specs (GIFT4CODE) both reduces schema errors and syntax errors and raises pass@k.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common across dataset: many ARCADE tasks are under-specified and DS-1000 contains diverse I/O types; the paper reports that few-shot prompting reduces but does not remove schema errors, and that vanilla LLMs 'often fail to follow intents with additional semantic constraints'.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model not trained to condition on program-execution-derived constraints; high diversity in how people express I/O specs; mismatch between exemplar/spec formats and real-world spec variants.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Derive I/O specifications from actual program execution of synthetic solutions and fine-tune model (GIFT4CODE) on parallel data of (context, intent+I/O spec) → solution; include diverse spec types (TypeDesc, I/O Examples, I/O Summary) during training; filter synthetic solutions by executability and API diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative improvements: adding I/O Summaries during fine-tuning yielded largest gains (e.g., ARCADE pass@20 Full Context improved to 55.47% with GIFT4CODE + I/O Summary). Fig.3 shows GIFT4CODE reduces schema and syntax errors relative to few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis / data-science code generation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Data Science Code Generation with Input-Output Specifications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e678.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e678.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Executable-but-wrong</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Executable-but-Semantically-Incorrect Code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A failure mode where model outputs are syntactically correct and execute without error but produce semantically incorrect results that do not match the intended behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code generation evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure of sampling multiple completions from the code LLM, executing them in the contextualized program state, and judging correctness via output comparison heuristics / annotated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer intent with/without I/O specs</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python notebook code executed to produce outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>semantic mismatch despite executability</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>High execution rate of generated programs does not imply high semantic correctness: baseline models often produce many executable but incorrect solutions (irrelevant outputs or wrong aggregations).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>program execution / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compute execution rate (fraction of generated samples that run without runtime errors) and compare to pass@k (fraction judged correct by output equivalence heuristics); inspect examples where code executes but output differs from annotated reference.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Execution rate vs pass@k plots (Fig.3), counts of error types; authors note that SyntheticFT decreased execution rate while improving semantic alignment (more schema errors but fewer irrelevant executable solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can inflate perceived model competence if only executability is used as a proxy; leads to false confidence and reduces real task success. The paper reports models with high execution rates but lower pass@k compared to GIFT4CODE models.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed frequently in baseline code LLM outputs; approx. 60% of sampled code was executable in synthetic data generation, but many executable outputs were not semantically correct; specific numeric execution rates are plotted in Fig.3 (no single number provided across all settings).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Evaluation that conflates executability with correctness; NL ambiguity leading to alternative plausible implementations that don't match the intended semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use execution-derived I/O specifications during training to force models to ground to execution states and to prefer implementations whose execution outputs align with intended I/O behavior; use functional equivalence heuristics for evaluation rather than executability alone.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Improved semantic correctness (higher pass@k) even if raw executability changed; GIFT4CODE attains higher pass@k than baseline despite different execution rates (Table 2 and Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Data Science Code Generation with Input-Output Specifications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e678.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e678.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noisy-spec mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inaccurate or Noisy LLM-Generated I/O Summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When I/O specifications themselves are produced by an LLM (to simulate user-provided specs), they can be noisy or inaccurate and thus introduce new mismatches between the NL description and actual code behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>I/O summary generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A 'generalist' LLM (PALM2) is used to produce NL I/O summaries from execution traces; at test time the authors deliberately withhold concrete variable states to simulate noisy user-provided I/O summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>LLM-generated I/O summaries (natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python notebook code solutions evaluated against ground-truth outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>noisy / inaccurate specification leading to misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some LLM-produced I/O summaries omit or mis-describe salient output details (Listing 6 shows an example where the generated summary differs from the ground-truth output), which can mislead the code LLM if used as input, leading to incorrect code generation or failed matches.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>specification generation / input to code LLM</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compare LLM-generated I/O summaries to ground-truth execution outputs and annotated references; observe mismatches and downstream generation failures (example in Appendix C, Listing 6).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative example-based comparison (examples provided); overall system evaluated with simulated noisy summaries at test time and measured using pass@k (results still improved with GIFT4CODE despite noise).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Noisy summaries can cause incorrect code generation, but using execution-derived summaries for training still improved robustness: GIFT4CODE trained on such summaries achieved the best pass@k. The paper explicitly simulates noisy summaries at test time and still reports improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Occasional — concrete examples in appendices show mismatches; not quantified across entire dataset but authors note this phenomenon and simulate it during testing.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Limitations of the 'generalist' LLM as a labeler (imperfect summarization) and deliberate obscuring of concrete I/O states to simulate realistic, noisy user specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Train the code LLM on a mixture of (intent + execution-derived I/O specs) including noisy variants, filter synthetic examples by executability and API diversity, and use multiple types of specs (TypeDesc, I/O Examples, I/O Summary) to increase robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Despite noise, instruction fine-tuning with I/O summaries produced the largest net gains in pass@k (e.g., ARCADE pass@20 Full Context to 55.47%), indicating the approach improves robustness to noisy specs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis / data-science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Data Science Code Generation with Input-Output Specifications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e678.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e678.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spec-format brittleness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Spec Exemplars vs Real-World Spec-Format Diversity (syntax errors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch between few-shot exemplars' specification formats and the wide variety of real-world schema/spec expressions causes models to produce syntax errors or malformed code when they attempt to use specs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Few-shot prompting with I/O specifications</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A prompting setup where few-shot exemplars include I/O specs; the code LLM is queried with similar spec-augmented prompts at inference time without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>few-shot exemplars containing I/O specifications (NL summaries or examples)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated Python snippets using schema names (string-valued column names etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>format mismatch leading to syntactic generation errors</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Few-shot prompting with specs reduced schema-understanding errors but led to many syntax errors (e.g., attempting to reference columns by unescaped names producing malformed identifiers like df. Engine volume), because exemplars cannot cover the massive diversity of real schema tokenizations and naming styles.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>code generation (string/identifier handling / tokenization of schema names)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Counted syntax-error frequency in generated samples and qualitatively inspected examples (Fig.2 Example 2 shows df. Engine volume causing syntax error). Fig.3 compares error-type frequencies across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Frequency counts of syntax errors vs schema errors vs execution rate (Fig.3); observed that few-shot prompting increased syntax errors compared to baseline and SyntheticFT.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Few-shot prompting with specs alone can worsen real-world performance by converting schema errors to syntax errors; this reduces executability and harms pass@k unless mitigated via fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified as a percentage but identified as a common failure mode in qualitative analysis and in aggregated error-type plots (Fig.3 shows a visible increase in syntax errors for few-shot-with-specs).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Inability of few-shot exemplars to cover the combinatorial variety of schema string formats; models overfit to exemplars' formatting and produce unescaped identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Instruction fine-tuning on synthetic data that includes diverse execution-derived specifications so the model learns robust ways to reference string-valued schema elements; filter training data for API diversity; combine with mixture datasets to keep context understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>GIFT4CODE reduced syntax errors relative to few-shot prompting with specs and achieved the best pass@k and execution/semantic tradeoffs (Fig.3 and Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program synthesis / data-science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounding Data Science Code Generation with Input-Output Specifications', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automating string processing in spreadsheets using input-output examples <em>(Rating: 2)</em></li>
                <li>RobustFill: Neural program learning under noisy I/O <em>(Rating: 2)</em></li>
                <li>Execution-guided neural program synthesis <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>DS1000: A natural and reliable benchmark for data science code generation <em>(Rating: 2)</em></li>
                <li>On the ingredients of an effective zero-shot semantic parser <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-678",
    "paper_id": "paper-5baa4a4b53deb79fec20bb1c131cdc7bf1a202f6",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "NL-code misalignment",
            "name_full": "Natural Language Intent vs Generated Code Misalignment",
            "brief_description": "Observed mismatch where a code LLM produces plausible, executable code that does not satisfy the developer's natural-language intent because the NL intent is ambiguous or underspecified for complex data-science tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code LLM for data-science notebook completion",
            "system_description": "A PaLM-based decoder-only code language model (62B) used to translate developer NL intents (often short, interactive prompts) plus programmatic context into Python code manipulating complex objects (pandas DataFrames, tensors) in notebooks.",
            "nl_description_type": "developer NL intent (short interactive prompts / question-style)",
            "code_implementation_type": "Jupyter notebook Python code / completion snippets",
            "gap_type": "ambiguous description / incomplete specification (NL intent underspecified)",
            "gap_description": "Many real-world data-science intents are too concise or ambiguous to uniquely define the correct transformation or aggregation; the model therefore generates code that is syntactically valid and executable but implements a different interpretation than the user intended (examples: failing to group by a requested column, or aggregating wrong columns).",
            "gap_location": "problem specification / code generation stage (mapping NL intent to program semantics)",
            "detection_method": "Execution of generated code and comparing execution outputs to intended I/O behavior (annotated program or human-intent); qualitative inspection of failure examples (e.g., Fig.2) showing generated code not matching intent.",
            "measurement_method": "pass@k metrics (pass@5/pass@20 for ARCADE; pass@1 for DS-1000) measured before/after remediation; functional-equivalence heuristics on execution outputs comparing predicted program output to annotated reference; qualitative error-type counts (schema errors, syntax errors) and execution rate.",
            "impact_on_results": "Substantially lowers semantic correctness (pass@k). Example: base code LLM pass@20 on ARCADE (Full Context) = 37.47% → GIFT4CODE (no spec synthetic FT) = 46.94% → with I/O Summary fine-tuning = 55.47%, demonstrating that misalignment materially reduces task success rates.",
            "frequency_or_prevalence": "High in data-science benchmarks: ARCADE reports &gt;50% of problems as under-specified; synthetic code sampling showed ~60% of raw model samples were executable but many did not match intent.",
            "root_cause": "Ambiguous or concise NL intents combined with lack of training data that pairs intents with precise I/O constraints; models are not sufficiently grounded to program execution states.",
            "mitigation_approach": "GIFT4CODE: generate synthetic NL intents + code, execute candidates to derive execution-based I/O specifications (TypeDesc, I/O Examples, I/O Summaries), then instruction-fine-tune the code LLM on examples where intents are augmented with these I/O specs.",
            "mitigation_effectiveness": "Effective: fine-tuning with execution-derived I/O summaries produced the largest gains (e.g., pass@20 Full Context from 46.94% → 55.47% when using I/O Summary in training); synthetic fine-tuning without specs also produced large gains relative to base.",
            "domain_or_field": "machine learning / code generation (data-science notebooks)",
            "reproducibility_impact": true,
            "uuid": "e678.0",
            "source_info": {
                "paper_title": "Grounding Data Science Code Generation with Input-Output Specifications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "I/O spec non-adherence",
            "name_full": "Failure to Follow Input-Output Specifications",
            "brief_description": "Code LLMs frequently generate code that violates explicit I/O specifications appended to the NL intent — producing outputs that do not match provided I/O examples, types, or NL I/O summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NL + I/O specification conditioned code generation",
            "system_description": "Generation pipeline where the model receives an NL intent plus additional I/O constraints (variable type descriptions, concrete I/O examples, or LLM-produced NL I/O summaries) and returns Python code expected to satisfy those constraints.",
            "nl_description_type": "I/O specifications (TypeDesc, concrete I/O examples, LLM-generated I/O summaries)",
            "code_implementation_type": "notebook Python snippet / program solution",
            "gap_type": "incomplete/ignored specification (model disregards or misuses specifications)",
            "gap_description": "Despite providing I/O specs, vanilla code LLMs often ignore or misinterpret these constraints and produce code that fails to satisfy them (e.g., calling non-existing columns, returning tables with different schema/layout than specified).",
            "gap_location": "schema understanding / variable referencing / code generation",
            "detection_method": "Execute generated code and trace input/output variables; compare outputs to I/O examples or to the salient columns/types described in the I/O summary; observe runtime errors (KeyError) or semantic mismatches.",
            "measurement_method": "Error-type counts (schema errors, KeyErrors), execution rate, and pass@k; authors report reduced schema-understanding errors when I/O specs are present but also changes in syntax-error frequency (Fig.3).",
            "impact_on_results": "When specifications are ignored, user effort increases (developers must fix outputs); pass@k remains low for models that fail to utilize specs. Few-shot prompting with specs reduced schema errors but increased syntax errors; instruction fine-tuning with execution-derived specs (GIFT4CODE) both reduces schema errors and syntax errors and raises pass@k.",
            "frequency_or_prevalence": "Common across dataset: many ARCADE tasks are under-specified and DS-1000 contains diverse I/O types; the paper reports that few-shot prompting reduces but does not remove schema errors, and that vanilla LLMs 'often fail to follow intents with additional semantic constraints'.",
            "root_cause": "Model not trained to condition on program-execution-derived constraints; high diversity in how people express I/O specs; mismatch between exemplar/spec formats and real-world spec variants.",
            "mitigation_approach": "Derive I/O specifications from actual program execution of synthetic solutions and fine-tune model (GIFT4CODE) on parallel data of (context, intent+I/O spec) → solution; include diverse spec types (TypeDesc, I/O Examples, I/O Summary) during training; filter synthetic solutions by executability and API diversity.",
            "mitigation_effectiveness": "Quantitative improvements: adding I/O Summaries during fine-tuning yielded largest gains (e.g., ARCADE pass@20 Full Context improved to 55.47% with GIFT4CODE + I/O Summary). Fig.3 shows GIFT4CODE reduces schema and syntax errors relative to few-shot prompting.",
            "domain_or_field": "machine learning / program synthesis / data-science code generation",
            "reproducibility_impact": true,
            "uuid": "e678.1",
            "source_info": {
                "paper_title": "Grounding Data Science Code Generation with Input-Output Specifications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Executable-but-wrong",
            "name_full": "Executable-but-Semantically-Incorrect Code",
            "brief_description": "A failure mode where model outputs are syntactically correct and execute without error but produce semantically incorrect results that do not match the intended behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code generation evaluation pipeline",
            "system_description": "Procedure of sampling multiple completions from the code LLM, executing them in the contextualized program state, and judging correctness via output comparison heuristics / annotated programs.",
            "nl_description_type": "developer intent with/without I/O specs",
            "code_implementation_type": "Python notebook code executed to produce outputs",
            "gap_type": "semantic mismatch despite executability",
            "gap_description": "High execution rate of generated programs does not imply high semantic correctness: baseline models often produce many executable but incorrect solutions (irrelevant outputs or wrong aggregations).",
            "gap_location": "program execution / evaluation",
            "detection_method": "Compute execution rate (fraction of generated samples that run without runtime errors) and compare to pass@k (fraction judged correct by output equivalence heuristics); inspect examples where code executes but output differs from annotated reference.",
            "measurement_method": "Execution rate vs pass@k plots (Fig.3), counts of error types; authors note that SyntheticFT decreased execution rate while improving semantic alignment (more schema errors but fewer irrelevant executable solutions).",
            "impact_on_results": "Can inflate perceived model competence if only executability is used as a proxy; leads to false confidence and reduces real task success. The paper reports models with high execution rates but lower pass@k compared to GIFT4CODE models.",
            "frequency_or_prevalence": "Observed frequently in baseline code LLM outputs; approx. 60% of sampled code was executable in synthetic data generation, but many executable outputs were not semantically correct; specific numeric execution rates are plotted in Fig.3 (no single number provided across all settings).",
            "root_cause": "Evaluation that conflates executability with correctness; NL ambiguity leading to alternative plausible implementations that don't match the intended semantics.",
            "mitigation_approach": "Use execution-derived I/O specifications during training to force models to ground to execution states and to prefer implementations whose execution outputs align with intended I/O behavior; use functional equivalence heuristics for evaluation rather than executability alone.",
            "mitigation_effectiveness": "Improved semantic correctness (higher pass@k) even if raw executability changed; GIFT4CODE attains higher pass@k than baseline despite different execution rates (Table 2 and Fig.3).",
            "domain_or_field": "machine learning / program synthesis / evaluation methodology",
            "reproducibility_impact": true,
            "uuid": "e678.2",
            "source_info": {
                "paper_title": "Grounding Data Science Code Generation with Input-Output Specifications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Noisy-spec mismatch",
            "name_full": "Inaccurate or Noisy LLM-Generated I/O Summaries",
            "brief_description": "When I/O specifications themselves are produced by an LLM (to simulate user-provided specs), they can be noisy or inaccurate and thus introduce new mismatches between the NL description and actual code behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "I/O summary generation and evaluation",
            "system_description": "A 'generalist' LLM (PALM2) is used to produce NL I/O summaries from execution traces; at test time the authors deliberately withhold concrete variable states to simulate noisy user-provided I/O summaries.",
            "nl_description_type": "LLM-generated I/O summaries (natural language)",
            "code_implementation_type": "Python notebook code solutions evaluated against ground-truth outputs",
            "gap_type": "noisy / inaccurate specification leading to misalignment",
            "gap_description": "Some LLM-produced I/O summaries omit or mis-describe salient output details (Listing 6 shows an example where the generated summary differs from the ground-truth output), which can mislead the code LLM if used as input, leading to incorrect code generation or failed matches.",
            "gap_location": "specification generation / input to code LLM",
            "detection_method": "Compare LLM-generated I/O summaries to ground-truth execution outputs and annotated references; observe mismatches and downstream generation failures (example in Appendix C, Listing 6).",
            "measurement_method": "Qualitative example-based comparison (examples provided); overall system evaluated with simulated noisy summaries at test time and measured using pass@k (results still improved with GIFT4CODE despite noise).",
            "impact_on_results": "Noisy summaries can cause incorrect code generation, but using execution-derived summaries for training still improved robustness: GIFT4CODE trained on such summaries achieved the best pass@k. The paper explicitly simulates noisy summaries at test time and still reports improvements.",
            "frequency_or_prevalence": "Occasional — concrete examples in appendices show mismatches; not quantified across entire dataset but authors note this phenomenon and simulate it during testing.",
            "root_cause": "Limitations of the 'generalist' LLM as a labeler (imperfect summarization) and deliberate obscuring of concrete I/O states to simulate realistic, noisy user specifications.",
            "mitigation_approach": "Train the code LLM on a mixture of (intent + execution-derived I/O specs) including noisy variants, filter synthetic examples by executability and API diversity, and use multiple types of specs (TypeDesc, I/O Examples, I/O Summary) to increase robustness.",
            "mitigation_effectiveness": "Despite noise, instruction fine-tuning with I/O summaries produced the largest net gains in pass@k (e.g., ARCADE pass@20 Full Context to 55.47%), indicating the approach improves robustness to noisy specs.",
            "domain_or_field": "machine learning / program synthesis / data-science",
            "reproducibility_impact": true,
            "uuid": "e678.3",
            "source_info": {
                "paper_title": "Grounding Data Science Code Generation with Input-Output Specifications",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Spec-format brittleness",
            "name_full": "Few-shot Spec Exemplars vs Real-World Spec-Format Diversity (syntax errors)",
            "brief_description": "Mismatch between few-shot exemplars' specification formats and the wide variety of real-world schema/spec expressions causes models to produce syntax errors or malformed code when they attempt to use specs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Few-shot prompting with I/O specifications",
            "system_description": "A prompting setup where few-shot exemplars include I/O specs; the code LLM is queried with similar spec-augmented prompts at inference time without fine-tuning.",
            "nl_description_type": "few-shot exemplars containing I/O specifications (NL summaries or examples)",
            "code_implementation_type": "generated Python snippets using schema names (string-valued column names etc.)",
            "gap_type": "format mismatch leading to syntactic generation errors",
            "gap_description": "Few-shot prompting with specs reduced schema-understanding errors but led to many syntax errors (e.g., attempting to reference columns by unescaped names producing malformed identifiers like df. Engine volume), because exemplars cannot cover the massive diversity of real schema tokenizations and naming styles.",
            "gap_location": "code generation (string/identifier handling / tokenization of schema names)",
            "detection_method": "Counted syntax-error frequency in generated samples and qualitatively inspected examples (Fig.2 Example 2 shows df. Engine volume causing syntax error). Fig.3 compares error-type frequencies across methods.",
            "measurement_method": "Frequency counts of syntax errors vs schema errors vs execution rate (Fig.3); observed that few-shot prompting increased syntax errors compared to baseline and SyntheticFT.",
            "impact_on_results": "Few-shot prompting with specs alone can worsen real-world performance by converting schema errors to syntax errors; this reduces executability and harms pass@k unless mitigated via fine-tuning.",
            "frequency_or_prevalence": "Not quantified as a percentage but identified as a common failure mode in qualitative analysis and in aggregated error-type plots (Fig.3 shows a visible increase in syntax errors for few-shot-with-specs).",
            "root_cause": "Inability of few-shot exemplars to cover the combinatorial variety of schema string formats; models overfit to exemplars' formatting and produce unescaped identifiers.",
            "mitigation_approach": "Instruction fine-tuning on synthetic data that includes diverse execution-derived specifications so the model learns robust ways to reference string-valued schema elements; filter training data for API diversity; combine with mixture datasets to keep context understanding.",
            "mitigation_effectiveness": "GIFT4CODE reduced syntax errors relative to few-shot prompting with specs and achieved the best pass@k and execution/semantic tradeoffs (Fig.3 and Table 2).",
            "domain_or_field": "machine learning / program synthesis / data-science",
            "reproducibility_impact": true,
            "uuid": "e678.4",
            "source_info": {
                "paper_title": "Grounding Data Science Code Generation with Input-Output Specifications",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automating string processing in spreadsheets using input-output examples",
            "rating": 2
        },
        {
            "paper_title": "RobustFill: Neural program learning under noisy I/O",
            "rating": 2
        },
        {
            "paper_title": "Execution-guided neural program synthesis",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "DS1000: A natural and reliable benchmark for data science code generation",
            "rating": 2
        },
        {
            "paper_title": "On the ingredients of an effective zero-shot semantic parser",
            "rating": 1
        }
    ],
    "cost": 0.016278,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grounding Data Science Code Generation with Input-Output Specifications</h1>
<p>Yeming Wen ${ }^{1}$ Pengcheng Yin ${ }^{2}$ Kensen Shi ${ }^{2}$ Henryk Michalewski ${ }^{2}$ Swarat Chaudhuri ${ }^{1}$ Alex Polozov ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4CODE, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data science benchmarks, ARCADE and DS-1000. The results demonstrate a significant improvement in the LLM's ability to generate code that is not only executable but also accurately aligned with user specifications, substantially improving the quality of code generation for complex data science tasks.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have recently shown tremendous promise at generating code from natural language prompts (Chen et al., 2021a; Austin et al., 2021; Li et al., 2023; 2022; Nijkamp et al., 2022; Fried et al., 2022; Li et al., 2023). In particular, LLMs trained on code have been shown to excel at solving interview-style coding problems, represented by benchmarks such as HumanEval (Chen et al., 2021a). In these tasks, natural language (NL) "intents"</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(e.g. $\boldsymbol{x}$ in Fig. 1) can succinctly define the programming problem at hand. The objective of the LLM is to produce output that is aligned with this intent.</p>
<p>However, in the real world, not all programming tasks can be easily described using compact NL intents. Among these, data science programming stands out as a vital domain, where data scientists spend considerable time on data wrangling and exploratory data analysis (EDA). Whereas problems in benchmarks like HumanEval involve basic Python types that can be easily tested, data science tasks often involve complex operations on data structures like Pandas DataFrames or PyTorch Tensors. The inherent ambiguity in expressing these tasks through NL intents (e.g. green text in Fig. 1) often leads to misalignment between developers' requirements and the code generated (Yin et al., 2022).</p>
<p>Input-output (I/O) specifications (e.g. red text in Fig. 1), ranging from concrete I/O examples to high-level NL summaries, are a natural way to reduce the ambiguity of NL intents (Gulwani et al., 2015; Balog et al., 2016; Jain et al., 2022). Prior to the emergence of LLMs, specifications served as essential problem descriptions in program synthesis (Gulwani, 2016; Devlin et al., 2017; Shi et al., 2020). Real-world synthesis systems like FlashFill are testimony to the adoption and effectiveness of I/O specifications (Gulwani, 2011; Gulwani et al., 2012). In this work, we consider the problem of LLM-based code generation in data science when the prompt provided to the LLM consists of both a NL intent and an additional I/O specification.</p>
<p>Unfortunately, code LLMs often fail to follow intents with additional semantic constraints like I/O specifications out-of-the-box, leading to plausible solutions that fail to satisfy the provided specifications (e.g. $\boldsymbol{y}^{\prime}$, Fig. 1). This can pose unnecessary burden on developers who are then required to fix the generated code (Bird et al., 2023). Such a misalignment between the user's intent and the model's predictions primarily stems from the lack of training data formatted with such specifications.</p>
<p>Instruction fine-tuning has emerged as an effective strategy to tackle the issue of misalignment (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022). Classical approaches for instruction tuning typically require a substantial amount of parallel labeled data of NL intents and gold model responses. The process of gathering such data is labor-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left: Illustration of how users prompt code LLMs with NL intents and I/O specifications to generate code with complex outputs (pandas Dataframes). Vanilla code LLMs fail to understand I/O specifications. Right: Our proposed instruction tuning approach uses synthetic intents and code solutions, where intents are augmented with I/O specifications derived from program execution. Models trained on such synthetic data could better follow a user's intent.
intensive and time-consuming. Recent studies have suggested that generating synthetic instruction-following data using the LLM itself is a promising approach to improve alignment, with empirical success on natural language text generation tasks (Wang et al., 2022a; Honovich et al., 2022a; Taori et al., 2023; Peng et al., 2023, inter alia).</p>
<p>In this paper we build upon the recent success of instruction tuning using synthetic data and fine-tune code LLMs to follow NL intents with additional I/O specifications. Unlike existing approaches, our key insight is to leverage program execution for synthetic data generation. First, in contrast to other open-ended text generation tasks where assessing the quality of target responses is challenging, the quality of synthetic code generation data can be easily improved using heuristics such as code executability (Yin et al., 2022). Moreover, from the program execution states one could derive precise and aligned I/O specifications that can be included in the intents to supervise a model to follow those extra semantic constraints (Fig. 1, Right). In other words, when fine-tuned on such synthetic data, a model learns to ground NL task descriptions to program execution states expressed as I/O specifications (Berant et al., 2013).</p>
<p>We apply our grounded instruction fine-tuning for code (GIFT4CODE) method to two challenging natural language to code generation applications: synthesizing complex pandas programs in computational notebooks (ARCADE, Yin et al. (2022)) and answering data science questions on Stack Overflow (DS-1000, Lai et al. (2022)). First, we
demonstrate the value of leveraging program execution information by showing that strong code LLMs can still be improved after fine-tuning on our synthetic data without including any I/O specifications. Then, to further align model predictions to various types of user-provided I/O specifications, we derive those specifications at different levels of abstraction from code execution results. This ranges from concrete input/output examples to NL summaries of target variables (specifications in Fig. 1). By fine-tuning on parallel data of intents with I/O specifications and their target code solutions, the model is better at producing code that is more likely to execute to the desired outcome. Although our experiments focused on the data science domain, it's important to note that the methodology underlying GIFT4CODE is general and adaptable. It can be applied to different domains that require specifications for a precise task description.</p>
<h2>2 Problem Formulation</h2>
<p>Natural Language to Code Generation Code generation considers the task of translating a developer's natural language intent $\boldsymbol{x}$ into a machine-executable program $\boldsymbol{y}$ (e.g. Fig. 1, Left). An intent usually contains a succinct and potentially ambiguous task description. For tasks with complex outputs, the intent may also include additional I/O specifications as extra clarifications. ${ }^{1}$ Code generation tasks are often contextualized, meaning that an</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>intent is associated with certain programmatic contexts $\boldsymbol{c}$ (Iyer et al., 2018), such as the code that a developer has already written in an IDE prior to the intent (e.g. df = pd.read_csv("flights.csv") for $\boldsymbol{x}$, not shown in Fig. 1). Intuitively, a model needs to leverage both the intent and the programmatic context (e.g. variable df) to generate a suitable code solution.</p>
<p>Supervised Instruction Tuning for Code LLMs Supervised instruction tuning aims to improve code LLMs by fine-tuning them on parallel data of intents and target code solutions. In this paper we consider automatically synthesizing such parallel data by prompting LLMs using fewshot demonstrations (other approaches are discussed in §5). Specifically, the synthetic dataset consists of examples ${\langle\boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y}\rangle}$ of intents $\boldsymbol{x}$ with programmatic contexts $\boldsymbol{c}$ and their generated code solutions $\boldsymbol{y}$. The pivotal question then becomes how to create a high-quality synthetic ${\langle\boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y}\rangle}$ that facilitates the instruction fine-tuning process.</p>
<h2>3 Gift4Code: Learning to Follow Intents with I/O Specifications</h2>
<p>In this section we elaborate on GIFT4CODE, our proposed approach to fine-tune code LLMs to better follow developers' natural language intents along with I/O specifications, using synthetic parallel data. Fig. 1(Right) illustrates an overview of GIFT4CODE. We first synthesize a collection of intents with code solutions via few-shot prompting (§3.1), and then execute model-predicted code to derive I/O specifications from execution results (§3.2). Finally, we fine-tune the code LLM to predict code solutions given intents inlined with I/O specifications (§3.3).</p>
<h3>3.1 Generating Synthetic Intents and Code Solutions</h3>
<p>Programmatic Contexts We initialize a program state given some programmatic context and generate a series of contextualized NL-to-code problems for that context. As an example, the synthetic problems in Fig. 1 (Right) could have the contextual code df = pd.read_csv("world_statistics.csv"), which initializes the DataFrame variable df, subsequently used in the generated synthetic examples. The fact that our problems are contextualized sets our approach apart from existing instruction-tuning methods for text generation models (Wang et al., 2022a; Honovich et al., 2022a), where synthetic examples do not depend on any particular contexts. In our case, we mine those programmatic contexts from real-world code repositories, such as tabular datasets (e.g., .csv) used in data science notebooks on Github (§4).</p>
<p>Creating Initial NL Intents Given a programmatic context $\boldsymbol{c}$, we few-shot prompt an LLM to create a sequence of natural language intents $\left{\boldsymbol{x}<em 1="1">{i}\right}$ (e.g. $\boldsymbol{x}</em>$ in Fig. 1(Right)).}, \boldsymbol{x}_{2</p>
<p>A problem $\boldsymbol{x}<em _i="&lt;i">{i}$ that appears later in the sequence might depend on the earlier ones $\left{\boldsymbol{x}</em>\right}$ (Nijkamp et al., 2022; Yin et al., 2022). To generate NL intents, we use a "generalist" PALM2 LLM (Google, 2023), instead of the code LLM that we aim to improve, since predicting intents conditioned on some context is similar to other text generation tasks, which could be better handled by a LM trained on generalpurpose text data (Zelikman et al., 2022). The PALM2 LLM achieves competitive results with GPT-4 on a variety of NL reasoning tasks. Empirically, we observe that the problems generated by this LLM cover a wide range of tasks relevant to the given programmatic context. Readers can refer to Appendix B for examples. Note that those model-predicted intents do not come with I/O specifications yet.</p>
<p>Predicting Code Solutions After generating an intent $\boldsymbol{x}$, we then prompt the code LLM to get a code solution $\boldsymbol{y}$ for $\boldsymbol{x}$ (e.g. $\boldsymbol{y}_{1}$ in Fig. 1(Right)). Specifically, a prompt to the LLM is the concatenation of the programmatic context $\boldsymbol{c}$ and the intent $\boldsymbol{x}$, with additional few-shot demonstrations of $\left{\left\langle\boldsymbol{c}^{\prime}, \boldsymbol{x}^{\prime}, \boldsymbol{y}^{\prime}\right\rangle\right}$. Since many NL intents can be ambiguous and there could exist multiple alternative solutions (e.g. without additional I/O specifications, the intent in green in Fig. 1(Left) could be answered using tables with different layouts; see more in (Yin et al., 2022)), we therefore draw multiple candidate code solutions ${\boldsymbol{y}}$ for each intent. Intuitively, ${\boldsymbol{y}}$ could have a variety of alternative solutions for $\boldsymbol{x}$, each leading to different execution results. This equips the model with the capacity to predict code for the same task but with different user-provided I/O specifications.</p>
<p>Improving Quality of Synthetic Data The quality of synthetic data is a fundamental issue of data augmentation in instruction tuning (Wang et al., 2022a; Honovich et al., 2022a), and existing approaches in text generation typically resort to simple and noisy heuristics (e.g. rejecting examples with different inputs but the same output). As motivated in §1, for NL-to-code generation, we can reliably enhance the quality of candidate code solutions by leveraging inherent program properties, such as filtering out any code that is not executable given the provided programmatic context.</p>
<h3>3.2 Code Execution \&amp; Inference of I/O Specifications</h3>
<p>Given the set of synthetic problems ${\langle\boldsymbol{x},{\boldsymbol{y}}\rangle}$ generated by few-shot prompting, we execute the code for each problem (step 2, Fig. 1(Right)) and derive I/O specifications from the execution results as additional semantic constraints to be included in the intents (step 3, Fig. 1(Right)).</p>
<p>Specifically, for each candidate solution $\boldsymbol{y}$ of an intent, we first execute its original programmatic context $\boldsymbol{c}$, followed by executing $\boldsymbol{y}$. We trace the execution to collect the set of input and output variables in $\boldsymbol{y}$, denoted as ${v}$, which are used to derive I/O specifications (details below). Executing code with arbitrary programmatic contexts collected from</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Spec. Type</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Example I/O Specification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TypeDesc</td>
<td style="text-align: center;">Variable type name</td>
<td style="text-align: center;">Generate a variable with name df and type pandas . DataFrame</td>
</tr>
<tr>
<td style="text-align: center;">I/O Examples</td>
<td style="text-align: center;">Concrete I/O examples</td>
<td style="text-align: center;">Output variable df:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">[Bangalore(float)</td>
</tr>
<tr>
<td style="text-align: center;">I/O Summary</td>
<td style="text-align: center;">LLM-generated NL summaries of I/O examples</td>
<td style="text-align: center;">Given the user intent and the code, the salient columns (at most given 3) in the input dataframe are airline, source_city, destination_city. The output dataframe has columns (at most given 3) such as Delhi, Mumbai, Chennai.</td>
</tr>
</tbody>
</table>
<p>Table 1: Types of I/O specifications proposed in this work at different levels of abstraction. Example specifications are for the intent in Fig. 1(Left). Only the output specifications for I/O Examples are shown for brevity.
the wild is highly non-trivial due to issues such as library dependency. However, the use of synthetic data alleviates the need for a complex environment setup.</p>
<p>Given the set of input and output variables extracted from execution results, we formulate an I/O specification, denoted as $\boldsymbol{z}$, which serves as additional information to augment a developer's intent, thereby providing a more comprehensive problem description. The level of detail and the style of these I/O specifications can vary based on the complexity of the problem and the developer's preferences. In this work, we investigate three distinct types of I/O specifications, each characterized by its own linguistic style and level of abstraction, as illustrated in Tab. 1.</p>
<p>First, as a simple baseline, we utilize the variable type (TypeDesc, Tab. 1) as the I/O specification. Next, we incorporate the concrete values of the input/output variables into the specification, which we refer to as I/O Examples. This is reminiscent of classical program synthesis using I/O examples (Gulwani et al., 2012; Alur et al., 2013; Balog et al., 2016). However, in our scenario, these I/O examples are used in conjunction with natural language (NL) intents to define the problem, in line with (Jain et al., 2022). Given that the majority of the problems in our synthetic dataset involve complex Python objects like pandas DataFrames, we simplify the I/O specification to include only partial variable states (e.g. by excluding some rows and columns in large DataFrames). Please refer to Appendix C for details.</p>
<p>In our effort to generate a more natural variety of I/O specifications that closely resemble the style of specifications in developers' NL intents, we employ an LLM to summarize the values of input/output variables ${v}$ into a succinct natural language description $\boldsymbol{z}$ (I/O Summary). Intuitively, the NL I/O summary includes salient information in the variables that can best clarify the original intent (e.g. the subset of columns in a DataFrame that are most relevant to solve a problem, as in Tab. 1, Bottom).</p>
<p>Specifically, we few-shot prompt the "generalist" PALM2 to generate $\boldsymbol{z}$, using information from its programmatic context $\boldsymbol{c}$, the intent $\boldsymbol{x}$, the code solution $\boldsymbol{y}$, as well as I/O variables ${v}$, i.e. $\boldsymbol{z} \sim P_{\mathrm{LLM}}(\cdot \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y},{v})$. We then update the intent $\boldsymbol{x}$ by appending $\boldsymbol{z}$ to it. The few-shot exemplars used for prompting cover example I/O summaries for various types of Python objects, such as nested container types (e.g. nested dicts), along with more complex objects like Pandas DataFrames and PyTorch or Tensorflow Tensors. See Appendix C for additional details.</p>
<h3>3.3 Fine-tuning Code LLMs to Follow Intents with I/O Specifications</h3>
<p>Our approach, GIFT4CODE, aims to fine-tune code LLMs to generate code that adheres closely to the desired intents which are supplemented by I/O specifications. In our synthetic training data, each example $\langle\boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y}\rangle$ consists of a programmatic context $\boldsymbol{c}$, an intent $\boldsymbol{x}$ augmented with I/O specifications, and the corresponding code solution $\boldsymbol{y}$. During fine-tuning, the code LLM learns to generate code that not only satisfies the provided intents but also respects the specified I/O constraints, while leveraging any relevant information in the programmatic contexts. In other words, we optimize $P_{\mathrm{LLM}}(\boldsymbol{y} \mid \boldsymbol{c}, \boldsymbol{x})$. It is worth noting that the code LLM that undergoes this optimization is different from the "generalist" PALM2 LLM employed to generate the NL intents and I/O specification $\boldsymbol{z}$.</p>
<h2>4 Experiments</h2>
<p>The core research question explored in this section is whether GIFT4CODE enhances the LLM's ability to follow developers' NL intents with complex I/O specifications. While common code generation benchmarks like HumanEval (Chen et al., 2021a) and MBPP (Austin et al.,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2021) feature simple algorithmic tasks (e.g., sorting) utilizing basic Python data types (e.g., lists), thus allowing for the use of concrete I/O examples as specifications, they lack the diverse and complex I/O specifications that we aim to explore. For more open-ended tasks in the data science domain, the output data type is more complex and diverse (e.g., pandas DataFrames, pytorch Tensors ). Hence, we applied our method to two different data science code generation applications.</p>
<p>Arcade (Yin et al., 2022) is a benchmark of natural language to code generation in interactive data science notebooks. Each evaluation notebook consists of a series of interrelated NL-to-code problems in data wrangling (e.g. "Minmax normalize numeric columns") and exploratory data analysis (e.g. intents in Fig. 1) using the pandas library. ARCADE features succinct NL intents to reflect the style of ephemeral queries from developers when prompting LLMs for code completion. More than $50 \%$ of the dataset's problems are under-specified, which means that additional I/O specifications could provide extra clarification. To construct programmatic contexts for synthetic training data generation, we scraped 7,500 CSV files that are used in public Jupyter notebooks. Each context contains a DataFrame import statement, for example, df = pd.read_csv ( $\cdot$ ), followed by an NL description of the DataFrame to help the LLM understand its content. We generated 6 intents for each programmatic context and sampled 5 candidate code solutions for each intent. Roughly $60 \%$ of the code samples were executable. After filtering based on executability and API diversity (§3.1), we obtained around $20 K$ synthetic examples for instruction fine-tuning.</p>
<p>Our synthetic data only comprises pairs of questions and code samples which lack rich context. To avoid regression in context understanding during instruction fine-tuning, we crafted a mixture dataset which combines the synthetic data and the Python data used to fine-tune the code LLM. Note that this Python data does not contain any execution-based signals or I/O specifications. After approximately 1,500 instruction tuning steps with a batch size of 64 , the model reaches its optimal performance. This process consumed about 1.5 epochs of our synthetic dataset.</p>
<p>DS-1000 (Lai et al., 2022) is a benchmark of data science problems sourced from Stack Overflow (SO). Compared to ARCADE, problems in DS-1000 feature a wider variety of I/O types, such as numpy/scipy Arrays and pytorch/tensorflow Tensors, making it particularly appealing to evaluate our instruction tuning approach aimed at generating code following I/O specifications. However, in contrast to ARCADE which features succinct NL intents, DS-1000 follows the typical style of detailed problem descriptions found in SO posts. These elaborate descriptions often include additional information such as task
background and descriptions of unsuccessful attempts with an average length of 140 words. Given that such elaborate intents may not reflect the style of developers' prompts to code LLMs, we do not focus on generating intents with similar styles. Instead, we held-out 500 problems in DS1000 and use their annotated intents as training data, while evaluating on the remaining problems. ${ }^{3}$</p>
<h3>4.1 Setup</h3>
<p>Base Code LLM We use a PALM based, decoder-only code language model with 62B parameters (Chowdhery et al., 2022). The model was first pre-trained on a collection of 1.3 T tokens of web documents and github code data, and was then fine-tuned on a disjoint set of 64B Python code tokens together with 10B tokens from Python Jupyter notebooks. The fine-tuning process, including the GIFT4CODE procedure, can be conducted through the fine-tuning API on Google cloud Vertex AI. Notably, this base model has strong performance in data science programming tasks, outperforming the StarCoder-15B Python model on ARCADE, as evidenced in Tab. $2^{4}$.</p>
<p>Learning Methods We evaluated the performance of both the baseline and instruction-tuned models across a range of data formats, as shown in Tab. 2. For each I/O specification type, we augmented the intents and few-shot exemplars with specifications of the corresponding type. Similarly, at test time, we augmented the intents with the same type of I/O specifications. For the few-shot prompting, we manually created exemplars for all types of specifications. These exemplars were prepended to the prompt when querying the LLM for code generation.</p>
<p>Simulate Noisy I/O Specifications at Test Time At testing time, the generation of I/O Summary underwent a minor modification from the process detailed in $\S 3.2$. We remove the concrete input/output variable states ${v}$ to produce noisy I/O summaries, simulating scenarios where users might give noisy I/O specifications (Devlin et al., 2017). We illustrate an example in Appendix C where the LLM generates an imperfect specification. While the "generalist" LLM (PALM2) uses the code solution to generate noisy I/O summaries, we remark that the code LLM, which we aim to evaluate, does not have access to the ground truth solution. In other words, the "generalist" LLM acts merely as a "data labeler" to create I/O summaries in prompts in order to construct the evaluation dataset. It is also a common practice in program synthesis to derive specifications from</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Spec. Type</th>
<th style="text-align: center;">ARCADE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DS-1000 <br> pass@1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">pass@5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">pass@20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">No Context</td>
<td style="text-align: center;">Full Context</td>
<td style="text-align: center;">No Context</td>
<td style="text-align: center;">Full Context</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">StarCoder 15B</td>
<td style="text-align: center;">no spec.</td>
<td style="text-align: center;">11.75</td>
<td style="text-align: center;">22.38</td>
<td style="text-align: center;">17.24</td>
<td style="text-align: center;">32.52</td>
<td style="text-align: center;">21.52</td>
</tr>
<tr>
<td style="text-align: center;">Code LLM 62B</td>
<td style="text-align: center;">no spec.</td>
<td style="text-align: center;">12.45</td>
<td style="text-align: center;">27.75</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">37.47</td>
<td style="text-align: center;">22.62</td>
</tr>
<tr>
<td style="text-align: center;">+ GIFT4CODE</td>
<td style="text-align: center;">no spec.</td>
<td style="text-align: center;">20.78</td>
<td style="text-align: center;">34.33</td>
<td style="text-align: center;">33.40</td>
<td style="text-align: center;">46.94</td>
<td style="text-align: center;">24.56</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Code LLM 62B</td>
<td style="text-align: center;">TypeDesc</td>
<td style="text-align: center;">16.58</td>
<td style="text-align: center;">29.68</td>
<td style="text-align: center;">29.68</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">25.90</td>
</tr>
<tr>
<td style="text-align: center;">+ GIFT4CODE</td>
<td style="text-align: center;">TypeDesc</td>
<td style="text-align: center;">21.52</td>
<td style="text-align: center;">36.73</td>
<td style="text-align: center;">33.58</td>
<td style="text-align: center;">48.61</td>
<td style="text-align: center;">27.35</td>
</tr>
<tr>
<td style="text-align: center;">Code LLM 62B</td>
<td style="text-align: center;">I/O Examples</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">30.79</td>
<td style="text-align: center;">43.23</td>
<td style="text-align: center;">26.41</td>
</tr>
<tr>
<td style="text-align: center;">+ GIFT4CODE</td>
<td style="text-align: center;">I/O Examples</td>
<td style="text-align: center;">25.23</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">38.03</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">28.66</td>
</tr>
<tr>
<td style="text-align: center;">Code LLM 62B</td>
<td style="text-align: center;">I/O Summary</td>
<td style="text-align: center;">23.75</td>
<td style="text-align: center;">37.11</td>
<td style="text-align: center;">34.50</td>
<td style="text-align: center;">46.75</td>
<td style="text-align: center;">26.25</td>
</tr>
<tr>
<td style="text-align: center;">+ GIFT4CODE</td>
<td style="text-align: center;">I/O Summary</td>
<td style="text-align: center;">28.01</td>
<td style="text-align: center;">43.79</td>
<td style="text-align: center;">43.04</td>
<td style="text-align: center;">55.47</td>
<td style="text-align: center;">29.34</td>
</tr>
</tbody>
</table>
<p>Table 2: pass@ $k$ on ARCADE and DS-1000. For each type of I/O specification in Tab. 1 (e.g. $+\mathrm{I} / \mathrm{O}$ Summary), intents are augmented with I/O specifications of that type (e.g. intents inline with I/O summary) in fine-tuning data or few-shot exemplars. At test time, input intents use the same type of I/O specifications.
ground truth solutions, which then serve as the sole input to the model during its evaluation (Balog et al., 2016).</p>
<p>Metrics Developing unit tests for data science programming tasks is often non-trivial. For example, creating a test for a data wrangling task like "What are the most populous cities in each country" (as shown in Fig. 1) is not feasible. In the ARCADE dataset, each question is accompanied by an annotated program. Then the evaluation uses a set of heuristics to approximate whether the execution output of a predicted program and the annotated reference are functionally equivalent, by comparing their outputs. On the DS-1000 dataset, evaluation relies on the provided tests.</p>
<p>We adopted the pass@ $k$ metrics as defined in Chen et al. (2021a); Austin et al. (2021), which is calculated as the fraction of problems with at least one correct sample given $k$ samples. Following Yin et al. (2022), we drew 50 samples to calculate pass@5 and pass@20 to reduce the variance in ARCADE. Similar to Lai et al. (2022), we drew 40 samples to calculate pass@1 on DS-1000. Consistent with the original works' settings, the sampling temperature was set to 0.8 for ARCADE and to 0.2 for DS-1000 respectively.</p>
<h3>4.2 Main Results</h3>
<p>Tab. 2 presents the pass@ $k$ results on ARCADE and DS1000. We evaluate both few-shot prompting and fine-tuning with synthetic data. Specifically, for ARCADE we evaluate on two versions of the dataset. First, we consider the original version where an intent is prefixed by prior notebook cells as its programmatic context (Full Context), as well as a No Context ablation to simulate the scenario where users query a code LLM using an intent without any context. This no-context setting is more challenging, where the zero-shot performance of the base code LLM is nearly halved. The
standard errors in all cells of the table are less than $0.5 \%$, and are excluded for clarity in presentation.</p>
<p>In our few-shot prompting experiments, we observe that pass@ $k$ generally improves with more detailed I/O specifications. Interestingly, on ARCADE, the improvements from prompting using I/O specifications compared to the baseline where no I/O specifications were used (no spec), are more notable in the more challenging no-context scenario (e.g. $12.45 \mapsto 23.75$ v.s. $27.75 \mapsto 37.11$ for $+I / O$ Summary). This trend suggests that additional specifications could provide more valuable clarifications when adequate programmatic contexts are lacking.</p>
<p>Next, we fine-tune the base code LLM using our synthetic parallel data with different types of I/O specifications. Even without using any I/O specifications in the synthetic data, GIFT4CODE already registers significant improvements on ARCADE (pass@20: $37.47 \mapsto 46.94$ ). The modelpredicted code solutions are filtered using executability heuristics, which helps improve the quality of the synthetic data, and a model fine-tuned on such data could generally be better at following users' intents, even without I/O specifications. Moreover, by fine-tuning the model to follow intents with additional I/O specifications, we observe significantly better results. With I/O summaries (+I/O Summary), GIFT4CODE improves pass@20 from 46.75 to 55.47 . We also remark that instruction fine-tuning using natural language I/O summaries yields the best results on both datasets. Intuitively, those I/O summaries could encode salient information in target input and output variables through natural language descriptions, which could make it easier for the model to capture patterns in the data.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Two examples on ARCADE. Left: the base code LLM does not follow the intent to group the "League" column. Its few-shot prompting variant failed to correctly utilize the specifications. GIFT4CODE's solution aligns to the user's intent. Right: code LLM tries to call a non-exsiting column, leading to a KeyError. Its few-shot variant follows the specifications incorrectly, leading to the syntax error. GIFT4CODE generates correct solution. The prompt always includes dataframe schema, regardless of whether specifications are provided.</p>
<h3>4.3 Qualitative Analysis</h3>
<p>To gain deeper insights into the behavior of the different models, we present a qualitative analysis of the baseline model, its few-shot prompting variant with LLM specifications (I/O Summary), and our proposed GIFT4CODE approach. We illustrate this analysis using two examples from the ARCADE dataset, as shown in Fig. 2.</p>
<p>In the first example (Fig. 2, Left), the base code LLM (cell 4) fails to group the "League" column as requested in the user's intent. Note that the provided solution is still executable so it cannot be filtered by executability heuristics. The fewshot prompting variant with I/O summary (cell 5) also fails here. It struggles to correctly utilize these specifications despite selecting the correct salient columns, leading to an output that does not meet the user's requirement either. In contrast, the output from GIFT4CODE (cell 6) successfully generates a solution which computes the sum of the two salient columns then sorts the result, effectively utilizing the provided specifications and adhering to the user's intent.</p>
<p>The second example (Fig. 2, Right) further underscores the advantages of GIFT4CODE. The baseline code LLM (cell 4) attempts to call a non-existing column (Turbo_Type), leading to a KeyError. This represents a failure mode that the model tries to adhere a user's intent but generates an inexecutable solution that refers to incorrect input variables due to lack of I/O specifications. The few-shot prompting vari-
ant (cell 5) presents another interesting failure mode. While the model is trying to follow the additional I/O specification (presumably because of the few-shot demonstrations) by referring to the Engine volume column in the specification, it fails to generate a syntactically correct code snippet (df. Engine volume). It is important to note that this is a common failure mode of the few-shot prompting model, as we explain in Fig. 3 later. Once again, GIFT4CODE outperforms other settings, generating solutions that answer the natural language question while following specifications.</p>
<h3>4.4 Execution Rate vs Pass@ $k$</h3>
<p>We delve further into the experimental results to examine the relationship between executability and the quality of the generated code. Surprisingly, we observe that a model with a higher execution rate does not necessarily produce better code. Fig. 3 plots the frequency of common error types alongside the code execution rates of different models' predictions. The baseline code LLM, despite generating a substantial amount of executable code (higher $\star$ ), often produces incorrect (irrelevant) solutions, leading to a high executable rate but low pass@ $k$ accuracies (Tab. 2). This suggests that a model's ability to generate executable code does not necessarily indicate its competence in generating semantically correct code that aligns with the user's intent. This insight is further evidenced when we fine-tune the model on synthetic data without I/O specifications, labeled</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Frequency of error types and code execution rate for different methods. Bottom x-axis stands for the counts of schema errors and syntax errors. Top x-axis represents execution rate. Instruction fine-tuning without specifications (SyntheticFT) decreases executability. Few-shot prompting with specifications reduces schema understanding errors with more syntax errors. GIFT4CODE achieves the best performance by combining their strengths.
as SyntheticFT in Fig. 3. The model's execution rate decreases in this scenario because it attempts to better align with user intent, leading to a higher rate of schema understanding errors (e.g. referring to non-existing columns as in cell 4, Example 2, Fig. 2).</p>
<p>Incorporating I/O specifications using few-shot prompting leads to another interesting observation. We observe a reduction in schema understanding errors, indicating that the model indeed benefits from the extra specifications. However, given the vast diversity of Python I/O specifications, it is impossible to cover all variants within the few-shot exemplars. Consequently, the model struggles to reliably leverage the specifications, leading to a surge in syntax errors when referring to arbitrarily string-valued schema elements (e.g. cell 5, Example 2, Fig. 2). GIFT4CODE effectively mitigates these syntax errors, leading to a higher execution rate while achieving the best pass@ $k$ accuracies.</p>
<h2>5 Related Work</h2>
<p>Execution Guided Code Generation One area of study primarily focuses on utilizing execution as I/O examples, facilitating the synthesis of programs that align with the intended behavior. Gulwani (2016) involves synthesizing intended programs in an underlying domain-specific language (DSL) from example based specifications. This method has been further explored and adapted to different applications in subsequent studies (Devlin et al., 2017; Chen et al., 2018; Bunel et al., 2018). Another strand of research (Chen et al., 2021b; Wang et al., 2018; Ellis et al., 2019) leverages intermediate execution results to guide the search of programs. More recently, there have been attempts to utilize program execution results to verify and select code samples predicted
by LLMs, either during auto-regressive decoding to prune search space (Zhang et al., 2023), or by few-shot prompting (Chen et al., 2023) and post-hoc reranking (Shi et al., 2022; Ni et al., 2023a). CodeT (Chen et al., 2022) was proposed to generate tests then to conduct dual execution agreements to filter the generated solutions during the inference time.</p>
<p>Instruction Fine-tuning Instruction fine-tuning is a widely adopted approach to address the misalignment issue in LLM-generated content. LLMs such as FLAN (Wei et al., 2021), which excel at understanding and executing instructions from prompts, are trained on labeled training data. Reinforcement learning with human feedback (RLHF) aims to mitigate the amount of labeling effort using modelbased reward (Ouyang et al., 2022). Other works also confirmed the effectiveness of using instructional data in the fine-tuning stage (Mishra et al., 2021; Sanh et al., 2021; Chung et al., 2022; Wang et al., 2022b). To lower labeling cost, several recent works explored the possibility of automatic instruction generation (Ye et al., 2022; Zhou et al., 2022; Honovich et al., 2022b). STAR (Zelikman et al., 2022) bootstraps the model's reasoning ability using selfgenerated rationales. Our work differs from this line by considering execution-based specifications. Additionally, recent works attempted to distill instruction following data from more capable LLMs that have already been instructiontuned (Honovich et al., 2022a; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023). In contrast, GIFT4CODE generates synthetic data from vanilla LLMs that have not gone through instruction-tunning.</p>
<p>Synthetic Data from LLMs Besides generating data for instruction following, a number of recent studies have also harnessed general-purpose LLMs to generate realistic synthetic data in areas where labeled data limited, such as language understanding and clinical research (Rosenbaum et al., 2022a; Tang et al., 2023; Borisov et al., 2022; Liu et al., 2022; Rosenbaum et al., 2022b; Josifoski et al., 2023). To improve the quality of synthetic data extracted from LLMs, such approaches usually apply a rejection sampling procedure and filter predictions based on domain-specific heuristics such as logical consistency (Bhagavatula et al., 2022; Yin et al., 2022). GIFT4CODE is in spirit of this line in that it leverages program execution feedback to filter code predictions (Xu et al., 2020).</p>
<h2>6 Conclusion</h2>
<p>We have presented GIFT4CODE, a framework for instruction fine-tuning large language models of code in which the training is guided by execution based specifications. Empirically, we demonstrated how our approach enhances the quality of generated code by following user-provided specifications, substantially improving accuracy on two challenging data science benchmarks, ARCADE and DS-1000.</p>
<h2>References</h2>
<p>Alur, R., Bodík, R., Juniwal, G., Martin, M. M. K., Raghothaman, M., Seshia, S. A., Singh, R., SolarLezama, A., Torlak, E., and Udupa, A. Syntax-guided synthesis. 2013 Formal Methods in Computer-Aided Design, pp. 1-8, 2013.</p>
<p>Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. ArXiv, abs/1611.01989, 2016.</p>
<p>Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP), 2013.</p>
<p>Bhagavatula, C., Hwang, J. D., Downey, D., Bras, R. L., Lu, X., Sakaguchi, K., Swayamdipta, S., West, P., and Choi, Y. I2d2: Inductive knowledge distillation with neurologic and self-imitation. ArXiv, abs/2212.09246, 2022.</p>
<p>Bird, C., Ford, D., Zimmermann, T., Forsgren, N., Kalliamvakou, E., Lowdermilk, T., and Gazit, I. Taking flight with copilot: Early insights and opportunities of ai-powered pair-programming tools. Queue, 20(6):35-57, jan 2023. ISSN 1542-7730. doi: 10.1145/3582083. URL https://doi.org/10.1145/3582083.</p>
<p>Borisov, V., Sessler, K., Leemann, T., Pawelczyk, M., and Kasneci, G. Language models are realistic tabular data generators. ArXiv, abs/2210.06280, 2022.</p>
<p>Bunel, R., Hausknecht, M. J., Devlin, J., Singh, R., and Kohli, P. Leveraging grammar and reinforcement learning for neural program synthesis. ArXiv, abs/1805.04276, 2018.</p>
<p>Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation with generated tests. ArXiv, abs/2207.10397, 2022. URL https://api.semanticscholar. org/CorpusID:250920542.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S. A., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford,
A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021a.</p>
<p>Chen, X., Liu, C., and Song, D. X. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2018.</p>
<p>Chen, X., Song, D. X., and Tian, Y. Latent execution for neural program synthesis beyond domain-specific languages. In Neural Information Processing Systems, 2021b.</p>
<p>Chen, X., Lin, M., Schärli, N., and Zhou, D. Teaching large language models to self-debug. ArXiv, abs/2304.05128, 2023.</p>
<p>Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N. M., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B. C., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., García, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Díaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K. S., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A. W., Zhao, V., Huang, Y., Dai, A. M., Yu, H., Petrov, S., hsin Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.</p>
<p>Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., rahman Mohamed, A., and Kohli, P. Robustfill: Neural program learning under noisy i/o. ArXiv, abs/1703.07469, 2017.</p>
<p>Ellis, K., Nye, M., Pu, Y., Sosa, F., Tenenbaum, J. B., and Solar-Lezama, A. Write, execute, assess: Program syn-</p>
<p>thesis with a repl. In Neural Information Processing Systems, 2019.</p>
<p>Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.</p>
<p>Google. PaLM2 technical report. https: //ai.google/static/documents/ palm2techreport.pdf, 2023. Accessed: 2023-0510 .</p>
<p>Gulwani, S. Automating string processing in spreadsheets using input-output examples. In ACM-SIGACT Symposium on Principles of Programming Languages, 2011. URL https://api.semanticscholar. org/CorpusID:886323.</p>
<p>Gulwani, S. Programming by examples - and its applications in data wrangling. In Dependable Software Systems Engineering, 2016.</p>
<p>Gulwani, S., Harris, W. R., and Singh, R. Spreadsheet data manipulation using examples. Commun. ACM, 55: 97-105, 2012.</p>
<p>Gulwani, S., Hernández-Orallo, J., Kitzelmann, E., Muggleton, S. H., Schmid, U., and Zorn, B. Inductive programming meets the real world. Communications of the ACM, 58(11):90-99, 2015.</p>
<p>Honovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models with (almost) no human labor. ArXiv, abs/2212.09689, 2022a.</p>
<p>Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. ArXiv, abs/2205.10782, 2022b.</p>
<p>Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Mapping language to code in programmatic context. ArXiv, abs/1808.09588, 2018.</p>
<p>Jain, N., Vaidyanath, S., Iyer, A., Natarajan, N., Parthasarathy, S., Rajamani, S., and Sharma, R. Jigsaw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, pp. 1219-1231, 2022.</p>
<p>Josifoski, M., Sakota, M., Peyrard, M., and West, R. Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction. ArXiv, abs/2303.04132, 2023.</p>
<p>Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., tau Yih, S. W., Fried, D., Wang, S., and Yu, T. Ds1000: A natural and reliable benchmark for data science code generation. ArXiv, abs/2211.11501, 2022.</p>
<p>Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M.-H., Umapathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes, S. M., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may the source be with you! 2023.</p>
<p>Li, Y., Choi, D. H., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Tom, Eccles, Keeling, J., Gimeno, F., Lago, A. D., Hubert, T., Choy, P., de, C., d’Autume, M., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal, S., Alexey, Cherepanov, Molloy, J., Mankowitz, D. J., Robson, E. S., Kohli, P., de, N., Freitas, Kavukcuoglu, K., and Vinyals, O. Competition-level code generation with alphacode. Science, 378:1092 - 1097, 2022.</p>
<p>Liu, Q., Ye, Z., Yu, T., Blunsom, P., and Song, L. Augmenting multi-turn text-to-sql datasets with self-play. In Conference on Empirical Methods in Natural Language Processing, 2022.</p>
<p>Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Crosstask generalization via natural language crowdsourcing instructions. In Annual Meeting of the Association for Computational Linguistics, 2021.</p>
<p>Ni, A., Iyer, S., Radev, D. R., Stoyanov, V., tau Yih, W., Wang, S. I., and Lin, X. V. Lever: Learning to verify language-to-code generation with execution. ArXiv, abs/2302.08468, 2023a.</p>
<p>Ni, A., Yin, P., Zhao, Y., Riddell, M., Feng, T., Shen, R., Yin, S., Liu, Y., Yavuz, S., Xiong, C., Joty, S. R., Zhou, Y., Radev, D. R., and Cohan, A. L2ceval: Evaluating language-to-code generation capabilities of large language models. ArXiv, abs/2309.17446, 2023b. URL https://api.semanticscholar. org/CorpusID:263310373.</p>
<p>Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. Codegen: An open large language model for code with multi-turn program synthesis. 2022.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller,</p>
<p>L. E., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. J. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.</p>
<p>Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruction tuning with gpt-4. ArXiv, abs/2304.03277, 2023.</p>
<p>Rosenbaum, A., Soltan, S., Hamza, W., Saffari, A., Damonte, M., and Groves, I. Clasp: Few-shot cross-lingual data augmentation for semantic parsing. In $A A C L, 2022 a$.</p>
<p>Rosenbaum, A., Soltan, S., Hamza, W., Versley, Y., and Boese, M. Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging. In International Conference on Computational Linguistics, 2022b.</p>
<p>Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M. P., Ferrer, C. C., Grattafiori, A., Xiong, W., D’efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code. ArXiv, abs/2308.12950, 2023. URL https://api.semanticscholar. org/CorpusID:261100919.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Févry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot task generalization. ArXiv, abs/2110.08207, 2021.</p>
<p>Shi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code translation with execution. ArXiv, abs/2204.11454, 2022.</p>
<p>Shi, K., Bieber, D., and Singh, R. Tf-coder: Program synthesis for tensor manipulations. ACM Transactions on Programming Languages and Systems (TOPLAS), 44:1 36, 2020. URL https://api.semanticscholar. org/CorpusID:214605958.</p>
<p>Tang, R., Han, X., Jiang, X., and Hu, X. Does synthetic data generation of llms help clinical text mining? ArXiv, abs/2303.04360, 2023.</p>
<p>Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/ stanford_alpaca, 2023.</p>
<p>Wang, C., Tatwawadi, K., Brockschmidt, M., Huang, P.-S., Mao, Y., Polozov, O., and Singh, R. Robust text-tosql generation with execution-guided decoding. arXiv: Computation and Language, 2018.</p>
<p>Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. ArXiv, abs/2212.10560, 2022a.</p>
<p>Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., Pal, K. K., Moradshahi, M., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R., Verma, P., Puri, R. S., Karia, R., Sampat, S. K., Doshi, S., Mishra, S. D., Reddy, S., Patro, S., Dixit, T., Shen, X., Baral, C., Choi, Y., Smith, N. A., Hajishirzi, H., and Khashabi, D. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Conference on Empirical Methods in Natural Language Processing, 2022b.</p>
<p>Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021.</p>
<p>Xu, S., Semnani, S. J., Campagna, G., and Lam, M. S. Autoqa: From databases to q\&amp;a semantic parsers with only synthetic training data. ArXiv, abs/2010.04806, 2020.</p>
<p>Ye, S., Kim, D., Jang, J., Shin, J., and Seo, M. Guess the instruction! flipped learning makes language models stronger zero-shot learners. ArXiv, abs/2210.02969, 2022.</p>
<p>Yin, P., Wieting, J. F., Sil, A., and Neubig, G. On the ingredients of an effective zero-shot semantic parser. In Annual Conference of the Association for Computational Linguistics (ACL), Dublin, Ireland, May 2022. URL https://arxiv.org/abs/2110.08381.</p>
<p>Zelikman, E., Huang, Q., Poesia, G., Goodman, N. D., and Haber, N. Parsel: A (de-)compositional framework for algorithmic reasoning with language models. 2022.</p>
<p>Zhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B., and Gan, C. Planning with large language models for code generation. ArXiv, abs/2303.05510, 2023.</p>
<p>Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are humanlevel prompt engineers. ArXiv, abs/2211.01910, 2022.</p>
<h1>A Applying GIFT4CODE to Data Science Code Generation</h1>
<p>In this appendix section, we demonstrate the practical application of our proposed approach GIFT4CODE, as discussed in $\S 3$, to a specific dataset, ARCADE (Yin et al., 2022). We follow the same setup as described in $\S 3$. Our starting point involves a "Generalist LLM" (PALM2) and a Code LLM (PaLM based 62B model in our case), the alignment of which we seek to improve.</p>
<h2>A. 1 Synthetic Data Generation</h2>
<p>We first gathered CSV files from diverse GitHub repositories focusing on data science projects. These CSV files encompass a wide range of data types, structures, and domains, serving as the programmatic context to initialize the synthetic data generation. Subsequently, we employed a "Generalist" PALM2 LLM specifically designed for natural language understanding tasks, distinguished from the base code LLMs. This model was utilized to generate natural language questions based on the information extracted from the collected CSV files. For the following sections, we denote the set of natural number from 0 to $W$ and $\mathbb{N}_{W}={1,2,3, \ldots, W}$.</p>
<p>Creating NL Intents (Questions) Using the entire CSV file as the programmatic context to prompt the "Generalist" PALM2 LLM is infeasible due to its length after tokenization. Instead, we extracted the header for each CSV file as the programmatic context $\boldsymbol{c}<em i="i">{i}$ to query the LLM to generate the NL intents $\left{\boldsymbol{x}</em>}\right}$. Given the length of an entire CSV file after tokenization, using it as the programmatic context to prompt the "generalist" LLM is impractical. Instead, we derive the header and its first three rows from each CSV file as the programmatic context, denoted as $\boldsymbol{c<em i="i">{i}$, to query the LLM to produce NL intents $\boldsymbol{x}</em>}$. To construct the few-shot exemplars, we randomly selected a subset of CSV headers, denoted as $\boldsymbol{C}^{\prime}=\left{\boldsymbol{c<em P="P">{i}^{\prime} \mid i \in \mathbb{N}</em>}\right}$ where $P&lt;10$. The prime symbol ${ }^{\prime}$ in the superscript denotes the variable will be used in the few-shot exemplars. We manually crafted a set of 10 data science-related questions corresponding to each $\boldsymbol{c<em i="i">{i}^{\prime}$, denoted as $\boldsymbol{X}</em>}^{\prime}=\left{\boldsymbol{x<em 10="10">{i}^{j} \mid j \in \mathbb{N}</em>}\right}$. This process allowed us to create a few-shot prompt set $\mathcal{P}$ that consists of pairs of CSV headers and associated questions, formulated as $\mathcal{P}=\left{\left(\boldsymbol{c<em i="i">{i}^{\prime}, \boldsymbol{X}</em>$.}^{\prime}\right)\right}_{i=1}^{P</p>
<p>We employed the standard few-shot prompting approach, which concatenates $\mathcal{P}$ to each CSV header $\boldsymbol{c}<em i="i">{i}$ when querying the LLM. In specific, each header $\boldsymbol{c}</em>}$ is augmented to $\hat{\boldsymbol{c}<em i="i">{i}=\left(\mathcal{P}, \boldsymbol{c}</em>}\right)$. In this configuration, the LLM is prompted to generate questions related to $\boldsymbol{c<em i="i">{i}$, while emulating the style of $\mathcal{P}$. After generation, we perform diversity filtering on $\boldsymbol{X}</em>}$. Let $\widetilde{\boldsymbol{X}<em i="i">{i}$ denote the set of NL questions after the filtering process. We initialize $\widetilde{\boldsymbol{X}}</em>}$ as $\left{q_{0}\right}$ where $q_{0} \sim \mathcal{U}\left(\boldsymbol{X<em i="i">{i}\right)$ ), a question randomly selected from $\boldsymbol{X}</em>}$. Similar to Wang et al. (2022a), we iterate over $\left{\boldsymbol{X<em 0="0">{i} \backslash q</em>}\right}$, a new question is added to the set $\widetilde{\boldsymbol{X}<em i="i">{i}$ only when its ROUGE-L overlap with any $q \in \widetilde{\boldsymbol{X}}</em>$ to represent the set of NL intents after the filtering process. Empirically, we observe that the NL questions generated by this LLM encompassed a wide range of tasks relevant to the given programmatic context, as shown in Listing 2 and Listing 3.}$ is less than 0.7 . Questions that are repetitive are also filtered out. For notation simplicity, in the following sections, we use $\boldsymbol{X}_{i</p>
<p>Predicting Code Solutions Subsequently, we employed the code LLM, which we aim to improve, to generate code that addresses these NL questions. For each question $q_{i}^{j} \in \boldsymbol{X}<em i="i">{i}$ where $j$ denotes the $j^{\text {th }}$ question associated with the programmatic context $\boldsymbol{c}</em>}$, we can draw code solutions from $P_{\mathrm{LLM}}\left(\boldsymbol{y} \mid \boldsymbol{c<em i="i">{i}, q</em>$ does not contain any I/O specifications yet.}^{j}\right)$. This leads to a pool of code samples ${\boldsymbol{y}}_{i}^{j}$. Following this, we applied heuristic filtering on these code samples, adopting two criteria: 1) Maximizing the diversity of Python library API calls, and 2) Ensuring the produced execution output is meaningful, for instance, yielding a pandas. DataFrame object if the code is executable. This process resulted in a synthetic dataset $\mathcal{D}$ that enables instruction fine-tuning. Notice that $\mathcal{D</p>
<p>The synthetic data generation process leverages the capabilities of the LLM to create a diverse set of natural language intents that correspond to the header information extracted from the CSV files. By incorporating the knowledge and understanding of "Generalist" PALM2 LLM, we are able to generate a rich and varied synthetic dataset that contains a wide range of data science tasks and scenarios.</p>
<p>Executing Code Samples Executing code derived from a pre-training corpus which was used to train the code LLM can be challenging, as it often demands notoriously complicated environment setup. In contrast, the synthetic code samples offer a significant advantage, as they can be executed more easily without the need for complex environment setup. Moreover, the execution of synthetic code samples enables us to track variables and their states both before and after running the code. This information can be utilized to identify input variables that exhibit state changes after execution. We label these variables as inputs to the code. In addition, by examining the final states of these input variables, we can effectively establish</p>
<p>the input-output specifications of the code.</p>
<h1>B LLM Generated NL Intents</h1>
<p>In this section, we demonstrated the NL questions generated by the "generalist" PALM2 LLM on the ARCADE dataset. To begin, we provide an exemplar used in the few-shot prompt when querying the LLM to generate NL questions, as seen in Listing 1. The given exemplar consists of the CSV header along with the first three rows. If any entry within the first three rows exceeds 50 characters in its string representation, we truncate it to maintain conciseness. As shown in Listing 1, we handcrafted 10 diverse questions, covering as many data science topics as possible.</p>
<p>In this section, we provide two examples of NL intent generation. Each of these examples, as shown in Listing 2 and Listing 3, includes both the programmatic context and the output generated by the LLM. Listing 2 illustrates an instance regarding a Pokémon game experience. Notably, the LLM tends to generate relatively basic questions in this context, which don't necessitate the use of complex Python APIs such as pandas. groupby. Conversely, Listing 3 presents an example related to a Netflix TV and movie dataset. In this case, the LLM produces more comprehensive questions. Addressing these questions requires multiple API calls, indicating a higher level of complexity.</p>
<h2>C I/O Summary Examples</h2>
<p>To begin with, we showed an example of a few-shot exemplar used to prompt the "generalist" PALM2 LLM in generating an I/O summary for the ARCADE dataset, as detailed in section 3.2. The exemplar in Listing 4 comprises an input dataframe schema, a Python code solution, execution output, and user intent. The anticipated response to this prompt is an I/O summary, outlining the input-output variable names and their types. In this example, there is only one variable - ""alc"" which is a pandas. DataFrame. Next, the LLM is expected to give a succinct description on the salient input columns, as well as a brief summary of the example output columns.</p>
<p>We present two examples of LLM generated I/O summaries on the ARCADE dataset, as illustrated in Listing 5 and Listing 6. As mentioned in $\S 4.1$, we deliberately obscure the execution output details when prompting the LLM to generate an I/O summary. This step helps to more realistically simulate user provided specifications. Each example consists of its prompt we used to query the LLM for the I/O summary, the resulting example augmented by the generated I/O summary and the ground truth execution output which was never exposed to LLM.</p>
<p>The first example Listing 5 focuses on a dataset detailing mobile phone information, with the user intent being to determine the quantity of different smartphones released each decade. The subsequent I/O summary generated by the LLM identifies the "smartphone" and "year" columns as the most relevant from the input dataframe and describes the output values as being of int64 type, with an index year. The generated I/O summary correctly describes the ground truth presented in the bottom of Listing 5.</p>
<p>In the second example Listing 6, the LLM is dealing with movie data where the user's intent is to chronologically list the worldwide gross of films released each month from November 2021 to June 2022. The corresponding I/O summary generated by the LLM correctly identified the salient columns in the input dataframe, "Worldwide" and "Released Date". However, an observed discrepancy between the ground truth output and the I/O summary indicates that the LLM's generation is not entirely accurate for this specific example.</p>
<p>These two test examples on ARCADE illustrate the LLM's capabilities in generating I/O summaries, while highlighting the potential discrepancies that may occur.</p>
<h2>D Additional Experimental Results</h2>
<p>In this section, we offer two additional experiments that supplement the results presented in $\S 4.2$. We have demonstrated the GIFT4CODE model consistently outperforms in all types of I/O specifications. A natural follow-up question might be whether the instruction fine-tuning degrades the model's programming ability if no specifications are provided by the user. To address this concern, we have conducted further experiments where an instruction tuned model is evaluated on the ARCADE dataset, in the absence of any specification.</p>
<p>The results in Tab. 3 demonstrate that while the instruction tuned model does perform slightly worse than the model fine-tuned on data with no specifications, the difference is marginal. In the pass@5 and pass@20 settings, both with no</p>
<div class="codehilite"><pre><span></span><code><span class="n">First</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="mi">4</span><span class="o">-</span><span class="n">wheeler</span><span class="o">-</span><span class="n">EV</span><span class="o">.</span><span class="n">csv</span><span class="w"> </span><span class="p">(</span><span class="n">column</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">types</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">parentheses</span><span class="p">)</span>
<span class="o">|</span><span class="w"> </span><span class="n">Comfort</span><span class="w"> </span><span class="p">(</span><span class="n">float</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Performance</span><span class="w"> </span><span class="p">(</span><span class="n">float</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Fuel</span><span class="w"> </span><span class="n">Economy</span><span class="w"> </span><span class="p">(</span><span class="n">float</span><span class="p">)</span>
<span class="o">|</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Money</span><span class="w"> </span><span class="p">(</span><span class="n">float</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Condition</span><span class="w"> </span><span class="p">(</span><span class="n">string</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">driven</span><span class="w"> </span><span class="p">(</span><span class="n">string</span><span class="p">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">rating</span><span class="w"> </span><span class="p">(</span><span class="n">float</span><span class="p">)</span>
<span class="o">|</span><span class="w"> </span><span class="n">model_name</span><span class="w"> </span><span class="p">(</span><span class="n">string</span><span class="p">)</span><span class="w"> </span><span class="o">|</span>
<span class="o">|---------------------------------------------------------------|---------------|</span>
<span class="o">|</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">New</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Few</span><span class="w"> </span><span class="n">hundred</span><span class="w"> </span><span class="n">kilometers</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">hyundai</span><span class="w"> </span><span class="n">kona</span><span class="w"> </span><span class="o">|</span>
<span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">New</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Haven</span><span class="err">&#39;</span><span class="n">t</span><span class="w"> </span><span class="n">driven</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">hyundai</span><span class="w"> </span><span class="n">kona</span><span class="w"> </span><span class="o">|</span>
<span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">New</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Few</span><span class="w"> </span><span class="n">thousand</span><span class="w"> </span><span class="n">kilometers</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">hyundai</span><span class="w"> </span><span class="n">kona</span><span class="w"> </span><span class="o">|</span>
</code></pre></div>

<p>First 3 rows from dataset 2-wheeler-EV.csv (column data types in parentheses)
| Owned for (string) | Ridden for (string) | rating (int)
| Visual Appeal (float) | Reliability (float) | Performance (float)
| Service Experience (float) | Extra Features (float) | Comfort (float)
| Maintenance cost (float) | Value for Money (float) | Model Name (string) |
|---------------------------------------------------------------|---------------|
| Never owned | nan | 1 | 3 | 4 | nan | nan | nan | 4 | nan | 1 | TVS iQube |
| &gt; 1 yr | &lt; 5000 kms | 1 | 3 | 1 | nan | 1 | nan | 3 | nan | 3 | TVS iQube |
| &lt; 3 months | &lt; 5000 kms | 3 | 4 | 4 | nan | 2 | nan | 5 | nan | 2 | TVS iQube |</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Here</span> <span class="s s-Atom">are</span> <span class="s s-Atom">a</span> <span class="s s-Atom">series</span> <span class="s s-Atom">of</span> <span class="s s-Atom">contextually</span> <span class="s s-Atom">dependent</span> <span class="s s-Atom">data</span> <span class="s s-Atom">wrangling</span> <span class="s s-Atom">and</span> <span class="s s-Atom">exploratory</span> <span class="s s-Atom">data</span> <span class="s s-Atom">analysis</span> <span class="s s-Atom">tasks</span> <span class="s s-Atom">for</span> <span class="s s-Atom">the</span> <span class="s s-Atom">dataset</span><span class="p">:</span>
<span class="nv">Task</span> <span class="mi">1</span><span class="o">:</span> <span class="nv">How</span> <span class="s s-Atom">many</span> <span class="s s-Atom">new</span> <span class="s s-Atom">cars</span> <span class="s s-Atom">have</span> <span class="s s-Atom">a</span> <span class="s s-Atom">performance</span> <span class="s s-Atom">rating</span> <span class="s s-Atom">of</span> <span class="s s-Atom">more</span> <span class="s s-Atom">than</span> <span class="mi">3</span><span class="nb">?</span>
<span class="nv">Task</span> <span class="mi">2</span><span class="o">:</span> <span class="nv">Among</span> <span class="s s-Atom">the</span> <span class="nv">Hyundai</span> <span class="nv">Kona</span> <span class="s s-Atom">reviews</span><span class="p">,</span> <span class="s s-Atom">do</span> <span class="s s-Atom">those</span> <span class="s s-Atom">with</span> <span class="s s-Atom">a</span> <span class="s s-Atom">purchase</span> <span class="s s-Atom">history</span> <span class="s s-Atom">have</span> <span class="s s-Atom">average</span> <span class="s s-Atom">performance</span> <span class="s s-Atom">rating</span> <span class="s s-Atom">better</span> <span class="s s-Atom">than</span> <span class="s s-Atom">the</span> <span class="s s-Atom">ones</span> <span class="s s-Atom">that</span> <span class="s s-Atom">have</span> <span class="o">not</span> <span class="s s-Atom">yet</span> <span class="s s-Atom">been</span> <span class="s s-Atom">purchased</span><span class="nb">?</span> <span class="nv">Task</span> <span class="mi">3</span><span class="o">:</span> <span class="nv">Create</span> <span class="s s-Atom">a</span> <span class="s s-Atom">new</span> <span class="s s-Atom">column</span> <span class="s s-Atom">called</span> <span class="s2">&quot;Review&#39;s Word Count&quot;</span> <span class="s s-Atom">and</span> <span class="nv">Show</span> <span class="s s-Atom">the</span> <span class="s s-Atom">rating</span> <span class="s s-Atom">of</span> <span class="s s-Atom">the</span> <span class="s s-Atom">shortest</span> <span class="s s-Atom">review</span>
<span class="nv">Task</span> <span class="mi">4</span><span class="o">:</span> <span class="nv">Which</span> <span class="s s-Atom">model</span> <span class="s s-Atom">has</span> <span class="s s-Atom">the</span> <span class="s s-Atom">highest</span> <span class="s s-Atom">average</span> <span class="s s-Atom">word</span> <span class="s s-Atom">count</span><span class="nb">?</span>
<span class="nv">Task</span> <span class="mi">5</span><span class="o">:</span> <span class="nv">How</span> <span class="s s-Atom">many</span> <span class="s s-Atom">reviews</span> <span class="s s-Atom">are</span> <span class="s s-Atom">for</span> <span class="s s-Atom">used</span> <span class="s s-Atom">cars</span> <span class="s s-Atom">for</span> <span class="s s-Atom">this</span> <span class="s s-Atom">model</span><span class="nb">?</span>
<span class="nv">Task</span> <span class="mi">6</span><span class="o">:</span> <span class="nv">What</span> <span class="s s-Atom">are</span> <span class="s s-Atom">the</span> <span class="s s-Atom">top</span> <span class="s s-Atom">five</span> <span class="s s-Atom">models</span> <span class="s s-Atom">with</span> <span class="s s-Atom">most</span> <span class="s s-Atom">number</span> <span class="s s-Atom">of</span> <span class="s s-Atom">bikes</span> <span class="s s-Atom">having</span> <span class="s s-Atom">mileage</span> <span class="s s-Atom">less</span> <span class="s s-Atom">than</span> <span class="mi">5000</span> <span class="s s-Atom">kilometers</span>
<span class="nv">Task</span> <span class="mi">7</span><span class="o">:</span> <span class="nv">Which</span> <span class="s s-Atom">of</span> <span class="s s-Atom">these</span> <span class="s s-Atom">models</span> <span class="s s-Atom">has</span> <span class="s s-Atom">the</span> <span class="s s-Atom">highest</span> <span class="s s-Atom">comfort</span> <span class="s s-Atom">score</span> <span class="s s-Atom">on</span> <span class="s s-Atom">average</span><span class="nb">?</span>
<span class="nv">Task</span> <span class="mi">8</span><span class="o">:</span> <span class="nv">What</span> <span class="s s-Atom">are</span> <span class="s s-Atom">those</span> <span class="s s-Atom">models</span> <span class="s s-Atom">mostly</span> <span class="s s-Atom">used</span> <span class="s s-Atom">for</span><span class="nb">?</span>
<span class="nv">Task</span> <span class="mi">9</span><span class="o">:</span> <span class="nv">How</span> <span class="s s-Atom">many</span> <span class="s s-Atom">times</span> <span class="s s-Atom">the</span> <span class="s s-Atom">word</span> <span class="s s-Atom">scooter</span> <span class="s s-Atom">was</span> <span class="s s-Atom">mentioned</span> <span class="s s-Atom">in</span> <span class="s s-Atom">the</span> <span class="s s-Atom">bike&#39;s reviews?</span>
<span class="s s-Atom">Task 10: Show a list of words that were mentioned only once in the reviews [END]</span>

<span class="s s-Atom">Listing 1: An exemplar in the few-shot prompt when querying LLM to generate NL intents.</span>

<span class="s s-Atom">| id (int) | identifier (string) | species_id (int) | height (int) | weight (int) | base_experience (int) | order (int) | is_default (int) |</span>
<span class="s s-Atom">| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |</span>
<span class="s s-Atom">| 1 | bulbasaur | 1 | 7 | 69 | 64 | 1 | 1 |</span>
<span class="s s-Atom">| 2 | ivysaur | 2 | 10 | 130 | 142 | 2 | 1 |</span>
<span class="s s-Atom">| 3 | venusaur | 3 | 20 | 1000 | 263 | 3 | 1 |</span>

<span class="s s-Atom">(a) The programmatic context to query the &quot;generalist&quot; PALLM2 LLM, albeit only a subset of the columns are displayed. Here are a series of contextually dependent data wrangling and exploratory data analysis tasks for the dataset:</span>
<span class="s s-Atom">Task 1: How many pokemons are there in the Pokedex?</span>
<span class="s s-Atom">Task 2: Find the total number of columns that are integers.</span>
<span class="s s-Atom">Task 3: Calculate the ratio of mean weight to height for each pokemon</span>
<span class="s s-Atom">Task 4: What is the weight of &quot;Snorlax&quot; ?</span>
<span class="s s-Atom">Task 5: How many pokemon have the same average base experience as their id?</span>
<span class="s s-Atom">Task 6: Find the order for each pokemon that weighs less than 100 pounds</span>
<span class="s s-Atom">Task 7: What is the &quot;mean&quot; and &quot;median&quot; of &quot;height&quot; column ?</span>
<span class="s s-Atom">Task 8: Show the names of the pokemons with minimum and maximum weight, height and base experience.</span>
<span class="s s-Atom">Task 9: Show the first 20 and last 10 pokemon with their average base experience. Task 10: Create a new column called &quot;size_cat&quot; that has size categories for pokemon (child: 1-10, teenager: 11-20, adult: 21+)</span>
<span class="s s-Atom">(b) LLM generated questions for the above programmatic context.</span>

<span class="s s-Atom">Listing 2: Programmatic context and generated questions on an example concerning pokemon game experience dataset.</span>
<span class="s s-Atom">context and full context, the model&#39;s</span> <span class="nf">performance</span> <span class="p">(</span><span class="s s-Atom">all</span> <span class="s s-Atom">types</span> <span class="s s-Atom">of</span> <span class="s s-Atom">specifications</span><span class="p">)</span> <span class="s s-Atom">only</span> <span class="s s-Atom">decreases</span> <span class="s s-Atom">by</span> <span class="s s-Atom">most</span> <span class="err">$</span><span class="mi">2</span> <span class="s s-Atom">\</span><span class="c1">%$ when compared with the model without specifications. This is expected as the discrepancy between the fine-tuning data and the testing data could lead to a minor regression in performance. This marginal decrease in performance is counterbalanced by the significant improvement we previously observed in $\S 4.2$. Therefore, GIFT4CODE with I/O summary still remains the superior method for instruction fine-tuning.</span>
</code></pre></div>

<p>| show_id (string) | type (string) | title (string) | director (string) |
| --- | --- | --- | --- | --- | --- |
| country (string) | date_added (string) | release_year (int) | ... |
| s1 | Movie | Dick Johnson Is Dead | Kirsten Johnson | United States |
|  | September 25, 2021 | 2020 | PG-13 | 90 min | ... |
| s2 | TV Show | Blood &amp; Water | nan | South Africa |
|  | September 24, 2021 | 2021 | TV-MA | 2 Seasons | ... |
| s3 | TV Show | Ganglands | Julien Leclercq | nan |
|  | September 24, 2021 | 2021 | TV-MA | 1 Season | ... |</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="nx">a</span><span class="p">)</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">programmatic</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">query</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;generalist&quot;</span><span class="w"> </span><span class="nx">PsLM2</span><span class="w"> </span><span class="nx">LLM</span><span class="p">,</span><span class="w"> </span><span class="nx">albeit</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">subset</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">columns</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">displayed</span><span class="p">.</span><span class="w"> </span><span class="nx">Here</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">series</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">contextually</span><span class="w"> </span><span class="nx">dependent</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="nx">wrangling</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">exploratory</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="nx">analysis</span><span class="w"> </span><span class="nx">tasks</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">dataset</span><span class="p">:</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">count</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">seasons</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">show</span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="nx">How</span><span class="w"> </span><span class="nx">Many</span><span class="w"> </span><span class="nx">shows</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="s">&quot;TV14&quot;</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">rating</span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span><span class="w"> </span><span class="nx">How</span><span class="w"> </span><span class="nx">many</span><span class="w"> </span><span class="nx">movies</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">rating</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">$</span><span class="mi">13</span><span class="o">+</span><span class="err">$</span><span class="w"> </span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="nx">Show</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">top</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="nx">TV</span><span class="w"> </span><span class="nx">shows</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">most</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">seasons</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="nx">Make</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">column</span><span class="w"> </span><span class="s">&quot;genre&quot;</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">combines</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">genres</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">column</span><span class="p">.</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">6</span><span class="p">:</span><span class="w"> </span><span class="nx">Show</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">casts</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">been</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">least</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nx">shows</span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">7</span><span class="p">:</span><span class="w"> </span><span class="nx">How</span><span class="w"> </span><span class="nx">many</span><span class="w"> </span><span class="nx">TV</span><span class="w"> </span><span class="nx">Shows</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">been</span><span class="w"> </span><span class="nx">released</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="mi">2017</span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">8</span><span class="p">:</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">director</span><span class="p">,</span><span class="w"> </span><span class="nx">how</span><span class="w"> </span><span class="nx">many</span><span class="w"> </span><span class="nx">shows</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">been</span><span class="w"> </span><span class="nx">added</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">2020</span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">9</span><span class="p">:</span><span class="w"> </span><span class="nx">Show</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">movies</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">director</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">name</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">letter</span><span class="w"> </span><span class="sc">&#39;b&#39;</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">it</span><span class="p">.</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">10</span><span class="p">:</span><span class="w"> </span><span class="nx">Show</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">shows</span><span class="w"> </span><span class="nx">released</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="mi">2020</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">genre</span><span class="w"> </span><span class="s">&quot;Documentaries&quot;</span><span class="p">?</span>
<span class="nx">Task</span><span class="w"> </span><span class="mi">11</span><span class="p">:</span><span class="w"> </span><span class="nx">For</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="k">type</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">average</span><span class="w"> </span><span class="nx">durations</span><span class="p">?</span>
<span class="p">(</span><span class="nx">Show</span><span class="w"> </span><span class="nx">dataframe</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="k">type</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">duration</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">columns</span><span class="p">)</span>
<span class="p">(</span><span class="nx">b</span><span class="p">)</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">generated</span><span class="w"> </span><span class="nx">questions</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">above</span><span class="w"> </span><span class="nx">programmatic</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span>

<span class="nx">Listing</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span><span class="w"> </span><span class="nx">Programmatic</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">generated</span><span class="w"> </span><span class="nx">questions</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="nx">concerning</span><span class="w"> </span><span class="nx">Netflix</span><span class="w"> </span><span class="nx">TV</span><span class="w"> </span><span class="nx">show</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">movie</span><span class="w"> </span><span class="nx">dataset</span><span class="p">.</span>
</code></pre></div>

<p>"""
The input dataframe schema is:
Schema of Dataframes:
Columns in alc with example values:
country (Afghanistan), beer_servings (0), spirit_servings (0), wine_servings (0),
total_litres_of_pure_alcohol (0.0), continent (AS)
" " "</p>
<h1>The Python solution is:</h1>
<p>import pandas as pd
alc = pd.read_csv("drinks.csv")
alc['continent'].unique()
alc.groupby('continent').agg({'beer_servings': np.mean}).sort_values(
    'beer_servings', ascending=True)
alc.dropna()</p>
<h1>The execution output is:</h1>
<p>alc:
| beer_servings (float) |
|----------------|
| 102 |
| 20 |</p>
<h1>The user intent is:</h1>
<p>Rank the continents that consume the least amount of beer on average.</p>
<h1>The I/O specification is:</h1>
<p>alc: a pandas.core.DataFrame. Given the user intent and the code, the salient
columns (at most given 3) in the input dataframe are beer_servings, continent.
The output dataframe has columns (at most given 3) such as beer_servings.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Listing</span><span class="w"> </span><span class="mh">4</span><span class="o">:</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">exemplar</span><span class="w"> </span><span class="n">regarding</span><span class="w"> </span><span class="n">pandas</span><span class="p">.</span><span class="w"> </span><span class="n">DataFrame</span><span class="p">.</span>
</code></pre></div>

<p>" " "
The input schema is:</p>
<h1>Schema of Dataframes:</h1>
<h1>Columns in phones with example values:</h1>
<h1>manufacturer (Nokia), model (1100), form (Bar), smartphone (No), year (2003)</h1>
<p>" " "</p>
<h1>The Python solution is:</h1>
<p>yearly_smartphones = phones.groupby(['year', 'smartphone'],
    as_index=False).size().pivot_table(
        index='year',columns='smartphone', values='size').fillna(0)
yearly_smartphones.groupby((yearly_smartphones.index//10)*10).Yes.sum()</p>
<h1>The execution output is:</h1>
<div class="codehilite"><pre><span></span><code>__output
</code></pre></div>

<p>pandas.core.series.Series</p>
<h1>The user intent is:</h1>
<p>How many different smartphones were released each decade
The I/O specification is:</p>
<div class="codehilite"><pre><span></span><code><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="n">The</span> <span class="n">prompt</span> <span class="n">used</span> <span class="n">to</span> <span class="n">query</span> <span class="n">LLM</span> <span class="k">for</span> <span class="n">I</span><span class="o">/</span><span class="n">O</span> <span class="n">summary</span><span class="o">.</span>
\<span class="c1"># In[ ] :</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="n">phones</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;dataset/best-selling-mobile-phones.csv&#39;</span><span class="p">)</span>
\<span class="c1"># In[ ] :</span>
\<span class="c1"># Schema of Dataframes:</span>
\<span class="c1"># Columns in phones with example values:</span>
\<span class="c1"># manufacturer (Nokia), model (1100), form (Bar), smartphone (No), year (2003)</span>
\<span class="c1"># In[ ] :</span>
<span class="n">phones</span><span class="p">[</span><span class="n">phones</span><span class="o">.</span><span class="n">form</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;touchscreen&#39;</span><span class="p">)]</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span>
<span class="s1">&#39;manufacturer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
\<span class="c1"># In[ ] :</span>
<span class="n">year_phones</span> <span class="o">=</span> <span class="n">phones</span><span class="p">[</span><span class="n">phones</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">phones</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="kp">max</span><span class="p">()</span><span class="o">-</span><span class="mi">15</span><span class="p">]</span>
<span class="n">year_phones</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;year&#39;</span><span class="p">,</span><span class="s1">&#39;manufacturer&#39;</span><span class="p">,</span><span class="s1">&#39;form&#39;</span><span class="p">],</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="kp">size</span><span class="p">()</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span>
<span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span><span class="s1">&#39;manufacturer&#39;</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;form&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
\<span class="c1"># In[ ] :</span>
<span class="n">How</span> <span class="n">many</span> <span class="n">different</span> <span class="n">smartphones</span> <span class="n">were</span> <span class="n">released</span> <span class="n">each</span> <span class="n">decade</span><span class="err">?</span>
<span class="s2">&quot; &quot;</span> <span class="s2">&quot;</span>
<span class="n">Input</span><span class="o">-</span><span class="n">output</span> <span class="n">Summary</span><span class="p">:</span>
<span class="n">__output__</span><span class="p">:</span> <span class="n">a</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">series</span><span class="o">.</span><span class="n">Series</span><span class="o">.</span> <span class="n">Given</span> <span class="n">the</span> <span class="n">user</span> <span class="n">intent</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">code</span><span class="p">,</span> <span class="n">the</span> <span class="n">salient</span> <span class="n">columns</span> <span class="p">(</span><span class="n">at</span> <span class="n">most</span> <span class="n">given</span> <span class="mi">3</span><span class="p">)</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">input</span> <span class="n">dataframe</span> <span class="n">are</span> <span class="n">smartphone</span><span class="p">,</span> <span class="n">year</span><span class="o">.</span> <span class="n">The</span> <span class="n">output</span> <span class="n">values</span> <span class="n">are</span> <span class="n">of</span> <span class="nb">type</span> <span class="n">int64</span><span class="p">,</span> <span class="k">with</span> <span class="n">an</span> <span class="n">index</span> <span class="n">year</span><span class="o">.</span> <span class="n">Here</span> <span class="ow">is</span> <span class="n">my</span> <span class="n">code</span> <span class="n">solution</span><span class="p">:</span> <span class="s2">&quot; &quot;</span> <span class="s2">&quot;</span>
\<span class="c1"># In[ ] :</span>
<span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="n">A</span> <span class="kp">test</span> <span class="n">example</span> <span class="n">on</span> <span class="n">ARCADE</span> <span class="n">augmented</span> <span class="k">with</span> <span class="n">a</span> <span class="n">LLM</span> <span class="n">generated</span> <span class="n">I</span><span class="o">/</span><span class="n">O</span> <span class="n">summary</span><span class="o">.</span>
</code></pre></div>

<p>year
1990
    0.0
2000
    12.0
2010
    58.0
Name: Yes, dtype: float64</p>
<div class="codehilite"><pre><span></span><code><span class="ss">(</span><span class="nv">c</span><span class="ss">)</span><span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">ground</span><span class="w"> </span><span class="nv">truth</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">output</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">above</span><span class="w"> </span><span class="nv">test</span><span class="w"> </span><span class="nv">example</span>.

<span class="nv">Listing</span><span class="w"> </span><span class="mi">5</span>:<span class="w"> </span><span class="nv">An</span><span class="w"> </span><span class="nv">example</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">I</span><span class="o">/</span><span class="nv">O</span><span class="w"> </span><span class="nv">summary</span><span class="w"> </span><span class="nv">generated</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">LLM</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">ARCADE</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">pandas</span>.<span class="w"> </span><span class="nv">Series</span>.
</code></pre></div>

<p>" " "
The input schema is:
Schema of Dataframes:
Columns in df with example values:
Movie (JugJugg Jeeyo), Worldwide (74.5), India Net (50.24), India Gross (54.5),
Budget (100), Verdict (None), Movie Type (Bollywood), Released Date (24-Jun-22)
" " "</p>
<h1>The Python solution is:</h1>
<p>df_t[['month', 'Worldwide']].groupby('month').sum().T</p>
<h1>The execution output is:</h1>
<div class="codehilite"><pre><span></span><code>__output
</code></pre></div>

<p>pandas.core.frame.DataFrame</p>
<h1>The user intent is:</h1>
<p>List the worldwide gross of the films released for each month since November,
2021 to June 2022. List the months in chronological order.</p>
<h1>The I/O specification is:</h1>
<p>```</p>
<p>(a) The prompt used to query LLM for I/O summary.
# In[ ] :
from datetime import datetime
import pandas as pd
df = pd.read_csv('dataset/bollywood_2022.csv')
# In[ ] :
# Schema of Dataframes:
# Columns in df with example values:
# Movie (JugJugg Jeeyo), Worldwide (74.5), India Net (50.24), India Gross (54.5),
# Overseas (20.0), Movie Type (Bollywood), Released Date (24-Jun-22)
# In[ ] :
df.columns = [column.replace(' ', '') for column in df.columns]
# In[ ] :
List the worldwide gross of the films released for each month # since
November, 2021 to June 2022. List the months in chronological order.
" " "
Input-output Summary:
<strong>output</strong>: a pandas.core.DataFrame. Given the user intent and the code, the salient columns (at most given 3) in the input dataframe are Worldwide, Released Date. The output dataframe has columns (at most given 3) such as month, Worldwide. Here is my code solution:
" " "
# In[ ] :
(b) A test example on ARCADE augmented with a LLM generated I/O summary.
| April, 2022 (float) | December, 2021 (float) | February, 2022 (float)
| January, 2022 (float) | June, 2022 (float) | March, 2022 (float)
| May, 2022 (float) | November, 2021 (float) |
|------------------------------------------------------------|
| 1748.13 | 10988.6 | 1812.99 | 114.12 | 4763.05 | 3849.25 | 7730.75 | 169.23 |
(c) The ground truth dataframe of the output in the above test example.</p>
<p>Listing 6: An noisy example of LLM generated I/O summary. The LLM generated I/O specification is inaccurate as evident from the discrepancies between the I/O summary in part (b) and the ground truth in part (c).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">ARCADE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">pass@5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">pass@20</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No Context</td>
<td style="text-align: center;">Full Context</td>
<td style="text-align: center;">No Context</td>
<td style="text-align: center;">Full Context</td>
</tr>
<tr>
<td style="text-align: center;">GIFT4CODE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Code LLM (no spec.)</td>
<td style="text-align: center;">20.78</td>
<td style="text-align: center;">34.33</td>
<td style="text-align: center;">33.40</td>
<td style="text-align: center;">46.94</td>
</tr>
<tr>
<td style="text-align: center;">+ TypeDesc</td>
<td style="text-align: center;">20.59</td>
<td style="text-align: center;">33.43</td>
<td style="text-align: center;">32.98</td>
<td style="text-align: center;">46.01</td>
</tr>
<tr>
<td style="text-align: center;">+ I/O Examples</td>
<td style="text-align: center;">20.04</td>
<td style="text-align: center;">31.76</td>
<td style="text-align: center;">31.40</td>
<td style="text-align: center;">43.60</td>
</tr>
<tr>
<td style="text-align: center;">+ I/O Summary</td>
<td style="text-align: center;">20.04</td>
<td style="text-align: center;">32.39</td>
<td style="text-align: center;">32.24</td>
<td style="text-align: center;">45.14</td>
</tr>
</tbody>
</table>
<p>Table 3: pass@ $k$ on ARCADE. For each type of I/O specification in Tab. 3 (e.g. $+1 / 0$ Summary), intents are augmented with I/O specifications of that type (e.g. intents inline with I/O summary) in fine-tuning data. At test time, input intents do not contain have any specification.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We only use the annotated intents, while the code solutions and I/O specifications are still predicted by the LLM. We ensure the training and evaluation problems are disjoint and from different Stack Overflow posts.
${ }^{4}$ StarCoder 15B has slightly higher pass@1 on DS-1000 compared to numbers in Ni et al. (2023b).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
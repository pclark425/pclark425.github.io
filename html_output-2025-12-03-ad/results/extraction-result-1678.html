<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1678 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1678</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1678</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-264146219</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.10606v1.pdf" target="_blank">BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning</a></p>
                <p><strong>Paper Abstract:</strong> Domain randomization (DR), which entails training a policy with randomized dynamics, has proven to be a simple yet effective algorithm for reducing the gap between simulation and the real world. However, DR often requires careful tuning of randomization parameters. Methods like Bayesian Domain Randomization (Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue by automating parameter range selection using real-world experience. While effective, these algorithms often require long computation time, as a new policy is trained from scratch every iteration. In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune), which inherits the spirit of BayRn but aims to significantly accelerate the learning processes by fine-tuning from previously learned policy. This idea leads to a critical question: which previous policy should we use as a prior during fine-tuning? We investigated four different fine-tuning strategies and compared them against baseline algorithms in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments. Our analysis demonstrates that our method yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1678.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1678.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Gym locomotion (Hopper/HalfCheetah/Ant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Gym locomotion benchmark tasks: Hopper-v3, HalfCheetah-v3, Ant-v3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard simulated locomotion benchmark environments used to evaluate sim-to-real transfer techniques; in this paper they are used as simulated testbeds where domain-randomized policies are trained and evaluated in a held-out ground-truth simulation to proxy real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Hopper-v3 / HalfCheetah-v3 / Ant-v3</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Simulated legged robots for 2D (Hopper, HalfCheetah) and 3D (Ant) locomotion tasks; agents learn torques/actions to maximize forward distance while avoiding falls.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>OpenAI Gym (benchmark environments)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based simulation environments for continuous-control locomotion tasks (standard benchmark wrappers; simulate rigid-body dynamics, contacts, actuators and rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics simulation (benchmark simulators used as proxies for real dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body dynamics, contact dynamics, friction, actuator forces/torques, state observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>simulator-specific approximations to contacts and friction, no explicit modeling of hardware delays/joint slackness/complex unmodeled real-world perturbations (only approximated via randomization), sensor noise not explicitly described for these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Evaluations performed in a held-out 'ground-truth' simulated environment (used as a proxy for the real world) rather than on physical hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Dynamic locomotion policies (maximizing forward distance without falling).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning (policy optimization) with domain randomization; BayRnTune fine-tuning across Bayesian-optimized DR parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Maximum historical episodic reward evaluated in the ground-truth environment (aggregated as median across seeds); implicit success = higher episodic reward / distance traveled.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized dynamics parameters such as mass distributions (including mass multiplier and mass shift for HalfCheetah), control frequency, friction coefficients, and random external perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in dynamics (mass distribution, friction), modelling assumptions in simulator, under-modeled effects like joint slackness and control delays; multimodality of reward landscape causing non-monotonic relation between parameter distance and policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Adaptive selection of DR ranges via Bayesian optimization combined with strategic fine-tuning from previous checkpoints (BayRnTune); the Infinite Chain fine-tuning strategy that continues training a single checkpoint longer tended to work best.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No numeric fidelity thresholds given; qualitative observation that proximity to ground-truth parameters often correlates with better performance for some tasks (Hopper, Ant) but not universally (multi-modal cases exist).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuning was performed in simulation only: initial 'bootstrap' training for a large number of timesteps followed by repeated fine-tuning (T_Tune) from selected checkpoints guided by tuning strategies; no real-world fine-tuning performed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BayRnTune (Bayesian domain randomization with strategic fine-tuning) improved sample efficiency and final reward on standard locomotion benchmarks compared to vanilla DR and Bayesian DR; Infinite Chain (continuous fine-tuning of the last policy) often performed best, suggesting long continued training from a single checkpoint plus adaptive DR ranges helps transfer to a ground-truth dynamics proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1678.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1678.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BALLU walking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Buoyancy Assisted Lightweight Legged Unit (BALLU) walking task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A buoyancy-assisted balloon-based bipedal robot simulated in PyBullet; used to evaluate sim-to-real sensitivity because small dynamics changes (buoyancy, drag, friction) can dramatically alter behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>BALLU (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A balloon-supported two-legged robot where buoyancy keeps the base afloat; control requires careful handling of sensitive dynamics to walk forward while avoiding sinking or uncontrolled flotation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics locomotion (specialized buoyancy-assisted legged robot)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet (custom BALLU simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics simulator modeling rigid-body dynamics, contacts, friction, drag, external perturbation forces and robot actuators for the BALLU model.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics with dynamics randomization; intended to capture friction, drag and perturbations but with simplifying assumptions (not fully high-fidelity for buoyancy-sensitive behaviors).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>friction, drag coefficients, external perturbation forces, contact dynamics and actuator controls; buoyancy and mass modeled but not randomized in experiments (to avoid sinking/float-away).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Mass and buoyancy were not randomized (small changes could destabilize the robot); possible simplifications in fluid/balloon interactions and high-fidelity aerodynamics; sensor and actuator latencies not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>No hardware deployment performed in this study; evaluation used a ground-truth simulated environment as proxy for the real world.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Walking forward quickly while minimizing lateral velocity (locomotion policy robust to friction/drag/perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning with domain randomization and BayRnTune fine-tuning strategies; T_Bootstrap = 6M, T_Tune = 2M, total 32M timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Episodic reward composed of forward velocity reward minus lateral penalty; also reported walking distance within 20s.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported in ground-truth simulated evaluation: learned policy achieved reward 56.09 vs Vanilla DR 14.11; walked 4.19 m more within 20 seconds compared to baseline (values are simulation-ground-truth proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized friction, drag coefficients, and external perturbation forces; mass and buoyancy not randomized to avoid extreme failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Highly sensitive dynamics (buoyancy effects), multimodal reward landscapes where policies learned on different friction values (e.g., high friction) can yield unexpectedly robust behaviors; unmodeled aerodynamic/balloon interactions could hinder real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>BayRnTune with strategic fine-tuning (Best Only used for BALLU) and BO-guided adjustment of DR ranges enabled reaching high reward regimes in the ground-truth simulation; choosing not to randomize mass/buoyancy avoided catastrophic mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative requirement provided; qualitative finding that BO can converge to alternate high-performing parameter regions (e.g., friction=1.7) that produce different policy modes (jumping-like) which may be robust across ranges—indicating that matching ground-truth is not always required.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuning performed entirely in simulation; first bootstrap training of 6M steps then fine-tuning steps of 2M guided by checkpoint-selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BayRnTune enabled substantially better performance (reward and distance) on a sensitive buoyancy-assisted robot in simulation-proxy evaluations compared to vanilla DR; however, BO sometimes converged to alternate high-performing simulation parameters (multi-modality) rather than the true parameters, highlighting that high reward in sim does not always imply parameter identification of ground-truth dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1678.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1678.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quadruped pushing (Unitree A1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quadrupedal puck-pushing task with Unitree A1 (simulated in IsaacGym)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quadruped robot trained to push a cylindrical puck to a fixed target using IsaacGym with randomized surface friction; used to evaluate BayRnTune's ability to find DR parameters and speed up policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Unitree A1 (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A quadrupedal robot capable of locomotion and interaction with objects; in the experiment it pushes a puck to a target location while controlling locomotion and contact forces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics manipulation combined with locomotion (object pushing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>NVIDIA IsaacGym</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A GPU-parallelized physics simulator modeling rigid-body dynamics, contacts, friction, joint torques, and concurrent multi-instance simulation for large-scale RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-throughput approximate physics with attention to contact/friction dynamics (GPU-accelerated); still an approximate simulator relative to hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>contact dynamics, surface friction (puck and ground), joint torques, joint angles/velocities, collisions, multi-agent parallelism; per-instance randomized puck friction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Potential simplifications in fine-grained contact micro-physics, sensing noise, motor latency/actuator dynamics, and real-world variability beyond the randomized friction distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>No physical deployment reported; evaluation performed against a ground-truth dynamics simulation (truncated normal friction distribution centered at 0.75) serving as the 'real' proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Pushing a puck to a target location with a quadruped robot while maintaining stable locomotion and contact behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning with residual perturbation policies built on pre-learned walking skill; domain randomization with BO and BayRnTune fine-tuning; large-scale parallel training (4096 instances) in IsaacGym.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Weighted episodic reward (sum of multiple components) and distance/error to target (average error reported in cm).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>In ground-truth simulated evaluation (proxy real): BayRnTune agents achieved reward ≈ 4330 vs Vanilla DR ≈ 3350; BayRnTune reduced average object-to-target error to ≈ 1 cm (nearly perfect) and pushed puck 40 cm closer than baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized puck surface friction in range [0.5, 1.25]; BO modeled true parameter as truncated normal (center 0.75, sd 0.01) for oracle comparison; policies trained as constrained residual perturbations to pre-learned walking skill.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Surface friction mismatch, contact dynamics complexity, and limits of simulator realism (actuator/sensor delays and real friction variability) are noted contributors to potential sim-to-real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>BayRnTune with Infinite Chain tuning strategy (continuing training from last checkpoint) and BO-guided DR parameter identification, plus training residual policies on top of pre-learned walking skill and massive parallel simulation, enabled near-oracle performance in the ground-truth simulated proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No explicit quantitative fidelity thresholds; paper shows that identifying friction close to ground-truth correlated with good performance in this task but does not give a numeric tolerance.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuning was done in simulation; large-scale training (2.8B timesteps total with T_Bootstrap=280M and T_Tune=280M reported for this task) and checkpoint-based fine-tuning strategies were used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BayRnTune with strategic fine-tuning and BO-adapted DR ranges yielded policies that matched or approached oracle (ground-truth-parameter-trained) performance in a simulated quadruped pushing task; careful DR (e.g., friction randomization) plus fine-tuning from suitable checkpoints can strongly improve transfer to a ground-truth dynamics proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Data-efficient domain randomization with bayesian optimization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Residual physics learning and system identification for sim-to-real transfer of policies on buoyancy assisted legged robots <em>(Rating: 2)</em></li>
                <li>Active domain randomization <em>(Rating: 1)</em></li>
                <li>Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1678",
    "paper_id": "paper-264146219",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "OpenAI Gym locomotion (Hopper/HalfCheetah/Ant)",
            "name_full": "OpenAI Gym locomotion benchmark tasks: Hopper-v3, HalfCheetah-v3, Ant-v3",
            "brief_description": "Standard simulated locomotion benchmark environments used to evaluate sim-to-real transfer techniques; in this paper they are used as simulated testbeds where domain-randomized policies are trained and evaluated in a held-out ground-truth simulation to proxy real-world transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "Hopper-v3 / HalfCheetah-v3 / Ant-v3",
            "agent_system_description": "Simulated legged robots for 2D (Hopper, HalfCheetah) and 3D (Ant) locomotion tasks; agents learn torques/actions to maximize forward distance while avoiding falls.",
            "domain": "general robotics locomotion",
            "virtual_environment_name": "OpenAI Gym (benchmark environments)",
            "virtual_environment_description": "Physics-based simulation environments for continuous-control locomotion tasks (standard benchmark wrappers; simulate rigid-body dynamics, contacts, actuators and rewards).",
            "simulation_fidelity_level": "approximate physics simulation (benchmark simulators used as proxies for real dynamics)",
            "fidelity_aspects_modeled": "rigid-body dynamics, contact dynamics, friction, actuator forces/torques, state observations",
            "fidelity_aspects_simplified": "simulator-specific approximations to contacts and friction, no explicit modeling of hardware delays/joint slackness/complex unmodeled real-world perturbations (only approximated via randomization), sensor noise not explicitly described for these experiments",
            "real_environment_description": "Evaluations performed in a held-out 'ground-truth' simulated environment (used as a proxy for the real world) rather than on physical hardware.",
            "task_or_skill_transferred": "Dynamic locomotion policies (maximizing forward distance without falling).",
            "training_method": "Deep reinforcement learning (policy optimization) with domain randomization; BayRnTune fine-tuning across Bayesian-optimized DR parameters.",
            "transfer_success_metric": "Maximum historical episodic reward evaluated in the ground-truth environment (aggregated as median across seeds); implicit success = higher episodic reward / distance traveled.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized dynamics parameters such as mass distributions (including mass multiplier and mass shift for HalfCheetah), control frequency, friction coefficients, and random external perturbations.",
            "sim_to_real_gap_factors": "Mismatch in dynamics (mass distribution, friction), modelling assumptions in simulator, under-modeled effects like joint slackness and control delays; multimodality of reward landscape causing non-monotonic relation between parameter distance and policy performance.",
            "transfer_enabling_conditions": "Adaptive selection of DR ranges via Bayesian optimization combined with strategic fine-tuning from previous checkpoints (BayRnTune); the Infinite Chain fine-tuning strategy that continues training a single checkpoint longer tended to work best.",
            "fidelity_requirements_identified": "No numeric fidelity thresholds given; qualitative observation that proximity to ground-truth parameters often correlates with better performance for some tasks (Hopper, Ant) but not universally (multi-modal cases exist).",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Fine-tuning was performed in simulation only: initial 'bootstrap' training for a large number of timesteps followed by repeated fine-tuning (T_Tune) from selected checkpoints guided by tuning strategies; no real-world fine-tuning performed.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "BayRnTune (Bayesian domain randomization with strategic fine-tuning) improved sample efficiency and final reward on standard locomotion benchmarks compared to vanilla DR and Bayesian DR; Infinite Chain (continuous fine-tuning of the last policy) often performed best, suggesting long continued training from a single checkpoint plus adaptive DR ranges helps transfer to a ground-truth dynamics proxy.",
            "uuid": "e1678.0",
            "source_info": {
                "paper_title": "BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "BALLU walking",
            "name_full": "Buoyancy Assisted Lightweight Legged Unit (BALLU) walking task",
            "brief_description": "A buoyancy-assisted balloon-based bipedal robot simulated in PyBullet; used to evaluate sim-to-real sensitivity because small dynamics changes (buoyancy, drag, friction) can dramatically alter behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "BALLU (simulated)",
            "agent_system_description": "A balloon-supported two-legged robot where buoyancy keeps the base afloat; control requires careful handling of sensitive dynamics to walk forward while avoiding sinking or uncontrolled flotation.",
            "domain": "robotics locomotion (specialized buoyancy-assisted legged robot)",
            "virtual_environment_name": "PyBullet (custom BALLU simulation)",
            "virtual_environment_description": "A physics simulator modeling rigid-body dynamics, contacts, friction, drag, external perturbation forces and robot actuators for the BALLU model.",
            "simulation_fidelity_level": "approximate physics with dynamics randomization; intended to capture friction, drag and perturbations but with simplifying assumptions (not fully high-fidelity for buoyancy-sensitive behaviors).",
            "fidelity_aspects_modeled": "friction, drag coefficients, external perturbation forces, contact dynamics and actuator controls; buoyancy and mass modeled but not randomized in experiments (to avoid sinking/float-away).",
            "fidelity_aspects_simplified": "Mass and buoyancy were not randomized (small changes could destabilize the robot); possible simplifications in fluid/balloon interactions and high-fidelity aerodynamics; sensor and actuator latencies not detailed.",
            "real_environment_description": "No hardware deployment performed in this study; evaluation used a ground-truth simulated environment as proxy for the real world.",
            "task_or_skill_transferred": "Walking forward quickly while minimizing lateral velocity (locomotion policy robust to friction/drag/perturbations).",
            "training_method": "Deep reinforcement learning with domain randomization and BayRnTune fine-tuning strategies; T_Bootstrap = 6M, T_Tune = 2M, total 32M timesteps.",
            "transfer_success_metric": "Episodic reward composed of forward velocity reward minus lateral penalty; also reported walking distance within 20s.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Reported in ground-truth simulated evaluation: learned policy achieved reward 56.09 vs Vanilla DR 14.11; walked 4.19 m more within 20 seconds compared to baseline (values are simulation-ground-truth proxies).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized friction, drag coefficients, and external perturbation forces; mass and buoyancy not randomized to avoid extreme failure modes.",
            "sim_to_real_gap_factors": "Highly sensitive dynamics (buoyancy effects), multimodal reward landscapes where policies learned on different friction values (e.g., high friction) can yield unexpectedly robust behaviors; unmodeled aerodynamic/balloon interactions could hinder real transfer.",
            "transfer_enabling_conditions": "BayRnTune with strategic fine-tuning (Best Only used for BALLU) and BO-guided adjustment of DR ranges enabled reaching high reward regimes in the ground-truth simulation; choosing not to randomize mass/buoyancy avoided catastrophic mismatch.",
            "fidelity_requirements_identified": "No quantitative requirement provided; qualitative finding that BO can converge to alternate high-performing parameter regions (e.g., friction=1.7) that produce different policy modes (jumping-like) which may be robust across ranges—indicating that matching ground-truth is not always required.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Fine-tuning performed entirely in simulation; first bootstrap training of 6M steps then fine-tuning steps of 2M guided by checkpoint-selection strategies.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "BayRnTune enabled substantially better performance (reward and distance) on a sensitive buoyancy-assisted robot in simulation-proxy evaluations compared to vanilla DR; however, BO sometimes converged to alternate high-performing simulation parameters (multi-modality) rather than the true parameters, highlighting that high reward in sim does not always imply parameter identification of ground-truth dynamics.",
            "uuid": "e1678.1",
            "source_info": {
                "paper_title": "BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Quadruped pushing (Unitree A1)",
            "name_full": "Quadrupedal puck-pushing task with Unitree A1 (simulated in IsaacGym)",
            "brief_description": "A quadruped robot trained to push a cylindrical puck to a fixed target using IsaacGym with randomized surface friction; used to evaluate BayRnTune's ability to find DR parameters and speed up policy learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "Unitree A1 (simulated)",
            "agent_system_description": "A quadrupedal robot capable of locomotion and interaction with objects; in the experiment it pushes a puck to a target location while controlling locomotion and contact forces.",
            "domain": "robotics manipulation combined with locomotion (object pushing)",
            "virtual_environment_name": "NVIDIA IsaacGym",
            "virtual_environment_description": "A GPU-parallelized physics simulator modeling rigid-body dynamics, contacts, friction, joint torques, and concurrent multi-instance simulation for large-scale RL training.",
            "simulation_fidelity_level": "high-throughput approximate physics with attention to contact/friction dynamics (GPU-accelerated); still an approximate simulator relative to hardware.",
            "fidelity_aspects_modeled": "contact dynamics, surface friction (puck and ground), joint torques, joint angles/velocities, collisions, multi-agent parallelism; per-instance randomized puck friction.",
            "fidelity_aspects_simplified": "Potential simplifications in fine-grained contact micro-physics, sensing noise, motor latency/actuator dynamics, and real-world variability beyond the randomized friction distribution.",
            "real_environment_description": "No physical deployment reported; evaluation performed against a ground-truth dynamics simulation (truncated normal friction distribution centered at 0.75) serving as the 'real' proxy.",
            "task_or_skill_transferred": "Pushing a puck to a target location with a quadruped robot while maintaining stable locomotion and contact behavior.",
            "training_method": "Deep reinforcement learning with residual perturbation policies built on pre-learned walking skill; domain randomization with BO and BayRnTune fine-tuning; large-scale parallel training (4096 instances) in IsaacGym.",
            "transfer_success_metric": "Weighted episodic reward (sum of multiple components) and distance/error to target (average error reported in cm).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "In ground-truth simulated evaluation (proxy real): BayRnTune agents achieved reward ≈ 4330 vs Vanilla DR ≈ 3350; BayRnTune reduced average object-to-target error to ≈ 1 cm (nearly perfect) and pushed puck 40 cm closer than baseline.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized puck surface friction in range [0.5, 1.25]; BO modeled true parameter as truncated normal (center 0.75, sd 0.01) for oracle comparison; policies trained as constrained residual perturbations to pre-learned walking skill.",
            "sim_to_real_gap_factors": "Surface friction mismatch, contact dynamics complexity, and limits of simulator realism (actuator/sensor delays and real friction variability) are noted contributors to potential sim-to-real gap.",
            "transfer_enabling_conditions": "BayRnTune with Infinite Chain tuning strategy (continuing training from last checkpoint) and BO-guided DR parameter identification, plus training residual policies on top of pre-learned walking skill and massive parallel simulation, enabled near-oracle performance in the ground-truth simulated proxy.",
            "fidelity_requirements_identified": "No explicit quantitative fidelity thresholds; paper shows that identifying friction close to ground-truth correlated with good performance in this task but does not give a numeric tolerance.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Fine-tuning was done in simulation; large-scale training (2.8B timesteps total with T_Bootstrap=280M and T_Tune=280M reported for this task) and checkpoint-based fine-tuning strategies were used.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "BayRnTune with strategic fine-tuning and BO-adapted DR ranges yielded policies that matched or approached oracle (ground-truth-parameter-trained) performance in a simulated quadruped pushing task; careful DR (e.g., friction randomization) plus fine-tuning from suitable checkpoints can strongly improve transfer to a ground-truth dynamics proxy.",
            "uuid": "e1678.2",
            "source_info": {
                "paper_title": "BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Data-efficient domain randomization with bayesian optimization",
            "rating": 2,
            "sanitized_title": "dataefficient_domain_randomization_with_bayesian_optimization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Residual physics learning and system identification for sim-to-real transfer of policies on buoyancy assisted legged robots",
            "rating": 2,
            "sanitized_title": "residual_physics_learning_and_system_identification_for_simtoreal_transfer_of_policies_on_buoyancy_assisted_legged_robots"
        },
        {
            "paper_title": "Active domain randomization",
            "rating": 1,
            "sanitized_title": "active_domain_randomization"
        },
        {
            "paper_title": "Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators",
            "rating": 1,
            "sanitized_title": "bayessim_adaptive_domain_randomization_via_probabilistic_inference_for_robotics_simulators"
        }
    ],
    "cost": 0.011982999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning
16 Oct 2023</p>
<p>Tianle Huang 
Nitish Sontakke nitishsontakke@gatech.edu 
K Niranjan Kumar niranjankumar@gatech.edu 
Irfan Essa 
Stefanos Nikolaidis nikolaid@usc.edu 
SN is with University of Southern California
90007Los AngelesCAUSA</p>
<p>Dennis W Hong dennishong@ucla.edu 
DH is with
University of California
90095Los AngelesCAUSA</p>
<p>Sehoon Ha sehoonha@gatech.edu </p>
<p>Georgia Institute of Tech-nology
30332GAUSA</p>
<p>BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning
16 Oct 2023662D537F2ECF8BB4C614FA34A58A0FF8arXiv:2310.10606v1[cs.RO]
Domain randomization (DR), which entails training a policy with randomized dynamics, has proven to be a simple yet effective algorithm for reducing the gap between simulation and the real world.However, DR often requires careful tuning of randomization parameters.Methods like Bayesian Domain Randomization (Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue by automating parameter range selection using real-world experience.While effective, these algorithms often require long computation time, as a new policy is trained from scratch every iteration.In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune), which inherits the spirit of BayRn but aims to significantly accelerate the learning processes by fine-tuning from previously learned policy.This idea leads to a critical question: which previous policy should we use as a prior during fine-tuning?We investigated four different fine-tuning strategies and compared them against baseline algorithms in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments.Our analysis demonstrates that our method yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR.</p>
<p>I. INTRODUCTION</p>
<p>Deep reinforcement learning (deep RL) has emerged as a promising tool for developing control policies for a wide range of robot control problems, from manipulation to locomotion.However, deep RL trains a policy from a massive amount of simulation experience and transfers it to real robots, often resulting in a serious challenge known as the "sim-to-real" gap [1].The difference between simulation and the real world can cause degraded hardware performance or pose risks to the robot and its surroundings.One common approach to reducing the gap is via system identification [2], [3].However, this approach requires prior knowledge from an expert to tune the system parameter spaces.An alternative technique is domain randomization (DR) [4], where an agent is exposed to a diverse set of parameters during training to improve its generalization capabilities.The deep RL community often adopts this technique due to its simplicity and effectiveness over traditional system identification.</p>
<p>However, determining the appropriate parameters for domain randomization is not straightforward, which has a significant impact on the policy performance.If the range Fig. 1.</p>
<p>We investigated our novel sim-to-real learning algorithm, BayRnTune, on many robotic tasks, including BALLU walking (top) and Quadruped pushing (bottom).is too narrow, the policy will not be robust to changes in the testing environment.If it is too wide, the learned behavior becomes sub-optimal and conservative [5].To this end, there has been an investigation into automated techniques for adjusting the range of the parameters.Muratore et al. [6] proposed Bayesian DR by combining DR with a gradientfree optimizer, Bayesian Optimization, which adjusts the parameters based on the real-world evaluation.However, this approach requires repetitive policy training with different hyperparameters, which is computationally expensive.</p>
<p>In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning, BayRnTune, which is designed to accelerate Bayesian DR by leveraging policy fine-tuning.Our intuition is that the optimal policy for one DR configuration is likely to be close to a previously trained policy and can be reached via fine-tuning, instead of learning from scratch, effectively reducing the training time.However, this strategy raises another follow-up question: What are the best previous policy parameters, or "checkpoints", to resume from?We investigate four different tuning strategies, Normalized Closest Only, Infinite Chain, Best Only, and Best of Last 5, based on their prioritization of the realworld performance, time-difference, or the distance in the parameter space.</p>
<p>We conducted five simulated sim-to-real experiments on three OpenAI gym benchmark environments (Hopper, HalfCheetah, and Ant) and two robotic control problems (BALLU walking and Quadruped pushing, Figure 1).Our results indicate that the proposed BayRnTune performed 25% to 400% better compared to the baselines of vanilla DR and Bayesian DR, and was able to achieve higher rewards.In addition, we also found that the Infinite Chain strategy works the best in most of the problems.We further discuss their convergence, which motivates future research directions.</p>
<p>II. RELATED WORK</p>
<p>A. Sim-to-Real Transfer.</p>
<p>This paragraph describes related work in sim-to-real transfer with a focus on low-level control.Due to its importance, a variety of techniques have been investigated to mitigate the sim-to-real or reality gap [1].Many prior works focus on improving simulation fidelity by performing system identification [7], [8], [9], [10], [11], [12], establishing the learned models of the environment [13], [14], or learning residual dynamics [15], [16], [17], [18].Alternative approaches involve addressing the differences in data distribution between simulated environments and the real world.This includes domain randomization [4], [19], [20], [21], adaptation [20], [22], [23], meta-learning [24], [17], and fine-tuning simulationtrained policies on hardware [25].There exists another line of research that aims to circumvent the sim-to-real gap by learning policies directly on real robots [26], [27].Despite extensive research carried out in this domain, the sim-to-real problem has not been fully solved yet and remains one of the prevalent problems in robotics.</p>
<p>B. Domain Randomization</p>
<p>Domain randomization (DR) has emerged as one of the most effective and popular techniques for tackling the simto-real problem, particularly in the context of deep RL approaches.The appeal of domain randomization lies in its simplicity and effectiveness in tackling complex phenomena such as mismatch in dynamics, battery discharge, joint slackness, and sensor noise.In addition, DR can also be extended with many techniques mentioned in the previous sections due to its simplicity.These methods have been used to deploy policies successfully on different types of robots, such as manipulators [4], [19], [28], bipedal robots [21], and quadrupedal robots [20], and many more.</p>
<p>However, one challenge with domain randomization is that it is not straightforward to determine the ranges of the randomization parameters.If it is too narrow, the resulting policy will not be robust.If it is too wide, deep RL will learn a conservative policy that is sub-optimal.Therefore, researchers have proposed systematic approaches to determine the parameters of DR, such as BayesSim [29], Online BayesSim [30], Automatic DR [28], Bayesian DR [6], Neural posterior DR [31], Neural posterior DR [31] and Active Domain Randomization [32], which typically leverage realworld evaluation data to calibrate the DR ranges.While being effective, these techniques can be sample-inefficient because it requires repetitive training in simulation and testing in the real world.On top of this work, we aim to propose a more sample-efficient approach by combining it with fine-tuning.</p>
<p>III. BACKGROUND</p>
<p>Deep Reinforcement Learning.The task of robot control can be formulated as a Markov Decision Process (MDP).An MDP consists of a state space S, an action space A, a stochastic transition function p(s t+1 |s t , a t ), a reward function r(s t , a t ), and an initial state distribution p(s 0 ).By executing a policy π θ (a t |s t ), which is parameterized by θ, we can generate a sequence of states and actions known as a trajectory, denoted as τ = (s 0 , a 0 , s 1 , a 1 , . ..).The probability of generating a trajectory τ under the policy π is given by ρ π θ (τ ) = p(s 0 ) t π θ t (a t |s t )p(s t+1 |s t , a t ).Our objective is to find the policy that maximizes the expected sum of rewards over all possible trajectories, as defined by the objective function:
J(π θ ) = E τ ∼ρπ θ T t=0 r(s t , a t ) .
Domain Randomization and Bayesian Domain Randomization.Domain Randomization (DR) is a widely recognized technique for reducing the sim-to-real gap.Typically, DR ranges are selected by centering them around parameters believed to be close to the actual values of real-world physics.However, two issues arise from this approach.First, users require domain knowledge acquired through calibration or some form of system identification to make an informed selection.Second, even if the correct range is chosen in the simulation, it may not accurately reflect the true dynamics due to underlying modeling assumptions made in the simulator or unmodeled dynamics, such as joint slackness or control delays.</p>
<p>To address the challenge of determining suitable DR parameters, [6] propose the Bayesian Domain Randomization (Bayesian DR) algorithm.This algorithm treats the search for DR parameters that maximize the performance in the real world as an optimization problem.Specifically, Bayesian optimization (BO), a gradient-free and sample-efficient algorithm, is employed to identify candidate DR parameters.The real-world performance is obtained by deploying the learned policy and then feeding the results back into the optimizer.By leveraging this feedback mechanism, the algorithm effectively automates the search for favorable DR parameters.</p>
<p>IV. BAYRNTUNE: BAYESIAN DR WITH FINETUNING</p>
<p>A. Algorithm</p>
<p>We introduce the BayRnTune algorithm, which aims to improve the sample efficiency of vanilla Bayesian Domain Randomization via policy fine-tuning.In the Bayesian DR approach, the model is trained from scratch in every optimization step.However, this can be computationally expensive and time-consuming.In contrast, we propose to fine-tune a policy from an existing checkpoint under the assumption that the optimal policy for one DR range is not too far from the optimum for another, and hence, we can improve the sample efficiency drastically.</p>
<p>The BayRnTune Algorithm addresses this inefficiency by preserving all previous checkpoints along with their associated real-world rewards.During each Bayesian optimization step, the algorithm employs a tuning strategy (refer to Section IV-B), which takes the BO-proposed next parameters as inputs and searches the history of all previous optimization steps to determine the appropriate checkpoint to continue the training process from.Once the checkpoint is determined, the learning resumes and continues to fine-tune the given policy.</p>
<p>For each optimization step, we train the model for T Tune steps.However, the first optimization step is trained for T Bootstrap steps, which is typically much larger than T Tune to avoid evaluation of premature policies.For a detailed description, refer to Algorithm 1.
θ + ← TuneStrat(ϕ i ; h) ▷ Checkpoint lookup 7: θ i ← PolOpt(θ + ; ϕ i ; T Tune ) ▷ Fine-tuning 8: r i ← RealEval(π(θ i )) ▷ Real-world eval 9: BayOpt.update(ϕ i ; r i ) ▷ Update BO 10: h.append ((θ i , ϕ i , r i )) 11: end for 12: i * ← argmax i r i
▷ Search the best policy 13: return π(θ i * )</p>
<p>B. Tuning strategies</p>
<p>The BayRnTune algorithm is centered around a tuning strategy that plays a crucial role in determining the next steps of optimization.This strategy takes into account several inputs, including the DR parameter for the current optimization step ϕ i , the past policies θ 1 . . .θ i−1 , their associated DR parameters ϕ 1 , . . ., ϕ i−1 , and the real-world rewards r 1 , . . ., r i−1 .Based on these inputs h = [(θ 1 , ϕ 1 , r 1 ), . . ., (θ i−1 , ϕ i−1 , r i−1 )], the tuning strategy makes decisions on which policy to train from θ + .</p>
<p>In this paper, we have investigated four different tuning strategies: Normalized Closest Only, Infinite Chain, Best Only, and Best of Last 5. Normalized Closest Only.The first tuning strategy is Normalized Closest Only.It always chooses the policy that is trained with DR parameters that are "closest" to the current DR parameters.To take various dimensions into consideration, we normalize each DR parameter by its standard deviation, and "closest" is defined to have the smallest 2-norm between the normalized DR parameters.Specifically, we compute the running mean µ and standard deviation σ during optimization and get the parameters φ = diag(σ) −1 (ϕ − µ).The checkpoint is then chosen by minimizing the 2-norm of the difference between parameters.
i + = argmin 1≤i ′ ≤i−1 φi,• − φi ′ ,• 2 , θ + = θ i + .
Infinite Chain.In contrast to Normalized Closest Only, the second tuning strategy, Infinite Chain simply always chooses to continue to train from the last policy.Intuitively, it is fine tuning one singular model.Specifically,
θ + = θ i−1 .
Best Only.The third tuning strategy is Best Only, which will always continue to train from the policy that produced the highest real world rewards.Specifically,
i + = argmax 1≤i ′ ≤i−1 r i ′ , θ + = θ i + .
Best of Last M .The fourth tuning strategy Best of Last M chooses the policy that reaches the highest real world rewards from the last M optimization steps.Specifically,
i + = argmax max{1,i−M }≤i ′ ≤i−1 θ i ′ , θ + = θ i + .
For this study, we specifically choose M = 5, which yields the Best of Last 5 strategy.</p>
<p>Note Best of Last M can be viewed as a generalization of Infinite Chain and Best Only, as they are special cases where M = 1 and M → ∞, respectively.Choice of M can seen as a balance between exploration vs. exploitation.A larger M encourages taking advantage of previously found good policies, while a smaller M pushes the policy to explore more.</p>
<p>V. RESULTS</p>
<p>In this section, we perform various experiments to analyze the effectiveness of our method.Specifically, we designed our experiments to answer the following three questions.</p>
<p>1) Is our BayRnTune more sample-efficient than the baselines, vanilla DR and BayRn? 2) Which tuning strategy works best for our method?3) Do parameters in BayRnTune converge to the ground truth?</p>
<p>A. Task Description</p>
<p>We evaluated the proposed method in five simulated environments, including three benchmark environments and two robotic tasks.OpenAI Benchmark.Our first three environments are selected from the OpenAI Gym benchmark suite [33]: Hopper-v3, HalfCheetah-v3, and Ant-v3.All the tasks involve locomotion of the simulated robots either in 2D (Hopper and HalfCheetah) or 3D environments (Ant), where the goal is to maximize the distance traveled without falling.For all the experiments, we employ the same BayRnTune hyperparameters, T Bootstrap = 1M and T Tune = 200K.Hopper-v3 is trained for 10M environmental steps, while HalfCheetah-v3 and Ant-v3 are trained for 20M steps to account for increased complexity.To enable sim-to-real research, we randomize dynamics parameters, such as mass distributions, control frequencies, frictions, and random perturbations as approximation of randomized dynamics in the real world.In the HalfCheetah environment, we introduce the meta parameters of mass multiplier and mass shift that affect multiple body masses.Particularly, mass shift is designed to move COM forward (positive values) or backward (negative values) without changing the robot's total mass.The parameters are summarized in Table I.BALLU Walking.Buoyancy Assisted Lightweight Legged Unit (BALLU) [34] is a ballon-based legged robot that can walk with its two lightweight legs connected to a base kept afloat using an array of helium-filled balloons.Because of its sensitive dynamics, control of BALLU requires effective sim-to-real transfer.To evaluate our method on this robot, we design a simulated environment using PyBullet [35] similar to the work of Sontakke et al. [18].Here, the task is to walk as quickly as possible while overcoming randomized friction, drag coefficients, and external perturbation forces (Table I).We do not randomize the mass and buoyancy parameters because small changes can cause the robot to sink or float away.We train policies for a total of 32M time steps with T Bootstrap = 6M time steps and T Tune = 2M time steps.Our reward function consists of two terms that incentivize forward velocity and penalize lateral velocity respectively: r t = w vel v xt − w penalty v y t .We set w vel = 1.0 and w penalty = 0.1 in our experiments.Quadruped Pushing.In this environment, a quadruped robot, Unitree A1, is tasked with moving a cylindrical puck to a given target location.The environment is simulated using NVIDIA IsaacGym [36], a massively parallelizable GPUbased physics simulator.The environment is set up similar to the "Push object" task in [37], with a few key differences: the target location is fixed and the surface friction of the puck is randomized to be in the range [0.5, 1.25].We choose the true underlying surface friction to be modeled as a truncated normal distribution centered at 0.75 with a standard deviation of 0.01.We use a reward function and training strategy similar to prior work [37].We define the reward function as a weighted sum of a set of eight terms that control different aspects of the robot motion.We denote the velocity as v, the angular velocity as ω, the joint angles as q, joint velocities as q, joint torques as τ , number of robot parts (excluding feet) in contact with the environment as n contact , action taken at a given step as a t , the position of the target relative to the object being pushed as x t2o , and the simulation time-step as ∆t.The reward at time t is defined as the weighted sum of the following quantities:</p>
<p>1) Linear velocity tracking:
exp(−(v target − v) 2 /σ 1 )
2) Angular velocity tracking:
exp(−(ω z target − ω z ) 2 /σ 2 )
3) Joint acceleration penalty:
( qt− qt−1 ∆t )2 4
) Collision penalty: n contact 5) Action change penalty : (a t − a t−1 ) 2 6) Torque penalty :
τ 2 7) Object-target distance: exp(− ∥x t2o ∥ /σ 3 )
The final reward function is computed by scaling the above terms with weights [0.01, 0.01, −2.5 × 10 −7 , −1.0, −0.01, −1.0 × 10 −4 , 4.0] respectively and then adding them together.</p>
<p>We train all our policies as constrained residual perturbations to a pre-learned walking skill.We simulate 4096 robots in parallel, each initialized with its own puck friction, and train all our policies for a total of 30k iterations.The baseline is trained by uniformly sampling the range in Table I.The oracle is trained on true parameter distribution directly.Our BO approach models the true parameter as a truncated normal distribution.We train policies for a total of 2.8B time steps with T Bootstrap = 280M and T Tune = 280M.</p>
<p>B. Baseline Algorithms</p>
<p>We employ several baselines for evaluation.</p>
<p>• Domain Randomization (Vanilla DR): This method trains the agent using a large uniform DR range without any adaptation mechanism [4].• Bayesian DR: This method adaptively updates the randomization range using Bayesian Optimization, as proposed by Muratore et al. [6].Each optimization step is trained for 1M timesteps.• Oracle: For robotic tasks, we also compare our work with an agent trained with the ground-truth dynamics parameters.We intend to use this baseline as an indicator of the upper limits of performance.For the first three OpenAI Gym experiments, we compare our method with all four tuning strategies against the first two baselines, Vanilla DR and Bayesian DR.We use the Best Only tuning strategy for the BALLU walking task, whereas for the quadruped pushing task, we selected Infinite Chain.We compare these robotic tasks against the Vanilla DR and Oracle baselines.</p>
<p>C. Evaluation</p>
<p>We summarize the performance of agents in the environment with ground-truth dynamics in Figures 2 and 3. Note that the agents do not have access to the groundtruth parameters during training, just like the sim-to-real gap.Instead of standard learning curves, we illustrate the maximum historical episodic reward over time.This aims to consider real-world deployment scenarios where engineers deploy policies regularly and select the best-performing one.Therefore, curves are monotonically increasing over time by definition.We also aggregate multiple experiments by taking their median values in case of benchmark environments to account for outliers and mean for the robotic tasks.OpenAI Gym Benchmark.Overall, three of our strategies (Infinite Chain, Best Only, and Best of Last 5) outperformed both baselines, vanilla DR and Bayesian DR, by significant margins on all three tasks.The Normalized Closest Only strategy only worked well in Hopper-v3.Vanilla DR demonstrated reasonable results in all three scenarios, but it often exhibited saturated learning curves due to the lack of adaptation mechanisms.In all experiments, Bayesian DR showed poor performance.However, this is not to say Bayesian DR does not work.The reason it underperforms is that in each optimization step, the policy is only trained for 1M timesteps and it is not enough to fully saturate the training, which requires about 5M-10M timesteps.On the other hand, all other methods yield the final policy within a total of 10M or 20M timesteps.</p>
<p>In our experience, Infinite Chain demonstrated the best performance overall: it achieved the highest rewards in Hopper and second-best in the other two environments.In Ant-v3, it was also almost comparable to the top-performing strategy, Best of Last 5.The outstanding performance of Infinite Chain can be because it trains the same policy for a longer period than the other strategies.In that sense, Infinite Chain is somewhat similar to Vanilla DR, except for its adaptation mechanism that greatly boosted its performance.Both Best Only and Best of Last 5 worked reasonably, while the best strategy varied according to the tasks.</p>
<p>Interestingly, Normalized Closest Only does not work very well, except for the Hopper environment.This may highlight the multi-modality of control problems: the shorter distance in the randomization parameter space does not necessarily indicate the similarity in the policy parameter space.As a result, we observed that Normalized Closest Only often exhibited premature solutions in many checkpoints.Robotic Tasks Benchmark.For the two robotic tasks, we compared our BayRnTune with the Best Only and Infinite Chain tuning strategies against Vanilla DR and Oracle, as discussed in Section V-B.For both tasks, our method demonstrated the best performance, comparable to the Oracle trained with ground-truth environment parameters.For the BALLU Walking task, our learned policy achieved a reward of 56.09 compared to 14.11 of Vanilla DR, walking 4.19 m more within 20 seconds.For the Quadrupedal Pushing task, our agents were able to push the puck 40 cm closer than the baseline, Vanilla DR, which results in rewards of 4.33k and 3.35k respectively.The policy trained with our method pushes the puck to the target location almost perfectly, with an average error of 1 cm.These evaluations indicate that the effectiveness of the proposed BayRnTune is not only limited to simple benchmark environments but also applicable to a wider range of robotic applications.The motions are illustrated in Fig. 4 and the supplemental video.</p>
<p>D. Convergence</p>
<p>Bayesian optimization-based strategies, including the prior work of Bayesian DR and ours, are designed to find the best-performing simulation parameters by maximizing the given black-box objective function.These algorithms are loosely based on the intuition that learning with ground-truth parameters likely results in the maximum episodic rewards.However, it may not be true when the parameters and the performance have non-linear relationships.</p>
<p>Figure 5 illustrates both cases by plotting the performance over the parameters.The ground-truth parameter is denoted by a vertical line.The sizes of the circles are the associated standard deviations, and the colors represent the iteration numbers.In the Hopper and Quadrupedal pushing environments, we observe clear trends that the deviation from the ground-truth parameters leads to worse performance.For instance, the identified friction of 0.85 is very similar to the ground-truth friction of 0.75 in the quadruped pushing environment.On the other hand, in the BALLU walking environment, the BO found another peak performance at the friction of 1.7 and converged to that parameter.We suspect that its jumping-like policies are easier to learn in highfriction surfaces, which are happen to be robust in a wide range of frictions.</p>
<p>VI. CONCLUSION</p>
<p>This work presents Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune).Based on Bayesian DR, our algorithm is designed to remove the overhead of repeatedly training policies from scratch and improve sample efficiency by adopting fine-tuning strategies.To determine the starting policy for fine-tuning, we design four strategies, Normalized Closest Only, Infinite Chain, Best Only, and Best of Last N, which are based on different philosophies.We evaluated our method on five different tasks, including three OpenAI Gym benchmarks and two robotic tasks on the BALLU robot and the quadrupedal robot.Our BayRnTune significantly outperformed baselines, with the infinite chain strategy performing the best on average.We further discussed the convergence of the estimated DR parameters in the analysis.</p>
<p>There exist several interesting future research directions.As discussed in the results section, fine-tuning strategies perform differently in different environments.We believe this is due to the nature of the problem, including the multimodality of the solution.More rigorous analysis of the effect of reward landscape is another interesting future research direction.To further verify our method's efficacy, we aim to repeat these experiments on BALLU and A1 hardware in the future.</p>
<p>Algorithm 1
1
BayRnTune Algorithm Input: Total number of optimization steps N ; DR parameter range Φ = [ϕ min , ϕ max ]; A parameterized policy π(•); Model training algorithm PolOpt(θ; ϕ; T ), which trains the model parameterized by θ in the environment parameterized by ϕ for T timestemps; Tuning strategy TuneStrat, see IV-B for more details; Oracle for evaluating a model in the real world RealEval.Output: A learned control policy π(θ).</p>
<p>1 :
1
Initialize an optimizer BayOpt on space Φ 2: θ 0 ← PolOpt(θ rand ; ϕ 0 ; T Bootstrap ) ▷ Initial training 3: h = [] ▷ Initialize the history 4: for i ← 1 to N do 5: ϕ i ← BayOpt.query()▷ The next DR params 6:</p>
<p>Fig. 2 .
2
Fig. 2. Maximum episodic rewards evaluated in the ground-truth environment.The agents do not have access to the ground-truth parameters.The experiments are repeated with 6 (Ant-v3) or 10 (Hopper-v3 and HalfCheetah-v3) different random seeds and take median values for aggregation.</p>
<p>Fig. 3 .
3
Fig. 3. Robot experiments results.Our BayRnTune (green) outperformed Vanilla DR (blue) and was comparable to Oracle (orange).</p>
<p>Fig. 4 .
4
Fig. 4. Comparison of Vanilla DR (left) and our BayRnTune (right) on two robotic tasks, BALLU Walking (top) and Quadruped Pushing (bottom).</p>
<p>Fig. 5 .
5
Fig. 5. Performance over parameters.While ground-truth parameters lead to the best performance in Hopper (first row) and Quadrupedal pushing (third row) environments, the BO can find another peak at the 1.7 friction value due to the multi-modality of the problem as in the BALLU walking environment (second row).</p>
<p>ACKNOWLEDGEMENTThis work is supported by the National Science Foundation under Award #2024768.
Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. Springer1995</p>
<p>Nonlinear system identification using coevolution of models and tests. J C Bongard, H Lipson, IEEE Transactions on Evolutionary Computation. 942005</p>
<p>System identification without lennart ljung: what would have been different?. M Gevers, Studentlitteratur AB. 22006Norrtalje</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Dynamics randomization revisited: A case study for quadrupedal locomotion. Z Xie, X Da, M Van De Panne, B Babich, A Garg, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Data-efficient domain randomization with bayesian optimization. F Muratore, C Eilers, M Gienger, J Peters, IEEE Robotics and Automation Letters. 622021</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, arXiv:1702.024532017arXiv preprint</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.103322018arXiv preprint</p>
<p>Sim-to-real transfer for biped locomotion. W Yu, V C Kumar, G Turk, C K Liu, 2019 ieee/rsj international conference on intelligent robots and systems (iros). IEEE2019</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 42658722019</p>
<p>Tunenet: Oneshot residual tuning for system identification and sim-to-real robot task transfer. A Allevato, E S Short, M Pryor, A Thomaz, Conference on Robot Learning. PMLR2020</p>
<p>Autotuned sim-to-real transfer. Y Du, O Watkins, T Darrell, P Abbeel, D Pathak, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Reducing hardware experiments for model learning and policy optimization. S Ha, K Yamane, 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE2015</p>
<p>Data efficient reinforcement learning for legged robots. Y Yang, K Caluwaerts, A Iscen, T Zhang, J Tan, V Sindhwani, Conference on Robot Learning. PMLR2020</p>
<p>Sim-toreal transfer with neural-augmented robot simulation. F Golemo, A A Taiga, A Courville, P.-Y Oudeyer, Conference on Robot Learning. PMLR2018</p>
<p>Tossingbot: Learning to throw arbitrary objects with residual physics. A Zeng, S Song, J Lee, A Rodriguez, T Funkhouser, IEEE Transactions on Robotics. 3642020</p>
<p>Neural-fly enables rapid learning for agile flight in strong winds. M O'connell, G Shi, X Shi, K Azizzadenesheli, A Anandkumar, Y Yue, S.-J Chung, Science Robotics. 76665972022</p>
<p>Residual physics learning and system identification for sim-to-real transfer of policies on buoyancy assisted legged robots. N Sontakke, H Chae, S Lee, T Huang, D W Hong, S Ha, arXiv:2303.095972023arXiv preprint</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, arXiv:2004.007842020arXiv preprint</p>
<p>Blind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, arXiv:2105.083282021arXiv preprint</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, arXiv:2107.040342021arXiv preprint</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Learning fast adaptation with meta strategy optimization. W Yu, J Tan, Y Bai, E Coumans, S Ha, IEEE Robotics and Automation Letters. 522020</p>
<p>Legged robots that keep on learning: Fine-tuning locomotion policies in the real world. L Smith, J C Kew, X B Peng, S Ha, J Tan, S Levine, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Learning to walk via deep reinforcement learning. T Haarnoja, S Ha, A Zhou, J Tan, G Tucker, S Levine, arXiv:1812.111032018arXiv preprint</p>
<p>Learning to walk in the real world with minimal human effort. S Ha, P Xu, Z Tan, S Levine, J Tan, arXiv:2002.085502020arXiv preprint</p>
<p>Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.071132019arXiv preprint</p>
<p>Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. F Ramos, R C Possas, D Fox, arXiv:1906.017282019arXiv preprint</p>
<p>Online bayessim for combined simulator parameter inference and policy improvement. R Possas, L Barcelos, R Oliveira, D Fox, F Ramos, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>Neural posterior domain randomization. F Muratore, T Gruner, F Wiese, B Belousov, M Gienger, J Peters, Conference on Robot Learning. PMLR2022</p>
<p>Active domain randomization. B Mehta, M Diaz, F Golemo, C J Pal, L Paull, Conference on Robot Learning. PMLR2020</p>
<p>Openai gym. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, 2016</p>
<p>Ballu2: A safe and affordable buoyancy assisted biped. H Chae, M S Ahn, D Noh, H Nam, D Hong, Frontiers in Robotics and AI. 2902021</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Cascaded compositional residual learning for complex interactive behaviors. K N Kumar, I Essa, S Ha, IEEE Robotics and Automation Letters. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
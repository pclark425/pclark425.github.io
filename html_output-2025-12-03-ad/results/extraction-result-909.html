<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-909 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-909</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-909</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-f4cf4246f3882aa6337e9c05d5675a3b8463a32e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f4cf4246f3882aa6337e9c05d5675a3b8463a32e" target="_blank">ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> It is shown that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.</p>
                <p><strong>Paper Abstract:</strong> We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like “Rinse off a mug and place it in the coffee maker.” and low-level language instructions like “Walk to the coffee maker on the right.” ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision- and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e909.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e909.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEQ2SEQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CNN-LSTM Sequence-to-Sequence Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised imitation-learning agent that encodes egocentric RGB observations with a frozen ResNet-18 CNN, encodes concatenated high-level goal and low-level instructions with a bi-directional LSTM with attention, and decodes actions with an LSTM that predicts discrete actions and a pixelwise interaction mask via a small deconvolution network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SEQ2SEQ (CNN-ResNet18 + BiLSTM encoder + LSTM decoder with attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ResNet-18 conv features (frozen) -> two 1x1 convs + FC to produce visual feature; Bi-LSTM language encoder on [goal <SEP> instructions]; attention over language; LSTM decoder receives [v_t; attended_lang; prev_action] and outputs logits for 13 discrete actions and a 1xHxW binary interaction mask (via deconv). Trained end-to-end via imitation learning on expert trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED benchmark (language-conditioned embodied task execution)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation + sequential decision-making + object interaction (multi-step procedural tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Validation Seen: Task Success 2.4% (path-weighted 1.1%); Goal-Condition Success 9.4% (path-weighted 5.7%). Validation Unseen: Task Success 0.1% (0.0% PW); Goal-Condition 6.8% (4.7%). Test Seen: Task 2.1% (1.0% PW); Goal-Condition 7.4% (4.7%). Overall reported task success is <5% and goal-condition success ~8% on average.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>visual CNN encoder (frozen), Bi-LSTM language encoder, attention mechanism, LSTM decoder, deconv mask predictor</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised imitation learning (behavioral cloning from expert PDDL-generated trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Long-horizon tasks, partial observability, large action/state space, irreversible state changes, need for pixelwise object grounding and multi-step planning; simple seq2seq lacks hierarchy, object/state trackers, and explicit planning needed for these interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e909.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEQ2SEQ+PM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SEQ2SEQ with Progress Monitoring (auxiliary losses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The SEQ2SEQ baseline augmented with two auxiliary progress-monitoring modules that predict normalized progress through the expert trajectory and the number of subgoals completed, trained with L2 losses to provide temporal/goal-aware signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SEQ2SEQ+PM (Progress Monitoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same core architecture as SEQ2SEQ plus two supervised auxiliary heads conditioned on decoder state and inputs: (1) p_t predicting normalized timestep t/T with sigmoid (L2 loss) and (2) c_t predicting fraction of subgoals completed (L2 loss); auxiliary losses scaled by 0.1 during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED benchmark (language-conditioned embodied task execution)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation + sequential decision-making + multi-step procedural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Validation Seen: Task Success 3.7% (path-weighted 2.1%); Goal-Condition Success 10.0% (path-weighted 7.0%). Validation Unseen: Task 0.0%; Goal-Condition 6.9% (5.1% PW). Test Seen: Task 4.0% (2.0% PW); Goal-Condition 9.4% (6.3% PW). Effect is small but consistent improvement over SEQ2SEQ (e.g., Task Success ↑ from 2.4% to 3.7% on Val Seen; Goal-Cond ↑ from 9.4% to 10.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>adds auxiliary progress prediction heads (progress scalar and subgoal count) to decoder; retains original CNN-LSTM/attention pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised imitation learning + auxiliary supervised L2 losses for progress/subgoal prediction</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (auxiliary objectives) / training augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Two auxiliary predictors trained jointly with action and mask prediction: (1) progress head p_t predicting normalized timestep t/T (sigmoid + L2), and (2) subgoal head c_t predicting fraction of subgoals completed (sigmoid + L2). These are intended to provide temporal and coarse-language-alignment signals to the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Marginal improvements: example Val Seen Task Success 2.4% -> 3.7%, Goal-Condition Success 9.4% -> 10.0%; improved path-weighted scores and slightly more efficient behavior but did not close the large gap to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>While auxiliary progress signals help avoid repetition and improve stop prediction, the model still lacks hierarchical planning, explicit object/state tracking, and modular components required for long-horizon, compositional interactive tasks, limiting gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e909.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No Language ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-only ablation (No Language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of the SEQ2SEQ model where language input is removed to study dataset/model biases and the contribution of visual input alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>No Language (vision-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture but with no language encoder/attention (model conditions only on visual features and previous actions). Trained with imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation + interaction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Validation Seen: Task Success 0.0% (0.0% PW); Goal-Condition Success 5.9% (3.4% PW). Test Seen: Task 0.2% (0.0% PW); Goal-Condition 5.0% (3.2% PW).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>vision-only input; retains action decoder and mask predictor</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>ablation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Remove textual input to measure whether vision alone can drive task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Model completes a small fraction of goal-conditions by exploiting frequent object interactions from training (Goal-Cond ~5-6%) but cannot perform full tasks; demonstrates that language is necessary for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of language prevents following instructions and coordinating multi-step plans; vision-only biases can complete some repeated/simple interactions but not long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e909.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No Vision ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-only ablation (No Vision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of the SEQ2SEQ model where visual input is removed to study how much language alone can drive action policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>No Vision (language-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture but with no visual encoder; model conditions on encoded language and previous actions; trained with imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation + interaction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Validation Seen: Task Success 0.0% (0.0% PW); Goal-Condition Success 5.7% (4.7% PW). Test Seen: Task 0.0% (0.0% PW); Goal-Condition 3.9% (3.2% PW).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>language-only input; retains decoder and mask prediction (but masks degenerate w/out vision)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>ablation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Remove visual input to measure reliance on language and dataset biases.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Language-only model completes a small subset of goal-conditions by following low-level navigation instructions and memorizing common interaction masks (Goal-Cond ~4-6%), but fails at full tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Without visual grounding, the model cannot localize or correctly disambiguate object instances and scene states; limited to following rote instruction sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e909.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random agent baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Random Action + Random Mask Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that chooses actions uniformly at random and samples interaction masks uniformly, used to show task difficulty by chance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Random agent (uniform actions + uniform masks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Picks one of 13 discrete actions uniformly every timestep and generates a uniformly random interaction mask for interaction actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation + sequential decision-making + object interaction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>0% Task and Goal-Condition Success across all folds (random agent achieves no success).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Huge branching factor and long horizons (avg ~50 steps, 12 actions -> extremely low probability of random success) plus pixelwise mask requirement make chance performance effectively zero.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e909.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human keyboard-and-mouse operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human participants performing ALFRED tasks via the same simulated action interface used by agents, providing an upper-bound on achievable performance given the action constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Human operator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Human subjects allowed to practice in AI2-THOR then execute tasks via keyboard-and-mouse action interface identical to agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>embodied navigation + multi-step interaction</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Human Task Success 91.0% (Path-weighted 86.0%) on a sampled unseen-test subset; much higher than model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Humans can plan, remember object states, and adapt interactively; models lack these capabilities, explaining the large human-model gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e909.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comparison to V&L Nav / EQA results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance contrast with vision-and-language navigation and embodied QA prior work</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that SEQ2SEQ and progress-monitoring architectures perform well in prior vision-and-language navigation and embodied QA tasks, but the same architectures fail (much lower success) on ALFRED's long-horizon, interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-monitoring navigation agent via auxiliary progress estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SEQ2SEQ + Progress Monitoring (in prior V&L navigation literature)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work (e.g., Ma et al.) applied sequence models with auxiliary progress estimation successfully to navigation-only tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>progress monitor auxiliary head (prior work), similar LSTM/attention pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper highlights that methods effective for navigation/embodied QA (shorter-horizon tasks) do not transfer with the same effectiveness to longer-horizon interactive tasks like ALFRED.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>ALFRED adds object interactions, irreversible state changes, long horizons, richer action and state spaces, and pixelwise mask requirements, which amplify challenges beyond navigation/QA settings where prior models had success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e909.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e909.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Suggested interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proposed directions: hierarchy, modularity, structured planning, pretrained perception</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors propose that hierarchical and modular architectures, explicit object segmentation/perception modules, and structured planning/reasoning could help close the gap on long-horizon interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model; suggested classes of interventions include hierarchical RL/planning, modular neural controllers, pretrained object-segmentation modules, and symbolic/planning hybrids.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>ALFRED (target domain)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / modular control / tool use / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>hierarchy, modular controllers, pretrained object segmentation, explicit state tracking, structured symbolic planners</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>suggested: combinations of supervised, modular pretraining, and planning-aware training; not evaluated in paper</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change / hybrid approach / pretrained perception</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Authors recommend exploiting hierarchy (temporal abstraction), modularity (separate perceptual and control modules or neural modules), pretrained object segmentation for interaction masks, and structured planning or symbolic components to track object state and achieve long-horizon goals.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of modular/hierarchical planning and robust perception in end-to-end seq2seq agents makes it difficult to handle ALFRED's compositional and long-horizon interactive demands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-monitoring navigation agent via auxiliary progress estimation. <em>(Rating: 2)</em></li>
                <li>Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. <em>(Rating: 2)</em></li>
                <li>Embodied Question Answering. <em>(Rating: 2)</em></li>
                <li>Neural Modular Control for Embodied Question Answering. <em>(Rating: 1)</em></li>
                <li>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-909",
    "paper_id": "paper-f4cf4246f3882aa6337e9c05d5675a3b8463a32e",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "SEQ2SEQ",
            "name_full": "CNN-LSTM Sequence-to-Sequence Baseline",
            "brief_description": "A supervised imitation-learning agent that encodes egocentric RGB observations with a frozen ResNet-18 CNN, encodes concatenated high-level goal and low-level instructions with a bi-directional LSTM with attention, and decodes actions with an LSTM that predicts discrete actions and a pixelwise interaction mask via a small deconvolution network.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "SEQ2SEQ (CNN-ResNet18 + BiLSTM encoder + LSTM decoder with attention)",
            "model_description": "ResNet-18 conv features (frozen) -&gt; two 1x1 convs + FC to produce visual feature; Bi-LSTM language encoder on [goal &lt;SEP&gt; instructions]; attention over language; LSTM decoder receives [v_t; attended_lang; prev_action] and outputs logits for 13 discrete actions and a 1xHxW binary interaction mask (via deconv). Trained end-to-end via imitation learning on expert trajectories.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED benchmark (language-conditioned embodied task execution)",
            "interactive_task_type": "embodied navigation + sequential decision-making + object interaction (multi-step procedural tasks)",
            "interactive_performance": "Validation Seen: Task Success 2.4% (path-weighted 1.1%); Goal-Condition Success 9.4% (path-weighted 5.7%). Validation Unseen: Task Success 0.1% (0.0% PW); Goal-Condition 6.8% (4.7%). Test Seen: Task 2.1% (1.0% PW); Goal-Condition 7.4% (4.7%). Overall reported task success is &lt;5% and goal-condition success ~8% on average.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "visual CNN encoder (frozen), Bi-LSTM language encoder, attention mechanism, LSTM decoder, deconv mask predictor",
            "training_method": "supervised imitation learning (behavioral cloning from expert PDDL-generated trajectories)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Long-horizon tasks, partial observability, large action/state space, irreversible state changes, need for pixelwise object grounding and multi-step planning; simple seq2seq lacks hierarchy, object/state trackers, and explicit planning needed for these interactive tasks.",
            "uuid": "e909.0",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "SEQ2SEQ+PM",
            "name_full": "SEQ2SEQ with Progress Monitoring (auxiliary losses)",
            "brief_description": "The SEQ2SEQ baseline augmented with two auxiliary progress-monitoring modules that predict normalized progress through the expert trajectory and the number of subgoals completed, trained with L2 losses to provide temporal/goal-aware signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "SEQ2SEQ+PM (Progress Monitoring)",
            "model_description": "Same core architecture as SEQ2SEQ plus two supervised auxiliary heads conditioned on decoder state and inputs: (1) p_t predicting normalized timestep t/T with sigmoid (L2 loss) and (2) c_t predicting fraction of subgoals completed (L2 loss); auxiliary losses scaled by 0.1 during training.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED benchmark (language-conditioned embodied task execution)",
            "interactive_task_type": "embodied navigation + sequential decision-making + multi-step procedural tasks",
            "interactive_performance": "Validation Seen: Task Success 3.7% (path-weighted 2.1%); Goal-Condition Success 10.0% (path-weighted 7.0%). Validation Unseen: Task 0.0%; Goal-Condition 6.9% (5.1% PW). Test Seen: Task 4.0% (2.0% PW); Goal-Condition 9.4% (6.3% PW). Effect is small but consistent improvement over SEQ2SEQ (e.g., Task Success ↑ from 2.4% to 3.7% on Val Seen; Goal-Cond ↑ from 9.4% to 10.0%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "adds auxiliary progress prediction heads (progress scalar and subgoal count) to decoder; retains original CNN-LSTM/attention pipeline",
            "training_method": "supervised imitation learning + auxiliary supervised L2 losses for progress/subgoal prediction",
            "intervention_type": "architectural change (auxiliary objectives) / training augmentation",
            "intervention_description": "Two auxiliary predictors trained jointly with action and mask prediction: (1) progress head p_t predicting normalized timestep t/T (sigmoid + L2), and (2) subgoal head c_t predicting fraction of subgoals completed (sigmoid + L2). These are intended to provide temporal and coarse-language-alignment signals to the decoder.",
            "intervention_effect": "Marginal improvements: example Val Seen Task Success 2.4% -&gt; 3.7%, Goal-Condition Success 9.4% -&gt; 10.0%; improved path-weighted scores and slightly more efficient behavior but did not close the large gap to human performance.",
            "hypothesized_cause_of_gap": "While auxiliary progress signals help avoid repetition and improve stop prediction, the model still lacks hierarchical planning, explicit object/state tracking, and modular components required for long-horizon, compositional interactive tasks, limiting gains.",
            "uuid": "e909.1",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "No Language ablation",
            "name_full": "Vision-only ablation (No Language)",
            "brief_description": "An ablation of the SEQ2SEQ model where language input is removed to study dataset/model biases and the contribution of visual input alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "No Language (vision-only)",
            "model_description": "Same architecture but with no language encoder/attention (model conditions only on visual features and previous actions). Trained with imitation learning.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED benchmark",
            "interactive_task_type": "embodied navigation + interaction",
            "interactive_performance": "Validation Seen: Task Success 0.0% (0.0% PW); Goal-Condition Success 5.9% (3.4% PW). Test Seen: Task 0.2% (0.0% PW); Goal-Condition 5.0% (3.2% PW).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "vision-only input; retains action decoder and mask predictor",
            "training_method": "supervised imitation learning",
            "intervention_type": "ablation",
            "intervention_description": "Remove textual input to measure whether vision alone can drive task completion.",
            "intervention_effect": "Model completes a small fraction of goal-conditions by exploiting frequent object interactions from training (Goal-Cond ~5-6%) but cannot perform full tasks; demonstrates that language is necessary for many tasks.",
            "hypothesized_cause_of_gap": "Lack of language prevents following instructions and coordinating multi-step plans; vision-only biases can complete some repeated/simple interactions but not long-horizon tasks.",
            "uuid": "e909.2",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "No Vision ablation",
            "name_full": "Language-only ablation (No Vision)",
            "brief_description": "An ablation of the SEQ2SEQ model where visual input is removed to study how much language alone can drive action policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "No Vision (language-only)",
            "model_description": "Same architecture but with no visual encoder; model conditions on encoded language and previous actions; trained with imitation learning.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED benchmark",
            "interactive_task_type": "embodied navigation + interaction",
            "interactive_performance": "Validation Seen: Task Success 0.0% (0.0% PW); Goal-Condition Success 5.7% (4.7% PW). Test Seen: Task 0.0% (0.0% PW); Goal-Condition 3.9% (3.2% PW).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "language-only input; retains decoder and mask prediction (but masks degenerate w/out vision)",
            "training_method": "supervised imitation learning",
            "intervention_type": "ablation",
            "intervention_description": "Remove visual input to measure reliance on language and dataset biases.",
            "intervention_effect": "Language-only model completes a small subset of goal-conditions by following low-level navigation instructions and memorizing common interaction masks (Goal-Cond ~4-6%), but fails at full tasks.",
            "hypothesized_cause_of_gap": "Without visual grounding, the model cannot localize or correctly disambiguate object instances and scene states; limited to following rote instruction sequences.",
            "uuid": "e909.3",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Random agent baseline",
            "name_full": "Uniform Random Action + Random Mask Agent",
            "brief_description": "A baseline that chooses actions uniformly at random and samples interaction masks uniformly, used to show task difficulty by chance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Random agent (uniform actions + uniform masks)",
            "model_description": "Picks one of 13 discrete actions uniformly every timestep and generates a uniformly random interaction mask for interaction actions.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED benchmark",
            "interactive_task_type": "embodied navigation + sequential decision-making + object interaction",
            "interactive_performance": "0% Task and Goal-Condition Success across all folds (random agent achieves no success).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Huge branching factor and long horizons (avg ~50 steps, 12 actions -&gt; extremely low probability of random success) plus pixelwise mask requirement make chance performance effectively zero.",
            "uuid": "e909.4",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Human baseline",
            "name_full": "Human keyboard-and-mouse operator",
            "brief_description": "Human participants performing ALFRED tasks via the same simulated action interface used by agents, providing an upper-bound on achievable performance given the action constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Human operator",
            "model_description": "Human subjects allowed to practice in AI2-THOR then execute tasks via keyboard-and-mouse action interface identical to agents.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED benchmark",
            "interactive_task_type": "embodied navigation + multi-step interaction",
            "interactive_performance": "Human Task Success 91.0% (Path-weighted 86.0%) on a sampled unseen-test subset; much higher than model performance.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Humans can plan, remember object states, and adapt interactively; models lack these capabilities, explaining the large human-model gap.",
            "uuid": "e909.5",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Comparison to V&L Nav / EQA results",
            "name_full": "Performance contrast with vision-and-language navigation and embodied QA prior work",
            "brief_description": "The paper notes that SEQ2SEQ and progress-monitoring architectures perform well in prior vision-and-language navigation and embodied QA tasks, but the same architectures fail (much lower success) on ALFRED's long-horizon, interactive tasks.",
            "citation_title": "Self-monitoring navigation agent via auxiliary progress estimation.",
            "mention_or_use": "mention",
            "model_or_agent_name": "SEQ2SEQ + Progress Monitoring (in prior V&L navigation literature)",
            "model_description": "Prior work (e.g., Ma et al.) applied sequence models with auxiliary progress estimation successfully to navigation-only tasks.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "progress monitor auxiliary head (prior work), similar LSTM/attention pipelines",
            "training_method": null,
            "intervention_type": null,
            "intervention_description": "Paper highlights that methods effective for navigation/embodied QA (shorter-horizon tasks) do not transfer with the same effectiveness to longer-horizon interactive tasks like ALFRED.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "ALFRED adds object interactions, irreversible state changes, long horizons, richer action and state spaces, and pixelwise mask requirements, which amplify challenges beyond navigation/QA settings where prior models had success.",
            "uuid": "e909.6",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Suggested interventions",
            "name_full": "Proposed directions: hierarchy, modularity, structured planning, pretrained perception",
            "brief_description": "The authors propose that hierarchical and modular architectures, explicit object segmentation/perception modules, and structured planning/reasoning could help close the gap on long-horizon interactive tasks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": "Not a single model; suggested classes of interventions include hierarchical RL/planning, modular neural controllers, pretrained object-segmentation modules, and symbolic/planning hybrids.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "ALFRED (target domain)",
            "interactive_task_type": "planning / modular control / tool use / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "hierarchy, modular controllers, pretrained object segmentation, explicit state tracking, structured symbolic planners",
            "training_method": "suggested: combinations of supervised, modular pretraining, and planning-aware training; not evaluated in paper",
            "intervention_type": "architectural change / hybrid approach / pretrained perception",
            "intervention_description": "Authors recommend exploiting hierarchy (temporal abstraction), modularity (separate perceptual and control modules or neural modules), pretrained object segmentation for interaction masks, and structured planning or symbolic components to track object state and achieve long-horizon goals.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Lack of modular/hierarchical planning and robust perception in end-to-end seq2seq agents makes it difficult to handle ALFRED's compositional and long-horizon interactive demands.",
            "uuid": "e909.7",
            "source_info": {
                "paper_title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
                "publication_date_yy_mm": "2019-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-monitoring navigation agent via auxiliary progress estimation.",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments.",
            "rating": 2
        },
        {
            "paper_title": "Embodied Question Answering.",
            "rating": 2
        },
        {
            "paper_title": "Neural Modular Control for Embodied Question Answering.",
            "rating": 1
        },
        {
            "paper_title": "Tactical rewind: Self-correction via backtracking in vision-and-language navigation.",
            "rating": 1
        }
    ],
    "cost": 0.014960250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ALFRED 4 <br> A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</h1>
<p>Mohit Shridhar ${ }^{1}$<br>Jesse Thomason ${ }^{1}$ Daniel Gordon ${ }^{1}$<br>Yonatan Bisk ${ }^{1,2,3}$<br>Winson Han ${ }^{3}$ Roozbeh Mottaghi ${ }^{1,3}$ Luke Zettlemoyer ${ }^{1}$ Dieter Fox ${ }^{1,4}$<br>AskForALFRED . com</p>
<h4>Abstract</h4>
<p>We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with nonreversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like "Rinse off a mug and place it in the coffee maker." and low-level language instructions like "Walk to the coffee maker on the right." ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.</p>
<h2>1. Introduction</h2>
<p>A robot operating in human spaces must learn to connect natural language to the world. This symbol grounding [21] problem has largely focused on connecting language to static images. However, robots need to understand task-oriented language, for example "Rinse off a mug and place it in the coffee maker," as illustrated in Figure 1.</p>
<p>Platforms for translating language to action have become increasingly popular, spawning new test-beds [3, 12, 14, 42]. These benchmarks include language-driven navigation and embodied question answering, which have seen dramatic improvements in modeling thanks to environments like Matterport 3D [3, 11], AI2-THOR [26], and AI Habitat [45]. However, these datasets ignore complexities arising from describing task-oriented behaviors with objects.</p>
<p>We introduce ALFRED, a new benchmark for con-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ALFRED consists of 25k language directives corresponding to expert demonstrations of household tasks. We highlight several frames corresponding to portions of the accompanying language instruction. ALFRED involves interactions with objects, keeping track of state changes, and references to previous instructions.
necting human language to actions, behaviors, and objects in interactive visual environments. Planner-based expert demonstrations are accompanied by both high- and lowlevel human language instructions in 120 indoor scenes in AI2-THOR 2.0 [26]. These demonstrations involve partial observability, long action horizons, underspecified natural language, and irreversible actions.</p>
<p>ALFRED includes 25,743 English language directives describing 8,055 expert demonstrations averaging 50 steps each, resulting in 428,322 image-action pairs. Motivated by work in robotics on segmentation-based grasping [37], agents in ALFRED interact with objects visually, specifying a pixelwise interaction mask of the target object. This inference is more realistic than simple object class prediction, where localization is treated as a solved problem. Existing beam-search [17, 48, 53] and backtracking solutions [24, 29] are infeasible due to the larger action and state spaces, long horizon, and inability to undo certain actions.</p>
<table>
<thead>
<tr>
<th></th>
<th>— Language -</th>
<th></th>
<th>— Virtual Environment -</th>
<th></th>
<th></th>
<th>— Inference -</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td># Human</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Annotations</td>
<td>Granularity</td>
<td>Visual</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Quality</td>
<td>Movable</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Objects</td>
<td>State</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Changes</td>
<td>Vis. Obs.</td>
<td>Navigation</td>
<td>Interaction</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TACoS [43]</td>
<td>17k+</td>
<td>High\&amp;Low</td>
<td>Photos</td>
<td>$\times$</td>
<td>$\times$</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>R2R [3]; Touchdown [14]</td>
<td>21k+; 9.3k+</td>
<td>Low</td>
<td>Photos</td>
<td>$\times$</td>
<td>$\times$</td>
<td>Ego</td>
<td>Graph</td>
<td>$\times$</td>
</tr>
<tr>
<td>EQA [15]</td>
<td>$\times$</td>
<td>High</td>
<td>Low</td>
<td>$\times$</td>
<td>$\times$</td>
<td>Ego</td>
<td>Discrete</td>
<td>$\times$</td>
</tr>
<tr>
<td>Matterport EQA [55]</td>
<td>$\times$</td>
<td>High</td>
<td>Photos</td>
<td>$\times$</td>
<td>$\times$</td>
<td>Ego</td>
<td>Discrete</td>
<td>$\times$</td>
</tr>
<tr>
<td>IQA [20]</td>
<td>$\times$</td>
<td>High</td>
<td>High</td>
<td>$\times$</td>
<td>$\checkmark$</td>
<td>Ego</td>
<td>Discrete</td>
<td>Discrete</td>
</tr>
<tr>
<td>VirtualHome [42]</td>
<td>2.7k+</td>
<td>High\&amp;Low</td>
<td>High</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$3^{\text {rd }}$ Person</td>
<td>$\times$</td>
<td>Discrete</td>
</tr>
<tr>
<td>VSP [58]</td>
<td>$\times$</td>
<td>High</td>
<td>High</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>Ego</td>
<td>$\times$</td>
<td>Discrete</td>
</tr>
<tr>
<td>ALFRED</td>
<td>25k+</td>
<td>High\&amp;Low</td>
<td>High</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>Ego</td>
<td>Discrete</td>
<td>Discrete</td>
</tr>
<tr>
<td>+ Mask</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset comparison. ALFRED is the first interactive visual dataset to include high-level goal and low-level natural language instructions for object and environment interactions. TACoS [43] provides detailed high- and low-level text descriptions of cooking videos, but does not facilitate task execution. For navigation, ALFRED enables discretized, gridbased movement, while other datasets use topological graph navigation or avoid navigation altogether. ALFRED requires an agent to generate spatially located interaction masks for action commands. By contrast, other datasets only require choosing from a discrete set of available interactions and object classes or offer no interactive capability.</p>
<p>To establish baseline performance levels, we evaluate a sequence-to-sequence model akin to existing vision-andlanguage navigation tasks [28]. This model is not effective on the complex tasks in ALFRED, achieving less than 5\% success rates. For analysis, we also evaluate individual subgoals. While performance is better for isolated sub-goals, the model lacks the reasoning capacity for long-horizon and compositional task planning.</p>
<p>In summary, ALFRED facilitates learning models that translate from language to sequences of actions and interactions in a visually and physically realistic simulation environment. This benchmark captures many challenges present in real-world settings for translating human language to robot actions for accomplishing household tasks. Models that can overcome these challenges will begin to close the gap towards real-world, language-driven robotics.</p>
<h2>2. Related Work</h2>
<p>Table 1 summarizes the benefits of ALFRED relative to other visual action datasets with language annotations. Vision \&amp; Language Navigation. In vision-and-language navigation tasks, either natural or templated language describes a route to a goal location through egocentric visual observations [3, 12, 13, 14, 31]. Since the proposal of R2R [3], researchers have dramatically improved the navigation performance of models [17, 24, 29, 53, 54] with techniques like progress monitoring [28], as well as introduced task variants with additional, on-route instructions [38, 39, 51]. Much of this research is limited to static environments. By contrast, ALFRED tasks include navigation, object interactions, and state changes. Vision \&amp; Language Task Completion. There are sev- eral existing benchmarks based on simple block worlds and fully observable scenes [9, 34]. ALFRED provides more difficult tasks in richer, visually complex scenes, and uses partially observable environments. The CHAI benchmark [33] evaluates agents performing household instructions, but uses a generic "interact" action. ALFRED has seven manipulation actions, such as pick up, turn on, and open, state changes like clean versus dirty, and variation in language and visual complexity.</p>
<p>Previous work in the original AI2-THOR environment investigated the task of visual semantic planning [19, 58]. Artificial language came from templates, and environment interaction was handled with discrete class predictions, for example selecting apple as the target object from predefined options. ALFRED features human language instructions, and object selections are carried out with class-agnostic, pixelwise interaction masks. In VirtualHome [42], programs are generated from video demonstration and natural language instructions, but inference does not involve egocentric visual and action feedback or partial observability.</p>
<p>There is an extensive literature on language-based instruction following in the natural language processing community. There, research has focused on mapping instructions to actions [5, 13, 32, 36, 49], but these works do not involve visual, interactive environments. Embodied Question Answering. Existing datasets for visual question answering in embodied environments use templated language or static scenes [15, 20, 55, 57]. In ALFRED, rather than answering a question, the agent must complete a task specified using natural language, which requires both navigation and interaction with objects. Instruction Alignment. Language annotations of videos enable discovering visual correspondences between words</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Pick</th>
<th style="text-align: left;">Stack</th>
<th style="text-align: left;">Pick Two</th>
<th style="text-align: left;">Clean</th>
<th style="text-align: left;">Heat</th>
<th style="text-align: left;">Cool</th>
<th style="text-align: left;">Examine</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">&amp; Place</td>
<td style="text-align: left;">&amp; Place</td>
<td style="text-align: left;">&amp; Place</td>
<td style="text-align: left;">&amp; Place</td>
<td style="text-align: left;">&amp; Place</td>
<td style="text-align: left;">&amp; Place</td>
<td style="text-align: left;">in Light</td>
</tr>
<tr>
<td style="text-align: left;">item(s)</td>
<td style="text-align: left;">Book</td>
<td style="text-align: left;">Fork (in) Cup</td>
<td style="text-align: left;">Spray Bottle</td>
<td style="text-align: left;">Dish Sponge</td>
<td style="text-align: left;">Potato Slice</td>
<td style="text-align: left;">Egg</td>
<td style="text-align: left;">Credit Card</td>
</tr>
<tr>
<td style="text-align: left;">receptacle</td>
<td style="text-align: left;">Desk</td>
<td style="text-align: left;">Counter Top</td>
<td style="text-align: left;">Toilet Tank</td>
<td style="text-align: left;">Cart</td>
<td style="text-align: left;">Counter Top</td>
<td style="text-align: left;">Side Table</td>
<td style="text-align: left;">Desk Lamp</td>
</tr>
<tr>
<td style="text-align: left;">scene #</td>
<td style="text-align: left;">Bedroom 14</td>
<td style="text-align: left;">Kitchen 10</td>
<td style="text-align: left;">Bathroom 2</td>
<td style="text-align: left;">Bathroom 1</td>
<td style="text-align: left;">Kitchen 8</td>
<td style="text-align: left;">Kitchen 21</td>
<td style="text-align: left;">Bedroom 24</td>
</tr>
<tr>
<td style="text-align: left;">export</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">demonstration</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: ALFRED annotations. We introduce 7 different task types parameterized by 84 object classes in 120 scenes. An example of each task type is given above. For the Clean \&amp; Place demonstration, we also show the three crowdsourced language directives. Please see the supplemental material for example demonstrations and language for each task.
and concepts [1, 46, 43, 56, 59]. ALFRED requires performing tasks in an interactive setting as opposed to learning from recorded videos.
Robotics Instruction Following. Instruction following is a long-standing topic of interest in robotics [7, 10, 30, 35, 40, 41, 47, 52]. Lines of research consider different tasks such as cooking [10], table clearing [40], and mobile manipulation [30]. In general, they are limited to a few scenes [35], consider a small number of objects [30], or use the same environment for training and testing [7]. In contrast, ALFRED includes 120 scenes, many object classes with diverse appearances, and a test set of unseen environments.</p>
<h2>3. The ALFRED Dataset</h2>
<p>The ALFRED dataset comprises 25,743 language directives corresponding to 8,055 expert demonstration episodes. Each directive includes a high-level goal and a set of step-by-step instructions. Each expert demonstration can be deterministically replayed in the AI2-THOR 2.0 simulator.</p>
<h3>3.1. Expert Demonstrations</h3>
<p>Expert demonstrations are composed of an agent's egocentric visual observations of the environment and what action is taken at each timestep as well as ground-truth interaction masks. These demonstrations are generated by a planner [23] using metadata not available to the agent at inference time. Navigation actions move the agent or change its camera orientation, while manipulation actions include picking and placing objects, opening and closing cabinets and drawers, and turning appliances on and off. Interactions can involve multiple objects, such as using a knife to slice an apple, cleaning a mug in the sink, and heating a potato in the microwave. Manipulation actions are accompanied by a ground truth segmentation of the target object.</p>
<p>Figure 2 gives examples of the high-level agent tasks in ALFRED, like putting a cleaned object at a destination. These tasks are parameterized by the object of focus, the destination receptacle (e.g., table top), the scene in which to carry out the task, and in the case of Stack \&amp; Place, a base object (e.g., plate). ALFRED contains expert demonstrations of these seven tasks executed using combinations of 58 unique object classes and 26 receptacle object classes across 120 different indoor scenes. For object classes like potato slice, the agent must first pick up a knife and find a potato to create slices. All object classes contain multiple visual variations with different shapes, textures, and colors. For example, there are 30 unique variants of the $a p$ ple class. Indoor scenes include different room types: 30 each of kitchens, bathrooms, bedrooms, and living rooms.</p>
<p>For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps. The distributions of actions steps in ALFRED demonstrations versus related datasets is given in Figure 3. As an example, for task parameters ${$ task: Heat \&amp; Place, object: potato, destination: counter top, scene: KITCHEN-8$}$, we generate three different expert demonstrations by starting the agent and objects in randomly chosen locations. Object start positions have some commonsense, class-specific constraints, for example a fork can start inside a drawer, but an apple cannot.</p>
<p>Contrasting navigation-only datasets where expert demonstrations can come from an $A^{*}$ planner, our state space includes object positions and state changes. Thus, to generate expert demonstrations we encode the agent and object states, as well as high-level environment dynamics, into Planning Domain Definition Language (PDDL) rules [18]. We then define task-specific PDDL goal conditions, for example that a heated potato is resting on a table top. Note</p>
<table>
<thead>
<tr>
<th></th>
<th>Train</th>
<th>Validation</th>
<th></th>
<th>Test</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Seen</td>
<td>Unseen</td>
<td>Seen</td>
<td>Unseen</td>
</tr>
<tr>
<td># Annotations</td>
<td>21,023</td>
<td>820</td>
<td>821</td>
<td>1,533</td>
<td>1,529</td>
</tr>
<tr>
<td># Scenes</td>
<td>108</td>
<td>88</td>
<td>4</td>
<td>107</td>
<td>8</td>
</tr>
</tbody>
</table>
<p>Table 2: ALFRED Data Splits. All expert demonstrations and associated language directives in the validation and test folds are distinct from those in the train fold. The validation and test sets are split into seen and unseen folds. Scenes in the seen folds of validation and test data are subsets of those in the train fold. Scenes in the unseen validation and test folds are distinct from the train folds and from each other.
that the planner encodes the environment as fully observable and has perfect knowledge about world dynamics. For training and testing agent models, however, the environment is partially observable: it is only viewed through the agent's egocentric vision as actions are carried out.</p>
<p>We split these expert demonstrations into training, validation, and test folds (Table 2). Following work in vision-and-language navigation [3], we further split the validation and test into two conditions: seen and unseen environments. This split facilitates examining how well models generalize to entirely new spaces with novel object class variations.</p>
<h3>3.2. Language Directives</h3>
<p>For every expert demonstration, we collect open vocabulary, free-form language directives from at least three different annotators using Amazon Mechanical Turk (AMT), resulting in 25 k total language directives. Language directives include a high-level goal together with low-level instructions, as shown in Figures 1 and 2. The distribution of language annotation token lengths in ALFRED versus related datasets is given in Figure 3.</p>
<p>AMT workers are told to write instructions to tell a "smart robot" how to accomplish what is shown in a video. We create a video of each expert demonstration and segment it such that each segment corresponds to an instruction. We consult the PDDL plan for the expert demonstration to identify task sub-goals, for example the many low-level steps to navigate to a knife, or the several steps to heat a potato slice in the microwave once standing in front of it. We visually highlight action sequences related to sub-goals via colored timeline bars below the video. In each HIT (Human Intelligence Task), a worker watches the video, then writes low-level, step-by-step instructions for each highlighted sub-goal segment. The worker also writes a high-level goal that summarizes what the robot should accomplish during the expert demonstration.</p>
<p>These directives are validated through a second HIT by at least two annotators, with a possible third tie-breaker. For validation, we show a worker all three language directive
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Comparison to Existing Datasets. Expert demonstration steps and instruction tokens of ALFRED compared to other datasets with human language for action sequences: Touchdown (TD) [14], VirtualHome (VH) [42], and Room-to-Room (R2R) [3]. The total number of demonstrations or annotations is given with the dataset label.
annotations without the video. The worker selects whether the three directives describe the same actions, and if not, which is most different. If a directive is chosen as most different by a majority of validation workers, it is removed and the demonstration is subsequently re-annotated by another worker. Qualitatively, these rejected annotations contain incorrect object referents (e.g., "egg" instead of "potato") or directions (e.g., "go left towards..." instead of "right").</p>
<h2>4. Baseline Models</h2>
<p>An agent trained for ALFRED tasks needs to jointly reason over vision and language input and produce a sequence of low-level actions to interact with the environment.</p>
<h3>4.1. Sequence-to-Sequence Models</h3>
<p>We model the interactive agent with a CNN-LSTM sequence-to-sequence (SEQ2SEQ) architecture. A CNN enodes the visual input, a bidirectional-LSTM generates a representation of the language input, and a decoder LSTM infers a sequence of low-level actions while attending over the encoded language. See Figure 4 for an overview and the supplementary material for implementation details.</p>
<p>Supervision. We train all models using imitation learning on expert trajectories. This ensures the language directives match the visual inputs. At each timestep, the model is trained to produce the expert action and associated interaction mask for manipulation actions.</p>
<p>We note that a DAgger-style [44] student-forcing paradigm in ALFRED is non-trivial, even disregarding language alignment. Obtaining expert demonstration actions on the fly in navigation-only datasets like R2R [3] only requires rerunning $A^{*}$. In ALFRED, on the fly demonstrations requires re-planning. In same cases re-planning is not</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Model overview. At each step, our model reweights the instruction based on the history ( $\hat{x}<em t="t">{t}$ ), and combines the current observation features $\left(v</em>\right)$ and a pixelwise interaction mask over the observed image to indicate an object.
possible: if during a task of ${$ Clean \&amp; Place, apple, refrigerator, KITCHEN-3 $}$ a student-forcing model slices the only apple in the scene, the action cannot be recovered from and the task cannot be completed.}\right)$ and the previously executed action $\left(a_{t-1}\right)$. These are passed as input to an LSTM cell to produce the current hidden state. Finally, the new hidden state $\left(h_{t}\right)$ is combined with the previous features to predict both the next action $\left(a_{t</p>
<p>Visual encoding. Each visual observation $o_{t}$ is encoded with a frozen ResNet-18 [22] CNN, where we take the output of the final convolution layer to preserve spatial information necessary for grounding specific objects in the visual frame. We embed this output using two more $1 \times 1$ convolution layers and a fully-connected layer. During training, a set of $T$ observations from the expert demonstration is encoded as $\bar{V}=\left\langle v_{1}, v_{2}, \ldots, v_{T}\right\rangle$, where $v_{t}$ is the visual feature vector at time-step $t$.</p>
<p>Language encoding. Given a natural language goal $\bar{G}=\left\langle g_{1}, g_{2}, \ldots g_{L_{g}}\right\rangle$ of $L_{g}$ words, and step-bystep instructions $\bar{S}=\left\langle s_{1}, s_{2} \ldots s_{L_{s}}\right\rangle$ of $L_{s}$ words, we append them into a single input sequence $\bar{X}=$ $\left\langle g_{1}, g_{2}, \ldots g_{L_{g}},&lt;\mathrm{SEP}\right\rangle, s_{1}, s_{2} \ldots s_{L_{s}}$ ) with the <SEP> token indicating the separation between the high-level goal and low-level instructions. This sequence is fed into a bidirectional LSTM encoder to produce an encoding $x=$ $\left{x_{1}, x_{2}, \ldots, x_{L_{g}+L_{s}}\right}$ for each word in $\bar{X}$.</p>
<p>Attention over language. The agent's action at each timestep is based on an attention mechanism weighting tokens in the instruction. We perform soft-attention on the language features $x$ to compute the attention distribution $\alpha_{t}$ conditioned on the hidden state of the decoder $h_{t-1}$ from the last timestep:</p>
<p>$$
\begin{aligned}
z_{t} &amp; =\left(W_{x} h_{t-1}\right)^{\top} x \
\alpha_{t} &amp; =\operatorname{Softmax}\left(z_{t}\right) \
\hat{x}<em t="t">{t} &amp; =\alpha</em> x
\end{aligned}
$$}^{\top</p>
<p>where $W_{x}$ are learnable parameters of a fully-connected layer, $z_{t}$ is a vector of scalar values that represent the attention mass for each word in $x$, and $\hat{x}<em t="t">{t}$ is the weighted sum of $x$ over the attention distribution $\alpha</em>$.}$ induced from $z_{t</p>
<p>Action decoding. At each timestep $t$, upon receiving a new observation image $o_{t}$, the LSTM decoder takes in the visual feature $v_{t}$, language feature $\hat{x}<em t-1="t-1">{t}$, and the previous action $a</em>$ :}$, and outputs a new hidden state $h_{t</p>
<p>$$
\begin{aligned}
u_{t} &amp; =\left[v_{t} ; \hat{x}<em t-1="t-1">{t} ; a</em>\right] \
h_{t} &amp; =\operatorname{LSTM}\left(u_{t}, h_{t-1}\right)
\end{aligned}
$$</p>
<p>where [; ] denotes concatenation. The hidden state $h_{t}$ is used to obtain the attention weighted language feature $\hat{x}_{t+1}$.</p>
<p>Action and mask prediction. The agent interacts with the environment by choosing an action and producing a pixelwise binary mask indicating a specific object in the frame. Although AI2-THOR supports continuous control for agent navigation and object manipulation, we discretize the action space. The agent chooses from among 13 actions. There are 5 navigation actions: MoveAhead, RotateRight, RotateLeft, LookUp, and LookDown together with 7 interaction actions: Pickup, Put, Open, Close, ToggleOn, ToggleOff, and Slice. Interaction actions require a pixelwise mask to denote the object of interest. ${ }^{1}$ Finally, the agent predicts a Stop action to end the episode. We concatenate the hidden state $h_{t}$ with the input features $u_{t}$ and train two separate networks to predict the next action</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$a_{t}$ and interaction mask $m_{t}$ :</p>
<p>$$
\begin{aligned}
a_{t} &amp; =\operatorname{argmax}\left(W_{a}\left[h_{t} ; u_{t}\right]\right) \
m_{t} &amp; =\sigma\left(\operatorname{deconv}\left[h_{t} ; u_{t}\right]\right)
\end{aligned}
$$</p>
<p>where $W_{a}$ are learnable parameters of a fully connected layer, deconv is a three-layer deconvolution network, and $\sigma$ is a sigmoid activation function. Action selection is trained using softmax cross entropy with the expert action. The interaction masks are learned end-to-end in a supervised manner based on ground-truth object segmentations using binary cross-entropy loss. The mask loss is rebalanced to account for sparsity in these dense masks in which target objects can take up a small portion of the visual frame.</p>
<h3>4.2. Progress Monitors</h3>
<p>ALFRED tasks require reasoning over long sequences of images and instruction words. We propose two auxiliary losses (Eq. $4 \&amp; 5$ ) that use additional temporal information to reduce this burden and form a sequence-to-sequence model with progress monitoring (SEQ2SEQ+PM).</p>
<p>Ma et al. [28] showed that agents benefit from maintaining an internal estimate of their progress towards the goal for navigation tasks. Akin to learning a value function in reinforcement learning, progress monitoring helps to learn the utility of each state in the process of achieving the overall task. Intuitively, this allows our agent to better distinguish between visually similar states such as just before putting an object in the microwave versus just after taking the object out. We introduce a simple module that predicts progress, $p_{t} \in[0,1]$, conditioned on the decoder hidden state $h_{t}$ and the concatenated input $u_{t}$ :</p>
<p>$$
p_{t}=\sigma\left(W_{p}\left[h_{t} ; u_{t}\right]\right)
$$</p>
<p>The supervision for $p_{t}$ is based on normalized time-step values $t / T$, where $t$ is the current time-step, and $T$ is the total length of the expert demonstration (trained via L2 loss).</p>
<p>We also train the agent to predict the number of subgoals completed so far, $c_{t}$. These sub-goals represent segments in the demonstration corresponding to sequences of actions like navigation, pickup, and heating as identified in the PDDL plan, discussed in Section 3.2. Each segment has a corresponding language instruction, but the alignment must be learned. This sub-goal prediction encourages the agent to coarsely track its progress through the language directive. This prediction is also conditioned on the decoder hidden state $h_{t}$ and the concatenated input $u_{t}$ :</p>
<p>$$
c_{t}=\sigma\left(W_{c}\left[h_{t} ; u_{t}\right]\right)
$$</p>
<p>We train $c_{t}$ in a supervised fashion by using the normalized number of sub-goals accomplished in the expert trajectory at each timestep, $c_{t} / C$, as the ground-truth label for a task with $C$ sub-goals. We again train with an L2 loss.</p>
<h2>5. Experiments</h2>
<p>We evaluate the baseline models in the AI2-THOR simulator. When evaluating on test folds, we run models with the lowest validation loss. Episodes that exceed 1000 steps or cause more than 10 failed actions are terminated. Failed actions arise from bumping into walls or predicting action interaction masks for incompatible objects, such as attempting to Pickup a counter top. These limitations encourage efficiency and reliability. We assess the overall and partial success of models' task executions across episodes.</p>
<h3>5.1. Evaluation Metrics</h3>
<p>ALFRED allows us to evaluate both full task and task goal-condition completion. In navigation-only tasks, one can only measure how far the agent is from the goal. In ALFRED, we can also evaluate whether task goal-conditions have been completed, for example that a potato has been sliced. For all of our experiments, we report both Task Success and Goal-Condition Success. Each Goal-Condition relies on multiple instructions, for example navigating to an object and then slicing it.</p>
<p>Task Success. Each expert demonstration is parameterized by a task to be performed, as illustrated in Figure 2. Task Success is defined as 1 if the object positions and state changes correspond correctly to the task goal-conditions at the end of the action sequence, and 0 otherwise. Consider the task: "Put a hot potato slice on the counter". The agent succeeds if, at the end of the episode, any potato slice object has changed to the heated state and is resting on any counter top surface.</p>
<p>Goal-Condition Success. The goal-condition success of a model is the ratio of goal-conditions completed at the end of an episode to those necessary to have finished a task. For example, in the previous Heat \&amp; Place example, there are four goal-conditions. First, a potato must be sliced. Second, a potato slice should become heated. Third, a potato slice should come to rest on a counter top. Fourth, the same potato slice that is heated should be on the counter top. If the agent slices a potato, then moves a slice to the counter top without heating it, then the goal-condition success score is $2 / 4=50 \%$. On average, tasks in ALFRED have 2.55 goal conditions. The final score is calculated as the average goal-condition success of each episode. Task success is 1 only if goal-condition success is 1 .</p>
<p>Path Weighted Metrics. We include a Path Weighted version of both metrics that considers the length of the expert demonstration [2]. Expert demonstrations found via a PDDL solver on global information are not guaranteed to be optimal. However, they avoid exploration, use shortest path</p>
<table>
<thead>
<tr>
<th>Validation</th>
<th></th>
<th></th>
<th></th>
<th>Test</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Seen</td>
<td></td>
<td>Unseen</td>
<td></td>
<td>Seen</td>
<td></td>
<td>Unseen</td>
</tr>
<tr>
<td>Model</td>
<td>Task</td>
<td>Goal-Cond</td>
<td>Task</td>
<td>Goal-Cond</td>
<td>Task</td>
<td>Goal-Cond</td>
<td>Task</td>
</tr>
<tr>
<td>No Language</td>
<td>0.0 (0.0)</td>
<td>5.9 (3.4)</td>
<td>0.0 (0.0)</td>
<td>6.5 (4.7)</td>
<td>0.2 (0.0)</td>
<td>5.0 (3.2)</td>
<td>0.2 (0.0)</td>
</tr>
<tr>
<td>No Vision</td>
<td>0.0 (0.0)</td>
<td>5.7 (4.7)</td>
<td>0.0 (0.0)</td>
<td>6.8 (6.0)</td>
<td>0.0 (0.0)</td>
<td>3.9 (3.2)</td>
<td>0.2 (0.1)</td>
</tr>
<tr>
<td>Goal-only</td>
<td>0.1 (0.0)</td>
<td>6.5 (4.3)</td>
<td>0.0 (0.0)</td>
<td>6.8 (5.0)</td>
<td>0.1 (0.1)</td>
<td>5.0 (3.7)</td>
<td>0.2 (0.0)</td>
</tr>
<tr>
<td>Instructions-only</td>
<td>2.3 (1.1)</td>
<td>9.4 (6.1)</td>
<td>0.0 (0.0)</td>
<td>7.0 (4.9)</td>
<td>2.7 (1.4)</td>
<td>8.2 (5.5)</td>
<td>0.5 (0.2)</td>
</tr>
<tr>
<td>SEQ2SEQ</td>
<td>2.4 (1.1)</td>
<td>9.4 (5.7)</td>
<td>0.1 (0.0)</td>
<td>6.8 (4.7)</td>
<td>2.1 (1.0)</td>
<td>7.4 (4.7)</td>
<td>0.5 (0.2)</td>
</tr>
<tr>
<td>+ PM Progress-only</td>
<td>2.1 (1.1)</td>
<td>8.7 (5.6)</td>
<td>0.0 (0.0)</td>
<td>6.9 (5.0)</td>
<td>3.0 (1.7)</td>
<td>8.0 (5.5)</td>
<td>0.3 (0.1)</td>
</tr>
<tr>
<td>+ PM SubGoal-only</td>
<td>2.1 (1.2)</td>
<td>9.6 (5.5)</td>
<td>0.0 (0.0)</td>
<td>6.6 (4.6)</td>
<td>3.8 (1.7)</td>
<td>8.9 (5.6)</td>
<td>0.5 (0.2)</td>
</tr>
<tr>
<td>+ PM Both</td>
<td>3.7 (2.1)</td>
<td>10.0 (7.0)</td>
<td>0.0 (0.0)</td>
<td>6.9 (5.1)</td>
<td>4.0 (2.0)</td>
<td>9.4 (6.3)</td>
<td>0.4 (0.1)</td>
</tr>
<tr>
<td>Human</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>91.0 (85.8)</td>
</tr>
</tbody>
</table>
<p>Table 3: Task and Goal-Condition Success. For each metric, the corresponding path weighted metrics are given in parentheses. The highest values per fold and metric are shown in blue. All values are percentages. navigation, and are generally efficient. The path weighted score $p_{s}$ for metric $s$ is given as</p>
<p>$$ p_{s}=s \times \frac{L^{<em>}}{\max \left(L^{</em>}, \hat{L}\right)} $$</p>
<p>where $\hat{L}$ is the number of actions the model took in the episode, and $L^{*}$ is the number of actions in the expert demonstration. Intuitively, a model receives half-credit for taking twice as long as the expert to accomplish a task.</p>
<h3>5.2. Sub-Goal Evaluation</h3>
<p>Completing the entire sequence of actions required to finish a task is challenging. In addition to assessing full task success, we study the ability of a model to accomplish the next sub-goal conditioned on the preceding expert sequence. The agent is tested by first forcing it to follow the expert demonstration to maintain a history of states leading up to the sub-goal, then requiring it to complete the sub-goal conditioned on the entire language directive and current visual observation. For the task "Put a hot potato slice on the counter" for example, we can evaluate the sub-goal of navigating to the potato after using the expert demonstration to navigate to and pick up a knife. The tasks in ALFRED contain on average 7.5 such sub-goals (results in Table 4).</p>
<h2>6. Analysis</h2>
<p>Results from our experiments are presented in Table 3. We find that the initial model, without spatial or semantic maps, object segmentations, or explicit object-state tracking, performs poorly on ALFRED's long-horizon tasks with high-dimensional state-spaces. The SEQ2SEQ model achieves $\sim 8 \%$ goal-condition success rate, showing that the agent does learn to partially complete some tasks. This headroom (as compared with humans) motivates further research into models that can perform the complex vision-and-language planning introduced by ALFRED. The performance starkly contrasts other vision-and- language datasets focused on navigation, where sequence-to-sequence with progress monitoring performs well [28].</p>
<h3>6.1. Random Agent</h3>
<p>A random agent is commonly employed as a baseline in vision-and-language tasks. In ALFRED, an agent that chooses a uniform random action and generates a uniform random interaction mask at each timestep achieves $0 \%$ on all folds, even without an API failure limit.</p>
<h3>6.2. Unimodal Ablations</h3>
<p>Previous work established that learned agents without visual inputs, language inputs, or both performed better than random agents and were competitive with initial baselines for several navigation and question answering tasks [50]. These performance gaps were due to structural biases in the datasets or issues with model capacity. We evaluate these ablation baselines (No Language and No Vision) to study vision and language bias in ALFRED.</p>
<p>The unimodal ablation performances in Table 3 indicate that both vision and language modalities are necessary to accomplish the tasks in ALFRED. The No Language model finishes some goal-conditions by interacting with familiar objects seen during training. The No Vision model similarly finishes some goal-conditions by following lowlevel language instructions for navigation and memorizing interaction masks for common objects like microwaves that are centered in the visual frame.</p>
<h3>6.3. Model Ablations</h3>
<p>We additionally ablate the amount of language supervision available to the model, as language directives are given as both a high-level goal and step-by-step instructions. Providing only high-level, underspecified goal language (Goal-only) is insufficient to complete the tasks, but is enough to complete some goal-conditions. Using just low-level, step-by-step instructions (InStructions-only</p>
<p>performs similarly to using both high- and low-levels. Thus, this simple model does not seem to exploit the goal instruction to plan out sub-goals for step-by-step execution.</p>
<p>The two progress monitoring signals are marginally helpful, increasing the success rate from $\sim 1 \%$ to $\sim 2 \%$. Progress monitoring leads to more efficient task completion, as indicated by the consistently higher path weighted scores. They may help avoid action repetition and with the prediction of the Stop action.</p>
<p>The agent takes more steps than the expert in all cases, as indicated by the lower path weighted scores. Sometimes, this is caused by failing to keep track of state-changes, for example heating up an egg in the microwave multiple times. Further, the models also do not generalize well to unseen scenes, due to the overall visual complexity in ALFRED arising from new scenes and novel object class instances.</p>
<h3>6.4. Human evaluation</h3>
<p>We obtained a human evaluation of 100 randomly sampled directives from the unseen test fold. The experiment involved 5 participants who completed 20 tasks each using a keyboard-and-mouse interface. Before the experiment, the participants were allowed to familiarize themselves with AI2-THOR. The action-space and task restrictions were identical to that of the baseline models. Overall, the participants obtained a high success rate of $91 \%$, while taking slightly longer than the expert with $86 \%$ path-length weighted success rate. This indicates that the directives in ALFRED are well-aligned with the demonstrations.</p>
<h3>6.5. Sub-Goal Performance</h3>
<p>We also examine performance of the SEQ2SEQ model on individual sub-goals in ALFRED. For this experiment, we use the expert trajectory to move the agent through the episode up to the sub-task. Then, the agent begins inference based on the language directive and current visual frame.</p>
<p>Table 4 presents path-length weighted success scores for 8 sub-goals. Goto and Pickup sub-tasks with the SEQ2SEQ+PM model achieve $\sim 51 \%$ and $\sim 32 \%$, respectively, even in seen environments. Visual semantic navigation is considerably harder in unseen environments. Similarly, interaction masks for Pickup actions in unseen environments are worse due to unfamiliar scenes and object instances. Simple sub-goals like Cool, and Heat are achieved at a high success rate of $\sim 90 \%$ because these tasks are mostly object-agnostic. For example, the agent becomes familiar with using microwaves to heat things regardless of the object in-hand, because microwaves have little visual diversity across kitchens. Overall, the sub-goal evaluations indicate that models that exploit modularity and hierarchy, or make use of pretrained object segmentation models, may make headway on full task sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sub-Goal Ablations - Validation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No Lang</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2S</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2S + PM</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No Lang</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2S</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S2S + PM</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">46</td>
</tr>
</tbody>
</table>
<p>Table 4: Evaluations by path weighted sub-goal success. All values are percentages. The highest values per fold and task are shown in blue. We note that the No Vision model achieves less than $2 \%$ on all sub-goals. See supplemental material for more.</p>
<h2>7. Conclusions</h2>
<p>We introduced ALFRED $\widehat{\mathbf{\Omega}}$, a benchmark for learning to map natural language instructions and egocentric vision to sequences of actions. ALFRED moves us closer to a community goal of language-driven robots capable of navigation and interaction. The environment dynamics and interaction mask predictions required in ALFRED narrow the gap between what is required of agents in simulation and robots operating in the real world [37].</p>
<p>We use ALFRED to evaluate a sequence-to-sequence model with progress monitoring, shown to be effective in other vision-and-language navigation tasks [28]. While this model is relatively competent at accomplishing some subgoals (e.g. operating microwaves is similar across Heat \&amp; Place tasks), the overall task success rates are poor. The long horizon of ALFRED tasks poses a significant challenge with sub-problems including visual semantic navigation, object detection, referring expression grounding, and action grounding. These challenges may be approachable by models that exploit hierarchy [8, 27], modularity [4, 16], and structured reasoning and planning [6]. We are encouraged by the possibilities and challenges that the ALFRED benchmark introduces to the community.</p>
<h2>Acknowledgements</h2>
<p>Thanks to our UW colleagues for helpful feedback, to Eli VanderBilt and Eric Kolve for their help with AI2-THOR and leaderboard setup, and Victor Zhong for early modeling design. And finally, thanks to Ranjay Krishna for sharing the Mechanical Turk annotation interface. This research was supported in part by the ARO (ARO-W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364, NSF-NRI1637479), and the Allen Institute for Artificial Intelligence.</p>
<h2>References</h2>
<p>[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Unsupervised learning from narrated instruction videos. In CVPR, 2016. 3
[2] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 6
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, 2018. 1, 2, 4, 13
[4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, June 2016. 8
[5] Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 2013. 2
[6] Masataro Asai and Alex Fukunaga. Classical planning in deep latent space: Bridging the subsymbolic-symbolic boundary. In AAAI, 2018. 8
[7] Michael Beetz, Ulrich Klank, Ingo Kresse, Alexis Maldonado, Lorenz Mösenlechner, Dejan Pangercic, Thomas Rühr, and Moritz Tenorth. Robotic roommates making pancakes. In IEEE-RAS, 2011. 3
[8] Yonatan Bisk, Daniel Marcu, and William Wong. Towards a dataset for human computer communication via grounded language acquisition. In AAAI Workshop on Symbiotic Cognitive Systems, 2016. 8
[9] Yonatan Bisk, Deniz Yuret, and Daniel Marcu. Natural language communication with robots. In NAACL, 2016. 2
[10] Mario Bollini, Stefanie Tellex, Tyler Thompson, Nicholas Roy, and Daniela Rus. Interpreting and executing recipes with a cooking robot. In ISER, 2012. 3
[11] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGBD data in indoor environments. 3DV, 2017. 1
[12] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for taskoriented language grounding. In AAAI, 2017. 1, 2
[13] David L Chen and Raymond J Mooney. Learning to interpret natural language navigation instructions from observations. In AAAI, 2011. 2
[14] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In CVPR, 2019. 1, 2, 4
[15] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied Question Answering. In CVPR, 2018. 2
[16] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Neural Modular Control for Embodied Question Answering. In CoRL, 2018. 8
[17] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In NeurIPS, 2018. 1, 2
[18] Malik Ghallab, Adele Howe, Craig Knoblock, Drew McDermott, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddl the planning domain definition language. 1998. 3
[19] Daniel Gordon, Dieter Fox, and Ali Farhadi. What should i do now? marrying reinforcement learning and symbolic planning. arXiv preprint arXiv:1901.01492, 2018. 2
[20] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2017. 2
[21] Stevan Harnad. The symbol grounding problem. Physica D, 42:335-346, 1990. 1
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5, 12
[23] Jörg Hoffmann and Bernhard Nebel. The ff planning system: Fast plan generation through heuristic search. JAIR, 2001. 3
[24] Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. In CVPR, 2019. 1, 2
[25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 13
[26] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv preprint arXiv:1712.05474, 2017. 1, 18
[27] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In NeurIPS, pages 3675-3683, 2016. 8
[28] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Selfmonitoring navigation agent via auxiliary progress estimation. In ICLR, 2019. 2, 6, 7, 8
[29] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. The regretful agent: Heuristic-aided navigation through progress estimation. In CVPR, 2019. 1, 2
[30] James MacGlashan, Monica Babes-Vroman, Marie desJardins, Michael L. Littman, Smaranda Muresan, Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang. Grounding english commands to reward functions. In RSS, 2015. 3
[31] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006. 2
[32] Jonathan Malmaud, Earl Wagner, Nancy Chang, and Kevin Murphy. Cooking with semantics. In ACL Workshop on Semantic Parsing, 2014. 2</p>
<p>[33] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018. 2
[34] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017. 2
[35] Dipendra Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me daved: Context-sensitive grounding of natural language to manipulation instructions. In RSS, 2014. 3
[36] Dipendra Misra, Kejia Tao, Percy Liang, and Ashutosh Saxena. Environment-driven lexicon induction for high-level instructions. In ACL, 2015. 2
[37] Arsalan Mousavian, Clemens Eppner, and Dieter Fox. 6-dof graspnet: Variational grasp generation for object manipulation. In ICCV, 2019. 1, 8
[38] Khanh Nguyen and Hal Daumé III. Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning. In EMNLP, 2019. 2
[39] Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In CVPR, 2019. 2
[40] Daniel Nyga, Subhro Roy, Rohan Paul, Daehyung Park, Mihai Pomarlan, Michael Beetz, and Nicholas Roy. Grounding robot plans from natural language instructions with incomplete world knowledge. In CoRL, 2018. 3
[41] Rohan Paul, Jacob Arkin, Derya Aksaray, Nicholas Roy, and Thomas M. Howard. Efficient grounding of abstract spatial concepts for natural language interaction with robot platforms. IJRR, 2018. 3
[42] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In CVPR, 2018. 1, 2, 4
[43] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. Grounding action descriptions in videos. TACL, 2013. 2, 3
[44] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to noregret online learning. In AISTATS, 2011. 4
[45] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied ai research. In ICCV, 2019. 1
[46] Ozan Sener, Amir R. Zamir, Silvio Savarese, and Ashutosh Saxena. Unsupervised semantic parsing of video collections. In ICCV, 2015. 3
[47] Mohit Shridhar and David Hsu. Interactive visual grounding of referring expressions for human-robot interaction. In RSS, 2018. 3
[48] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In NAACL, 2019. 1
[49] Moritz Tenorth, Daniel Nyga, and Michael Beetz. Understanding and executing instructions for everyday manipulation tasks from the world wide web. In ICRA, 2010. 2
[50] Jesse Thomason, Daniel Gordon, and Yonatan Bisk. Shifting the baseline: Single modality performance on visual navigation \&amp; qa. In NAACL, 2019. 7
[51] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In CoRL, 2019. 2
[52] Jesse Thomason, Shiqi Zhang, Raymond Mooney, and Peter Stone. Learning to interpret natural language commands through human-robot dialog. In IJCAI, 2015. 3
[53] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. In CVPR, 2019. 1, 2
[54] Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. In ECCV, 2018. 2
[55] Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, and Dhruv Batra. Embodied question answering in photorealistic environments with point cloud perception. In CVPR, 2019. 2
[56] Haonan Yu and Jeffrey Mark Siskind. Grounded language learning from video described with sentences. In ACL, 2013. 3
[57] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L. Berg, and Dhruv Batra. Multi-target embodied question answering. In CVPR, 2019. 2
[58] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li FeiFei, Abhinav Gupta, Roozbeh Mottaghi, and Ali Farhadi. Visual semantic planning using deep successor representations. In ICCV, 2017. 2
[59] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Crosstask weakly supervised learning from instructional videos. In CVPR, 2019. 3</p>
<h2>Appendix A. Dataset Details</h2>
<p>We give additional information about the generation of expert demonstrations in AI2-THOR, language directives, the annotation interface used to collect directives, and samples of annotations with their associated demonstrations.</p>
<h2>A.1. Expert Demonstrations</h2>
<p>When sampling task parameters, we employ an active strategy to maximize data heterogeneity. Figure F1 shows the distribution of high-level task across train, validation seen, and validation unseen folds. Figure F2 shows the distribution of subgoals across task types. And Figures F6 and F7 give the distributions of pickup objects and receptacles across the dataset. Each task parameter sample is defined by $(t, s, o, r, m)$, where</p>
<ul>
<li>$t=$ the task type;</li>
<li>$s=$ the scene in AI2-THOR;</li>
<li>$o=$ the object class to be picked up;</li>
<li>$r=$ the final destination for $o$ or $\emptyset$ for Examine;</li>
<li>$m=$ the secondary object class for Stack \&amp; Place tasks ( $\emptyset$ for other task types).</li>
</ul>
<p>To construct the next tuple, we first find the largest source of imbalance in the current set of tuples. For example if $o=$ apple is more common than $o=$ plunger, $o=$ plunger will be ranked higher than $o=$ apple. We additionally account for the prior distribution of each entity
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure F1: Task distribution across train, validation seen and unseen dataset splits.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure F2: Subgoal distribution across 7 task types.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure F3: The number of unique tokens introduced per annotation of language directives.
(e.g., if cup is already represented in the data often as both $o$ and $m$, it becomes disfavored by the sampling algorithm for all slots). We do this greedily across all slots until the tuple is complete. Given any partial piece of information about the task, the distributions of the remaining task parameters remain heterogeneous under this sampling, weakening baseline priors such as ignoring the language input and always executing a common task in the environment.</p>
<p>Once a task parameter sample is complete, the chosen scene is instantiated, objects and agent start position are randomized, and the relevant room data is encoded into PDDL rules for an expert demonstration. If the PDDL planner cannot generate an expert demonstration given the room configuration, or if the agent fails an action during execution, for example by running into walls or opening doors onto itself due to physical constraints, the episode is abandoned. We gather three distinct expert demonstrations per task parameter sample. These demonstrations are further vetted by rolling them forward using our wrapper to the AI2-THOR API to ensure that a "perfect" model can reproduce the demonstration. The full sampling generation and verification code will be published along with the dataset.</p>
<h2>A.2. Example Language Directives</h2>
<p>We chose to gather three directives per demonstration empirically. For a subset of over 700 demonstrations, we gathered up to 6 language directives from different annotators. We find that after three annotations, fewer than 10 unique tokens on average are introduced by additional annotators (Figure F3).</p>
<h2>A.3. Annotation Interface</h2>
<p>Figure F4 shows the Mechanical Turk interface used to gather language annotations. Workers were presented with a video of the expert demonstration with timeline segments indicating sub-goals. The workers annotated each segment</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure F4: Mechanical Turk Annotation Interface.
while scrubbing through the video, and wrote a short summary description for the entire sequence. We payed workers $\$ 0.7$ per annotation. During vetting, annotators were paid $\$ 0.35$ per HIT (Human Interaction Task) to compare 5 sets of three directives each. These wages were set based on local minimum-wage rates and average completion time.</p>
<h2>A.4. Vocabulary Distributions</h2>
<p>Figure F8 shows vocabulary statistics of the language in ALFRED.</p>
<h2>A.5. Dataset Examples</h2>
<p>Figure F9 shows 7 expert trajectories (one per task type) and their accompanied annotations.</p>
<h2>Appendix B. Implementation Details</h2>
<p>We describe implementation and training details of our baseline Sequence-to-Sequence models.</p>
<p>Preprocessing We tokenize the language directives and convert all tokens to lower-case. During dataset generation, we save images from AI2-THOR $300 \times 300$ pixels, and later resize them to $224 \times 224$ during training. The generation</p>
<p>Val Seen
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Val Unseen
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure F5: Predicted interaction masks. Masks generated by the SEQ2SEQ+PM model are displayed in green.
pipeline saves initialization information for objects and the agent, so all demonstration can be perfectly replayed in the simulator. Researchers can use this replay feature to augment the dataset by saving high-res images, depth maps, or object-segmentation masks.</p>
<p>Network Architecture We use a pretrained ResNet-18 [22] to extract $512 \times 7 \times 7$ features from the conv5 layer. These features are fed into a two-layer CNN with $1 \times 1$ convolutions to reduce the channel dimension from 512 to 64 . The $64 \times 7 \times 7$ output is flattened, and a fully-connected layer produces a 2500 -dimensional visual feature $v_{t}$.</p>
<p>The language encoder is a bi-directional LSTM with a hidden-dimension of 100 . We do not use pretrained language models to initialize the LSTM, and the encodings are learned from scratch in an end-to-end manner. We also use a self-attention mechanism to attend over the encodings to initialize the hidden-state of the decoder LSTM.</p>
<p>The action decoder is an LSTM with a hidden-dimension of 512 . The actor is a fully-connected layer that outputs logits for 13 actions. The mask decoder is a three-layer deconvolution network, which takes in the concatenated vector $u_{t}$ and transforms it into $64 \times 7 \times 7$ features with a fullyconnected layer. These features are subsequently up-scaled into a $1 \times 300 \times 300$ binary mask through three layers of deconvolutions and up-sampling with bi-linear interpolation.</p>
<p>Training The models were implemented with PyTorch and trained with the Adam optimizer [25] at a learning rate of 1e-4. We use dropout of 0.3 on the visual features and the decoder hidden state, tuned on the validation data. Both the action and mask losses are weighted equally, while the auxiliary losses are scaled with a factor of 0.1 . For evaluation, we choose models with the lowest loss on the validation seen set. It should be noted that, due to the nature of the tasks, low validation loss might not directly lead to better evaluation performance since the agent does not have to exactly imitate the expert to complete the task.</p>
<p>Notes on Random Agent Unlike discretized navigation where taking random actions might allow the agent to stumble upon the goal, ALFRED tasks are much harder to achieve by chance. The action space branching factor of Room-to-Room navigation [3], for example, is $4^{6} \approx 4000$ ( 6 average steps and 4 navigation actions). By contrast, the ALFRED average branching factor is $12^{50} \approx 10^{53}$ ( 50 average steps for 12 actions). Beyond action type prediction, the ALFRED state space resulting from dynamic environments and the need to produce pixel-wise masks for interactive actions explodes further.</p>
<h2>B.1. Predicted Masks</h2>
<p>Figure F5 shows a few examples of masks generated by the SEQ2SEQ+PM model in seen and unseen validation scenes. The Microwave mask accurately captures the contours of the object since the model is familiar with receptacles in seen environments. In contrast, the Sink mask in the unseen scene poorly fits the unfamiliar object topology.</p>
<h2>Appendix C. Additional Results</h2>
<h2>C.1. Performance by Task Type</h2>
<p>In Table A1, we present success rates across the 7 task types. Even the best performing model, SEQ2SEQ+PM, mostly succeeds in solving some short-horizon tasks like Pick \&amp; Place and Examine. Long horizon tasks like Stack \&amp; Place and Pick Two \&amp; Place have near zero success rates across all models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Ablations - Validation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No Language</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEQ2SEQ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEQ2SEQ+PM</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Task Type</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Unseen</td>
</tr>
<tr>
<td style="text-align: center;">Pick \&amp; Place</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Stack \&amp; Place</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Pick Two</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Clean \&amp; Place</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Heat \&amp; Place</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Cool \&amp; Place</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Examine</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table A1: Success percentages across 7 task types. The highest values are shown in blue.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure F6: Pickup distributions in the train, validation seen and unseen folds.</p>
<p>Figure F7: Receptacle distributions in the train, validation seen and unseen folds.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure F8: Vocabulary Distributions. Frequency distributions of 100 most common verbs, nouns, other words (non-verbs and non-nouns), and all words.</p>
<p>Pick \&amp; Place
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Pick Two \&amp; Place
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure F9: Dataset Examples. Annotations for seven expert demonstrations.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure F9: Dataset Examples. Annotations for seven expert demonstrations.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure F10: Visual diversity of AI2-THOR [26] scenes. Top to bottom rows: kitchens, living rooms, bedrooms, and bathrooms. Object locations are randomized based on placeable surface areas and class constraints. See https: / /ai2thor.allenai.org/ithor/demo/ for an interactive demo.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The final object chosen by the interaction API is based on the Intersection-over-Union (IoU) score between the predicted mask and the ground-truth object mask from the simulator.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9269 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9269</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9269</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-267069067</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.11624v5.pdf" target="_blank">In-context Learning with Retrieved Demonstrations for Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9269.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9269.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (rationale-including demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt format where demonstrations include intermediate reasoning steps (rationales) before the final answer; reported to substantially improve multi-step reasoning for sufficiently large LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various large LLMs (unspecified; benefits stronger for larger models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step reasoning / mathematical and complex reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require multi-step reasoning or intermediate inference to reach the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot demonstrations formatted as Query: q_i, Rationale: r_i, Answer: a_i (rationale inserted before final answer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard few-shot demonstrations without rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Including a rationale (CoT) gives the model intermediate steps to follow, which significantly improves problem-solving ability especially for larger LLMs; effectiveness depends on model size (larger than a certain size benefit more).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT demonstrations place the chain-of-thought/rationale prior to the final answer in the prompt; referenced empirical studies (e.g., Wei et al., 2022; Suzgun et al., 2022) show improved performance for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9269.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>"Let's think step by step"</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt wording: "Let's think step by step"</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A short phrase added to prompts that elicits step-by-step reasoning and substantially improves problem-solving on reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in survey; demonstrated in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks (zero-shot / few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring reasoning; in the cited work this is applied in zero-shot reasoning setups.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot/shot prompt augmented with the phrase "Let's think step by step" to encourage chain-of-thought style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Same prompt without the added phrase.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>described as "substantially" improved on targeted problems (no numeric metric reported in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The phrase primes the model to produce stepwise reasoning; acts as a simple instruction that triggers chain-of-thought behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Simple text insertion into prompt (no change to model weights); reported effects are qualitative in survey (cited empirical work contains quantitative results).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9269.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>"According to Wikipedia"</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt wording: "According to Wikipedia"</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding source-based wording to a prompt (e.g., 'According to Wikipedia') increases factuality of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>prompting language models improves quoting from pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified in survey; cited study reported effect)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factuality / knowledge-grounded generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where factual accuracy or citing sources matters.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt augmented with source-oriented phrasing such as 'According to Wikipedia' prior to the question or instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Same prompt without source-oriented phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The wording nudges the model to surface information consistent with the specified source (improves factuality/quotation behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Simple addition of short phrase in prompt; survey cites Weller et al. (2023) as evidence for changed model behavior toward factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9269.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstration ordering sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to order of few-shot demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The order in which few-shot examples are presented can markedly change performance â€” varying from near-random to state-of-the-art on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (unspecified in survey references)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General few-shot tasks (multiple benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks where few-shot demonstrations are used to condition model behavior; ordering affects model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompt with multiple demonstrations arranged in a particular sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Same demonstrations permuted into other orders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Described qualitatively: performance can range from near-random to state-of-the-art depending on order (no single numeric metric provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Potentially very large (survey quotes range from near-random to state-of-the-art as reported by Lu et al., 2022b).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMs are sensitive to positional/contextual cues in the prompt; later parts of the prompt and examples near the end have outsized influence (Zhao et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Order permutations of same demonstration set; Zhao et al. (2021) additionally report that answers appearing near the end of the prompt are more likely to be produced by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9269.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number of demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of few-shot demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model performance generally improves with more demonstrations but with diminishing returns; generation tasks derive more benefit than classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs broadly (cited Brown et al., 2020 and later work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General few-shot tasks (classification and generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Any task solved via in-context learning where k demonstrations are provided in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting with varying k (number of demonstrations) until context window limits are reached.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Smaller vs larger numbers of demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Described qualitatively: increasing k improves performance but with diminishing returns; generation tasks benefit more than classification (Li et al., 2023b).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More examples provide more task-specific signal, but context window limits and redundancy reduce marginal gains; generation tasks benefit because examples show output format and longer outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Practical ceiling set by model context size; cited empirical trends across multiple studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9269.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieved vs Random demonstrations (general RetICL gain)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-based in-context learning (RetICL) vs random/static demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieving demonstrations tailored to the input generally improves LLM performance over using fixed, random or manually curated demonstration sets across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dr. icl: Demonstration-retrieved in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM, FLAN (cited experiments showing benefits in Luo et al., 2023); other LLMs in referenced studies</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (NLI, sentiment, semantic parsing, QA, reasoning, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where a retrieval corpus of demonstrations is available and a retriever selects k examples tailored to each query.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts built from demonstrations retrieved per-query from a corpus (RetICL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Fixed/static demonstration sets or randomly selected few-shots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Described as "substantial improvements" in multiple works; specific magnitudes vary by task/study.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Per-query retrieval supplies more relevant/useful exemplars (similarity, diversity, structural match), improving signal for the model and reducing brittleness to fixed demo choices.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>RetICL variants include one-shot (top-k), clustering, and iterative retrieval; retriever type (BM25, SBERT, dual-encoder) and retrieval corpus composition affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9269.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rubin et al. retrieval gains (BM25 / EPR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical gains from retrieval reported by Rubin et al. (BM25 and EPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported large empirical improvements when using retrieval-based demonstrations compared to random selection in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to retrieve prompts for in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used in Rubin et al. (survey does not specify exact model names here)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-generation / semantic parsing / other evaluated tasks in Rubin et al.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where demonstration retrieval was applied in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>RetICL with BM25 and EPR retrieval methods used to select few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BM25: reported as 25+% better than random; EPR: reported as 30% better than random (numbers quoted in survey text from Rubin et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+25% (BM25) and +30% (EPR) vs random demonstrations as reported by Rubin et al.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Better exemplar relevance and selection reduces misleading examples and provides clearer task patterns to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Survey quotes these effect sizes directly from Rubin et al.; exact task(s), metrics, and model(s) producing these deltas are from that cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9269.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 vs Dual-encoder (GTR) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of sparse (BM25) vs dense dual-encoder (GTR) retrievers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across several tasks the average performance difference between BM25 and a dual-encoder (GTR) retriever was small (within ~0.5%), with neither consistently dominating across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dr. icl: Demonstration-retrieved in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM and FLAN mentioned as inference LLMs in related experiments; retrievers compared were BM25 and GTR</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple tasks (five tasks compared in Luo et al. summary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of benchmark tasks used to compare retriever types for selecting demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>RetICL with retrieval performed by BM25 or by a pretrained dual-encoder (GTR).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>BM25 vs GTR (dual-encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average performance of BM25 and GTR within 0.5% of each other (survey reports this average difference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>~0.5% average difference between BM25 and GTR across the compared tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Sparse term-matching and dense semantic retrieval can both yield high-quality demonstrations; relative advantage depends on task characteristics (term overlap vs semantic matching).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported by Luo et al. (survey cites comparison across five tasks); specific task-level wins/losses vary (BM25 wins some, GTR wins others).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9269.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval harms commonsense/coreference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative effect of retrieval on commonsense and coreference tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Some studies report that retrieved demonstrations can harm performance on commonsense reasoning and coreference resolution tasks, even performing worse than random demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Uprise: Universal prompt retrieval for improving zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs in referenced studies (unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Commonsense reasoning (e.g., CSQA/CMSQA) and coreference resolution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring commonsense or discourse/coreference understanding rather than factual or structural similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>RetICL (similarity-based retrieval) used to select demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random demonstrations or other selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported negative effects on these tasks in multiple works (e.g., Zhang et al., 2022b; Ye et al., 2023a; Cheng et al., 2023); no single numeric magnitude provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Simple similarity-based retrieval can select superficially similar but unhelpful or misleading exemplars for these tasks; diverse or random exemplars may better expose the model to necessary reasoning patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Survey cites multiple works reporting worse-than-random results for similarity-based RetICL on commonsense reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9269.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Similarity vs Diversity retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval objective: Similarity-based vs Diversity-based exemplar selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different retrieval objectives (select most similar examples vs select a diverse set) affect downstream ICL performance; diversity (e.g., via DPP) can improve tasks where coverage and multiple perspectives matter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Compositional exemplars for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrievers (BM25, SBERT, DPP-based methods) used to select demonstrations; inference LLMs vary by study</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning, classification with unknown label spaces, tasks benefiting from varied exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where either close similarity or coverage/diverse exemplars could be beneficial (e.g., math reasoning, tasks with unfamiliar output symbol spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>RetICL selecting top-k similar examples vs selecting a diverse set (clustering, DPP, coverage-based selection).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Similarity-only retrieval vs diversity-aware retrieval methods (DPP, clustering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Survey reports that diversity and coverage can be important (e.g., Ye et al. fine-tuned DPP improved over similarity-only methods); no unified numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Diversity avoids redundant demonstrations, brings different perspectives, and increases coverage of relevant structures/answers; similarity helps when exemplars must closely match the query structure or solution pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Diversity modeled via Determinantal Point Processes (DPP) or clustering; some studies report better performance with diversity for math/reasoning tasks and when the model is unfamiliar with the output symbol space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9269.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9269.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval reduces ordering sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieved demonstrations reduce sensitivity to demonstration order</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using retrieved, per-query demonstrations makes LLMs less sensitive to the order in which examples are presented compared to using static example sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unified demonstration retriever for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs evaluated in Li et al. (2023b) and related works (unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General few-shot tasks where ordering sensitivity was previously observed</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks affected by demonstration ordering when using static exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>RetICL (per-query retrieval of demonstrations) with examples ordered in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Static/fixed demonstration sets (various orders).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Per-query retrieval yields examples that are more relevant and consistent with the query, reducing the chance that ordering (which example is last or first) will determine model output.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Survey cites Li et al. (2023b) reporting reduced sensitivity to ordering when using retrieved demonstrations versus static exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning with Retrieved Demonstrations for Language Models: A Survey', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Learning to retrieve prompts for in-context learning <em>(Rating: 2)</em></li>
                <li>Dr. icl: Demonstration-retrieved in-context learning <em>(Rating: 2)</em></li>
                <li>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Compositional exemplars for in-context learning <em>(Rating: 2)</em></li>
                <li>Uprise: Universal prompt retrieval for improving zero-shot evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9269",
    "paper_id": "paper-267069067",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (rationale-including demonstrations)",
            "brief_description": "Prompt format where demonstrations include intermediate reasoning steps (rationales) before the final answer; reported to substantially improve multi-step reasoning for sufficiently large LLMs.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "various large LLMs (unspecified; benefits stronger for larger models)",
            "model_size": null,
            "task_name": "Multi-step reasoning / mathematical and complex reasoning tasks",
            "task_description": "Tasks that require multi-step reasoning or intermediate inference to reach the answer.",
            "presentation_format": "Few-shot demonstrations formatted as Query: q_i, Rationale: r_i, Answer: a_i (rationale inserted before final answer).",
            "comparison_format": "Standard few-shot demonstrations without rationales.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Including a rationale (CoT) gives the model intermediate steps to follow, which significantly improves problem-solving ability especially for larger LLMs; effectiveness depends on model size (larger than a certain size benefit more).",
            "null_or_negative_result": false,
            "experimental_details": "CoT demonstrations place the chain-of-thought/rationale prior to the final answer in the prompt; referenced empirical studies (e.g., Wei et al., 2022; Suzgun et al., 2022) show improved performance for larger models.",
            "uuid": "e9269.0",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "\"Let's think step by step\"",
            "name_full": "Prompt wording: \"Let's think step by step\"",
            "brief_description": "A short phrase added to prompts that elicits step-by-step reasoning and substantially improves problem-solving on reasoning benchmarks.",
            "citation_title": "Large language models are zero-shot reasoners.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in survey; demonstrated in cited work)",
            "model_size": null,
            "task_name": "Reasoning tasks (zero-shot / few-shot)",
            "task_description": "Tasks requiring reasoning; in the cited work this is applied in zero-shot reasoning setups.",
            "presentation_format": "Zero-shot/shot prompt augmented with the phrase \"Let's think step by step\" to encourage chain-of-thought style outputs.",
            "comparison_format": "Same prompt without the added phrase.",
            "performance": "described as \"substantially\" improved on targeted problems (no numeric metric reported in survey)",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The phrase primes the model to produce stepwise reasoning; acts as a simple instruction that triggers chain-of-thought behavior.",
            "null_or_negative_result": false,
            "experimental_details": "Simple text insertion into prompt (no change to model weights); reported effects are qualitative in survey (cited empirical work contains quantitative results).",
            "uuid": "e9269.1",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "\"According to Wikipedia\"",
            "name_full": "Prompt wording: \"According to Wikipedia\"",
            "brief_description": "Adding source-based wording to a prompt (e.g., 'According to Wikipedia') increases factuality of model outputs.",
            "citation_title": "prompting language models improves quoting from pretraining data.",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified in survey; cited study reported effect)",
            "model_size": null,
            "task_name": "Factuality / knowledge-grounded generation",
            "task_description": "Tasks where factual accuracy or citing sources matters.",
            "presentation_format": "Prompt augmented with source-oriented phrasing such as 'According to Wikipedia' prior to the question or instruction.",
            "comparison_format": "Same prompt without source-oriented phrasing.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The wording nudges the model to surface information consistent with the specified source (improves factuality/quotation behavior).",
            "null_or_negative_result": false,
            "experimental_details": "Simple addition of short phrase in prompt; survey cites Weller et al. (2023) as evidence for changed model behavior toward factuality.",
            "uuid": "e9269.2",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Demonstration ordering sensitivity",
            "name_full": "Sensitivity to order of few-shot demonstrations",
            "brief_description": "The order in which few-shot examples are presented can markedly change performance â€” varying from near-random to state-of-the-art on some tasks.",
            "citation_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "mention_or_use": "mention",
            "model_name": "various LLMs (unspecified in survey references)",
            "model_size": null,
            "task_name": "General few-shot tasks (multiple benchmarks)",
            "task_description": "Benchmarks where few-shot demonstrations are used to condition model behavior; ordering affects model predictions.",
            "presentation_format": "Few-shot prompt with multiple demonstrations arranged in a particular sequence.",
            "comparison_format": "Same demonstrations permuted into other orders.",
            "performance": "Described qualitatively: performance can range from near-random to state-of-the-art depending on order (no single numeric metric provided in survey).",
            "performance_comparison": null,
            "format_effect_size": "Potentially very large (survey quotes range from near-random to state-of-the-art as reported by Lu et al., 2022b).",
            "explanation_or_hypothesis": "LLMs are sensitive to positional/contextual cues in the prompt; later parts of the prompt and examples near the end have outsized influence (Zhao et al., 2021).",
            "null_or_negative_result": false,
            "experimental_details": "Order permutations of same demonstration set; Zhao et al. (2021) additionally report that answers appearing near the end of the prompt are more likely to be produced by the model.",
            "uuid": "e9269.3",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Number of demonstrations",
            "name_full": "Effect of number of few-shot demonstrations",
            "brief_description": "Model performance generally improves with more demonstrations but with diminishing returns; generation tasks derive more benefit than classification tasks.",
            "citation_title": "Language models are few-shot learners.",
            "mention_or_use": "mention",
            "model_name": "LLMs broadly (cited Brown et al., 2020 and later work)",
            "model_size": null,
            "task_name": "General few-shot tasks (classification and generation)",
            "task_description": "Any task solved via in-context learning where k demonstrations are provided in the prompt.",
            "presentation_format": "Few-shot prompting with varying k (number of demonstrations) until context window limits are reached.",
            "comparison_format": "Smaller vs larger numbers of demonstrations.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "Described qualitatively: increasing k improves performance but with diminishing returns; generation tasks benefit more than classification (Li et al., 2023b).",
            "explanation_or_hypothesis": "More examples provide more task-specific signal, but context window limits and redundancy reduce marginal gains; generation tasks benefit because examples show output format and longer outputs.",
            "null_or_negative_result": false,
            "experimental_details": "Practical ceiling set by model context size; cited empirical trends across multiple studies.",
            "uuid": "e9269.4",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Retrieved vs Random demonstrations (general RetICL gain)",
            "name_full": "Retrieval-based in-context learning (RetICL) vs random/static demonstrations",
            "brief_description": "Retrieving demonstrations tailored to the input generally improves LLM performance over using fixed, random or manually curated demonstration sets across many tasks.",
            "citation_title": "Dr. icl: Demonstration-retrieved in-context learning.",
            "mention_or_use": "mention",
            "model_name": "PaLM, FLAN (cited experiments showing benefits in Luo et al., 2023); other LLMs in referenced studies",
            "model_size": null,
            "task_name": "Various (NLI, sentiment, semantic parsing, QA, reasoning, etc.)",
            "task_description": "Tasks where a retrieval corpus of demonstrations is available and a retriever selects k examples tailored to each query.",
            "presentation_format": "Few-shot prompts built from demonstrations retrieved per-query from a corpus (RetICL).",
            "comparison_format": "Fixed/static demonstration sets or randomly selected few-shots.",
            "performance": null,
            "performance_comparison": "Described as \"substantial improvements\" in multiple works; specific magnitudes vary by task/study.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Per-query retrieval supplies more relevant/useful exemplars (similarity, diversity, structural match), improving signal for the model and reducing brittleness to fixed demo choices.",
            "null_or_negative_result": false,
            "experimental_details": "RetICL variants include one-shot (top-k), clustering, and iterative retrieval; retriever type (BM25, SBERT, dual-encoder) and retrieval corpus composition affect outcomes.",
            "uuid": "e9269.5",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Rubin et al. retrieval gains (BM25 / EPR)",
            "name_full": "Empirical gains from retrieval reported by Rubin et al. (BM25 and EPR)",
            "brief_description": "Reported large empirical improvements when using retrieval-based demonstrations compared to random selection in their experiments.",
            "citation_title": "Learning to retrieve prompts for in-context learning",
            "mention_or_use": "mention",
            "model_name": "LLMs used in Rubin et al. (survey does not specify exact model names here)",
            "model_size": null,
            "task_name": "Text-generation / semantic parsing / other evaluated tasks in Rubin et al.",
            "task_description": "Tasks where demonstration retrieval was applied in the cited work.",
            "presentation_format": "RetICL with BM25 and EPR retrieval methods used to select few-shot examples.",
            "comparison_format": "Random demonstrations.",
            "performance": "BM25: reported as 25+% better than random; EPR: reported as 30% better than random (numbers quoted in survey text from Rubin et al., 2022).",
            "performance_comparison": null,
            "format_effect_size": "+25% (BM25) and +30% (EPR) vs random demonstrations as reported by Rubin et al.",
            "explanation_or_hypothesis": "Better exemplar relevance and selection reduces misleading examples and provides clearer task patterns to the model.",
            "null_or_negative_result": false,
            "experimental_details": "Survey quotes these effect sizes directly from Rubin et al.; exact task(s), metrics, and model(s) producing these deltas are from that cited paper.",
            "uuid": "e9269.6",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "BM25 vs Dual-encoder (GTR) comparison",
            "name_full": "Comparison of sparse (BM25) vs dense dual-encoder (GTR) retrievers",
            "brief_description": "Across several tasks the average performance difference between BM25 and a dual-encoder (GTR) retriever was small (within ~0.5%), with neither consistently dominating across tasks.",
            "citation_title": "Dr. icl: Demonstration-retrieved in-context learning.",
            "mention_or_use": "mention",
            "model_name": "PaLM and FLAN mentioned as inference LLMs in related experiments; retrievers compared were BM25 and GTR",
            "model_size": null,
            "task_name": "Multiple tasks (five tasks compared in Luo et al. summary)",
            "task_description": "A set of benchmark tasks used to compare retriever types for selecting demonstrations.",
            "presentation_format": "RetICL with retrieval performed by BM25 or by a pretrained dual-encoder (GTR).",
            "comparison_format": "BM25 vs GTR (dual-encoder).",
            "performance": "Average performance of BM25 and GTR within 0.5% of each other (survey reports this average difference).",
            "performance_comparison": null,
            "format_effect_size": "~0.5% average difference between BM25 and GTR across the compared tasks.",
            "explanation_or_hypothesis": "Sparse term-matching and dense semantic retrieval can both yield high-quality demonstrations; relative advantage depends on task characteristics (term overlap vs semantic matching).",
            "null_or_negative_result": false,
            "experimental_details": "Reported by Luo et al. (survey cites comparison across five tasks); specific task-level wins/losses vary (BM25 wins some, GTR wins others).",
            "uuid": "e9269.7",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Retrieval harms commonsense/coreference",
            "name_full": "Negative effect of retrieval on commonsense and coreference tasks",
            "brief_description": "Some studies report that retrieved demonstrations can harm performance on commonsense reasoning and coreference resolution tasks, even performing worse than random demonstrations.",
            "citation_title": "Uprise: Universal prompt retrieval for improving zero-shot evaluation.",
            "mention_or_use": "mention",
            "model_name": "LLMs in referenced studies (unspecified in survey)",
            "model_size": null,
            "task_name": "Commonsense reasoning (e.g., CSQA/CMSQA) and coreference resolution",
            "task_description": "Tasks requiring commonsense or discourse/coreference understanding rather than factual or structural similarity.",
            "presentation_format": "RetICL (similarity-based retrieval) used to select demonstrations.",
            "comparison_format": "Random demonstrations or other selection strategies.",
            "performance": "Reported negative effects on these tasks in multiple works (e.g., Zhang et al., 2022b; Ye et al., 2023a; Cheng et al., 2023); no single numeric magnitude provided in survey.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Simple similarity-based retrieval can select superficially similar but unhelpful or misleading exemplars for these tasks; diverse or random exemplars may better expose the model to necessary reasoning patterns.",
            "null_or_negative_result": true,
            "experimental_details": "Survey cites multiple works reporting worse-than-random results for similarity-based RetICL on commonsense reasoning benchmarks.",
            "uuid": "e9269.8",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Similarity vs Diversity retrieval",
            "name_full": "Retrieval objective: Similarity-based vs Diversity-based exemplar selection",
            "brief_description": "Different retrieval objectives (select most similar examples vs select a diverse set) affect downstream ICL performance; diversity (e.g., via DPP) can improve tasks where coverage and multiple perspectives matter.",
            "citation_title": "Compositional exemplars for in-context learning.",
            "mention_or_use": "mention",
            "model_name": "Retrievers (BM25, SBERT, DPP-based methods) used to select demonstrations; inference LLMs vary by study",
            "model_size": null,
            "task_name": "Mathematical reasoning, classification with unknown label spaces, tasks benefiting from varied exemplars",
            "task_description": "Tasks where either close similarity or coverage/diverse exemplars could be beneficial (e.g., math reasoning, tasks with unfamiliar output symbol spaces).",
            "presentation_format": "RetICL selecting top-k similar examples vs selecting a diverse set (clustering, DPP, coverage-based selection).",
            "comparison_format": "Similarity-only retrieval vs diversity-aware retrieval methods (DPP, clustering).",
            "performance": "Survey reports that diversity and coverage can be important (e.g., Ye et al. fine-tuned DPP improved over similarity-only methods); no unified numeric metric provided.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Diversity avoids redundant demonstrations, brings different perspectives, and increases coverage of relevant structures/answers; similarity helps when exemplars must closely match the query structure or solution pattern.",
            "null_or_negative_result": false,
            "experimental_details": "Diversity modeled via Determinantal Point Processes (DPP) or clustering; some studies report better performance with diversity for math/reasoning tasks and when the model is unfamiliar with the output symbol space.",
            "uuid": "e9269.9",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Retrieval reduces ordering sensitivity",
            "name_full": "Retrieved demonstrations reduce sensitivity to demonstration order",
            "brief_description": "Using retrieved, per-query demonstrations makes LLMs less sensitive to the order in which examples are presented compared to using static example sets.",
            "citation_title": "Unified demonstration retriever for in-context learning.",
            "mention_or_use": "mention",
            "model_name": "LLMs evaluated in Li et al. (2023b) and related works (unspecified in survey)",
            "model_size": null,
            "task_name": "General few-shot tasks where ordering sensitivity was previously observed",
            "task_description": "Tasks affected by demonstration ordering when using static exemplars.",
            "presentation_format": "RetICL (per-query retrieval of demonstrations) with examples ordered in prompt.",
            "comparison_format": "Static/fixed demonstration sets (various orders).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Per-query retrieval yields examples that are more relevant and consistent with the query, reducing the chance that ordering (which example is last or first) will determine model output.",
            "null_or_negative_result": false,
            "experimental_details": "Survey cites Li et al. (2023b) reporting reduced sensitivity to ordering when using retrieved demonstrations versus static exemplars.",
            "uuid": "e9269.10",
            "source_info": {
                "paper_title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Learning to retrieve prompts for in-context learning",
            "rating": 2,
            "sanitized_title": "learning_to_retrieve_prompts_for_incontext_learning"
        },
        {
            "paper_title": "Dr. icl: Demonstration-retrieved in-context learning",
            "rating": 2,
            "sanitized_title": "dr_icl_demonstrationretrieved_incontext_learning"
        },
        {
            "paper_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "rating": 2,
            "sanitized_title": "fantastically_ordered_prompts_and_where_to_find_them_overcoming_fewshot_prompt_order_sensitivity"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Compositional exemplars for in-context learning",
            "rating": 2,
            "sanitized_title": "compositional_exemplars_for_incontext_learning"
        },
        {
            "paper_title": "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
            "rating": 1,
            "sanitized_title": "uprise_universal_prompt_retrieval_for_improving_zeroshot_evaluation"
        }
    ],
    "cost": 0.01950025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>In-context Learning with Retrieved Demonstrations for Language Models: A Survey</p>
<p>Man Luo 
Arizona State University</p>
<p>Xin Xu 
Google Research</p>
<p>Yue Liu 
Google Research</p>
<p>Panupong Pasupat ppasupat@google.com 
Google Research</p>
<p>Mehran Kazemi mehrankazemi@google.com 
Google Research</p>
<p>In-context Learning with Retrieved Demonstrations for Language Models: A Survey
4388800FE3F58214B9D49747B678FFD9
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context.However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations.Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query.The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems.This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection.In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area.In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.</p>
<p>Introduction</p>
<p>Few-shot in-context learning (ICL) is the ability of large language models (LLMs) to perform a new task when a few input-output examples, or demonstrations, for the new task are given alongside the actual task input.Importantly, the model parameters do not have to be fine-tuned towards the new task.ICL is popularized by the work on pre-trained large language models, which can perform ICL without being trained to do so (Brown et al., 2020), though smaller language models can also be explicitly trained to perform ICL (Min et al., 2022a).</p>
<p>ICL presents several advantages over the conventional methodology for adapting language models to a downstream task, which typically involves initial pre-training followed by subsequent fine-tuning.One significant merit of ICL is the circumvention of fine-tuning, which might not always be possible due to limited access to the model parameters or constraints on computational resources (Brown et al., 2020).Furthermore, ICL avoids common issues associated with fine-tuning, such as overfitting (Ying, Figure 1: Structure of the Survey.</p>
<p>unlocking the true potential of the LLMs.The effectiveness of such demonstrations is influenced by factors such as the quality, quantity, and ordering of the demonstrations.</p>
<p>Retrieval-based ICL (RetICL) presents a paradigm shift in the optimization of language model performance, moving beyond static, pre-defined demonstration sets to a dynamic, context-sensitive approach.At the heart of this innovation is the concept of adaptive demonstration selection, where a specialized retriever intelligently curates tailored demonstrations for each specific task input.This method has not only consistently outshined approaches relying on random or static hand-crafted demonstrations but has also demonstrated a remarkable resilience to a variety of influencing factors.</p>
<p>The efficacy of RetICL pivots on the "relevance" and "usefulness" of the demonstrations it selects, a process intricately influenced by multiple elements.These include the nature of the retriever-ranging from general off-the-shelf models to finely-tuned, domain-specific variants-the source and diversity of the retrieval corpus, the retriever's objectives (focusing on either similarity or diversity), and the strategies for integrating multiple demonstrations.Over the past two years, numerous and sometimes concurrent works have studied RetICL each with different terminology and with variations in problem definition and subsequent methodologies, making it difficult to comprehend the current state of research and practice in RetICL, especially for newcomers to the field.In this comprehensive survey, we meticulously analyze 22 seminal papers in the field of RetICL, as detailed in Table 1, and provide a categorization of their main building blocks (See Figure 1).Our work not only provides a thorough synthesis of existing research but also underscores the areas where RetICL significantly surpasses previous ICL methods, and illuminates many paths forward for future innovations in this area, thus serving as a critical resource for ICL.</p>
<p>2 Few-shot In-context Learning for Language Models Language models (LMs) (Zhao et al., 2023;Rosenfeld, 2000) are probabilistic models that assign probabilities to sequences of words and are essential components in many tasks.Let s represent a sequence of words (e.g., a sentence) and w 1 , w 2 , . . ., w n represent the tokens in the sequence.Based on the chain rule, the probability p(s) can be decomposed into the following product of probabilities: where each element in the product corresponds to the probability of a token given the previous tokens.Based on the above decomposition, an LM can be constructed by learning the probability of the next token given the previous ones.</p>
<p>Earlier LMs were mostly based on N-gram models which are based on the Markovian assumption that the next token only depends on the recent context (Jurafsky, 2021).Based on this assumption, p(w k | w 1 , . . ., w kâˆ’1 ) is approximated, e.g., by p(w k | w kâˆ’2 , w kâˆ’1 ) in the case of a bi-gram model; p(w k | w kâˆ’2 , w kâˆ’1 ) is then approximated statistically based on the number of times w k appeared after w kâˆ’2 in a large corpora of text, w kâˆ’1 divided by the total number of times w kâˆ’2 , w kâˆ’1 appeared in the corpora.</p>
<p>With the advent of word embeddings (Bengio et al., 2000;Mikolov et al., 2013), neural approaches to language modeling gained more popularity, in which a neural network is used to predict the next token probability.The use of powerful neural networks such as long-short term memory (LSTM) models (Hochreiter and Schmidhuber, 1997) and Transformer models (Vaswani et al., 2017) allowed for predicting the next token probability based on a much longer and a variable length context, thus enabling better estimation of p(w k | w 1 , . . ., w kâˆ’1 ).</p>
<p>The increased power of neural LMs led to a new learning paradigm for NLP problems.Historically, the dominant learning paradigm for NLP problems was to train models on task-specific data from scratch.Consequently, for each new task, the model had to learn everything from scratch.This often resulted in poor generalization, especially in the cases where previously unobserved vocabulary was observed at the test time.In the subsequent paradigm, an LM was first pre-trained on a large corpora of text making it learn about how language works and gain a vast amount of knowledge about the world (Petroni et al., 2019;Lin et al., 2020;Sung et al., 2021;Yuan et al., 2023); the pre-trained LM (PLM) was then further finetuned on data from the new tasks (Sarzynska-Wawer et al., 2021;Devlin et al., 2018) thus teaching the general PLM the specifics of the new task.This paradigm often resulted in faster learning and higher predictive performance.It was later shown that further finetuning a PLM on multiple tasks leads to better transfer of knowledge across tasks and may lead to better performance on new tasks (Raffel et al., 2020).</p>
<p>In-Context Learning</p>
<p>As the scale of the PLMs and the scale of the datasets on which these models were pre-trained increased -leading to pre-trained Large Language Models (LLMs), it was discovered that pre-trained LLMs (hereafter, referred to as LLMs for brevity) have a remarkable capability of learning in-context from a few demonstrations (Brown et al., 2020).That is, LLMs were shown to be able to adapt to new tasks by only seeing a few examples of the new task in their input, as opposed to needing additional training data or fine-tuning.This is typically referred to as few-shot in-context learning.Let T be a task and q * âˆ¼ T represent a sample query from this task for which we would like to find an answer using an LLM.In the case of few-shot learning, we find or construct multiple demonstrations {d 1 , . . ., d k } where each demonstration d i = (q i , a i ) contains a query q i âˆ¼ T and the answer a i to that query, and feed an input of the form q 1 a 1 . . .q k a k q * to the LLM.The input is typically referred to as prompt.It is common to add some separator tokens to the prompt so the boundaries of the demonstrations and the questions and answers within those demonstrations are clear.An example prompt will then be as follows:</p>
<p>Demonstration 1 : Query : q 1 , Answer : a 1 ... Demonstration k + 1 : Query : q * , Answer :</p>
<p>Seeing the demonstrations as a few examples of the task, LLMs learn from the demonstrations in context (without any weight updates) and use a similar pattern to provide an answer to the query q * .Few-shot learning is a remarkable capability of LLMs given that they are not trained on such data during their pre-training.While LLMs show strong few-shot learning capabilities off-the-shelve, it has been shown that warming them up by finetuning them on few-shot data from multiple tasks will further boost their few-shot learning capability (Min et al., 2022a;Chen et al., 2022;Radford et al., 2019).</p>
<p>Another remarkable ICL capability of LLMs is to learn from in-context instructions: finetuning LLMs on instructions from multiple tasks makes them learn to follow instructions for new tasks (Ouyang et al., 2022;Longpre et al., 2023;Zhang et al., 2023).In this case, commonly known as instruction tuning, the LLM is finetuned on data of the type I T , q T , a where I T represents the instructions for a task T describing how the task should be performed, q T represents a query from task T and a represents the answer.The finetuning is performed on data from multiple tasks and multiple queries from each task.It is also possible to combine instructions with few-shot demonstrations in which case an example prompt may be as follows:</p>
<p>[Task instructions] Demonstration 1 : Query : q 1 , Answer : a 1 ... Demonstration k + 1 : Query : q * , Answer :</p>
<p>Benefits of ICL: Compared to the aforementioned approach of utilizing LLMs which involves pre-training followed by fine-tuning, ICL offers several key advantages.Firstly, fine-tuning may not always be feasible due to restricted access to the LLM, inadequate computational resources, or inadequately labeled data (Brown et al., 2020), whereas ICL requires fewer resources, less data, and is easier to serve through API calls.Additionally, ICL avoids the issues commonly associated with fine-tuning, such as overfitting or shocks (Ying, 2019;Kazemi et al., 2023a), as it does not modify the model's parameters, allowing it to remain general.</p>
<p>What Makes for Good Demonstrations?</p>
<p>Several works try to provide theoretical justifications and insights into how LLMs learn from a few in-context demonstrations (Xie et al., 2021;Garg et al., 2022;Von Oswald et al., 2023).However, the exact reasons behind this capability are still largely unclear making it difficult to select optimal few-shot demonstrations.Fortunately, various empirical results show the effect of the few-shot demonstrations on the predictive accuracy of the LLMs and provide suggestions on the best practices for preparing them.They also show the brittleness of the LLMs in the choice, format, and order of the few-shot demonstrations.Here, we describe some of the more prominent ones.</p>
<p>Number of Demonstrations:</p>
<p>LLMs generally benefit from more demonstrations, but as the number of demonstrations increases the rate of improvement typically decreases (Brown et al., 2020;Ye et al., 2023b;Min et al., 2022b).Generation tasks have been shown to benefit from an increased number of demonstrations more than classification tasks (Li et al., 2023b).Toward increasing the number of demonstrations, one barrier is the maximum context size of the LLM.While the size of the context has been increasing over time with newer LLMs, it may still be problematic for datasets with long input texts or classification datasets with many classes.</p>
<p>Demonstration Formatting: Various works have shown that the formatting and wording of the prompts can play a crucial role in the performance of the LLM (Jiang et al., 2020;Shin et al., 2020;Kojima et al.;Yang et al., 2023).For example, Kojima et al. show that simply adding Let's think step by step to the prompt makes LLMs reason step by step and solve substantially more problems, and Weller et al. (2023) show that adding According to Wikipedia to the prompt makes them more factual.Moreover, Min et al. (2022b) shows that besides the text formatting, the label space and the distribution of the input text in the demonstrations are also of immense importance.</p>
<p>Order of Demonstrations: The order of demonstrations has been shown to substantially affect the model performance.For example, Lu et al. (2022b) show that on some tasks, the model performance can range from near-random to state-of-the-art depending on the order of the prompts, and Zhao et al. (2021) show that answers appearing toward the end of the prompt are more likely to be predicted by the model.</p>
<p>Diversity of Demonstrations:</p>
<p>Another important factor in the success of few-shot learning is the diversity of the demonstrations.Naik et al. (2023) propose DiversePrompting where for the question of a demonstration, an LLM is used to generate different ways of solving the problem, and then those solutions are used in the prompt.Zhang et al. (2022b) propose to select a diverse set of questions as few-shot examples.Ma et al. (2023) propose a fairness metric for selecting demonstrations which encourages selecting diverse few-shot demonstrations that produce a near uniform predictive distribution for a semantic-free input.</p>
<p>Chain of Thought (CoT): It has been shown that including a rationale for the answer significantly improves model performance, especially for models that are larger than a certain size (Suzgun et al., 2022).The rationale is commonly known as chain of thought (CoT) (Wei et al., 2022).In the case of CoT prompting, the demonstrations are typically formatted as:</p>
<p>Query : q i , Rationale : r i , Answer : a i with the rationale appearing before the final answer.Several works have investigated the reason behind the efficacy of CoT prompting and how to improve the prompts and rationales (Wang et al., 2022a;Lanham et al., 2023).</p>
<p>In-context Learning with Demonstration Retrieval</p>
<p>Traditionally, the same set of few-shot demonstrations is used on all queries, which can be suboptimal especially when there are high variations among the queries.An alternative is to retrieve few-shot demonstrations that are tailored to the current query.Previous work has shown that demonstration retrieval leads to substantial improvements in the task metrics, compared to manually curated or randomly selected demonstrations (Luo et al., 2023;Ye et al., 2023a).Furthermore, LLMs have been shown to become less sensitive to the factors such as demonstration ordering (Section 2.2) when retrieved demonstrations are used (Li et al., 2023b).</p>
<p>This section gives an overview of the retrieval-based ICL (RetICL).We start by defining ICL with retrieved demonstrations.Formally, given a query q * and a retrieval corpus C, a demonstration retriever DR selects a set of demonstrations {d 1 , . . ., d k } âˆ¼ C, where each demonstration is
d i = (q i , a i ). The LLM input sequence becomes (d 1 , . . . , d k , q * ).
The goal of the retriever is to select demonstrations that maximize the probability of the correct answer a * .</p>
<p>The success of RetICL depends on several factors.This section explores design choices, including the retrieval objectives, retrieval inference strategy, and retrieval corpus.Then in Sections 4 and 5, we explore the retriever models and how to train them to tailor to downstream tasks.</p>
<p>Retrieval Objectives: Similarity and Diversity</p>
<p>Various retrieval objectives for selecting and tailoring in-context examples for LLMs have been explored (Luo et al., 2023;Rubin et al., 2022;Ye et al., 2023a;Dalvi et al., 2022;Cheng et al., 2023;Li et al., 2023b).There are two primary retrieval objectives for selecting demonstrations: similarity and diversity.Similarity involves selecting demonstrations most akin to the query and can be based on language similarity (term matching or semantic matching), structural aspects (sentence structure, reasoning structure, etc.), or other criteria.Most studies focus on language similarity, with fewer addressing structural similarity, often due to the challenges in extracting a query's structure in many tasks (Levy et al., 2022).Beyond similarity, some work has found that the diversity of demonstrations is important.The motivations for diversity include avoiding repetitive demonstrations (Zhang et al., 2022b), bringing different perspectives (Yu et al., 2023), and maximizing the demonstrations' coverage of the test query, in terms of covering either its words or syntactic structures (Levy et al., 2022).Measuring the diversity of multiple demonstrations is a major technical challenge.Ye et al. (2023a) applied determinantal point processes (DPP) a probabilistic model to measure the negative interaction (Kulesza et al., 2012), to measure the diversity.Levy et al. (2022) found that diversity and coverage are important when the model is unfamiliar with the output symbols space.It is noteworthy that researchers have found that ICL benefits more from demonstrations with higher complexity in some scenarios (Fu et al., 2022), where they define the complexity in terms of the query length or reasoning steps.However, Fu et al. (2022) employed heuristic rules to define complexity and pre-selected demonstrations accordingly.Their research revealed that using a similarity-based retriever led to improved performance in a specific mathematical reasoning task.This might indicate that combining similarity and complexity considerations could be a promising strategy for enhancing the approach to reasoning tasks.</p>
<p>Inference Strategy to Retrieve Few-shots Demonstrations</p>
<p>This section explores various strategies for employing a retriever to gather k demonstrations.We divide these into three distinct methodologies.</p>
<p>One-hoc Retrieval This is the most basic retrieval strategy.To obtain k demonstrations, given a query, the retriever ranks the demonstrations based on some scoring criteria and then selects the top-k demonstrations.Thus, each demonstration is chosen independently of the others.This method is straightforward and fast, however, it might not yield the best combination of k demonstrations as these demonstrations might be homogeneous.</p>
<p>Clustering Retrieval To mitigate the issue of homogeneity in one-hot retrieval, clustering retrieval approaches (Li et al., 2022;Zhang et al., 2022b;Li and Qiu, 2023b) categorize all demonstrations into k sub-groups aiming to group similar demonstrations together.Then given a query, the retriever picks the most similar demonstration from each sub-group resulting in a final set of k demonstrations.The core principle of clustering is to select a diverse range of demonstrations.Most of the work use SBERT Reimers and Gurevych (2019a) to encode the demonstrations (only the question or the entire demonstrations) and then apply k-means for clustering.</p>
<p>Iterative Retrieval The earlier retrieval strategies acquire each demonstration independently.However, in iterative retrieval, a retriever selects demonstrations based on both the query and previously retrieved demonstrations.This process starts with a single query, for which the retriever finds one best demonstration.The query is then augmented (e.g.combined with the demonstration) to retrieve the next demonstration.This step is iteratively executed k times to gather k demonstrations.The general idea is to select the demonstrations that can complement each other.An an example of a work from this categorym, Scarlatos and Lan ( 2023) train an LSTM retriever using a reinforcement learning framework.During the inference phase, the retriever processes the input query to select the best initial demonstration.It then generates a new query representation by integrating the query with prior demonstrations, specifically utilizing the hidden state representation from the LSTM model.This process of updating the query representation and obtaining subsequent demonstrations continues iteratively until k demonstrations are retrieved.</p>
<p>Retrieval Corpus</p>
<p>The retrieval corpus forms a pool of demonstrations that the retriever can access.Using annotated data is one of the most straightforward ways to construct the retrieval corpus.This setting assumes that training data related to a task is available, and thus can be used as the retrieval corpus.Under this setting, there are three main ways to construct the corpus that we will discuss individually below.</p>
<p>In While there might be other differences between the two setups that may affect the final performance, this comparison implies that retrieving from a carefully selected subset might have comparable results to retrieving from the entire training set.</p>
<p>Mix-Domain</p>
<p>The previous scenario has one individual retrieval corpus for different tasks.Assuming that we want to test model performance on two tasks, then in the in-domain setting, there will be two retrieval corpora separately.Furthermore, the in-domain setting assumes that the model has knowledge about which task the test question belongs to such that when it comes to the retrieval phase, it knows which corpus to select the demonstrations from.However, this assumption does not hold in several real-world applications of LLMs.In the mix-domain setting (Wang et al., 2023a;Li et al., 2023b), the retrieval corpus is constructed from the combination of all tasks.At the inference time, given a question, the retriever will retrieve demonstrations from this mixed corpus; the demonstrations can come from the same domain as the test question or from other tasks.</p>
<p>Cross-Domain In this setting, IID human-annotated demonstrations are not available for the test queries, so one uses annotated demonstrations from other similar tasks (Cheng et al., 2023;Shi et al., 2022).Note that this is different from the mix-domain setting where part of the corpus is IID and part of it is not.For instance, Shi et al. (2022) describes a scenario where the goal is to parse a Chinese query into SQL.However, the demonstrations are sourced from an English Text-to-SQL corpus, a domain with significantly more resources than the target domain.Shi et al. (2022) employs this high-resource data as the retrieval corpus.To adapt to the target domain during inference with a LLM, the target query is translated into the same language as the demonstrations.Nie et al. (2022) presents a similar approach, retrieving demonstrations from high-resource domains to address low-resource queries.However, their retrieval pool consists of multiple high-resource sources.</p>
<p>Unlabelled Queries with Automatically Generated Answers The previous three corpora all presuppose the availability of human-annotated data.However, this assumption may not hold in real-life scenarios, particularly in streaming settings where users can pose questions without any pre-annotated answers.Several studies (Zhang et al., 2022b;Li and Qiu, 2023b) have suggested using LLMs to generate answers for unlabeled data.They apply filtering techniques to determine the quality of these generated answers, adding only those examples with high-quality answers to the retrieval corpus.The most widely used filtering technique is based on self-consistency (Wang et al., 2022c).This approach involves prompting the language model to generate multiple chains of thought and answers, then selecting the most common answer as the final response.</p>
<p>Free Form Corpus Another approach to deal with the lack of human-annotated data for similar tasks is create pseudo-demonstrations from unstructured text.Toward this goal, Lyu et al. (2022) utilized the Demix dataset (Gururangan et al., 2022), which is not tailored for any specific task.To generate pusedo-demonstrations, a retriever selects the top-k most relevant sentences from the dataset.Subsequently, arbitrary labels are attached to each sentence to form the examples.Li et al. (2022) propose a synthetic question answering generation method to create QA pairs using the synthetic generated passages by an LLM.</p>
<p>Off-the-shelf Demonstration Retrievers</p>
<p>To achieve the retrieval objectives outlined above, researchers have explored various types of demonstration retrievers.A typical demonstration retriever encodes examples from the retrieval corpus and the query into some vector representations, and then a similarity measure (e.g.cosine similarity) is calculated between candidate demonstration embeddings and the query embedding to locate the most related demonstrations.Given the limited understanding of the underlying mechanism through which retrieved demonstrations enhance the performance of LLMs, initial research efforts focused on a heuristic evaluation of readily available retrievers for this task.Subsequent research endeavors explored the design and development of learning-based retrievers specifically customized for retrieving demonstrations.This section reviews representative off-the-shelf models and we will discuss the learning-based models in Section 5.</p>
<p>Term-based Similarity BM25 (Robertson et al., 2009) is one of the most popular term-based scoring methods due to its simplicity and effectiveness in producing relevant results.It takes into account both term frequencies and document lengths.It has been empirically demonstrated in various works (Luo et al., 2023;Rubin et al., 2022;Agrawal et al., 2022;Ye et al., 2023a;Dalvi et al., 2022) that using BM25 to select similar examples as few-shots in ICL can help improve the performance of many LLM inference tasks.While BM25 has become a standard baseline model in the field, it is not without its limitations.Due to its sole reliance on term frequency and document length, this approach may overlook crucial aspects such as semantic meaning and sentence structure, potentially leading to inaccuracies in certain instances.Another drawback is that BM25 lacks the capability for fine-tuning in downstream tasks, making it less competitive compared to neural models which can be fine-tuned and customized for specific downstream tasks.</p>
<p>Sentence Embedding Similarity In this approach, queries and documents are encoded to the same dense embedding space using an off-the-shelf sentence embedding model, and then similarity scores (e.g.cosine similarity) are calculated to rank the most relevant documents for each query.</p>
<p>A rich collection of sentence embedding methodologies exists in the literature.Sentence-BERT (SBERT) (Reimers and Gurevych, 2019a) is a modification of the pretrained BERT network that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings.The effectiveness of SBERT embeddings for demonstration retrieval has been investigated in several works (Rubin et al., 2022;Li and Qiu, 2023b;Wang et al., 2023a), and the results show that retrieving demonstrations based on SBERT embeddings often provides a boost in performance compared to zero-shot or random few-shot selection.In the KATE method (Liu et al., 2022), the authors studied using vanilla RoBERTa (Liu et al., 2019) and finetuned RoBERTa on NLI (Bowman et al., 2015b) and STS-B (Cer et al., 2017) datasets for selecting good demonstrations, and found that the finetuned version on task-related datasets offered further empirical gains.Note that here, the demonstration retriever is not trained for ICL demonstration retrieval based on task-specific data (a topic which we will discuss in Section 5); instead, the retriever is finetuned related tasks to provide a better notion of similarity for the task at hand.So we still categorize it as an off-the-shelf retriever.Shi et al. (2022) extends the use case to cross-lingual few-shot retrieval in the Text to-SQL semantic parsing task, and they use mSBERT (Reimers and Gurevych, 2019b), mUSE (Yang et al., 2019) and mT5 (Xue et al., 2020) as the baseline models for comparison.Other widely used baseline models for demonstration retrieval include E5 base (Wang et al., 2022b), SimCSE (Gao et al., 2021b).Instead of relying on "word matches" as in BM25, these sentence embedding similarity approaches can better capture semantic similarity (for example, synonyms, and related topics), however computationally they might be more expensive.</p>
<p>Pretrained Dual Encoder In the context of demonstration retrieval where the goal is to identify relevant examples for a given query, the query is typically a question, while the examples may contain additional information such as answers, chains of thoughts, supporting knowledge, or even follow different patterns.Therefore, transforming them into a uniform embedding space to calculate relevance might not be the most effective approach.In this case, LLM retrieval architectures such as Dual Encoder that are pretrained on retrieval or question-answering tasks can better grasp the intricate relationships between complex logical concepts and reasoning processes by employing different semantic embeddings for queries and candidates (Li and Qiu, 2023b).In practice, training a dual-encoder can be highly expensive as it typically requires a large training corpus.Fortunately, there are publicly available pretrained retrievers, although not specifically optimized for few-shot retrieval tasks, already demonstrating success in helping LLMs to learn from the selected examples.Luo et al. (2023) studied applying GTR (Ni et al., 2021) to select semantically similar examples as demonstrations, and empirically proved that this approach brought in better performance gain than random fewshots for both PaLM (Chowdhery et al., 2023) and FLAN (Chung et al., 2022) models.GTR is a T5-based dual encoder model that is pretrained on the CommunityQA (Abujabal et al., 2019) and finetuned on the MS Marco dataset (Nguyen et al., 2016).Moreover, Khattab et al. (2022) reported results for employing ColBERTv2 (Santhanam et al., 2021) as the retrieval module in their DEMONSTRATE-SEARCH-PREDICT (DSP) framework for ICL.ColBERTv2 is a state-of-art retrieval model that adopts the late interaction architecture (Khattab and Zaharia, 2020) and is trained on the MS Marco dataset.In the proposed framework, it is used to retrieve both (i) related knowledge during the search stage and (2) top k similar examples as demonstrations.</p>
<p>Fine-tuned Demonstrations Retrievers</p>
<p>Although off-the-shelf retrievers have shown some promise in retrieving demonstrations for LLMs, the retrieved demonstrations given by the off-the-shelf retrievers might not represent the nature of the task and how the task should be solved in general.Therefore, it might lead to sub-optimal performance.Researchers thus have started to explore learning-based methods to further push the boundaries.A typical objective when designing a good demonstration retriever is: if an LLM finds a demonstration useful when being used as an illustrative example, the retriever should be encouraged to rank the demonstration higher.This allows us to train models directly relying on signals from query and output pairs in the task of interest, without human annotations.To develop a demonstration retriever, the majority of approaches utilize current dual encoder models (Karpukhin et al., 2020;Ni et al., 2021).The key variations lie in the methods of gathering training data and formulating training objectives.We will explore these aspects in more detail in the subsequent sections.</p>
<p>Collecting Training Data for Demonstration Retriever</p>
<p>Based on LLMs Signals A popular approach to collecting training examples is to use the supervisory signals from LLMs.In this case, a typical paradigm is to first employ some filtering mechanisms (Cheng et al., 2023) or unsupervised retrievers (e.g.BM25 and SBERT) (Luo et al., 2023) as the initial retriever, this step can help limit the pool size for mining the right training data.Then a scoring LLM, which serves as a proxy for the inference LLM, is used to score each candidate demonstration d.Here the score is defined as s(e) = p(a|d, q) which is the conditional probability of output answer a given the input query q and demonstration d.Another approach is to train a smaller reward model that can provide more fine-grained supervision for dense retrievers.For example, Wang et al. (2023a) proposed to finetune a cross-encoder model serving as a teacher model for training the retriever.</p>
<p>Once a score is obtained, a retriever can be trained that predicts these scores directly (Ye et al., 2023a).Alternatively, the candidate demonstrations can be ranked for each query based on their scores, considering the top-ranked demonstrations as positive examples that help the LLM get to the right answer and the bottom-ranked ones as negative examples that mislead the LLM towards the wrong answers; then a retriever can be trained which separates positive examples from negative examples (Rubin et al., 2022;Cheng et al., 2023;Luo et al., 2023).</p>
<p>There are different strategies for choosing the scoring LLM.Ideally, one uses the inference LLM itself as the scorer in order to perfectly reflect its preferences (Li et al., 2023b;Shi et al., 2022).However, training retrievers requires large amounts of labeled data, and it may be expensive use very large models for labeling.Consequently, for scoring one may gravitate towards utilizing smaller models, especially those within the same model family as the inference LLM (Luo et al., 2023;Cheng et al., 2023;Rubin et al., 2022).</p>
<p>Model-Free One approach to collecting training data for demonstration retriever is to directly measure the similarity between the labels of the candidate demonstrations and the label of the query, and use this similarity as a proxy of the importance of a demonstration (Hu et al., 2022;Poesia et al., 2021).For instance, Hu et al. ( 2022) explored a dialogue context where labels are structured as a sequence of stages.The similarity between a query's label and a demonstration's label is determined by calculating the average F1 scores of these two labels.This method adopts a heuristic approach (i.e.stage changes), presuming that the similarity metric can closely resemble the preference for good demonstrations from an LLM, and it often necessitates domain-specific expertise for design.</p>
<p>Training Objectives</p>
<p>Thus far, we have explored the creation of training data for demonstration retrievers in the context of ICL.We now proceed to examine the commonly used loss functions for training retrievers.</p>
<p>List-wise Ranking Loss The list-wise ranking approach looks at a list of candidate documents for a given query and tries to capture the correct ordering for it.Li et al. (2023b) proposed to inject the ranking signals into the retriever using an approach inspired by LambdaRank (Burges, 2010).More formally, given each query q, they first rank all l candidate documents according to their relevant scores S = {s(d i )} l i=1 , according to which the associated ranking R = {r(d i )} l i=1 is computed.Then the loss function is defined as follows:
L listwise = di,dj max 0, 1 r(d i ) âˆ’ 1 r(d j ) * log(1 + e sim(qi,dj )âˆ’sim(qi,di) )
where sim(q, d) is the relavance between a candidate demonstration d and the input q.In the listwise ranking objective, retriever can benefit from the full ranking of the candidate set to make accurate predictions for the most relevant demonstrations.However, obtaining the full ranking list and calculating the loss function on top of it might be very expensive and time-consuming.Additionally, the model is trained to discern the relative preferences between examples without explicitly determining whether an example can serve as an absolute good demonstration.</p>
<p>InfoNCE Loss Another widely adopted training procedure is contrastive learning using the In-foNCE loss (Rubin et al., 2022;Cheng et al., 2023;Luo et al., 2023).When positive and negative examples can be correctly identified, InfoNCE loss is an effective loss function because it can take advantage of the supervisory labels to produce a representation that sets apart the useful examples for demonstration retrieval.In this approach, each training instance is given in the form of
&lt; q i , d + i , d âˆ’ i,1 , ...d âˆ’ i,k &gt;.
Here d + i is a selected positive example concerning the input q i , and the negative examples consist of one hard negative example d âˆ’ i,1 and k random examples from the other instances in the same mini-batch.Then the typical contrastive loss can be defined as
L cont = L(q i , d + i , d âˆ’ i,1 , ...d âˆ’ i,k ) = âˆ’ log e sim(qi,d + i ) e sim(qi,d + i ) + k j=1 e sim(qi,d âˆ’ i,j )
The
L distill = KL(p LLM ||p retriever ) = K k=1 p LLM (d k )log p LLM (d k ) p retriever (d k )
Multiple Objectives In Wang et al. (2023a), the authors proposed to train the demonstration retriever model with combined objectives: (1) knowledge distillation from the trained reward model which can capture the preferences of LLMs over the retrieved candidates (2) InfoNCE-based contrastive loss to incorporate the in-batch negatives.More specifically, the resulting loss function is as follows:</p>
<p>L combined = Î±L cont + L distill Here Î± is a constant that controls the relative importance of the two losses.They claimed that with the multi-objective function, both the absolute scores and supervised signals are taken into consideration.Li et al. (2023b) trains a universal retriever with both list-wise ranking loss and the InfoCNE loss.</p>
<p>Iterative Training Regarding training strategies, most research efforts have centered on fine-tuning a single retriever.Wang et al. (2023a) and Li et al. (2023b) instead proposed to iterate the retriever model multiple times.More specifically, the retriever trained in iteration i will be employed to retrieve a new set of candidates for the subsequent iteration i + 1.Such an iterative training approach allows progressively improving retriever quality by mining better positive and hard negative examples at each iteration.</p>
<p>Diversity Training</p>
<p>The Determinantal Point Process model (Alex Kulesz, 2012) defines a probability distribution over all the combinations of candidate demonstrations, giving high probability to subsets that contain relevant and diverse items (Levy et al., 2022).It models diversity by incorporating cross-candidate similarity scores, and models similarity via a per-candidate relevance score, i.e., a similarity score between a candidate and the test query.In addition to using DPP directly (Levy et al., 2022), Ye et al. (2023a) also fine-tuned a DPP model and demonstrated meaningful improvements over pure similarity-based methods.</p>
<p>Re-ranker Training It is not uncommon that people adopt a two-stage retriever-reranker architecture for ICL retrieval in the literature to further improve the exemplar selection process (Shi et al., 2022).Generally, a dual-encoder-based retriever can encode query and candidate documents for fast indexing and searching, but neglect the finer-grained token-level interactions.Cross-encoder-based reranker, on the other hand, can capture the subtle relationship but is time-consuming.We can benefit from both of these methods by chaining two methods together.In the first stage, a retriever model is used to quickly select the top N examplers to limit the candidate pool of interest, then a reranker reranks the retrieved N examplars and uses the top K exemplars to construct a prompt.Sigmoid cross-entropy loss is typically used for training the reranker.Lu et al. (2022a) also utilizes a similar structure as the reranker to select the demonstrations from random candidates.The reranker is trained using reinforcement learning.</p>
<p>Summary</p>
<p>Here, we summarize the advantages and disadvantages of various retriever models.The off-theshelf retrievers are easy to use without any downstream task finetuning and typically demonstrate stronger performance than random demonstrations.One exception is in commonsense reasoning tasks where Zhang et al. (2022b) and Ye et al. (2023a) found that for these tasks, random demonstrations are consistently better than retrieval-based method.Cheng et al. (2023) also show that retrieved demonstrations harm commonsense reasoning and coreference resolution tasks.Among the three categories of off-the-shelf retrievers, sparse retrievers such as BM25 are more index-efficient.This feature becomes particularly valuable when dealing with large volumes of demonstrations and limited hardware memory, making BM25 a preferable choice under such circumstances.In contrast, sentenceembedding similarity-based methods and dual-encoder-based retrieval systems, which are trained on language tasks, excel in capturing more semantically focused retrieval.Regarding performance, Luo et al. (2023) compared BM25 with dual encoder (GTR) across 5 tasks, and they found that the average performance of these two is very similar (within 0.5% difference), and BM25 outperformed the dual encoder in some tasks and vice versa.In another study, Ye et al. (2023a) observed a similar trend highlighting that no single retriever consistently outperforms others across different tasks.Both Rubin et al. (2022) and Li et al. (2023b) found that BM25 is better than SBERT on semantic parsing tasks, while Li et al. (2023b) found that SBERT is better than BM25 on sentiment analysis tasks.Nevertheless, retrievers that are fine-tuned demonstrate superior performance compared to their off-the-shelf counterparts.The main drawback of fine-tuned retrievers lies in the high cost of obtaining training data.Additionally, the common practice of employing task-specific retrievers complicates the system and limits its generalizability.Li et al. (2023b) proposed to train a universal retriever that shows stronger performance than task-specific demonstration retriever (e.g.EPR (Rubin et al., 2022)) on most of the tasks.</p>
<p>Applications</p>
<p>The effectiveness of retrieval-based ICL has been showed in four categories of tasks.1).natural language understanding, 2-reasoning, 3-knowledge-based QA, and 4-Text generation.We discuss each category below.</p>
<p>Natural language understanding tasks that benefit from RetICL include sentiment analysis (SA) (Socher et al., 2013;Zhang et al., 2015;Go et al., 2009), paraphrase detection (PD) (Dolan et al., 2004;Zhang et al., 2019), reading comprehension (RC) (Rajpurkar et al., 2016;Khashabi et al., 2018;Clark et al., 2019;Khashabi et al., 2018;Clark et al., 2019;Mihaylov et al., 2018), and natural language inference (NLI) (Williams et al., 2018;Wang et al., 2018;Bowman et al., 2015a;De Marneffe et al., 2008).Specially, RetICL shows noticeable improvements on SA and NLI tasks (Liu et al., 2022;Ye et al., 2023a).</p>
<p>Reasoning tasks tasks that benefit from RetICL include mathematical reasoning (Cobbe et al., 2021;Lu et al., 2022a;Ling et al., 2017), commonsense reasoning (CSR) (Talmor et al., 2019;Zellers et al., 2019;Bisk et al., 2020;Roemmele et al., 2011),and ethical Reasoning (Jiang et al., 2021).Such tasks are usually accompanied by CoT, but Zhang et al. (2022b) found that CoT does not help that much for the commonsense reasoning task.Many works have shown that retrieval-based methods are worse than random demonstrations on commonsense reasoning tasks (e.g.CMSQA).A simple similarity-based retrieval method does not show significant improvement in mathematical reasoning tasks, and Zhang et al. (2022b) shows that diversity is important for mathematical reasoning tasks.The iterative retrieval strategy shows the most significant improvement on mathematical reasoning tasks Scarlatos and Lan (2023).</p>
<p>In Knowledge-based QA, external knowledge is required to answer the question (Berant et al., 2013;Kwiatkowski et al., 2019;Joshi et al., 2017;Clark et al., 2018).To tackle such tasks, the state-ofthe-art systems usually retrieve relevant passages that might contain the answer to the question, and then feed such passages and questions together to a language model to generate the answer.Liu et al. (2022) shows that using retrieval-based ICL (sentence semantic similarity-based retriever with GPT-3) is almost comparable to a fine-tuned method.Ye et al. (2023a) shows that BM25 achieve 10+% improvement on open-domain QA.</p>
<p>Text generation tasks that benefit from RetICL includes code generation (CodeGen) (Zelle and Mooney, 1996;Lin et al., 2018), semantic parsing (SP) (Wolfson et al., 2020;Li et al., 2021;Andreas et al., 2020), text-to-SQL (Shi et al., 2022), Table -to-text (Table2Text) generation (Parikh et al., 2020); Data-to-Text (D2T) (Nan et al., 2021;DuÅ¡ek et al., 2019).Rubin et al. (2022) shows that the retrieved demonstrations significantly outperform random demonstrations (e.g.BM25 is 25+% better than random, and EPR is 30% better than random).</p>
<p>Apart from different types of tasks, Hongjin et al. (2022) shows that in scenarios with limited training data, RetICL outperforms fine-tuning a model on such sparse data.Furthermore, leveraging data from a high-resource domain can enhance performance in a low-resource domain, as seen in cross-lingual contexts (Shi et al., 2022;Nie et al., 2022;Cheng et al., 2023).</p>
<p>7 Discussion of Future Direction Retrieve Demonstrations From Raw Text Much research assumes the availability of annotated samples that can be utilized as a retrieval corpus.Yet, when faced with a novel task, it is often the case that no such training dataset exists.While there are preliminary efforts to create pseudo demonstrations from open-ended corpora like Wikipedia (Lyu et al., 2022), the proposed method is restricted to classification tasks and the label to the demonstrations are randomly assigned.A potential approach to obtain pseudo demonstrations for generation tasks is Wan et al. (2023), where they assume a set of unlabelled queries available (without ground truth labels), and use LLMs to generate chain-of-thoughts and answers and then apply self-consistency (Wang et al., 2022c) to select high-quality demonstrations to form pseudo demonstrations pool.Employing this method of generating answers with sentences retrieved from a free-form corpus could potentially create high-quality pseudo demonstrations.</p>
<p>Choosing the Type of Retriever Another critical consideration in this domain is the selection of the type of retriever.The options range from neural model retrievers and sparse retrievers to template-based retrievers.The objectives range from similarity and diversity to complexity.Current research implies that no single type has emerged as universally superior.This leads to an important open question: Is there a potential for a specific type of retriever to consistently yield superior performance across a variety of tasks?Investigating this will be a key direction for future research.</p>
<p>Retriever Training Methods In Section 5, we explore various methods for training a retriever to search demonstrations.These methods largely depend on using an LLM to identify positive and negative demonstrations for a given question.This approach, while innovative, comes with significant computational demands.Moreover, the ambiguity in choosing what constitutes a positive or negative demonstration raises concerns about the quality of the training data for the retriever (Hashimoto et al., 2023).Addressing these challenges is crucial for the development of more efficient and reliable retriever training methods.</p>
<p>Active Demonstration Retrieval Much of the current research is based on a static framework where the retrieval corpus remains constant.In practical situations, input distributions may change over time and models may come across new instances.In these cases, one may like to keep updating the retrieval corpus based on the new incoming queries, but since the labels for the new samples may not be available, a selection strategy is needed to select a representative subset of the incoming examples for annotation so they can be added to the retrieval corpus.This problem is reminiscent of the well-studied active learning problem.Zhang et al. (2022a) have studied how to actively select examples with unlabelled data, however, this study is not based on the retrieval setting.The combination of active learning and RetICL can be an interesting future direction.</p>
<p>Retrieved Demonstrations for Small LM Most of the existing work focus on LLMs (more than 100 B parameters).Research into small LMs has recently received more attention due to its inference efficiency.Examining and improving the ICL capabilities of small LMs by better (ideally optimal) demonstration selection is an interesting direction to explore.</p>
<p>Theoretical Understanding of Why Are Similar Demonstrations Better Demonstrations?While lots of research has shown that similar demonstrations are better than random, it is still unknown why similar demonstrations are helpful.Ye et al. (2023a) found that the generation task is more beneficial compared to the classification task, and one possible reason is that the retrieved demonstrations could have similar answers to the input question and the model might just copy the answers.However, such an explanation does not illustrate why retrieved demonstrations are better than random ones on classification tasks.There are some hypotheses that similar demonstrations help locate the knowledge in the LLMs or based on the hypothesis of LLM conduct implicit gradient descent, then the similar demonstrations provide a more useful training signal to the input query.The research on why ICL works can help understand why similar demonstrations are better than random ones.</p>
<p>Fine-tuning For RetICL Few-shot Learning Most of the existing research utilizes a frozen LLM as the few-shot learner in the RetICL framework.As demonstrated by Gao et al. (2021a); Izacard et al. (2023), fine-tuning LLMs on a few-shot learning task can be a promising approach to enhancing LLM performance during inference.An intriguing avenue could involve adapting the fine-tuning strategy from open-domain question answering models to RetICL (Lewis et al., 2020;Guu et al., 2020).</p>
<p>RetICL for Vision and Language Models Vision and language models (VL) have demonstrated proficiency as few-shot learners (Alayrac et al., 2022;Awadalla et al., 2023;Li et al., 2023a), and researchers have integrated retrieval augmented generation methods with VL models (Luo et al., 2021;Gao et al., 2022;Yasunaga et al., 2023).In lieu of employing random demonstrations, Yang et al. (2024) leverages a retriever to select demonstrations for image captioning tasks.Additionally, Peng et al. (2023) propose an ICD-LM to generate demonstrations for a given query, with each demonstration being represented by a token.The effectiveness of this method is evidenced by its performance on image captioning and vision question answering tasks.Despite the increasing significance of vision and language generation models in real-world applications, research on RetICL for VL models remains relatively underexplored.</p>
<p>Conclusion</p>
<p>This survey concentrates on few-shots In-Context Learning (ICL) using retrieved examples for large language models, a key aspect of Retrieval-Augmented Generation (RAG).We outline various retrieval strategies, diverse retrieval models, retrieval pools, techniques for training demonstration retrievers, and applications.Based on the comprehensive understanding of current trends, we suggest several promising future paths for enhancing the efficacy and functionality of this approach.</p>
<p>p(s) = p(w 1 )p(w 2 | w 1 ) . . .p(w n | w 1 , . . ., w nâˆ’1 ) = n k=1 p(w k | w 1 , . . ., w kâˆ’1 )</p>
<p>-Domain In this setting, an in-domain training set, independently and identically distribution (IID) with the test queries, is available and serves as the retrieval corpus.Most existing work take the full training set as the corpus.However, to be more annotation efficient, Hongjin et al. (2022) uses only a subset M of the training set N which includes the most representative and diverse ones, where |M | &lt;&lt; |N |.One question that remains unanswered from the work of Hongjin et al. (2022) is how the predictive performance is affected as a function of retrieving from a subset M instead of the entire training set N .While there is no follow-up work to answer this question, the closest comparison we find is the results in Ye et al. (2023a) where a similar setup as Hongjin et al. (2022) is used except that they use the entire training set as the retrieval corpus, and report lower performance on the SST-5 dataset (compare the Figure 3 in Hongjin et al. (2022) and Table3in(Ye et al., 2023a)).</p>
<p>random negative examples from the same mini-batch are called in-batch negatives.They are typically selected from both the positive examples and hard negative examples of other instances.Distillation by KL Divergence Ye et al. (2023a) claims that although the InfoNCE loss has been found effective in training demonstration retrievers and can learn which examples might be superior to others, it has the same treatment for all negative examples and the predicted scores from LLM are not fully utilized.As an alternative to train a demonstration retriever using positive and negative examples,Shi et al. (2022) proposed to train the retriever by directly distilling the LLM's scoring function.More specifically, the retriever model is designed to produce ranking scores that match the usefulness of a demonstration to help with the LLM inference; this is done by minimizing the KL-divergence between the top K examples score distribution from scoring LLM and the ranking score distribution produced by the retriever</p>
<p>AcknowledgeWe would like to thank Siva Reddy, Suzanna Sia, Andrew Drozdov and Yang Xu for recommending additional references for the initial draft.Special thanks are extended to Andrew Drozdov for suggesting the direction of future work on fine-tuning for few-shot learning, and to Xu Yang for suggesting RetICL for VL models.
ComQA: A community-sourced dataset for complex factoid question answering with paraphrase clusters. Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum, 10.18653/v1/N19-1027Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>In-context examples selection for machine translation. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad, arXiv:2212.024372022arXiv preprint</p>
<p>In-context examples selection for machine translation. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>Determinantal point processes for machine learning. Ben Taskar, Alex Kulesz, Foundations and TrendsÂ® in Machine Learning. 20125</p>
<p>Task-oriented dialogue as dataflow synthesis. Jacob Andreas, John Bufe, David Burkett, Charles ChenJr, Josh Clausman, Jean Crawford, Kate Crim, Jordan Deloach, Leah Dorner, Jason Eisner, 2020Transactions of the Association for Computational Linguistics8</p>
<p>Openflamingo: An open-source framework for training large autoregressive vision-language models. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Sagawa, arXiv:2308.013902023arXiv preprint</p>
<p>A neural probabilistic language model. Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, Advances in neural information processing systems. 200013</p>
<p>Semantic parsing on freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, Thirty-Fourth AAAI Conference on Artificial Intelligence. 2020</p>
<p>A large annotated corpus for learning natural language inference. Samuel Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015a</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)2015b</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>From ranknet to lambdarank to lambdamart: An overview. J C Christopher, Burges, MSR-TR-2010-822010Microsoft ResearchTechnical Report</p>
<p>Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. M Daniel, Mona T Cer, Eneko Diab, IÃ±igo Agirre, Lucia Lopez-Gazpio, Specia, CoRR, abs/1708.000552017</p>
<p>Improving in-context few-shot learning via self-supervised training. Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, Zornitsa Kozareva, arXiv:2205.017032022arXiv preprint</p>
<p>Uprise: Universal prompt retrieval for improving zero-shot evaluation. Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang, arXiv:2303.085182023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.100442019arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, ArXiv, abs/1803.054572018</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. Bhavana Dalvi, Oyvind Tafjord, Peter Clark, arXiv:2204.130742022arXiv preprint</p>
<p>Finding contradictions in text. Marie-Catherine De Marneffe, Anna N Rafferty, Christopher D Manning, Proceedings of acl-08: Hlt. acl-08: Hlt2008</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, arXiv:2305.14314Qlora: Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. Bill Dolan, Chris Quirk, Chris Brockett, Proceedings of the 20th international conference on Computational Linguistics. the 20th international conference on Computational Linguistics2004350</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael SchÃ¤rli, Ekin AkyÃ¼rek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew Mccallum, Donald Metzler, arXiv:2310.14408Parade: Passage ranking using demonstrations with large language models. 2023arXiv preprint</p>
<p>Semantic noise matters for neural natural language generation. OndÅ™ej DuÅ¡ek, David M Howcroft, Verena Rieser, Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language Generation2019</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.00720Complexity-based prompting for multi-step reasoning. 2022arXiv preprint</p>
<p>Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering. Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, Prem Natarajan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Ambiguity-aware in-context learning with large language models. Lingyu Gao, Aditi Chaudhary, Krishna Srinivasan, Kazuma Hashimoto, Karthik Raman, Michael Bendersky, arXiv:2309.079002023arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021a1</p>
<p>SimCSE: Simple contrastive learning of sentence embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021b</p>
<p>What can transformers learn in-context? a case study of simple function classes. Shivam Garg, Dimitris Tsipras, Percy S Liang, Gregory Valiant, Advances in Neural Information Processing Systems. 202235</p>
<p>Twitter sentiment classification using distant supervision. Alec Go, Richa Bhayani, Lei Huang, 2009. 20091StanfordCS224N project report</p>
<p>Demystifying prompts in language models via perplexity estimation. Srini Hila Gonen, Terra Iyer, Noah A Blevins, Luke Smith, Zettlemoyer, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Demix layers: Disentangling domains for modular language modeling. Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, Luke Zettlemoyer, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang, International conference on machine learning. PMLR2020</p>
<p>Take one step at a time to know incremental utility of demonstration: An analysis on reranking for few-shot in-context learning. Kazuma Hashimoto, Karthik Raman, Michael Bendersky, arXiv:2311.096192023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Long short-term memory. Sepp Hochreiter, JÃ¼rgen Schmidhuber, Neural computation. 981997</p>
<p>Selective annotation makes language models better few-shot learners. Jungo Su Hongjin, Chen Henry Kasai, Weijia Wu, Tianlu Shi, Jiayi Wang, Rui Xin, Mari Zhang, Luke Ostendorf, Noah A Zettlemoyer, Smith, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2021</p>
<p>In-context learning for few-shot dialogue state tracking. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A Smith, Mari Ostendorf, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, Journal of Machine Learning Research. 242512023</p>
<p>Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Le Ronan, Jenny Bras, Jesse Liang, Keisuke Dodge, Maxwell Sakaguchi, Jon Forbes, Saadia Borchardt, Gabriel, arXiv:2110.07574Can machines learn morality? the delphi experiment. 2021arXiv preprint</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020Association for Computational Linguistics8</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>N-gram language models. James H Jurafsky, Dan ; Martin, Speech and Language Processing. 20213rd ed.</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Understanding finetuning for factual knowledge extraction from language models. Mehran Kazemi, Sid Mittal, Deepak Ramachandran, arXiv:2301.112932023aarXiv preprint</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. Najoung Seyed Mehran Kazemi, Deepti Kim, Xin Bhatia, Deepak Xu, Ramachandran, ACL. 2023b</p>
<p>Looking beyond the surface:a challenge set for reading comprehension over multiple sentences. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, Dan Roth, Proceedings of North American Chapter. North American Chapterthe Association for Computational Linguistics2018</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>Colbert: Efficient and effective passage search via contextualized late interaction over BERT. Omar Khattab, Matei Zaharia, CoRR, abs/2004.128322020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2022</p>
<p>Determinantal point processes for machine learning. Alex Kulesza, Ben Taskar, Foundations and TrendsÂ® in Machine Learning. 20125</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Diverse demonstrations improve in-context compositional generalization. Itay Levy, Ben Bogin, Jonathan Berant, arXiv:2212.068002022arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-Tau Yih, Tim RocktÃ¤schel, Advances in Neural Information Processing Systems. 202033</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu, arXiv:2306.05425Mimic-it: Multi-modal in-context instruction tuning. 2023aarXiv preprint</p>
<p>Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, Yashar Mehdad, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>Self-prompting large language models for zero-shot open-domain qa. Junlong Li, Zhuosheng Zhang, Hai Zhao, arXiv:2212.086352022arXiv preprint</p>
<p>Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, arXiv:2305.04320Xiaoling Wang, and Xipeng Qiu. 2023b. Unified demonstration retriever for in-context learning. arXiv preprint</p>
<p>Finding supporting examples for in-context learning. Xiaonan Li, Xipeng Qiu, arXiv:2302.135392023aarXiv preprint</p>
<p>Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts. Xiaonan Li, Xipeng Qiu, arXiv:2305.051812023barXiv preprint</p>
<p>Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models. Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna, Ren, arXiv:2005.006832020arXiv preprint</p>
<p>Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system. Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, Michael D Ernst, Proceedings of the Eleventh International Conference on Language Resources and Evaluation. the Eleventh International Conference on Language Resources and Evaluation2018. 2018</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, Weizhu Chen, The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2022. DeeLIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. 2023arXiv preprint</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, The Eleventh International Conference on Learning Representations. 2022a</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2022b1</p>
<p>Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao, arXiv:2305.14128Dr. icl: Demonstration-retrieved in-context learning. 2023arXiv preprint</p>
<p>Weakly-supervised visualretriever-reader for knowledge-based question answering. Man Luo, Yankai Zeng, Pratyay Banerjee, Chitta Baral, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Z-icl: Zero-shot in-context learning with pseudo-demonstrations. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi, arXiv:2212.098652022arXiv preprint</p>
<p>Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, Bingzhe Wu, arXiv:2303.13217Fairness-guided few-shot prompting for large language models. 2023arXiv preprint</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang, arXiv:2201.06009Memory-assisted prompt editing to improve gpt-3 after deployment. 2022arXiv preprint</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, EMNLP. 2018</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781Efficient estimation of word representations in vector space. 2013arXiv preprint</p>
<p>In-context learning for text classification with many labels. Aristides Milios, Siva Reddy, Dzmitry Bahdanau, Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP. the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP2023</p>
<p>Metaicl: Learning to learn in context. Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022a</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022barXiv preprint</p>
<p>Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, Besmira Nushi, arXiv:2310.07088Diversity of thought improves reasoning abilities of large language models. 2023arXiv preprint</p>
<p>Dart: Open-domain structured data record to text generation. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Ms marco: A human generated machine reading comprehension dataset. choice. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, 2016. 2640660</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo HernÃ¡ndez Ãbrego, Ji Ma, Y Vincent, Yi Zhao, Keith B Luan, Ming-Wei Hall, Chang, arXiv:2112.07899Large dual encoders are generalizable retrievers. 2021arXiv preprint</p>
<p>Cross-lingual retrieval augmented prompt for low-resource languages. Ercong Nie, Sheng Liang, Helmut Schmid, Hinrich SchÃ¼tze, ArXiv, abs/2212.096512022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Totto: A controlled table-to-text generation dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Yingzhe Peng, Xu Yang, Haoxuan Ma, Shuo Xu, Chi Zhang, Yucheng Han, Hanwang Zhang, arXiv:2312.10104Icd-lm: Configuring vision-language in-context demonstrations by language modeling. 2023arXiv preprint</p>
<p>Fabio Petroni, Tim RocktÃ¤schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066Language models as knowledge bases?. 2019arXiv preprint</p>
<p>Synchromesh: Reliable code generation from pre-trained language models. Gabriel Poesia, Alex Polozov, Ashish Vu Le, Gustavo Tiwari, Christopher Soares, Sumit Meek, Gulwani, International Conference on Learning Representations. 2021</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250Squad: 100,000+ questions for machine comprehension of text. 2016arXiv preprint</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019a</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019barXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and TrendsÂ® in Information Retrieval. 342009</p>
<p>Choice of plausible alternatives: An evaluation of commonsense causal reasoning. Melissa Roemmele, Cosmin Adrian Bejan, Andrew S Gordon, 2011 AAAI Spring Symposium Series. 2011</p>
<p>Two decades of statistical language modeling: Where do we go from here?. Ronald Rosenfeld, Proceedings of the IEEE. 8882000</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Colbertv2: Effective and efficient retrieval via lightweight late interaction. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia, CoRR, abs/2112.014882021</p>
<p>Detecting formal thought disorder by deep contextualized word representations. Justyna Sarzynska-Wawer, Aleksander Wawer, Aleksandra Pawlak, Julia Szymanowska, Izabela Stefaniak, Michal Jarkiewicz, Lukasz Okruszek, Psychiatry Research. 3041141352021</p>
<p>Reticl: Sequential retrieval of in-context examples with reinforcement learning. Alexander Scarlatos, Andrew Lan, arXiv:2305.145022023arXiv preprint</p>
<p>Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing. Peng Shi, Rui Zhang, He Bai, Jimmy Lin, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, arXiv:2010.159802020arXiv preprint</p>
<p>In-context learning as maintaining coherency: A study of onthe-fly machine translation using large language models. Suzanna Sia, Kevin Duh, Proceedings of Machine Translation Summit XIX. Research Track, Machine Translation Summit XIX20231</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang, arXiv:2109.07154Can language models be biomedical knowledge bases?. 2021arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Transformers learn in-context by gradient descent. Johannes Von, Oswald , Eyvind Niklasson, Ettore Randazzo, JoÃ£o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov, International Conference on Machine Learning. PMLR2023</p>
<p>Better zero-shot reasoning with self-adaptive prompting. Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan O Arik, Tomas Pfister, arXiv:2305.141062023arXiv preprint</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, arXiv:2212.100012022aarXiv preprint</p>
<p>Text embeddings by weakly-supervised contrastive pre-training. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, 2022b</p>
<p>Learning to retrieve in-context examples for large language models. Liang Wang, Nan Yang, Furu Wei, arXiv:2307.071642023aarXiv preprint</p>
<p>Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. Xinyi Wang, Wanrong Zhu, William Yang, Wang , arXiv:2301.119162023barXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022c</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>prompting language models improves quoting from pretraining data. Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme, arXiv:2305.132522023arXiv preprint</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter. Long Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20181</p>
<p>Break it down: A question understanding benchmark. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, Jonathan Berant, Transactions of the Association for Computational Linguistics. 82020</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2021</p>
<p>Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, CoRR, abs/2010.11934</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Exploring diverse incontext configurations for image captioning. Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng, CoRR, abs/1907.04307Advances in Neural Information Processing Systems. Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo HernÃ¡ndez Ãbrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil, 2024. 201936Multilingual universal sentence encoder for semantic retrieval</p>
<p>Retrieval-augmented multimodal language modeling. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, International Conference on Machine Learning. PMLR2023</p>
<p>Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, Lingpeng Kong, arXiv:2302.05698Compositional exemplars for in-context learning. 2023aarXiv preprint</p>
<p>Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, Minjoon Seo, arXiv:2302.14691-context instruction learning. 2023barXiv preprint</p>
<p>An overview of overfitting and its solutions. Xue Ying, Journal of physics: Conference series. IOP Publishing2019116822022</p>
<p>Generate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Quan Yuan, Mehran Kazemi, Xin Xu, Isaac Noble, Vaiva Imbrasaite, Deepak Ramachandran, arXiv:2308.15299Tasklama: probing the complex task understanding of language models. 2023arXiv preprint</p>
<p>Learning to parse database queries using inductive logic programming. M John, Raymond J Zelle, Mooney, Proceedings of the national conference on artificial intelligence. the national conference on artificial intelligence1996</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, arXiv:2308.10792Instruction tuning for large language models: A survey. 2023arXiv preprint</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in neural information processing systems. 201528</p>
<p>Active example selection for in-context learning. Yiming Zhang, Shi Feng, Chenhao Tan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022a</p>
<p>PAWS: Paraphrase Adversaries from Word Scrambling. Yuan Zhang, Jason Baldridge, Luheng He, Proc. of NAACL. of NAACL2019</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2022b</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6810 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6810</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6810</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-265128609</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.06158v1.pdf" target="_blank">Language Models can be Logical Solvers</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning datasets demonstrate that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6810.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6810.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGIPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGIPT (Solver-derived Instruction-tuned Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language model fine-tuned to imitate the internal, stepwise reasoning actions of a deductive symbolic solver by training on solver-derived instruction-tuning data so it can directly generate implied facts from natural-language logical contexts and answer deductive queries without calling an external solver at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LOGIPT (fine-tuned Vicuna/CodeLlama variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LMs (Vicuna-13B or CodeLlama-13B) fine-tuned on a dataset constructed from solver (pyke) reasoning traces; trained to output predicate definitions, known facts/rules, solver-like step actions (bind/unbind, fail & backtrack) and final implied facts, enabling direct deduction from NL contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (instruction-tuned to emulate symbolic solver reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Solver-derived instruction-tuning dataset constructed from public deductive reasoning datasets (ProofWriter, PrOntoQA) by revealing and refining pyke solver internal reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Model is fine-tuned to emulate symbolic-solver stepwise reasoning (solver-trace distillation); it generates all implied facts from premises and rules then matches the query (OWA/CWA handling).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>pyke expert system was used offline to generate solver traces and implied facts for constructing the instruction-tuning dataset, but LOGIPT does not call an external solver at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Public deductive reasoning benchmarks containing NL contexts with facts/rules and queries under open-world (ProofWriter) and closed-world (PrOntoQA subset) assumptions for evaluating deductive inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical reasoning / logical deduction (generate implied facts and decide query truth: True/False/Unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (multiple-choice correctness of True/False/Unknown or True/False depending on dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Outperforms state-of-the-art solver-augmented baseline LogicLM (GPT-4) by +9.84 percentage points on ProofWriter and by +13.20 percentage points on PrOntoQA (absolute improvement); (exact per-model accuracy values reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Significant positive improvement vs solver-augmented LMs (LogicLM with GPT-4) and versus few-shot prompting on closed-source LLMs; also surpasses or matches chain-of-thought GPT-4 on some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning LMs on explicit, solver-derived stepwise reasoning traces lets relatively small (13B) open-source models act as reliable deductive solvers and surpass solver-augmented pipelines that rely on NL->SL parsing plus external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires filtered, syntax-valid examples generated by a symbolic solver (pyke) for training; mixing datasets with different reasoning assumptions (OWA vs CWA) without careful reformatting degrades performance; depends on quality/coverage of solver traces and training data; not claimed to generalize to all theorem proving domains beyond the studied deductive benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6810.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solver-augmented LMs (general pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solver-augmented Language Models (LM -> Symbolic Language -> External Solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where an LM parses NL logical problems into a symbolic language (e.g., Prolog-style) and an external deterministic symbolic solver (theorem prover/expert system) executes precise reasoning to produce the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Solver-augmented LMs (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs (various) are used as NL->SL parsers (often via instruction + few-shot examples) and external symbolic solvers (e.g., pyke) perform deterministic reasoning over the parsed facts and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + external symbolic solver (neuro-symbolic pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not centrally trained as a single model in this paper; relies on existing NL-to-SL prompts and in-context examples. Evaluation used public deductive datasets (ProofWriter, PrOntoQA) for end-to-end parsing+solving.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Parsing NL to symbolic language (Prolog-like) via LM, then invoking deterministic symbolic solver to derive implied facts and answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>External symbolic solver (pyke expert system used in experiments) receives parsed symbolic facts/rules and deterministically computes implied facts / query truth values.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same deductive reasoning benchmarks used to test NL->SL parsing success and solver execution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive reasoning via symbolic execution; NL-to-SL parsing + symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Parsing successful rate (percentage of executable logical formulations) and final answer accuracy after solver execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Parsing success can be low; example: Vicuna-13B parsing success on ProofWriter reported as ~17% in preliminary experiments. End-to-end accuracy depends heavily on parsing success; state-of-the-art solver-augmented baseline LogicLM (GPT-4) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Solver-augmented approaches yield faithful answers when parsing is correct, but overall end-to-end performance suffers when NL->SL parsing fails; LOGIPT (this paper) outperforms these pipelines by avoiding parsing errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deterministic symbolic solvers guarantee correctness given executable input, but the brittle NL->SL parsing stage by LMs is a major failure point; parser error rates can severely degrade system performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High sensitivity to NL-to-SL parsing errors (parsing failures lead to solver execution failure and no answer); relies on correct, executable symbolic representations; requires explicit grammar/instructions and good in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6810.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLM (solver-augmented LM from Pan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative solver-augmented method that uses LLMs (e.g., ChatGPT/GPT-4) to parse NL into symbolic inputs and then invokes an external symbolic solver for faithful deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicLM (with GPT-4 / ChatGPT as parser)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline using closed-source LLMs for NL->SL parsing combined with external symbolic solvers to compute answers deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (closed-source) + external symbolic solver</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not trained in this work; evaluated as a baseline using public deductive datasets (ProofWriter, PrOntoQA) following prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>NL->SL parsing by LM then symbolic reasoning by solver.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses an external logical solver (as in the general solver-augmented pipeline) to perform deduction on parsed symbolic inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Deductive reasoning datasets with NL contexts, facts/rules, and queries under OWA/CWA assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical reasoning (faithful symbolic solving)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used as state-of-the-art baseline; LOGIPT reports absolute improvements of +9.84% on ProofWriter and +13.20% on PrOntoQA over LogicLM (GPT-4) in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LogicLM (GPT-4) is the baseline solver-augmented approach; LOGIPT substantially outperforms it on the two evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Solver-augmented baselines are strong when parsing is successful, but they are still outperformed by LOGIPT which sidesteps the brittle parsing step.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inherits brittle dependency on NL->SL parsing; if parsing fails or produces syntax-invalid formulations, solver cannot execute and system fails.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6810.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-context prompting technique that elicits step-by-step intermediate reasoning from LMs by providing few-shot examples of detailed reasoning chains before asking for an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (applied to various LMs such as GPT-4, ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt engineering technique; does not change model weights but elicits internal multi-step reasoning through example chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with prompting technique (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Few-shot demonstrations / in-context examples (used across experiments for fair comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompt-induced stepwise natural-language reasoning (chain-of-thought) to produce intermediate steps and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as prompting baseline on deductive reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive reasoning via NL chain-of-thought explanations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT on GPT-4 achieved competitive results on some settings (PrOntoQA) and in one case comparable to LOGIPT; otherwise, CoT and standard prompting often performed near random for many open-source models in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>On PrOntoQA, GPT-4 with CoT achieved results comparable to LOGIPT in one reported experiment; generally, CoT helps closed-source models but many open-source LMs still performed poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT can improve reasoning for large closed-source models but does not solve the NL-to-SL parsing brittleness and often fails on complex deductive benchmarks for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Produces natural-language reasoning which can be ambiguous/hallucinated and not guaranteed faithful to logical proof; for many models CoT did not yield reliable performance on strict deductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6810.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard (few-shot) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard few-shot in-context prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompting approach that supplies a few examples in-context for direct answering or NL-to-SL parsing, without eliciting explicit chain-of-thought steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Standard few-shot prompting (applied to multiple LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In-context learning by showing few demonstrations; expects model to follow formatting instructions to parse or answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Few-shot in-context examples used consistently across baseline evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Direct in-context mapping from NL to answer (or to symbolic parse) using limited examples.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used to gauge performance of prompting baselines on deductive reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive reasoning via direct few-shot prompting / NL->SL parsing where prompted</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Many open-source LMs with standard prompting produced accuracies near random; e.g., CodeLlama-13B-Base had 0.00 accuracy with standard prompting on these tasks as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Standard prompting often underperforms CoT and much worse than LOGIPT and solver-augmented methods when parsing fails.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Few-shot standard prompting is insufficient for reliable strict deductive reasoning in many LMs; some closed-source LMs also showed weak standard prompting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Highly sensitive to model capability to follow demonstrations; often fails to format correct symbolic parses or answer formats, making it unreliable for strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6810.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-13B (vicuna-13b-v1.5-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source chatbot model derived by fine-tuning LLaMA-2 on ShareGPT conversation data, used as a base LM and as an underlying model for LOGIPT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter transformer chatbot fine-tuned from LLaMA-2 on user-shared conversations (ShareGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (LLaMA-2 derivative, chat-finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on LLaMA-2 corpus; finetuned on ShareGPT conversation data (as described by Vicuna authors); in this paper further fine-tuned with solver-derived instruction-tuning data to produce LOGIPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used with standard/CoT prompting baselines and as a LOGIPT underlying model after instruction fine-tuning to emulate solver reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as underlying LM for LOGIPT and baseline prompting evaluations on deductive reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical reasoning (when fine-tuned as LOGIPT) or baseline prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When used as LOGIPT (vicuna-13b-v1.5-16k underlying), achieved best-performing results on PrOntoQA in the paper (LOGIPT variant outperformed LogicLM by +13.20% absolute on PrOntoQA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Vicuna as an underlying LOGIPT outperformed solver-augmented GPT-4 baseline on some datasets; as an un-fine-tuned baseline it had low NL->SL parsing success (example parsing success ~17% reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When fine-tuned via the solver-derived instruction data, Vicuna-based LOGIPT can match or surpass closed-source models on strict deductive tasks despite smaller size.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Out-of-the-box (without solver-derived fine-tuning) exhibited poor NL->SL parsing and low end-to-end deductive performance; required careful fine-tuning to acquire solver-like reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6810.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama-13B (base and instruct variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of 13B-parameter open foundation models optimized for code tasks, provided in base and instruction-tuned variants; used as underlying models and fine-tuned into LOGIPT where they achieved strong deductive reasoning after instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code llama: Open foundation models for code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-13B (base and Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter transformer family adapted for code; base (CodeLlama-13b-hf) and instruction-finetuned (CodeLlama-13b-Instruct-hf) variants included in evaluations and LOGIPT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (code-specialized foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Code-focused pretraining corpora (as described by CodeLlama authors); instruction-finetuned variant exposed to extra instruction-following tokens; further fine-tuned on solver-derived instruction-tuning data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used with standard/CoT prompting baselines and as a LOGIPT underlying model after fine-tuning to emulate solver reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used for deductive reasoning evaluations; CodeLlama LOGIPT variants achieved top reported performance on ProofWriter in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive logical reasoning (when fine-tuned as LOGIPT) or baseline prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CodeLlama-13B-Base with standard prompting had 0.00 accuracy on tested tasks; after LOGIPT fine-tuning CodeLlama variants achieved the best-performing model on ProofWriter, with an absolute improvement over LogicLM (GPT-4) reported (e.g., overall LOGIPT improvements reported vs LogicLM: +9.84% ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Instruction-tuned and LOGIPT-fine-tuned CodeLlama models outperformed their base prompting behavior and outperformed other open-source baselines, demonstrating strong gains from solver-derived fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code foundation models, once fine-tuned with solver-derived traces, show strong deductive reasoning ability and can surpass other 13B chat models and even closed-source baselines in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Base code models without instruction fine-tuning fail to follow few-shot demonstrations and may output no answers; require instruction finetuning and solver-derived data to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6810.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / ChatGPT / GPT-3.5 (closed-source baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-source LLMs: GPT-4, ChatGPT (gpt-3.5-turbo), GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large closed-source transformer-based LLMs used as baselines for parsing and prompting (standard and CoT); included in solver-augmented approaches (e.g., LogicLM) and in few-shot prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, gpt-3.5-turbo (ChatGPT), text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer LLMs from OpenAI used for NL->SL parsing, few-shot standard prompting, and CoT prompting baselines; used by prior solver-augmented pipelines (e.g., LogicLM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (closed-source; exact architecture details proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Proprietary mixed web/text/code corpora (not specified in this paper); in experiments these models were used via API for prompting or solver-augmentation baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting (standard or CoT) and as parsers for solver-augmented pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter, PrOntoQA</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as closed-source baselines and as parsers in solver-augmented methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Deductive reasoning (via prompting or as parser for symbolic solver pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Closed-source models with standard prompting sometimes performed near random on these deductive tasks; GPT-4 CoT performed well on some PrOntoQA settings and served as a strong baseline for LogicLM, but was outperformed by LOGIPT in the reported experiments (LOGIPT beat LogicLM(GPT-4) by +9.84% on ProofWriter and +13.20% on PrOntoQA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>While GPT-4 and GPT-3.5 are strong LMs generally, their standard prompting and even CoT did not consistently match LOGIPT on the strict deductive benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large closed-source LMs are not immune to failures on strict deductive tasks; specialized training or integration with reliable symbolic reasoning traces improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prompting-based use can still produce parsing errors or hallucinated reasoning; closed-source models used as NL->SL parsers still suffer from parsing brittleness that affects end-to-end solver-augmented systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6810.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6810.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pyke expert system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pyke (Python expert system / symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf deterministic Prolog-style expert system used in this paper to execute symbolic reasoning, reveal internal solver actions (bind/unbind, fail & backtrack), and generate training traces for LOGIPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Applying expert system technology to code reuse with pyke.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pyke expert system</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deterministic rule-based expert system capable of accepting Prolog-like facts and rules, performing forward/backward inference and producing implied facts; used both as the symbolic solver in pipelines and as the trace generator for training data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Deterministic symbolic rule-based solver / expert system</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Deterministic application of rules to known facts, performing binding/unbinding and backtracking to derive implied facts (Prolog-style reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Used as the symbolic solver in experiments and was instrumented/modified to reveal its otherwise invisible internal reasoning trace for dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used to generate traces for ProofWriter and PrOntoQA-derived training cases</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>pyke was used to execute symbolic reasoning on parsed Prolog-like inputs obtained (initially) from GPT-4 parsing to produce implied facts and intermediate solver actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Symbolic deduction / theorem-proving style rule application</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used as oracle to generate ground-truth implied facts and reasoning traces; not reported with an accuracy metric itself in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Not applicable as pyke is the deterministic solver whose traces were used to supervise LOGIPT; pyke outputs are considered ground truth for deduction steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instrumenting a symbolic solver to reveal internal actions enables creating fine-grained instruction-tuning data that an LM can learn to emulate, improving end-to-end deductive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>pyke is a domain-specific symbolic tool; the dataset generation required filtering out NL->SL parser failures (only syntax-valid cases were retained), so LOGIPT training depends on coverage of the generated traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models can be Logical Solvers', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report. <em>(Rating: 1)</em></li>
                <li>Applying expert system technology to code reuse with pyke. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6810",
    "paper_id": "paper-265128609",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LOGIPT",
            "name_full": "LOGIPT (Solver-derived Instruction-tuned Language Model)",
            "brief_description": "A language model fine-tuned to imitate the internal, stepwise reasoning actions of a deductive symbolic solver by training on solver-derived instruction-tuning data so it can directly generate implied facts from natural-language logical contexts and answer deductive queries without calling an external solver at inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LOGIPT (fine-tuned Vicuna/CodeLlama variants)",
            "model_description": "Open-source transformer LMs (Vicuna-13B or CodeLlama-13B) fine-tuned on a dataset constructed from solver (pyke) reasoning traces; trained to output predicate definitions, known facts/rules, solver-like step actions (bind/unbind, fail & backtrack) and final implied facts, enabling direct deduction from NL contexts.",
            "model_size": "13B",
            "architecture_type": "Transformer (instruction-tuned to emulate symbolic solver reasoning)",
            "training_data": "Solver-derived instruction-tuning dataset constructed from public deductive reasoning datasets (ProofWriter, PrOntoQA) by revealing and refining pyke solver internal reasoning traces.",
            "reasoning_method": "Model is fine-tuned to emulate symbolic-solver stepwise reasoning (solver-trace distillation); it generates all implied facts from premises and rules then matches the query (OWA/CWA handling).",
            "external_tool_used": false,
            "external_tool_description": "pyke expert system was used offline to generate solver traces and implied facts for constructing the instruction-tuning dataset, but LOGIPT does not call an external solver at inference.",
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Public deductive reasoning benchmarks containing NL contexts with facts/rules and queries under open-world (ProofWriter) and closed-world (PrOntoQA subset) assumptions for evaluating deductive inference.",
            "task_type": "Deductive logical reasoning / logical deduction (generate implied facts and decide query truth: True/False/Unknown)",
            "performance_metric": "Accuracy (multiple-choice correctness of True/False/Unknown or True/False depending on dataset)",
            "performance_value": "Outperforms state-of-the-art solver-augmented baseline LogicLM (GPT-4) by +9.84 percentage points on ProofWriter and by +13.20 percentage points on PrOntoQA (absolute improvement); (exact per-model accuracy values reported in paper tables).",
            "comparison_with_baseline": "Significant positive improvement vs solver-augmented LMs (LogicLM with GPT-4) and versus few-shot prompting on closed-source LLMs; also surpasses or matches chain-of-thought GPT-4 on some settings.",
            "key_findings": "Fine-tuning LMs on explicit, solver-derived stepwise reasoning traces lets relatively small (13B) open-source models act as reliable deductive solvers and surpass solver-augmented pipelines that rely on NL-&gt;SL parsing plus external solvers.",
            "limitations": "Requires filtered, syntax-valid examples generated by a symbolic solver (pyke) for training; mixing datasets with different reasoning assumptions (OWA vs CWA) without careful reformatting degrades performance; depends on quality/coverage of solver traces and training data; not claimed to generalize to all theorem proving domains beyond the studied deductive benchmarks.",
            "uuid": "e6810.0",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Solver-augmented LMs (general pipeline)",
            "name_full": "Solver-augmented Language Models (LM -&gt; Symbolic Language -&gt; External Solver)",
            "brief_description": "An approach where an LM parses NL logical problems into a symbolic language (e.g., Prolog-style) and an external deterministic symbolic solver (theorem prover/expert system) executes precise reasoning to produce the final answer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Solver-augmented LMs (generic)",
            "model_description": "LMs (various) are used as NL-&gt;SL parsers (often via instruction + few-shot examples) and external symbolic solvers (e.g., pyke) perform deterministic reasoning over the parsed facts and rules.",
            "model_size": null,
            "architecture_type": "Transformer + external symbolic solver (neuro-symbolic pipeline)",
            "training_data": "Not centrally trained as a single model in this paper; relies on existing NL-to-SL prompts and in-context examples. Evaluation used public deductive datasets (ProofWriter, PrOntoQA) for end-to-end parsing+solving.",
            "reasoning_method": "Parsing NL to symbolic language (Prolog-like) via LM, then invoking deterministic symbolic solver to derive implied facts and answer queries.",
            "external_tool_used": true,
            "external_tool_description": "External symbolic solver (pyke expert system used in experiments) receives parsed symbolic facts/rules and deterministically computes implied facts / query truth values.",
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Same deductive reasoning benchmarks used to test NL-&gt;SL parsing success and solver execution correctness.",
            "task_type": "Deductive reasoning via symbolic execution; NL-to-SL parsing + symbolic reasoning",
            "performance_metric": "Parsing successful rate (percentage of executable logical formulations) and final answer accuracy after solver execution",
            "performance_value": "Parsing success can be low; example: Vicuna-13B parsing success on ProofWriter reported as ~17% in preliminary experiments. End-to-end accuracy depends heavily on parsing success; state-of-the-art solver-augmented baseline LogicLM (GPT-4) used for comparison.",
            "comparison_with_baseline": "Solver-augmented approaches yield faithful answers when parsing is correct, but overall end-to-end performance suffers when NL-&gt;SL parsing fails; LOGIPT (this paper) outperforms these pipelines by avoiding parsing errors.",
            "key_findings": "Deterministic symbolic solvers guarantee correctness given executable input, but the brittle NL-&gt;SL parsing stage by LMs is a major failure point; parser error rates can severely degrade system performance.",
            "limitations": "High sensitivity to NL-to-SL parsing errors (parsing failures lead to solver execution failure and no answer); relies on correct, executable symbolic representations; requires explicit grammar/instructions and good in-context learning.",
            "uuid": "e6810.1",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LogicLM",
            "name_full": "LogicLM (solver-augmented LM from Pan et al., 2023)",
            "brief_description": "Representative solver-augmented method that uses LLMs (e.g., ChatGPT/GPT-4) to parse NL into symbolic inputs and then invokes an external symbolic solver for faithful deductive reasoning.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "use",
            "model_name": "LogicLM (with GPT-4 / ChatGPT as parser)",
            "model_description": "Pipeline using closed-source LLMs for NL-&gt;SL parsing combined with external symbolic solvers to compute answers deterministically.",
            "model_size": null,
            "architecture_type": "Transformer (closed-source) + external symbolic solver",
            "training_data": "Not trained in this work; evaluated as a baseline using public deductive datasets (ProofWriter, PrOntoQA) following prior work.",
            "reasoning_method": "NL-&gt;SL parsing by LM then symbolic reasoning by solver.",
            "external_tool_used": true,
            "external_tool_description": "Uses an external logical solver (as in the general solver-augmented pipeline) to perform deduction on parsed symbolic inputs.",
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Deductive reasoning datasets with NL contexts, facts/rules, and queries under OWA/CWA assumptions.",
            "task_type": "Deductive logical reasoning (faithful symbolic solving)",
            "performance_metric": "Accuracy",
            "performance_value": "Used as state-of-the-art baseline; LOGIPT reports absolute improvements of +9.84% on ProofWriter and +13.20% on PrOntoQA over LogicLM (GPT-4) in the paper's experiments.",
            "comparison_with_baseline": "LogicLM (GPT-4) is the baseline solver-augmented approach; LOGIPT substantially outperforms it on the two evaluated benchmarks.",
            "key_findings": "Solver-augmented baselines are strong when parsing is successful, but they are still outperformed by LOGIPT which sidesteps the brittle parsing step.",
            "limitations": "Inherits brittle dependency on NL-&gt;SL parsing; if parsing fails or produces syntax-invalid formulations, solver cannot execute and system fails.",
            "uuid": "e6810.2",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "An in-context prompting technique that elicits step-by-step intermediate reasoning from LMs by providing few-shot examples of detailed reasoning chains before asking for an answer.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought prompting (applied to various LMs such as GPT-4, ChatGPT)",
            "model_description": "Prompt engineering technique; does not change model weights but elicits internal multi-step reasoning through example chains.",
            "model_size": null,
            "architecture_type": "Transformer with prompting technique (CoT)",
            "training_data": "Few-shot demonstrations / in-context examples (used across experiments for fair comparison).",
            "reasoning_method": "Prompt-induced stepwise natural-language reasoning (chain-of-thought) to produce intermediate steps and final answer.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Used as prompting baseline on deductive reasoning benchmarks.",
            "task_type": "Deductive reasoning via NL chain-of-thought explanations",
            "performance_metric": "Accuracy",
            "performance_value": "CoT on GPT-4 achieved competitive results on some settings (PrOntoQA) and in one case comparable to LOGIPT; otherwise, CoT and standard prompting often performed near random for many open-source models in these tasks.",
            "comparison_with_baseline": "On PrOntoQA, GPT-4 with CoT achieved results comparable to LOGIPT in one reported experiment; generally, CoT helps closed-source models but many open-source LMs still performed poorly.",
            "key_findings": "CoT can improve reasoning for large closed-source models but does not solve the NL-to-SL parsing brittleness and often fails on complex deductive benchmarks for many models.",
            "limitations": "Produces natural-language reasoning which can be ambiguous/hallucinated and not guaranteed faithful to logical proof; for many models CoT did not yield reliable performance on strict deductive tasks.",
            "uuid": "e6810.3",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Standard (few-shot) prompting",
            "name_full": "Standard few-shot in-context prompting",
            "brief_description": "Baseline prompting approach that supplies a few examples in-context for direct answering or NL-to-SL parsing, without eliciting explicit chain-of-thought steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Standard few-shot prompting (applied to multiple LMs)",
            "model_description": "In-context learning by showing few demonstrations; expects model to follow formatting instructions to parse or answer.",
            "model_size": null,
            "architecture_type": "Transformer with few-shot prompting",
            "training_data": "Few-shot in-context examples used consistently across baseline evaluations.",
            "reasoning_method": "Direct in-context mapping from NL to answer (or to symbolic parse) using limited examples.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Used to gauge performance of prompting baselines on deductive reasoning tasks.",
            "task_type": "Deductive reasoning via direct few-shot prompting / NL-&gt;SL parsing where prompted",
            "performance_metric": "Accuracy",
            "performance_value": "Many open-source LMs with standard prompting produced accuracies near random; e.g., CodeLlama-13B-Base had 0.00 accuracy with standard prompting on these tasks as reported.",
            "comparison_with_baseline": "Standard prompting often underperforms CoT and much worse than LOGIPT and solver-augmented methods when parsing fails.",
            "key_findings": "Few-shot standard prompting is insufficient for reliable strict deductive reasoning in many LMs; some closed-source LMs also showed weak standard prompting performance.",
            "limitations": "Highly sensitive to model capability to follow demonstrations; often fails to format correct symbolic parses or answer formats, making it unreliable for strict logical reasoning.",
            "uuid": "e6810.4",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Vicuna-13B",
            "name_full": "Vicuna-13B (vicuna-13b-v1.5-16k)",
            "brief_description": "An open-source chatbot model derived by fine-tuning LLaMA-2 on ShareGPT conversation data, used as a base LM and as an underlying model for LOGIPT in experiments.",
            "citation_title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B",
            "model_description": "13B-parameter transformer chatbot fine-tuned from LLaMA-2 on user-shared conversations (ShareGPT).",
            "model_size": "13B",
            "architecture_type": "Transformer (LLaMA-2 derivative, chat-finetuned)",
            "training_data": "Pretraining on LLaMA-2 corpus; finetuned on ShareGPT conversation data (as described by Vicuna authors); in this paper further fine-tuned with solver-derived instruction-tuning data to produce LOGIPT variants.",
            "reasoning_method": "Used with standard/CoT prompting baselines and as a LOGIPT underlying model after instruction fine-tuning to emulate solver reasoning.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Used as underlying LM for LOGIPT and baseline prompting evaluations on deductive reasoning datasets.",
            "task_type": "Deductive logical reasoning (when fine-tuned as LOGIPT) or baseline prompting",
            "performance_metric": "Accuracy",
            "performance_value": "When used as LOGIPT (vicuna-13b-v1.5-16k underlying), achieved best-performing results on PrOntoQA in the paper (LOGIPT variant outperformed LogicLM by +13.20% absolute on PrOntoQA).",
            "comparison_with_baseline": "Vicuna as an underlying LOGIPT outperformed solver-augmented GPT-4 baseline on some datasets; as an un-fine-tuned baseline it had low NL-&gt;SL parsing success (example parsing success ~17% reported).",
            "key_findings": "When fine-tuned via the solver-derived instruction data, Vicuna-based LOGIPT can match or surpass closed-source models on strict deductive tasks despite smaller size.",
            "limitations": "Out-of-the-box (without solver-derived fine-tuning) exhibited poor NL-&gt;SL parsing and low end-to-end deductive performance; required careful fine-tuning to acquire solver-like reasoning.",
            "uuid": "e6810.5",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CodeLlama-13B",
            "name_full": "CodeLlama-13B (base and instruct variants)",
            "brief_description": "A family of 13B-parameter open foundation models optimized for code tasks, provided in base and instruction-tuned variants; used as underlying models and fine-tuned into LOGIPT where they achieved strong deductive reasoning after instruction tuning.",
            "citation_title": "Code llama: Open foundation models for code.",
            "mention_or_use": "use",
            "model_name": "CodeLlama-13B (base and Instruct)",
            "model_description": "13B-parameter transformer family adapted for code; base (CodeLlama-13b-hf) and instruction-finetuned (CodeLlama-13b-Instruct-hf) variants included in evaluations and LOGIPT fine-tuning.",
            "model_size": "13B",
            "architecture_type": "Transformer (code-specialized foundation model)",
            "training_data": "Code-focused pretraining corpora (as described by CodeLlama authors); instruction-finetuned variant exposed to extra instruction-following tokens; further fine-tuned on solver-derived instruction-tuning data in this paper.",
            "reasoning_method": "Used with standard/CoT prompting baselines and as a LOGIPT underlying model after fine-tuning to emulate solver reasoning.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Used for deductive reasoning evaluations; CodeLlama LOGIPT variants achieved top reported performance on ProofWriter in the paper.",
            "task_type": "Deductive logical reasoning (when fine-tuned as LOGIPT) or baseline prompting",
            "performance_metric": "Accuracy",
            "performance_value": "CodeLlama-13B-Base with standard prompting had 0.00 accuracy on tested tasks; after LOGIPT fine-tuning CodeLlama variants achieved the best-performing model on ProofWriter, with an absolute improvement over LogicLM (GPT-4) reported (e.g., overall LOGIPT improvements reported vs LogicLM: +9.84% ProofWriter).",
            "comparison_with_baseline": "Instruction-tuned and LOGIPT-fine-tuned CodeLlama models outperformed their base prompting behavior and outperformed other open-source baselines, demonstrating strong gains from solver-derived fine-tuning.",
            "key_findings": "Code foundation models, once fine-tuned with solver-derived traces, show strong deductive reasoning ability and can surpass other 13B chat models and even closed-source baselines in these tasks.",
            "limitations": "Base code models without instruction fine-tuning fail to follow few-shot demonstrations and may output no answers; require instruction finetuning and solver-derived data to be effective.",
            "uuid": "e6810.6",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4 / ChatGPT / GPT-3.5 (closed-source baselines)",
            "name_full": "Closed-source LLMs: GPT-4, ChatGPT (gpt-3.5-turbo), GPT-3.5 (text-davinci-003)",
            "brief_description": "Large closed-source transformer-based LLMs used as baselines for parsing and prompting (standard and CoT); included in solver-augmented approaches (e.g., LogicLM) and in few-shot prompting experiments.",
            "citation_title": "Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4, gpt-3.5-turbo (ChatGPT), text-davinci-003",
            "model_description": "Closed-source transformer LLMs from OpenAI used for NL-&gt;SL parsing, few-shot standard prompting, and CoT prompting baselines; used by prior solver-augmented pipelines (e.g., LogicLM).",
            "model_size": null,
            "architecture_type": "Transformer (closed-source; exact architecture details proprietary)",
            "training_data": "Proprietary mixed web/text/code corpora (not specified in this paper); in experiments these models were used via API for prompting or solver-augmentation baselines.",
            "reasoning_method": "Prompting (standard or CoT) and as parsers for solver-augmented pipelines.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter, PrOntoQA",
            "benchmark_description": "Used as closed-source baselines and as parsers in solver-augmented methods.",
            "task_type": "Deductive reasoning (via prompting or as parser for symbolic solver pipelines)",
            "performance_metric": "Accuracy",
            "performance_value": "Closed-source models with standard prompting sometimes performed near random on these deductive tasks; GPT-4 CoT performed well on some PrOntoQA settings and served as a strong baseline for LogicLM, but was outperformed by LOGIPT in the reported experiments (LOGIPT beat LogicLM(GPT-4) by +9.84% on ProofWriter and +13.20% on PrOntoQA).",
            "comparison_with_baseline": "While GPT-4 and GPT-3.5 are strong LMs generally, their standard prompting and even CoT did not consistently match LOGIPT on the strict deductive benchmarks in this paper.",
            "key_findings": "Large closed-source LMs are not immune to failures on strict deductive tasks; specialized training or integration with reliable symbolic reasoning traces improves performance.",
            "limitations": "Prompting-based use can still produce parsing errors or hallucinated reasoning; closed-source models used as NL-&gt;SL parsers still suffer from parsing brittleness that affects end-to-end solver-augmented systems.",
            "uuid": "e6810.7",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "pyke expert system",
            "name_full": "pyke (Python expert system / symbolic solver)",
            "brief_description": "An off-the-shelf deterministic Prolog-style expert system used in this paper to execute symbolic reasoning, reveal internal solver actions (bind/unbind, fail & backtrack), and generate training traces for LOGIPT.",
            "citation_title": "Applying expert system technology to code reuse with pyke.",
            "mention_or_use": "use",
            "model_name": "pyke expert system",
            "model_description": "A deterministic rule-based expert system capable of accepting Prolog-like facts and rules, performing forward/backward inference and producing implied facts; used both as the symbolic solver in pipelines and as the trace generator for training data.",
            "model_size": null,
            "architecture_type": "Deterministic symbolic rule-based solver / expert system",
            "training_data": null,
            "reasoning_method": "Deterministic application of rules to known facts, performing binding/unbinding and backtracking to derive implied facts (Prolog-style reasoning).",
            "external_tool_used": true,
            "external_tool_description": "Used as the symbolic solver in experiments and was instrumented/modified to reveal its otherwise invisible internal reasoning trace for dataset construction.",
            "benchmark_name": "Used to generate traces for ProofWriter and PrOntoQA-derived training cases",
            "benchmark_description": "pyke was used to execute symbolic reasoning on parsed Prolog-like inputs obtained (initially) from GPT-4 parsing to produce implied facts and intermediate solver actions.",
            "task_type": "Symbolic deduction / theorem-proving style rule application",
            "performance_metric": "Used as oracle to generate ground-truth implied facts and reasoning traces; not reported with an accuracy metric itself in this paper.",
            "performance_value": null,
            "comparison_with_baseline": "Not applicable as pyke is the deterministic solver whose traces were used to supervise LOGIPT; pyke outputs are considered ground truth for deduction steps.",
            "key_findings": "Instrumenting a symbolic solver to reveal internal actions enables creating fine-grained instruction-tuning data that an LM can learn to emulate, improving end-to-end deductive performance.",
            "limitations": "pyke is a domain-specific symbolic tool; the dataset generation required filtering out NL-&gt;SL parser failures (only syntax-valid cases were retained), so LOGIPT training depends on coverage of the generated traces.",
            "uuid": "e6810.8",
            "source_info": {
                "paper_title": "Language Models can be Logical Solvers",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Applying expert system technology to code reuse with pyke.",
            "rating": 2,
            "sanitized_title": "applying_expert_system_technology_to_code_reuse_with_pyke"
        }
    ],
    "cost": 0.01742625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models can be Logical Solvers
10 Nov 2023</p>
<p>Jiazhan Feng fengjiazhan@pku.edu.cn 
Peking University
Beijing</p>
<p>Ruochen Xu 
Microsoft Azure AI
Redmond</p>
<p>Junheng Hao junhenghao@microsoft.com 
Microsoft Azure AI
Redmond</p>
<p>Hiteshi Sharma 
Microsoft Azure AI
Redmond</p>
<p>Yelong Shen 
Microsoft Azure AI
Redmond</p>
<p>Dongyan Zhao zhaody@pku.edu.cn 
Peking University
Beijing</p>
<p>Weizhu Chen wzchen@microsoft.com 
Microsoft Azure AI
Redmond</p>
<p>Language Models can be Logical Solvers
10 Nov 202303ED0BE6B08ACDCDB5429424B2795E1FarXiv:2311.06158v1[cs.CL]Green('Charlie'True) Green('Charlie'False)
Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decisionmaking.Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge.The stateof-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers.Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of the external logical solver and no answer to the logical questions.In this paper, we introduce LOGIPT, a novel language model that directly emulates the reasoning processes of logical solvers and bypasses the parsing errors by learning to strict adherence to solver syntax and grammar.LOGIPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers.Experimental results on two public deductive reasoning datasets demonstrate that LOGIPT outperforms state-of-the-art solver-augmented LMs and fewshot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Introduction</p>
<p>Logical reasoning is a foundational element of human intelligence, holding a pivotal role in tasks like problem-solving, decision-making, and critical thinking (Huang and Chang, 2023).Recently, substantial advancements have been achieved in the field of NLP through the development of large language models (LLMs) (OpenAI, 2022(OpenAI, , 2023;;Google, 2023;Touvron et al., 2023a,b).It has been noted that language models (LMs) could potentially display reasoning capabilities when they reach a certain scale threshold (e.g., training compute, model parameters, etc.) (Kaplan et al., 2020;Wei et al., 2022a;Hoffmann et al., 2022).To this end, LLMs can answer logical questions with explicit reasoning steps when prompted with a simple snippet: "Let's think step by step."(Kojima et al., 2022) or step-wise explanations of reasoning (i.e., "chain of thoughts") (Wei et al., 2022b).</p>
<p>While LLMs have made significant progress, complex logical reasoning remains challenging (Valmeekam et al., 2022;Liu et al., 2023b).Some prior work (Tafjord et al., 2022;Ling et al., 2023) aimed to enable LMs to perform logical reasoning via specialized module fine-tuning, where reasoning is in natural language (NL).However, the ambiguity and complexity of NL can lead to undesired issues like hallucinations and unfaithful reasoning (Saparov and He, 2023;Gao et al., 2023).To this end, recent work has begun to augment LLMs with access to external Solvers (Chen et al., 2022;Ye et al., 2023;Pan et al., 2023).In this paper, we focus on the logical solvers, which are theorem provers that can be any automated reasoning tool for checking the truth value of logical formulas in symbolic language (SL).Invoking logical solvers can guarantee the accuracy of logical reasoning and relieve the burden of LLMs to execute intricate and precise deductive reasoning.</p>
<p>The data flow of the aforementioned solveraugmented LMs is depicted in Figure 1(a).At the outset, the information of logical questions is stored in NL.It is subsequently fed into a LM for parsing into a symbolic representation suitable for solver-input format.Finally, the SL information is dispatched to a symbolic solver, which yields the truth value of the logical question.However, during this process, any NL-to-SL parsing errors will inevitably result in the failure of the reasoning process and no answer to the question.In our pre- liminary experiments, we observed that the parsing successful rate (i.e., percentage of executable logical formulations) of Vicuna-13B (Chiang et al., 2023) on ProofWriter (Tafjord et al., 2021) is only 17%, significantly below the expected performance.</p>
<p>In addressing parsing failures, current methods either directly use LLMs to reason in NL solely or rely on the solver's erroneous message to regenerate parsing results, but these approaches don't fundamentally resolve the problem.</p>
<p>In this paper, we introduce LOGIPT, a novel LM designed to mimic the reasoning process of logical solvers, enabling it to solve deductive reasoning tasks.We first construct an instruction-tuning dataset containing NL logical questions and their corresponding solver's symbolic reasoning process.After filtering out cases having invalid syntax, we fine-tune open-source LMs like Vicuna or CodeLlama (Roziere et al., 2023) with this data to create LOGIPT.Then, LOGIPT can generate all implied facts given premises and rules, allowing us to determine the truth value of a logical query by matching it with implied facts or outputting 'unknown' if it cannot be determined.The data flow of our pipeline is presented in Figure 1(b,c).We can bypass the syntax or grammatical errors derived from NL-to-SL parsing by directly outputting the answers with a fine-tuned LOGIPT.</p>
<p>Our approach is akin to the process of distillation, whereby we distill knowledge from a symbolic model (i.e., solver) into a neural network (i.e., LM).However, the reasoning process of solvers is invisible to users and we can only obtain the answers without intermediate reasoning steps.We design a pipeline to reveal and formalize solvers' invisible reasoning processes, creating instructiontuning datasets with visible and interpretable sym-bolic reasoning steps (see Figure 3).</p>
<p>Our main contributions are three-fold:</p>
<p> To the best of our knowledge, we are the first to propose empowering LLMs to directly learn the reasoning process of logical solvers, thereby acquiring similar reasoning capability for addressing deductive reasoning tasks. Our proposed LOGIPT, can directly act as a deductive solver and output all Facts implied from NL logical questions while bypassing the syntax or grammatical errors derived from NL-to-SL parsing of solver-augmented LMs. Evaluation results on two public deductive reasoning datasets show that LOGIPT can outperform state-of-the-art solver-augmented LMs, and few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Preliminary</p>
<p>Deductive Reasoning</p>
<p>Deductive reasoning is an essential type of logical reasoning problem.It typically commences with known facts and rules from logical context, then proceeds through a series of inference steps until the query can be proved or disproved (Poole and Mackworth, 2010).In this paper, we consider the Prolog logic programming language (Clocksin and Mellish, 2003;Krner et al., 2022), which stands as the most prominent symbolic language for describing deductive reasoning problems.We showcased a deductive reasoning question along with its corresponding Prolog syntax representation in Figure 2.</p>
<p>For each question, we denote the NL description as Context.The Context can further be parsed into Facts, Rules, and Query1 .Specifically, a Fact F = P (a 1 ,    , a t ) is a symbolic statement with a predicate P and t arguments {a 1 ,    , a t } where a i can be a variable, entity, number or bool.For example, Green('Charlie', True) means "Charlie is green"; Rules are presented in the form of clauses
F 1      F m  F m+1      F n , where F i is a Fact. The Rule means "if each F i  {F 1 ,    , F m }
is true, then we can imply that all Facts in {F m+1 ,    , F n } are also true."For example, Furry($x, True)  Quiet($x, True) indicates if variable $x is furry, then $x is quiet; a Query Q is also in the format of a Fact that needs to be proved based on Facts and Rules.</p>
<p>Solver-augmented LMs</p>
<p>Solver-augmented LMs have demonstrated remarkable performance in deductive reasoning tasks.As shown in Figure 1(a), these model can be generally divided into two stages: Problem Formulation (from LMs to Symbolic Solvers) and Symbolic Reasoning (from Symbolic Solvers to Answers).</p>
<p>In Problem Formulation stage, an LM is used to parse an NL logical question into symbolic representation (Figure 2).The process can be accomplished by providing LM with detailed instructions about the grammar of Prolog, alongside a few demonstrations as in-context examples (Ouyang et al., 2022).The LM is expected to identify the symbolic Facts, Rules, and Query from the NL logical question following the instructions; In Symbolic Reasoning stage, a solver takes in the symbolic representation obtained in the previous stage and conducts symbolic reasoning.The reasoning process of external off-the-shelf solver, e.g., pyke expert system (Frederiksen, 2008), is deterministic and invisible.Then, the truth value of the parsed Query, which is the only output of the solver, can be treated as the answer to the given question.</p>
<p>Analysis on the Parsing Successful Rate</p>
<p>Through the aforementioned two phases, once the solver-augmented LMs correctly formulate the problem, the answers obtained through symbolic reasoning will be faithful, attributed to the deterministic nature of the solver.However, this heavily relies on the in-context learning capabilities of LMs.Therefore, we first calculate the parsing successful rate of three selected open-source LLMs on two deductive reasoning datasets in Table 1.Firstly, we observe that CodeLlama-13B-Base (CodeLlama-13b-hf) is unable to effectively conduct NL-to-SL parsing due to the limited in-context learning capabilities in natural languages.Then we can find that replacing the Base model with the Instruct version (CodeLlama-13b-Instruct-hf) can alleviate this issue, which may be attributed to the fact that the Instruct version is further finetuned with an additional approx.5B tokens to better follow human instructions.Overall, open-source LLMs still exhibit parsing performance significantly lower than expected in some cases.</p>
<p>LoGiPT</p>
<p>In this paper, we aim to mitigate the parsing issue and present a novel LM, LOGIPT instructed to imitate the logical reasoning process of Solvers for deductive reasoning tasks.To achieve this, we first reveal the solver reasoning process when solving logical problems ( 3.1).Then, we construct a solver-derived instruction-tuning dataset, comprising NL logical questions and corresponding SL reasoning steps ( 3.2).Finally, we fine-tune open-source LLMs using this dataset to develop LOGIPT ( 3.3).</p>
<p>Revealing the Solver Reasoning Process</p>
<p>Before operating on the solvers, we first adopt gpt-4 as the problem formulator for NL-to-SL parsing with instructions about the grammar and</p>
<p>Turn-1 (from human):</p>
<p>Task Description: Given a statement containing logical facts and rules in natural languages as context, the task is to:</p>
<ol>
<li>define all the predicates in logic forms from the context 2. parse the statement into known facts and rules based on the defined predicates 3. begin reasoning to obtain all implied facts Context: Charlie is green.Charlie is white.Dave is blue.Dave is furry.Dave is white.Fiona is blue.Fiona is green.Fiona is nice.Fiona is rough.Fiona is white.All furry people are quiet.All green, white people are nice.If someone is blue and quiet then they are green.</li>
</ol>
<p>Reasoning:</p>
<p>Turn-2 (from LM):</p>
<p>We first define following predicates and corresponding natural language explanations: Green($x, bool) ::: Is x green?(... more predicates here ...)</p>
<p>We have following known facts from the context: Green('Charlie', True) (... more facts here ...)</p>
<p>We have following known rules from the context: rule1:
Furry($x, True)  Quiet($x, True) rule2: Green($x, True)  White($x, True)  Nice($x, True) rule3: Blue($x, True)  Quiet($x, True)  Green($x,</p>
<p>Turn-3 (from human):</p>
<p>Question: Based on the above information, is the following comment true, false, or unknown?Dave is not green.
Options: A) True B) False C) Unknown
Now you should:</p>
<ol>
<li>reason first carefully with the above known facts and implied facts 2. then output your answer strictly in the format of "So, the correct option is: X".The X should be replaced by "A) True", "B) False" or "C) Unknown"</li>
</ol>
<p>Turn-4 (from LM):</p>
<p>The comment can be parsed into: Green('Dave', False) According to the implied fact: Green('Dave', True)</p>
<p>The comment contradicts the fact.So, the correct option is: B) False</p>
<p>Figure 3: A comprehensive 4-turn training example of our instruction-tuning data.We highlight the initial occurrences of each functionality described in 3.1 using the corresponding colors.We omit some predicates and Facts in Turn-2 due to limited space.Hint: this figure is color-sensitive.</p>
<p>few-shot demonstrations 2 , and obtain the SL representations of all training logical questions of the given logical datasets.Then, consistent with solveraugmented methods, we adopt pyke expert system as the symbolic solver in this work that can make inferences using the Prolog symbolic language.Given a logical question, pyke first sets up a knowledge base and injects all known Facts and Rules (Figure 2) from solver's inputs.Then, it iteratively applies Rules on already known or implied Facts, aiming at obtaining more implied Facts until the Query is proved or disproved.The reasoning process executed by pyke solver is invisible to users and solver-augmented LMs use the solver as a black-box.We hypothesis the 'chain-of-thought' reasoning process of the solver is valuable and LLMs are able to learn from it.To this end, we first modify the source code of the pyke 3 to achieve the following functionalities: With the aforementioned instructions, we can obtain the revealed solver's reasoning process for the construction of training data.We also high-lighted the initial occurrences of each functionality using the corresponding colors in Figure 3 (Turn-2), where a case will be described in detail in the next section.</p>
<p>Constructing the Instruction-tuning Data</p>
<p>However, as previously mentioned, we cannot guarantee that LMs can definitely complete the NL-to-SL parsing on arbitrary questions.To this end, we first filter out all unsuccessfully parsed training cases that cannot be executed by pyke.Then we reorganize and refine the filtered training data to enhance the interpretability of the solver-derived reasoning steps.For each case, we divide the reasoning process into four conversational turns (Turn-1&amp;3 for human and Turn-2&amp;4 for LM), which will be described elaborately in the following paragraphs.We also provide a comprehensive training example of our instruction-tuning data4 in Figure 3, and the full version is also included in Appendix C.</p>
<p>Turn-1: Instructions &amp; NL logical Context.</p>
<p>For each NL logical question within the training set, we begin by stripping away the specific Query statement while retaining the question Context and subsequently integrating it with elaborately crafted instructions.Taking the case in Figure 3 as an example, we temporarily exclude the Query 'Dave is not green' from the 'Context' field.Here, we only consider Query-agnostic question description to ensure that LMs initially focus on the logical background itself.This is because sometimes the ground-truth answer is 'Unknown' (e.g., cases in ProofWriter).The truth value of the Query cannot be inferred from the Context, and therefore we need to deduce all implied Facts first.</p>
<p>Turn-2: Query-agnostic Solver-derived Reasoning.As we have acquired the solver's symbolic reasoning data in the revealing phase, our goal in Turn-2 is to further refine and enhance the reasoning process to achieve a more readable form of the solver's reasoning process.Specifically, for each logical question, we first define all necessary predicates and append the corresponding natural language explanations.Then we list the known Facts and Rules extracted from the Context with interleaved NL instructions.</p>
<p>After that, we represent the application of each Rule by utilizing separate blocks, line by line.We strive to preserve as many solver actions as possible, such as 'Binding' and 'Unbinding', as well as the acquisition of new implied Facts, and so forth.Noting that this information has already been obtained during the revealing phase, we focus on the refinement of the solver-derived reasoning process.Finally, we enumerate all newly implied Facts to enable the model to perform an interim review.</p>
<p>Turn-3: Query &amp; Answering Instructions.In Turn-3, we present instructions for answering a given Query.Following prior works (Ceri et al., 1989;Tafjord et al., 2021), a Query can be considered true within a certain logical context if it is explicitly mentioned or if it can be implied through several Rule applications.To handle negation, we consider two distinct assumptions: 1) the open-world assumption (OWA) that treats any fact that cannot be provable as special truth value 'unknown'; 2) the closed-world assumption (CWA) where any fact not provable is assumed 'false'.Following both assumptions, we adjust the answering instructions, particularly the 'Options' part.</p>
<p>Turn-4: Query-based Reasoning &amp; Formatted Answer.In the final Turn-4, we compare the parsed Query with all the known Facts and implied Facts, expecting the model to perform basic language inference and generate answer options in the desired format.</p>
<p>Fine-tuning Open-source LLMs</p>
<p>After obtaining the refined deductive reasoning instruction-tuning dataset, we can perform finetuning on open-source LLMs with the expectation that the trained model (i.e., LOGIPT) can possess reasoning abilities similar to those of solvers.Consequently, for any given Query, we can bypass the syntax or grammatical errors derived from NL-to-SL parsing by directly generating the answer with a fine-tuned LOGIPT.</p>
<p>Experiments</p>
<p>We construct our solver-derived instruction-tuning data on two public deductive reasoning datasets and evaluate LOGIPT on corresponding test sets.</p>
<p>Datasets</p>
<p>ProofWriter (Tafjord et al., 2021)</p>
<p>Baselines</p>
<p>We consider comparing LOGIPT with following groups of baselines:</p>
<p>Closed-source LMs: We include the Chat-GPT (gpt-3.5-turbo)(OpenAI, 2022), GPT-3.5 (text-davinci-003) (Ouyang et al., 2022) and GPT-4 (gpt-4) (OpenAI, 2023) as closed-source LMs for evaluation following Pan et al. (2023).</p>
<p>Open-source LMs:</p>
<p>We also evaluate opensource LMs for research community.Specifically, we choose Vicuna-13B (vicuna-13b-v1.5-16k)(Chiang et al., 2023), a chatbot trained by finetuning LLaMA-2 (Touvron et al., 2023b) on usershared conversations collected from ShareGPT5 , and CodeLlama-13B (Roziere et al., 2023), foundation models for code tasks.We select the base version (CodeLlama-13b-hf), and instruction finetuned version (CodeLlama-13b-Instruct-hf).</p>
<p>Solver-argumented LMs: Finally, we compare our model against the solver-argumented LMs.We focus on the representative LogicLM (Pan et al., 2023) with underlying LLMs ChatGPT (gpt-3.5-turbo),and , which serve as the state-of-theart deductive reasoning methods.</p>
<p>Apart from the LMs, we also analyze two types of prompting methods: i) Standard prompting that uses in-context learning with few-shot demonstrations to directly answer the given question; ii) Chain-of-Thought (CoT) that utilizes step-by-step problem-solving process to generate explanations where few-shot demonstrations are also provided, and then outputs the final answer.For a fair comparison, we use the same in-context examples, shown in Appendix A and B, for NL-to-SL parsing when evaluating all models on the same dataset, consistent with Pan et al. (2023).To enhance the clarification, we also provide a specific baseline 'Random Answering' that randomly outputs answer options.</p>
<p>Implementation Details</p>
<p>During the fine-tuning phase, we use a batch size of 32 per GPU and a learning rate of 1e-5 for all opensource LMs.We train our model on 8 Nvidia A100-80G GPUs with DeepSpeed ZeRO-3 (Rasley et al., 2020) for 12 hours on 2 epochs.For reproducibility, we use greedy decoding and set the temperature to 0 and the maximum context length to 8192.As for baselines, we strictly follow the setting of Pan et al. (2023).Given that all instances are presented in the form of multiple-choice questions, we assess the model's performance by the accuracy of selecting the correct answer option.</p>
<p>Main Results</p>
<p>We report the results of LOGIPT and baselines on Table 2 and have following main findings:</p>
<p>1) When prompting with few-shot examples, open-source LMs exhibit notably poor deductive reasoning capabilities, with their outputs closed to random answering.Even the Standard prompting models of ChatGPT (gpt-3.5-turbo)and GPT-3.5 (text-davinci-003) exhibit a similar performance to random answering.This once again demonstrates that it is considerably difficult for many LLMs to solve logical reasoning tasks.</p>
<p>2) LOGIPT is significantly superior to the state-of-the-art solver-augmented LMs by a large margin on both deductive reasoning benchmarks.In ProofWriter, our best-performing model, LOGIPT (CodeLlama-13b-hf), outperforms the currently state-of-the-art LogicLM (gpt-4) by an absolute improvement of 9.84%.Mean-while, in PrOntoQA, our best-performing model LOGIPT (vicuna-13b-v1.5-16k)exhibits an even higher absolute improvement of 13.20% than LogicLM (gpt-4).This indicates that our approach is better than the pipeline of problem formulation first and then reasoning with solvers, and finetuning with solver-derived reasoning data can facilitate the deductive reasoning capacity of LMs.</p>
<p>3) LOGIPT significantly outperforms all selected open/closed-source LMs on both datasets, except for the CoT experiment on the PrOntoQA data where LOGIPT achieves comparable results with GPT-4 CoT.This is surprising considering that our underlying open-source LMs are merely 13B parameters in size.As for the baseline experiments of GPT-4, our performance on ProofWriter also significantly surpasses that of GPT-4's Standard and CoT prompting versions, as well as the Standard version of PrOntoQA.These results further demonstrate that open-source LMs, when coupled with solver-simulated reasoning capacity, can achieve performance on par with or even superior to closedsource GPT models.</p>
<p>4) The accuracy of CodeLlama-13B-Base (CodeLlama-13b-hf) with Standard prompting was 0.00, and the performance of the CoT version was close to random answering.By examining the outputs, we found that this is due to the CodeLlama-13B-Base's inability to follow the provided fewshot demonstrations, resulting in outputting no answering options.The introduction of the Instruct version of CodeLlama-13B mitigates this issue to some extent.However, after training with LOGIPT, the CodeLlama models far less encounter this issue (i.e., following the right answering format in both test sets) and even achieve better performance than the Vicuna version of LOGIPT.This demonstrates the potential of code foundation models in logical reasoning tasks, consistent with the finding on prior work (Yue et al., 2023).</p>
<p>Further Analysis</p>
<p>Impact of Solver-derived Reasoning Formats</p>
<p>We further investigate the impact of different solverderived reasoning formats on the model's performance.Specifically, we consider the following format variations: 1) w/o 'unbind' statements that we remove all 'Unbind' statements from Turn-2 to investigate the utility of the explicit retention of this action from the solver; 2) w/o 'fail &amp; back-   track' statements that we removing all 'Fail &amp; backtrack' statements from Turn-2.During the solver's reasoning process, it is expected to encounter situations in which, after binding a value, the solver realizes that not all premises are satisfied (e.g., 'Fiona is blue' but 'Fiona is not quiet' for application of Rule3 in Figure 3).Consequently, a 'Fail &amp; backtrack' operation occurs (highlighted in color in Figure 3).We explore the effectiveness of explicitly stating these operations.</p>
<p>We present the accuracy of the variations on solver-derived reasoning format on ProofWriter in Table 3 where several observations can be made: 1) regardless of using the default format, removing 'Unbind' statements, or removing 'Fail &amp; backtrack' statements, it can not be determined which format guarantees the optimal results.To retain the maximum amount of action information that the solver can provide, we still adopt the de-fault settings in LOGIPT; 2) whether 'Unbind' statements are removed or 'Fail &amp; backtrack' statements are removed, there is always an experiment under each open-source LMs that can surpass the default LOGIPT results.This further enhances the best performance of LOGIPT shown in Table 2.</p>
<p>Impact of SL Reasoning Representations</p>
<p>We are also curious about the impact of SL reasoning representations.Therefore, we include additional experiments in Table 3, denoted as w/ NL representation that we re-translate the symbolic representation (e.g., Green('Charlie', True)) back to its original NL version (e.g., Charlie is green.)and replace the original symbolic representation in Turn-2.From the table, we can find that replacing SL representations with NL results in a significant decrease in model performance, further emphasizing that symbolic representations are superior to NL representations in deductive reasoning tasks.</p>
<p>Effectiveness of Merging Data from Different Reasoning Assumptions</p>
<p>Since ProofWriter is an open-world assumption and PrOntoQA is labeled within a closed-world assumption, we also perform a further investigation on whether both reasoning assumptions can benefit each other.Specifically, we first merge both constructed training data and then test LOGIPT on each test set.The experimental results are shown in Table 4.We can conclude that if we directly mix the two types of data for training, the results on their respective test sets will be slightly lower than those obtained from training solely on their respective datasets.Therefore, we conducted an in-depth analysis of the underlying reasons and observed that in PrOntoQA, the majority of Rules are in the format of 'Every/Each A is (not) B' or 'A are (not) B'.While in ProofWriter, the predominant structure of Rules consists of: 'If someone is A, then they are B' or 'If something is A, then it is B'.Therefore, we conducted an additional set of experiments in which the Rule format of two training sets was randomly reformatted into the four aforementioned types using regular expression (denoted as 'Both (Reformat)').Then, we test the model on the original test sets.We can observe that by employing this approach, the code models yield improved performance on ProofWriter.Thus, the style/genre of logical context must also be taken into consideration to maximize the efficacy of transfer learning in logical reasoning.</p>
<p>Related Work</p>
<p>Logical Reasoning with LMs.Recent efforts in adapting Large Language Models (LLMs) for logical reasoning tasks generally adopt direct finetuning specialized modules (Clark et al., 2020;Tafjord et al., 2021Tafjord et al., , 2022;;Yang et al., 2022) or in-context learning (Zhou et al., 2022;Lyu et al., 2023;Ling et al., 2023), where reasoning in NL is used by both groups of methods.Fine-tuning approaches involve training the full model or specialized modules, enhancing LLMs with module-level logical reasoning skills like proof, enumeration, and abduction (Tafjord et al., 2021).The in-context learning approaches create specific prompts to encourage LLMs' step-by-step reasoning skills.Common methods encompass chain-of-thought prompting (Wei et al., 2022b;Chen et al., 2023), which produces explanations before delivering a final answer, and least-to-most prompting (Zhou et al., 2022), which deconstructs a problem into simpler components that can be resolved individually.Some recent work has focused on combining neural networks with symbolic reasoning (Tian et al., 2022;Pryor et al., 2022;Pan et al., 2023), especially the solver-augmented LMs that parse NL logical questions into symbolic representations, then utilizing external logical solvers for answering.Despite their impressive performance, parsing errors can lead to solver execution failure and logical question-answering issues.To address this, we propose LOGIPT, which directly imitates the solver's reasoning ability and outputs the answer.</p>
<p>Augmented LMs for Reasoning.Recent work has begun to augment LMs to overcome their inherent limitations such as the incapacity to access up-to-date information or conduct accurate mathematical reasoning.They augment with external tools and resources, such as the information retriever (Shi et al., 2023;Lazaridou et al., 2022), planner (Liu et al., 2023a) and other pre-trained models (Shen et al., 2023).Specifically, to enhance the reasoning capacity, recent work resort to external off-the-shelf Solvers including programmatic interpreters (Chen et al., 2022;Gao et al., 2023), satisfiability solvers (Ye et al., 2023), logical solvers (Pan et al., 2023) or their hybrids (Poesia et al., 2023).Most of them utilize the LMs to parse the NL question to symbolic representations and then invoke solvers to reason in SL.In this paper, we concentrate on logical solvers, automated tools for validating the truth value of logical formulas.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel LOGIPT that can directly act as a logical solver for deductive reasoning tasks.LOGIPT can output all facts implied from NL logical questions, while bypassing the syntax or grammatical errors derived from NL-to-SL parsing of solver-augmented LMs.We conducted numerous analytical experiments on two public deductive reasoning benchmarks.Evaluation results show that LOGIPT can significantly outperform state-of-the-art solver-augmented LMs, and surpass or be comparable with few-shot prompting methods on competitive LLMs like ChatGPT or GPT-4.</p>
<p>Turn-4 (from LM):</p>
<p>The comment can be parsed into: Green('Dave', False) According to the implied fact: Green('Dave', True)</p>
<p>The comment contradicts the fact.So, the correct option is: B) False</p>
<p>Figure 1 :
1
Figure 1: Data flow of current solver-augmented LMs for inference (a), and our pipeline for LOGIPT (b,c).</p>
<p>Figure 2 :
2
Figure 2: A deductive reasoning question derived from ProofWriter and its parsed Facts, Rules, and Query.</p>
<p>True) Now begin reasoning to obtain all implied facts: Use rule1: Furry($x, True)  Quiet($x, True) Bind $x to 'Dave' Obtain a new implied fact: Quiet('Dave', True) Unbind $x Finish implied with rule1 Use rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Charlie' Obtain a new implied fact: Nice('Charlie', True) Unbind $x Bind $x to 'Fiona' Obtain an already known or implied fact: Nice('Fiona', True) Unbind $x Finish implied with rule2 Use rule3: Blue($x, True)  Quiet($x, True)  Green($x, True) Bind $x to 'Dave' Obtain a new implied fact: Green('Dave', True) Reuse rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Dave' Obtain a new implied fact: Nice('Dave', True) Finish implied with rule2 Unbind $x Bind $x to 'Fiona' Unbind $x Finish implied with rule3 Finally, we obtain following implied facts: Green('Dave', True) (... more facts here ...) Finish reasoning</p>
<p>Figure 4 :
4
Figure 4: The full version of the comprehensive 4-turn training example of our instruction-tuning data shown in Figure 3.</p>
<p>Table 2 :
2
Main results on two evaluation datasets.The best results of LOGIPT are in bold and the best results within each dataset are underlined.
is a commonly
of each example is one of {True, False, Unknown}.to ProofWriter, PrOntoQA is in a closed-world assumption (CWA) subset where the answer of each example is one of {True, False}.For training, we merely merge all subsets with fictional characters and obtained 15,940 training cases after filtering out syntax-invalid ones.</p>
<p>Table 3 :
3
The accuracy of the variations on solver-derived reasoning format, and replacing SL representations with NL on ProofWriter.The best results on each underlying LMs are underlined.
Train setTest SetVCN CLBCLIPrOntoQAPrOntoQA96.40 95.60 96.20BothPrOntoQA91.00 87.00 89.00Both (Reformat) PrOntoQA90.00 87.00 77.80ProofWriterProofWriter 81.17 89.50 81.67BothProofWriter 79.33 87.17 79.67Both (Reformat) ProofWriter 79.00 90.83 84.50</p>
<p>Table 4 :
4
The accuracy of LOGIPT trained with merged data and tested on single data with different underlying
LMs. 'VCN', 'CLB', and 'CLI' respectively representVicuna-13B, CodeLlama-13B-Base, and CodeLlama-13B-Instruct. 'Both' means 'ProofWriter + PrOntoQA'.
In this paper, the term 'Query' refers to a specific sentence of statement or comment, while 'question' is used in a broader sense to denote the description of a logical problem.
In the original case, the Query is 'Charlie is not green.'. We replace it with 'Dave is not green.' for better illustration.
https://sharegpt.com/
A Instructions for NL-to-SL Parsing on ProofWriterTask Description: You are given a problem description and a question.The task is to: 1) define all the predicates in the problem 2) parse the problem into logic rules based on the defined predicates 3) write all the facts mentioned in the problem 4) parse the question into the logic formProblem:Anne is quiet.Erin is furry.(... more context here ...) All red people are young.Question:Based on the above information, is the following statement true, false, or unknown?Anne is white.Predicates:
What you always wanted to know about datalog(and never dared to ask). Stefano Ceri, Georg Gottlob, Letizia Tanca, IEEE transactions on knowledge and data engineering. 111989</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models. Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.143232023arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-202020</p>
<p>Programming in PROLOG. Christopher S William F Clocksin, Mellish, 2003Springer Science &amp; Business Media</p>
<p>Applying expert system technology to code reuse with pyke. Bruce Frederiksen, 2008PyCon: Chicago</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. 2023PMLR. Google. 2023. Google bard</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Fifty years of prolog and beyond. Philipp Krner, Michael Leuschel, Joo Barbosa, Santos Vtor, Vernica Costa, Manuel V Dahl, Jose F Hermenegildo, Jan Morales, Daniel Wielemaker, Salvador Diaz, Abreu, Theory and Practice of Logic Programming. 2262022</p>
<p>Internetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022arXiv preprint</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.03872Deductive verification of chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-ofthought reasoning. 2023arXiv preprint</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>ArXiv, abs/2303.08774Gpt-4 technical report. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, arXiv:2306.04031Certified reasoning with language models. 2023arXiv preprint</p>
<p>L David, Alan K Poole, Mackworth, Artificial Intelligence: foundations of computational agents. Cambridge University Press2010</p>
<p>Connor Pryor, Charles Dickens, Eriq Augustine, Alon Albalak, William Wang, Lise Getoor, arXiv:2205.14268Neupsl: Neural probabilistic soft logic. 2022arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jrmy Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, 10.18653/v1/2022.emnlp-main.134Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Weakly supervised neural symbolic learning for cognitive tasks. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Prajjwal Soumya, Shruti Bhargava, Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, 10.18653/v1/2022.emnlp-main.7Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Xi Ye, Qiaochu Chen, arXiv:2305.09656Isil Dillig, and Greg Durrett. 2023. Satisfiability-aided language models using declarative prompting. arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2022</p>
<p>statement into known facts and rules based on the defined predicates 3. begin reasoning to obtain all implied facts Context: Charlie is green. Charlie is white. Dave is blue. Dave is furry. Dave is white. Fiona is blue. Fiona is green. Fiona is nice. Fiona is rough. Fiona is white. All furry people are quiet. All green, white people are nice. If someone is blue and quiet then they are green. Reasoning: Turn-2 (from LM): We first define following predicates and corresponding natural language explanations: Green($x, bool) ::: Is x green? White($x, bool) ::: Is x white? Blue($x, bool) ::: Is x blue? Furry($x, bool) ::: Is x furry? Nice($x, bool) ::: Is x nice?. Truefrom human): Task Description: Given a statement containing logical facts and rules in natural languages as context, the task is to: 1. define all the predicates in logic forms from the context 2. parse the. Rough($x, bool) ::: Is x rough? Quiet($x, bool) ::: Is x quiet? We have following known facts from the context: Green('Charlie</p>
<p>Quiet($x, True)  Green($x, True) Now begin reasoning to obtain all implied facts: Use rule1: Furry($x, True)  Quiet($x, True) Bind $x to 'Dave' Obtain a new implied fact: Quiet('Dave', True) Unbind $x Finish implied with rule1 Use rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Charlie' Obtain a new implied fact: Nice('Charlie', True) Unbind $x Bind $x to 'Fiona' Obtain an already known or implied fact: Nice('Fiona', True) Unbind $x Finish implied with rule2 Use rule3: Blue($x, True)  Quiet($x, True)  Green($x, True) Bind $x to 'Dave' Obtain a new implied fact: Green('Dave', True) Reuse rule2: Green($x, True)  White($x, True)  Nice($x, True) Bind $x to 'Dave' Obtain a new implied fact: Nice('Dave', True) Finish implied with rule2 Unbind $x Bind $x to 'Fiona' Unbind $x Finish implied with rule3 Finally, we obtain following implied facts: Green('Dave', True) Nice('Charlie', True) Nice('Dave', True) Quiet('Dave', True) Finish reasoning Turn-3 (from human): Question: Based on the above information, is the following comment true, false, or unknown? Dave is not green. Options: A) True B) False C) Unknown Now you should: 1. reason first carefully with the above known facts and implied facts 2. ( Green, ' Fiona, ' True) White('charlie, ' True) White('dave, ) True, ( White, ' Fiona, ) True, ' Blue('dave, ) True, ( Blue, ' Fiona, ) True, ( Furry, ' 'dave, ) True, ( Nice, ' Fiona, ) True, Rough, We have following known rules from the context: rule1: Furry($x, True)  Quiet($x, True) rule2: Green($x, True)  White($x, True)  Nice($x, True) rule3: Blue($x, True) . then output your answer strictly in the format of "So, the correct option is: X". The X should be replaced by "A) True", "B) False" or "C) Unknown"</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8249 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8249</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8249</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-267627459</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07757v1.pdf" target="_blank">Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model</a></p>
                <p><strong>Paper Abstract:</strong> Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8249.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8249.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stepwise Inference (CoT / Scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stepwise Inference (Chain-of-Thought / Scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Protocol where the model is allowed or prompted to produce intermediate tokens (a sequence of nodes) representing substeps toward a final goal; implemented here by training/evaluating models on sequences that include full paths between start and goal nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-style autoregressive Transformer (nanoGPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decode-only autoregressive Transformer based on nanoGPT: primary experiments use a 2-layer Transformer (GPT-style) with embedding dim 64; mechanistic analyses use a 1-layer attention-only variant with varied embedding dimensionality (4–64).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot chain-of-thought (stepwise inference via prompt prefix)', 'scratchpad (explicit intermediate tokens)', 'few-shot chain-of-thought when combined with exemplars (see separate entity)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Stepwise inference implemented by providing training sequences that explicitly contain intermediate nodes (full paths). Zero-shot style is simulated by allowing generation of intermediate tokens via a special prefix; scratchpad is represented by sequence format that writes intermediate computations/nodes into the autoregressive buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared two dataset regimes / models: (A) trained on sequences with intermediate steps (stepwise inference allowed) and (B) trained on sequences with only start and goal tokens (direct inference). Evaluation compares classification accuracy on held-out start/goal pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Custom synthetic graph navigation task (Bernoulli DAGs and hierarchical DAGs): predict or generate a path between a start node Xs and a goal node Xg; datasets constructed with/without intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Stepwise inference yields a substantial accuracy improvement over direct inference (the paper refers to this as the 'stepwise inference gap'); improvement is observed on both Bernoulli and hierarchical DAGs and is larger for hierarchical graphs. The gap persists under up to 20% token corruption (label noise). Exact per-condition numeric accuracies are presented in figures but not enumerated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Stepwise inference markedly improves ability to 'stitch' together sub-paths learned during training, particularly when training only contains short paths and test requires longer stitched paths; benefits larger in hierarchical graphs; robustness to moderate label noise; training dynamics show the model first eliminates hallucinated edges then learns planning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Providing intermediate steps during training or prompting (stepwise inference) substantially improves transformer performance on multi-step navigation tasks, especially when test-time tasks require composing shorter trained paths into longer ones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8249.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8249.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Inference (no intermediate steps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach where models are trained to map start and goal nodes directly to a final token indicating 'path' or 'no path' without producing intermediate nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-style autoregressive Transformer (nanoGPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 2-layer autoregressive Transformer as above used in direct-inference condition; trained on sequences containing only start and goal tokens plus terminal 'path'/'no-path' token.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['direct mapping (single-step decision)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>No intermediate tokens are provided or generated; model trained to predict a final classification token (p1/p0) from start and goal tokens only.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as the paired baseline to stepwise inference: distinct models trained on the 'no-intermediate' dataset and compared on held-out start/goal pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same custom graph navigation classification task as above (path existence classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Lower accuracy than stepwise-inference-trained models across Bernoulli and hierarchical DAGs; the difference (stepwise inference gap) is more pronounced in hierarchical graphs and when training only contains short paths relative to test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Direct inference underperforms when the test requires composing multiple subpaths that were observed separately during training; indicates planning/compositional stitching is harder without explicit intermediate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Direct inference performs worse than stepwise inference on multi-step graph navigation, demonstrating the advantage of allowing intermediate step generation for planning and compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8249.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8249.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diversity–Accuracy Tradeoff (Sampling Temperature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity–Accuracy Tradeoff induced by Sampling Temperature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical phenomenon where increasing sampling temperature increases diversity (number of unique generated paths) but decreases accuracy (probability generated path is valid and reaches the goal).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-style autoregressive Transformer (nanoGPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>2-layer autoregressive Transformer; generations sampled at varying temperatures (0.0–3.0) for a fixed start/goal pair; 3000 generation trials per temperature sweep.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sampling-based diverse generation (varying temperature)', 'single-step greedy decoding (low temperature / temperature=0)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Diversity induced by stochastic sampling at higher temperature; accuracy measured as fraction of generated sequences that constitute valid paths ending at the goal; diversity measured as count of unique valid paths produced.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Prompted the same trained model 3,000 times for a fixed start/goal pair while sweeping sampling temperature from 0.0 to 3.0, and measured unique valid-path counts (diversity) and fraction valid & goal-reaching (accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Single-pair path-generation task within the synthetic graph navigation setup (fixed Xs and Xg).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative (figure-backed) result: lower temperatures produce few but high-accuracy paths; higher temperatures produce many unique paths with lower validity/accuracy. Exact numeric tradeoff curves are shown in Fig.4 but not enumerated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>There exists an explicit tradeoff: sampling hyperparameters that increase output diversity reduce path validity/accuracy. This is the paper's first quantitative demonstration of such a tradeoff for Transformer outputs in a planning-style task.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generation hyperparameters (sampling temperature) control a tradeoff between exploration (diverse candidate reasoning traces) and reliability (validity/accuracy); higher diversity comes at the cost of correctness in stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8249.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8249.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simplicity Bias / Distance-based Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simplicity Bias and Emergent Distance-based (Pattern-matching) Navigation Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The trained model exhibits a simplicity bias—favoring shorter paths—and mechanistically implements an approximate distance-based next-step rule realized via learned token/value embeddings and inner-product-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>1-layer attention-only Transformer (mechanistic probe) and full 2-layer model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mechanistic analyses used a 1-head, 1-layer attention-only Transformer variant (no MLP/post-LN) to visualize attention and extract value/token embedding operations; the full trained 2-layer model was compared against a simplified algorithm derived from embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['pattern-matching via learned embeddings (approximate distance minimization)', 'single-algorithm greedy next-step selection']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model computes value embeddings for current and goal tokens, sums them, and selects next token whose token embedding has maximal inner product with this sum—effectively moving to neighbor with minimal embedding-inferred distance to goal.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mechanistic ablation: strip model to single-head attention, visualize attention (shows concentration on goal and current), derive simplified algorithm (v = v_goal + v_current, choose token maximizing inner product), and compare its outputs/accuracy to full model; also analyze inner product vs ground-truth graph distance correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same synthetic graph navigation task; mechanistic metrics on held-out pairs and path similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Simplified algorithm accuracy on 500 held-out (Xs,Xg) pairs: 99.8% vs full trained model 99.6% (reported in text). Path outputs between simplified algorithm and full model match nearly exactly: over 75% of paths are identical (string-edit / Levenshtein analysis). The inner product of goal value-embedding with candidate-next token embedding is negatively correlated with ground-truth distance (least-squares slope reported as -0.106 in Fig.7e).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Model implements a compact, repeatable heuristic (move to neighbor whose learned embedding is closest to goal embedding) leading to a simplicity/short-path bias; this pattern-matching strategy explains preference for shortest/quickest solutions and helps account for certain failure modes (omitted important intermediates, 'shortcut' solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The model converges to a single approximate algorithm (inner-product-based distance heuristic) to perform multi-step planning; this learned, repeated method induces a simplicity bias (shorter paths), explains output preferences, and accounts for high similarity between generated and simplified-algorithm paths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8249.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8249.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context Exemplars (Few-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context Exemplars for Few-shot Chain-of-Thought (Motif chaining / compositional control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of exemplar sequences sampled from motifs (small subgraphs) placed in context to steer model navigation across chains of motifs via 'ghost edges'; probes compositional generalization, length generalization, and conflict/primacy effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-style autoregressive Transformer (nanoGPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>2-layer GPT-style model trained on motif-chained datasets; contexts assembled by concatenating exemplar sequences that demonstrate traversal between motifs connected by ghost edges; test tasks involve unseen motif-order combinations and longer motif chains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['few-shot chain-of-thought via in-context exemplars', 'subgoal planning using exemplars (ghost-edge subgoals)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Exemplars are constructed by sampling paths across adjacent motifs connected by ghost edges; these exemplar sequences are concatenated into context so the model can infer which motifs are connected and follow those subgoals to generate a path from start to goal across multiple motifs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Train on a set of motif-order chains (e.g., up to chain length 4); at test, present unseen motif-order combinations (held-out orders) and vary number of exemplars (chain length) as well as include conflicting exemplar chains to probe primacy; measure steerability success and whether path includes all ghost edges and terminates correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Motif-chaining synthetic navigation task: multiple Bernoulli DAG motifs connected by ghost edges to form chains; exemplars provided in-context to indicate connections.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Model successfully follows exemplar-defined chains and generalizes to unseen orders up to the maximum chain length seen in training (n up to 4 in experiments); beyond training chain length the model fails to navigate the chain. In conflict conditions, the model shows a strong primacy bias toward the first exemplar chain in context (quantitative curves in Fig.9 but exact numbers not enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>In-context exemplars can control the model's preferred path and elicit compositional generalization; the model treats nodes across ghost edges as subgoals (attention patterns support this). However, length generalization is limited to chain lengths seen in training, and when multiple conflicting exemplar chains are present the model favors the first chain (primacy bias).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Exemplar-based stepwise inference enables controllable, compositional navigation across motifs (few-shot CoT), but the model's ability to generalize to longer reasoning chains is bounded by training chain lengths and exhibits primacy bias when exemplars conflict.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 2)</em></li>
                <li>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 2)</em></li>
                <li>Evaluating cognitive maps in large language models with cogeval: No emergent planning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8249",
    "paper_id": "paper-267627459",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Stepwise Inference (CoT / Scratchpad)",
            "name_full": "Stepwise Inference (Chain-of-Thought / Scratchpad)",
            "brief_description": "Protocol where the model is allowed or prompted to produce intermediate tokens (a sequence of nodes) representing substeps toward a final goal; implemented here by training/evaluating models on sequences that include full paths between start and goal nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-style autoregressive Transformer (nanoGPT-based)",
            "model_description": "Decode-only autoregressive Transformer based on nanoGPT: primary experiments use a 2-layer Transformer (GPT-style) with embedding dim 64; mechanistic analyses use a 1-layer attention-only variant with varied embedding dimensionality (4–64).",
            "reasoning_methods": [
                "zero-shot chain-of-thought (stepwise inference via prompt prefix)",
                "scratchpad (explicit intermediate tokens)",
                "few-shot chain-of-thought when combined with exemplars (see separate entity)"
            ],
            "reasoning_methods_description": "Stepwise inference implemented by providing training sequences that explicitly contain intermediate nodes (full paths). Zero-shot style is simulated by allowing generation of intermediate tokens via a special prefix; scratchpad is represented by sequence format that writes intermediate computations/nodes into the autoregressive buffer.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared two dataset regimes / models: (A) trained on sequences with intermediate steps (stepwise inference allowed) and (B) trained on sequences with only start and goal tokens (direct inference). Evaluation compares classification accuracy on held-out start/goal pairs.",
            "task_or_benchmark": "Custom synthetic graph navigation task (Bernoulli DAGs and hierarchical DAGs): predict or generate a path between a start node Xs and a goal node Xg; datasets constructed with/without intermediate steps.",
            "performance_results": "Stepwise inference yields a substantial accuracy improvement over direct inference (the paper refers to this as the 'stepwise inference gap'); improvement is observed on both Bernoulli and hierarchical DAGs and is larger for hierarchical graphs. The gap persists under up to 20% token corruption (label noise). Exact per-condition numeric accuracies are presented in figures but not enumerated in the text.",
            "qualitative_findings": "Stepwise inference markedly improves ability to 'stitch' together sub-paths learned during training, particularly when training only contains short paths and test requires longer stitched paths; benefits larger in hierarchical graphs; robustness to moderate label noise; training dynamics show the model first eliminates hallucinated edges then learns planning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Providing intermediate steps during training or prompting (stepwise inference) substantially improves transformer performance on multi-step navigation tasks, especially when test-time tasks require composing shorter trained paths into longer ones.",
            "uuid": "e8249.0",
            "source_info": {
                "paper_title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Direct Inference",
            "name_full": "Direct Inference (no intermediate steps)",
            "brief_description": "Baseline approach where models are trained to map start and goal nodes directly to a final token indicating 'path' or 'no path' without producing intermediate nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-style autoregressive Transformer (nanoGPT-based)",
            "model_description": "Same 2-layer autoregressive Transformer as above used in direct-inference condition; trained on sequences containing only start and goal tokens plus terminal 'path'/'no-path' token.",
            "reasoning_methods": [
                "direct mapping (single-step decision)"
            ],
            "reasoning_methods_description": "No intermediate tokens are provided or generated; model trained to predict a final classification token (p1/p0) from start and goal tokens only.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as the paired baseline to stepwise inference: distinct models trained on the 'no-intermediate' dataset and compared on held-out start/goal pairs.",
            "task_or_benchmark": "Same custom graph navigation classification task as above (path existence classification).",
            "performance_results": "Lower accuracy than stepwise-inference-trained models across Bernoulli and hierarchical DAGs; the difference (stepwise inference gap) is more pronounced in hierarchical graphs and when training only contains short paths relative to test tasks.",
            "qualitative_findings": "Direct inference underperforms when the test requires composing multiple subpaths that were observed separately during training; indicates planning/compositional stitching is harder without explicit intermediate outputs.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Direct inference performs worse than stepwise inference on multi-step graph navigation, demonstrating the advantage of allowing intermediate step generation for planning and compositional generalization.",
            "uuid": "e8249.1",
            "source_info": {
                "paper_title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Diversity–Accuracy Tradeoff (Sampling Temperature)",
            "name_full": "Diversity–Accuracy Tradeoff induced by Sampling Temperature",
            "brief_description": "Empirical phenomenon where increasing sampling temperature increases diversity (number of unique generated paths) but decreases accuracy (probability generated path is valid and reaches the goal).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-style autoregressive Transformer (nanoGPT-based)",
            "model_description": "2-layer autoregressive Transformer; generations sampled at varying temperatures (0.0–3.0) for a fixed start/goal pair; 3000 generation trials per temperature sweep.",
            "reasoning_methods": [
                "sampling-based diverse generation (varying temperature)",
                "single-step greedy decoding (low temperature / temperature=0)"
            ],
            "reasoning_methods_description": "Diversity induced by stochastic sampling at higher temperature; accuracy measured as fraction of generated sequences that constitute valid paths ending at the goal; diversity measured as count of unique valid paths produced.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Prompted the same trained model 3,000 times for a fixed start/goal pair while sweeping sampling temperature from 0.0 to 3.0, and measured unique valid-path counts (diversity) and fraction valid & goal-reaching (accuracy).",
            "task_or_benchmark": "Single-pair path-generation task within the synthetic graph navigation setup (fixed Xs and Xg).",
            "performance_results": "Qualitative (figure-backed) result: lower temperatures produce few but high-accuracy paths; higher temperatures produce many unique paths with lower validity/accuracy. Exact numeric tradeoff curves are shown in Fig.4 but not enumerated in the text.",
            "qualitative_findings": "There exists an explicit tradeoff: sampling hyperparameters that increase output diversity reduce path validity/accuracy. This is the paper's first quantitative demonstration of such a tradeoff for Transformer outputs in a planning-style task.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Generation hyperparameters (sampling temperature) control a tradeoff between exploration (diverse candidate reasoning traces) and reliability (validity/accuracy); higher diversity comes at the cost of correctness in stepwise reasoning.",
            "uuid": "e8249.2",
            "source_info": {
                "paper_title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Simplicity Bias / Distance-based Algorithm",
            "name_full": "Simplicity Bias and Emergent Distance-based (Pattern-matching) Navigation Algorithm",
            "brief_description": "The trained model exhibits a simplicity bias—favoring shorter paths—and mechanistically implements an approximate distance-based next-step rule realized via learned token/value embeddings and inner-product-based selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "1-layer attention-only Transformer (mechanistic probe) and full 2-layer model",
            "model_description": "Mechanistic analyses used a 1-head, 1-layer attention-only Transformer variant (no MLP/post-LN) to visualize attention and extract value/token embedding operations; the full trained 2-layer model was compared against a simplified algorithm derived from embeddings.",
            "reasoning_methods": [
                "pattern-matching via learned embeddings (approximate distance minimization)",
                "single-algorithm greedy next-step selection"
            ],
            "reasoning_methods_description": "Model computes value embeddings for current and goal tokens, sums them, and selects next token whose token embedding has maximal inner product with this sum—effectively moving to neighbor with minimal embedding-inferred distance to goal.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Mechanistic ablation: strip model to single-head attention, visualize attention (shows concentration on goal and current), derive simplified algorithm (v = v_goal + v_current, choose token maximizing inner product), and compare its outputs/accuracy to full model; also analyze inner product vs ground-truth graph distance correlation.",
            "task_or_benchmark": "Same synthetic graph navigation task; mechanistic metrics on held-out pairs and path similarity.",
            "performance_results": "Simplified algorithm accuracy on 500 held-out (Xs,Xg) pairs: 99.8% vs full trained model 99.6% (reported in text). Path outputs between simplified algorithm and full model match nearly exactly: over 75% of paths are identical (string-edit / Levenshtein analysis). The inner product of goal value-embedding with candidate-next token embedding is negatively correlated with ground-truth distance (least-squares slope reported as -0.106 in Fig.7e).",
            "qualitative_findings": "Model implements a compact, repeatable heuristic (move to neighbor whose learned embedding is closest to goal embedding) leading to a simplicity/short-path bias; this pattern-matching strategy explains preference for shortest/quickest solutions and helps account for certain failure modes (omitted important intermediates, 'shortcut' solutions).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The model converges to a single approximate algorithm (inner-product-based distance heuristic) to perform multi-step planning; this learned, repeated method induces a simplicity bias (shorter paths), explains output preferences, and accounts for high similarity between generated and simplified-algorithm paths.",
            "uuid": "e8249.3",
            "source_info": {
                "paper_title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "In-context Exemplars (Few-shot CoT)",
            "name_full": "In-context Exemplars for Few-shot Chain-of-Thought (Motif chaining / compositional control)",
            "brief_description": "Use of exemplar sequences sampled from motifs (small subgraphs) placed in context to steer model navigation across chains of motifs via 'ghost edges'; probes compositional generalization, length generalization, and conflict/primacy effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-style autoregressive Transformer (nanoGPT-based)",
            "model_description": "2-layer GPT-style model trained on motif-chained datasets; contexts assembled by concatenating exemplar sequences that demonstrate traversal between motifs connected by ghost edges; test tasks involve unseen motif-order combinations and longer motif chains.",
            "reasoning_methods": [
                "few-shot chain-of-thought via in-context exemplars",
                "subgoal planning using exemplars (ghost-edge subgoals)"
            ],
            "reasoning_methods_description": "Exemplars are constructed by sampling paths across adjacent motifs connected by ghost edges; these exemplar sequences are concatenated into context so the model can infer which motifs are connected and follow those subgoals to generate a path from start to goal across multiple motifs.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Train on a set of motif-order chains (e.g., up to chain length 4); at test, present unseen motif-order combinations (held-out orders) and vary number of exemplars (chain length) as well as include conflicting exemplar chains to probe primacy; measure steerability success and whether path includes all ghost edges and terminates correctly.",
            "task_or_benchmark": "Motif-chaining synthetic navigation task: multiple Bernoulli DAG motifs connected by ghost edges to form chains; exemplars provided in-context to indicate connections.",
            "performance_results": "Model successfully follows exemplar-defined chains and generalizes to unseen orders up to the maximum chain length seen in training (n up to 4 in experiments); beyond training chain length the model fails to navigate the chain. In conflict conditions, the model shows a strong primacy bias toward the first exemplar chain in context (quantitative curves in Fig.9 but exact numbers not enumerated in text).",
            "qualitative_findings": "In-context exemplars can control the model's preferred path and elicit compositional generalization; the model treats nodes across ghost edges as subgoals (attention patterns support this). However, length generalization is limited to chain lengths seen in training, and when multiple conflicting exemplar chains are present the model favors the first chain (primacy bias).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Exemplar-based stepwise inference enables controllable, compositional navigation across motifs (few-shot CoT), but the model's ability to generalize to longer reasoning chains is bounded by training chain lengths and exhibits primacy bias when exemplars conflict.",
            "uuid": "e8249.4",
            "source_info": {
                "paper_title": "Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 2,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 2,
            "sanitized_title": "language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "Evaluating cognitive maps in large language models with cogeval: No emergent planning",
            "rating": 2,
            "sanitized_title": "evaluating_cognitive_maps_in_large_language_models_with_cogeval_no_emergent_planning"
        }
    ],
    "cost": 0.014158500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model
12 Feb 2024</p>
<p>Mikail Khona 
Massachusetts Institute of Technology</p>
<p>NTT Physics and Informatics Lab 3 CIIRC
University of Ostrava</p>
<p>Maya Okawa 
NTT Physics and Informatics Lab 3 CIIRC
University of Ostrava</p>
<p>Jan Hula 
Rahul Ramesh 
NTT Physics and Informatics Lab 3 CIIRC
University of Ostrava</p>
<p>University of Pennsylvania</p>
<p>Kento Nishi 
NTT Physics and Informatics Lab 3 CIIRC
University of Ostrava</p>
<p>Center for Brain Science
Harvard University</p>
<p>Robert Dick 
University of Michigan</p>
<p>Ekdeep Singh Lubana 
Center for Brain Science
Harvard University</p>
<p>University of Michigan</p>
<p>Hidenori Tanaka <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#104;&#105;&#100;&#101;&#110;&#111;&#114;&#105;&#46;&#116;&#97;&#110;&#97;&#107;&#97;&#64;&#102;&#97;&#115;&#46;&#104;&#97;&#114;&#118;&#97;&#114;&#100;&#46;&#101;&#100;&#117;">&#104;&#105;&#100;&#101;&#110;&#111;&#114;&#105;&#46;&#116;&#97;&#110;&#97;&#107;&#97;&#64;&#102;&#97;&#115;&#46;&#104;&#97;&#114;&#118;&#97;&#114;&#100;&#46;&#101;&#100;&#117;</a>. 
NTT Physics and Informatics Lab 3 CIIRC
University of Ostrava</p>
<p>Center for Brain Science
Harvard University</p>
<p>Hidenori Tanaka</p>
<p>Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model
12 Feb 20242A8831DE1A0B55A6F9640BF0480D0041arXiv:2402.07757v1[cs.LG]
Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems.Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive.To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful.Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph.Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars.Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.</p>
<p>Introduction</p>
<p>Transformers, the backbone of large language models (LLMs), have revolutionized several domains of machine learning (OpenAI, 2023;Anil et al., 2023;Gemini et al., 2023; Touvron et al., 2023).An intriguing capability that emerges with training of Transformers on large-scale language modeling datasets is the ability to perform stepwise inference, such as zero-shot chain-of-thought (CoT) (Kojima et al., 2022), use of scratchpads (Nye et al., 2021), few-shot CoT (Wei et al., 2022), and variants of these protocols (Creswell et al., 2022;Yao et al., 2023;Besta et al., 2023;Creswell &amp; Shanahan, 2022;Press et al., 2022).Specifically, in stepwise inference, the model is asked to or shown exemplars describing how to decompose a broader problem into multiple sub-problems.Solving these sub-problems in a step-by-step manner simplifies the overall task and significantly improves performance (see Fig. 1).Arguably, stepwise inference protocols are the workhorse behind the "sparks" of intelligence demonstrated by LLMs (Bubeck et al., 2023;Suzgun et al., 2022;Lu et al., 2023;Huang &amp; Chang, 2022)-yet, their inner workings are poorly understood.</p>
<p>Motivated by the above, we aim to design and study an abstraction which enables a precise understanding of stepwise inference in Transformers.Specifically, we argue that tasks which see maximum benefit from stepwise inference can be cast as a graph navigation problem: given an input describing the data to operate on and a goal to be achieved, a sequence of primitive skills (e.g., ability to perform arithmetic operations) is chained such that each skill acts on the previous skill's output, ultimately to achieve the given goal.If the input data, the final goal, and the sequence of intermediate outputs are represented as a sequence of nodes of a graph, along with primitive skills as edges connecting these nodes, the overall task can be re-imagined as navigating nodes of the graph via the execution of primitive skills.</p>
<p>Several logical reasoning problems come under the purview of this abstraction (LaValle, 2006;Cormen et al., 2022;Momennejad et al., 2023;Dziri et al., 2023;Saparov &amp; He, 2023): e.g., in Fig. 1a, we show how the problem of Tower of Hanoi can be decomposed into simpler sub-problems.</p>
<p>See also Appendix B for several more examples.</p>
<p>This work.We design a graph navigation task wherein a Transformer is trained from scratch to predict whether two nodes from a well-defined graph can be connected via a path.A special prefix indicates to the model whether it can generate intermediate outputs to solve the task, i.e., if it can generate a sequence of nodes to infer a path connecting the two nodes; alternatively, exemplars demonstrating navigation to "regions" of the graph are provided.Our framework assumes the model has perfect skills, i.e, any failures in the task are a consequence of incorrect plans for navigating the graph.This is justified because a skill-based failure is the most trivial mechanism via which stepwise inference protocols can fail; in contrast, inability to plan is an independent and underexplored axis for understanding stepwise inference.Overall, we make the following contributions.</p>
<p>• A Framework for Investigating Stepwise Inference.</p>
<p>We propose a synthetic graph navigation task as an abstraction of scenarios where stepwise inference protocols help Transformers improve performance, showing that we can replicate and explain behaviors seen with use of stepwise inference in prior work.For instance, the structure of the data generating process (the graph) impacts whether stepwise inference will yield any benefits (Prystawski &amp; Goodman, 2023).We identify further novel behaviors of stepwise inference as well, such as the existence of a tradeoff between diversity of outputs generated by the model and its accuracy with respect to inference hyperparameters (e.g., sampling temperature).• Demonstrating a Simplicity Bias in Stepwise Inference.When multiple solutions are possible for an input, we demonstrate the existence of a simplicity bias: the model prefers to follow the shortest path connecting two nodes.We assess this result mechanistically by identifying the underlying algorithm learned by the model to solve the task, showing the bias is likely a consequence of a "pattern matching" behavior that has been hypothesized to cause LLMs to fail in complex reasoning problems (Dziri et al., 2023).• Controllability via In-Context Exemplars.We show the model's preferred path to navigate between two nodes can be controlled via use of in-context exemplars.We use this setup to evaluate the model's ability to generalize to paths of longer length and the influence of exemplars which conflict with each other, i.e., that steer the model along different paths.</p>
<p>Stepwise Inference as Graph Navigation</p>
<p>In this section, we define our setup for studying how stepwise inference aids Transformers in solving complex reasoning problems.Specifically, we define a graph navigation task wherein, given a start and a goal node, a Transformer is autoregressively trained to produce a sequence of nodes that concludes at the goal node.In our experiments, we consider two scenarios: one where in-context exemplars are absent (see Fig. 2a) and another where they are present (see Fig. 2b).The former scenario emulates protocols such as the scratchpad and zero-shot Chain of Thought (CoT) (Kojima et al., 2022;Nye et al., 2021), while the latter models few-shot CoT (Wei et al., 2022).In Section 2.1, we set up our experiment to explore these two scenarios.In the subsequent sections, we explicitly analyze the benefits of stepwise inference in both scenarios: without in-context exemplars (Section 2.2) and with in-context exemplars (Section 2.3).We refer the reader to a detailed related work on stepwise inference protocols in Appendix A and further discussion on graph navigation as a model of stepwise inference which is in Appendix B.</p>
<p>Preliminaries: Bernoulli and Hierarchical DAGs</p>
<p>We use directed acyclic graphs (DAGs) to define our graph navigation tasks.DAGs are a natural mathematical abstraction to study multi-step, logical reasoning problems: e.g., as discussed in Dziri et al. (2023), the output of any deterministic algorithm can be represented as a DAG.Specifically, a DAG is defined as G := (N, E), where
N := {X i } |N |
i=1 denotes the set of nodes in the graph and E := {(X i , X j )} Xi,Xj ∈N denotes the set of directed edges across the nodes.The edges of a DAG are captured by its adjacency matrix A, where A ij = 1 if (X i , X j ) ∈ E. A directed simple path is a sequence of distinct nodes of G which are joined by a sequence of edges.If two nodes are connected via a directed simple path, we call them pathconnected.The first node of a path is referred to as the start node, which we denote as X s , and the last node as the goal node, which we denote as X g .</p>
<p>We briefly discuss the process of construction of DAGs used in our work and how paths are sampled from them; a more
g 1 g 3 X 32 X 45 X 51 X 12 X 23 X 37 X 43 X 15 X 18 X 28 X 5 X 35 X 26 X 48 goal: X 45 X 51 X 15 X 48 X 5 X 5 X 32 Start Goal goal: X 45 X 51 X 15 X 48 X 5 X 5 X 32 Context goal: X 99 X 14 X 8 X 60 X 44 X 64 X 64 goal:
4. Prompt model to perform inference with context   thorough description is provided in Appendix C.1.We define a Bernoulli DAG of N nodes, whose adjacency matrix has an upper triangular structure with Bernoulli entries with edge density p, such that p(A ij = 1) = p.We ensure that all nodes have at least one edge (see Fig. 2a).The resulting DAG exhibits a bell-shaped path length distribution (see Fig. 11 in Appendix C.1).We also define a hierarchical DAG, wherein the nodes follow a feedforward, layered structure such that all nodes at a given layer are only connected to nodes in the following layer (see Fig. 2a).In particular, for every node n l in layer l and n l+1 in layer l + 1, we draw a directed edge (n l , n l+1 ) with probability p, which we refer to as edge density.On average, between any two layers of a hierarchical DAG, there are pN 2 edges and each node in an intermediate layer has an out-degree and in-degree of pN .The number of paths from a particular node in layer l to layer l ′ &gt; l is exponential and given by (pN ) l ′ −l ; this is quantified in the path length distribution shown in Appendix Fig. 11.For both graph structures, source nodes are nodes that do not have any parent nodes, and the nodes that do not have any children nodes are sink nodes.
X 28 X 18 X 28 X 77 X 24 X 58 X 42 X 98 X 10 X 98 goal: Layer 1 Layer 2 Layer 3 Layer 4 X 9 X 4 X 11 X 3 X 1 X 8 X 10 X 6 X 5 X 2 X 7 X 32 X 2 X 1 X 3 X 7 X 5 X 1 X 6 2.X 9 X 4 X 11 X 3 X 1 X 8 X 10 X 6 X 5 X 2 X 7 X 32 X 3 X 6 X 2 X 4 X 2 goal: p1 p0 X 9 X 8 goal: X 98 X 45 X 51 X 15 X 26 X 35 X 99 X 14 X 18 X 28 ⋯ X 10 X 18 X 28 X 18 X 28 X 77 X 24</p>
<p>Modeling stepwise inference without exemplars</p>
<p>Zero-shot CoT (Kojima et al., 2022) and scratchpads (Nye et al., 2021) represent two examples of stepwise inference protocols that do not rely on exemplars.For instance, in the zero-shot CoT approach, the input of the model is augmented with the phrase let's think step by step.This encourages the model to generate outputs that elaborate on the intermediate steps required to solve the target problem, thereby enhancing accuracy by breaking down the target problem into several simpler problems.</p>
<p>To compare the model's performance with stepwise inference and without stepwise inference (i.e., direct inference), we create two datasets: one including intermediate steps and the other without them.Each dataset is subsequently used to train distinct models.During the test phase, we present these trained models with pairs of nodes and task them to determine the existence of a path between the nodes.A model's performance is assessed based on its accuracy in classifying whether a path exists.</p>
<p>Fig. 2a shows how we generate the datasets above.First, we define a DAG denoted as G. Within this graph, for each dataset instance, we sample a start node X s and a goal node X g and then identify all feasible paths between these two nodes.From the identified paths, we select one to form a sequence of tokens, S.This procedure is iterated for other node pairs within the graph G to compile the complete dataset.For the dataset with stepwise inference, we use all the intermediate steps, including the start node X s and the goal node X g , to form S. For the dataset without stepwise inference (i.e., direct inference), we only use the start node X s and the goal node X g .We introduce a binary variable path ∈ {p 1 , p 0 } to denote whether there is a path between the start and goal nodes.We append the 'path' token p 1 to the end of the sequence S if there is at least one path between the start and goal nodes; otherwise, we append the 'no path' token p 0 .</p>
<p>Example: For the example path in Fig. 2a, in the dataset with stepwise inference, the sequence of tokens S includes the intermediate steps and takes the form goal :X 2 X 4 X 3 X 6 X 2 p 1 .For the dataset without stepwise inference (i.e., direct inference), the sequence S does not contain intermediate steps and has the form goal :X 2 X 4 p 1 .</p>
<p>Modeling stepwise inference with exemplars</p>
<p>Here we examine the influence of stepwise inference on model performance when in-context exemplars are present.This scenario is prominently exemplified by protocols based on few-shot CoT (Wei et al., 2022;Creswell et al., 2022).</p>
<p>Specifically, we extend the setup with a single DAG described in Section 2.2 by incorporating a set of DAGs, which we call motifs.The data generation process is shown in Fig. 2b.First, we generate a set of n Bernoulli DAGs denoted by g = {g i } n i=1 and randomly select a subset of K motifs from this set {g j1 , g j2 , . . ., g j K } ⊂ g.Then, we add edges between the sink node of each motif g j k and the source node of the subsequent motif g j k+1 , forming a chain of motifs g i1 → g i2 → • • • → g i K .These interconnecting edges are termed ghost edges.We sample paths from each pair of motifs linked by a ghost edge to establish the context.We select a start node from the sink nodes of one motif, X s ∈ g, and a goal node from the source nodes of a different motif, X g ∈ g ′ , then sample a path between them, denoted as e gg ′ .This procedure generates a sequence of nodes spanning across motifs, g → g ′ , including exactly one ghost edge.We refer to this as an exemplar sequence and use them as in-context samples.Exemplars to model few-shot CoT are represented as e gg ′ and denote a exemplar sequence from the motif g → g ′ .Finally, we select a start node X s ∈ g i1 and a goal node X g ∈ g i K .We then prompt the model to either directly output a path that connects the node pair X s and X g , or to provide exemplars demonstrating traversal between motifs within the specified context.Recall that our graph is constructed from a combination of K motifs.For the training dataset, we intentionally exclude 20% of the combinations.For the test dataset, we randomly select motifs from the remaining combinations in g, and sample sequences that illustrate how to navigate between two nodes within this graph.From training data, a model can learn the structure and interconnections of motifs; yet, during testing, it faces unseen combinations of these motifs.Correspondingly, the model must use the context to infer the overall structure of the graph.In essence, an exemplar tells the model which motifs are connected via ghost edges and hence can be navigated between.</p>
<p>Example: We directly study the path of navigation outputted by the model in this setup, i.e., no special tokens are used.A sample is constructed by selecting motifs to define in-context exemplars, say g i1 , g i2 , g i3 .For every successive pair of motifs, we construct an exemplar and put them together to create the context.To do this, we select two (start, goal) pairs: X s1 ∈ g i1 , X g1 ∈ g i2 and X s2 ∈ g i2 , X g2 ∈ g i3 .We sample exemplar sequences starting and ending at these node pairs: one sequence from g i1 to g i2 , goal :X g1 X s1 X 1 . . .X k1 X g1 , and another from g i2 to g i3 , goal :X g2 X s2 X ′ 1 . . .X ′ k2 X g2 .These sequences act as exemplars to be provided in context to the model when it is shown an input.The number of exemplars can vary from two to four, which correspond to chains of motifs of length three to five.The input itself is defined by choosing a goal node X g ∈ g i3 , a start node X s ∈ g i1 , and a path through an intermediate node
X inter ∈ g i2 ; e.g., goal :X g X s X ′′ 1 . . . X inter . . . X ′′ k1 X g3 . Here, X s X ′′ 1 .
. .X inter is a path between motifs g i1 and g i2 , while X inter . . .X ′′ k X g3 is a path between motifs g i2 and g i3 .When exemplars are not provided, the model must rely on its internalized knowledge to infer whether there exist two connected motifs that can be used to move from the start to goal node.The context exemplars simplify the problem by telling the model the motifs above are connected.</p>
<p>Results: Stepwise Navigation</p>
<p>In this section, we discuss findings on how stepwise inference affects the model's ability to solve problems.We investigate two scenarios: in the absence of in-context exemplars (Section 3.1) and in the presence of them (Section 3.2).For all experiments, unless stated otherwise, we use a 2-layer Transformer defined by Karpathy (2021) to mimic the GPT architecture (Brown et al., 2020).For more details on the experimental setup, please refer to Appendix C.3 for model architecture details and Appendix D for training data generation and train/test split.</p>
<p>Navigation without exemplars</p>
<p>We assess the performance of the model by evaluating its ability to classify whether there is a path given a pair of nodes during the test phase.Specifically, we randomly sample pairs of start and goal nodes that were not seen in the training data and observe whether the model outputs either the 'path' token p 1 or the 'no path' token p 0 .</p>
<p>STEPWISE INFERENCE GAP</p>
<p>Fig. 3 shows the accuracy of classifying 'path' or 'no path' for two different types of graphs: a Bernoulli graph and a hierarchical graph.We observe that for both types of graphs, the use of stepwise inference significantly improves the model's performance compared to direct inference, with more pronounced improvements noted for the hierarchical graph.Following Prystawski &amp; Goodman (2023), we refer to the improvement in performance observed between stepwise inference and direct inference as the "stepwise inference gap".We even simulate the effect of noisy realworld labels by introducing random corruptions into the tokens and found that the results above continue to hold, as detailed in Appendix Fig. 13.</p>
<p>To further probe the results above, we control for path lengths in the hierarchical graph.Specifically, to set the maximum path length in the training data to ∆, we choose a starting layer l and a goal layer l ′ such that l ′ − l &lt; ∆.Then, we sample starting nodes from layer l and goal nodes from layer l ′ .For the test data, we select node pairs with l ′ − l ≥ ∆. Results are shown in Fig. 3(c).We plot the classification accuracy across various values of ∆ and observe that the smaller the value of ∆, the greater the stepwise inference gap becomes.We hypothesize this happens because when the training data only includes short paths, the  model needs to more effectively 'stitch' the paths observed during training, which, as a recursive task, is more feasible via stepwise inference.</p>
<p>DIVERSITY-ACCURACY TRADEOFF WITH HIGHER SAMPLING TEMPERATURES</p>
<p>Here, we investigate how the sampling temperature of the autoregressive Transformer affects the diversity of the generations produced by the model and its accuracy.To this end, we fixed the start and goal nodes and prompted the model 3,000 times, varying the sampling temperatures from 0.0 to 3.0.We define accuracy as the probability that a generated path consists of valid edges and correctly terminates at the designated goal node.Diversity is defined as the number of unique paths generated.As shown in Fig. 4, there is a clear trade-off between the diversity of the paths generated by the model and their accuracy.We term this phenomenon the diversity-accuracy tradeoff : at lower sampling temperatures, the model generates fewer but more accurate and valid paths; in constrast, higher sampling temperatures result in greater path diversity but reduced accuracy.Our result provides the first explicit demonstration of a trade-off between the accuracy and diversity of Transformer outputs.To the best of our knowledge, this phenomena has not been quantitatively studied before.</p>
<p>PREFERENCE FOR SHORTER PATHS</p>
<p>Note that there are multiple possible paths the model can choose from in the pursuit of inferring a path that connects a start and goal node.We showed that by increasing the sampling temperature, a diverse set of paths can be gen-average path length between nodes model generated path legnth erated; however, by default, which path does the model prefer?To evaluate this, we compare the actual path lengths between nodes in the test data with those generated by our trained model in the Bernoulli graph setup.In Fig. 5a, we observe that the model consistently produces paths that are shorter, on average, than the paths in the ground truth DAG.This observation suggests that the model exhibits a simplicity bias, tending to find the quickest path to solve the target problem.However, simplicity biases have been shown to yield oversimplification of a problem, forcing a model to learn spurious features (Shah et al., 2020;Lubana et al., 2023).In the context of stepwise inference, this can amount to omission of important intermediate steps, similar to 'shortcut solutions' arising from pattern-matching behaviors discussed in prior work on Transformers (Liu et al., 2022;Dziri et al., 2023).</p>
<p>EVOLUTION OF FAILURES IN STEPWISE INFERENCE OVER TRAINING</p>
<p>In the above discussion, we evaluated how stepwise inference assists a model in successfully completing a complex, multi-step task.We now assess how it fails.Specifically, assume that for a given graph G, the model produces a sequence of nodes X s X 1 . . .X k . . .X t starting at the start node X s .Following (Saparov &amp; He, 2023;Momennejad et al., 2023), we define two categories of potential failures.</p>
<p>• Misstep (X k , X k+1 ) / ∈ G: An edge produced by the model does not exist in the DAG, commonly referred to as "hallucinations".</p>
<p>• Planning failure X t ̸ = X g : The model produces a path that does not terminate at the goal node.In Fig. 6, we examine the learning dynamics for each failure mode.The figure indicates that the model initially acquires the skill to circumvent missteps (the blue line).Subsequently, it develops the ability to plan effectively, which is shown by a decrease in planning failures (the red line).By integrating these abilities-avoiding missteps and minimizing planning failures-the model is finally able to generate accurate paths for node pairs not seen during training.</p>
<p>MECHANISTIC BASIS OF THE LEARNED GRAPH NAVIGATION ALGORITHM</p>
<p>Our results above elicit several intriguing behaviors attributable to stepwise inference.We next take a more mechanistic lens to explain why these behaviors possibly occur.</p>
<p>We hypothesize that the model learns embeddings for the nodes of the graph that enable easy computation of an approximate distance measure.This suggests that to move closer to the goal node, one can simply transition to the node that exhibits the least distance from the goal node.For the detailed intuition guiding our analysis, see Appendix F.</p>
<p>To verify this, we first strip the model down to a singlehead, self-attention layer.We visualize the attention scores for this minimal model in Fig. 7a, observing they are are concentrated on the goal node and the current node.This suggests that the model utilizes only the embedding values of the goal X g and the current nodes X current to select the next token.Inspired by this observation, we develop a simplified algorithm that mimics the behavior of the model, as outlined in detail in Fig. 7b.First, we extract the value embeddings for X g and X current using the weight matrix</p>
<p>Shorter path bias</p>
<p>Preference for next step:  (Navarro, 2001) between paths generated by the full trained model and the simplified algorithm for the same (Xs, Xg) pairs.(e) The short path bias can be attributed to the inner products between the token embedding of the next chosen token Xnext and the value embedding of Xg and vg.We observe that nodes Xnext further away from Xg have a lower inner product, indicating that the model's embedding of nodes reflects the underlying graph topology.The red line denotes the best least squares fit and has a slope of −0.106.W V from the self-attention layer, yielding v g and v current , respectively.We then merge these embeddings into a single vector v, i.e., v = v g + v current .Finally, we determine the next token by identifying the node whose token embedding has the highest inner product with v.This operation mimics the logit computation in a full Transformer.
X current W E W E wcurrent = WE X current wg = WE X g v current v g v g + v current X next = arg max
In Fig. 7c, we demonstrate the simplified algorithm retrieved via the process above matches the accuracy of the full trained model.Furthermore, in Fig. 7d, we find that the paths generated by our simplified algorithm and those produced by the full trained model are nearly identical.Herein, we use a string edit distance metric (Navarro, 2001) to quantify the similarity between the two sets of paths and find that over 75% of paths are identical.</p>
<p>Given that accuracy is computed over test nodes not seen in the training data, it is likely that the model encodes a notion of distance between two nodes on the graph in its embedding, as we hypothesized.Indeed, in Fig. 7e, we find that the inner product of the embedding of v g with the token embeddings of X next is negatively correlated with the distance between these two nodes in the ground truth DAG; here, we used the average path length as a distance measure over the graph.Since potential nodes with shorter paths to the goal node have a higher logit value, this implies they will be more likely to be predicted, thus showing the origin of the short path bias we observed in Sec.3.1.3.This is a mechanistic explanation of the pattern-matching behavior of Dziri et al. (2023) in the context of our task.</p>
<p>Navigation with exemplars</p>
<p>The single graph setting let us explore zero-shot navigation and stepwise reasoning, where the model relied on knowledge internalized over pretraining for stepwise navigation towards a goal.Next, we study how context can influence the model generated paths, how subgoals that are provided in-context can guide the model's navigation, and how the content of the exemplars affects the navigation path chosen by the model.Our results shed some light on and create hypotheses for (1) compositional generalization, (2) length generalization, and (3) impact of conflicting, long context.</p>
<p>COMPOSITIONAL GENERALIZATION</p>
<p>We find that the model can successfully follow the chain defined by the in-context exemplars.An example output produced by the model is in Fig. 2 g 2 → g 9 .We also find that the model generalizes to arbitrary orders of motifs strung out, including those that did not occur consecutively in the training data, up to the length in the training data (see Fig. 8).In other words, in-context control is capable of eliciting compositional generalization (Li et al., 2023), if appropriately trained.Further, we see that the attentional patterns used by the model suggest that while navigating across motifs, the model treats nodes across ghost edges as subgoals (see Appendix Fig. 17).</p>
<p>NUMBER OF INTERMEDIATE MOTIFS</p>
<p>In Fig. 8, we vary the number of exemplars provided to the model.This is equivalent to stringing together a longer chain of exemplar sequences across motifs to navigate over.We define successful steering via a product of indicator variables that measure (i) whether the path ended at the specified goal and (ii) that each ghost edge, and thus the intermediate motif, was present in the path.We computed the probability by averaging over distinct source nodes from g i1 and sink nodes from g i K .We find that the model can generalize well to unseen orders of motifs up to the maximum number chained together in the training data, after which the model fails to navigate.We hypothesize that even when using stepwise inference methods at scale, the model will fail to generalize to reasoning chains longer than those present in its training data.</p>
<p>PRIMACY BIAS TOWARDS THE FIRST EXEMPLAR IN THE CASE OF CONFLICT</p>
<p>Language models are generally prompted with several exemplars in context.Some of these exemplars may have incorrect or even conflicting information with respect to other exemplars, for example in a multiple choice Q&amp;A task (Hendrycks et al., 2020;Pal et al., 2022;Srivastava et al., 2022).The model has to choose the relevant informa- To construct the context, we selected an initial motif gi 1 , a terminal motif gi T and two intermediate motifs ginter and g ′ inter .We string them together so that the motif has two possible paths: gi 1 → ginter → gi T and gi 1 → g ′ inter → gi T .In this case of two conflicting chains in-context, the model has a bias to pick the chain that appears first in context.tion between these exemplars to solve the specified task.Motivated by this, we quantitatively study the behavior of the model when a noisy context with exemplars with conflicting information are provided.Specifically, we study a case where two chains of motifs are used to design exemplars for our task, such that the exemplars start from the same set of initial and terminal motifs g i1 and g i T , but with distinct intermediate motifs g inter and g ′ inter .The model is then prompted with X s ∈ g i1 and X g ∈ g i T , after in-context exemplars in order: e gi 1 ,ginter , e ginter,gi T , e gi 2 ,g ′ inter , e g ′ inter ,gi T .Results are shown in Fig. 9.We find that the model does indeed navigate to the goal, thus following the prompt, but has a strong bias toward choosing a path defined by the first chain over the second, i.e., g i1 → g inter → g i T .This result is similar to what happens at scale with large context windows, where content in the middle of a long context window is ignored (Liu et al., 2023).</p>
<p>Conclusion</p>
<p>In this work, we introduced a synthetic graph navigation task to investigate the behavior, training dynamics, and mechanisms of Transformers under stepwise inference protocols.Despite its simplicity, our synthetic setup has provided key insights into the role of the structural properties of the data, a diversity-accuracy tradeoff in sampling, and a simplicity bias of stepwise inference protocols.In addition, we explored the model's navigation preferences and their controllability through in-context exemplars, modeled length generalization, and responses to longer contexts with conflicting exemplars.Like all papers that rely on synthetic abstractions, our goal was to develop such hypotheses to explain an interesting phenomena seen in practical scenarios.A promising future direction for our work thus is to test the hypotheses we have formulated in large language models, as well as generalize and test the mechanistic interpretation of the learned Transformer algorithm in practical scenarios.</p>
<p>Impact Statement</p>
<p>This paper provides a comprehensive scientific analysis of a Transformer model that solves a small-scale synthetic task.We believe that the scientific findings presented in this paper will lay the groundwork for the development of more reliable and interpretable AI systems for the benefit of society.</p>
<p>A. Detailed Related Work</p>
<p>Stepwise inference protocols Large language models (LLMs) have been shown to possess sophisticated and human-like reasoning and problem-solving abilities (Srivastava et al., 2022).Chain-of-thought or scratchpad reasoning refers to many similar and related phenomena involving multiple intermediate steps of reasoning generated internally and autoregressively by the language model.First described by Nye et al. (2021); Kojima et al. (2022), adding prompts such as 'think step by step' allows the LLM to autonomously generate intermediate steps of reasoning and computation, improving accuracy and quality of its responses.This is referred to as zero-shot chain-of-thought.A related set of phenomena, few-shot chain-of-thought prompting (Wei et al., 2022) occurs when the language model is shown exemplars of reasoning before being prompted with a reasoning task.The model follows the structure of logic in these exemplars, solving the task with higher accuracy.Further, there have been several prompting strategies developed, all of which rely on sampling intermediate steps, such as tree-of-thoughts (Yao et al., 2023), graph-of-thoughts (Besta et al., 2023), program-of-thoughts (Chen et al., 2022), self-ask (Press et al., 2022).There are also methods which use more than one LLM, such as STaR (Zelikman et al., 2022), RAP (Hao et al., 2023), Selection-Inference (SI) (Creswell et al., 2022;Creswell &amp; Shanahan, 2022).</p>
<p>Understanding stepwise inference Dziri et al. (2023) study how LLMs solve multi-step reasoning tasks and argue that models likely fail because they reduce most multi-step reasoning tasks to linearized sub-graph matching, essentially learning 'shortcut solutions' (Liu et al., 2022).Momennejad et al. (2023) study in-context graph navigation in LLMs, finding that they fail to do precise planning.Saparov &amp; He (2023) introduce a synthetic dataset called PrOntoQA to systematically study the failure modes of chain of thought in the GPT3 family fine-tuned on the dataset and find that misleading steps of reasoning are a common cause of failure in the best-performing models.Chen et al. (2023) find that chain-of-thought fails at compositional generalization and counterfactual reasoning.Wang et al. (2022a); Schaeffer et al. (2023) find that the content of the exemplars is less relevant to accuracy than their syntactic structure.Razeghi et al. (2022) find that the accuracy of reasoning is correlated with the frequencies of occurrence in the pretraining dataset.Recently, a few works have used theoretical approaches to characterize and explain stepwise inference.Li et al. (2023) study in-context learning of random MLPs and find that a Transformer that outputs the values of intermediate hidden layers achieves better generalization.Feng et al. (2023) show that with stepwise reasoning, Transformers can solve dynamic programming problems, and Prystawski &amp; Goodman (2023) study reasoning traces in Transformers trained to learn the conditionals of a Bayes network.There are also several puzzling phenomena in the prompts used to elicit few-shot chain-of-thought reasoning: chain-of-thought can be improved by sampling methods such as self-consistency (Wang et al., 2022b); prompts might not reflect the true reasoning process used by the language model, as identified by Turpin et al. (2023); and the accuracy of the model can be sensitive to the order in which prompts are provided (Lu et al., 2021).</p>
<p>B. Why graph navigation?</p>
<p>In this section, we describe examples of various computational tasks that have been cast as graph navigation in literature to study Transformers and LLMs.</p>
<p>• First order logic: Saparov &amp; He (2023) study simple DAGs as models of first order logical reasoning.They construct ontologies (see Fig. 10a) and prompt LLMs to do analogical reasoning.</p>
<p>• Mathematical expression evaluation: Dziri et al. ( 2023) study mathematical expression evaluation in large scale LLMs as DAG navigation (see Fig. 10b).Any mathematical expression can be decomposed into elementary computations which are chained together.</p>
<p>• Planning and spatial navigation: Momennejad et al. (2023) evaluates many large scale LLMs such as ChatGPT-4 and Claude2 on synthetically designed planning and navigation tasks (see Fig. 10c).</p>
<p>• Formal grammars and natural language: Allen-Zhu &amp; Li (2023) studies Transformers trained on context-free grammars (CFGs) which are DAGs.Another motivation for the study of graph navigation comes from linguistics and natural language syntax (Chomsky, 2002).Every sentence in a language can broken down into its syntactic or parse tree, a special case of a directed acyclic graph.For example, the sentence 'I drive a car to my college' can be parsed as the following graph: ('I': Noun phrase, 'drive a car to my college': Verb Phrase) → ('drive': Verb, 'a car': Noun Phrase, 'to my college': Prepositional Phrase) → ('a': Determiner, 'car': Noun), ('to': Preposition, 'my college': Noun Phrase) → ('my': Determiner, 'college': Noun).Effective stepwise reasoning consists of several elementary logical steps put together in a goal-directed path that terminates at a precise state (LaValle, 2006).We argue that graph navigation problems provide such a fundamental framework for studying stepwise inference.Graphs provide a universal language for modeling and solving complex problems across various domains.Whether it is optimizing network traffic, analyzing social networks, sequencing genetic data, or solving puzzles like the Travelling Salesman Problem, the underlying structure can often be mapped onto a graph (Cormen et al., 2022;Momennejad et al., 2023;Dziri et al., 2023;Saparov &amp; He, 2023).</p>
<p>C. Setup and construction of graph and model</p>
<p>C.1. Graph structures</p>
<p>Here we describe the properties of the DAGs we use, the training setup, model architecture, and hyperparameters.</p>
<p>We use two DAG structures, as shown in Fig. 11.Specifically, Bernoulli DAGs are constructed by randomly generating an upper-triangular matrix where each entry has probability p of existing.Hierarchical DAGs are generated by predefining L sets of nodes and drawing an edge between a node n l in layer l and n l+1 in layer l + 1 with probability p; we constrain the graph to be connected.These generation processes lead to different path diversity and path length distributions, which affect the efficacy of stepwise inference, as shown in our results.Below, we provide algorithms to generate our graph structures.</p>
<p>Hierarchical graph</p>
<p>x 9</p>
<p>x 11 x 4</p>
<p>x 1 x 3 x 8</p>
<p>x 6 x 10 x 5</p>
<p>x 7</p>
<p>x 2 x 12</p>
<p>Layer 1 Layer 2</p>
<p>Layer 3</p>
<p>Layer 4</p>
<p>Directed Acyclic Graph (DAG) Structures</p>
<p>C.3. Architecture details and loss function</p>
<p>LOSS FUNCTION</p>
<p>For training, we tokenize every node and we use the standard language modeling objective, next-token prediction with a cross entropy loss.Here target n is the 1-shifted version of the training sequence and x n are the logit outputs of the model at the n th timestep.For model architecture, we use a GPT based decode-only Transformer with a causal self-attention mask.Our implementation is based on the nanoGPT repository1 .
L(x n , target n) = − log exp(βx n, target n ) #tokens v=0 exp(βx n,v ) = − log softmax(βx n ) target n prob(target n)(1
The Transformer architecture consists of repeated blocks of pre-LayerNorm, causal multi-head self-attention, post-LayerNorm, and an MLP with skip connections (see Fig. 12).The MLP contains two fully-connected layers with a GELU non-linearity (Hendrycks &amp; Gimpel, 2016).The dimensionality of the hidden layer of the MLP is 4x the embedding dimensionality.We do not include any dropout in the model or biases in the linear layers.We use weight-tying (Press &amp; Wolf, 2016) in the embedding and un-embedding layers.</p>
<p>To do the mechanistic study, we consider a 1 layer attention-only Transformer without a few modifications: We remove the MLP and post-LayerNorm and vary the embedding dimensionality from four to 64.This 1L Transformer is described by the following model equations.Here X token ∈ R vocab size×T denotes the tokens, W E ∈ R nembd×vocab size is the positional embedding matrix, W pos ∈ R nembd×T is the token embedding matrix, and X ∈ R nembd×T .
X = W E (X token ) + W pos (X token ) X = LN(X) X = X + softmax(X T W T Q W K X)W V X z = softmax(W T E X) next token = argmax all tokens z</p>
<p>D. Training protocol for experiments</p>
<p>For experiments in our setup without exemplars, we randomly generate either a hierarchical graph or a Bernoulli graph G with N = 200 nodes.In the Bernoulli graph setting the probability of an edge p = 0.05; similarly, in the hierarchical graph, the probability of an edge between a node in layer l and layer l + 1 is p = 0.05.We choose 10 layers with 20 nodes each to match the number of nodes in the two graph types.We convert all the nodes to tokens, along with a special goal token which corresponds to a [BOS] token and an end token which corresponds to an [EOS] token.We use another token, pad, for padding as well.</p>
<p>Train-Test split To generate training data corresponding to path connected node pairs, we first put all edges (which are paths of length one) into the training data.This procedure was done in all experiments to ensure that full knowledge of the graph was presented to the model.Further, we generate all simple paths between every pair of nodes in the graph.A variable fraction of these paths are included in the training data, depending on experimental conditions which we outline below.</p>
<p>For experiments in Figs.3a-b, we pick 20% of the path-connected nodes and put all simple paths between them into the training data for each graph type.We also add an equal number of non-path connected nodes to the training data.</p>
<p>In Fig. 3c, for each value of the path-length threshold parameter, which sets the maximum length of paths in the training dataset, we pick paths corresponding to 20% of the allowed path-connected node pairs and put them into the training data, while the remainder are held out evaluations.For the non-path connected pairs, we simply take all node pairs that are not path-connected and add a fraction of these node pairs into the training data, chosen to roughly balance the number of path-connected node pairs according to the experimental conditions.The rest are held-out for evaluation.</p>
<p>For the motif experiments in Fig. 8, we generate a set of 10 motifs, each with a Bernoulli graph structure of 100 nodes with a bernoulli parameter p = 0.95.We then divide the 45 possible motif orders into a set of 35 and 10 that we put into train and test respectively.For generating the context, we combine 3-6 motifs according to the allowed orders, and then sample exemplars as well as the final sequence that traverses the full motif chain by choosing start and goal nodes from the set of sources and sinks respectively.</p>
<p>E. Additional experimental results</p>
<p>Label noise in training data In Fig. 13, we mimic real-world language data, abundant in ambiguity and polysemy, by corrupting (a) 5%, (b) 10% and (c) 20% of tokens in a single graph scenario.To achieve this, we replaced a randomly chosen 5% and 10% of the tokens in the training data with random tokens.We observe that the gap between stepwise inference and direct inference persists in both scenarios.This finding indicates that stepwise inference remains effective in more realistic settings with noise.</p>
<p>Varying edge density In Fig. 14, we swept the density of the graph from 0.08 to 0.12 on a hierarchical graph.We observe a stepwise inference gap in all cases.The stepwise inference gap becomes smaller for larger densities.This is because  the more likely the nodes are to be connected, the more likely it is for shortest paths to exist between nodes and thus less "stitching" is needed (Broadbent &amp; Hammersley, 1957).</p>
<p>Short path bias Fig. 15 presents a density plot comparing the average lengths of actual paths with those generated by the model in a Bernoulli graph.This observation verifies the model tends to produce shorter paths between a given pair of start and goal nodes.</p>
<p>Effect of varying embedding dimensionality in the single graph scenario Here we consider the 1-layer Transformer without MLP and post-LayerNorm and ask the following question: for a fixed underlying graph size and training data, how does the model performance vary as we sweep embedding dimensionality.Intuitively, if the embedding dimensionality is large, the model should be able to generalize better by learning a better embedding of the node tokens.We see that beyond a critical dimensionality (which is around 20 for a graph of 200 nodes), the model generalizes to all held out (start, goal) node pairs with a fairly abrupt transition (see Fig. 16).</p>
<p>F. Intuition guiding the mechanistic analysis</p>
<p>In this section, we present the intuition that served as the hypothesis guiding our mechanistic analysis.</p>
<p>Consider the optimal maximum likelihood estimator designed to solve our graph navigation task.Given a start node X s and an incomplete sequence of predicted nodes X 1 , . . ., X k in the pursuit of navigating to the goal node X g , the estimator works the following way: X next = arg max X ′ P (X ′ |X s ; X 1 , . . ., X k ; X g ) Since the task is conditionally Markovian: the choice of the next step will be independent of the history when conditioned on X g and X k .Accordingly, we have:
X next = argmax X ′ P (X ′ |X k , X g ) = arg max X ′ P (X g |X ′ , X k )P (X ′ , X k ) P (X g , X k )
This decomposition leads to interpretable terms which shed light on what algorithm the model might use: log P (X ′ |X k , X g ) = log P (X g |X ′ , X k ) + log P (X ′ , X k ) − log P (X g , X k )</p>
<p>These terms can be interpreted as follows:</p>
<ol>
<li>
<p>log P (X g , X k ) describes the prompt.</p>
</li>
<li>
<p>log P (X ′ , X k ) describes the knowledge of the world model: How well does the model know the ground truth structure of the graph?</p>
</li>
<li>
<p>log P (X g |X ′ , X k ) corresponds to goal-directed behavior: What X ′ is most likely to lead to the goal?</p>
</li>
</ol>
<p>Let C(X k ) denote the subset of nodes in the graph that are children of the node X k .Then, while selecting the next token that has the highest likelihood, note that terms (1) and (2) cannot be optimized over: the former does not depend on X ′ and the latter, for the optimal predictor, will be 1 /|C(X k )| if X ′ ∈ C(X k ) and 0 otherwise.Accordingly, the only term that can be optimized over is the third one, i.e., the one that measures how likely the goal is if the next state is X ′ .However, due to term (2), X ′ ∈ C(X k )-that is, the possible set of next tokens is constrained to the set C(X k ).Now, exploiting the task's conditional Markovian nature again, we have P (X g |X ′ , X k ) = P (X g |X ′ : X ′ ∈ C(X k )).</p>
<p>Heuristically, assume that P (X g |X ′ ) ∝ e −d(Xg,X ′ ) • I(X ′ ∈ C(X k )), where d(X i , X j ) is a measure that describes the distance between nodes X i and X j , while respecting the topology of the graph, and I is an indicator function that is 1 if its input is True, and 0 if not.Then, we have log P (X g |X ′ , X k ) ∝ −d(X g , X ′ ) • I(X ′ ∈ C(X k )).</p>
<p>The intuitive argument above, though likely to be approximate, suggests that a possible solution the model can learn via autoregressive training in our graph navigation setup is (i) compute the distance between all neighboring nodes of the current node and the goal node, (ii) move to the node that has the least distance, and (iii) repeat.The algorithm we uncover in our analysis in Sec.3.1.4in fact functions in a similar way: the model is constantly computing a inner product between the goal token's representation and the embeddings of all tokens; we find this inner product is highest for the neighbors of the current token.Then, the highest inner product token is outputted and the process is repeated.Since the embeddings are not normalized, this inner product is not exactly the Euclidean distance-we expected as much, since the topology of the graph will have to be accounted for and learning an inner product based metric will be easier for a model (because most operations therein are inner products).</p>
<p>Further, in the case of motifs, we expect that the model contructs a path through checkpoints defined by ghost edges, which act as subgoals.To explain, given a set of K motifs strung together in-context {g i1 , g i2 , g i3 , . . .g i K }, we have the set of K-1 ghost edges, one for each exemplar: {(X sink (g i1 ), X source (g i2 )), (X sink (g i2 ), X source (g i3 )), . . .(X sink (g i K−1 ), X source (g i K ))}.Thus, we hypothesize that the model identifies all K-1 ghost edges from its context and plans sub-paths to each ghost edge in pursuit of the goal.Preliminary analyses of attention patterns in Fig. 17 provides evidence for this hypothesis.</p>
<p>F.1. Generalizing static word embeddings to 3-way relations</p>
<p>Static word embedding algorithms such as Word2vec are trained by sampling pairs of words (w i , w c ) that appear in the same context and adjusting their embeddings so that their inner product is higher than an inner product with the embedding of word w i and a randomly sampled word from the vocabulary.The algorithm can be understood as performing a low-rank factorization of the matrix of co-occurrence statistics.In the case of Word2vec, the matrix is factorized as P = I • C where I contains word vectors in its rows and C contains word covectors in its columns.Therefore, every word has two types of embeddings.One is used when the word appears in the first position in the pair (which corresponds to center words), and the second when it appears in the second position (which corresponds to context words).</p>
<p>"Figure 1 .
1
Figure 1.Examples of stepwise inference protocols and how they can be cast as a graph navigation problem.(a) Zero-shot chain-of-thought (Kojima et al., 2022) involves asking a model to produce intermediate outputs to perform complex multi-step computations, such as solving the Tower of Hanoi problem.Casting the configurations of the rods in Tower of Hanoi as nodes of a graph, we can see that the problem is essentially traversal over states describing different configurations of the setup to reach the desired configuration (the goal state).(b) Scratchpad (Nye et al., 2021) improves LLMs' ability to perform complex multi-step computations, such as arithmetic, when they write intermediate computation steps to a buffer called a scratchpad.</p>
<p>Figure 2 .
2
Figure 2. Data generating process.(a) In absence of exemplars.This figure illustrates the step-by-step process of generating a training dataset using a single underlying graph.1) A directed acyclic graph (DAG) is generated, which can be either hierarchically structured or Bernoulli.2) A start node and a goal node are selected.3) All possible paths connecting the start and goal nodes are sampled, and one path is randomly selected.4) The chosen path is then represented in a task-specific format.(b) In presence of exemplars.The process of generating a training dataset by combining multiple subgraphs (motifs) involves the following.(1.) Start by building a set of Bernoulli directed acyclic graphs (DAGs).(2.) Pick a subset of K of these DAGs {gi 1 , gi 2 , ..gi K } and connect them together using "ghost edges" to create a chain of motifs gi 1 → gi 2 → • • • → gi K .(3.) Sample exemplars from every pair of motifs that have been connected by a ghost edge to construct the context.(4.)Now choose a start node Xs ∈ gi 1 and a goal node Xg ∈ gi K and construct a sequence passing through the whole chain of motifs.</p>
<p>Figure 3 .
3
Figure 3. Advantage of stepwise inference in graph navigation tasks and stitching: (a) In the Bernoulli DAG, stepwise inference demonstrates an advantage over direct inference in predicting whether given node pairs are connected.(b) This advantage is further pronounced in hierarchical DAGs, where the distances between nodes are greater than in Bernoulli DAGs.(c) The stepwise inference gap arises when the training set contains paths that are shorter than the paths required to connect nodes at test time.(d) The stepwise inference is beneficial when the model must connect paths seen during training: the red, green, and blue paths represent subsets of paths seen during training; we find the model produces paths that combine these subsets during the test phase.</p>
<p>Figure 4 .
4
Figure 4. Diversity vs. accuracy trade-off for different sampling temperatures of the Transformer model: As the sampling temperature increases, the diversity of paths generated by the model also increases, while the accuracy decreases.This tradeoff is captured by measuring the number of unique valid paths (top panel), indicating that there is an optimal temperature for sampling.The dashed line represents the ground truth path diversity.</p>
<p>Figure 5 .
5
Figure5.Model outputs are biased toward shorter paths.We compared the average lengths of ground-truth paths for a specific set of node pairs and the paths produced by the model for these same pairs in the Bernoulli DAG.We observe that the model tends to generate shorter paths than the actual ones.This observation points to a "simplicity bias" in the trained model towards favoring shorter over potentially more accurate or realistic paths.</p>
<p>Figure 6 .
6
Figure6.Learning dynamics for two failure modes: misstep and planning failure.We measure the probability of missteps and planning failures in the model's outputs.A misstep refers to an instance where the model generates an edge that is not present in the graph, while a planning failure means that the model outputs a path that fails to reach the intended goal node.Initially, the model learns to avoid missteps.Subsequently, around the 200 th optimization step, it begins to effectively learn planning.The accuracy curves are averaged over three models, each trained with a distinct random seed.</p>
<p>value embeddings of nodes 3. Decode next token by maximizing dot product with token embeddings.</p>
<p>Figure 7 .
7
Figure 7. Mechanistic analysis of the graph navigation algorithm: Emergent linear representation.(a) Attention maps from the 1-layer, attention-only Transformer, highlighting the model's attention on the goal token Xg and the current token Xcurrent.(b) Steps of our simplified algorithm that emulates the 1-layer, attention-only Transformer are as follows.(1.)We extract value embeddings for Xg and Xcurrent, ignoring other tokens for simplicity.(2.) Next, we compute the value embeddings of the goal vg and current vcurrent nodes and add them together v = vg + vcurrent.(3.)We then compute the token embedding with the highest inner product with v, approximating the token that receives the highest logit score after the single forward pass.(c) Comparison of the model's accuracy on a set of 500 held-out node pairs (Xs, Xg) using our simplified algorithm (99.8%) versus the full trained model (99.6%).(d) The paths generated by the simplified algorithm almost exactly match the paths generated by the full trained model.Path similarity on 2000 held-out node pairs was compared by measuring the Levenshtein edit distance(Navarro, 2001) between paths generated by the full trained model and the simplified algorithm for the same (Xs, Xg) pairs.(e) The short path bias can be attributed to the inner products between the token embedding of the next chosen token Xnext and the value embedding of Xg and vg.We observe that nodes Xnext further away from Xg have a lower inner product, indicating that the model's embedding of nodes reflects the underlying graph topology.The red line denotes the best least squares fit and has a slope of −0.106.</p>
<p>Figure 8 .
8
Figure 8. In-context steerability and length generalization.We vary the number of intermediate motifs ginter in a chain of motifs constructed for the particular context gi 1 → {ginter →}n → gi K .The path generated by the model follows the path described by the chain in context until n = 4, which is the maximum chain length in the training data.</p>
<p>FirstFigure 9 .
9
Figure 9. How does the model handle conflicting exemplars?To construct the context, we selected an initial motif gi 1 , a terminal motif gi T and two intermediate motifs ginter and g ′ inter .We string them together so that the motif has two possible paths: gi 1 → ginter → gi T and gi 1 → g ′ inter → gi T .In this case of two conflicting chains in-context, the model has a bias to pick the chain that appears first in context.</p>
<p>Figure 10 .
10
Figure 10.Examples of stepwise inference as graph navigation in LLM evaluations: [Figures taken from respective papers] (a) An example graph created for a prompt (left) from the ProntoQ&amp;A dataset (Saparov &amp; He, 2023) (b) (Dziri et al., 2023) studies how simple algorithms such as multiplication of digits can be represented as a graph (c) CogEval (Momennejad et al., 2023) studies many large scale LLMs such as ChatGPT-4 and Claude2 on planning and navigation tasks.(d) Mathematical expression evaluation in the case of additionof two numbers can be visualized as a series of steps of a digit-wise addition algorithm.</p>
<p>Figure 11 .
11
Figure 11.Construction and properties of Hierarchical and Bernoulli DAGs: (top) Schematic of hierarchical and Bernoulli graphs.Hierarchical graphs are organized into layers with connections only between nodes of successive layers but Bernoulli graphs have no such structure.(middle) Path diversity is defined as the number of paths between any two path connected nodes.(bottom) Path length distributions: Owing to the hierarchical nature, the path length distribution is exponential in hierarchical graphs whereas it is more Gaussian-like for Bernoulli graphs.</p>
<p>Figure 12 .
12
Figure 12.The architecture of GPT-style (Radford et al., 2019) decode-only Transformers.Note the presence of both pre and post-LayerNorm in each Transformer block.Figure from methods section of Ramesh et al. (2023).</p>
<p>Figure 13 .
13
Figure 13.Persistence of stepwise inference gap with corrupted tokens: In this experiment with setup identical to Fig. 3a-b, (a) 5%, (b) 10% and (c) 20% of tokens were randomly corrupted to mimic real world language data.The stepwise-inference gap persists.</p>
<p>Figure 14 .
14
Figure14.The effect of varying edge density in the single graph scenario: Here we vary p, the edge density of connectivity in the graph, from 0.08 in the left-most plot to 0.12 in the right-most plot, in steps of 0.01.The stepwise inference gap persists in all cases.</p>
<p>Figure 15 .
15
Figure 15.Model outputs are biased toward shorter paths.</p>
<p>Figure 16 .
16
Figure16.Varying embedding dimensionality in 1 layer models: We find that there is a critical embedding dimensionality (around 20 for a Bernoulli graph of size 200 nodes and p = 0.05) above which the model can generalize to all held-out node pairs.</p>
<p>Figure 17 .
17
Figure 17.Attention pattern after in-context exemplars: Here we visualize the portion of the attention map after prompting with four in-context exemplar sequences.The model generates attentional patterns that treat the node across ghost edges as a subgoal.</p>
<p>DAGs).(2.) Pick a subset of K of these DAGs {gi 1 , gi 2 , ..gi K } and connect them together using "ghost edges" to create a chain of motifs gi 1 → gi 2 → • • • → gi K .(3.) Sample exemplars from every pair of motifs that have been connected by a ghost edge to construct the context.(4.)Now choose a start node Xs ∈ gi 1 and a goal node Xg ∈ gi K and construct a sequence passing through the whole chain of motifs.</p>
<p>This figure illustrates the step-by-step process of generating a training dataset using a single underlying graph.1) A directed acyclic graph (DAG) is generated, which can be either hierarchically structured or Bernoulli.2) A start node and a goal node are selected.3) All possible paths connecting the start and goal nodes are sampled, and one path is randomly selected.4) The chosen path is then represented in a task-specific format.(b) In presence of exemplars.The process of generating a training dataset by combining multiple subgraphs (motifs) involves the following.(1.) Start by building a set of Bernoulli directed acyclic graphs (</p>
<p>Table 1 .
1
) Hyperparameters of the Transformer models used for all experiments except mechanistic analyses
HyperparameterValuelearning rate10 −4Batch size64Context length32OptimizerAdamMomentum0.9, 0.95Activation functionGeLUNumber of blocks2Embedding dimension 64
available at https://github.com/karpathy/nanoGPT
C.2. Motif constructionIn the multi-graph scenario, we first construct a set of n graphs (in our experiments, we use Bernoulli DAGs with n = 10) denoted by G = {g 1 , g 2 , ..., g n }.To construct the training data, we first create all pairwise motif orders {(g i → g j )}.For test evaluations, we held out 10 out of these 45 motif orders.C.2.1. CONSTRUCTION OF EXEMPLAR SEQUENCESTo provide examples in-context, we create exemplar sequences connecting motifs, say g i1 and g i2 .In our construction, we select X s to be source node in g i1 and X g to be a sink node in g i2 .Further, we choose a sink of g i1 , X sink (g i1 ) and a source of g i2 , X source (g i2 ) and connect them via a ghost edge: (X sink (g i1 ), X source (g i2 )).These intermediate nodes are subgoals for the path that the model has to produce.Finally putting everything together, the exemplar sequence has the following form: goal: X g X s . . .X sink (g i1 )X source (g i2 ) . . .X g .Here, X s . . .X sink is a path from a source to a sink in g i1 and X source (g i2 ) . . .X g is a path from a source to a sink in g i2 .To be precise, we summarize this process into the algorithm below.Algorithm 3 Generate In-context ExemplarsRequire: {g i1 , g i2 }, two motifs across which a ghost edge will be placed.1: X s ← Sample sources(g i1 ) 2: X g ← Sample sinks(g i2 ) 3: (X ghost edge pre , X ghost edge post ) ← (Sample sinks(g i1 ), Sample sources(g i2 ))4: (X s . . .X ghost edge pre ) ← Sample path (g i1 ) 5: (X ghost edge post . . .X g ) ← Sample path (g i2 ) 6: return e gi 1 ,gi 2 ← X s . . .X ghost edge pre X ghost edge post . . .X g After providing a set of exemplar sequences in-context, we chain them together to create a longer sequence.To be precise, given a set of K motifs {g i1 , g i2 , g i3 , . . .g i K }, we have the set of K − 1 ghost edges, one for each exemplar: {(X sink (g i1 ), X source (g i2 )), (X sink (g i2 ), X source (g i3 )), . . .(X sink (g i K−1 ), X source (g i K ))}.To create the final path, we choose goal X g ∈ g i1 and start X s ∈ g i K .This path has every ghost edge from the list present in it.Inspired by the interpretability results, we argue our graph navigation task can also be solved using a similar low-rank factorization method that is generalized to 3-way relations.In this case, the tensor T to be factorized is third-order, and for each node, we have three types of embeddings: one which is used when the node acts as a goal, one when it acts as the current state, and one when it acts as the next possible state.Since we do not deal with a natural corpus with different frequency of occurrence of individual nodes, we can we set the numbers T ijk to be proportional to the length l ijk of a path which goes through an ordered pair of neighbour nodes (i, j) to a node k.If there is no such path, we set the length to infinity.The target value T ijk can be seen as a preference for a node j when the goal is to reach the node k from the node i; it is equal to l −1 ijk / j ′ l −1 ij ′ k .Inspired by the learned algorithm, we can use low-rank tensor factorization to approximate this matrix by the following expression T ijk ≈ Tijk = (s i + g k ) • n i , where s i , g k , n i are the three types of learnable embeddings.Therefore, by interpreting the trained Transformer, we can obtain a simple algorithm that can be potentially useful in setups that deal with 3-way relationships.We leave this for future work.
Physics of language models: Part 1, context-free grammar. Z Allen-Zhu, Y Li, arXiv:2305.136732023arXiv preprint</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S. S. Palm. 22023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, L Gianinazzi, J Gajda, T Lehmann, M Podstawski, H Niewiadomski, P Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Percolation processes: I. crystals and mazes. S R Broadbent, J M Hammersley, Mathematical proceedings of the Cambridge philosophical society. Cambridge University Press195753</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Two failures of selfconsistency in the multi-step reasoning of llms. A Chen, J Phang, A Parrish, V Padmakumar, C Zhao, S R Bowman, K Cho, arXiv:2305.142792023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Introduction to algorithms. N Chomsky, T H Cormen, C E Leiserson, R L Rivest, C Stein, 2002. 2022MIT pressSyntactic structures</p>
<p>A Creswell, M Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Selectioninference: Exploiting large language models for interpretable logical reasoning. A Creswell, M Shanahan, I Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. N Dziri, X Lu, M Sclar, X L Li, L Jian, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, arXiv:2305.186542023arXiv preprint</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. G Feng, Y Gu, B Zhang, H Ye, D He, L Wang, arXiv:2305.154082023arXiv preprint</p>
<p>T Gemini, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805family of highly capable multimodal models. 2023arXiv preprint</p>
<p>S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>D Hendrycks, K Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>J Huang, K C Chang, .-C, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>. A Karpathy, Nanogpt, 2021Github link</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Planning algorithms. S Lavalle, google schola. Cambridge University Press20062</p>
<p>Dissecting chain-of-thought: A study on compositional in-context learning of mlps. Y Li, K Sreenivasan, A Giannou, D Papailiopoulos, S Oymak, arXiv:2305.188692023arXiv preprint</p>
<p>B Liu, J T Ash, S Goel, A Krishnamurthy, C Zhang, arXiv:2210.10749Transformers learn shortcuts to automata. 2022arXiv preprint</p>
<p>N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>S Lu, I Bigoulaeva, R Sachdeva, H T Madabushi, I Gurevych, arXiv:2309.01809Are emergent abilities in large language models just in-context learning?. 2023arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Y Lu, M Bartolo, A Moore, S Riedel, P Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>Mechanistic mode connectivity. E S Lubana, E J Bigelow, R P Dick, D Krueger, H Tanaka, International Conference on Machine Learning. PMLR2023</p>
<p>Evaluating cognitive maps in large language models with cogeval: No emergent planning. I Momennejad, H Hasanbeig, F V Frujeri, H Sharma, R O Ness, N Jojic, H Palangi, J Larson, Advances in neural information processing systems. 202337</p>
<p>A guided tour to approximate string matching. G Navarro, ACM computing surveys (CSUR). 200133</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.001142021arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. arXiv. 2023</p>
<p>A large-scale multi-subject multi-choice dataset for medical domain question answering. A Pal, L K Umapathi, M Sankarasubbu, Medmcqa, Conference on Health, Inference, and Learning. PMLR2022</p>
<p>Using the output embedding to improve language models. O Press, L Wolf, arXiv:1608.058592016arXiv preprint</p>
<p>O Press, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, arXiv:2210.03350Measuring and narrowing the compositionality gap in language models. 2022arXiv preprint</p>
<p>Why think step-bystep? reasoning emerges from the locality of experience. B Prystawski, N D Goodman, arXiv:2304.038432023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>How capable can a transformer become? a study on synthetic. R Ramesh, M Khona, R P Dick, H Tanaka, E S Lubana, arXiv:2311.129972023interpretable tasks. arXiv preprint</p>
<p>Impact of pretraining term frequencies on few-shot reasoning. Y Razeghi, I V Logan, R L Gardner, M Singh, S , arXiv:2202.072062022arXiv preprint</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, The Eleventh International Conference on Learning Representations. 2023</p>
<p>R Schaeffer, K Pistunova, S Khanna, S Consul, S Koyejo, arXiv:2307.10573Invalid logic, equivalent gains: The bizarreness of reasoning in language model prompting. 2023arXiv preprint</p>
<p>The pitfalls of simplicity bias in neural networks. H Shah, K Tamuly, A Raghunathan, P Jain, P Netrapalli, Advances in Neural Information Processing Systems. 202033</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. M Turpin, J Michael, E Perez, S R Bowman, arXiv:2305.043882023arXiv preprint</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. B Wang, S Min, X Deng, J Shen, Y Wu, L Zettlemoyer, H Sun, arXiv:2212.100012022aarXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022barXiv preprint</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>E Zelikman, J Mu, N D Goodman, Y T Wu, Star, Self-taught reasoner bootstrapping reasoning with reasoning. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
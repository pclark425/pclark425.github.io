<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1413 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1413</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1413</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-cee5546f40e4d3643fcd3011c28c9bc108c5839a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cee5546f40e4d3643fcd3011c28c9bc108c5839a" target="_blank">Plan2Vec: Unsupervised Representation Learning by Latent Plans</a></p>
                <p><strong>Paper Venue:</strong> Conference on Learning for Dynamics & Control</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.</p>
                <p><strong>Paper Abstract:</strong> In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1413.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1413.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan2vec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan2vec: Unsupervised Representation Learning by Latent Plans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative, graph-based world model that constructs a weighted graph of observations from offline sequences, learns a local reachability metric d via contrastive learning, and distills shortest-path plans on that graph into a goal-conditioned global metric D_phi (a latent embedding with an ℓ^p distance) used for fast planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Plan2vec (graph + latent plan distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World represented as a weighted directed graph G whose vertices are observations and edge weights are a learned local metric d(x_i,x_j). Long-range planning targets (shortest-path distances) are generated by heuristic graph search and used as supervised targets to train a Siamese/ResNet embedding φ(x) with an ℓ^p distance head (D_φ) so that shortest paths in G are embedded as geodesics in latent space; two variants: amortized search (no bootstrapping) and value-iteration-like (bootstrapped with target net and k-step lookahead).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>graph-based latent world model (discriminative latent metric model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual navigation (simulated maze, StreetLearn real-world navigation) and deformable object manipulation (rope tying), general goal-reaching tasks from image observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Planning success rate on goal-reaching tasks; correlation between learned latent distance D_φ and ground-truth shortest-path distance; 1-step neighbor classification accuracy of local metric d (train/test accuracy for neighbor detection); visualization of latent maps (2D/3D) to inspect metric structure.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Planning success rates (Plan2vec L2): Open Room 90.0% ± 2.0, Table 76.4% ± 9.2, C-Maze 80.2% ± 6.3 (from Table 1). StreetLearn 1-step success: Tiny 92.2% ± 2.9, Small 57.2% ± 4.3, Medium 51.4% ± 6.9 (Table 2). Local metric neighbor accuracy (threshold 1.5): Open train 98.6% ±0.7 / test 97.8% ±0.3; Rope train 96.6% ±0.9 / test 92.4% ±1.5; Street Learn train 99.2% ±0.5 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: learned embedding frequently yields an interpretable metric map (2D/3D visualizations show geographic/topological layout); shortest-paths appear as roughly straight lines in latent space; provides explicit nearest-neighbor edges and plans that can be visualized as image sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of low-dimensional (2D/3D) latent embeddings and heatmaps of predicted distance; inspection of visual plans (sequences of images along planned path); inspection of neighbor predictions from local metric d.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: e.g., StreetLearn largest subset (1.4k views) reaches convergence in just under 2 hours on a single V100 GPU; small subset training can take ≈9 minutes. Inference/planning: amortized planning via learned value is linear in plan depth; graph search (Dijkstra) is quadratic in planning depth. Memory: precompute adjacency matrix and cache Siamese latent vectors to speed search.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reported to be ≈1 order of magnitude more sample-efficient than fitted Q-iteration / standard deep Q-learning on the offline datasets (requires at least 1 magnitude less data to reach similar performance). Amortized planning is linear in plan depth versus Dijkstra's quadratic scaling; A* with learned heuristic often approaches optimality with about one expansion per planning step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High success on simulated and real navigation tasks as above; in rope manipulation, plan2vec can produce feasible visual plans that perturb rope configurations locally and generalize to goals from different trajectories (quantitative rope metrics not provided). Outperforms VAE baselines and SPTM (Semi-Parametric Topological Memory) in low-budget planning regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The world model prioritizes task-relevant (distance-to-goal) structure rather than reconstructive fidelity; this leads to strong downstream policy/planning utility (high planning success and fast inference) despite not being generative. Long-horizon plan distillation is important: inclusion of long-range plans during training is necessary to correctly learn distances between far-apart regions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include sensitivity to errors in the learned local metric d (so-called 'worm-hole' false connections can degrade the global metric D); the method is discriminative and does not model full observation generation (so not suitable when full predictive reconstruction is needed); fitted value-iteration variant is unstable on graphs due to cycles, whereas search-based supervision is stable but requires building/caching the graph; low-dimensional embeddings can introduce aliasing for very long distances unless dimensionality increased.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses contrastive / NCE and a 1:1:2 sampling ratio (identical:neighbor:far) to train local metric d; builds a weighted directed graph with threshold d0 for edges; uses heuristic search (SPF/A*/Dijkstra) to generate shortest-path regression targets; trains φ with Siamese or ResNet18 trunk and an ℓ^p metric head (p tuned per dataset); optionally uses bootstrapped FVI-style training with target network and k-step BFS lookahead; caches latent vectors and precomputes adjacency to accelerate search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to VAE-based embeddings: VAEs fold structure and perform worse on long-range planning. Compared to SPTM (1-step local metric): Plan2vec produces better long-range value estimates and higher success in limited planning budgets. Compared to fitted Q-iteration / DQN: Plan2vec is more sample-efficient (≈10x less data) and yields faster planning. Compared to generative latent dynamics models (E2C/RCE/InfoGAN): those model local forward dynamics and can be plannable locally but are limited to local relationships; Plan2vec explicitly learns global shortest-path distances and prioritizes task-relevant distances.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using graph-search-derived, long-horizon plan targets during training (amortized search/no-bootstrapping) to learn accurate global distances; use sufficient latent dimensionality to avoid aliasing for long-range distances; caching embeddings and precomputing adjacency speeds training; choose ℓ^p metric (p tuned per dataset, e.g., p≈1.2–2) and consider A* with learned heuristic for efficient planning. Also, include long-horizon examples in training to learn inter-region distances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan2Vec: Unsupervised Representation Learning by Latent Plans', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1413.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1413.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based dataset model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted observation graph (dataset-as-graph) used as an explicit model of state space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that constructs a weighted (directed) graph G = (V,E) from offline observation sequences where vertices are images/observations and edges connect near-neighbors as determined by a local metric d; edge weights encode local step cost used for shortest-path planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Weighted observation graph (G)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Explicit topological model: nodes are observations, edges exist when learned or labeled local metric d(x_i,x_j) ≤ d0; edge weights are set to d(x_i,x_j) (or step-wise distance). Graph serves as the substrate for heuristic search (Dijkstra/A*/SPF) to produce long-horizon plan targets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit topological world model (graph / semi-parametric model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>All domains in paper: simulated maze navigation, rope manipulation, StreetLearn navigation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality measured by: correctness of neighbor edges (1-step neighbor accuracy), ability of graph-search plans to match true trajectories, and downstream planning success rate using distilled D_φ.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Local neighbor accuracy high (see Plan2vec entry and Table 5). When graph built from correct neighbors, distilled metric D_φ achieves high planning success; if graph contains spurious 'worm-holes', global metric degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High interpretability: graph edges correspond to empirically observed or predicted 1-step transitions, plans are explicit sequences of observations and can be visualized, aiding inspection of feasibility and failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of planned image sequences; plotting expanded nodes during search; visualizing new transitions discovered by the learned local metric.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Graph construction requires computing local metric over candidate pairs and thresholding; precomputing adjacency matrix makes search instantaneous during training; memory scales with squared number of stored vertices if naive adjacency stored (paper precomputes adjacency for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Graph-search supervision amortizes planning cost into learned D_φ, leading to linear-in-depth planning at inference versus quadratic scaling for brute-force Dijkstra on full graph.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables long-horizon plan generation across datasets; combined with Plan2vec distillation yields the planning success rates reported above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Graph captures local dynamics from offline data, enabling model-based planning without a dynamics function; when the local metric is accurate, graph provides strong supervision for learning globally-consistent distances.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Graph-based models are sensitive to incorrect edges (worm-holes). Dense graphs or precomputing full adjacency can be memory intensive; sparsity and correct thresholding (d0) are important design trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use labeled transitions from dataset as initial edges (weight 1), learn local metric d to generalize and add new edges (loop closure), threshold d0 selects connectivity, precompute adjacency matrix and cache embeddings to accelerate graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pure parametric latent generative dynamics models, graph-based model is simpler (topological) and directly encodes reachability; compared to fully parametric dynamics simulators, it avoids forward-modeling errors but requires representative coverage in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests using accurate local metric training, appropriate neighbor threshold d0, caching latent vectors in a Siamese architecture, and precomputing adjacency to make graph search efficient; include long-horizon training plans to learn inter-region distances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan2Vec: Unsupervised Representation Learning by Latent Plans', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1413.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1413.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local metric d (contrastive neighbor model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned local reachability metric d(x,x') trained by contrastive (NCE) / supervised regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned function (Siamese-style / convolutional network) that predicts local reachability between two observations (1-step neighbor, identical, far-apart) trained with noise-contrastive estimation and regression targets; it defines edges in the observation graph and supplies edge weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Local metric d (learned via NCE / contrastive and supervised regression)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Siamese convolutional networks taking stacked image pairs to produce a scalar score representing local distance/reachability; training mixes labeled identical/neighbor/far pairs with smoothed L1 loss and an NCE objective. Architectures vary per domain (several conv layers -> linear layers -> scalar output).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned discriminative local reachability model (contrastive Siamese)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used across maze navigation, rope manipulation, and StreetLearn navigation to decide adjacency and edge weight in graph.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>1-step neighbor classification accuracy (binary threshold at d0), validation accuracy on held-out neighbor prediction, and inspection of predicted neighbor lists versus ground-truth transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported 1-step neighbor accuracies (threshold 1.5): Open train 98.6% ±0.7 / test 97.8% ±0.3; Table train 98.3% ±1.1 / test 97.6% ±0.3; Wall train 98.3% ±1.2 / test 97.4% ±0.4; Rope train 96.6% ±0.9 / test 92.4% ±1.5; StreetLearn train 99.2% ±0.5 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: outputs are scalar reachability scores and the top predicted neighbors can be visualized; errors (false positives) are identifiable as worm-holes in the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualizing top neighbor images for a query and plotting predicted neighbor edges; inspecting distribution of score vs ground-truth L2 distance (Fig. 10a).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Trained with mini-batches; specific parameter counts given in architecture pseudocode (convolutional stacks with final linear layers); training epochs small for most domains (e.g., 40 epochs for Maze local metric), inference is cheap per pair but global graph construction can be costly if many pairwise comparisons required.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Enables sparse graph construction so that planning can be efficient; caching Siamese embeddings reduces repeated compute to a kernel operation ||z_i - z_g|| during search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High neighbor detection accuracy leads to good downstream planning; poor local metric (worm-holes) degrades the global metric and planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-quality local metric is essential: it determines graph connectivity and thus the fidelity of global shortest-path estimates. Improving local metric directly improves global D_φ quality.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher negative sampling ratio during NCE improves learning but increases training cost; threshold choice (d0) trades graph connectivity (coverage) versus false shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive/NCE loss with 1:1:2 sampling ratio for identical:neighbor:far; smoothed L1 loss for regression labels in maze; Siamese and two-channel stacked input for image pairs; caching of latent embeddings to speed repeated kernel evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to purely visual generative similarity (e.g., VAE embeddings): local metric is trained specifically to predict reachability, so it outperforms general unsupervised visual embeddings at defining graph edges for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests tuning negative sample ratio and threshold d0, increasing latent dimensionality to reduce aliasing for long-range distances, and caching embeddings to accelerate search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan2Vec: Unsupervised Representation Learning by Latent Plans', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1413.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1413.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative latent dynamics models (E2C/RCE/L-SBMP/InfoGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embed to Control (E2C), Robust Controllable Embedding (RCE), L-SBMP, causal InfoGAN and related generative latent-space forward models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of generative latent-dynamics world models that learn latent spaces and forward-predictive dynamics (often with locally-linear structure) to enable planning/control from images; mentioned as prior work contrasting Plan2vec's discriminative, global-distance focus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embed to control: A locally linear latent dynamics model for control from raw images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generative latent dynamics models (E2C, RCE, L-SBMP, causal InfoGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that learn an encoder/decoder (often VAE-style) and a latent dynamics model enabling forward prediction; typically trained to reconstruct observations and predict next latent states under actions, yielding plannable latent dynamics but focused on local-step predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent generative world model (VAE-like + latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control from images, planning in local neighborhoods, visuomotor control and manipulation (prior works applied to robotic control tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured by reconstruction loss, next-state prediction error (MSE), and utility for locally-planned trajectories; paper does not report specific metrics for these models (cites prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: latent variables can correspond to plannable factors and local dynamics, but models focus on reconstructive fidelity and local transitions rather than global shortest-path geometry; interpretability varies by method (some aim for disentanglement).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reconstruction inspection, visualization of latent dynamics and locally-planned rollouts (methods vary by work).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed in this paper; generative latent dynamics models typically require training encoder/decoder and transition models (compute depends on architecture), but the paper notes they are limited to modeling local relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper claims these models are limited to local relationships and thus less suitable for learning global, long-horizon distance metrics compared to Plan2vec's graph-search distilled supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>While plannable locally, high local predictive fidelity does not guarantee correct global shortest-path distance representation; thus they may be less useful for tasks requiring global distance estimates from offline, disjoint trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generative modeling aims for reconstruction and local dynamics which may come at the cost of not explicitly encoding long-range shortest-path structure; Plan2vec trades full generative fidelity for task-relevant (distance) fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Typically include encoder-decoder architectures (VAEs), locally-linear latent dynamics constraints, and action-conditioned transition models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper contrasts them with Plan2vec: generative latent dynamics are plannable but focused on local relations; Plan2vec is discriminative and explicitly learns a global shortest-path metric, improving long-horizon planning from offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail here; paper suggests that for global-distance learning from offline data, discriminative graph-search-based supervision (Plan2vec) is more effective than purely local generative latent models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan2Vec: Unsupervised Representation Learning by Latent Plans', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1413.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1413.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dyna (Dyna-style unroll)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dyna: an integrated architecture for learning, planning, and reacting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL architecture that interleaves learning from real experience and planning using a model to generate simulated experience; Plan2vec uses a Dyna-styled graph unroll where graph-search acts as an expert policy to generate training trajectories for the value metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dyna, an integrated architecture for learning, planning, and reacting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dyna-style model-based unroll (graph-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conceptual use: graph-search generated trajectories (on the dataset graph) are treated as model-based simulated rollouts (Dyna-style) to provide supervised regression targets for training the global metric D_φ; no explicit learned forward dynamics are required because the graph serves as the model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>model-based planning paradigm (graph as model) / hybrid planning-learning loop</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used within Plan2vec training for offline goal-conditioned value/distillation (navigation, rope manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of generated rollouts judged by downstream effect on learned D_φ (planning success rates) and stability of training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported as standalone numbers; Plan2vec's reported success metrics reflect the effectiveness of this Dyna-style supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: generated plans are explicit sequences of graph vertices (images) that can be visualized and audited.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of simulated plan rollouts generated by graph-search and used as training targets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Graph-search can be expensive if done on-the-fly for many goal pairs, but Plan2vec amortizes this cost by training a parametric D_φ so that inference/planning becomes cheap (linear). Precomputing adjacency and caching embeddings reduces runtime cost during training.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dyna-style graph rollouts enable sample efficiency gains compared to naively training value functions from replay buffers; Plan2vec reports ≈1 order of magnitude less data required than fitted Q-iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Contributes to Plan2vec's high planning success rates reported above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Using a strong search expert (graph search) to generate long-horizon targets improves the learned global metric relative to memory-less or short-horizon samplers, particularly in harder domains requiring lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generating long-horizon plans via search is helpful but requires a reasonably accurate local graph; doing gradient-based relaxation (FVI) on graphs is unstable due to cycles, so Dyna-style search supervision preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use graph-search (SPF/A*/Dijkstra) as sampling policy for targets, optionally use bootstrapping (target network) for the FVI variant, limit search depth h in bootstrapped variant to trade stability vs long-range information.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Shows advantages over memory-less sampling and standard fitted value-iteration on graphs (stability and sample efficiency); when combined with parametric distillation, planning inference becomes much cheaper than brute-force graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan2Vec: Unsupervised Representation Learning by Latent Plans', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embed to control: A locally linear latent dynamics model for control from raw images <em>(Rating: 2)</em></li>
                <li>Robust locally-linear controllable embedding <em>(Rating: 2)</em></li>
                <li>Learning plannable representations with causal infogan <em>(Rating: 2)</em></li>
                <li>Dyna, an integrated architecture for learning, planning, and reacting <em>(Rating: 2)</em></li>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 2)</em></li>
                <li>Value iteration networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1413",
    "paper_id": "paper-cee5546f40e4d3643fcd3011c28c9bc108c5839a",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Plan2vec",
            "name_full": "Plan2vec: Unsupervised Representation Learning by Latent Plans",
            "brief_description": "A discriminative, graph-based world model that constructs a weighted graph of observations from offline sequences, learns a local reachability metric d via contrastive learning, and distills shortest-path plans on that graph into a goal-conditioned global metric D_phi (a latent embedding with an ℓ^p distance) used for fast planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Plan2vec (graph + latent plan distillation)",
            "model_description": "World represented as a weighted directed graph G whose vertices are observations and edge weights are a learned local metric d(x_i,x_j). Long-range planning targets (shortest-path distances) are generated by heuristic graph search and used as supervised targets to train a Siamese/ResNet embedding φ(x) with an ℓ^p distance head (D_φ) so that shortest paths in G are embedded as geodesics in latent space; two variants: amortized search (no bootstrapping) and value-iteration-like (bootstrapped with target net and k-step lookahead).",
            "model_type": "graph-based latent world model (discriminative latent metric model)",
            "task_domain": "Visual navigation (simulated maze, StreetLearn real-world navigation) and deformable object manipulation (rope tying), general goal-reaching tasks from image observations",
            "fidelity_metric": "Planning success rate on goal-reaching tasks; correlation between learned latent distance D_φ and ground-truth shortest-path distance; 1-step neighbor classification accuracy of local metric d (train/test accuracy for neighbor detection); visualization of latent maps (2D/3D) to inspect metric structure.",
            "fidelity_performance": "Planning success rates (Plan2vec L2): Open Room 90.0% ± 2.0, Table 76.4% ± 9.2, C-Maze 80.2% ± 6.3 (from Table 1). StreetLearn 1-step success: Tiny 92.2% ± 2.9, Small 57.2% ± 4.3, Medium 51.4% ± 6.9 (Table 2). Local metric neighbor accuracy (threshold 1.5): Open train 98.6% ±0.7 / test 97.8% ±0.3; Rope train 96.6% ±0.9 / test 92.4% ±1.5; Street Learn train 99.2% ±0.5 (Table 5).",
            "interpretability_assessment": "High: learned embedding frequently yields an interpretable metric map (2D/3D visualizations show geographic/topological layout); shortest-paths appear as roughly straight lines in latent space; provides explicit nearest-neighbor edges and plans that can be visualized as image sequences.",
            "interpretability_method": "Visualization of low-dimensional (2D/3D) latent embeddings and heatmaps of predicted distance; inspection of visual plans (sequences of images along planned path); inspection of neighbor predictions from local metric d.",
            "computational_cost": "Training: e.g., StreetLearn largest subset (1.4k views) reaches convergence in just under 2 hours on a single V100 GPU; small subset training can take ≈9 minutes. Inference/planning: amortized planning via learned value is linear in plan depth; graph search (Dijkstra) is quadratic in planning depth. Memory: precompute adjacency matrix and cache Siamese latent vectors to speed search.",
            "efficiency_comparison": "Reported to be ≈1 order of magnitude more sample-efficient than fitted Q-iteration / standard deep Q-learning on the offline datasets (requires at least 1 magnitude less data to reach similar performance). Amortized planning is linear in plan depth versus Dijkstra's quadratic scaling; A* with learned heuristic often approaches optimality with about one expansion per planning step.",
            "task_performance": "High success on simulated and real navigation tasks as above; in rope manipulation, plan2vec can produce feasible visual plans that perturb rope configurations locally and generalize to goals from different trajectories (quantitative rope metrics not provided). Outperforms VAE baselines and SPTM (Semi-Parametric Topological Memory) in low-budget planning regimes.",
            "task_utility_analysis": "The world model prioritizes task-relevant (distance-to-goal) structure rather than reconstructive fidelity; this leads to strong downstream policy/planning utility (high planning success and fast inference) despite not being generative. Long-horizon plan distillation is important: inclusion of long-range plans during training is necessary to correctly learn distances between far-apart regions.",
            "tradeoffs_observed": "Trade-offs include sensitivity to errors in the learned local metric d (so-called 'worm-hole' false connections can degrade the global metric D); the method is discriminative and does not model full observation generation (so not suitable when full predictive reconstruction is needed); fitted value-iteration variant is unstable on graphs due to cycles, whereas search-based supervision is stable but requires building/caching the graph; low-dimensional embeddings can introduce aliasing for very long distances unless dimensionality increased.",
            "design_choices": "Uses contrastive / NCE and a 1:1:2 sampling ratio (identical:neighbor:far) to train local metric d; builds a weighted directed graph with threshold d0 for edges; uses heuristic search (SPF/A*/Dijkstra) to generate shortest-path regression targets; trains φ with Siamese or ResNet18 trunk and an ℓ^p metric head (p tuned per dataset); optionally uses bootstrapped FVI-style training with target network and k-step BFS lookahead; caches latent vectors and precomputes adjacency to accelerate search.",
            "comparison_to_alternatives": "Compared to VAE-based embeddings: VAEs fold structure and perform worse on long-range planning. Compared to SPTM (1-step local metric): Plan2vec produces better long-range value estimates and higher success in limited planning budgets. Compared to fitted Q-iteration / DQN: Plan2vec is more sample-efficient (≈10x less data) and yields faster planning. Compared to generative latent dynamics models (E2C/RCE/InfoGAN): those model local forward dynamics and can be plannable locally but are limited to local relationships; Plan2vec explicitly learns global shortest-path distances and prioritizes task-relevant distances.",
            "optimal_configuration": "Paper recommends using graph-search-derived, long-horizon plan targets during training (amortized search/no-bootstrapping) to learn accurate global distances; use sufficient latent dimensionality to avoid aliasing for long-range distances; caching embeddings and precomputing adjacency speeds training; choose ℓ^p metric (p tuned per dataset, e.g., p≈1.2–2) and consider A* with learned heuristic for efficient planning. Also, include long-horizon examples in training to learn inter-region distances.",
            "uuid": "e1413.0",
            "source_info": {
                "paper_title": "Plan2Vec: Unsupervised Representation Learning by Latent Plans",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Graph-based dataset model",
            "name_full": "Weighted observation graph (dataset-as-graph) used as an explicit model of state space",
            "brief_description": "A model that constructs a weighted (directed) graph G = (V,E) from offline observation sequences where vertices are images/observations and edges connect near-neighbors as determined by a local metric d; edge weights encode local step cost used for shortest-path planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Weighted observation graph (G)",
            "model_description": "Explicit topological model: nodes are observations, edges exist when learned or labeled local metric d(x_i,x_j) ≤ d0; edge weights are set to d(x_i,x_j) (or step-wise distance). Graph serves as the substrate for heuristic search (Dijkstra/A*/SPF) to produce long-horizon plan targets.",
            "model_type": "explicit topological world model (graph / semi-parametric model)",
            "task_domain": "All domains in paper: simulated maze navigation, rope manipulation, StreetLearn navigation",
            "fidelity_metric": "Quality measured by: correctness of neighbor edges (1-step neighbor accuracy), ability of graph-search plans to match true trajectories, and downstream planning success rate using distilled D_φ.",
            "fidelity_performance": "Local neighbor accuracy high (see Plan2vec entry and Table 5). When graph built from correct neighbors, distilled metric D_φ achieves high planning success; if graph contains spurious 'worm-holes', global metric degrades.",
            "interpretability_assessment": "High interpretability: graph edges correspond to empirically observed or predicted 1-step transitions, plans are explicit sequences of observations and can be visualized, aiding inspection of feasibility and failure modes.",
            "interpretability_method": "Visual inspection of planned image sequences; plotting expanded nodes during search; visualizing new transitions discovered by the learned local metric.",
            "computational_cost": "Graph construction requires computing local metric over candidate pairs and thresholding; precomputing adjacency matrix makes search instantaneous during training; memory scales with squared number of stored vertices if naive adjacency stored (paper precomputes adjacency for experiments).",
            "efficiency_comparison": "Graph-search supervision amortizes planning cost into learned D_φ, leading to linear-in-depth planning at inference versus quadratic scaling for brute-force Dijkstra on full graph.",
            "task_performance": "Enables long-horizon plan generation across datasets; combined with Plan2vec distillation yields the planning success rates reported above.",
            "task_utility_analysis": "Graph captures local dynamics from offline data, enabling model-based planning without a dynamics function; when the local metric is accurate, graph provides strong supervision for learning globally-consistent distances.",
            "tradeoffs_observed": "Graph-based models are sensitive to incorrect edges (worm-holes). Dense graphs or precomputing full adjacency can be memory intensive; sparsity and correct thresholding (d0) are important design trade-offs.",
            "design_choices": "Use labeled transitions from dataset as initial edges (weight 1), learn local metric d to generalize and add new edges (loop closure), threshold d0 selects connectivity, precompute adjacency matrix and cache embeddings to accelerate graph search.",
            "comparison_to_alternatives": "Compared to pure parametric latent generative dynamics models, graph-based model is simpler (topological) and directly encodes reachability; compared to fully parametric dynamics simulators, it avoids forward-modeling errors but requires representative coverage in the dataset.",
            "optimal_configuration": "Paper suggests using accurate local metric training, appropriate neighbor threshold d0, caching latent vectors in a Siamese architecture, and precomputing adjacency to make graph search efficient; include long-horizon training plans to learn inter-region distances.",
            "uuid": "e1413.1",
            "source_info": {
                "paper_title": "Plan2Vec: Unsupervised Representation Learning by Latent Plans",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Local metric d (contrastive neighbor model)",
            "name_full": "Learned local reachability metric d(x,x') trained by contrastive (NCE) / supervised regression",
            "brief_description": "A learned function (Siamese-style / convolutional network) that predicts local reachability between two observations (1-step neighbor, identical, far-apart) trained with noise-contrastive estimation and regression targets; it defines edges in the observation graph and supplies edge weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Local metric d (learned via NCE / contrastive and supervised regression)",
            "model_description": "Siamese convolutional networks taking stacked image pairs to produce a scalar score representing local distance/reachability; training mixes labeled identical/neighbor/far pairs with smoothed L1 loss and an NCE objective. Architectures vary per domain (several conv layers -&gt; linear layers -&gt; scalar output).",
            "model_type": "learned discriminative local reachability model (contrastive Siamese)",
            "task_domain": "Used across maze navigation, rope manipulation, and StreetLearn navigation to decide adjacency and edge weight in graph.",
            "fidelity_metric": "1-step neighbor classification accuracy (binary threshold at d0), validation accuracy on held-out neighbor prediction, and inspection of predicted neighbor lists versus ground-truth transitions.",
            "fidelity_performance": "Reported 1-step neighbor accuracies (threshold 1.5): Open train 98.6% ±0.7 / test 97.8% ±0.3; Table train 98.3% ±1.1 / test 97.6% ±0.3; Wall train 98.3% ±1.2 / test 97.4% ±0.4; Rope train 96.6% ±0.9 / test 92.4% ±1.5; StreetLearn train 99.2% ±0.5 (Table 5).",
            "interpretability_assessment": "Moderately interpretable: outputs are scalar reachability scores and the top predicted neighbors can be visualized; errors (false positives) are identifiable as worm-holes in the graph.",
            "interpretability_method": "Visualizing top neighbor images for a query and plotting predicted neighbor edges; inspecting distribution of score vs ground-truth L2 distance (Fig. 10a).",
            "computational_cost": "Trained with mini-batches; specific parameter counts given in architecture pseudocode (convolutional stacks with final linear layers); training epochs small for most domains (e.g., 40 epochs for Maze local metric), inference is cheap per pair but global graph construction can be costly if many pairwise comparisons required.",
            "efficiency_comparison": "Enables sparse graph construction so that planning can be efficient; caching Siamese embeddings reduces repeated compute to a kernel operation ||z_i - z_g|| during search.",
            "task_performance": "High neighbor detection accuracy leads to good downstream planning; poor local metric (worm-holes) degrades the global metric and planning performance.",
            "task_utility_analysis": "High-quality local metric is essential: it determines graph connectivity and thus the fidelity of global shortest-path estimates. Improving local metric directly improves global D_φ quality.",
            "tradeoffs_observed": "Higher negative sampling ratio during NCE improves learning but increases training cost; threshold choice (d0) trades graph connectivity (coverage) versus false shortcuts.",
            "design_choices": "Contrastive/NCE loss with 1:1:2 sampling ratio for identical:neighbor:far; smoothed L1 loss for regression labels in maze; Siamese and two-channel stacked input for image pairs; caching of latent embeddings to speed repeated kernel evaluations.",
            "comparison_to_alternatives": "Compared implicitly to purely visual generative similarity (e.g., VAE embeddings): local metric is trained specifically to predict reachability, so it outperforms general unsupervised visual embeddings at defining graph edges for planning.",
            "optimal_configuration": "Paper suggests tuning negative sample ratio and threshold d0, increasing latent dimensionality to reduce aliasing for long-range distances, and caching embeddings to accelerate search.",
            "uuid": "e1413.2",
            "source_info": {
                "paper_title": "Plan2Vec: Unsupervised Representation Learning by Latent Plans",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Generative latent dynamics models (E2C/RCE/L-SBMP/InfoGAN)",
            "name_full": "Embed to Control (E2C), Robust Controllable Embedding (RCE), L-SBMP, causal InfoGAN and related generative latent-space forward models",
            "brief_description": "A family of generative latent-dynamics world models that learn latent spaces and forward-predictive dynamics (often with locally-linear structure) to enable planning/control from images; mentioned as prior work contrasting Plan2vec's discriminative, global-distance focus.",
            "citation_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "mention_or_use": "mention",
            "model_name": "Generative latent dynamics models (E2C, RCE, L-SBMP, causal InfoGAN)",
            "model_description": "Models that learn an encoder/decoder (often VAE-style) and a latent dynamics model enabling forward prediction; typically trained to reconstruct observations and predict next latent states under actions, yielding plannable latent dynamics but focused on local-step predictions.",
            "model_type": "latent generative world model (VAE-like + latent dynamics)",
            "task_domain": "Control from images, planning in local neighborhoods, visuomotor control and manipulation (prior works applied to robotic control tasks).",
            "fidelity_metric": "Typically measured by reconstruction loss, next-state prediction error (MSE), and utility for locally-planned trajectories; paper does not report specific metrics for these models (cites prior work).",
            "fidelity_performance": null,
            "interpretability_assessment": "Partially interpretable: latent variables can correspond to plannable factors and local dynamics, but models focus on reconstructive fidelity and local transitions rather than global shortest-path geometry; interpretability varies by method (some aim for disentanglement).",
            "interpretability_method": "Reconstruction inspection, visualization of latent dynamics and locally-planned rollouts (methods vary by work).",
            "computational_cost": "Not detailed in this paper; generative latent dynamics models typically require training encoder/decoder and transition models (compute depends on architecture), but the paper notes they are limited to modeling local relationships.",
            "efficiency_comparison": "Paper claims these models are limited to local relationships and thus less suitable for learning global, long-horizon distance metrics compared to Plan2vec's graph-search distilled supervision.",
            "task_performance": null,
            "task_utility_analysis": "While plannable locally, high local predictive fidelity does not guarantee correct global shortest-path distance representation; thus they may be less useful for tasks requiring global distance estimates from offline, disjoint trajectories.",
            "tradeoffs_observed": "Generative modeling aims for reconstruction and local dynamics which may come at the cost of not explicitly encoding long-range shortest-path structure; Plan2vec trades full generative fidelity for task-relevant (distance) fidelity.",
            "design_choices": "Typically include encoder-decoder architectures (VAEs), locally-linear latent dynamics constraints, and action-conditioned transition models.",
            "comparison_to_alternatives": "Paper contrasts them with Plan2vec: generative latent dynamics are plannable but focused on local relations; Plan2vec is discriminative and explicitly learns a global shortest-path metric, improving long-horizon planning from offline data.",
            "optimal_configuration": "Not discussed in detail here; paper suggests that for global-distance learning from offline data, discriminative graph-search-based supervision (Plan2vec) is more effective than purely local generative latent models.",
            "uuid": "e1413.3",
            "source_info": {
                "paper_title": "Plan2Vec: Unsupervised Representation Learning by Latent Plans",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "Dyna (Dyna-style unroll)",
            "name_full": "Dyna: an integrated architecture for learning, planning, and reacting",
            "brief_description": "A model-based RL architecture that interleaves learning from real experience and planning using a model to generate simulated experience; Plan2vec uses a Dyna-styled graph unroll where graph-search acts as an expert policy to generate training trajectories for the value metric.",
            "citation_title": "Dyna, an integrated architecture for learning, planning, and reacting",
            "mention_or_use": "use",
            "model_name": "Dyna-style model-based unroll (graph-based)",
            "model_description": "Conceptual use: graph-search generated trajectories (on the dataset graph) are treated as model-based simulated rollouts (Dyna-style) to provide supervised regression targets for training the global metric D_φ; no explicit learned forward dynamics are required because the graph serves as the model.",
            "model_type": "model-based planning paradigm (graph as model) / hybrid planning-learning loop",
            "task_domain": "Used within Plan2vec training for offline goal-conditioned value/distillation (navigation, rope manipulation).",
            "fidelity_metric": "Quality of generated rollouts judged by downstream effect on learned D_φ (planning success rates) and stability of training.",
            "fidelity_performance": "Not reported as standalone numbers; Plan2vec's reported success metrics reflect the effectiveness of this Dyna-style supervision.",
            "interpretability_assessment": "High: generated plans are explicit sequences of graph vertices (images) that can be visualized and audited.",
            "interpretability_method": "Visual inspection of simulated plan rollouts generated by graph-search and used as training targets.",
            "computational_cost": "Graph-search can be expensive if done on-the-fly for many goal pairs, but Plan2vec amortizes this cost by training a parametric D_φ so that inference/planning becomes cheap (linear). Precomputing adjacency and caching embeddings reduces runtime cost during training.",
            "efficiency_comparison": "Dyna-style graph rollouts enable sample efficiency gains compared to naively training value functions from replay buffers; Plan2vec reports ≈1 order of magnitude less data required than fitted Q-iteration.",
            "task_performance": "Contributes to Plan2vec's high planning success rates reported above.",
            "task_utility_analysis": "Using a strong search expert (graph search) to generate long-horizon targets improves the learned global metric relative to memory-less or short-horizon samplers, particularly in harder domains requiring lookahead.",
            "tradeoffs_observed": "Generating long-horizon plans via search is helpful but requires a reasonably accurate local graph; doing gradient-based relaxation (FVI) on graphs is unstable due to cycles, so Dyna-style search supervision preferred.",
            "design_choices": "Use graph-search (SPF/A*/Dijkstra) as sampling policy for targets, optionally use bootstrapping (target network) for the FVI variant, limit search depth h in bootstrapped variant to trade stability vs long-range information.",
            "comparison_to_alternatives": "Shows advantages over memory-less sampling and standard fitted value-iteration on graphs (stability and sample efficiency); when combined with parametric distillation, planning inference becomes much cheaper than brute-force graph search.",
            "uuid": "e1413.4",
            "source_info": {
                "paper_title": "Plan2Vec: Unsupervised Representation Learning by Latent Plans",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "rating": 2
        },
        {
            "paper_title": "Robust locally-linear controllable embedding",
            "rating": 2
        },
        {
            "paper_title": "Learning plannable representations with causal infogan",
            "rating": 2
        },
        {
            "paper_title": "Dyna, an integrated architecture for learning, planning, and reacting",
            "rating": 2
        },
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 2
        },
        {
            "paper_title": "Value iteration networks",
            "rating": 1
        }
    ],
    "cost": 0.0182375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Plan2vec: Unsupervised Representation Learning by Latent Plans</h1>
<p>Ge Yang ${ }^{<em> \dagger}$<br>Amy Zhang ${ }^{</em> \dagger}$<br>Ari S. Morcos ${ }^{\dagger}$<br>Joelle Pineau ${ }^{\ddagger \ddagger}$<br>Pieter Abbeel ${ }^{\S}$<br>Roberto Calandra ${ }^{\dagger}$<br>${ }^{\dagger}$ Facebook AI Research, ${ }^{\ddagger}$ McGill University, ${ }^{\S}$ UC Berkeley</p>
<p>GE.IKE.YANG@GMAIL.COM AMYZHANG@FB.COM ARIMORCOS@FB.COM JPINEAU@CS.MCGILL.CA PABBEEL@CS.BERKELEY.EDU RCALANDRA@FB.COM</p>
<p>Editors: A. Bayen, A. Jadbabaie, G. J. Pappas, P. Parrilo, B. Recht, C. Tomlin, M. Zeilinger</p>
<h4>Abstract</h4>
<p>In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goalconditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space. Additional results and videos can be found at https://geyang.github.io/plan2vec.</p>
<h2>1. Introduction</h2>
<p>A good representation of the state space is essential to an intelligent agent that is trying to accomplish tasks in the world. For this reason, we look at representation learning through the lens of reinforcement learning. Under the standard Markov decision process (MDP, Bellman 4) formulation, the state space $S$ appears as the input domain for two types of functions. The first type is local, such as the transition probability $\mathbb{P}$ and the step-wise reward $R$. The second type is non-local and requires integration along paths, such as the state value $V(s)$ or the $Q$-function [14; 41; 26; 1]. For a specific type of task that can be formulated as accomplishing goals [19], the goal-conditioned value function $V(s, g)$ becomes a (negative) metric. The very focus of modern reinforcement learning is to learn $V$, for it parametrically encodes optimal plans. In policy search, such distance function acts are useful as a shaped reward $[14 ; 16 ; 45]$.</p>
<p>In this paper, we ask the question: is there a way to learn this type of metric representation without explicitly involving interactions with the environment, using only offline exploratory data that are abundantly available? Our key insight is that one can remove the need for learning dynamics by modeling these local relationships between near-neighbors as the edges of a graph, then use heuristic search to generate optimal long horizon plans for path integration. Our proposed method plan2vec - appears in two variants: the first uses regression towards a planned trajectory similar to dynamic distance learning [16] but with a strong search expert, whereas the second uses fitted value iteration $[10 ; 5 ; 35]$.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: High-level schematics of plan2vec. The dataset contains sequences of observations. The graph building step can be considered semi-supervised learning, where the real transitions are the labeled data, and the task is to find new transitions by learning a local metric $d$ that generalizes. The representation learning step can be considered learning an embedding $\phi$ of the graph. Plan2vec uses plans made on the graph to generate value targets for $D$, the shortest-path-distance metric.</p>
<p>To help illustrate our method, we lead the introduction of plan2vec with a set of simulated visual navigation tasks. We show the importance of using graph search as a sampling policy as opposed to a memory-less planner by looking at the planning success rate for k -steps of plan-ahead (see Fig. 6). Then we demonstrate our approach on deformable object manipulation with a rope dataset [50] that is otherwise difficult to model. Finally, we tackle a challenging real-world navigation dataset Street Learn [29] to show that plan2vec is able to learn to navigate from sequences of Street View images driving through the streets, with no access to the ground-truth GPS location data. Interestingly, we found that the representation plan2vec learns contains an interpretable metric map, despite that the graph it distills from is topological in nature.</p>
<h1>2. Technical Background</h1>
<p>The goal of reinforcement learning is to find a policy distribution $\pi(a \mid s)$ that maps from the state space $S$ to the action space $A$ for a given Markov decision process (MDP) [4], such that it maximizes the return, defined as the discounted sum of future rewards $J_{\pi}=\sum_{t} \gamma^{t} R\left(s_{t}, a_{t}\right)$. The optimality of a policy is provided by the Bellman equation</p>
<p>$$
V(s)=\mathcal{T} V^{<em>}, \quad \text { where } \quad \mathcal{T} V^{</em>} \equiv R\left(s, a, s^{\prime}\right)+\max <em s_prime="s^{\prime">{a} \sum</em>\right)
$$}} \mathbb{P}\left(s^{\prime} \mid s, a\right) \gamma V^{*}\left(s^{\prime</p>
<p>$\mathbb{P}\left(s^{\prime} \mid s, a\right)$ is the transition probability. $\mathcal{T}$ is the contraction operator defined recursively on the state-value function $V(s)$. We further assume that the MDP is fully observable, so there $\exists$ a mapping $\phi\left(o_{s}\right) \mapsto z_{s}$ from the space of observations $O$ to a latent space $Z$, for each state $s$.</p>
<p>When deep neural network is used as a function approximator [30], learning is typically implemented as sample-based regression towards an n-step bootstrapped value target [31]</p>
<p>$$
\mathcal{L}=\left|V\left(s_{0}\right)-\sum_{t=0}^{n} \gamma^{t-1} r-V^{*}\left(s_{n}\right)\right|_{2}
$$</p>
<p>Generalized Value Function as A Metric Learning to achieve goals is an important subproblem of reinforcement learning [19]. In a goal reaching task, the agent incurs a cost of $-d\left(s, s^{\prime}\right)$ at each step. The distance-to-goal $D(s, g)$ refers to the shortest path distance $\min <em _sim="\sim" _tau="\tau" x="x">{\tau} \sum</em>\right)$} d\left(x_{i}, x_{i+1</p>
<p>between $s$ and $g$. This formulation offers additional structure in that $D$ is a metric that satisfies the triangular inequality</p>
<p>$$
\forall s^{\prime} \in \mathcal{S}, D(s, g) \leq D\left(s, s^{\prime}\right)+D\left(s^{\prime}, g\right)
$$</p>
<p>For this reason, the generalized value function (GVF, Sutton et al. 47) family of algorithms [27; 39; 23; 43] formulate learning a goal-conditioned Q value function as learning predictive features. For our purpose of doing unsupervised representation learning without actions, this can be simplified as learning a value $V\left(o, o_{g}\right)=-D_{\phi}\left(o, o_{g}\right)$ where $D_{\phi}\left(o, o_{g}\right) \equiv|\phi(o), \phi\left(o_{g}\right)|_{p}$ is the distance between the latent features vectors.</p>
<p>Dataset as A Graph For a dataset of images $\left{x_{i}\right}$ there $\exists$ a weighted graph $\mathcal{G}=\langle\mathcal{V}, \mathcal{E}\rangle$ where each vertex $v_{i}$ corresponds to an image $x_{i} . e_{i j} \in \mathcal{E}$ iff according to a local metric $d, d\left(x_{i}, x_{j}\right)&lt;d_{0}$. We let the edge $e_{i j}$ weight by $w_{i j}=d\left(x_{i}, x_{j}\right)$. If we make the additional assumption that the data are sequences of observations, then the graph is directed.</p>
<h1>3. Unsupervised Representation Learning by Latent Planning</h1>
<p>Plan2vec is built upon the idea that for a collection of images with a local metric $d$, the graph $\mathcal{G}$ weighted by $d$ is embedded by a Riemann manifold, the metric of which is the shortest-path-distance $D$. By choosing the function class $D_{\phi}$ that decomposes into an embedding function $\phi(x)$ and a metric $|\cdot|_{p}$, we project $D$ to a $\ell^{p}$-metric space with a vector embedding.</p>
<p>Problem Formulation Plan2vec models the representation learning problem as learning how to play a goal-reaching-game in which one is tasked to find the shortest path from one observation to another, by hopping between near neighbors (Fig. 2). Under the context of reinforcement learning, plan2vec treats the graph as a model of the state space, and learns from Dyna-styled unroll using graph-search as an expert policy [46; 2]. Plan2vec treats the construction of the graph as a semi-supervised problem (Fig. 1). It first adds transitions from the dataset as edges with weight 1 . It then uses these as labeled data to learn a local metric $d$, to generalize to other pairs of images as a form of loop closure. An edge $e_{i j}$ is created between the node $v_{i}$ and $v_{j}$ if the distance between the corresponding images $d\left(x_{i}, x_{j}\right) \leq d_{0}$, a hyper parameter.</p>
<p>Learning Local Metric Noise-contrastive estimation cast representation learning as maximizing the contrast between two distributions: the joint distribution between related views $p\left(x, x^{+}\right)$, versus the product of the marginals $p(x) p\left(x^{-}\right)[32 ; 13 ; 49]$</p>
<p>$$
L_{\mathrm{NCE}}=-\log \frac{\exp S\left(x, x^{+}\right)}{\exp S\left(x, x^{+}\right)+\sum_{i}^{k} \exp S\left(x, x_{i}^{-}\right)}
$$</p>
<p>where $S$ is the similarity function to be learned. $\left\langle x, x^{+}\right\rangle$is the positive pair sampled in-context. $\left\langle x, x_{i}^{-}\right\rangle$is the negative pair sampled independently from the marginals. Under the context of control, Eq. 4 has a natural interpretation as maximizing the log-probability that a pair $\left\langle x, x^{+}\right\rangle$is reachable against pairs sampled at random, and is related to the distance metric by $d\left(x, x^{\prime}\right) \propto-\log p\left(x, x^{\prime}\right)$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Memoryless sampling vs search.</p>
<p>Algorithm 1 Plan2vec via Amortized Search
Require: weighted directed graph $\mathcal{G}=\langle\mathcal{V}, \mathcal{E}\rangle$
Require: Shortest Path First search algorithm SPF
1: Initialize $D_{\Phi}\left(x, x^{\prime}\right)$, let $V=-D$.
2: while not converged do
3: $\quad$ sample $x_{v_{s}}, x_{v_{g}}$ and $v_{s}, v_{g} \in \mathcal{G}$ as start and goal
4: find shortest plan $\tau^{<em>}=\operatorname{SPF}\left(\mathcal{G}, v_{s}, v_{g}, D_{\Phi}\right)$
5: minimize $\delta=\left|D_{\Phi}\left(x_{s}, x_{g}\right)-\sum_{v_{i} \sim \tau^{</em>}} d\left(x_{v_{i}}, x_{v_{i+1}}\right)\right|$
In the maze domain we directly regress the distance metric $d$ towards one of {identical, close, or far-apart $}$ with a nominal distance of ${0,1,2}$</p>
<p>$$
\mathcal{L}<em t="t">{\mathrm{d}}=|d(x, x)-0|+\left|d\left(x</em>\right)-2\right|
$$}, x_{t+1}\right)-1\right|+\left|d\left(x, x^{-</p>
<p>When $d_{\phi}$ is a Siamese network with an $\ell^{2}$ metric, the first term can be dropped. We use smoothed $L_{1}$ loss for all terms.</p>
<p>Learning Representation by Latent Plans Plan2vec samples pairs of images $x_{s}$ and $x_{g}$ and their corresponding vertices $v_{s}$ and $v_{g}$ from the graph $\mathcal{G}$, then uses heuristic search to find the shortest path $\tau^{*}$ in-between (Algorithm 1). Non-learning search algorithms typically discard the search tree after backtrack (step 4). Plan2vec collect these to generate regression targets for learning the value estimate with or without value bootstrapping. With the latter, one can use a fixed search depth $h$.</p>
<p>$$
\underset{\text { no bootstrapping }}{\mathrm{V}\left(\mathrm{x}<em _mathrm_g="\mathrm{g">{\mathrm{s}}, \mathrm{x}</em>}}\right)}=-\sum_{i}} d\left(x_{v_{i}}, x_{v_{i+1}}\right) \quad \underset{\text { bootstrapped }}{\mathrm{V}\left(\mathrm{x<em _mathrm_g="\mathrm{g">{\mathrm{s}}, \mathrm{x}</em>\right)
$$}}\right)}=-\sum_{i=0}^{h-1}} d\left(x_{v_{i}}, x_{v_{i+1}}\right)+V\left(x_{v_{h}}, x_{v_{g}</p>
<p>We experimented with both fitted value-iteration (FVI) and amortized heuristic search for learning on a graph. The main short-coming with FVI is that relaxation for finding the shortest path occurs via gradient-based, iterative updates. Such scheme is unstable when applied to a graph as cycles within each rollout stall learning; whereas heuristic search explicitly avoid vertex-revisit at planning time.</p>
<h1>4. Related Works</h1>
<p>Plan2Vec builds upon two rich bodies of literature: unsupervised methods that learn an embedding from a local context, and value-based reinforcement learning methods that learn a policy. In the first category, time-contrastive network (TCN), skip-gram (word2vec), contrastive predictive coding (CPC) and locally linear embeddings [40; 28; 32; 36] are a family of methods that embed images, word tokens and image patches by making each sample similar to its neighbors in a small neighboring context. Similarly, graph embedding algorithms such as DeepWalk, Node2vec and diffusion maps [33; 11; 44] randomly sample short trajectories in the neighborhood of a node to provide context. The locality of such context is restrictive, because one can not expect clear supervision from samples further apart. Plan2vec solves this problem by replacing those random processes with graph-search to directly generate long-horizon distance targets between nodes that are arbitrarily far apart.</p>
<p>Embed to control (E2C), robust controllable embedding (RCE), L-SBMP and causal InfoGAN $[52 ; 3 ; 18 ; 24]$ are a line of generative models that incorporate forward modeling in the latent space. They show that the learned representation is plannable, but the models are limited to modeling local relationships. Plan2vec differs by explicitly learning a shortest-path-distance metric to embed the weighted graph on a Riemann manifold that encodes all optimal plans as geodesics. In addition, plan2vec is purely discriminative, and focuses only on those features that are relevant towards predicting long-horizon distance relationships.</p>
<p>In the second category are differentiable planning algorithms on a grid world [12; 48; 25] and gradient-based planning methods that require supervision through expert demonstration [45; 54]. Plan2vec works in continuous state space, with random and off-policy exploratory data as a pretraining step. Additionally, the metric that plan2vec learns can be used as an intrinsic reward in self-supervised or task-agnostic RL [51; 8; 20; 34], to reduce the need of human designed reward.</p>
<p>Finally, plan2vec builds upon prior methods that plan over a graph with various assumptions [37; 38; 55; 6]. We compare against semi-parametric topological memory [37], and show that with a learned value function, plan2vec is able to make more intelligent choices at test time, under limited planning budget.</p>
<h1>5. Experimental Evaluation</h1>
<p>In this section, we experimentally answer the following questions: 1) What kind of representation can we learn via planning? 2) How does Dyna-style unroll on the graph affect the sample complexity? 3) Why is graph search needed? and finally, 4) Would plan2vec work in domains other than navigation, or learn features that are not visually apparent?</p>
<p>To answer these questions, we first examine plan2vec quantitatively on a simulated 2D navigation
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Visual navigation environments: Open, Table, and C-Maze. Agent in blue. Red sphere indicates the desired goal.
domain. Then we extend plan2vec to the challenging deformable object manipulation task, were the task is to tie a piece of rope. Finally, we show that plan2vec can learn non-visual features such as the agent's geolocation purely from first-person views without requiring ground-truth GPS data.</p>
<h3>5.1. Simulated Navigation</h3>
<p>The maze domain is a square, 2-dimensional arena with continuous $(x, y)$ coordinates. A top-down camera view is fed to the robot (block in blue). We use ground-truth coordinates for evaluation only. Our experiment covers three room layouts with increasing levels of difficulty: an open room, a room with a table in the middle, and a room</p>
<p>Table 1: Planning Performance on 2D Navigation</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Success Rate (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Image Input</td>
<td style="text-align: center;">Open Room</td>
<td style="text-align: center;">Table</td>
<td style="text-align: center;">C-Maze</td>
</tr>
<tr>
<td style="text-align: left;">plan2vec (L2)</td>
<td style="text-align: center;">$\mathbf{9 0 . 0} \pm \mathbf{2 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4} \pm \mathbf{9 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 2} \pm \mathbf{6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">SPTM (1-step)</td>
<td style="text-align: center;">$39.7 \pm 6.1$</td>
<td style="text-align: center;">$23.7 \pm 6.1$</td>
<td style="text-align: center;">$31.4 \pm 6.5$</td>
</tr>
<tr>
<td style="text-align: left;">VAE</td>
<td style="text-align: center;">$73.9 \pm 4.3$</td>
<td style="text-align: center;">$30.2 \pm 6.5$</td>
<td style="text-align: center;">$52.7 \pm 5.8$</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$3.2 \pm 2.5$</td>
<td style="text-align: center;">$3.5 \pm 2.5$</td>
<td style="text-align: center;">$4.7 \pm 2.8$</td>
</tr>
</tbody>
</table>
<p>with a wall that separates it into two corridors that resembles a C-shaped maze (see Fig. 4).</p>
<p>We first qualitatively verify the representation that plan2vec learns by making the latent space 2-dimensional. This allows us to directly visualize the latent vectors by plan2vec against those by a VAE ([22], see Fig. 5). The embedding VAE learns folds onto itself, whereas plan2vec learns an</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: 2-dimensional latent embedding. Plan2vec's embedding demonstrates clear global structure beyond close neighbors.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Left: Success rate vs the number of rollouts used for learning, Plan2vec vs DQN. Center: Success rate with k-step planahead, Plan2vec vs SPTM and a random baseline. Right: Planning Cost, Dijkstra's grows quadratically whereas plan2vec is linear. Lower is better.
embedding that respects the overall topological structure of the domain. Furthermore, observations from two opposite ends of the C-Maze are pulled apart, which reflects the longer shortest-pathdistance in-between. In other words, Plan2vec embeds optimal plans as roughly straight lines in its learned latent space.</p>
<p>We further study how much data it takes for plan2vec to learn compared to standard off-line reinforcement methods such as fitted Q-iteration [35]. We generate a fixed dataset, then vary the amount given to both plan2vec and a standard deep Q-learning algorithm during training. We plot the planning performance of the learned value function in Fig. 6a. Both methods achieve $100 \%$ when given sufficient data, but plan2vec requires at least 1 magnitudes less. This encouraging result shows the benefit of learning from a graphical model as opposed to replays from a linear buffer, and plan2vec's ability to efficiently construct optimal plans from off-policy, exploratory experience.</p>
<p>Combination of search and value learning is required in harder domains that requires a strong behavior policy [42; 15; 2]. In Fig. 6b, both plan2vec and SPTM improves in performance with more lookahead search budge, but plan2vec, which distils from a search expert during training, acquires a more informative long-range value estimate and better performance. When we compare how the cost of finding the shortest path scales with the amount of planning lookahead (see Fig. 6c). We found that plan2vec is linear in plan depth, as it amortizes the planning cost from training; whereas Dijkstra's is quadratic.</p>
<h1>5.2. Manipulation of Deformable Objects</h1>
<p>We now apply plan2vec to learn representations of a deformable object that lacks a structured configuration space. Past methods in this space either rely on learning a generator function [24], or
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Example of visual plan generated by plan2vec on the Rope Domain showing steps coming from two different trajectories (8 and 3). Each transition only perturbs the configuration of the rope locally. The numbers above denote the trajectory and time step the image is from, the number below represents the score by the local metric $f_{\phi}$. Note that the transition from sequence $8 \rightarrow$ sequence 3 occurred in-between the 3rd and 4th step. All the other transitions are real physical transitions.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Planning Cost. Gray dots show the vertices that are expanded during search. We color the expanded nodes with plan2vec in red to make the few expanded nodes more visible. The plans span 200 steps, $\sim 1.2$ kilometers each. Number on top of bars show the average cost per planning step. With a strong heuristic ( $\ell^{1}$ distance), A* is more economical than Dijkstra's. But with a learned heuristic plan2vec approaches optimality: a single expansion per step.
model-free reinforcement learning that can only accomplish a single task [53]. In contrast, plan2vec is purely discriminative, and can generalize to a dynamic set of goals.</p>
<p>We apply our method to a recent rope dataset [50]. This dataset comprises of 18 sequences that include in-total 14 k gray scale photos of a piece of rope. Two pegs fixiated on the table impose constraints that need to be respected during each transition. After training, plan2vec is able to find a visual plan given any pair of start and goal configurations regardless of whether they come from the same trajectory. Fig. 7 shows an example of such plans found by plan2vec. Each step only slightly perturbs the configuration of the rope, making the entire plan feasible.</p>
<p>It is difficult to design quantitative evaluation metrics for this domain. For evaluation, we select a start and goal image from the same trajectory, and compare the visual plans made by plan2vec against the ground-truth sequence in-between. We include these additional results in the appendix.</p>
<h1>5.3. Beyond Visual Similarity</h1>
<p>In previous domains, visual similarity goes a long way in revealing the distance in the configuration space. Generative models rely on such prior in order to learn, which make them potentially less suitable for learning distance information that are visually inconspicuous. Navigation in a real-world scenario offers a great example - it is impossible to tell the direction based off</p>
<p>Table 2: 1-step Planning Performance on StreetLearn. Goals are sampled within 50 steps of the starting point.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Success Rate (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Street Learn</td>
<td style="text-align: center;">Tiny</td>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">Medium</td>
</tr>
<tr>
<td style="text-align: left;">Plan2vec (Ours)</td>
<td style="text-align: center;">$\mathbf{9 2 . 2} \pm \mathbf{2 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 2} \pm \mathbf{4 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 4} \pm \mathbf{6 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">SPTM (1-step)</td>
<td style="text-align: center;">$31.5 \pm 5.8$</td>
<td style="text-align: center;">$19.3 \pm 5.8$</td>
<td style="text-align: center;">$20.2 \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: left;">VAE</td>
<td style="text-align: center;">$25.5 \pm 5.6$</td>
<td style="text-align: center;">$14.4 \pm 4.8$</td>
<td style="text-align: center;">$16.9 \pm 5.5$</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$19.9 \pm 5.4$</td>
<td style="text-align: center;">$12.0 \pm 5.2$</td>
<td style="text-align: center;">$12.7 \pm 4.6$</td>
</tr>
</tbody>
</table>
<p>two photos alone. Yet a city resident knows exactly how to navigate from one to another.
We now apply plan2vec to the challenging large scale navigation dataset Street Learn [29]. We found that plan2vec's supervised learning objective can learn a high-quality value estimate on a large, $1.4 k$ subset of Street Learn just under two hours, using only sequences of camera image and step-wise distance without access to the GPS locations. We inspect the learned embedding by restricting the latent space to 2-dimension, and discover a high-quality metric map (Fig. 8a).</p>
<p>Internalizing such a map can speed up planning and improve generalization. In Fig. 8 we compare the cost of heuristic search with and without using the distance function plan2vec learns. Dijkstra's SPF algorithm expands all nodes in the graph exhaustively, whereas $A^{*}$ using the Manhattan distance</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Plan2vec's internal metric map generalize to new tasks. (a) Pink and black color codes the four quadrants ( $\mathrm{b}, \mathrm{c}$ ) shows the train and test tasks (d) Red shows the distance prediction on test set, black on training set. (e, f) Embedding learned using this restricted training set is similar to a control using the entire dataset. Orientation of learned embedding depends on seed. Alignment to the axis is due to use of $p=1.2$. Randomly picked amongst 3 seeds.
as search heuristic fails to understand the diagonal streets near Broadway. Plan2vec almost optimally captures the shortest-path-distance on this domain, and out performs all other methods.</p>
<p>In Table 2, we artificially limit the computation and memory budget for the planner by setting both the lookahead depth $k$ and the memory size $|\mathcal{H}|$ for the priority queue to 1 . In this interesting regime, a good planning heuristic is necessary for good performance. We train an embedding $\phi(x)$ with each method, then use an $\ell^{2}$ metric defined on this embedding as the search heuristic. The VAE baseline barely performs above random. This is expected for unsupervised methods that rely on visual inductive priors for embedding. In comparison to SPTM's 1-step local metric $d$, plan2vec performs $2-3 \times$ better consistently across all three datasets.</p>
<h1>5.4. Generalization With A Metric Map</h1>
<p>An important reason to distill plans into a neural network is generalization. In previous experiments, planning generalizes to previously unseen tasks by interpolation. Now we want to ask: how about we remove training tasks that go between large areas of the map - would plan2vec still able to generalize to bundles of task configurations it has never seen during training? In this experiment, we divide the map into four quadrants (Fig. 9a) and remove tasks that route between diagonally opposing quadrants during training. To our surprise, the learned embedding is as good as the control that trains with the entire task set. This result depends on the connectivity of the road network, but it shows that plan2vec can sometimes generalize despite of categorical removal of training tasks.</p>
<h2>6. Conclusion</h2>
<p>We have presented a discriminative and unsupervised approach to learn long-horizon distance relationships via planning. Our method does not generate images, is model-agnostic, and requires no access to expert action data. In comparison to model-free reinforcement learning methods that sample directly from the environment, our model-based approach makes more efficient use of otherwise disjoint trajectories. The embedding plan2vec learns encodes the shortest-path between observations as geodesics in the latent space, which reduce iterative planning to fast, parameterized lookup. We demonstrate these desirable properties on one simulated and two challenging real-world datasets, and propose plan2vec as a valuable pre-training step for reinforcement learning agents from off-line exploratory data.</p>
<h1>References</h1>
<p>[1] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.
[2] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Advances in Neural Information Processing Systems 30, pages 5360-5370. 2017.
[3] Ershad Banijamali, Rui Shu, Mohammad Ghavamzadeh, Hung Bui, and Ali Ghodsi. Robust locally-linear controllable embedding. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2018.
[4] Richard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, pages 679-684, 1957.
[5] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6(Apr):503-556, 2005.
[6] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planning and reinforcement learning. ArXiv, abs/1906.05253, 2019.
[7] Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and Shimon Whiteson. Treeqn and atreec: Differentiable tree-structured models for deep reinforcement learning. In ICLR 2018, 2018.
[8] Carlos Florensa, Jonas Degrave, Nicolas Heess, Jost Tobias Springenberg, and Martin A. Riedmiller. Self-supervised learning of image embedding for continuous control. arXiv preprint arXiv:1901.00943, 2019.
[9] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in high dimensions via hashing. VLDB J., 1999. ISSN 1066-8888.
[10] Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning Proceedings 1995, pages 261-268. Elsevier, 1995.
[11] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. KDD, 2016:855-864, August 2016. ISSN 2154-817X. doi: 10.1145/2939672.2939754.
[12] Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. International Journal of Computer Vision, Oct 2019.
[13] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 297-304, 2010.
[14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.</p>
<p>[15] Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Tobias Pfaff, Theophane Weber, Lars Buesing, and Peter W Battaglia. Combining q-learning and search with amortized value estimates. arXiv preprint arXiv:1912.02807, December 2019.
[16] Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance learning for unsupervised and semi-supervised skill discovery. arXiv preprint arXiv:1907.08225, 2019.
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
[18] Brian Ichter and Marco Pavone. Robot motion planning in learned latent spaces. arXiv preprint arXiv:1807.10366, 2018.
[19] Leslie P Kaelbling. Learning to achieve goals. IJCAI, 1993. ISSN 1045-0823.
[20] Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, and Sergey Levine. Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation. arXiv preprint arXiv:1709.10489, 2017.
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, December 2014.
[22] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[23] Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep successor reinforcement learning. arXiv preprint arXiv:1606.02396, 2016.
[24] Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J Russell, and Pieter Abbeel. Learning plannable representations with causal infogan. In Advances in Neural Information Processing Systems, pages 8747-8758, 2018.
[25] Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric Xing, and Ruslan Salakhutdinov. Gated path planning networks. arXiv preprint arXiv:1806.06408, 2018.
[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[27] Michael L Littman, Richard S Sutton, and Satinder P Singh. Predictive representations of state. Advances in Neural Information Processing Systems, 2001.
[28] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
[29] Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, and Raia Hadsell. Learning to navigate in cities without a map. In Advances in Neural Information Processing Systems, 2018.</p>
<p>[30] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
[31] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (ICML), pages 1928-1937, June 2016.
[32] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[33] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701-710. ACM, 2014.
[34] Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019.
[35] Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pages 317-328. Springer, 2005.
[36] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):2323-2326, 2000.
[37] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.
[38] Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, and Sylvain Gelly. Episodic Curiosity through Reachability. arXiv preprint arXiv:1810.02274, 2018.
[39] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning (ICML), pages 1312-1320, 2015.
[40] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In IEEE International Conference on Robotics and Automation (ICRA), pages 1134-1141, 2018.
[41] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International Conference on Machine Learning (ICML), 2014.
[42] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis</p>
<p>Hassabis. Mastering the game of go without human knowledge. Nature, 550(7676):354-359, October 2017. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature24270.
[43] Satinder P Singh, Michael L Littman, Nicholas K Jong, David Pardoe, and Peter Stone. Learning predictive state representations. International Conference on Machine Learning (ICML), 2003.
[44] Richard Socher and Matthias Hein. Manifold learning and dimensionality reduction with diffusion maps. In Seminar report, Saarland University, 2008.
[45] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks: Learning generalizable representations for visuomotor control. In International Conference on Machine Learning (ICML), volume 80, pages 4732-4741, 10-15 Jul 2018.
[46] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bull., 2(4):160-163, July 1991. ISSN 0163-5719. doi: 10.1145/122344.122377.
[47] Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International Conference on $A u$ tonomous Agents and Multiagent Systems-Volume 2, pages 761-768. International Foundation for Autonomous Agents and Multiagent Systems, 2011.
[48] Aviv Tamar, YI WU, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, pages 2154-2162. 2016.
[49] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.
[50] Angelina Wang, Thanard Kurutach, Aviv Tamar, and Pieter Abbeel. Learning robotic manipulation through visual planning and acting. In Deep RL Workshop at NeurIPS, 2018.
[51] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, November 2018.
[52] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pages 2746-2754, 2015.
[53] Yilin Wu, Wilson Yan, Thanard Kurutach, Lerrel Pinto, and Pieter Abbeel. Learning to manipulate deformable objects without demonstrations. arXiv preprint arXiv:1910.13439, October 2019.
[54] Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, and Chelsea Finn. Unsupervised visuomotor control through distributional planning networks. arXiv preprint arXiv:1902.05542, 2019.
[55] Amy Zhang, Sainbayar Sukhbaatar, Adam Lerer, Arthur Szlam, and Rob Fergus. Composable planning with attributes. In International Conference on Machine Learning (ICML), pages $5837-5846,2018$.</p>
<h1>A. Algorithmic Details</h1>
<p>We experiment with two variants. The two algorithms differ by the sampling policy and the regression target for the value function. The main variant in Algorithm 1 uses graph search as the sampling policy and no bootstrapping during learning. Search terminates either when the goal is reached, or it has exhaustively searched the entire graph without finding a path. We precompute the pairwise adjacency matrix so search is instantaneous. To use $A^{*}$ in the place of Dijkstra's during training, each node expansion would require pairwise comparison with respect of the goal. Savinov et al. 37 introduced a trick to speed this up by pre-computing all of the distance-to-goal value for a fixed goal. This can be sped up additionally by caching the latent vectors in a Siamese architecture [38], so that only the kernel operation $\left|z_{i}-z_{g}\right|$ needs to be computed.</p>
<p>The second variant, show in Algorithm 2 is similar to TreeQN [7]. It uses breath-first-search (BFS) with a fixed search depth $k$ at each step before making a greedy selection to minimize $D\left(v_{t+1}, v_{g}\right)$. When the lookahead depth $k=1$, this is identical to standard 1-step Q-learning. When $k&gt;1$ BFS is strictly stronger than $\epsilon$-greedy, because it exhaustively finds the neighbor within the $k$-step ball around the current vertex as opposed to 1-step neighbors. We use a target network for bootstrapping the values. We found when applied to the graph, this variant is unstable and is sensitive to hyperparameters. Training stability improves with larger $k$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mh">2</span><span class="w"> </span><span class="n">Plan2Vec</span><span class="w"> </span><span class="n">via</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="n">Iteration</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">lookahead</span><span class="w"> </span><span class="n">\(k\),</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">limit</span><span class="w"> </span><span class="n">\(h\)</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">weighted</span><span class="w"> </span><span class="n">directed</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">\(\mathcal{G}=\langle\mathcal{V},</span><span class="w"> </span><span class="n">\mathcal{E}\rangle\)</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">Breadth</span><span class="o">-</span><span class="n">first</span><span class="w"> </span><span class="n">search</span><span class="w"> </span><span class="p">(</span><span class="n">BFS</span><span class="p">)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">\(D_{\Phi}\left(x,</span><span class="w"> </span><span class="n">x</span><span class="o">^</span><span class="p">{</span><span class="n">\prime}\right)\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">\(D_{\Phi}^{+}\)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">converged</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">images</span><span class="w"> </span><span class="n">\(x_{s},</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">g</span><span class="p">}</span><span class="n">\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">corresponding</span><span class="w"> </span><span class="n">vertices</span><span class="w"> </span><span class="n">\(v_{s},</span><span class="w"> </span><span class="n">v_</span><span class="p">{</span><span class="n">g</span><span class="p">}</span><span class="w"> </span><span class="n">\in</span><span class="w"> </span><span class="n">\mathcal{G}\)</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">goal</span>
<span class="w">        </span><span class="n">Initialize</span><span class="w"> </span><span class="n">\(v=v_{s}\),</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="n">\(l=0\)</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="n">\(v</span><span class="w"> </span><span class="n">\not</span><span class="w"> </span><span class="n">\equiv</span><span class="w"> </span><span class="n">v_</span><span class="p">{</span><span class="n">g</span><span class="p">}</span><span class="n">\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">\(|p|&lt;h\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="n">Pick</span><span class="w"> </span><span class="n">\(v^{\prime}=\arg</span><span class="w"> </span><span class="n">\min</span><span class="w"> </span><span class="n">_</span><span class="p">{</span><span class="n">v</span><span class="w"> </span><span class="n">\in</span><span class="w"> </span><span class="n">N</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">)}</span><span class="w"> </span><span class="n">D_</span><span class="p">{</span><span class="n">\Phi}\left(x_{v},</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">g</span><span class="p">}</span><span class="n">\right)\)</span>
<span class="w">            </span><span class="n">Find</span><span class="w"> </span><span class="n">subplan</span><span class="w"> </span><span class="n">\(\left\{v_{i}\right\}=\operatorname{BFS}\left(\mathcal{G},</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="o">^</span><span class="p">{</span><span class="n">\prime}\right)\)</span>
<span class="w">            </span><span class="n">Add</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">subplan</span><span class="w"> </span><span class="n">\(l</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">l</span><span class="o">+</span><span class="n">\sum_{i}</span><span class="w"> </span><span class="n">v_</span><span class="p">{</span><span class="n">j</span><span class="w"> </span><span class="n">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="n">\)</span>
<span class="w">            </span><span class="n">Aggregate</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">\(p</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="n">\oplus\left\{v_{i}\right\}\)</span>
<span class="w">            </span><span class="n">Assign</span><span class="w"> </span><span class="n">\(v</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">v</span><span class="o">^</span><span class="p">{</span><span class="n">\prime}\)</span>
<span class="w">            </span><span class="n">minimize</span><span class="w"> </span><span class="n">\(\delta=\left|D_{\Phi}\left(x_{s},</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">g</span><span class="p">}</span><span class="n">\right)-\left(l+D_{\Phi}^{+}\left(x_{v},</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">g</span><span class="p">}</span><span class="n">\right)\right)\right|\)</span>
<span class="w">        </span><span class="n">periodically</span><span class="w"> </span><span class="n">update</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">\(D_{\Phi}^{+}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">D_</span><span class="p">{</span><span class="n">\Phi}\)</span>
</code></pre></div>

<h2>B. Experimental Setup</h2>
<p>We use $64 \times 64$ gray-scale images for all domains.</p>
<h2>B.1. Maze Domain</h2>
<p>We collect data samples in parallel from 20 random policy, for a total of 1000 rollouts ( 50 each). Each rollout is 10 steps. We train the local metric function for 40 epochs, at a learning rate of $10^{-4}$ using the Adam optimizer [21]. To construct training pairs, we set the sample ration to $1: 1: 2$ for the label 'identical', 'neighbor', and 'far-away'. It is know in the contrastive learning literature that</p>
<p>increasing the ratio of negative examples improves learning [49]. With ground-truth data, we are able to verify the quality of the local metric function by directly visualizing the off-trajectory neighbors it finds. The accuracy is evaluated over a 10 -fold validation set. We latter found that using a negative hinge loss for the third category improves performance, but all results reported here are carried using smoothed $\ell^{1}$ loss.</p>
<p>To learn the global metric, during each value iteration, we collect a batch of 20 parallel planning trajectories, 20 steps each. We then run 6 optimization epochs with a batch size of 32 samples per mini-batch. We found this parameter setting perform well.</p>
<h1>B.2. Rope Domain</h1>
<p>Details on the dataset is available in [50]. We consider images separated by $k=2$ or less as neighbors. This is a hyperparameter that can be adjusted depending on the data. We use a 10 -fold train/test split for evaluation. Due to the large size of the pair-wise dataset, we only train for 5 epochs, with a mini-batch size of 16 at learning rate of $10^{-4}$ using the Adam optimizer [21].</p>
<h2>B.3. StreetLearn</h2>
<p>We created four subsets from Street Learn that cover increasingly larger areas. We use the smaller three sets for evaluation, and show case the learned metric map with the largest one. The Street Learn dataset is very sparse in that views are around 10 meters apart. For this reason generalization from the local metric is not as critical as for the maze domain, as the majority of the edges come from the sampled transitions.</p>
<p>We calculate the ground distance using the latitude/longitude coordinates multiplied with a Mercator correction factor on the latitude ( $\approx 1.74$ ). We scale the 1 -step ground distance that is used to construct the graph with a scaling factor. This step is critical because otherwise gradient masking occurs due to the finite precision at those small values. It additionally prevents the mismatch between the initial parameterization of the network and the distribution of the distance targets.</p>
<p>Plan2vec's supervised objective makes learning the complex street topology directly from camera input very computationally efficient. Full convergence on the largest dataset takes just under 2 hours on a single V100 GPU. On the small dataset, the entire training takes 9 minutes.</p>
<p>Table 3: Details of Street Learn Subsets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Tiny</th>
<th style="text-align: center;">Small</th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;">Large</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Views</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">255</td>
<td style="text-align: center;">501</td>
<td style="text-align: center;">1495</td>
</tr>
<tr>
<td style="text-align: center;">Map Area</td>
<td style="text-align: center;">40.72891,</td>
<td style="text-align: center;">40.72731,</td>
<td style="text-align: center;">40.72690,</td>
<td style="text-align: center;">40.72601,</td>
</tr>
<tr>
<td style="text-align: center;">lat, long,</td>
<td style="text-align: center;">-73.99694,</td>
<td style="text-align: center;">-73.99698,</td>
<td style="text-align: center;">-73.99798,</td>
<td style="text-align: center;">-73.99700,</td>
</tr>
<tr>
<td style="text-align: center;">height, width</td>
<td style="text-align: center;">0.00143 ,</td>
<td style="text-align: center;">0.00349 ,</td>
<td style="text-align: center;">0.00475 ,</td>
<td style="text-align: center;">0.00799 ,</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00191</td>
<td style="text-align: center;">0.00397</td>
<td style="text-align: center;">0.00648</td>
<td style="text-align: center;">0.01000</td>
</tr>
<tr>
<td style="text-align: center;">Map Area</td>
<td style="text-align: center;">$0.025 \mathrm{~km}^{2}$</td>
<td style="text-align: center;">$0.4 \mathrm{~km}^{2}$</td>
<td style="text-align: center;">$0.64 \mathrm{~km}^{2}$</td>
<td style="text-align: center;">$1.6 \mathrm{~km}^{2}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Street Learn Hyper Parameters</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Tiny</th>
<th style="text-align: center;">Small</th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;">Large</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Scaling</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">700</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">4000</td>
</tr>
<tr>
<td style="text-align: center;">Num Epochs</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$2 k$</td>
<td style="text-align: center;">$5 k$</td>
<td style="text-align: center;">$10 k$</td>
</tr>
<tr>
<td style="text-align: center;">Batch Size</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">$1^{-4}$</td>
<td style="text-align: center;">$3^{-4}$</td>
<td style="text-align: center;">$1^{-5}$</td>
<td style="text-align: center;">$1^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Metric $\ell^{p}$</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<h1>C. Additional Results with Maze</h1>
<p>Fig. 10a shows the distribution of the score against ground-truth distance. In shorter ranges, the learned model is able to recover the local metric. But it saturates as the distance increases. We found that aliasing goes down as we increase the dimensionality of the latent space. One can think of this as the network initially emulating a Gaussian random projection, a form of content locality sensitive hashing (LSH, see [9]). The score is well-behaved and it is easy to pick suitable values for the neighbor threshold (indicated by the ceiling of the red points). We plot new transitions found by the local metric against those in the dataset (blue). Fig. 10 b visualizes the sampled trajectories (in blue, of length 4), whereas Fig. 10 c shows the new ones found by the learned local metric function.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: (a) Local metric score in comparison to ground-truth $L_{2}$ distance with predicted neighbors in red. We thin the ensemble by 200 and 40 times to reduce cluttering. (b) Trajectories given in the dataset. (c) Points from different trajectories are connected by generalizing the local-metric function. Out-of-training-set Connections shown in red. (d) Step sequence in C-Maze, learned via Plan2Vec. Gray dashed circle is the goal position. Red dot is the planned next step (1-step), greedy w.r.t the global metric function being learned. Blue dots are the neighbors sampled using the local metric function. Gray dot indicates the current and past positions of the agent. Sequence shows the agent getting around the wall in C-Maze. (e) Learned value function for a goal location on the bottom left corner (white dashed circle). Blue color is further away, red is close.</p>
<h2>D. Additional Results with Rope</h2>
<p>We show examples of positive and negative pairs for training the local metric in Fig. 11. Fig. 12 shows randomly selected images from the dataset versus their top neighbors according to $d$. Fig. 13 shows a particular trajectory from the rope dataset, versus a plan found by Dijkstra's shortest-path search algorithm using a local metric $d$, and one found by plan2vec after training.</p>
<p>Figure 11: Examples of rope pairs that are connected (positive, left), and not connected (negative, right).</p>
<p>Neighbors Found by Local Metric
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Neighbors from the rope dataset. Left-most column are the image used to query for its neighbors in each row. Number on top are the local-metric scores; red color indicates the negative examples that is above the cut-off threshold of 1.4. Number on the bottom shows the index of the image.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Examples of trajectories using $o_{s}$ and $o_{g}$ randomly sampled from a single trajectory. (Top) Original Trajectory, but showing only every fourth frame, (Middle) Learned representation + Dijkstra, (Bottom) Plan2Vec. Numbers in top left corners denote ground truth trajectory and index of each image, numbers in bottom left are local metric values. These planned trajectories are much longer horizon than previously possible with [24].</p>
<h1>E. Additional Results on Street Learn</h1>
<p>Bellow we show additional results on generalization. In a larger map area, removal of diagonal bundles of task configurations during training results in under estimation of the distances in between
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 14: When large bundles of tasks are missing during training, plan2vec underestimates the distance in-between (Pink task pairs and red markers in (c) and (d)). This result shows the importance in using long-horizon plans as learning targets when learning distances. Control in (f) learns from all task-configurations.</p>
<p>(see Fig. 14d). This ablation study shows that to learn metric information between observations that are far apart, long-horizon plans between those areas have to be involved during training. In Q-learning, this is accomplished by iterative value-bootstrapping, which is a much slower.</p>
<h1>E.1. 3-dimensional Latent Space</h1>
<p>We include additional visualization of the street map on a 3-dimensional latent space.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 15: Additional visualization of learned embeddings in a 3-dimensional latent space. (a,b) Manhattan-large; (c-g) Manhattan-medium. $p=1.2$, showing results from all random seeds. Not cherry-picked.</p>
<h2>F. Additional Results on Local Metric $d$</h2>
<p>In this section we list the training and test accuracy of the local metric for all domains. Plan2vec is model-based, and is sensitive to "worm-hole" connections that are misidentified by the local metric. For this reason, improvements to the local metric and the graph will translate into improvements of the metric $D$.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: center;">Accuracy $( \pm 25 \%)$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Domain</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: right;">Open</td>
<td style="text-align: center;">$98.6 \% \pm 0.7$</td>
<td style="text-align: center;">$97.8 \% \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: right;">Table</td>
<td style="text-align: center;">$98.3 \% \pm 1.1$</td>
<td style="text-align: center;">$97.6 \% \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: right;">Wall</td>
<td style="text-align: center;">$98.3 \% \pm 1.2$</td>
<td style="text-align: center;">$97.4 \% \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: right;">Rope</td>
<td style="text-align: center;">$96.6 \% \pm 0.9$</td>
<td style="text-align: center;">$92.4 \% \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: right;">Street Learn</td>
<td style="text-align: center;">$99.2 \% \pm 0.5$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 5: Prediction accuracy for 1-step neighbors on all domains when threshold is set to 1.5 , right in-between 1 and 2. The local metric function can detect neighbors consistently. Results are averaged over 5 seeds. On Street Learn we use all samples from Manhattan-large during training due to the sparsity of the view points in comparison to the large map size.</p>
<h1>G. Architectural Details</h1>
<p>Plan2vec is model-agnostic and can work with a variety of different architectures. We list the details of the network used during our experiments below in the form of pseudocode.</p>
<p>Maze Local Metric is a five-layer convolution network. We stack the two input images channel wise.</p>
<div class="codehilite"><pre><span></span><code>LocalMetricConvLarge(
(trunk): Sequential(
    (0): Conv2d(2, 32, kernel_size=(4, 4), stride=(2, 2))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU()
    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (5): ReLU()
    (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (8): ReLU()
    (9): Conv2d(64, 32, kernel_size=(4, 4), stride=(2, 2))
    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    (11): ReLU()
    (12): View(-1, 128)
    (13): Linear(in_features=128, out_features=128, bias=True)
    (14): ReLU()
    (15): Linear(in_features=128, out_features=100, bias=True)
    (16): ReLU()
    (17): Linear(in_features=100, out_features=1, bias=True)
)
)
</code></pre></div>

<p>Maze Global Metric We increase the capacity of the network for the global metric, and adopt a Siamese architecture with an $\ell^{2}$-metric head.</p>
<div class="codehilite"><pre><span></span><code>GlobalMetricConvL2(
    (embed): Sequential(
        (0): Conv2d(1, 128, kernel_size=(7, 7), stride=(1, 1))
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU()
        (3): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1))
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (5): ReLU()
        (6): Conv2d(256, 256, kernel_size=(7, 7), stride=(2, 2))
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (8): ReLU()
        (9): Conv2d(256, 256, kernel_size=(7, 7), stride=(2, 2))
        (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (11): ReLU()
        (12): Conv2d(256, 256, kernel_size=(7, 7), stride=(2, 2))
        (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (14): ReLU()
        (15): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))
        (16): ReLU()
        (17): View(-1, *(256,))
        (18): Linear(in_features=256, out_features=2, bias=True)
    )
    (head): Lambda(a, b) =&gt; norm(a - b, p=2)
)
</code></pre></div>

<h1>Plan2VEC: Unsupervised Representation Learning by Latent Plans</h1>
<p>Rope and Street Learn uses the same local metric function. We stack two gray-scale images together into a 2-channel image for the local metric.</p>
<div class="codehilite"><pre><span></span><code>LocalMetric(
(trunk): Sequential(
(0): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2))
(1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
(2): ReLU()
(3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2))
(4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
(5): ReLU()
(6): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))
(7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
(8): ReLU()
(9): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))
(10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
(11): ReLU()
(12): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))
(13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
(14): ReLU()
(15): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))
(16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
(17): ReLU()
(18): View(-1, *(512,))
(19): Linear(in_features=512, out_features=128, bias=True)
(20): ReLU()
(21): Linear(in_features=128, out_features=100, bias=True)
(22): ReLU()
(23): Linear(in_features=100, out_features=1, bias=True)
)
)
</code></pre></div>

<p>To learn the global metric $D$, we use a ResNet18 trunk [17] with an $\ell^{p}$ metric head. The network is instantiated with the following pseudo code:</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> We use the ResNet18 from torchvision.
ResNet18L2(
    (embed): Sequential(
        (resnet_18): ResNet18([2, 2, 2, 2])
        (conv_1): Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    )
    (head): Lambda(a, b) =&gt; norm(a - b, p)
)
</code></pre></div>            </div>
        </div>

    </div>
</body>
</html>
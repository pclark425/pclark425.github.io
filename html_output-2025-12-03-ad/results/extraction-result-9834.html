<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9834 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9834</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9834</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-651e5bcc14f14605a879303e97572a27ea8c7956</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/651e5bcc14f14605a879303e97572a27ea8c7956" target="_blank">A Diversity-Promoting Objective Function for Neural Conversation Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes using Maximum Mutual Information (MMI) as the objective function in neural models, and demonstrates that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.</p>
                <p><strong>Paper Abstract:</strong> Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., "I don't know") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9834",
    "paper_id": "paper-651e5bcc14f14605a879303e97572a27ea8c7956",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004219499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Diversity-Promoting Objective Function for Neural Conversation Models</h1>
<p>Jiwei Li ${ }^{1 *}$ Michel Galley ${ }^{2}$ Chris Brockett ${ }^{2}$ Jianfeng Gao ${ }^{2}$ Bill Dolan ${ }^{2}$<br>${ }^{1}$ Stanford University, Stanford, CA, USA<br>jiweil@stanford.edu<br>${ }^{2}$ Microsoft Research, Redmond, WA, USA<br>{mgalley, chrisbkt,jfgao,billdol}@microsoft.com</p>
<h4>Abstract</h4>
<p>Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don't know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in Bleu scores on two conversational datasets and in human evaluations.</p>
<h2>1 Introduction</h2>
<p>Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (SEQ2SEQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). SEQ2SEQ models offer the promise of scalability and language-independence, together with the capacity</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011).</p>
<p>An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, however, neural conversation models tend to generate trivial or non-committal responses, often involving high-frequency phrases along the lines of I don't know or I'm OK (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015). Table 1 illustrates this phenomenon, showing top outputs from SEQ2SEQ models. All the top-ranked responses are generic. Responses that seem more meaningful or specific can also be found in the N-best lists, but rank much lower. In part at least, this behavior can be ascribed to the relative frequency of generic responses like I don't know in conversational datasets, in contrast with the relative sparsity of more contentful alternative responses. ${ }^{1}$ It appears that by optimizing for the likelihood of outputs given inputs, neural models assign high probability to "safe" responses. This objective function, common in related tasks such as machine translation, may be unsuited to generation tasks involving intrinsically diverse outputs. Intuitively, it seems desirable to take into account not only the dependency of responses on messages, but also the inverse, the likelihood that a message will be provided to a given response.</p>
<p>We propose to capture this intuition by using Maximum Mutual Information (MMI), first introduced in</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input: What are you doing?</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">-0.86 I don't know.</td>
<td style="text-align: left;">-1.09</td>
<td style="text-align: left;">Get out of here.</td>
</tr>
<tr>
<td style="text-align: left;">-1.03 I don't know!</td>
<td style="text-align: left;">-1.09</td>
<td style="text-align: left;">I'm going home.</td>
</tr>
<tr>
<td style="text-align: left;">-1.06 Nothing.</td>
<td style="text-align: left;">-1.09</td>
<td style="text-align: left;">Oh my god!</td>
</tr>
<tr>
<td style="text-align: left;">-1.09 Get out of the way.</td>
<td style="text-align: left;">-1.10</td>
<td style="text-align: left;">I'm talking to you.</td>
</tr>
<tr>
<td style="text-align: left;">Input: what is your name?</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">-0.91 I don't know.</td>
<td style="text-align: left;">$\ldots$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">-0.92 I don't know!</td>
<td style="text-align: left;">-1.55</td>
<td style="text-align: left;">My name is Robert.</td>
</tr>
<tr>
<td style="text-align: left;">-0.92 I don't know, sir.</td>
<td style="text-align: left;">-1.58</td>
<td style="text-align: left;">My name is John.</td>
</tr>
<tr>
<td style="text-align: left;">-0.97 Oh, my god!</td>
<td style="text-align: left;">-1.59</td>
<td style="text-align: left;">My name's John.</td>
</tr>
<tr>
<td style="text-align: left;">Input: How old are you?</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">-0.79 I don't know.</td>
<td style="text-align: left;">$\ldots$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">-1.06 I'm fine.</td>
<td style="text-align: left;">-1.64</td>
<td style="text-align: left;">Twenty-five.</td>
</tr>
<tr>
<td style="text-align: left;">-1.17 I'm all right.</td>
<td style="text-align: left;">-1.66</td>
<td style="text-align: left;">Five.</td>
</tr>
<tr>
<td style="text-align: left;">-1.17 I'm not sure.</td>
<td style="text-align: left;">-1.71</td>
<td style="text-align: left;">Eight.</td>
</tr>
</tbody>
</table>
<p>Table 1: Responses generated by a 4-layer SEQ2SEQ neural model trained on 20 million conversation pairs take from the OpenSubtitles dataset. Decoding is implemented with beam size set to 200. The top examples are the responses with the highest average probability loglikelihoods in the N-best list. Lower-ranked, less-generic responses were manually chosen.
speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs. Below, we present practical strategies for neural generation models that use MMI as an objective function. We show that use of MMI results in a clear decrease in the proportion of generic response sequences, generating correspondingly more varied and interesting outputs.</p>
<h2>2 Related work</h2>
<p>The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014).</p>
<p>We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et
al., 2015; Luong et al., 2015) has inspired attempts to extend these neural techniques to response generation. Sordoni et al. (2015) improved upon Ritter et al. (2011) by rescoring the output of a phrasal SMT-based conversation system with a SEQ2SEQ model that incorporates prior context. (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015) apply direct end-to-end SEQ2SEQ models These SEQ2SEQ models are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that can implicitly capture compositionality and long-span dependencies. (Wen et al., 2015) attempt to learn response templates from crowd-sourced data, whereas we seek to develop methods that can learn conversational patterns from naturally-occurring data.</p>
<p>Prior work in generation has sought to increase diversity, but with different goals and techniques. Carbonell and Goldstein (1998) and Gimpel (2013) produce multiple outputs that are mutually diverse, either non-redundant summary sentences or N-best lists. Our goal, however, is to produce a single nontrivial output, and our method does not require identifying lexical overlap to foster diversity. ${ }^{2}$</p>
<p>On a somewhat different task, Mao et al. (2015, Section 6) utilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood.</p>
<h2>3 Sequence-to-Sequence Models</h2>
<p>Given a sequence of inputs $X=\left{x_{1}, x_{2}, \ldots, x_{N_{x}}\right}$, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as $i_{k}, f_{k}$ and $o_{k}$. We distinguish $e$ and $h$ where $e_{k}$ denotes the vector for an individual text unit (for example, a word or sentence) at time step $k$ while $h_{k}$ denotes the vector computed by LSTM model at time $k$ by combining $e_{k}$ and $h_{k-1} . c_{k}$ is the cell state vector at time $k$, and $\sigma$ denotes the sigmoid function. Then, the vector representation $h_{k}$ for each time step</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$k$ is given by:</p>
<p>$$
\begin{gathered}
i_{k}=\sigma\left(W_{i} \cdot\left[h_{k-1}, e_{k}\right]\right) \
f_{k}=\sigma\left(W_{f} \cdot\left[h_{k-1}, e_{k}\right]\right) \
o_{k}=\sigma\left(W_{o} \cdot\left[h_{k-1}, e_{k}\right]\right) \
l_{k}=\tanh \left(W_{l} \cdot\left[h_{k-1}, e_{k}\right]\right) \
c_{k}=f_{k} \cdot c_{k-1}+i_{k} \cdot l_{k} \
h_{k}^{s}=o_{k} \cdot \tanh \left(c_{k}\right)
\end{gathered}
$$</p>
<p>where $W_{i}, W_{f}, W_{o}, W_{l} \in \mathbb{R}^{D \times 2 D}$. In SEQ2SEQ generation tasks, each input $X$ is paired with a sequence of outputs to predict: $Y=\left{y_{1}, y_{2}, \ldots, y_{N_{y}}\right}$. The LSTM defines a distribution over outputs and sequentially predicts tokens using a softmax function:</p>
<p>$$
\begin{aligned}
p(Y \mid X) &amp; =\prod_{k=1}^{N_{y}} p\left(y_{k} \mid x_{1}, x_{2}, \ldots, x_{t}, y_{1}, y_{2}, \ldots, y_{k-1}\right) \
&amp; =\prod_{k=1}^{N_{y}} \frac{\exp \left(f\left(h_{k-1}, e_{y_{k}}\right)\right)}{\sum_{y^{\prime}} \exp \left(f\left(h_{k-1}, e_{y^{\prime}}\right)\right)}
\end{aligned}
$$</p>
<p>where $f\left(h_{k-1}, e_{y_{k}}\right)$ denotes the activation function between $h_{k-1}$ and $e_{y_{k}}$, where $h_{k-1}$ is the representation output from the LSTM at time $k-1$. Each sentence concludes with a special end-of-sentence symbol EOS. Commonly, input and output use different LSTMs with separate compositional parameters to capture different compositional patterns.</p>
<p>During decoding, the algorithm terminates when an $E O S$ token is predicted. At each time step, either a greedy approach or beam search can be adopted for word prediction. Greedy search selects the token with the largest conditional probability, the embedding of which is then combined with preceding output to predict the token at the next step.</p>
<h2>4 MMI Models</h2>
<h3>4.1 Notation</h3>
<p>In the response generation task, let $S$ denote an input message sequence (source) $S=\left{s_{1}, s_{2}, \ldots, s_{N_{s}}\right}$ where $N_{s}$ denotes the number of words in $S$. Let $T$ (target) denote a sequence in response to source sequence $S$, where $T=\left{t_{1}, t_{2}, \ldots, t_{N_{t}}, E O S\right}, N_{t}$ is the length of the response (terminated by an EOS token) and $t$ denotes a word token that is associated with a $D$ dimensional distinct word embedding $e_{t} . V$ denotes vocabulary size.</p>
<h3>4.2 MMI Criterion</h3>
<p>The standard objective function for sequence-tosequence models is the log-likelihood of target $T$ given source $S$, which at test time yields the statistical decision problem:</p>
<p>$$
\hat{T}=\underset{T}{\arg \max }{\log p(T \mid S)}
$$</p>
<p>As discussed in the introduction, we surmise that this formulation leads to generic responses being generated, since it only selects for targets given sources, not the converse. To remedy this, we replace it with Maximum Mutual Information (MMI) as the objective function. In MMI, parameters are chosen to maximize (pairwise) mutual information between the source $S$ and the target $T$ :</p>
<p>$$
\log \frac{p(S, T)}{p(S) p(T)}
$$</p>
<p>This avoids favoring responses that unconditionally enjoy high probability, and instead biases towards those responses that are specific to the given input. The MMI objective can written as follows: ${ }^{3}$</p>
<p>$$
\hat{T}=\underset{T}{\arg \max }{\log p(T \mid S)-\log p(T)}
$$</p>
<p>We use a generalization of the MMI objective which introduces a hyperparameter $\lambda$ that controls how much to penalize generic responses:</p>
<p>$$
\hat{T}=\underset{T}{\arg \max }{\log p(T \mid S)-\lambda \log p(T)}
$$</p>
<p>An alternate formulation of the MMI objective uses Bayes' theorem:</p>
<p>$$
\log p(T)=\log p(T \mid S)+\log p(S)-\log p(S \mid T)
$$</p>
<p>which lets us rewrite Equation 9 as follows:</p>
<p>$$
\begin{aligned}
\hat{T}= &amp; \underset{T}{\arg \max }{(1-\lambda) \log p(T \mid S) \
&amp; \quad+\lambda \log p(S \mid T)-\lambda \log p(S)} \
= &amp; \underset{T}{\arg \max }{(1-\lambda) \log p(T \mid S)+\lambda \log p(S \mid T)}
\end{aligned}
$$</p>
<p>This weighted MMI objective function can thus be viewed as representing a tradeoff between sources</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>given targets (i.e., $p(S \mid T)$ ) and targets given sources (i.e., $p(T \mid S)$ ).</p>
<p>Although the MMI optimization criterion has been comprehensively studied for other tasks, such as acoustic modeling in speech recognition (Huang et al., 2001), adapting MMI to SEQ2SEQ training is empirically nontrivial. Moreover, we would like to be able to adjust the value $\lambda$ in Equation 9 without repeatedly training neural network models from scratch, which would otherwise be extremely timeconsuming. Accordingly, we did not train a joint model $(\log p(T \mid S)-\lambda \log p(T))$, but instead trained maximum likelihood models, and used the MMI criterion only during testing.</p>
<h3>4.3 Practical Considerations</h3>
<p>Responses can be generated either from Equation 9, i.e., $\log p(T \mid S)-\lambda \log p(T)$ or Equation 10, i.e., $(1-\lambda) \log p(T \mid S)+\lambda \log p(S \mid T)$. We will refer to these formulations as MMI-antiLM and MMI-bidi, respectively. However, these strategies are difficult to apply directly to decoding since they can lead to ungrammatical responses (with MMI-antiLM) or make decoding intractable (with MMI-bidi). In the rest of this section, we will discuss these issues and explain how we resolve them in practice.</p>
<h3>4.3.1 MMI-antiLM</h3>
<p>The second term of $\log p(T \mid S)-\lambda \log p(T)$ functions as an anti-language model. It penalizes not only high-frequency, generic responses, but also fluent ones and thus can lead to ungrammatical outputs. In theory, this issue should not arise when $\lambda$ is less than 1 , since ungrammatical sentences should always be more severely penalized by the first term of the equation, i.e., $\log p(T \mid S)$. In practice, however, we found that the model tends to select ungrammatical outputs that escaped being penalized by $p(T \mid S)$.</p>
<p>Solution Again, let $N_{t}$ be the length of target $T$. $p(T)$ in Equation 9 can be written as:</p>
<p>$$
p(T)=\prod_{k=1}^{N_{t}} p\left(t_{k} \mid t_{1}, t_{2}, \ldots, t_{k-1}\right)
$$</p>
<p>We replace the language model $p(T)$ with $U(T)$, which adapts the standard language model by multiplying by a weight $g(k)$ that is decremented mono-
tonically as the index of the current token $k$ increases:</p>
<p>$$
U(T)=\prod_{i=1}^{N_{t}} p\left(t_{k} \mid t_{1}, t_{2}, \ldots, t_{k-1}\right) \cdot g(k)
$$</p>
<p>The underlying intuition here is as follows. First, neural decoding combines the previously built representation with the word predicted at the current step. As decoding proceeds, the influence of the initial input on decoding (i.e., the source sentence representation) diminishes as additional previously-predicted words are encoded in the vector representations. ${ }^{4}$ In other words, the first words to be predicted significantly determine the remainder of the sentence. Penalizing words predicted early on by the language model contributes more to the diversity of the sentence than it does to words predicted later. Second, as the influence of the input on decoding declines, the influence of the language model comes to dominate. We have observed that ungrammatical segments tend to appear in the later parts of the sentences, especially in long sentences.</p>
<p>We adopt the most straightforward form of $g(k)$ by setting up a threshold $(\gamma)$ by penalizing the first $\gamma$ words where ${ }^{5}$</p>
<p>$$
g(k)= \begin{cases}1 &amp; \text { if } k \leq \gamma \ 0 &amp; \text { if } k&gt;\gamma\end{cases}
$$</p>
<p>The objective in Equation 9 can thus be rewritten as:</p>
<p>$$
\log p(T \mid S)-\lambda \log U(T)
$$</p>
<p>where direct decoding is tractable.</p>
<h3>4.3.2 MMI-bidi</h3>
<p>Direct decoding from $(1-\lambda) \log p(T \mid S)+$ $\lambda \log p(S \mid T)$ is intractable, as the second part (i.e., $p(S \mid T))$ requires completion of target generation before $p(S \mid T)$ can be effectively computed. Due to the enormous search space for target $T$, exploring all possibilities is infeasible.</p>
<p>For practical reasons, then, we turn to an approximation approach that involves first generating N-best lists given the first part of objective function, i.e.,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>standard SEQ2SEQ model $p(T \mid S)$. Then we rerank the N-best lists using the second term of the objective function. Since N-best lists produced by SEQ2SEQ models are generally grammatical, the final selected options are likely to be well-formed. Model reranking has obvious drawbacks. It results in non-globally-optimal solutions by first emphasizing standard SEQ2SEQ objectives. Moreover, it relies heavily on the system's success in generating a sufficiently diverse N-best set, requiring that a long list of N-best lists be generated for each message.</p>
<p>Nonetheless, these two variants of the MMI criterion work well in practice, significantly improving both interestingness and diversity.</p>
<h3>4.4 Training</h3>
<p>Recent research has shown that deep LSTMs work better than single-layer LSTMs for SEQ2SEQ tasks (Vinyals et al., 2015; Sutskever et al., 2014). We adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decoding, each of which consists of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000. Other training details are given below, broadly aligned with Sutskever et al. (2014).</p>
<ul>
<li>LSTM parameters and embeddings are initialized from a uniform distribution in $[-0.08$, $0.08]$.</li>
<li>Stochastic gradient decent is implemented using a fixed learning rate of 0.1 .</li>
<li>Batch size is set to 256.</li>
<li>Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 1 .
Our implementation on a single GPU processes at a speed of approximately 600-1200 tokens per second on a Tesla K40.</li>
</ul>
<p>The $p(S \mid T)$ model described in Section 4.3.1 was trained using the same model as that of $p(T \mid S)$, with messages $(S)$ and responses $(T)$ interchanged.</p>
<h3>4.5 Decoding</h3>
<h3>4.5.1 MMI-antiLM</h3>
<p>As described in Section 4.3.1, decoding using $\log p(T \mid S)-\lambda U(T)$ can be readily implemented by predicting tokens at each time-step. In addition, we found in our experiments that it is also important to take into account the length of responses in decod-
ing. We thus linearly combine the loss function with length penalization, leading to an ultimate score for a given target $T$ as follows:</p>
<p>$$
\operatorname{Score}(T)=p(T \mid S)-\lambda U(T)+\gamma N_{t}
$$</p>
<p>where $N_{t}$ denotes the length of the target and $\gamma$ denotes associated weight. We optimize $\gamma$ and $\lambda$ using MERT (Och, 2003) on N-best lists of response candidates. The N-best lists are generated using the decoder with beam size $B=200$. We set a maximum length of 20 for generated candidates. At each time step of decoding, we are presented with $B \times B$ candidates. We first add all hypotheses with an EOS token being generated at current time step to the N-best list. Next we preserve the top $B$ unfinished hypotheses and move to next time step. We therefore maintain beam size of 200 constant when some hypotheses are completed and taken down by adding in more unfinished hypotheses. This will lead the size of final N-best list for each input much larger than the beam size.</p>
<h3>4.5.2 MMI-bidi</h3>
<p>We generate N-best lists based on $P(T \mid S)$ and then rerank the list by linearly combining $p(T \mid S)$, $\lambda p(S \mid T)$, and $\gamma N_{t}$. We use MERT to tune the weights $\lambda$ and $\gamma$ on the development set. ${ }^{6}$</p>
<h2>5 Experiments</h2>
<h3>5.1 Datasets</h3>
<p>Twitter Conversation Triple Dataset We used an extension of the dataset described in Sordoni et al. (2015), which consists of 23 million conversational snippets randomly selected from a collection of 129 M context-message-response triples extracted from the Twitter Firehose over the 3-month period from June through August 2012. For the purposes of our experiments, we limited context to the turn in the conversation immediately preceding the message. In our LSTM models, we used a simple input model in which contexts and messages are concatenated to form the source input.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;"># of training instances</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">distinct-1</th>
<th style="text-align: center;">distinct-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SEQ2SEQ (baseline)</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">4.31</td>
<td style="text-align: center;">.023</td>
<td style="text-align: center;">.107</td>
</tr>
<tr>
<td style="text-align: left;">SEQ2SEQ (greedy)</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">4.51</td>
<td style="text-align: center;">.032</td>
<td style="text-align: center;">.148</td>
</tr>
<tr>
<td style="text-align: left;">MMI-antiLM: $\log p(T \mid S)-\lambda U(T)$</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">.033</td>
<td style="text-align: center;">.175</td>
</tr>
<tr>
<td style="text-align: left;">MMI-bidi: $(1-\lambda) \log p(T \mid S)+\lambda \log p(S \mid T)$</td>
<td style="text-align: center;">23 M</td>
<td style="text-align: center;">$\mathbf{5 . 2 2}$</td>
<td style="text-align: center;">.051</td>
<td style="text-align: center;">.270</td>
</tr>
<tr>
<td style="text-align: left;">SMT (Ritter et al., 2011)</td>
<td style="text-align: center;">50 M</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">.098</td>
<td style="text-align: center;">.351</td>
</tr>
<tr>
<td style="text-align: left;">SMT+neural reranking (Sordoni et al., 2015)</td>
<td style="text-align: center;">50 M</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">$\mathbf{. 1 0 1}$</td>
<td style="text-align: center;">$\mathbf{. 3 5 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on the Twitter dataset of 4-layer SEQ2SEQ models and MMI models. distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words.</p>
<p>For tuning and evaluation, we used the development dataset ( 2118 conversations) and the test dataset (2114 examples), augmented using information retrieval methods to create a multi-reference set, as described by Sordoni et al. (2015). The selection criteria for these two datasets included a component of relevance/interestingness, with the result that dull responses will tend to be penalized in evaluation.</p>
<p>OpenSubtitles dataset In addition to unscripted Twitter conversations, we also used the OpenSubtitles (OSDb) dataset (Tiedemann, 2009), a large, noisy, open-domain dataset containing roughly 60M70 M scripted lines spoken by movie characters. This dataset does not specify which character speaks each subtitle line, which prevents us from inferring speaker turns. Following Vinyals et al. (2015), we make the simplifying assumption that each line of subtitle constitutes a full speaker turn. Our models are trained to predict the current turn given the preceding ones based on the assumption that consecutive turns belong to the same conversation. This introduces a degree of noise, since consecutive lines may not appear in the same conversation or scene, and may not even be spoken by the same character.</p>
<p>This limitation potentially renders the OSDb dataset unreliable for evaluation purposes. For evaluation purposes, we therefore used data from the Internet Movie Script Database (IMSDB), ${ }^{7}$ which explicitly identifies which character speaks each line of the script. This allowed us to identify consecutive message-response pairs spoken by different characters. We randomly selected two subsets as development and test datasets, each containing 2 k pairs, with source and target length restricted to the range of $[6,18]$.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Performance of the SEQ2SEQ baseline and two MMI models on the OpenSubtitles dataset.</p>
<h3>5.2 Evaluation</h3>
<p>For parameter tuning and final evaluation, we used BleU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015). In the case of the Twitter models, we used multireference Bleu. As the IMSDB data is too limited to support extraction of multiple references, only single reference Bleu was used in training and evaluating the OSDb models.</p>
<p>We did not follow Vinyals et al. (2015) in using perplexity as evaluation metric. Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard SEQ2SEQ model in order to diversify the outputs. We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences (shown as distinct-1 and distinct-2 in Tables 2 and 3).</p>
<h3>5.3 Results</h3>
<p>Twitter Dataset We first report performance on Twitter datasets in Table 2, along with results for different models (i.e., Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset. The baseline is the SEQ2SEQ model with its standard likelihood objective and a beam size of 200. We compare this base-</p>
<p>line against greedy-search SEQ2SEQ (Vinyals and Le, 2015), which can help achieve higher diversity by increasing search errors. ${ }^{8}$</p>
<p>Machine Translation is the phrase-based MT system described in (Ritter et al., 2011). MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood "translation" probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015).
$M T+$ neural reranking is the phrase-based MT system, reranked using neural models. N-best lists are first generated from the MT system. Recurrent neural models generate scores for N-best list candidates given the input messages. These generated scores are re-incorporated to rerank all the candidates. Additional features to score [1-4]-gram matches between context and response and between message and context (context and message match CMM features) are also employed, as in Sordoni et al. (2015).
$M T+$ neural reranking achieves a Bleu score of 4.44 , which to the best of our knowledge represents the previous state-of-the-art performance on this Twitter dataset. Note that Machine Translation and $M T+$ neural reranking are trained on a much larger dataset of roughly 50 million examples. A significant performance boost is observed from MMIbidi over baseline SEQ2SEQ, both in terms of BleU score and diversity.</p>
<p>The beam size of 200 used in our main experiments is quite conservative, and Bleu scores only slightly degrade when reducing beam size to 20 . For MMIbidi, Bleu scores for beam sizes of 200, 50, 20 are respectively $5.90,5.86,5.76$. A beam size of 20 still produces relatively large N-best lists (173 elements on average) with responses of varying lengths, which offer enough diversity for the $p(S \mid T)$ model to have a significant effect.</p>
<p>OpenSubtitles Dataset All models achieve significantly lower Bleu scores on this dataset than on the Twitter dataset, primarily because the IMSDB data provides only single references for evaluation. We note, however, that baseline SEQ2SEQ models</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 6: MMI-bidi gains over comparator systems, based on pairwise human judgments.
yield lower levels of unigram diversity (distinct-1) on the OpenSubtitles dataset than on the Twitter data ( 0.0056 vs 0.017 ), which suggests that other factors may be in play. It is likely that movie dialogs are much more concise and information-rich than typical conversations on Twitter, making it harder to match gold-standard responses and causing the learned models to strongly favor safe, conservative responses.</p>
<p>Table 3 shows that the MMI-antiLM model yields a significant performance boost, with a Bleu score increase of up to $36 \%$ and a more than $200 \%$ jump in unigram diversity. Our interpretation of this huge performance improvement is that the diversity and complexity of input messages lead standard SEQ2SEQ models to generate very conservative responses, ${ }^{9}$ which fail to match the more interesting reference strings typical of this dataset. This interpretation is also supported by the fact that the MMI-bidi model does not produce as significant a performance boost as MMI-antiLM. In the case of MMI-bidi, N-best lists generated using standard SEQ2SEQ models remain conservative and uninteresting, attenuating the impact of later reranking. An important potential limitation of MMI-bidi model is thus that its performance hinges on the initial generation of a highly diverse, informative N-best list.</p>
<p>Qualitative Evaluation We employed crowdsourced judges to provide evaluations for a random sample of 1000 items in the Twitter test dataset. Table 6 shows the results of human evaluations between paired systems. Each output pair was ranked by 5 judges, who were asked to decide which of the two outputs was better. They were instructed to prefer outputs that were more specific (relevant) to the message and preceding context, as opposed to those that</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">message</th>
<th style="text-align: left;">SEQ2SEQ</th>
<th style="text-align: left;">MMI $P(T \mid S)-\lambda P(T)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I did not get the report from an mi-6 <br> agent.</td>
<td style="text-align: left;">I don't think that's a good idea.</td>
<td style="text-align: left;">You did the right thing, did you?</td>
</tr>
<tr>
<td style="text-align: left;">You haven't been given an assignment <br> in this case.</td>
<td style="text-align: left;">I don't know what you are talking about.</td>
<td style="text-align: left;">I've been looking all over for you.</td>
</tr>
<tr>
<td style="text-align: left;">I'm losing my grip.</td>
<td style="text-align: left;">I don't know what you are talking about.</td>
<td style="text-align: left;">I'm the only one in the world.</td>
</tr>
<tr>
<td style="text-align: left;">I am ready to help.</td>
<td style="text-align: left;">Come on, come on</td>
<td style="text-align: left;">I have something we need to talk about.</td>
</tr>
<tr>
<td style="text-align: left;">You programmed me to gather intelli- <br> gence. That's all I've ever done.</td>
<td style="text-align: left;">You know that, don't you?</td>
<td style="text-align: left;">You do have fun, don't you?</td>
</tr>
<tr>
<td style="text-align: left;">I mean, we'd have to talk to him.</td>
<td style="text-align: left;">I mean, I don't know.</td>
<td style="text-align: left;">I mean, he's a good guy.</td>
</tr>
<tr>
<td style="text-align: left;">This has nothing to do with him.</td>
<td style="text-align: left;">You don't know anything about him, do <br> you?</td>
<td style="text-align: left;">I know him as much.</td>
</tr>
<tr>
<td style="text-align: left;">How come you never say it?</td>
<td style="text-align: left;">I don't know</td>
<td style="text-align: left;">Because I don't want to hurt you</td>
</tr>
<tr>
<td style="text-align: left;">He is the one behind this scheme to crash <br> s-net.</td>
<td style="text-align: left;">I don't know what you are talking about</td>
<td style="text-align: left;">Yes, he is, isn't he?</td>
</tr>
<tr>
<td style="text-align: left;">How much time do you have here?</td>
<td style="text-align: left;">I don't know.</td>
<td style="text-align: left;">Not long enough. Sorry, sir.</td>
</tr>
</tbody>
</table>
<p>Table 4: Responses from the SEQ2SEQ baseline and MMI-antiLM models on the OpenSubtitles dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">message</th>
<th style="text-align: left;">SEQ2SEQ</th>
<th style="text-align: left;">MMI-bidi</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ohh I've never seen that! How long does <br> it take you guys to learn the drill?</td>
<td style="text-align: left;">I don't even know.</td>
<td style="text-align: left;">Like 2 weeks ago!!</td>
</tr>
<tr>
<td style="text-align: left;">You'll know when we sign Heskey in 2 <br> weeks</td>
<td style="text-align: left;">I'll see what I can do</td>
<td style="text-align: left;">I sure hope so..</td>
</tr>
<tr>
<td style="text-align: left;">I'm waiting!! I have even got biscuits <br> waiting for you??</td>
<td style="text-align: left;">I have no idea what you're talking about</td>
<td style="text-align: left;">I'll send you a pic when I get home xx</td>
</tr>
<tr>
<td style="text-align: left;">My parents actually had touch too much <br> played at their wedding.</td>
<td style="text-align: left;">I have no idea what you're talking about</td>
<td style="text-align: left;">How old are they??</td>
</tr>
<tr>
<td style="text-align: left;">Say anything you want... I know you <br> love me and I love you.</td>
<td style="text-align: left;">I love you so much.</td>
<td style="text-align: left;">I love you too, baby.</td>
</tr>
<tr>
<td style="text-align: left;">I am off all week next week</td>
<td style="text-align: left;">What time you going?</td>
<td style="text-align: left;">What time you going?</td>
</tr>
<tr>
<td style="text-align: left;">How are you doing?</td>
<td style="text-align: left;">I'm good, thanks.</td>
<td style="text-align: left;">I'm good, you?</td>
</tr>
</tbody>
</table>
<p>Table 5: Responses from the SEQ2SEQ baseline and MMI-bidi models on the Twitter dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input: What are you doing?</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1. I've been looking for you.</td>
<td style="text-align: left;">4. I told you to shut up.</td>
</tr>
<tr>
<td style="text-align: left;">2. I want to talk to you.</td>
<td style="text-align: left;">5. Get out of here.</td>
</tr>
<tr>
<td style="text-align: left;">3. Just making sure you're OK.</td>
<td style="text-align: left;">6. I'm looking for a doctor.</td>
</tr>
<tr>
<td style="text-align: left;">Input: What is your name?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1. Blue!</td>
<td style="text-align: left;">4. Daniel.</td>
</tr>
<tr>
<td style="text-align: left;">2. Peter.</td>
<td style="text-align: left;">5. My name is John.</td>
</tr>
<tr>
<td style="text-align: left;">3. Tyler.</td>
<td style="text-align: left;">6. My name is Robert.</td>
</tr>
<tr>
<td style="text-align: left;">Input: How old are you?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">1. Twenty-eight.</td>
<td style="text-align: left;">4. Five.</td>
</tr>
<tr>
<td style="text-align: left;">2. Twenty-four.</td>
<td style="text-align: left;">5. 15.</td>
</tr>
<tr>
<td style="text-align: left;">3. Long.</td>
<td style="text-align: left;">6. Eight.</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples generated by the MMI-antiLM model on the OpenSubtitles dataset.
were more generic. Ties were permitted. Identical strings were algorithmically assigned the same score. The mean of differences between outputs is shown as the gain for MMI-bidi over the competing system. At a significance level of $\alpha=0.05$, we find that MMIbidi outperforms both baseline and greedy SEQ2SEQ
systems, as well as the weaker SMT and SMT+RNN baselines. MMI-bidi outperforms SMT in human evaluations despite the greater lexical diversity of MT output.</p>
<p>Separately, judges were also asked to rate overall quality of MMI-bidi output over the same 1000-item sample in isolation, each output being evaluated by 7 judges in context using a 5-point scale. The mean rating was 3.84 (median: $3.85,1 \mathrm{st} \mathrm{Qu}: 3.57,3 \mathrm{rd} \mathrm{Qu}$ : 4.14), suggesting that overall MMI-bidi output does appear reasonably acceptable to human judges. ${ }^{10}$</p>
<p>Table 7 presents the N-best candidates generated using the MMI-bidi model for the inputs of Table 1.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We see that MMI generates significantly more interesting outputs than SEQ2SEQ.</p>
<p>In Tables 4 and 5, we present responses generated by different models. All examples were randomly sampled (without cherry picking). We see that the baseline SEQ2SEQ model tends to generate reasonable responses to simple messages such as How are you doing? or I love you. As the complexity of the message increases, however, the outputs switch to more conservative, duller forms, such as I don't know or I don't know what you are talking about. An occasional answer of this kind might go unnoticed in a natural conversation, but a dialog agent that always produces such responses risks being perceived as uncooperative. MMI-bidi models, on the other hand, produce far more diverse and interesting responses.</p>
<h2>6 Conclusions</h2>
<p>We investigated an issue encountered when applying SEQ2SEQ models to conversational response generation. These models tend to generate safe, commonplace responses (e.g., I don't know) regardless of the input. Our analysis suggests that the issue is at least in part attributable to the use of unidirectional likelihood of output (responses) given input (messages). To remedy this, we have proposed using Maximum Mutual Information (MMI) as the objective function. Our results demonstrate that the proposed MMI models produce more diverse and interesting responses, while improving quality as measured by Bleu and human evaluation.</p>
<p>To the best of our knowledge, this paper represents the first work to address the issue of output diversity in the neural generation framework. We have focused on the algorithmic dimensions of the problem. Unquestionably numerous other factors such as grounding, persona (of both user and agent), and intent also play a role in generating diverse, conversationally interesting outputs. These must be left for future investigation. Since the challenge of producing interesting outputs also arises in other neural generation tasks, including image-description generation, question answering, and potentially any task where mutual correspondences must be modeled, the implications of this work extend well beyond conversational response generation.</p>
<h2>Acknowledgments</h2>
<p>We thank the anonymous reviewers, as well as Dan Jurafsky, Alan Ritter, Stephanie Lukin, George Spithourakis, Alessandro Sordoni, Chris Quirk, Meg Mitchell, Jacob Devlin, Oriol Vinyals, and Dhruv Batra for their comments and suggestions.</p>
<h2>References</h2>
<p>David Ameixa, Luisa Coheur, Pedro Fialho, and Paulo Quaresma. 2014. Luke, I am your father: dealing with out-of-domain requests by using movies subtitles. In Intelligent Virtual Agents, pages 13-21. Springer.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. of the International Conference on Learning Representations (ICLR).
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1986. Maximum mutual information estimation of hidden Markov model parameters for speech recognition. Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP '86., pages 49-52.
Rafael E Banchs and Haizhou Li. 2012. IRIS: a chatoriented dialogue system based on the vector space model. In Proc. of the ACL 2012 System Demonstrations, pages 37-42.
Peter F. Brown. 1987. The Acoustic-modeling Problem in Automatic Speech Recognition. Ph.D. thesis, Carnegie Mellon University.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In In Research and Development in Information Retrieval, pages 335-336.
Yun-Nung Chen, Wei Yu Wang, and Alexander Rudnicky. 2013. An empirical investigation of sparse log-linear models for improved dialogue act classification. In Proc. of ICASSP, pages 8317-8321.
Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015. $\triangle$ Bleu: A discriminative metric for generation tasks with intrinsically diverse targets. In Proc. of ACL-IJCNLP, pages 445-450, Beijing, China, July.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proc. of ACL, pages 699-709.
Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory Shakhnarovich. 2013. A systematic exploration of diversity in machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100-1111.</p>
<p>Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):17351780.</p>
<p>Xuedong Huang, Alex Acero, Hsiao-Wuen Hon, and Raj Foreword By-Reddy. 2001. Spoken language processing: A guide to theory, algorithm, and system development. Prentice Hall.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics, pages 177-180, Prague, Czech Republic, June. Association for Computational Linguistics.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1):11-23.
Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proc. of ACL-IJCNLP, pages 11-19, Beijing, China.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille. 2015. Deep captioning with multimodal recurrent neural networks (m-RNN). ICLR.
Lasguido Nio, Sakriani Sakti, Graham Neubig, Tomoki Toda, Mirna Adriani, and Satoshi Nakamura. 2014. Developing non-goal dialog system based on examples of drama television. In Natural Interaction with Robots, Knowbots and Smartphones, pages 355-361. Springer.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160-167, Sapporo, Japan, July. Association for Computational Linguistics.
Alice H Oh and Alexander I Rudnicky. 2000. Stochastic language generation for spoken dialogue systems. In Proc. of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, pages 27-32. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL.
Roberto Pieraccini, David Suendermann, Krishna Dayanidhi, and Jackson Liscombe. 2009. Are we there yet? research in commercial spoken dialog systems. In Text, Speech and Dialogue, pages 3-13. Springer.
Adwait Ratnaparkhi. 2002. Trainable approaches to surface natural language generation and their application to conversational dialog systems. Computer Speech \&amp; Language, 16(3):435-455.</p>
<p>Alan Ritter, Colin Cherry, and William Dolan. 2011. Data-driven response generation in social media. In Proc. of EMNLP, pages 583-593.
Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2015. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808.
Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text conversation. In ACL-IJCNLP, pages 1577-1586.
Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. In Proc. of NAACL-HLT.
Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.
JÃ¶rg Tiedemann. 2009. News from OPUS - a collection of multilingual parallel corpora with tools and interfaces. In Recent advances in natural language processing, volume 5, pages 237-248.
Oriol Vinyals and Quoc Le. 2015. A neural conversational model. In Proc. of ICML Deep Learning Workshop.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar as a foreign language. In Proc. of NIPS.
Marilyn A Walker, Rashmi Prasad, and Amanda Stent. 2003. A trainable generator for recommendations in multimodal dialog. In INTERSPEECH.
William Yang Wang, Ron Artstein, Anton Leuski, and David Traum. 2011. Improving spoken dialogue understanding using phonetic mixture models. In FLAIRS.
Tsung-Hsien Wen, Milica Gasic, Nikola MrkÅ¡iÄ, Pei-Hao Su, David Vandyke, and Steve Young. 2015. Semantically conditioned LSTM-based natural language generation for spoken dialogue systems. In Proc. of EMNLP, pages 1711-1721, Lisbon, Portugal, September.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In David Blei and Francis Bach, editors, Proc. of the 32nd International Conference on Machine Learning (ICML15), pages 2048-2057. JMLR Workshop and Conference Proceedings.
Steve Young, Milica GaÅ¡iÄ, Simon Keizer, FranÃ§ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech \&amp; Language, 24(2):150-174.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ In the human evaluations, we asked the annotators to prefer responses that were more specific to the context only when doing the pairwise evaluations of systems. The absolute evaluation was conducted separately (on different days) on the best system, and annotators were asked to evaluate the overall quality of the response, specifically Provide your impression of overall quality of the response in this particular conversation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ The strings I don't know, I don't know what you are talking about, I don't think that is a good idea, and Oh my god constitute $32 \%$ percent of all generated responses.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
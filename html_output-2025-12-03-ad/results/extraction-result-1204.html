<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1204 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1204</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1204</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-227126471</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2011.11293v1.pdf" target="_blank">Evolutionary Planning in Latent Space</a></p>
                <p><strong>Paper Abstract:</strong> Planning is a powerful approach to reinforcement learning with several desirable properties. However, it requires a model of the world, which is not readily available in many real-life problems. In this paper, we propose to learn a world model that enables Evolutionary Planning in Latent Space (EPLS). We use a Variational Auto Encoder (VAE) to learn a compressed latent representation of individual observations and extend a Mixture Density Recurrent Neural Network (MDRNN) to learn a stochastic, multi-modal forward model of the world that can be used for planning. We use the Random Mutation Hill Climbing (RMHC) to find a sequence of actions that maximize expected reward in this learned model of the world. We demonstrate how to build a model of the world by bootstrapping it with rollouts from a random policy and iteratively refining it with rollouts from an increasingly accurate planning policy using the learned world model. After a few iterations of this refinement, our planning agents are better than standard model-free reinforcement learning approaches demonstrating the viability of our approach.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1204.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1204.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EPLS world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary Planning in Latent Space world model (ConvVAE + MDN-RNN extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned latent-space world model combining a convolutional VAE (64-dim latent) with an MDN-RNN (LSTM with mixture-of-Gaussians output) extended to also predict rewards and episode termination, used for online evolutionary planning (RMHC) in CarRacing-v0.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EPLS world model (ConvVAE + MDN-RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-time-step observations (64x64x3 images) are encoded by a convolutional VAE to a 64-dimensional latent z; an LSTM memory component (MDN-RNN, 512 hidden units) receives (z_t, a_t, h_{t-1}) and outputs a Gaussian-mixture model over next latent z_{t+1} (5 mixtures), a Gaussian prediction of reward (fixed variance=1) and a Bernoulli terminal probability. Planning is performed entirely in latent space by simulating trajectories with the MDN-RNN and evaluating action sequences via Random Mutation Hill Climbing (RMHC).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic recurrent latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control / racing (OpenAI Gym CarRacing-v0)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VAE reconstruction loss (pixel-wise MSE), MDN-RNN next-latent negative log-likelihood (GMM-NLL), MSE for reward prediction, binary cross-entropy for terminal prediction; overall MDN-RNN trained by minimizing -log p(z_t, r_t, τ_t | h_{t-1})</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric losses reported for VAE or MDN-RNN in the paper; fidelity is evaluated indirectly via task performance and qualitative reconstruction examples. The paper reports that a model trained on 'expert' rollouts yields substantially better task scores than one trained only on random rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Limited interpretability: latent space is 64-dimensional (learned, dense, not explicitly disentangled). Some interpretability via reconstructed frames from the VAE and simulated rollouts (planned trajectories) but inner dynamics remain a black-box neural model (LSTM+GMM).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructed frames from VAE and visualization/examples of planned trajectories; no formal disentanglement or probing methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model architecture: VAE with 4 conv + 4 deconv layers and 64-dim latent; MDN-RNN LSTM with 512 hidden units and 5 Gaussian mixture components. Training regimes: VAE trained on 10,000 rollouts for 50 epochs (Adam, lr=1e-4); MDN-RNN trained for 60 epochs (Adam, lr=1e-3) for non-iterative baseline; iterative updates used 500 rollouts per iteration and 10–60 epochs depending on experiment. No GPU count, wall-clock time, or parameter-count totals reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper claims learned world model + planning is more sample-efficient than model-free RL for sparse/requiring-sequential-reward tasks. Empirically, iterative world-model training with 500 rollouts per iteration improved task scores faster than retraining on 10k rollouts per iteration; no explicit FLOPs or runtime comparisons to baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported CarRacing-v0 mean scores (100 trials): Non-iterative random-model: 356 ± 177; Non-iterative expert-model (MDN-RNN trained on 5k random + 5k expert rollouts): 765 ± 102; Iterative model (5 iterations, 15 generations, horizon 20): 708 ± 195. These are compared to baselines: DQN 343 ± 18, A3C (continuous) 591 ± 45, World Model [6] 906 ± 21.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The learned world model enables effective online planning: iterative refinement of the world model by collecting rollouts with the planner improves task performance substantially over the random-trained model. High-quality training data (expert rollouts) yields higher model utility; however, even an iteratively improved model does not fully reach the best reported 'World Models' expert performance. Model predictive uncertainty limits useful planning horizon (horizon >20 gives diminishing or negative returns).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Key trade-offs documented: (1) Latent compression (64-dim) reduces computation and enables planning but limits long-horizon fidelity; planning too far ahead increases uncertainty so longer horizons degrade performance. (2) More evolutionary generations improve planning until local convergence (diminishing returns beyond ~15 generations). (3) Training on diverse/expert rollouts improves fidelity and task utility but requires access to better trajectories; iterative bootstrapping is a practical compromise. Interpretability is limited by using dense latent representations and black-box recurrent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choice of 64-dim latent (empirically better than 32), ConvVAE encoder/decoder with 4 conv/deconv layers, MDN-RNN (LSTM 512 units) with 5 Gaussian mixture components to capture stochastic multi-modality, explicit prediction heads for reward (Gaussian) and terminal (Bernoulli). Planning design uses RMHC in latent space with shift buffering, horizon default 20, generations typically 10 (15 found beneficial). Iterative training uses small per-iteration replay (500 rollouts) to balance sample-efficiency and training time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to model-free (DQN, A3C) the EPLS approach achieves higher mean scores than DQN and A3C in reported experiments (e.g., iterative model 708 > A3C 591). Compared to World Models [6], when trained with expert rollouts the baseline MDN-RNN achieves performance approaching World Models' reported behavior but still below the highest World Models score (906). Compared to PlaNet/Dreamer and MuZero, the paper notes PlaNet/Dreamer use more complex dynamics/planning/actors, and MuZero relies on large training data and GPU resources; EPLS favors a simpler MDN-RNN + evolutionary planner and emphasizes sample-efficiency via iterative bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Empirical recommendations from paper: use a 64-dim latent (better reconstructions than 32), MDN-RNN with 512 LSTM units and 5 mixtures, iterative training with modest-sized rollout batches (500 rollouts) yields fast improvements, planning horizon ≈20 and ~15 generations for RMHC are near-optimal (longer horizons or more generations give diminishing or negative returns due to model uncertainty and planning convergence). The paper advises balancing planning depth against model predictive uncertainty and improving training data coverage through iterative rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Planning in Latent Space', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1204.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1204.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Ha & Schmidhuber 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-space stochastic recurrent world model combining a VAE to compress pixel observations and an MDN-RNN to model latent dynamics; used to train policies inside the learned model rather than for online planning in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (VAE + MDN-RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VAE encoder/decoder maps frames to/from a low-dimensional latent; an MDN-RNN models latent transitions as a mixture-of-Gaussians conditional on actions; policy trained inside the simulated latent environment using the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic recurrent latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Video games / simulated environments (including CarRacing in original paper demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VAE reconstruction loss (pixel MSE) and MDN-RNN negative log-likelihood on next-latent; qualitative assessment via reconstructions and downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically reported in this paper; referenced as effective for training policies but original World Models did not learn environment reward in the same way (paper notes reward was not learned for planning in that work).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides some interpretability via decoded reconstructions from latent and can visualize agent's 'imagination' rollouts; underlying dynamics remain neural and not explicitly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Reconstruction visualization and imagined rollouts; no explicit disentanglement techniques discussed in this paper's treatment.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Described as comparatively lightweight relative to large tree-search methods; specific compute/time figures not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper contrasts World Models' approach (training policies inside learned model) with their own planning approach; World Models provided sample efficiency benefits previously reported but lacked full online planning demonstration with learned rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Referenced as achieving high scores in original work (World Models baseline score listed in this paper: 906 ± 21 on CarRacing-v0).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>World Models' learned latent dynamics allow training of policies in imagination, showing strong task utility when the model is trained on representative rollouts; however, the original approach did not demonstrate planning on a fully learned reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Original World Models favored training internal controllers inside the learned simulator (policy learning) rather than online planning; reward learning and planning were not fully explored, limiting direct planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent dimensionality smaller in original World Models (authors used 32 in original), uses MDN-RNN for stochastic dynamics, trains policy via gradient-based methods inside the learned environment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to PlaNet/Dreamer (later works) and the EPLS approach; World Models achieved strong task performance but differs in that it did not perform online planning with learned reward in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Original paper selected latent and MDN sizes by empirical reconstruction/policy performance; this paper references that using expert rollouts with an MDN-RNN can substantially improve downstream planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Planning in Latent Space', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1204.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1204.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (Learning latent dynamics for planning from pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent state-space model with both deterministic and stochastic components trained for multi-step prediction and latent-space planning using an adaptive randomized planning algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet (Recurrent State Space Model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent state-space model (RSSM) with deterministic and stochastic latent components, trained with a multi-step prediction objective; supports online planning in latent space via a randomized optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent state-space model, hybrid deterministic/stochastic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control from pixels (simulated control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Multi-step prediction accuracy in latent space and downstream control performance; training objective focuses on long-horizon predictive likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically reported in this paper; described as capable of effective latent planning but uses a more complex dynamics model than the EPLS approach.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent dynamics include deterministic/stochastic separation, but model remains a neural black box; interpretability not emphasized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper; original PlaNet work uses latent visualization and evaluation via rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Described as more complex than the MDN-RNN used in this paper; requires more sophisticated training and planning routines. Exact costs not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PlaNet uses a more sophisticated dynamics model and planning algorithm; the paper states PlaNet is most similar but more complicated than the EPLS MDN-RNN + RMHC approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported here; PlaNet is referenced as successful on pixel-based control benchmarks in its original publication.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PlaNet demonstrates that more structured latent dynamics and targeted multi-step objectives can enable effective long-horizon planning, but at the cost of model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>PlaNet trades model complexity (and likely higher compute) for improved multi-step predictive accuracy; EPLS opts for a simpler MDN-RNN for planning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Splits latent state into deterministic and stochastic parts (RSSM), explicit multi-step prediction objective to improve long-horizon accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Positioned as more complex but effective alternative to MDN-RNN/VAE pipelines; the paper notes PlaNet/Dreamer are similar lines of work but with different planning/execution strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; PlaNet's design is presented as a stronger but more complex dynamics model for latent planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Planning in Latent Space', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1204.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1204.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dream to Control (Dreamer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that uses PlaNet-style latent dynamics to learn action and value models in latent space and trains actor-critic policies by latent imagination rather than online planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (latent imagination with actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an RSSM latent dynamics model (like PlaNet) but trains an actor and value network inside the learned latent model to produce behaviors that consider rewards beyond finite horizons (latent imagination).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM) coupled with actor-critic policy learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based continuous control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Multi-step predictive likelihood and downstream policy return; fidelity judged by how well imagined rollouts support actor-critic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported quantitatively in this paper; described as a successful approach that forgoes online planning in favor of latent policy/value learning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Similar interpretability limitations as other latent models; latent rollouts provide qualitative insight but internal representations are learned and opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent visualizations and rollouts in original Dreamer work; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Dreamer uses additional training of actor and value networks in latent space; compute cost is higher than a pure MDN-RNN+evolutionary planner but concrete numbers not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dreamer avoids expensive online planning at runtime by learning policies in imagination; this shifts compute to offline training but can be more efficient at action time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified in this paper; original Dreamer work reports strong performance on pixel-based control benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrates that learning policies/value models in latent space can leverage the world model effectively without online planning, trading runtime planning cost for offline policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Dreamer trades online planning complexity for extra model components (actor/value) and offline training complexity; may be preferable when low-latency action is required.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses RSSM dynamics, trains actor and critic with imagined rollouts and value-estimation beyond horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper contrasts Dreamer (no online planning at runtime) with EPLS (online evolutionary planning in latent space), noting different trade-offs in complexity and runtime planning cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; Dreamer advocates learning value/policy models in latent space when planning at runtime is undesirable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Planning in Latent Space', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1204.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1204.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (Mastering Atari, Go, Chess and Shogi by planning with a learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned model used inside MCTS that predicts policy, value and dynamics for board/video games, enabling model-based planning without requiring a simulator of game rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero learned planning model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned representation and dynamics model that produces internal state representations, rewards, and policy/value predictors used by Monte-Carlo Tree Search for planning in discrete domains (board/video games).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent/world model used inside tree-search (hybrid learned-planner)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board and video games (Atari, Go, Chess, Shogi)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream planning performance (game win/scores) and internal prediction accuracy for value/reward; large-scale training yields high empirical fidelity suitable for MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper; referenced as obtaining state-of-the-art results on complex games but requiring extensive training data and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model internals are neural and not explicitly interpretable; planning via MCTS provides some introspection into search decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>MCTS search trajectories give insight into candidate actions; no explicit interpretability techniques discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Paper notes MuZero relies on extensive training data and substantial GPU resources (characterized here as potentially unrealistic for some applications). Exact compute costs not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MuZero achieves very high task performance but at the cost of very large training compute and data requirements compared to the lighter-weight MDN-RNN+RMHC approach used in EPLS.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Referenced as state-of-the-art on several game domains in original work; no numeric comparisons provided here beyond qualitative statements.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's learned model is highly useful for planning in domains with discrete high-quality simulators and large compute budgets; less suitable for low-data or compute-constrained real-world tasks according to the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Very high performance at high compute and data cost; less accessible for modest-resource experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Integrates learned dynamics with MCTS and predictors for policy/value; optimized on large datasets with heavy compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with EPLS as being more resource intensive; authors note MuZero's resource requirements as a downside for some applications.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here; original MuZero papers focus on large-scale training regimes for maximal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Planning in Latent Space', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1204.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1204.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Game Engine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Game Engine: Accurate learning of generalizable forward models from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to learn accurate forward models from pixels that can generalize across different sized game levels, currently demonstrated on grid-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural game engine: Accurate learning ofgeneralizable forward models from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Game Engine (pixel-based forward model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned forward-model architecture trained on pixel observations to predict future frames/states, emphasizing generalization across level sizes for grid-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural forward model (pixel-to-pixel / state forward model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Grid-based video games / game levels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction accuracy on next-frame / forward dynamics; original paper emphasizes generalization across level sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported quantitatively in this paper; authors of this paper note NGE achieves accurate forward models for grid games but is not readily a drop-in replacement for latent dynamics models used for continuous real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is neural and not explicitly interpretable; claims of generalization are architectural but do not focus on interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed here; original NGE work involves pixel prediction networks tailored for grid games.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper notes NGE currently only works well for grid-based games and is not ideal for real-world continuous control tasks compared to latent SSM approaches like PlaNet/Dreamer or MDN-RNN + VAE.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for generalizable forward prediction in grid-game domains but limited applicability to continuous pixel-control tasks according to the authors' commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Good generalization in grid domains but limited domain applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Architectural choices designed to generalize across level sizes in grid games; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Presented as complementary but domain-limited compared to latent dynamics models; recommended reading for grid-game forward modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolutionary Planning in Latent Space', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Neural game engine: Accurate learning of generalizable forward models from pixels <em>(Rating: 1)</em></li>
                <li>Bootstrapped model learning and error correction for planning with uncertainty in model-based rl <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1204",
    "paper_id": "paper-227126471",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "EPLS world model",
            "name_full": "Evolutionary Planning in Latent Space world model (ConvVAE + MDN-RNN extension)",
            "brief_description": "A learned latent-space world model combining a convolutional VAE (64-dim latent) with an MDN-RNN (LSTM with mixture-of-Gaussians output) extended to also predict rewards and episode termination, used for online evolutionary planning (RMHC) in CarRacing-v0.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "EPLS world model (ConvVAE + MDN-RNN)",
            "model_description": "Per-time-step observations (64x64x3 images) are encoded by a convolutional VAE to a 64-dimensional latent z; an LSTM memory component (MDN-RNN, 512 hidden units) receives (z_t, a_t, h_{t-1}) and outputs a Gaussian-mixture model over next latent z_{t+1} (5 mixtures), a Gaussian prediction of reward (fixed variance=1) and a Bernoulli terminal probability. Planning is performed entirely in latent space by simulating trajectories with the MDN-RNN and evaluating action sequences via Random Mutation Hill Climbing (RMHC).",
            "model_type": "latent world model (stochastic recurrent latent dynamics)",
            "task_domain": "Continuous control / racing (OpenAI Gym CarRacing-v0)",
            "fidelity_metric": "VAE reconstruction loss (pixel-wise MSE), MDN-RNN next-latent negative log-likelihood (GMM-NLL), MSE for reward prediction, binary cross-entropy for terminal prediction; overall MDN-RNN trained by minimizing -log p(z_t, r_t, τ_t | h_{t-1})",
            "fidelity_performance": "No explicit numeric losses reported for VAE or MDN-RNN in the paper; fidelity is evaluated indirectly via task performance and qualitative reconstruction examples. The paper reports that a model trained on 'expert' rollouts yields substantially better task scores than one trained only on random rollouts.",
            "interpretability_assessment": "Limited interpretability: latent space is 64-dimensional (learned, dense, not explicitly disentangled). Some interpretability via reconstructed frames from the VAE and simulated rollouts (planned trajectories) but inner dynamics remain a black-box neural model (LSTM+GMM).",
            "interpretability_method": "Visualization of reconstructed frames from VAE and visualization/examples of planned trajectories; no formal disentanglement or probing methods reported.",
            "computational_cost": "Model architecture: VAE with 4 conv + 4 deconv layers and 64-dim latent; MDN-RNN LSTM with 512 hidden units and 5 Gaussian mixture components. Training regimes: VAE trained on 10,000 rollouts for 50 epochs (Adam, lr=1e-4); MDN-RNN trained for 60 epochs (Adam, lr=1e-3) for non-iterative baseline; iterative updates used 500 rollouts per iteration and 10–60 epochs depending on experiment. No GPU count, wall-clock time, or parameter-count totals reported.",
            "efficiency_comparison": "Paper claims learned world model + planning is more sample-efficient than model-free RL for sparse/requiring-sequential-reward tasks. Empirically, iterative world-model training with 500 rollouts per iteration improved task scores faster than retraining on 10k rollouts per iteration; no explicit FLOPs or runtime comparisons to baselines provided.",
            "task_performance": "Reported CarRacing-v0 mean scores (100 trials): Non-iterative random-model: 356 ± 177; Non-iterative expert-model (MDN-RNN trained on 5k random + 5k expert rollouts): 765 ± 102; Iterative model (5 iterations, 15 generations, horizon 20): 708 ± 195. These are compared to baselines: DQN 343 ± 18, A3C (continuous) 591 ± 45, World Model [6] 906 ± 21.",
            "task_utility_analysis": "The learned world model enables effective online planning: iterative refinement of the world model by collecting rollouts with the planner improves task performance substantially over the random-trained model. High-quality training data (expert rollouts) yields higher model utility; however, even an iteratively improved model does not fully reach the best reported 'World Models' expert performance. Model predictive uncertainty limits useful planning horizon (horizon &gt;20 gives diminishing or negative returns).",
            "tradeoffs_observed": "Key trade-offs documented: (1) Latent compression (64-dim) reduces computation and enables planning but limits long-horizon fidelity; planning too far ahead increases uncertainty so longer horizons degrade performance. (2) More evolutionary generations improve planning until local convergence (diminishing returns beyond ~15 generations). (3) Training on diverse/expert rollouts improves fidelity and task utility but requires access to better trajectories; iterative bootstrapping is a practical compromise. Interpretability is limited by using dense latent representations and black-box recurrent dynamics.",
            "design_choices": "Choice of 64-dim latent (empirically better than 32), ConvVAE encoder/decoder with 4 conv/deconv layers, MDN-RNN (LSTM 512 units) with 5 Gaussian mixture components to capture stochastic multi-modality, explicit prediction heads for reward (Gaussian) and terminal (Bernoulli). Planning design uses RMHC in latent space with shift buffering, horizon default 20, generations typically 10 (15 found beneficial). Iterative training uses small per-iteration replay (500 rollouts) to balance sample-efficiency and training time.",
            "comparison_to_alternatives": "Compared to model-free (DQN, A3C) the EPLS approach achieves higher mean scores than DQN and A3C in reported experiments (e.g., iterative model 708 &gt; A3C 591). Compared to World Models [6], when trained with expert rollouts the baseline MDN-RNN achieves performance approaching World Models' reported behavior but still below the highest World Models score (906). Compared to PlaNet/Dreamer and MuZero, the paper notes PlaNet/Dreamer use more complex dynamics/planning/actors, and MuZero relies on large training data and GPU resources; EPLS favors a simpler MDN-RNN + evolutionary planner and emphasizes sample-efficiency via iterative bootstrapping.",
            "optimal_configuration": "Empirical recommendations from paper: use a 64-dim latent (better reconstructions than 32), MDN-RNN with 512 LSTM units and 5 mixtures, iterative training with modest-sized rollout batches (500 rollouts) yields fast improvements, planning horizon ≈20 and ~15 generations for RMHC are near-optimal (longer horizons or more generations give diminishing or negative returns due to model uncertainty and planning convergence). The paper advises balancing planning depth against model predictive uncertainty and improving training data coverage through iterative rollouts.",
            "uuid": "e1204.0",
            "source_info": {
                "paper_title": "Evolutionary Planning in Latent Space",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "World Models",
            "name_full": "World Models (Ha & Schmidhuber 2018)",
            "brief_description": "A latent-space stochastic recurrent world model combining a VAE to compress pixel observations and an MDN-RNN to model latent dynamics; used to train policies inside the learned model rather than for online planning in the original work.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (VAE + MDN-RNN)",
            "model_description": "VAE encoder/decoder maps frames to/from a low-dimensional latent; an MDN-RNN models latent transitions as a mixture-of-Gaussians conditional on actions; policy trained inside the simulated latent environment using the learned model.",
            "model_type": "latent world model (stochastic recurrent latent dynamics)",
            "task_domain": "Video games / simulated environments (including CarRacing in original paper demonstrations)",
            "fidelity_metric": "VAE reconstruction loss (pixel MSE) and MDN-RNN negative log-likelihood on next-latent; qualitative assessment via reconstructions and downstream policy performance.",
            "fidelity_performance": "Not numerically reported in this paper; referenced as effective for training policies but original World Models did not learn environment reward in the same way (paper notes reward was not learned for planning in that work).",
            "interpretability_assessment": "Provides some interpretability via decoded reconstructions from latent and can visualize agent's 'imagination' rollouts; underlying dynamics remain neural and not explicitly interpretable.",
            "interpretability_method": "Reconstruction visualization and imagined rollouts; no explicit disentanglement techniques discussed in this paper's treatment.",
            "computational_cost": "Described as comparatively lightweight relative to large tree-search methods; specific compute/time figures not given in this paper.",
            "efficiency_comparison": "Paper contrasts World Models' approach (training policies inside learned model) with their own planning approach; World Models provided sample efficiency benefits previously reported but lacked full online planning demonstration with learned rewards.",
            "task_performance": "Referenced as achieving high scores in original work (World Models baseline score listed in this paper: 906 ± 21 on CarRacing-v0).",
            "task_utility_analysis": "World Models' learned latent dynamics allow training of policies in imagination, showing strong task utility when the model is trained on representative rollouts; however, the original approach did not demonstrate planning on a fully learned reward model.",
            "tradeoffs_observed": "Original World Models favored training internal controllers inside the learned simulator (policy learning) rather than online planning; reward learning and planning were not fully explored, limiting direct planning utility.",
            "design_choices": "Latent dimensionality smaller in original World Models (authors used 32 in original), uses MDN-RNN for stochastic dynamics, trains policy via gradient-based methods inside the learned environment.",
            "comparison_to_alternatives": "Compared qualitatively to PlaNet/Dreamer (later works) and the EPLS approach; World Models achieved strong task performance but differs in that it did not perform online planning with learned reward in the original work.",
            "optimal_configuration": "Original paper selected latent and MDN sizes by empirical reconstruction/policy performance; this paper references that using expert rollouts with an MDN-RNN can substantially improve downstream planning performance.",
            "uuid": "e1204.1",
            "source_info": {
                "paper_title": "Evolutionary Planning in Latent Space",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "PlaNet",
            "name_full": "PlaNet (Learning latent dynamics for planning from pixels)",
            "brief_description": "A recurrent state-space model with both deterministic and stochastic components trained for multi-step prediction and latent-space planning using an adaptive randomized planning algorithm.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "mention",
            "model_name": "PlaNet (Recurrent State Space Model)",
            "model_description": "Recurrent state-space model (RSSM) with deterministic and stochastic latent components, trained with a multi-step prediction objective; supports online planning in latent space via a randomized optimizer.",
            "model_type": "latent world model (recurrent state-space model, hybrid deterministic/stochastic)",
            "task_domain": "Continuous control from pixels (simulated control tasks)",
            "fidelity_metric": "Multi-step prediction accuracy in latent space and downstream control performance; training objective focuses on long-horizon predictive likelihoods.",
            "fidelity_performance": "Not numerically reported in this paper; described as capable of effective latent planning but uses a more complex dynamics model than the EPLS approach.",
            "interpretability_assessment": "Latent dynamics include deterministic/stochastic separation, but model remains a neural black box; interpretability not emphasized in this paper.",
            "interpretability_method": "Not described in this paper; original PlaNet work uses latent visualization and evaluation via rollouts.",
            "computational_cost": "Described as more complex than the MDN-RNN used in this paper; requires more sophisticated training and planning routines. Exact costs not reported here.",
            "efficiency_comparison": "PlaNet uses a more sophisticated dynamics model and planning algorithm; the paper states PlaNet is most similar but more complicated than the EPLS MDN-RNN + RMHC approach.",
            "task_performance": "Not reported here; PlaNet is referenced as successful on pixel-based control benchmarks in its original publication.",
            "task_utility_analysis": "PlaNet demonstrates that more structured latent dynamics and targeted multi-step objectives can enable effective long-horizon planning, but at the cost of model complexity.",
            "tradeoffs_observed": "PlaNet trades model complexity (and likely higher compute) for improved multi-step predictive accuracy; EPLS opts for a simpler MDN-RNN for planning efficiency.",
            "design_choices": "Splits latent state into deterministic and stochastic parts (RSSM), explicit multi-step prediction objective to improve long-horizon accuracy.",
            "comparison_to_alternatives": "Positioned as more complex but effective alternative to MDN-RNN/VAE pipelines; the paper notes PlaNet/Dreamer are similar lines of work but with different planning/execution strategies.",
            "optimal_configuration": "Not specified in this paper; PlaNet's design is presented as a stronger but more complex dynamics model for latent planning.",
            "uuid": "e1204.2",
            "source_info": {
                "paper_title": "Evolutionary Planning in Latent Space",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dream to Control (Dreamer)",
            "brief_description": "A model that uses PlaNet-style latent dynamics to learn action and value models in latent space and trains actor-critic policies by latent imagination rather than online planning.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer (latent imagination with actor-critic)",
            "model_description": "Uses an RSSM latent dynamics model (like PlaNet) but trains an actor and value network inside the learned latent model to produce behaviors that consider rewards beyond finite horizons (latent imagination).",
            "model_type": "latent world model (RSSM) coupled with actor-critic policy learning",
            "task_domain": "Pixel-based continuous control tasks",
            "fidelity_metric": "Multi-step predictive likelihood and downstream policy return; fidelity judged by how well imagined rollouts support actor-critic learning.",
            "fidelity_performance": "Not reported quantitatively in this paper; described as a successful approach that forgoes online planning in favor of latent policy/value learning.",
            "interpretability_assessment": "Similar interpretability limitations as other latent models; latent rollouts provide qualitative insight but internal representations are learned and opaque.",
            "interpretability_method": "Latent visualizations and rollouts in original Dreamer work; not detailed here.",
            "computational_cost": "Dreamer uses additional training of actor and value networks in latent space; compute cost is higher than a pure MDN-RNN+evolutionary planner but concrete numbers not provided here.",
            "efficiency_comparison": "Dreamer avoids expensive online planning at runtime by learning policies in imagination; this shifts compute to offline training but can be more efficient at action time.",
            "task_performance": "Not quantified in this paper; original Dreamer work reports strong performance on pixel-based control benchmarks.",
            "task_utility_analysis": "Demonstrates that learning policies/value models in latent space can leverage the world model effectively without online planning, trading runtime planning cost for offline policy training.",
            "tradeoffs_observed": "Dreamer trades online planning complexity for extra model components (actor/value) and offline training complexity; may be preferable when low-latency action is required.",
            "design_choices": "Uses RSSM dynamics, trains actor and critic with imagined rollouts and value-estimation beyond horizon.",
            "comparison_to_alternatives": "Paper contrasts Dreamer (no online planning at runtime) with EPLS (online evolutionary planning in latent space), noting different trade-offs in complexity and runtime planning cost.",
            "optimal_configuration": "Not specified in this paper; Dreamer advocates learning value/policy models in latent space when planning at runtime is undesirable.",
            "uuid": "e1204.3",
            "source_info": {
                "paper_title": "Evolutionary Planning in Latent Space",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (Mastering Atari, Go, Chess and Shogi by planning with a learned model)",
            "brief_description": "A learned model used inside MCTS that predicts policy, value and dynamics for board/video games, enabling model-based planning without requiring a simulator of game rules.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero learned planning model",
            "model_description": "A learned representation and dynamics model that produces internal state representations, rewards, and policy/value predictors used by Monte-Carlo Tree Search for planning in discrete domains (board/video games).",
            "model_type": "latent/world model used inside tree-search (hybrid learned-planner)",
            "task_domain": "Board and video games (Atari, Go, Chess, Shogi)",
            "fidelity_metric": "Downstream planning performance (game win/scores) and internal prediction accuracy for value/reward; large-scale training yields high empirical fidelity suitable for MCTS.",
            "fidelity_performance": "Not quantified in this paper; referenced as obtaining state-of-the-art results on complex games but requiring extensive training data and compute.",
            "interpretability_assessment": "Model internals are neural and not explicitly interpretable; planning via MCTS provides some introspection into search decisions.",
            "interpretability_method": "MCTS search trajectories give insight into candidate actions; no explicit interpretability techniques discussed here.",
            "computational_cost": "Paper notes MuZero relies on extensive training data and substantial GPU resources (characterized here as potentially unrealistic for some applications). Exact compute costs not provided in this paper.",
            "efficiency_comparison": "MuZero achieves very high task performance but at the cost of very large training compute and data requirements compared to the lighter-weight MDN-RNN+RMHC approach used in EPLS.",
            "task_performance": "Referenced as state-of-the-art on several game domains in original work; no numeric comparisons provided here beyond qualitative statements.",
            "task_utility_analysis": "MuZero's learned model is highly useful for planning in domains with discrete high-quality simulators and large compute budgets; less suitable for low-data or compute-constrained real-world tasks according to the paper's discussion.",
            "tradeoffs_observed": "Very high performance at high compute and data cost; less accessible for modest-resource experiments.",
            "design_choices": "Integrates learned dynamics with MCTS and predictors for policy/value; optimized on large datasets with heavy compute.",
            "comparison_to_alternatives": "Contrasted with EPLS as being more resource intensive; authors note MuZero's resource requirements as a downside for some applications.",
            "optimal_configuration": "Not discussed here; original MuZero papers focus on large-scale training regimes for maximal performance.",
            "uuid": "e1204.4",
            "source_info": {
                "paper_title": "Evolutionary Planning in Latent Space",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Neural Game Engine",
            "name_full": "Neural Game Engine: Accurate learning of generalizable forward models from pixels",
            "brief_description": "A method to learn accurate forward models from pixels that can generalize across different sized game levels, currently demonstrated on grid-based games.",
            "citation_title": "Neural game engine: Accurate learning ofgeneralizable forward models from pixels",
            "mention_or_use": "mention",
            "model_name": "Neural Game Engine (pixel-based forward model)",
            "model_description": "A learned forward-model architecture trained on pixel observations to predict future frames/states, emphasizing generalization across level sizes for grid-based games.",
            "model_type": "neural forward model (pixel-to-pixel / state forward model)",
            "task_domain": "Grid-based video games / game levels",
            "fidelity_metric": "Prediction accuracy on next-frame / forward dynamics; original paper emphasizes generalization across level sizes.",
            "fidelity_performance": "Not reported quantitatively in this paper; authors of this paper note NGE achieves accurate forward models for grid games but is not readily a drop-in replacement for latent dynamics models used for continuous real-world tasks.",
            "interpretability_assessment": "Model is neural and not explicitly interpretable; claims of generalization are architectural but do not focus on interpretability.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not detailed here; original NGE work involves pixel prediction networks tailored for grid games.",
            "efficiency_comparison": "Paper notes NGE currently only works well for grid-based games and is not ideal for real-world continuous control tasks compared to latent SSM approaches like PlaNet/Dreamer or MDN-RNN + VAE.",
            "task_performance": "Not quantified here.",
            "task_utility_analysis": "Useful for generalizable forward prediction in grid-game domains but limited applicability to continuous pixel-control tasks according to the authors' commentary.",
            "tradeoffs_observed": "Good generalization in grid domains but limited domain applicability.",
            "design_choices": "Architectural choices designed to generalize across level sizes in grid games; specifics not detailed in this paper.",
            "comparison_to_alternatives": "Presented as complementary but domain-limited compared to latent dynamics models; recommended reading for grid-game forward modeling.",
            "uuid": "e1204.5",
            "source_info": {
                "paper_title": "Evolutionary Planning in Latent Space",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Neural game engine: Accurate learning of generalizable forward models from pixels",
            "rating": 1,
            "sanitized_title": "neural_game_engine_accurate_learning_of_generalizable_forward_models_from_pixels"
        },
        {
            "paper_title": "Bootstrapped model learning and error correction for planning with uncertainty in model-based rl",
            "rating": 1,
            "sanitized_title": "bootstrapped_model_learning_and_error_correction_for_planning_with_uncertainty_in_modelbased_rl"
        }
    ],
    "cost": 0.016545999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evolutionary Planning in Latent Space</p>
<p>Thor V A N Olesen 
IT-University of Copenhagen</p>
<p>Dennis T T Nguyen 
IT-University of Copenhagen</p>
<p>Rasmus B Palm 
IT-University of Copenhagen</p>
<p>Sebastian Risi 
IT-University of Copenhagen</p>
<p>Evolutionary Planning in Latent Space
World ModelsEvolutionary PlanningIterative TrainingModel- Based Reinforcement Learning
Planning is a powerful approach to reinforcement learning with several desirable properties. However, it requires a model of the world, which is not readily available in many real-life problems. In this paper, we propose to learn a world model that enables Evolutionary Planning in Latent Space (EPLS). We use a Variational Auto Encoder (VAE) to learn a compressed latent representation of individual observations and extend a Mixture Density Recurrent Neural Network (MDRNN) to learn a stochastic, multi-modal forward model of the world that can be used for planning. We use the Random Mutation Hill Climbing (RMHC) to find a sequence of actions that maximize expected reward in this learned model of the world. We demonstrate how to build a model of the world by bootstrapping it with rollouts from a random policy and iteratively refining it with rollouts from an increasingly accurate planning policy using the learned world model. After a few iterations of this refinement, our planning agents are better than standard model-free reinforcement learning approaches demonstrating the viability of our approach 1 2 .</p>
<p>Introduction</p>
<p>Planning-searching for action sequences that maximize expected reward-is a powerful approach to reinforcement learning problems which has recently lead to breakthroughs in hard domains such Go, Shogi, Chess, and Atari games [18,19,17]. To plan, the agent needs access to a model of the world which it can use to simulate the outcome of actions, to determine which course of action is best. Planning using a model of the world also allows you to introspect what the agent is planning and why it thinks a certain action is good. It even allows you to add constraints or change the objective at runtime.</p>
<p>For some domains, these world models are readily available, for instance, games, where the world model is given by the rules of the game. However, for many real-world problems, e.g. driving a car, they are not available.</p>
<p>In problems where a model of the world is not available one can instead use model-free reinforcement learning, which learns a policy that directly maps from the environment state to the actions that maximize expected reward. However, these approaches require a large number of samples of the real environmentwhich is often expensive to obtain-to learn the statistical relationship between states, actions, and rewards. This is especially true if the environment requires complex sequences of actions before any reward is observed.</p>
<p>Alternatively, a model of the world can be learned, which is the approach taken in this paper. This is known as model-based reinforcement learning. Learning a model of the world often requires much fewer samples of the environment than directly learning a policy since it can use supervised learning methods to predict the environment state transitions, whether or not the reward signal is sparse. Several models have been proposed for learning world models [6,17,8].</p>
<p>In this paper, we propose an extension to the Mixture Density Recurrent Neural Network (MDRNN) world model [6], which makes it suitable for planning, and demonstrate how we can do evolutionary planning entirely in the latent space of the learned world model. See figure 1 for examples of planned trajectories and figure 2 for an overview of the proposed method.</p>
<p>We further show how to iteratively improve the world model by using the existing world model to do the planning and using the planning policy to sample better rollouts of the environment, which in turn are used to train a better world model. This process is bootstrapped by using an initial random policy. We show that after only a few rounds of iterative refinement like this we achieve results that are better than standard model-free approaches, demonstrating the viability of the approach.</p>
<p>Related work</p>
<p>Planning</p>
<p>In planning an agent uses a model of the world to predict the consequences of its actions and select an optimal action sequence accordingly. Planning is a powerful technique that has recently lead to breakthroughs in hard domains such as Go, Chess, Shogu, and Atari [18,19,17].</p>
<p>Monte-Carlo Tree Search (MCTS) is a state-of-the-art planning algorithm for discrete action spaces, which iteratively builds a search tree that explores the most promising paths using a fast, often stochastic, rollout policy [3].</p>
<p>Rolling Horizon Evolutionary Algorithms (RHEA) encode individuals as sequences of actions and uses evolutionary algorithms to search for optimal trajectories. Rolling Horizon (RH) refers to how the first action of a plan is executed before the plan is reevaluated and adjusted, looking one step further into the future and slowly expanding the horizon [15,5]. RHEA naturally handles continuous action spaces. The authors in [20] show how to learn a prior for RHEA by training a value and policy network. The value network reduces the required planning horizon by estimating the rewards of future states. The policy network helps initialize the population of planning action trajectories to help narrow down the search scope to a near-optimal local action policy-subspace. In our approach, we use a randomly initialized set of planning trajectories that are improved iteratively with evolution.</p>
<p>Random Mutation Hill-Climb (RMHC) is a simple and effective type of evolutionary algorithm that repeats the process of randomly selecting a neighbour of a best-so-far solution and accepts the neighbour if it is better than or equal to the current best-so-far solution. This local search method starts with a solution and iteratively tries to improve it by taking random steps or restarting from another region in the policy space.</p>
<p>Planning approaches that rely on imperfect models may plan non-optimal trajectories. The authors of [14] suggest incorporating uncertainty estimation into the forward model, which enables the agent to perform better. In general planning under uncertainty has been extensively studied [2,13,10].</p>
<p>Learning world models</p>
<p>If a world model is not available it can be learned from observations of the environment. This is generally known as model-based Reinforcement Learning (RL).</p>
<p>World Models [6] introduces a stochastic recurrent world model which is learned from observations of the environment under an initially random policy. The model uses a Variational Auto-Encoder (VAE) to encode pixel inputs into a low dimensional latent vector. A recurrent neural network (RNN) is then trained to predict sequences of these latent vectors using a Gaussian Mixture Model to capture the uncertain and multi-modal nature of the environment. Notably, the authors do not use this world model for planning, but rather for training a relatively simple policy network.</p>
<p>In MuZero [17], the authors show how to do planning with a learned model in board and video games using tree-based search (MCTS) to enable imitation learning with a learned policy network.</p>
<p>In PlaNet [8], the authors have shown it is possible to do online planning in latent space using an adaptive randomized algorithm on a recurrent state-space model (SSM) with a deterministic and stochastic component and a multistep prediction objective. PlaNet [8] is the approach that is most similar to the work presented here (i.e., online planning on a learned model). However, it uses a rather complicated dynamics model and planning algorithm. In Dreamer [7], the authors use the PlaNet world model but no longer do online planning. Instead, their Dreamer agent uses an actor-critic approach to learn behaviors that consider rewards beyond a horizon. Namely, they learn an action model and value model in the latent space of the world model. Thus, their approach is similar to World Models [6] where they plan on a learned model by training a policy inside the simulated environment with backpropagation and gradient descent, instead of evolution. The novel part is using a value network to estimate rewards beyond a finite imagination horizon. Also, World Models [6] does not show how to do planning on a fully learned model, since the reward signal is not learned in their model. Finally, MuZero [17] relies on extensive training data and access to unrealistic GPU resources, which may not be feasible in practice.</p>
<p>In another related approach, Neural Game Engine [1], the authors show how to learn accurate forward models from pixels that can generalize to different size game levels. However, their methods currently only work on grid-based world games. The authors argue it does not work as a drop-in replacement for the kind of world models we need in real-life environments. For this purpose, the authors recommend looking into some of the previously presented methods that learn a latent dynamics model with a 2D state space model (SSM) like shown in PlaNet and Dreamer that both use a Recurrent State Space Model (RSSM).</p>
<p>Approach</p>
<p>We use a model-based RL approach to solve a continuous reinforcement learning control task. We achieve this through online evolutionary planning on a learned model of the environment. Our solution combines a world model [6] with rolling horizon evolutionary planning [15]. See figure 2 for an overview. Similar to the model in the original world model [6], our model uses a visual sensory component (V) to compress the current state into a small latent representation. We extend the memory component (M) so that it predicts the next latent state, the expected reward, and whether the environment terminates. The decision-making component in the original world model [6] uses a simple learned linear model that maps latent and hidden states directly to actions at each time step. In contrast, our work uses a random mutation hill-climbing (RMHC) planning algorithm as the decision-making component that exploits M to do online planning in latent space.</p>
<p>Learning the World Model</p>
<p>The visual component (V) is implemented as a convolutional variational autoencoder (ConvVAE), which learns an abstract, compressed representation The raw observation is compressed by V at each time step t to produce a latent vector zt. RMHC does planning by repeatedly generating, mutating, and evaluating action sequences a0, ..., aT in the learned world model, M. where T is the horizon. The learned world model, M, receives an action at, latent vector zt and hidden state ht and predicts the simulated reward rt, next latent vector zt+1, and next hidden state ht+1. The predicted states are used with the next action as inputs for M to let the agent simulate the trajectory in latent space. The first action of the plan with the highest expected total reward in the simulated environment is executed in the real environment. z t ∈ R 64 of states (i.e., frames) s t ∈ R 64×64×3 using an encoder and decoder as shown in figure 3.</p>
<p>The VAE encoder is a neural network that outputs a compressed representation of a state s (frame) using a deep convolutional neural network (DCNN) of four stacked convolutional layers and non-linear relu activations to compress the frame and two fully-connected (i.e., dense) layers that encode the convolutional Fig. 3. Flow diagram of a Variational Autoencoder (VAE). The VAE learns to encode frames into latent vectors by minimizing the pixel-wise difference between input frames and reconstructed frames (i.e., L2 or MSE) generated by decoding the latent vectors.</p>
<p>output into low dimension vectors µ z and σ z :
encoder : s ∈ R 64×64×3 → µ z ∈ R 64 , σ z ∈ R 64 .(1)
The means µ z and standard deviations σ z are used to sample a latent state z from a multivariate Gaussian with diagonal covariance:
z ∈ R 64 ∼ N (z|µ z , σ z ) .(2)
The decoder is a neural network that learns to decode and reconstruct the state (i.e., frame) s given the latent state z using a deep CNN of four stacked deconvolution layers:
decoder : z ∈ R 64 → s ∈ R 64×64×3 .(3)
Each convolution and deconvolution layer uses a stride of two. Convolutional and deconvolutional layers use relu activations. The output layer maps directly to pixel values between 0 and 1. The VAE is trained with the standard VAE loss [11]. We extend the memory component (M) of [6] to also output an expected reward r and a binary terminal signal τ to obtain a fully learned world model that can be used for planning entirely in latent space. M is an LSTM with 512 hidden units, which jointly models the next latent state z t , reward r t and whether or not the environment terminates, τ t ,
p(z t , r t , τ t |h t−1 ) = p(z t |h t−1 )p(r t |h t−1 )p(τ t |h t−1 ) .(4)
The LSTM hidden state h t depends on the previous hidden state h t−1 , the current action a t , and the current latent state z t such that h t = LSTM(z t , a t , h t−1 ).</p>
<p>Most complex environments are stochastic and multi-modal so p(z t |h t−1 ) is approximated as a mixture of Gaussian distribution (MD-RNN). The output of the MDRNN are the parameters π, µ, σ of a parametric Gaussian mixture model where π represents mixture probabilities:
p(z t |h t−1 ) = 5 k=1 π k N (z t |µ k , σ k ) ,(5)
where π, µ and Σ are linear functions of h t−1 and each mixture component is a multivariate Gaussian distribution with diagonal covariance. We model the reward r using a Gaussian with a fixed variance of 1 such that
p(r t |h t−1 ) = N (r t |µ τ t , 1) ,(6)
where µ τ t is a linear function of h t−1 . Finally we model the terminal state τ using a Bernoulli distribution,
p(τ t |h t−1 ) = p τt (1 − p) 1−τt ,(7)
where p = sigmoid(f (h t−1 )) is the sigmoid of a linear function of h t−1 . We train M by minimizing the negative log-likelihood of p(z t , r t , τ t |h t−1 ) for observed rollouts of the environment,
L = − log p(z t , r t , τ t |h t−1 ) = MSE(r t ,r t )+BCE(τ t ,τ t )+GMM-NLL(z t ,ẑ t ) ,(8)
where MSE is the mean squared error, BCE is the binary cross-entropy and GMM-NLL is the negative log likelihood of a gaussian mixture model andẑ,r,τ are the observed latents, reward and termination state.</p>
<p>Evolutionary planning in latent space</p>
<p>Once the world model is trained it can be used for planning. We use Random Mutation Hill Climbing (RMHC) which is a simple evolutionary algorithm. RMHC works by iteratively mutating and evaluating individuals, and letting the elite be the basis for the next round of mutation. We use RMHC to find a sequence of actions that maximize the expected reward as predicted by the world model. The length of the action sequence determines how far into the future the agent plans and is known as the horizon. Finally, we use shift buffering to avoid repeating the entire search process from scratch at every time step [4]. In short, after each planning step we pop the first action of the action sequence and add a new random action to the end of the action sequence. This modified plan is then the starting point for the next planning step. See figure 4 for an overview.</p>
<p>Experiments</p>
<p>We test our approach on the continuous control CarRacing-v0 domain [12], built with the Box2D physics engine. At every trial, the agent is exposed to a randomly generated track. Reaching a high score requires the agent to plan how to make each turn with continuous actions, which makes it a suitable test domain for our evolutionary latent planning approach. The environment yields Fig. 4. Planning details. RMHC initializes a random sequence of actions sampled from the environment and mutates it repeatedly across generations. Each plan is evaluated in latent space using the simulated environment where the fitness metric is the total undiscounted expected reward associated with executing the planning trajectory in latent space. a reward of -0.1 each time step and a reward of +1000/N for each visited track tile where N is the total number of tiles in the track. While it is not necessarily difficult to drive slowly around a track, reaching a high reward is difficult for many current RL methods [6].</p>
<p>Since the environment gives observations as high dimensional pixel images, these are first resized to 64 × 64 pixels. The resized images are used as observations in our world model. Pixels are stored as three floating-point values between 0 and 1 that represent each of the RGB channels. The dimension of our latent space is 64 since this yielded better reconstructions than using 32 as in [6]. Actions contain three numeric components that represent the degree of steering, acceleration, and braking.</p>
<p>Capturing rollouts The MDN-RNN and VAE models are trained in a supervised manner, which relies on access to a representative dataset of environment rollouts for training and testing. Each sample is a rollout of the environment and consists of a sequence of (state, action, reward, terminal) tuples. The state, reward, and terminal are produced by the environment given an action, which is produced by the policy. We initially use a random policy on the environment and record states, rewards, actions, and terminals for T steps. We use T = 500 for the non-iterative procedure and T = 250 for the iterative procedure. We found that using T = 250 was sufficient while speeding up the iterative training procedure.</p>
<p>Non-iterative training procedure The non-iterative training procedure follows the same approach as presented in the original world model work [6]. To train the VAE and MDN-RNN, we first collect a dataset of 10,000 rollouts using a random policy to explore the environment where we record the random action a t executed and the generated observations. The dataset is used to train the VAE so it can learn an abstract and compressed representation of the environment. The VAE is trained for 50 epochs with a learning rate of 1e − 4 using the Adam optimizer.</p>
<p>The MDN-RNN is trained on the 10,000 rollouts where each frame s t is preprocessed by the VAE into latent vector z t for each time step t. The latent vectors and actions a t are given to the MDN-RNN such that it can learn to model the next latent vector p(z t+1 |a t , z t , h t ) as a mixture of Gaussians and the reward r and the terminal d. The MDN-RNN consists of 512 hidden units and a mixture of 5 Gaussians. We train the MDN-RNN for 60 epochs with a learning rate of 1e − 3 using the Adam optimizer. In summary, the full non-iterative training procedure is shown below:</p>
<ol>
<li>Collect 10.000 rollouts with a random policy 2. Train world model using random rollouts. 3. Evaluate the agent on 100 random tracks using the RMHC planning policy.</li>
</ol>
<p>Iterative training procedure</p>
<p>Once the world model is trained non-iteratively, we can use it in conjunction with our planning algorithm (RMHC) to do online planning. However, while the agent may be able to somewhat stay on the road and drive slowly at corners, its performance is limited by our world model that is trained with a random policy only. Consequently, the dynamics associated with well-behaved driving might be underexplored, and hence our world model may not be able to fully represent this in latent space.</p>
<p>To address this we used an iterative training procedure as suggested in [6], in which we iteratively collect rollouts using our agents planning policy and improve our world model (and thus our planning) using the new rollouts. Intuitively, we expect planning using the learned world model to yield a better policy than a random one. These new rollouts using planning are stored in a replay buffer that contains both old and new rollouts, which allows the MDN-RNN to learn from both past and new experiences. We collect 500 rollouts per iteration. The iterative training procedure is as follows:</p>
<ol>
<li>Train MDN-RNN and VAE non-iteratively to obtain baseline model 2. Collect rollouts using RMHC planning policy and add them to the replay buffer 3. Train the world model using rollouts in replay buffer. 4. Evaluate the agent on 100 random tracks using RMHC planning policy. 5. Go back to (2) and repeat for I iterations or until the task is complete For both approaches, we found that training the VAE using 10k random rollouts was sufficient in representing different scenarios of the car racing environment across all our experiments. We used RMHC with a horizon of 20, and the action sequence was evolved for ten generations at every time step t with shift buffering.</li>
</ol>
<p>Results</p>
<p>Non-iterative training</p>
<p>The MDN-RNN serves as a predictive model for future latent z vectors that the VAE may produce and the expected reward r that the environment may produce. Thus the rollouts used to train the MDN-RNN may affect its predictive ability and how well it represents the real environment during online planning. Thus, we trained two MDN-RNNs using the non-iterative training procedure to obtain a random model and an expert model. The random model is trained on 10,000 random rollouts and acts as our baseline model for all iteratively-trained models. The expert model trains on 5,000 random rollouts and 5,000 expert rollouts. The expert rollouts are collected with the pre-trained agent in World Models [6]. The random rollouts allow the MDN-RNN to learn the consequences of bad-driving behavior, and the expert rollouts allow it to learn the positive reward signal associated with expert-driving. The expert model is a reference model used for comparison with our agent that helps determine how well an agent may perform when the MDN-RNN is exposed to a well-representative dataset.</p>
<p>Using the random model, the agent did learn to drive unsteadily around the track and sometimes plan around sharp corners. However, the agent only managed to achieve a mean score of 356.20 ± 176.69 with the highest score of 804. In contrast, using the expert model, the agent managed to obtain a mean score of 765.17 ± 102.18 with the highest score of 900. The expert rollouts improve the MDN-RNN's ability to capture the dynamics of the environment, which significantly improved the agent's performance ( Figure 5).</p>
<p>Iterative training</p>
<p>Since we cannot rely on access to pre-trained expert rollouts, we have implemented an iterative training procedure that allows the agent to improve its performance over time by learning from its own experiences. Namely, we generate rollouts by online planning with the RMHC evolutionary policy search method, which iteratively improve our world model. We investigate if the random baseline model can improve by using a small number of only 500 rollouts and a sequence Fig. 5. Total rewards of 100 trials with MDN-RNN trained on 10000 rollouts using random policy vs. an MDN-RNN trained on 5000+5000 rollouts using random and expert policy respectively. The latter yields a much higher total reward due to the dataset containing a mixture of random and well-behaved rollouts. Fig. 6. Mean total rewards and standard deviations over 100 trials across five iterations. The 0th iteration represents the mean total reward before iterative training, but after training on 10000 rollouts obtained from a random policy. Notice, using 500 rollouts (Experiment A) yields a faster training time and a better result compared to experiment B that uses 10.000 rollouts per iteration length of 250 experiences trained over ten epochs and five iterations. Figure 6 shows the mean total rewards after each of the five iterations.</p>
<p>Already after a single training iteration, the agent managed to get a mean score of 557.87 ± 244.97 and peaked at iteration 5 with a mean score of 656.82 ± 226.67. Despite not beating the expert model, we saw improvements throughout the iterations, and the agent managed to occasionally complete the game by scoring a total reward of 900 during benchmarks. While the first iteration yielded the most significant improvement in total average reward, the following iterations still improved. The large improvement seen in the first iteration might be due to the MDN-RNN learning the dynamics of more well-behaved driving from the agent's planning policy, which ultimately mitigates the errors made by the initially random model.</p>
<p>Investigating different planning horizons and generations</p>
<p>The benchmarks from iterative training show how refining the world model can affect the agent's planning capabilities. Given the best iterative model found after five training iterations, it is interesting to see how different horizon lengths and max generations affect the agent's ability to plan with the iterative model and the RMHC policy search method.</p>
<p>Both planning parameters are adjusted independently and individually to see how they affect planning. However, we must keep in mind that the horizon length and the maximum number of generations are very likely to be highly correlated. Thus, one should also conduct experiments where both parameters are adjusted together. Figure 7 shows how different parameter values affect the average total reward obtained by planning with RMHC on a model trained with five iterations across 100 trials. The baseline number of generations is 10. Reducing this to 5 shows a decrease in mean total reward, achieving a score of 473.53 ± 280.18. This reduction is likely due to the agent having less planning time and therefore not being able to converge to a better trajectory in a local policy subspace. Increasing the number of generations to 15 increases the mean reward to 707.79 ± 195.44, which is not surprising since it gives the agent more planning time. However, increasing the number of generations further did not improve the results. Presumably, this may imply that the agent has converged to a locally optimal trajectory in the simulated environment after being evolved 15 generations.</p>
<p>The results when varying the horizon are shown in Figure 7, right. The baseline horizon length is 20. Reducing this value to 5 resulted in poor planning and yielded a mean score of 31.91 ± 40.54. Seemingly, a small horizon exploits a more certain near-future but does not bring much information for long-term planning of a trajectory associated with well-behaved driving. Consequently, this kind of short-sighted agent may not be able to act in time before driving into the grass or know in what direction it should move. As we increase the horizon from 5 to 20, the mean score increases. The agent receives more information about the car's trajectory, which allows it to plan accordingly. However, a horizon beyond 20 does not help the agent, which is likely due to the increased uncertainty caused by planning too far into the future. The further the agent plans ahead, the more uncertain the trajectory becomes, which makes it less relevant to the current situation that the agent must act upon.</p>
<p>According to figure 7 and table 5.2, the main results are as follows. Firstly, the iterative training procedure significantly improved our random baseline model and showed improvement after only one iteration. Secondly, increasing the maximum number of generations to 15 and a horizon of 20 used in our RMHC policy search approach improved the total average reward obtained across 100 random tracks. However, increasing the parameter values more than this yields diminishing returns, and a slight decrease in total reward. This may be due to the model's inability to predict far into the future when using a high horizon or the planning trajectory having converged when using a large number of generations. Notice, we do not dynamically adjust the horizon and number of generations during iterative training but keep them fixed during all five iterations. Instead, we compare different combinations of parameters across whole runs of five iterations. To sum up, our results show it is possible to beat traditional model-free RL methods with an evolutionary online planning approach, although we are not yet able to consistently beat or match the learned expert model presented in World Models [6].</p>
<p>Methods</p>
<p>Mean scores DQN [16] 343 ± 18 Non-Iterative Random Model 356 ± 177 A3C (Continuous) [9] 591 ± 45 Iterative Model (5 iterations, 15 gen., 20 horizon) 708 ± 195 Non-Iterative -Expert Model 765 ± 102 World Model [6] 906 ± 21 Table 1. CarRacing-v0 approaches with mean scores over 100 trials. Our approaches are shown in bold.</p>
<p>Discussion and Future Work</p>
<p>While the agent reaches a decent score, it does fail occasionally. It usually happens when the agent is unable to correct itself due to loss of friction during turns at sharp corners with high speed. Compared to the expert model that enacts conservative driving-behavior, the current iterative model prefers more risky driving at high speed. Possibly, the expert policy has learned to slow down at corners, which helps maximize the reward. On the other hand, our planning agent does not seem to have explored sufficient rollouts of this kind to make the MD-RNN learn to associate higher rewards with slower driving when approaching corners.</p>
<p>Another issue occurs when the agent approaches the right corners. In many cases, the agent can complete right corners though there are times where the agent does not know whether to turn or not. In these scenarios, the agent usually brakes or slows down while trying to navigate the race track in a sensible direction. This phenomenon is likely due to the right turns being underrepresented in the generated tracks that are biased towards containing mainly left turns. Consequently, the MDN-RNN is unable to represent right turns in the simulated environment compared to other frequently occurring segments of the track. Arguably, both issues resolve by running more iterative training iterations. However, it also depends on how often the issues arise in the generated rollouts. Noteworthy, the issues occurred more often in the random model compared to the iterative model, which indicates that the iterative training procedure can be an effective method of improving the world model.</p>
<p>Fig. 2 .
2Evolutionary Planning in Latent Space (EPLS).</p>
<p>Fig. 7 .
7Left: max planning generations vs. mean rewards. Right: Horizon planning length vs. mean reward. While a minimum number of generations and horizon length are necessary for the agent to plan well, increasing these values further does not increase the performance of the agent.
AcknowledgmentsWe would like to thank Mathias Kristian Kyndlo Löwe for helping us with computational infrastructure. A special thanks go to Corentin Tallec and his team for providing the PyTorch open-source implementation of World Models[6]. We also thank Simon Lucas, Chris Bamford, and Alexander Dockhorn for helpful suggestions. This project was supported by a DFF Sapere Aude Starting Grant and by the Danish Ministry of Education and Science, Digital Pilot Hub and Skylab Digital.
Neural game engine: Accurate learning ofgeneralizable forward models from pixels. C Bamford, S Lucas, arXiv:2003.10520arXiv preprintBamford, C., Lucas, S.: Neural game engine: Accurate learning ofgeneraliz- able forward models from pixels. arXiv preprint arXiv:2003.10520 (2020)</p>
<p>An overview of planning under uncertainty. In: Artificial intelligence today. J Blythe, SpringerBlythe, J.: An overview of planning under uncertainty. In: Artificial intelli- gence today, pp. 85-110. Springer (1999)</p>
<p>A survey of monte carlo tree search methods. C B Browne, E Powley, D Whitehouse, S M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in games. 41Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., Colton, S.: A sur- vey of monte carlo tree search methods. IEEE Transactions on Computa- tional Intelligence and AI in games 4(1), 1-43 (2012)</p>
<p>Rolling horizon evolutionary algorithms for general video game playing. R D Gaina, S Devlin, S M Lucas, D Perez-Liebana, arXiv:2003.12331arXiv preprintGaina, R.D., Devlin, S., Lucas, S.M., Perez-Liebana, D.: Rolling horizon evolutionary algorithms for general video game playing. arXiv preprint arXiv:2003.12331 (2020)</p>
<p>Population seeding techniques for rolling horizon evolution in general video game playing. R D Gaina, S M Lucas, D Pérez-Liébana, 2017 IEEE Congress on Evolutionary Computation (CEC). IEEEGaina, R.D., Lucas, S.M., Pérez-Liébana, D.: Population seeding tech- niques for rolling horizon evolution in general video game playing. In: 2017 IEEE Congress on Evolutionary Computation (CEC). pp. 1956-1963. IEEE (2017)</p>
<p>D Ha, J Schmidhuber, arXiv:1803.10122World models. arXiv preprintHa, D., Schmidhuber, J.: World models. arXiv preprint arXiv:1803.10122 (2018)</p>
<p>D Hafner, T Lillicrap, J Ba, M Norouzi, arXiv:1912.01603Dream to control: Learning behaviors by latent imagination. arXiv preprintHafner, D., Lillicrap, T., Ba, J., Norouzi, M.: Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603 (2019)</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International Conference on Machine Learning. PMLRHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., Davidson, J.: Learning latent dynamics for planning from pixels. In: International Conference on Machine Learning. pp. 2555-2565. PMLR (2019)</p>
<p>S Jang, J Min, C Lee, Reinforcement car racing with a3c. Jang, S., Min, J., Lee, C.: Reinforcement car racing with a3c (2017)</p>
<p>Uncertaintyaware reinforcement learning for collision avoidance. G Kahn, A Villaflor, V Pong, P Abbeel, S Levine, arXiv:1702.01182arXiv preprintKahn, G., Villaflor, A., Pong, V., Abbeel, P., Levine, S.: Uncertainty- aware reinforcement learning for collision avoidance. arXiv preprint arXiv:1702.01182 (2017)</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. arXiv preprintKingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013)</p>
<p>O Klimov, Carracing-v0. Klimov, O.: Carracing-v0 (2016), https://gym.openai.com/envs/ CarRacing-v0/</p>
<p>Game-playing and game-learning automata. D Michie, Advances in programming and non-numerical computation. ElsevierMichie, D.: Game-playing and game-learning automata. In: Advances in programming and non-numerical computation, pp. 183-200. Elsevier (1966)</p>
<p>Bootstrapped model learning and error correction for planning with uncertainty in model-based rl. A Ovalle, S M Lucas, arXiv:2004.07155arXiv preprintOvalle, A., Lucas, S.M.: Bootstrapped model learning and error cor- rection for planning with uncertainty in model-based rl. arXiv preprint arXiv:2004.07155 (2020)</p>
<p>Rolling horizon evolution versus tree search for navigation in single-player real-time games. D Perez, S Samothrakis, S Lucas, P Rohlfshagen, Proceedings of the 15th annual conference on Genetic and evolutionary computation. the 15th annual conference on Genetic and evolutionary computationPerez, D., Samothrakis, S., Lucas, S., Rohlfshagen, P.: Rolling horizon evo- lution versus tree search for navigation in single-player real-time games. In: Proceedings of the 15th annual conference on Genetic and evolutionary computation. pp. 351-358 (2013)</p>
<p>Deep-q learning for box2d racecar rl problem. L Prieur, Prieur, L.: Deep-q learning for box2d racecar rl problem. (2017), https: //goo.gl/VpDqSw</p>
<p>J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, arXiv:1911.08265Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprintSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al.: Mas- tering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265 (2019)</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 5297587Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driess- che, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.: Mastering the game of go with deep neural networks and tree search. nature 529(7587), 484-489 (2016)</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, arXiv:1712.01815arXiv preprintSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815 (2017)</p>
<p>Enhancing rolling horizon evolution with policy and value networks. X Tong, W Liu, B Li, 2019 IEEE Conference on Games (CoG). IEEETong, X., Liu, W., Li, B.: Enhancing rolling horizon evolution with policy and value networks. In: 2019 IEEE Conference on Games (CoG). pp. 1-8. IEEE (2019)</p>            </div>
        </div>

    </div>
</body>
</html>
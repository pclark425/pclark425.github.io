<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1127 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1127</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1127</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-53102049</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1810.12162v2.pdf" target="_blank">Model-Based Active Exploration</a></p>
                <p><strong>Paper Abstract:</strong> Efficient exploration is an unsolved problem in Reinforcement Learning. We introduce Model-Based Active eXploration (MAX), an algorithm that actively explores the environment. It minimizes data required to comprehensively model the environment by planning to observe novel events, instead of merely reacting to novelty encountered by chance. Non-stationarity induced by traditional exploration bonus techniques is avoided by constructing fresh exploration policies only at time of action. In semi-random toy environments where directed exploration is critical to make progress, our algorithm is at least an order of magnitude more efficient than strong baselines.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1127.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1127.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-Based Active eXploration (MAX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based active exploration algorithm that uses an ensemble of learned forward dynamics models to estimate information gain (via Jensen-Shannon divergence) and plans actions (with MCTS + Thompson sampling) to maximize expected information about unknown transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAX</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Ensemble-based model-based RL agent: ensemble of M learned forward models (in experiments M=3, fully-connected neural networks outputting categorical next-state distributions) trained on collected transitions; constructs an exploration MDP where the reward is utility U(s,a) (JSD among ensemble next-state predictions); searches for exploration policies using open-loop planning with Monte Carlo Tree Search (25 planning iterations, 5 random trajectories per expansion) and Thompson sampling; retrains ensemble after environment steps and discards/recomputes policy to avoid non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-gain maximization via ensemble disagreement (Jensen-Shannon Divergence) combined with model-based planning (active learning / active experimental design).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses an approximate Bayesian prior represented by an ensemble of models; computes per-state-action utility U(s,a)=JSD over ensemble next-state predictions to quantify expected information gain; plans (MCTS with Thompson sampling) in the surrogate exploration MDP to select action sequences expected to resolve model disagreement (i.e., seek learnable unknowns); retrains ensemble on observed transitions and re-plans frequently to adapt to updated beliefs; stochasticity (risk) appears as uniform model confusion and is de-prioritized relative to conflicting model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random Chain Environment (Chain of length N, default N=50) and Continuous Mountain Car (continuous 2D state, 1D action)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics (learned), discrete chain with deterministic transitions except optional stochastic trap variant; sparse external rewards (pure-exploration setting—external reward ignored); also continuous, low-dimensional control environment (Mountain Car) tested; environments are fully observable MDPs but dynamics are initially unknown to agent.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Chain (default N=50): 50 discrete states, 2 actions (LEFT/RIGHT), episode length N+9 (59 steps), 2N possible transitions; stochastic trap variant where leftmost state transitions are probabilistic; Continuous Mountain Car: 2D continuous state, 1D continuous action (standard Mountain Car dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>MAX explored ~100% of chain transitions in ~15 episodes (median over runs); ensemble models reach near-perfect accuracy in <20 episodes; in comparisons MAX achieved full coverage far faster than baselines (see below).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (reactive methods) reached ~40% of transitions in 60 episodes (median) on the N=50 chain; random exploration probability to reach far-right states is O(2^{-N}) (not competitive).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very sample-efficient in the Chain: full coverage (~100% of transitions) in ≈15 episodes; models converge (<1% error) in <20 episodes. Performance scales to chain lengths 20–100 with similar behavior (more episodes required but same algorithm/hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Pure exploration objective (no external reward); no exploitation: agent optimizes expected information gain only. Avoids over-commitment by re-planning policies from the ensemble at each step and discarding stale policies; uses ensemble disagreement to direct exploration rather than prediction-error curiosity bonuses which can conflate noise with learnable uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Exploration Bonus DQN (count/prediction-error based exploration bonus) and Bootstrapped DQN (bootstrap heads approximating posterior / deep posterior sampling), plus random exploration baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MAX (active experimental design via model-ensemble JSD + planning) is at least an order of magnitude more sample-efficient than strong reactive baselines on the randomized Chain environment (100% coverage in ≈15 episodes vs ~40% at 60 episodes for baselines). MAX scales across chain lengths (20–100) without hyperparameter retuning, distinguishes learnable unknowns from stochastic risk (though trap noise slows it), and transfers to continuous control (Mountain Car) where it finds exploratory behaviors to reach goal regions without external reward. Robustness tests show more ensemble members, rollouts, and planning iterations improve performance, but the algorithm works with minimal settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Assumptions ignore second-order effects (visiting transitions changes future utilities) which can cause planning-to-loop behavior (oscillations between uncertain state pairs). Planning horizon is limited in practice (MCTS typically looks ~5 steps ahead with 25 iterations) so the method relies on an 'uncertainty gradient' to act near-greedily; stochastic trap near the start increases model-output variance and makes local uncertainty dominant, slowing progress toward distant uncertain regions; performance degrades but remains able to explore. Model bias and approximation in the ensemble can still affect behavior; computational cost of planning and ensemble training is non-zero though far cheaper than real environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Active Exploration', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1127.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1127.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrapped Deep Q-Network (Bootstrapped DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reactive exploration method using multiple Q-value network 'heads' trained on shared replay to approximate posterior sampling; a head is sampled per episode to induce temporally extended (Deep) exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Exploration via Bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bootstrapped DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free DQN extension with multiple output heads (10 heads in experiments) sharing a common network body; before each episode a head is selected and agent acts greedily with respect to that head; heads trained on shared replay (no bootstrap sampling used in this paper's implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Approximate posterior sampling (bootstrap ensemble of Q-heads) to induce deep exploration (Thompson-sampling-like behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Reactive: exploration arises by randomly selecting one Q-head per episode (stochastic policy selection); novel transitions added to replay may lead individual heads to overestimate values, biasing future policies toward those transitions when that head is sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random Chain Environment (Chain length N, default N=50)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics, discrete state space, sparse external reward; pure-exploration evaluation (external reward disregarded).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Chain N=50: 50 states, 2 actions, episode length 59 steps; also warm-up of 3 random episodes used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reactive baselines including Bootstrapped DQN reached ~40% of transitions after 60 episodes (median) on the N=50 Random Chain environment (significantly worse than MAX).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low in the Chain task relative to MAX: ~40% coverage at 60 episodes (median) versus MAX's ~100% at ~15 episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Any-time optimal RL balancing exploration and exploitation per-step via value-estimation; in this pure-exploration setting there is no external reward exploited, so the method operates as a reactive intrinsic exploration mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to MAX (model-based active exploration) and Exploration Bonus DQN in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Used as a strong reactive baseline; shows that reactive posterior-sampling-like approaches are far less sample-efficient in environments where directed planning for novel transitions is critical (Random Chain). Bootstrapped DQN did not match MAX's active planning-based information-seeking performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reactive behavior: does not explicitly plan to seek information, so in environments that require directed long-horizon exploration (e.g., Chain with swapped action effects) bootstrapped heads fail to find far-off transitions efficiently; in these experiments bootstrap sampling (per-head dataset differences) was not used, which may reduce diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Active Exploration', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1127.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1127.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exploration Bonus DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploration Bonus Deep Q-Network (count/prediction-error bonuses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reactive exploration variant of DQN in which transitions receive an intrinsic bonus (here implemented via an oracle transition table) to temporarily boost their value and encourage revisits to novel transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying Count-Based Exploration and Intrinsic Motivation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Exploration Bonus DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DQN augmented with exploration bonuses for novel transitions; in the paper's Chain experiments bonuses are provided by an oracle (transition look-up table) equivalent to one-shot learning of transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Intrinsic-reward-based exploration (exploration bonus derived from counts or prediction errors) — reactive intrinsic motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Reactive: when a transition is observed for the first time an exploration bonus is added to its reward estimates, temporarily increasing its Q-value and biasing policy to revisit novel transitions; the bonus must be later unlearned once novelty wears off (over-commitment risk).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random Chain Environment (Chain length N, default N=50)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics, discrete states, sparse external reward; pure-exploration experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Chain N=50: 50 states, 2 actions, episode length N+9 (59 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reached approximately 40% of transitions in 60 episodes (median) on the N=50 chain (similar to Bootstrapped DQN and substantially worse than MAX).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relatively low on the Chain task compared to MAX; reactive bonuses cause over-commitment and inefficient coverage in environments requiring directed long-horizon exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Reactive optimism-in-face-of-uncertainty approach: bonuses bias value estimates to encourage visiting novel states but require de-biasing over time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against MAX and Bootstrapped DQN in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>When bonuses are reactive and local, agents may over-commit and fail to direct exploration toward long-horizon uncertain transitions; performs much worse than MAX on the randomized Chain environment despite oracle bonuses in this implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Non-stationarity of intrinsic bonuses leads to over-commitment; methods that rely on prediction errors can conflate noise (stochastic trap) with learnable uncertainty and repeatedly visit risky but uninformative states.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Active Exploration', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1127.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1127.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIME (Variational Information Maximizing Exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian-inspired intrinsic-reward method that uses a Bayesian neural network to estimate information gain from transitions and biases policy search with that intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VIME: Variational Information Maximizing Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VIME</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a Bayesian Neural Network belief over dynamics and computes an approximation of posterior change (information gain) as an intrinsic reward (via variational inference) to guide exploration; used in prior work (cited) as a reactive intrinsic motivation within policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximization estimated via variational Bayesian networks (intrinsic reward).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Computes information gain per observed transition to provide intrinsic reward that biases the policy toward transitions that reduce model uncertainty; reactive (intrinsic-reward augmented) rather than planning in a surrogate MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances intrinsic information-gain reward with external reward in policy optimization (method is used as an auxiliary intrinsic objective).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Cited as a Bayesian approach that evaluates the value of experience using information gain, but it is reactive (adds intrinsic reward) rather than actively planning for information.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reactive intrinsic-reward methods can suffer from non-stationarity and may conflate stochasticity with learnable uncertainty; cited as related work rather than used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Active Exploration', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1127.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1127.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Planning-to-be-Surprised (Sun et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian framework for curiosity-driven exploration that uses expected information gain and dynamic programming to plan experiments; prior work limited to small tabular MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian optimal exploration (Sun et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Theoretical dynamic-programming approach that maximizes expected information gain (additive in expectation) over sequences of experiments in small/tractable MDPs (e.g., Dirichlet priors, tabular transitions).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Optimal Bayesian experimental design / expected information gain maximization via dynamic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Plans action sequences by maximizing the expected KL divergence (information gain) between prior and posterior over transition models; computes value of experiments ahead of execution (full Bayesian planning) when tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly optimizes information gain (exploration) potentially at the expense of external reward; computationally tractable only in small/structured settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Proves information gain is additive in expectation and proposes dynamic programming solutions for optimal curiosity-driven exploration; cited as a theoretical predecessor to MAX's principles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Approach is computationally intractable for realistic RL tasks (limited to small tabular MDPs), motivating approximations like MAX.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Active Exploration', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1127.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1127.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hester & Stone (2012)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsically Motivated Model Learning for Developing Curious Robots</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesis-resolving model-based RL exploration algorithm that plans to resolve uncertainty using model predictions and intrinsic motivation; shown to outperform prediction-error methods in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Intrinsically Motivated Model Learning for Developing Curious Robots</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hester & Stone model-based intrinsic exploration</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based exploration approach that uses model predictions and intrinsic rewards (information-seeking) to guide exploration, with planning performed over model predictions (their implementation uses mean predictions of the ensemble and additional bonuses).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Model-based intrinsic motivation / disagreement-driven exploration (planning to reduce hypothesis uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Plans using model predictions (mean prediction of ensemble) augmented with disagreement-based utility and optionally state-distance bonuses to select actions expected to improve model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Focuses on intrinsic model-learning improvement; reactive vs planning details vary in implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Cited as related model-based intrinsic exploration work that uses planning to resolve hypotheses but differs from MAX in using mean predictions and additional bonuses; provides empirical evidence that model-based, hypothesis-resolving exploration can outperform error-based intrinsic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Different planning/utility choices can affect behavior; cited for context rather than directly used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Active Exploration', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>VIME: Variational Information Maximizing Exploration <em>(Rating: 2)</em></li>
                <li>Deep Exploration via Bootstrapped DQN <em>(Rating: 2)</em></li>
                <li>Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments <em>(Rating: 2)</em></li>
                <li>Unifying Count-Based Exploration and Intrinsic Motivation <em>(Rating: 2)</em></li>
                <li>Intrinsically Motivated Model Learning for Developing Curious Robots <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1127",
    "paper_id": "paper-53102049",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "MAX",
            "name_full": "Model-Based Active eXploration (MAX)",
            "brief_description": "A model-based active exploration algorithm that uses an ensemble of learned forward dynamics models to estimate information gain (via Jensen-Shannon divergence) and plans actions (with MCTS + Thompson sampling) to maximize expected information about unknown transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MAX",
            "agent_description": "Ensemble-based model-based RL agent: ensemble of M learned forward models (in experiments M=3, fully-connected neural networks outputting categorical next-state distributions) trained on collected transitions; constructs an exploration MDP where the reward is utility U(s,a) (JSD among ensemble next-state predictions); searches for exploration policies using open-loop planning with Monte Carlo Tree Search (25 planning iterations, 5 random trajectories per expansion) and Thompson sampling; retrains ensemble after environment steps and discards/recomputes policy to avoid non-stationarity.",
            "adaptive_design_method": "Information-gain maximization via ensemble disagreement (Jensen-Shannon Divergence) combined with model-based planning (active learning / active experimental design).",
            "adaptation_strategy_description": "Uses an approximate Bayesian prior represented by an ensemble of models; computes per-state-action utility U(s,a)=JSD over ensemble next-state predictions to quantify expected information gain; plans (MCTS with Thompson sampling) in the surrogate exploration MDP to select action sequences expected to resolve model disagreement (i.e., seek learnable unknowns); retrains ensemble on observed transitions and re-plans frequently to adapt to updated beliefs; stochasticity (risk) appears as uniform model confusion and is de-prioritized relative to conflicting model predictions.",
            "environment_name": "Random Chain Environment (Chain of length N, default N=50) and Continuous Mountain Car (continuous 2D state, 1D action)",
            "environment_characteristics": "Unknown transition dynamics (learned), discrete chain with deterministic transitions except optional stochastic trap variant; sparse external rewards (pure-exploration setting—external reward ignored); also continuous, low-dimensional control environment (Mountain Car) tested; environments are fully observable MDPs but dynamics are initially unknown to agent.",
            "environment_complexity": "Chain (default N=50): 50 discrete states, 2 actions (LEFT/RIGHT), episode length N+9 (59 steps), 2N possible transitions; stochastic trap variant where leftmost state transitions are probabilistic; Continuous Mountain Car: 2D continuous state, 1D continuous action (standard Mountain Car dynamics).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "MAX explored ~100% of chain transitions in ~15 episodes (median over runs); ensemble models reach near-perfect accuracy in &lt;20 episodes; in comparisons MAX achieved full coverage far faster than baselines (see below).",
            "performance_without_adaptation": "Baselines (reactive methods) reached ~40% of transitions in 60 episodes (median) on the N=50 chain; random exploration probability to reach far-right states is O(2^{-N}) (not competitive).",
            "sample_efficiency": "Very sample-efficient in the Chain: full coverage (~100% of transitions) in ≈15 episodes; models converge (&lt;1% error) in &lt;20 episodes. Performance scales to chain lengths 20–100 with similar behavior (more episodes required but same algorithm/hyperparameters).",
            "exploration_exploitation_tradeoff": "Pure exploration objective (no external reward); no exploitation: agent optimizes expected information gain only. Avoids over-commitment by re-planning policies from the ensemble at each step and discarding stale policies; uses ensemble disagreement to direct exploration rather than prediction-error curiosity bonuses which can conflate noise with learnable uncertainty.",
            "comparison_methods": "Exploration Bonus DQN (count/prediction-error based exploration bonus) and Bootstrapped DQN (bootstrap heads approximating posterior / deep posterior sampling), plus random exploration baselines.",
            "key_results": "MAX (active experimental design via model-ensemble JSD + planning) is at least an order of magnitude more sample-efficient than strong reactive baselines on the randomized Chain environment (100% coverage in ≈15 episodes vs ~40% at 60 episodes for baselines). MAX scales across chain lengths (20–100) without hyperparameter retuning, distinguishes learnable unknowns from stochastic risk (though trap noise slows it), and transfers to continuous control (Mountain Car) where it finds exploratory behaviors to reach goal regions without external reward. Robustness tests show more ensemble members, rollouts, and planning iterations improve performance, but the algorithm works with minimal settings.",
            "limitations_or_failures": "Assumptions ignore second-order effects (visiting transitions changes future utilities) which can cause planning-to-loop behavior (oscillations between uncertain state pairs). Planning horizon is limited in practice (MCTS typically looks ~5 steps ahead with 25 iterations) so the method relies on an 'uncertainty gradient' to act near-greedily; stochastic trap near the start increases model-output variance and makes local uncertainty dominant, slowing progress toward distant uncertain regions; performance degrades but remains able to explore. Model bias and approximation in the ensemble can still affect behavior; computational cost of planning and ensemble training is non-zero though far cheaper than real environment interactions.",
            "uuid": "e1127.0",
            "source_info": {
                "paper_title": "Model-Based Active Exploration",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Bootstrapped DQN",
            "name_full": "Bootstrapped Deep Q-Network (Bootstrapped DQN)",
            "brief_description": "A reactive exploration method using multiple Q-value network 'heads' trained on shared replay to approximate posterior sampling; a head is sampled per episode to induce temporally extended (Deep) exploration.",
            "citation_title": "Deep Exploration via Bootstrapped DQN",
            "mention_or_use": "use",
            "agent_name": "Bootstrapped DQN",
            "agent_description": "Model-free DQN extension with multiple output heads (10 heads in experiments) sharing a common network body; before each episode a head is selected and agent acts greedily with respect to that head; heads trained on shared replay (no bootstrap sampling used in this paper's implementation).",
            "adaptive_design_method": "Approximate posterior sampling (bootstrap ensemble of Q-heads) to induce deep exploration (Thompson-sampling-like behavior).",
            "adaptation_strategy_description": "Reactive: exploration arises by randomly selecting one Q-head per episode (stochastic policy selection); novel transitions added to replay may lead individual heads to overestimate values, biasing future policies toward those transitions when that head is sampled.",
            "environment_name": "Random Chain Environment (Chain length N, default N=50)",
            "environment_characteristics": "Unknown transition dynamics, discrete state space, sparse external reward; pure-exploration evaluation (external reward disregarded).",
            "environment_complexity": "Chain N=50: 50 states, 2 actions, episode length 59 steps; also warm-up of 3 random episodes used.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reactive baselines including Bootstrapped DQN reached ~40% of transitions after 60 episodes (median) on the N=50 Random Chain environment (significantly worse than MAX).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Low in the Chain task relative to MAX: ~40% coverage at 60 episodes (median) versus MAX's ~100% at ~15 episodes.",
            "exploration_exploitation_tradeoff": "Any-time optimal RL balancing exploration and exploitation per-step via value-estimation; in this pure-exploration setting there is no external reward exploited, so the method operates as a reactive intrinsic exploration mechanism.",
            "comparison_methods": "Compared to MAX (model-based active exploration) and Exploration Bonus DQN in this paper.",
            "key_results": "Used as a strong reactive baseline; shows that reactive posterior-sampling-like approaches are far less sample-efficient in environments where directed planning for novel transitions is critical (Random Chain). Bootstrapped DQN did not match MAX's active planning-based information-seeking performance.",
            "limitations_or_failures": "Reactive behavior: does not explicitly plan to seek information, so in environments that require directed long-horizon exploration (e.g., Chain with swapped action effects) bootstrapped heads fail to find far-off transitions efficiently; in these experiments bootstrap sampling (per-head dataset differences) was not used, which may reduce diversity.",
            "uuid": "e1127.1",
            "source_info": {
                "paper_title": "Model-Based Active Exploration",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Exploration Bonus DQN",
            "name_full": "Exploration Bonus Deep Q-Network (count/prediction-error bonuses)",
            "brief_description": "A reactive exploration variant of DQN in which transitions receive an intrinsic bonus (here implemented via an oracle transition table) to temporarily boost their value and encourage revisits to novel transitions.",
            "citation_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
            "mention_or_use": "use",
            "agent_name": "Exploration Bonus DQN",
            "agent_description": "DQN augmented with exploration bonuses for novel transitions; in the paper's Chain experiments bonuses are provided by an oracle (transition look-up table) equivalent to one-shot learning of transitions.",
            "adaptive_design_method": "Intrinsic-reward-based exploration (exploration bonus derived from counts or prediction errors) — reactive intrinsic motivation.",
            "adaptation_strategy_description": "Reactive: when a transition is observed for the first time an exploration bonus is added to its reward estimates, temporarily increasing its Q-value and biasing policy to revisit novel transitions; the bonus must be later unlearned once novelty wears off (over-commitment risk).",
            "environment_name": "Random Chain Environment (Chain length N, default N=50)",
            "environment_characteristics": "Unknown transition dynamics, discrete states, sparse external reward; pure-exploration experimental setup.",
            "environment_complexity": "Chain N=50: 50 states, 2 actions, episode length N+9 (59 steps).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reached approximately 40% of transitions in 60 episodes (median) on the N=50 chain (similar to Bootstrapped DQN and substantially worse than MAX).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Relatively low on the Chain task compared to MAX; reactive bonuses cause over-commitment and inefficient coverage in environments requiring directed long-horizon exploration.",
            "exploration_exploitation_tradeoff": "Reactive optimism-in-face-of-uncertainty approach: bonuses bias value estimates to encourage visiting novel states but require de-biasing over time.",
            "comparison_methods": "Compared against MAX and Bootstrapped DQN in experiments.",
            "key_results": "When bonuses are reactive and local, agents may over-commit and fail to direct exploration toward long-horizon uncertain transitions; performs much worse than MAX on the randomized Chain environment despite oracle bonuses in this implementation.",
            "limitations_or_failures": "Non-stationarity of intrinsic bonuses leads to over-commitment; methods that rely on prediction errors can conflate noise (stochastic trap) with learnable uncertainty and repeatedly visit risky but uninformative states.",
            "uuid": "e1127.2",
            "source_info": {
                "paper_title": "Model-Based Active Exploration",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "VIME",
            "name_full": "VIME (Variational Information Maximizing Exploration)",
            "brief_description": "A Bayesian-inspired intrinsic-reward method that uses a Bayesian neural network to estimate information gain from transitions and biases policy search with that intrinsic reward.",
            "citation_title": "VIME: Variational Information Maximizing Exploration",
            "mention_or_use": "mention",
            "agent_name": "VIME",
            "agent_description": "Maintains a Bayesian Neural Network belief over dynamics and computes an approximation of posterior change (information gain) as an intrinsic reward (via variational inference) to guide exploration; used in prior work (cited) as a reactive intrinsic motivation within policy optimization.",
            "adaptive_design_method": "Information gain maximization estimated via variational Bayesian networks (intrinsic reward).",
            "adaptation_strategy_description": "Computes information gain per observed transition to provide intrinsic reward that biases the policy toward transitions that reduce model uncertainty; reactive (intrinsic-reward augmented) rather than planning in a surrogate MDP.",
            "environment_name": null,
            "environment_characteristics": null,
            "environment_complexity": null,
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Balances intrinsic information-gain reward with external reward in policy optimization (method is used as an auxiliary intrinsic objective).",
            "comparison_methods": null,
            "key_results": "Cited as a Bayesian approach that evaluates the value of experience using information gain, but it is reactive (adds intrinsic reward) rather than actively planning for information.",
            "limitations_or_failures": "Reactive intrinsic-reward methods can suffer from non-stationarity and may conflate stochasticity with learnable uncertainty; cited as related work rather than used in experiments.",
            "uuid": "e1127.3",
            "source_info": {
                "paper_title": "Model-Based Active Exploration",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Planning-to-be-Surprised (Sun et al.)",
            "name_full": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
            "brief_description": "A Bayesian framework for curiosity-driven exploration that uses expected information gain and dynamic programming to plan experiments; prior work limited to small tabular MDPs.",
            "citation_title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
            "mention_or_use": "mention",
            "agent_name": "Bayesian optimal exploration (Sun et al.)",
            "agent_description": "Theoretical dynamic-programming approach that maximizes expected information gain (additive in expectation) over sequences of experiments in small/tractable MDPs (e.g., Dirichlet priors, tabular transitions).",
            "adaptive_design_method": "Optimal Bayesian experimental design / expected information gain maximization via dynamic programming.",
            "adaptation_strategy_description": "Plans action sequences by maximizing the expected KL divergence (information gain) between prior and posterior over transition models; computes value of experiments ahead of execution (full Bayesian planning) when tractable.",
            "environment_name": null,
            "environment_characteristics": null,
            "environment_complexity": null,
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Explicitly optimizes information gain (exploration) potentially at the expense of external reward; computationally tractable only in small/structured settings.",
            "comparison_methods": null,
            "key_results": "Proves information gain is additive in expectation and proposes dynamic programming solutions for optimal curiosity-driven exploration; cited as a theoretical predecessor to MAX's principles.",
            "limitations_or_failures": "Approach is computationally intractable for realistic RL tasks (limited to small tabular MDPs), motivating approximations like MAX.",
            "uuid": "e1127.4",
            "source_info": {
                "paper_title": "Model-Based Active Exploration",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Hester & Stone (2012)",
            "name_full": "Intrinsically Motivated Model Learning for Developing Curious Robots",
            "brief_description": "A hypothesis-resolving model-based RL exploration algorithm that plans to resolve uncertainty using model predictions and intrinsic motivation; shown to outperform prediction-error methods in prior work.",
            "citation_title": "Intrinsically Motivated Model Learning for Developing Curious Robots",
            "mention_or_use": "mention",
            "agent_name": "Hester & Stone model-based intrinsic exploration",
            "agent_description": "Model-based exploration approach that uses model predictions and intrinsic rewards (information-seeking) to guide exploration, with planning performed over model predictions (their implementation uses mean predictions of the ensemble and additional bonuses).",
            "adaptive_design_method": "Model-based intrinsic motivation / disagreement-driven exploration (planning to reduce hypothesis uncertainty).",
            "adaptation_strategy_description": "Plans using model predictions (mean prediction of ensemble) augmented with disagreement-based utility and optionally state-distance bonuses to select actions expected to improve model quality.",
            "environment_name": null,
            "environment_characteristics": null,
            "environment_complexity": null,
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Focuses on intrinsic model-learning improvement; reactive vs planning details vary in implementation.",
            "comparison_methods": null,
            "key_results": "Cited as related model-based intrinsic exploration work that uses planning to resolve hypotheses but differs from MAX in using mean predictions and additional bonuses; provides empirical evidence that model-based, hypothesis-resolving exploration can outperform error-based intrinsic methods.",
            "limitations_or_failures": "Different planning/utility choices can affect behavior; cited for context rather than directly used.",
            "uuid": "e1127.5",
            "source_info": {
                "paper_title": "Model-Based Active Exploration",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "VIME: Variational Information Maximizing Exploration",
            "rating": 2,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "Deep Exploration via Bootstrapped DQN",
            "rating": 2,
            "sanitized_title": "deep_exploration_via_bootstrapped_dqn"
        },
        {
            "paper_title": "Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments",
            "rating": 2,
            "sanitized_title": "planning_to_be_surprised_optimal_bayesian_exploration_in_dynamic_environments"
        },
        {
            "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
            "rating": 2,
            "sanitized_title": "unifying_countbased_exploration_and_intrinsic_motivation"
        },
        {
            "paper_title": "Intrinsically Motivated Model Learning for Developing Curious Robots",
            "rating": 1,
            "sanitized_title": "intrinsically_motivated_model_learning_for_developing_curious_robots"
        }
    ],
    "cost": 0.01586325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Model-Based Active Exploration</p>
<p>Pranav Shyam pranav@nnaisense.com 
NNAISENSE
LuganoSwitzerland</p>
<p>Wojciech Jaśkowski wojciech@nnaisense.com 
NNAISENSE
LuganoSwitzerland</p>
<p>Faustino Gomez 
NNAISENSE
LuganoSwitzerland</p>
<p>Model-Based Active Exploration</p>
<p>Efficient exploration is an unsolved problem in Reinforcement Learning. We introduce Model-Based Active eXploration (MAX), an algorithm that actively explores the environment. It minimizes data required to comprehensively model the environment by planning to observe novel events, instead of merely reacting to novelty encountered by chance. Non-stationarity induced by traditional exploration bonus techniques is avoided by constructing fresh exploration policies only at time of action. In semi-random toy environments where directed exploration is critical to make progress, our algorithm is at least an order of magnitude more efficient than strong baselines.</p>
<p>Introduction</p>
<p>Exploration in large, high-dimensional Markov Decision Processes (MDPs) is an unsolved problem in Reinforcement Learning (RL). Current exploration methods (Osband et al., 2016;Bellemare et al., 2016;Houthooft et al., 2016;Pathak et al., 2017) are mostly reactive: the agent accidentally observes something "novel" and then decides to obtain more information about it. Further exploration in the vicinity of the novel state is carried out typically through an exploration bonus or intrinsic motivation reward which have to be unlearned once the novelty has worn off, making exploration inefficient-a problem we refer to as over-commitment.</p>
<p>However, exploration can also be active where the agent seeks out novelty based on its own "internal" estimate of what action sequences will lead to interesting transitions. This approach is inherently more powerful than reactive exploration, but requires a method to predict the consequences of actions and their degree of novelty. This problem can be formulated optimally in the Bayesian setting where the novelty of a given state transition can be measured by the disagreement between the next-state predictions made by members of a distribution over models of the environment. Unfortunately, this inference problem is intractable for any realistic RL task. To overcome this, we introduce Model-Based Active eXploration (MAX), an efficient algorithm, based on this principle, that approximates the idealized distribution using an ensemble of learned forward dynamics models. The algorithm identifies learnable unknowns or uncertainty representing novelty in the environment by measuring the amount of conflict between the predictions of the constituent models. It then attempts to seek uncertainty by constructing exploration policies that would resolve these conflicts using a novel policy evaluation technique. Crucially, the search for exploration policies is done entirely with the ensemble. Since, such models are generally several orders of magnitude cheaper in both cost and time than real environments, MAX avoids over-commitment by regularly learning fresh exploration policies. Unlearnable unknowns or risk, such as environment noise does not interfere with the process since stochasticity manifests itself as confusion among all models and not as a conflict. Therefore, random data is not interesting, despite its high information content.</p>
<p>While MAX can be used in conjunction with conventional action learning to maximize external reward, this paper is restricted to pure exploration: exploration disregarding or in the absence of external reward (Bubeck et al., 2009). This is both to better study the properties of the algorithm in isolation, irrespective of any particular external goal, and because it means that the learned models will be unbiased with respect to whatever downstream tasks they could potentially be used for.</p>
<p>Preliminary experiments show that MAX is significantly more efficient than reactive exploration techniques which use exploration bonuses or posterior sampling. They also strongly suggest that MAX is not sensitive to noise, scalable and robust to key-parameters.</p>
<p>The next section provides the theoretical foundation for this work and introduces the MAX algorithm.</p>
<p>Model-Based Active Exploration</p>
<p>The key idea behind our approach to true active exploration in the environment, or the external MDP, is to use a model, learned from the environment, as a surrogate or exploration MDP where the novelty of possible future transitions can be estimated before they have actually been encountered by the agent in the environment. The next section provides the formal context for the conceptual foundation of this work presented in Section 2.2, and Section 2.3 introduces an approximation to this idealized treatment that implements efficient model-based active exploration.</p>
<p>Problem Setup</p>
<p>Consider an environment represented by a Markov Decision Process (MDP) defined as the tuple (S, A, T , R, ρ 0 ), where S is the set of states, A is the set of actions, T : S × A × S → R + is the probabilistic transition function, R : S × A → R is the reward function, ρ 0 : S → R + is the distribution of the initial states, s 0 .</p>
<p>The objective of pure exploration is to learn a policy, π : S × A → R + , that selects actions probabilistically to efficiently accumulate information about the environment, irrespective of R. This problem is equivalent to the problem of learning an accurate model,T , of the transition function, T , while minimizing the number of state transitions, ζ, required to do so, where ζ = (s, a, s ) and s is the state resulting from action a being taken in state s. Let Γ be the space of all possible transition functions and P(Γ) be a probability distribution over transition functions that captures the current belief of how the environment behaves, with corresponding density function p(T ).</p>
<p>Given an accurateT , the policy can take actions in an exploration MDP (S, A,T , R, ρ 0 ), having the same set of states and actions (S, A), as if it were the external MDP.</p>
<p>Utility of an Exploration Policy</p>
<p>In the standard RL setting, a policy would be learned to take actions that maximize some function of the external reward received from the environment according to R, i.e. the return. Because pure active exploration cannot rely on R, and T is unknown, there needs to be a way to compute the amount of information conveyed about the environment by unseen state transitions that could be caused by the policy, which requires using a model of T , instead T itself to "internally" evaluate possible future transitions.</p>
<p>From a Bayesian perspective, this can be captured by the KL divergence between P(Γ), the (prior) distribution over transition functions before the transition, ζ, and P(Γ|ζ), the posterior distribution after ζ has occurred. This is commonly referred to as Information Gain, which we abbreviate as the utility, u(ζ), of a transition ζ: u(s, a, s ) = u(ζ) = D KL (P(Γ|ζ) P(Γ)).</p>
<p>(1)</p>
<p>The utility can be understood as the number of extra bits needed to specify the posterior relative to the prior, roughly the number of bits of information that was gathered about the external MDP. Given, u(ζ), it is now possible to compute the utility of the exploration policy, U (π), which is the expected utility over all possible transitions when π is used:
U (π) = E ζ∼P(·|π) [u(ζ)] ,(2)
which can be expanded into (see Appendix A):
U (π) = ET ∼P(Γ) E s,a∼P(S,A|π,T ) [U(s, a)] ,(3)
where U(s, a) = s u(s, a, s )p(s |a, s) ds .</p>
<p>It turns out that U(s, a) is the same as (see Appendix B):
U(s, a) = JSD{P(S|s, a,T ) |T ∼ P(Γ)}(5)
where JSD is the Jensen-Shannon Divergence or Information Radius, which captures the amount of disagreement present in a space of distributions. Hence, the utility of the state-action pair, U(s, a), is the disagreement, in terms of JSD, among the next-state distributions of all possible transition functions, given s and a, weighted by their probability. This means that, U(s, a) can be used to assess the novelty of potential transitions using the prior, i.e. without having to actually execute the transition in external MDP.</p>
<p>Therefore, U can provide a signal with which to update the exploration policy in the same way that R is normally used to update an action policy. Stated another way, U is to the exploration MDP what the R is to the external MDP, so that maximizing U results in the optimal exploration policy for the exploration MDP just as maximizing R results in the optimal action policy.</p>
<p>The exploration MDP is then defined by the tuple (S, A, T E , U, ρ E 0 ), where the sets S and A are identical to those of the external MDP being explored, and the transition function T E is potentially a different member of Γ at each state transition. Finally, the initial state distribution ρ E 0 is set to ρ E 0 (s t ) = 1 such that start state of exploration MDP is always the current state in the environment, s t . It is important to understand for what follows in the next section that the prior, P(Γ), is used twice in Equation 5:</p>
<ol>
<li>
<p>To specify the transition dynamics. The prior determines a distribution, P(S, A|π,T , ρ 0 ), over the set of possible state-action pairs that can result by sequentially executing an infinite number of actions according to π using all of the transition functions, starting in all states distributed according to ρ 0 .</p>
</li>
<li>
<p>To obtain the utility for a particular transition. Each state-action pair in P(S, A|π,T , ρ 0 ), when input to all the transition functions in Γ form a set of predicted next state distributions {P(S|s, a,T ) |T ∼ P(Γ)}, and U(s, a) is the JSD of this set.</p>
</li>
</ol>
<p>Since, in general, performing these steps is intractable, the next section presents a practical method to implement exploration based on these principles.</p>
<p>Bootstrap Ensemble Approximation</p>
<p>The prior, P(Γ), can be approximated using a bootstrap ensemble of M learned transition functions or models that are each trained independently using different subsets of the history, H, consisting of state transitions experienced by the agent while exploring the external MDP (Efron, 2012). Therefore, while P(Γ) is uniform when the agent is initialized, thereafter it is conditioned on agent's history H and so that the the general form of the prior is P(Γ|H). For generalizations that are warranted by observed data, there is a good chance that the models make similar predictions. If the generalization is not warranted by observed data, then the models could disagree owing to their exposure to different parts of the data distribution.</p>
<p>Since the ensemble contains models that were trained to accurately approximate the data, these models have higher probabilities or weights in p(T ). The probabilities of inaccurate models are negligible, so they do not affect the properties of the distribution significantly and can safely be ignored. Hence randomly sampling from the ensemble can approximate the true distribution, P(Γ|H), even with relatively small sample sizes (Lakshminarayanan et al., 2017). Recent work suggests that it possible to do so even in high-dimensional state and action spaces (Kurutach et al., 2018;Chua et al., 2018).</p>
<p>Assuming all models fit their corresponding data equally well, then they all have the same weights and can therefore be treated equally. Given an M -model ensemble approximating, the exploration MDP can be approximated by randomly selecting one of the M of the models with equal probability at each state transition. 
π ← Solve EXPLORATIONMDP(S, A, T E , U, ρ E 0 ) find best exploration policy 6: a t ∼ π(s t ) select action 7:
s t+1 ∼ P(S|, s t , a t , T ) act in environment according to policy 8: 
H ← H ∪ {(s t , a t ,sU(s, a) = JSD{P(S|s, a,T ) |T ∼ P(Γ)} (6) = H ET ∼P(Γ) P(S|s, a,T ) − ET ∼P(Γ) H(P(S|s, a,T ))(7)
where H(·) denotes the entropy of the distribution. Equation 6 can be summarized as the difference between the entropy of the average and the average entropy and can be approximated by averaging over samples taken from the ensemble:
U(s, a) H 1 N N i=1 P(S|s, a,T i ) − 1 N N i=1 H(P(S|s, a,T i ))(8)
The entropies in Equation 8 can also be evaluated in closed form since many models output the next state representations such as one-hot or Gaussian for which this is possible.</p>
<p>Algorithm 1 gives the high-level pseudocode of MAX. This process is essentially a model-based RL algorithm with exploration as its objective. At each step a fresh policy is initialized and improved to maximise its return in the exploration MDP. The solution policy thus obtained is used to act in the external MDP to collect data. Training the ensemble with the gathered data at each step gives us the approximate posterior. This posterior can then be used as approximate prior in the next exploration step.</p>
<p>The amount of environment experience needed to find exploration policies is substantially reduced by learning policies using the models in the ensemble. If the regions that the exploration policy visits have less disagreement among the predictions of models, then it is likely that the dynamics of the models are very close to environment. Hence the policy can rely on such dynamics to reach certain regions of the environment. If the policy visits areas where there is a lot of disagreement, then the models are likely to be wrong. Although their predicted future is incorrect and the policy cannot rely on it, the policy still directs the agent to regions with novelty.</p>
<p>MAX is also self-correcting to the errors of models based on function approximators. If learning some aspects of the environment is difficult, it will manifest itself as disagreement in the ensemble.</p>
<p>Hence MAX would collect more data about those aspects to aid learning.</p>
<p>Ideally, both training the model ensemble and policy optimization can occur at each step in the environment. But in practice they can both just be updated from step to step, episode to episode or over multiple episodes to minimize cost. But as soon as the ensemble is trained, the policy has to be updated or discarded and retrained from scratch. We used a randomized version of the Chain Environment (Figure 1), designed to be hard to explore proposed by (Osband et al., 2016) which is a simplified generalization of RiverSwim (Strehl and Littman, 2005). The environment consists of N states in a series. Starting in the second state (state 1) of an N state chain, the agent can move either left or right at each step. An episode lasts for N + 9 steps, after which the agent is reset again to the second state. The agent is first given 3 episodes of warm up during which the actions are chosen randomly. Trying to move outside the chain results in staying in place. The agent is rewarded only for staying in the edge states: 0.001 and 1 for the leftmost and the rightmost state, respectively. To make the problem harder to exploit by a simple policy that always goes to the right, the effect of each action was randomly swapped so that in some states, going RIGHT results in a leftward transition and vice-versa. We used chain length of N = 50 unless stated otherwise. With random exploration, it is unlikely to explore the environment comprehensively and to reach the far right states, in particular. The probability of the latter is in O(2 −N ). Thus, in order to explore efficiently, an agent needs to exploit the structure of the environment.</p>
<p>Evaluation Methodology</p>
<p>In the context of pure exploration the returns the methods obtain can be disregarded, shifting focus solely to how thoroughly they explore the MDP. A Random Chain Environment of length N has 2N transitions. A perfect exploration method should observe all transitions in the smallest number of episodes. Hence, the fraction of transitions observed by an agent so far was used as the metric. To asses both asymptotic performance and speed of the method, area under the curve over episodes was used.</p>
<p>Baseline Methods</p>
<p>We used two exploration methods based on the optimism in face of uncertainty (OFU) principle (Kaelbling et al., 1996): Exploration Bonus DQN (Bellemare et al., 2016) and Bootstrapped DQN (Osband et al., 2016). They are based on the sample efficient DQN algorithm (Mnih et al., 2015). Bootstrapped DQN is claimed to be better than "state of the art approaches to exploration via dithering ( -greedy), optimism and posterior sampling" (Osband et al., 2016). Both of them are reactive since they do not explicitly seek new transitions, but upon finding one, prioritize frequenting it. It is important to note that these baselines are "any-time-optimal" RL algorithms which minimize cumulative regret by trading-off exploration and exploitation in each action. In contrast, the agent incurs no cost for exploration in the pure exploration problem considered in this paper. Hence, the baselines tackle a more general problem and do not take advantage of additional assumptions available in the pure exploration setup.</p>
<p>Appendix C contains a detailed description and implementation details of the baseline methods. The Hyper-Parameters for both the baseline methods were tuned with grid search (see Appendix D).</p>
<p>MAX Implementation</p>
<p>The ensemble consisted of 3 forward models implemented as fully-connected neural networks, each receiving one-hot-encoded state of the environment and the action of the agent as input. The outputs of the network were categorical distributions over the (next) states. The networks were independently and randomly initialized which was found to be sufficient to foster the diversity for the out-of-sample predictions and hence bootstrap sampling was not used. After warm-up the models were trained for 150 iterations. Searching for an exploration policy in the exploration MDP, which was an open-loop plan, was performed using 25 rounds of Monte Carlo Tree Search (MCTS), which used 5 random trajectories for each tree expansion with Thompson Sampling as the selection policy. The best action was chosen based on the average value of the children. Models were trained after experiencing each transition in the environment and the MCTS tree was discarded after every step. The Hyper-Parameters of MAX were tuned with grid search for the chain length of N = 50 (see Appendix D). Figure 2 shows the percentage of explored transitions as a function of training episodes for all the methods. MAX explores 100% of the transitions in around 15 episodes while the baseline methods reach 40% in 60 episodes. Figures 5a, 5b and 6 in the appendix analyze the behaviour of the proposed algorithm. Fig. 5a plots the utility of the exploration policy during two learning episodes. Clearly, high utility correlates with experiencing novel transitions. Figure 5b, which shows the learning curves of individual models in an ensemble, indicates that less than 20 episodes is required to correctly learn all the transitions. The surrogate models are therefore indistinguishable from the environment. Notice also that the learning curves of the three models are very close to each other. This is because the models nearly always agree on the transitions from the training set and disagree on the others. The exploration is thus mainly driven by the divergence of predictions for the unobserved transitions. Figure 6 visualizes the transitions in the final two episodes of a run showing how MAX plans for uncertain (unvisited) transitions, which results in minimizing their corresponding uncertainties.</p>
<p>Results</p>
<p>Scalability We varied the chain length from 20 to 100 in intervals of 5. Figure 3a shows the experience curves obtained with MAX. Exploring longer chains require more episodes but the figure clearly indicates that the same algorithm that works well for the chain length of 50 works well for other lengths, without any extra hyper-parameter turning.</p>
<p>Sensitivity to Noise To see if MAX can actually distinguish between the environment risk and uncertainty, the left-most state (state 0) of the Chain Environment was modified to be a stochastic trap state. If the agent is in the trap state, regardless of the chosen action, it is equally likely to remain in state 0 or end up in state 1. A method that relies only on prediction errors cannot separate risk from uncertainty. Hence it would repeatedly visit the trap state as it is easier to reach than some far-off uncertain state. Figure 3b compares the performance of our method on both the normal Chain Environment and the one with a stochastic trap state. Although MAX is slowed down, it still manages to explore the transitions. The stochastic trap state increases the variance in the output of the models. This is because the models are trained on different outputs for the same input. Hence the resulting output distribution of the models are sensitive to the training samples the models have seen. This can cause disagreements and hence result in a higher utility right next to the initial state. The true uncertainty on the right, which is fainter, is no longer the dominant source of uncertainty. This causes even our method to struggle, despite being able to separate the two forms of uncertainty. </p>
<p>Robustness to Key Hyper-Parameters</p>
<p>We varied the number of models in the ensemble, the number of trajectories per an MCTS iteration, and the number of MCTS iterations. The results are shown in Figure 7. In general, more trajectories, more models in the ensemble and more planning iterations lead to better performance. Notice, however, that the algorithms works efficiently also for minimal settings.</p>
<p>Continuous Mountain Car</p>
<p>MAX can also be applied to environments with continuous state and action spaces. Figure 4 shows the result when MAX was used for the simple Continuous Mountain Car problem which has a 2D state space and a 1D action space. MAX was able to drive the car up and down the valley to develop the required resonance energy and reach the mountain top quickly without any external reward. </p>
<p>Discussion</p>
<p>Meta-Stability and Active Exploration A policy is meta-stable if it is sub-optimal and, at the same time, prevents the agent from gaining experience necessary to improve it (Watkins, 1989). That is, a policy can get stuck in a local optimum and not be able to get out of it. In the simple cases, undirected exploration techniques (Thrun, 1992) like adding random noise to the actions of the policy might be sufficient to break away of meta-stability. If the environment is ergodic, then reactive exploration strategies that use exploration bonuses can solve meta-stability. But active exploration of the form presented in this paper, can in principle break free of any type of meta-stability.</p>
<p>The Uncertainty Gradient While exploring an environment completely can be challenging, finding the best exploration policy in the exploration MDP requires exploration itself, which could also be hard. For the Chain Environment, within 25 MCTS planning rounds, the MCTS planner can typically look-ahead 5 states which is minuscule compared to the number of look-ahead steps required to plan to reach the right end of the chain starting from the left end. However in practice, our sample efficiency is dramatically greater due to the gradual uncertainty increase from left to right or the uncertainty gradient. Due to more visitation, states near the initial state have a higher representation in the dataset. There are less samples for transitions of states farther away from initial state hence there is more uncertainty. The uncertainty gradient is highly informative and the planner can get away acting nearly greedily.</p>
<p>Without the uncertainty gradient, MAX cannot do better than random search at exploring the exploration MDP. However, models are faster, often by several orders of magnitude and can be searched in parallel. The uncertainty gradient could naturally occur in many environments as it did in the Chain Environment. The uncertainty gradient could also be extracted by differentiating through the ensemble or by probing it by some other means.</p>
<p>Implications to Model Based RL MAX can also been seen as a natural exploration mechanism in model-based RL. Model-based RL promises to be significantly more efficient and more general compared to model-free RL methods. However, it suffers from model-bias (Deisenroth and Rasmussen, 2011): in certain regions of the state space, the models could deviate significantly from the external MDP. Model-bias could occur due to many reasons such as improper generalization and/or poor exploration. A strong policy search method could then exploit such degeneracy resulting in over-optimistic policies that fail in the environment. Thorough exploration is one way to potentially mitigate this issue. Since model-based RL does not have any inherent mechanism to explore, we consider our method to be an important addition to the model-based RL framework rather than merely being an application of it.</p>
<p>Limitations Our derivation makes the assumption that the utility of a policy is the average utility of the transitions possible when the policy is used. However, encountering a subset of those transitions and training the models can change the utility of the remaining transitions, thereby affecting the utility of the policy. We do not consider this second order effect in our derivation. In the Chain Environment for example, this effect leads to the agent planning to loop between pairs of uncertain states, rather than visiting many uncertain states. However ignoring this effect is not necessarily problematic as information gain is additive under expectation Sun et al. (2011).</p>
<p>Related Work</p>
<p>Our work is inspired by the framework developed in Schmidhuber (1997Schmidhuber ( , 2002, in which two adversarial reward-maximizing modules called the left brain and the right brain bet on outcomes of experimental protocols or algorithms they have collectively generated and agreed upon. Each brain is intrinsically motivated to outwit or surprise the other by proposing an experiment such that the other agrees on the experimental protocol but disagrees on the predicted outcome. After having executed the action sequence protocol approved by both brains, the surprised loser pays a reward to the winner in a zero sum game. MAX greatly simplifies this previous active exploration framework, distilling certain essential aspects. Two or more predictive models that may compute different hypotheses about the consequences of the actions of the agent, given observations are still used. However, there is only one single reward maximizer or RL machine, which is separate from the predictive models.</p>
<p>The information provided by an experiment was first analytically measured by Lindley (1956) in the form of expected information gain in Shannon sense (Shannon, 1948). Fedorov (1972) also proposed a theory of optimal resource allocation during experimentation. By the 1990s, information gain was used as an intrinsic reward for reinforcement learning (RL) systems (Storck et al., 1995). Even earlier, intrinsic reward signals were based on prediction errors of a predictive model (Schmidhuber, 1991a) and on the learning progress of a predictive model (Schmidhuber, 1991b). Thrun (1992) introduced notions of directed and undirected exploration in Reinforcement Learning. Optimal Bayesian experimental design (Chaloner and Verdinelli, 1995) is a framework for efficiently performing sequential experiments that uncover a phenomenon. However, usually the approach is restricted to linear models with Gaussian assumptions. Busetto et al. (2009) propose an optimal experimental design framework for model selection of nonlinear biochemical systems using expected information gain where they solve for the posterior using the Fokker-Planck equation. In Model Based Interval Estimation (Wiering, 1999) the uncertainty in the transition function captured by a surrogate model is used to boost Q-values of actions. In context of Active Learning, McCallumzy and Nigamy (1998) proposed to use Jensen Shannon Divergence among predictions of a committee of classifiers to identify the most useful sample to be labelled next among a pool of unlabelled samples. Itti and Baldi (2009) presented the surprise formulation used in Equation 1 and demonstrated a strong correlation surprise and human attention. At a high-level, MAX can be seen as a form of Bayesian Optimization (Snoek et al., 2012) adopted for exploration in RL which employs an inner search based optimization during planning. Curiosity has also been studied extensively from the perspective of developmental robotics (Oudeyer, 2018). A very general form of learning progress is compression progress used as an extra intrinsic reward for curious RL systems (Schmidhuber, 2009).</p>
<p>Following these, Sun et al. (2011) developed an optimal Bayesian framework for curiosity-driven exploration using learning progress. After proving that Information Gain is additive in expectation, a dynamic programming based algorithm was proposed to maximize Information Gain. Experiments however were limited to small tabular MDPs with a Dirichlet prior on transition probabilities. A similar Bayesian inspired, hypothesis-resolving, model-based RL exploration algorithm was proposed in Hester and Stone (2012) and shown to outperform prediction error based and other intrinsic motivation methods. In contrast to MAX, planning was performed by taking the mean prediction of an ensemble of models in order to optimize a disagreement-based utility measure which was also augmented with an additional state-distance bonus. Still and Precup (2012) derive a tradeoff between exploration and exploitation in an attempt to maximize the predictive power of the agent. Mohamed and Rezende (2015) combine Variational Inference and Deep Learning to form an objective based on mutual information to approximate agent empowerment. In comparison to our method, Houthooft et al. (2016) present a Bayesian approach to evaluate the value of experience taking a reactive approach. However, they also use Bayesian Neural Networks to maintain a belief over environment dynamics and use the information gain to bias the policy search with an intrinsic motivation reward component. Variational Inference is used to approximate the prior-posterior KL divergence. Bellemare et al. (2016) derive a notion of pseudo-count for estimating state visitation frequency in high-dimensional spaces. They then transform this into a form of exploration bonus that is maximized using DQN. Osband et al. (2016) propose Bootstrapped DQN which was used as a baseline. Pathak et al. (2017) use inverse models to avoid exploring learning anything that the agent cannot control to reduce risk. But they are still restricted to reactive exploration. A large scale study of curiosity driven exploration Burda et al. (2018) found that curiosity is correlated with the actual objectives of many environments, and report that using random features mitigates some of the non-stationarity implicit in methods based on curiosity.</p>
<p>Model-based RL has long been touted as the cure for sample inefficiency problems of modern RL (Schmidhuber, 1990;Deisenroth and Rasmussen, 2011). Yet, learning accurate models of highdimensional environments and exploiting them appropriately in downstream tasks is still an active area of research. Recently Kurutach et al. (2018) and Chua et al. (2018) have shown the potential of model-based RL when combined with Deep Learning in high-dimensional environments. In particular, we were inspired by Chua et al. (2018) who use probabilistic models and combine them with novel trajectory sampling techniques using particles to obtain better approximations of the returns in the environment.</p>
<p>Conclusion</p>
<p>We introduced MAX, a model-based RL algorithm for pure exploration. It can separate learnable and unlearnable unknowns and actively search for policies that seek learnable unknowns from the environment. The algorithm derived in this paper provides the means to use an ensemble of models for simulation and evaluation of an exploration policy. The quality of the exploration policy can therefore be directly optimized without real environment experience. MAX does not utilize nonstationary objectives or suffer from over-commitment as it is specified as a model-based RL algorithm. Preliminary experiments in an environment intractable for traditional exploration methods indicate that MAX is generic and could scale to large environments.</p>
<p>A Policy Evaluation</p>
<p>U (π) can be rewritten as: Expanding the equation from definition of KL divergence:
U (π) =D KL (P 1 P 2 ) = z p 1 (z) log p 1 (z) p 2 (z) dz U(s, a) = s T p(T |ζ) log p(T |ζ) p(T ) p(s |a, s) dT ds(12)
The prior p(Γ) and the posterior p(Γ|ζ) are related by Bayes rule:
p(Γ|ζ) = p(ζ|Γ)p(Γ) p(ζ)(13)
Equation 12 can be now rewritten as: 
U(s, a) = s T p(ζ|T )p(T ) p(ζ) log p(ζ|T ) p(
P(S|s, a,T ) represents a probability distribution over the next state with removal of the integrals over s . Given a space of distributions, the entropy of the average distribution minus the average entropy of distributions is the Jensen Shannon Divergence (JSD) of the space of distributions. It is also termed as the Information Radius and is defined as:
JSD{PZ | PZ ∼ P(Z)} = H Z PZp(PZ) dZ − Z H(PZ)p(PZ) dZ(19)
where Z is the space of distributions PZ. P(Z) is a compound probability distribution with p(PZ) as its density function.</p>
<p>Therefore, Equation 10 can be expressed as:</p>
<p>U(s, a) = JSD{P(S|s, a,T ) |T ∼ P(Γ)}</p>
<p>where Γ is the space of transition functions with probabilistic outputs.</p>
<p>C Baselines</p>
<p>C.1 Description</p>
<p>Exploration Bonus DQN is an extension of the DQN algorithm (Mnih et al., 2015), in which the transitions that are visited for the first time carry an extra bonus reward. This causes the value of those transitions to be temporarily over-estimated and results in more visits to novel regions of the environment. In general, the exploration bonuses are computed using prediction errors of forward models (Pathak et al., 2017). However, in our simple environment, the bonuses are provided by an oracle, implemented as a transition look-up table. This is equivalent to having a model that can learn about a transition in one-shot.</p>
<p>Bootstrapped DQN by Osband et al. (2016) also extends the DQN algorithm and it is based on the same underlying principle as Exploration Bonus DQN. Instead of introducing arbitrary extra rewards, it relies on stochastic over-estimation of values in novel states. Multiple Q-value networks, referred to as heads, are maintained and trained on a shared replay buffer. Before every episode, a head is randomly selected and the agent acts greedily with respect to it. If a novel transition (s, a, s ) is added to the replay buffer, the hope is that at least one of the heads over-estimates the value of s , causing the policy resulting from TD-bootstrapping to prefer this state. This leads to that particular state being further explored when that head is used to act.</p>
<p>Commonly used -greedy exploration was turned off in both cases, thereby making exploration reliant solely on the methods themselves.</p>
<p>C.2 Implementation Details</p>
<p>The Q-value functions were implemented with multi-layer fully connected neural networks. The state was encoded with thermometer encoding for Bootstrapped DQN as proposed by Osband et al. (2016).</p>
<p>The Q-value network in Exploration Bonus DQN was trained with one batch of data after each step in the environment. For Bootstrapped DQN, the Q-value network was trained for a number of iterations only after each episode, with the exact number of iterations being tuned. The number of Bootstrap heads was fixed to 10. Bootstrap sampling was not employed and all heads were trained on all transitions collected so far. Target networks and replays were used for both methods as in the standard DQN.</p>
<p>RMSprop optimizer with a momentum of 0.9 was used to minimize the Huber Loss with gradient clipping of 5.</p>
<p>Both baselines were given 3 episodes of warm up to populate the buffers, during which the actions were chosen randomly.</p>
<p>D Hyper-parameter Tuning</p>
<p>In the experiments (see Section 3) Each hyper-parameter configuration was tested using 5 different random seeds. Hyper-Parameters were ranked according to the median of the area under the exploration curve.</p>
<p>All neural networks consisted of Glorot-initialized, tanh-activated fully connected layers with their depth and width being tuned.      </p>
<p>13: until computation budget exhausted To approximate U(s, a), the JSD in Equation 5 can be expanded as (see Appendix B):</p>
<p>Figure 1 :
1Chain Environment of length 10.</p>
<p>Figure 2 :
2The percentage of explored transitions for the Random Chain Environment with the chain of length N = 50. For the first 3 episodes, marked by the vertical dotted line, actions were chosen at random. Each line corresponds to the median of 100 runs, and the shaded area spans the 25th and 75th percentiles.</p>
<p>Figure 3 :
3The MAX algorithm performance for (a) different chain lengths and (b) in the presence of a stochastic trap. Each learning curve is a median of 5 runs with different seeds. The vertical dotted line marks the ends of the warm-up phase.</p>
<p>Figure 4 :
4Exploration in Continuous Mountain Car Environment. Each plot shows a 2D grid representing the state space of the agent. The average uncertainty of a state over all actions determines the color of its corresponding point in the grid. The red dots mark the current experience of the agent.</p>
<p>a, s |π) ds da ds where p(s, a, s |π) is the probability of transition (s, a, s ) occurring given π is the acting policy. s, a, s )p(s |s, a)p(s, a|π) ds da ds(9)Define action utility function U(s, a) (Equation 4) which quantifies the net utility of taking action a from state s as: s, a)p(s, a|π) da dsSince the true transition function is unknown, the utility of a policy has to be marginalized over our belief of it:U (π) =T s a U(s, a)p(s, a|π,T )p(T ) da ds dT which reduces to the following expectation (Equation 11): U (π) = ET ∼P(Γ) E s,a∼P(S,A|π,T ) [U(s, a)] (11) B Action Evaluation To obtain a closed form expression for U(s, a), Equation 4 can be expanded using Equation 1 as: U(s, a) = s u(s, a, s )p(s |a, s) ds U(s, a) = s D KL (P(Γ|ζ) P(Γ)) p(s |a, s) ds</p>
<p>ζ) p(s |a, s) dT ds Substituting ζ = (s, 15 in 14, expanding and swapping integrals in the former term: U(s, a) = T s p(s |s, a,T ) log (p(s |s, a,T ))p(T ) ds dT − s T p(s |s, a,T )p(T ) log ( T p(s |s, a,T )p(T ) dT ) dT ds (16) − z p(z) log (p(z)) dz is just entropy H(P(z)). U(s, a) = H T P(S|s, a,T )p(T ) dT − T H(P(S|s, a,T ))p(T ) dT (17) = H ET ∼P(Γ) P(S|s, a,T ) − ET ∼P(Γ) H(P(S|s, a,T ))</p>
<p>Figure 5 :
5Exploration with MAX. (a) Utility of the exploration policy (in normalized log scale) during an exemplary episode. The red points mark encounters with novel transitions. (b) The accuracy of the models in the ensemble at each episode and the variance of model accuracy.</p>
<p>Figure 6 :
6Instance of uncertainty minimization:MAX behaviour in the second-to-last (top) and the last (bottom) episodes. The solid arrows (environment transitions) and are annotated with the utilities of predicted information gains. The arrowheads are circular for the unvisited transitions and regular for the visited ones (before an episode). The dotted arrows with step numbers visualize the path the agent was following during an episode (only the final steps are shown).</p>
<p>Figure 7 :
7Algorithm Properties. Each learning curve is a median of 5 runs with different seeds. Vertical dotted line marks the ends of the warm-up phase. Sub-figures show how the percentage of explored transitions varies with respect to (a) ensemble size, (b) number of rollouts and (c) planning iterations.</p>
<p>Table 1 :
1Hyper-Parameters for DQN with Exploration Bonus. Grid Size: 4.3kHyper-parameter 
Values </p>
<p>Exploration Bonus 
5 × 10 −3 10 −2 10 −1 
Replay Size 
256 
10 5 
Number of Hidden Layers 
1 
2 
3 
Hidden Layer Size 
32 
64 
128 
Learning Rate 
10 −2 
10 −3 10 −4 
Batch Size 
16 
32 
64 
Discount Factor 
0.95 
0.975 0.99 
Target Network Update Frequency 
16 
64 
256 </p>
<p>Table 2 :
2Hyper-Parameters for Bootstrapped DQN. Grid Size: 2.1kHyper-parameter 
Values </p>
<p>Number of Hidden Layers 
1 
2 
3 
Hidden Layer Size 
32 
64 
128 
Learning Rate 
10 −2 10 −3 10 −4 
Batch Size 
16 
32 
64 
Number of Training Iterations per Episode 
16 
32 
64 
Discount Factor 
0.95 0.975 0.99 
Target Network Update Frequency 
16 
64 
256 </p>
<p>Table 3 :
3Hyper-Parameters for MAX. Grid Size: 0.8kHyper-parameter 
Values </p>
<p>Number of Hidden Layers 
2 
3 
4 
Hidden Layer Size 
64 
128 
256 
Learning Rate 
10 −3 10 −4 
Batch Size 
16 
64 
256 
Number of Training Iterations per Episode 
16 
32 
64 
128 
Weight Decay 
0 
10 −5 10 −6 10 −7 </p>
<p>E Supplementary Figures </p>
<p>AcknowledgementsWe would like to thank Jan Koutník for helping prepare the figures and Jürgen Schmidhuber for his valuable input to the Related Work section. We would also like to thank Christian Osendorfer, Timon Willi, Marco Gallieri, Engin Toklu, Rupesh Kumar Srivastava and Garrett Andersen for their assistance and everyone at NNAISENSE for being part of a conducive research environment.
Unifying Count-Based Exploration and Intrinsic Motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. (2016). Unifying Count-Based Exploration and Intrinsic Motivation. In Advances in Neural Information Processing Systems, pages 1471-1479.</p>
<p>Pure Exploration In Multi-armed Bandits Problems. S Bubeck, R Munos, G Stoltz, International Conference On Algorithmic Learning Theory. SpringerBubeck, S., Munos, R., and Stoltz, G. (2009). Pure Exploration In Multi-armed Bandits Problems. In International Conference On Algorithmic Learning Theory, pages 23-37. Springer.</p>
<p>Y Burda, H Edwards, D Pathak, A Storkey, T Darrell, A A Efros, arXiv:1808.04355Large-Scale Study of Curiosity-Driven Learning. arXiv preprintBurda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A. A. (2018). Large-Scale Study of Curiosity-Driven Learning. arXiv preprint arXiv:1808.04355.</p>
<p>Optimized Expected Information Gain for Nonlinear Dynamical Systems. A G Busetto, C S Ong, J M Buhmann, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningACMBusetto, A. G., Ong, C. S., and Buhmann, J. M. (2009). Optimized Expected Information Gain for Nonlinear Dynamical Systems. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 97-104. ACM.</p>
<p>Bayesian Experimental Design: A Review. K Chaloner, I Verdinelli, Statistical Science. Chaloner, K. and Verdinelli, I. (1995). Bayesian Experimental Design: A Review. Statistical Science, pages 273-304.</p>
<p>K Chua, R Calandra, R Mcallister, S Levine, arXiv:1805.12114Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. arXiv preprintChua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. arXiv preprint arXiv:1805.12114.</p>
<p>PILCO: A Model-Based and Data-Efficient Approach to Policy Search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningDeisenroth, M. and Rasmussen, C. E. (2011). PILCO: A Model-Based and Data-Efficient Approach to Policy Search. In Proceedings of the 28th International Conference on Machine Learning, pages 465-472.</p>
<p>Bayesian Inference and the Parametric Bootstrap. B Efron, Annals of Applied Statistics. 64Efron, B. (2012). Bayesian Inference and the Parametric Bootstrap. Annals of Applied Statistics, 6(4):1971-1997.</p>
<p>Theory of Optimal Experiments Designs. V Fedorov, Academic PressFedorov, V. (1972). Theory of Optimal Experiments Designs. Academic Press.</p>
<p>Intrinsically Motivated Model Learning for Developing Curious Robots. T Hester, P Stone, Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on. IEEEHester, T. and Stone, P. (2012). Intrinsically Motivated Model Learning for Developing Curious Robots. In Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on, pages 1-6. IEEE.</p>
<p>VIME: Variational Information Maximizing Exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, Abbeel , P , Advances in Neural Information Processing Systems. Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. (2016). VIME: Variational Information Maximizing Exploration. In Advances in Neural Information Processing Systems, pages 1109-1117.</p>
<p>Bayesian Surprise Attracts Human Attention. L Itti, P Baldi, Vision Research. 4910Itti, L. and Baldi, P. (2009). Bayesian Surprise Attracts Human Attention. Vision Research, 49(10):1295-1306.</p>
<p>Reinforcement Learning: A Survey. L P Kaelbling, M L Littman, Moore , Journal of Artificial Intelligence Research. 4Kaelbling, L. P., Littman, M. L., and Moore, a. W. (1996). Reinforcement Learning: A Survey. Journal of Artificial Intelligence Research, 4:237-285.</p>
<p>. T Kurutach, I Clavera, Y Duan, A Tamar, Abbeel , P , arXiv:1802.10592Model-Ensemble Trust-Region Policy Optimization. arXiv preprintKurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018). Model-Ensemble Trust-Region Policy Optimization. arXiv preprint arXiv:1802.10592.</p>
<p>Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles. B Lakshminarayanan, A Pritzel, C Blundell, Advances in Neural Information Processing Systems. Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and Scalable Predictive Uncer- tainty Estimation Using Deep Ensembles. In Advances in Neural Information Processing Systems, pages 6402-6413.</p>
<p>On a Measure of the Information Provided by an Experiment. D V Lindley, The Annals of Mathematical Statistics. Lindley, D. V. (1956). On a Measure of the Information Provided by an Experiment. The Annals of Mathematical Statistics, pages 986-1005.</p>
<p>Employing EM and Pool-Based Active Learning for Text Classification. A K Mccallumzy, K Nigamy, Proceedings International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)CiteseerMcCallumzy, a. K. and Nigamy, K. (1998). Employing EM and Pool-Based Active Learning for Text Classification. In Proceedings International Conference on Machine Learning (ICML), pages 359-367. Citeseer.</p>
<p>Human-Level Control Through Deep Reinforcement Learning. V Mnih, K Kavukcuoglu, D Silver, . A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, . K Fidjeland, G Ostrovski, Nature. 5187540529Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, a. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, a. K., Ostrovski, G., et al. (2015). Human-Level Control Through Deep Reinforcement Learning. Nature, 518(7540):529.</p>
<p>Variational Information Maximisation For Intrinsically Motivated Reinforcement Learning. S Mohamed, D J Rezende, Advances In Neural Information Processing Systems. Mohamed, S. and Rezende, D. J. (2015). Variational Information Maximisation For Intrinsically Motivated Reinforcement Learning. In Advances In Neural Information Processing Systems, pages 2125-2133.</p>
<p>Deep Exploration via Bootstrapped DQN. I Osband, C Blundell, A Pritzel, B Van Roy, Advances in Neural Information Processing Systems. Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. (2016). Deep Exploration via Bootstrapped DQN. In Advances in Neural Information Processing Systems, pages 4026-4034.</p>
<p>P.-Y Oudeyer, arXiv:1802.10546Computational Theories of Curiosity-Driven Learning. arXiv preprintOudeyer, P.-Y. (2018). Computational Theories of Curiosity-Driven Learning. arXiv preprint arXiv:1802.10546.</p>
<p>Curiosity-Driven Exploration by Self-Supervised Prediction. D Pathak, P Agrawal, A A Efros, Darrell , T , Proceedings of The 35th International Conference On Machine Learning. The 35th International Conference On Machine LearningPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-Driven Exploration by Self-Supervised Prediction. In Proceedings of The 35th International Conference On Machine Learning.</p>
<p>Making the World Differentiable: On Using Fully Recurrent Self-Supervised Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments. J Schmidhuber, FKI-126-90Institut für Informatik, Technische Universität MünchenTechnical ReportrevisedSchmidhuber, J. (1990). Making the World Differentiable: On Using Fully Recurrent Self-Supervised Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environ- ments. Technical Report FKI-126-90 (revised), Institut für Informatik, Technische Universität München.</p>
<p>A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers. J Schmidhuber, Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats. Meyer, J. A. and Wilson, S. W.the International Conference on Simulation of Adaptive Behavior: From Animals to AnimatsMIT Press/Bradford BooksSchmidhuber, J. (1991a). A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers. In Meyer, J. A. and Wilson, S. W., editors, Proceedings of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats, pages 222-227. MIT Press/Bradford Books.</p>
<p>Curious Model-Building Control Systems. J Schmidhuber, Proceedings of the International Joint Conference on Neural Networks. the International Joint Conference on Neural NetworksSingaporeIEEE press2Schmidhuber, J. (1991b). Curious Model-Building Control Systems. In Proceedings of the Inter- national Joint Conference on Neural Networks, Singapore, volume 2, pages 1458-1463. IEEE press.</p>
<p>What's Interesting?. J Schmidhuber, IDSIA-35-97Technical ReportIDSIASchmidhuber, J. (1997). What's Interesting? Technical Report IDSIA-35-97, IDSIA.</p>
<p>Exploring the Predictable. J Schmidhuber, Advances in Evolutionary Computing. Ghosh, A. and Tsuitsui, S.SpringerSchmidhuber, J. (2002). Exploring the Predictable. In Ghosh, A. and Tsuitsui, S., editors, Advances in Evolutionary Computing, pages 579-612. Springer.</p>
<p>Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes. J ; G Schmidhuber, M V Butz, O Sigaud, G Baldassarre, Anticipatory Behavior in Adaptive Learning Systems. From Psychological Theories to Artificial Cognitive Systems. Springer5499Schmidhuber, J. (2009). Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes. In Pezzulo, G., Butz, M. V., Sigaud, O., and Baldassarre, G., editors, Anticipatory Behavior in Adaptive Learning Systems. From Psychological Theories to Artificial Cognitive Systems, volume 5499 of LNCS, pages 48-76. Springer.</p>
<p>A Mathematical Theory of Communication (Parts I and II). C E Shannon, Bell System Technical Journal. XXVIIShannon, C. E. (1948). A Mathematical Theory of Communication (Parts I and II). Bell System Technical Journal, XXVII:379-423.</p>
<p>Practical Bayesian Optimization Of Machine Learning Algorithms. J Snoek, H Larochelle, R P Adams, Advances in Neural Information Processing Systems. Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian Optimization Of Machine Learning Algorithms. In Advances in Neural Information Processing Systems, pages 2951-2959.</p>
<p>An Information-Theoretic Approach To Curiosity-Driven Reinforcement Learning. S Still, D Precup, Theory in Biosciences. 1313Still, S. and Precup, D. (2012). An Information-Theoretic Approach To Curiosity-Driven Reinforce- ment Learning. Theory in Biosciences, 131(3):139-148.</p>
<p>Reinforcement Driven Information Acquisition in Non-Deterministic Environments. J Storck, S Hochreiter, J Schmidhuber, Proceedings of the International Conference on Artificial Neural Networks. the International Conference on Artificial Neural NetworksParisCiteseer2Storck, J., Hochreiter, S., and Schmidhuber, J. (1995). Reinforcement Driven Information Acquisition in Non-Deterministic Environments. In Proceedings of the International Conference on Artificial Neural Networks, Paris, volume 2, pages 159-164. Citeseer.</p>
<p>A Theoretical Analysis of Model-Based Interval Estimation. A L Strehl, M L Littman, Proceedings of The 22nd International Conference On Machine Learning. The 22nd International Conference On Machine LearningACMStrehl, A. L. and Littman, M. L. (2005). A Theoretical Analysis of Model-Based Interval Estimation. In Proceedings of The 22nd International Conference On Machine Learning, pages 856-863. ACM.</p>
<p>Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic Environments. Y Sun, F Gomez, J Schmidhuber, International Conference on Artificial General Intelligence. SpringerSun, Y., Gomez, F., and Schmidhuber, J. (2011). Planning to Be Surprised: Optimal Bayesian Explo- ration in Dynamic Environments. In International Conference on Artificial General Intelligence, pages 41-51. Springer.</p>
<p>Efficient Exploration In Reinforcement Learning. S B Thrun, Technical ReportThrun, S. B. (1992). Efficient Exploration In Reinforcement Learning. Technical Report.</p>
<p>Learning From Delayed Rewards. C J C H Watkins, King's College, CambridgePhD thesisWatkins, C. J. C. H. (1989). Learning From Delayed Rewards. PhD thesis, King's College, Cambridge.</p>
<p>Explorations in Efficient Reinforcement Learning. M A Wiering, University of AmsterdamPhD thesisWiering, M. A. (1999). Explorations in Efficient Reinforcement Learning. PhD thesis, University of Amsterdam.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9273 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9273</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9273</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273811520</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.00914v1.pdf" target="_blank">AAD-LLM: Adaptive Anomaly Detection Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> For data-constrained, complex and dynamic industrial environments, there is a critical need for transferable and multimodal methodologies to enhance anomaly detection and therefore, prevent costs associated with system failures. Typically, traditional PdM approaches are not transferable or multimodal. This work examines the use of Large Language Models (LLMs) for anomaly detection in complex and dynamic manufacturing systems. The research aims to improve the transferability of anomaly detection models by leveraging Large Language Models (LLMs) and seeks to validate the enhanced effectiveness of the proposed approach in data-sparse industrial applications. The research also seeks to enable more collaborative decision-making between the model and plant operators by allowing for the enriching of input series data with semantics. Additionally, the research aims to address the issue of concept drift in dynamic industrial settings by integrating an adaptability mechanism. The literature review examines the latest developments in LLM time series tasks alongside associated adaptive anomaly detection methods to establish a robust theoretical framework for the proposed architecture. This paper presents a novel model framework (AAD-LLM) that doesn’t require any training or finetuning on the dataset it is applied to and is multimodal. Results suggest that anomaly detection can be converted into a "language" task to deliver effective, context-aware detection in data-constrained industrial applications. This work, therefore, contributes significantly to advancements in anomaly detection methodologies.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9273.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9273.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AAD-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Anomaly Detection using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that repurposes a frozen, instruction-finetuned LLM as a zero-shot, multimodal anomaly detector for multivariate time series by prompting with injected statistical summaries, domain context, SPC-based baselines, windowing, and an adaptability mechanism that updates a comparison dataset incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (instruction-finetuned LLM, frozen backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>multivariate time series (windowed segments; statistical summaries injected into text templates)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>industrial sensor data (plastics extrusion use-case) and SKoltech Anomaly Benchmark (SKAB) valve sensor datasets</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>point/segment outliers / anomalous time-series behavior (points outside SPC control limits and correlated anomalies across variables)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Zero-shot prompting of a frozen LLM: preprocess each variable with SPC (remove out-of-control points), initialize a comparison baseline window C_i, compute statistical derivatives (z-score, max), inject those numeric summaries and domain context into text templates, prompt the LLM for anomaly judgment, map LLM text output to binary via a use-case-specific binarization function that requires correlation of anomalies per domain rules, and update C_i adaptively when windows are non-anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against multiple classical and neural baselines from SKAB benchmarks including LSTMCaps, MSET, LSTMCapsV2, MSCRED, Vanilla LSTM, Conv-AE, LSTM-AE, LSTM-VAE, Vanilla AE, Isolation Forest (as listed in Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, F1 score, False Alarm Rate (FAR), Missed Alarm Rate (MAR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Use-case (plastics extrusion run-to-failure dataset): accuracy 70.7%, F1 = 77.0%. SKAB benchmark: reported accuracy 58.4%; reported AAD-LLM F1 = 0.564; reported FAR = 47.63%; reported MAR = 1.7% (values as reported in Table I). The authors also note an observed drop in accuracy on a specific input series caused by z-score comparison errors (accuracy decreased from 84.2% to 42.1% for that series).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>On SKAB AAD-LLM ranked 8th of 11 algorithms by F1, 5th by MAR, and last by FAR; authors report AAD-LLM underperformed many specialized NN/ML baselines on F1 and FAR but is unique in being multimodal and requiring no training/finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The LLM struggled with numerical comparisons and arithmetic (z-score comparison errors degraded SKAB performance); inconsistent behavior when performing math/comparisons; required manual restructuring of domain context to get consistent LLM decisions; evaluated only on static datasets (not online/streaming); high FAR on SKAB; needs additional tooling (e.g., RAG or retrieval of formulas) or auxiliary modules to handle reliable math/comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Demonstrates feasibility of converting anomaly detection into a 'language' task enabling zero-shot, multimodal detection that can incorporate domain semantics (expert rules) directly in prompts; uses SPC to define normal baselines instead of requiring the model to 'learn' normality (improves transferability); provides an adaptability mechanism by updating a comparison dataset C_i incrementally per non-anomalous window; shows LLMs can fuse textual semantics with numeric summaries for collaborative decision-making with operators, but numeric-comparison weaknesses are a critical bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAD-LLM: Adaptive Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9273.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9273.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3 (8B parameters, instruction-finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-finetuned large language model used as the frozen backbone in AAD-LLM to perform zero-shot reasoning on injected statistical summaries and domain context for anomaly judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (instruction-finetuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>multivariate time series (presented as text with injected statistics)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>industrial sensor PdM data and SKAB valve signals (same as AAD-LLM experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>time-series point/segment anomalies (as judged by prompt responses)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Used frozen: prompts contain domain context, z-scores and maxima for windows and baselines; the model outputs textual judgment which is binarized according to domain rules.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>See AAD-LLM comparisons (NNs and ML baselines in Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same metrics reported for AAD-LLM (accuracy, F1, FAR, MAR) since the model is the backbone of AAD-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported as part of AAD-LLM: use-case accuracy 70.7% and F1 77.0%; SKAB accuracy 58.4%; AAD-LLM F1 0.564, FAR 47.63%; performance degraded in cases due to numeric comparison errors by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>As above (AAD-LLM vs baselines). The paper emphasizes that Llama 3 8B was not finetuned on target data (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Llama 3's numeric comparison and arithmetic consistency were unreliable in prompts; manual prompt/context engineering required to get stable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Using an instruction-tuned LLM frozen at inference allows multimodal semantic fusion without per-dataset finetuning, but reliability on numeric reasoning is a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAD-LLM: Adaptive Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9273.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9273.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OFA (unified framework using frozen pretrained LLMs for time series tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that reuses frozen pretrained LLMs for a variety of time series analysis tasks and explored univariate anomaly detection, but required training of an input embedding layer (learned input transformation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>frozen pretrained LLM (framework-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (frozen LLM with trained input embedding layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>univariate time series (and other time series tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>general time series domains (benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>univariate anomaly detection (point/outlier detection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learned input embedding layer is trained to project time series representations for use with a frozen LLM; requires training of the embedding layer rather than purely zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>OFA was compared to SOTA models on classification, forecasting and anomaly detection benchmarks (details in original OFA work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to achieve superior or comparable results in classification, forecasting, anomaly detection, and few/zero-shot learning (metrics not enumerated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Described qualitatively as superior or comparable to SOTA in multiple tasks; specific numeric results not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>OFA outperformed or matched SOTA on multiple time series tasks according to the cited prior work, but it required training the input embedding layer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires learning input transformations (embedding layer) which reduces pure zero-shot transferability; prior work explored only univariate anomaly detection (no multivariate anomaly exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Shows frozen LLMs can be leveraged for anomaly detection when properly aligned with learned numeric embeddings, but such alignment requires dataset-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAD-LLM: Adaptive Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9273.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9273.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEST (Text prototype aligned embedding to activate LLM's ability for time series)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method aligning time series embeddings with LLMs to enable time series tasks without sacrificing language processing abilities; explored multiple tasks and improved performance, but required learned input transformations (embedding alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pretrained LLM (with aligned embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (embedding-aligned LLM approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>time series (forecasting, classification; alignment from numeric to text-prototype embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>general time series benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not primarily focused on multivariate anomaly detection in cited context (explored forecasting and classification; alignment could support anomaly tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Aligns time series embeddings to pretrained LLM representations (requires training/alignment); then leverages LLM capabilities for downstream time series tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to SOTA on forecasting and classification tasks per original paper (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported improved performance on univariate forecasting and multivariate classification relative to certain baselines (metrics not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Described qualitatively as superior on several tasks; exact numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>TEST demonstrated superior performance on several time series tasks compared to some SOTA models, but did not focus on multivariate anomaly detection in the cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires training/alignment of embeddings, so not pure zero-shot on raw numeric inputs; embedding function specifics were not fully specified in the summary presented.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Embedding alignment is a viable route to activate LLM time series abilities, but it trades off the zero-shot/no-training advantages highlighted by AAD-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAD-LLM: Adaptive Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9273.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9273.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time-LLM / Chronos / LLMTime / PromptCast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-LLM ; Chronos ; LLMTime ; PromptCast (LLM-based time series forecasting prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of prior works that repurpose large language models for time series forecasting via reprogramming, tokenization/quantization, or template-based natural language prompts; these works demonstrate LLM utility on forecasting (few/zero-shot) but do not focus on anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various pretrained LLMs (reprogrammed or with numeric tokenization/prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based LLM approaches (reprogramming, numeric tokenization, or manual prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>time series forecasting (uni- and multivariate depending on the work)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>healthcare, finance, transportation, and general forecasting benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>not directly applied to anomaly detection in the summarized descriptions (primarily forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Examples: Time-LLM reprograms time series into language-like inputs; Chronos tokenizes scaled/quantized values into fixed vocabulary; LLMTime uses numeric tokenization; PromptCast converts numeric sequences into template-based natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to statistical and transformer forecasting baselines in respective works; these papers reported outperforming or matching specialized models in few/zero-shot scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Forecasting metrics (not enumerated here); reported to outperform statistical baselines and be competitive with efficient transformer models for forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitatively reported strong forecasting performance and promising zero/few-shot transfer; anomaly detection not the primary focus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Often outperformed traditional statistical baselines and was competitive with specialized forecasting models in their respective evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>These works focus on forecasting; additional components or adaptations would be needed to apply the same strategies directly to anomaly detection (particularly multivariate, domain-semantic enriched anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Demonstrates multiple technical routes (reprogramming, tokenization, templating) to make numeric time series consumable by LLMs; these ideas underpin the AAD-LLM strategy of converting anomaly detection into a language task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAD-LLM: Adaptive Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9273.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9273.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SKAB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Skoltech Anomaly Benchmark (SKAB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A publicly available benchmark of labeled multichannel sensor signals used to evaluate anomaly detection algorithms; used in this paper to evaluate AAD-LLM on valve1 and valve2 datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>multichannel/multivariate time series sensor signals (datetime, accelerometer RMS, current, pressure, temperature, voltage, rateRMS, anomaly labels, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>instrumented testbed sensor data for anomaly detection research</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>labeled point anomalies and changepoints (binary anomaly labels provided in dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Used as a benchmark: features selected via Mann-Whitney U test (selected Accelerometer1RMS, Accelerometer2RMS, Temperature, Thermocouple, RateRMS), preprocessed with SPC, windowed, and fed through AAD-LLM for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Multiple classical and neural anomaly detection algorithms from SKAB evaluations (see Table I) were used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy, F1, FAR, MAR (as reported in the paper's evaluation table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AAD-LLM on SKAB: reported accuracy 58.4%; reported F1 0.564; issues with z-score comparison errors reduced performance on some series (example drop 84.2% → 42.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>AAD-LLM performed worse than several specialized baselines on SKAB by F1 and had high FAR, though it remains unique in being multimodal and requiring no training on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance on SKAB notably affected by LLM numeric-comparison errors; benchmark evaluation highlighted the need for more robust numeric handling and error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AAD-LLM: Adaptive Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Time-LLM: Time series forecasting by reprogramming large language models <em>(Rating: 2)</em></li>
                <li>Chronos: Learning the language of time series <em>(Rating: 2)</em></li>
                <li>LLMTime (effective numerical tokenization for LLMs on time series) <em>(Rating: 1)</em></li>
                <li>PromptCast: A new prompt-based learning paradigm for time series forecasting <em>(Rating: 1)</em></li>
                <li>One fits all: Power general time series analysis by pretrained lm (OFA) <em>(Rating: 2)</em></li>
                <li>TEST: Text prototype aligned embedding to activate LLM's ability for time series <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot time series forecasters <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9273",
    "paper_id": "paper-273811520",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "AAD-LLM",
            "name_full": "Adaptive Anomaly Detection using Large Language Models",
            "brief_description": "A framework that repurposes a frozen, instruction-finetuned LLM as a zero-shot, multimodal anomaly detector for multivariate time series by prompting with injected statistical summaries, domain context, SPC-based baselines, windowing, and an adaptability mechanism that updates a comparison dataset incrementally.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta Llama 3",
            "model_type": "transformer (instruction-finetuned LLM, frozen backbone)",
            "model_size": "8B",
            "data_type": "multivariate time series (windowed segments; statistical summaries injected into text templates)",
            "data_domain": "industrial sensor data (plastics extrusion use-case) and SKoltech Anomaly Benchmark (SKAB) valve sensor datasets",
            "anomaly_type": "point/segment outliers / anomalous time-series behavior (points outside SPC control limits and correlated anomalies across variables)",
            "method_description": "Zero-shot prompting of a frozen LLM: preprocess each variable with SPC (remove out-of-control points), initialize a comparison baseline window C_i, compute statistical derivatives (z-score, max), inject those numeric summaries and domain context into text templates, prompt the LLM for anomaly judgment, map LLM text output to binary via a use-case-specific binarization function that requires correlation of anomalies per domain rules, and update C_i adaptively when windows are non-anomalous.",
            "baseline_methods": "Compared against multiple classical and neural baselines from SKAB benchmarks including LSTMCaps, MSET, LSTMCapsV2, MSCRED, Vanilla LSTM, Conv-AE, LSTM-AE, LSTM-VAE, Vanilla AE, Isolation Forest (as listed in Table I).",
            "performance_metrics": "Accuracy, F1 score, False Alarm Rate (FAR), Missed Alarm Rate (MAR)",
            "performance_results": "Use-case (plastics extrusion run-to-failure dataset): accuracy 70.7%, F1 = 77.0%. SKAB benchmark: reported accuracy 58.4%; reported AAD-LLM F1 = 0.564; reported FAR = 47.63%; reported MAR = 1.7% (values as reported in Table I). The authors also note an observed drop in accuracy on a specific input series caused by z-score comparison errors (accuracy decreased from 84.2% to 42.1% for that series).",
            "comparison_to_baseline": "On SKAB AAD-LLM ranked 8th of 11 algorithms by F1, 5th by MAR, and last by FAR; authors report AAD-LLM underperformed many specialized NN/ML baselines on F1 and FAR but is unique in being multimodal and requiring no training/finetuning.",
            "limitations_or_failure_cases": "The LLM struggled with numerical comparisons and arithmetic (z-score comparison errors degraded SKAB performance); inconsistent behavior when performing math/comparisons; required manual restructuring of domain context to get consistent LLM decisions; evaluated only on static datasets (not online/streaming); high FAR on SKAB; needs additional tooling (e.g., RAG or retrieval of formulas) or auxiliary modules to handle reliable math/comparisons.",
            "unique_insights": "Demonstrates feasibility of converting anomaly detection into a 'language' task enabling zero-shot, multimodal detection that can incorporate domain semantics (expert rules) directly in prompts; uses SPC to define normal baselines instead of requiring the model to 'learn' normality (improves transferability); provides an adaptability mechanism by updating a comparison dataset C_i incrementally per non-anomalous window; shows LLMs can fuse textual semantics with numeric summaries for collaborative decision-making with operators, but numeric-comparison weaknesses are a critical bottleneck.",
            "uuid": "e9273.0",
            "source_info": {
                "paper_title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Meta Llama 3 8B",
            "name_full": "Meta Llama 3 (8B parameters, instruction-finetuned)",
            "brief_description": "An open-source instruction-finetuned large language model used as the frozen backbone in AAD-LLM to perform zero-shot reasoning on injected statistical summaries and domain context for anomaly judgments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Meta Llama 3",
            "model_type": "transformer (instruction-finetuned LLM)",
            "model_size": "8B",
            "data_type": "multivariate time series (presented as text with injected statistics)",
            "data_domain": "industrial sensor PdM data and SKAB valve signals (same as AAD-LLM experiments)",
            "anomaly_type": "time-series point/segment anomalies (as judged by prompt responses)",
            "method_description": "Used frozen: prompts contain domain context, z-scores and maxima for windows and baselines; the model outputs textual judgment which is binarized according to domain rules.",
            "baseline_methods": "See AAD-LLM comparisons (NNs and ML baselines in Table I).",
            "performance_metrics": "Same metrics reported for AAD-LLM (accuracy, F1, FAR, MAR) since the model is the backbone of AAD-LLM.",
            "performance_results": "Reported as part of AAD-LLM: use-case accuracy 70.7% and F1 77.0%; SKAB accuracy 58.4%; AAD-LLM F1 0.564, FAR 47.63%; performance degraded in cases due to numeric comparison errors by the LLM.",
            "comparison_to_baseline": "As above (AAD-LLM vs baselines). The paper emphasizes that Llama 3 8B was not finetuned on target data (zero-shot).",
            "limitations_or_failure_cases": "Llama 3's numeric comparison and arithmetic consistency were unreliable in prompts; manual prompt/context engineering required to get stable outputs.",
            "unique_insights": "Using an instruction-tuned LLM frozen at inference allows multimodal semantic fusion without per-dataset finetuning, but reliability on numeric reasoning is a limiting factor.",
            "uuid": "e9273.1",
            "source_info": {
                "paper_title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OFA",
            "name_full": "OFA (unified framework using frozen pretrained LLMs for time series tasks)",
            "brief_description": "A prior work that reuses frozen pretrained LLMs for a variety of time series analysis tasks and explored univariate anomaly detection, but required training of an input embedding layer (learned input transformation).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "frozen pretrained LLM (framework-specific)",
            "model_type": "transformer (frozen LLM with trained input embedding layer)",
            "model_size": null,
            "data_type": "univariate time series (and other time series tasks)",
            "data_domain": "general time series domains (benchmarks)",
            "anomaly_type": "univariate anomaly detection (point/outlier detection)",
            "method_description": "Learned input embedding layer is trained to project time series representations for use with a frozen LLM; requires training of the embedding layer rather than purely zero-shot prompting.",
            "baseline_methods": "OFA was compared to SOTA models on classification, forecasting and anomaly detection benchmarks (details in original OFA work).",
            "performance_metrics": "Reported to achieve superior or comparable results in classification, forecasting, anomaly detection, and few/zero-shot learning (metrics not enumerated in this paper).",
            "performance_results": "Described qualitatively as superior or comparable to SOTA in multiple tasks; specific numeric results not provided here.",
            "comparison_to_baseline": "OFA outperformed or matched SOTA on multiple time series tasks according to the cited prior work, but it required training the input embedding layer.",
            "limitations_or_failure_cases": "Requires learning input transformations (embedding layer) which reduces pure zero-shot transferability; prior work explored only univariate anomaly detection (no multivariate anomaly exploration).",
            "unique_insights": "Shows frozen LLMs can be leveraged for anomaly detection when properly aligned with learned numeric embeddings, but such alignment requires dataset-specific training.",
            "uuid": "e9273.2",
            "source_info": {
                "paper_title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "TEST",
            "name_full": "TEST (Text prototype aligned embedding to activate LLM's ability for time series)",
            "brief_description": "A method aligning time series embeddings with LLMs to enable time series tasks without sacrificing language processing abilities; explored multiple tasks and improved performance, but required learned input transformations (embedding alignment).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "pretrained LLM (with aligned embeddings)",
            "model_type": "transformer (embedding-aligned LLM approach)",
            "model_size": null,
            "data_type": "time series (forecasting, classification; alignment from numeric to text-prototype embeddings)",
            "data_domain": "general time series benchmarks",
            "anomaly_type": "not primarily focused on multivariate anomaly detection in cited context (explored forecasting and classification; alignment could support anomaly tasks)",
            "method_description": "Aligns time series embeddings to pretrained LLM representations (requires training/alignment); then leverages LLM capabilities for downstream time series tasks.",
            "baseline_methods": "Compared to SOTA on forecasting and classification tasks per original paper (not detailed here).",
            "performance_metrics": "Reported improved performance on univariate forecasting and multivariate classification relative to certain baselines (metrics not enumerated here).",
            "performance_results": "Described qualitatively as superior on several tasks; exact numbers not provided in this paper.",
            "comparison_to_baseline": "TEST demonstrated superior performance on several time series tasks compared to some SOTA models, but did not focus on multivariate anomaly detection in the cited works.",
            "limitations_or_failure_cases": "Requires training/alignment of embeddings, so not pure zero-shot on raw numeric inputs; embedding function specifics were not fully specified in the summary presented.",
            "unique_insights": "Embedding alignment is a viable route to activate LLM time series abilities, but it trades off the zero-shot/no-training advantages highlighted by AAD-LLM.",
            "uuid": "e9273.3",
            "source_info": {
                "paper_title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Time-LLM / Chronos / LLMTime / PromptCast",
            "name_full": "Time-LLM ; Chronos ; LLMTime ; PromptCast (LLM-based time series forecasting prior works)",
            "brief_description": "A set of prior works that repurpose large language models for time series forecasting via reprogramming, tokenization/quantization, or template-based natural language prompts; these works demonstrate LLM utility on forecasting (few/zero-shot) but do not focus on anomaly detection.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various pretrained LLMs (reprogrammed or with numeric tokenization/prompting)",
            "model_type": "transformer-based LLM approaches (reprogramming, numeric tokenization, or manual prompts)",
            "model_size": null,
            "data_type": "time series forecasting (uni- and multivariate depending on the work)",
            "data_domain": "healthcare, finance, transportation, and general forecasting benchmarks",
            "anomaly_type": "not directly applied to anomaly detection in the summarized descriptions (primarily forecasting)",
            "method_description": "Examples: Time-LLM reprograms time series into language-like inputs; Chronos tokenizes scaled/quantized values into fixed vocabulary; LLMTime uses numeric tokenization; PromptCast converts numeric sequences into template-based natural language prompts.",
            "baseline_methods": "Compared to statistical and transformer forecasting baselines in respective works; these papers reported outperforming or matching specialized models in few/zero-shot scenarios.",
            "performance_metrics": "Forecasting metrics (not enumerated here); reported to outperform statistical baselines and be competitive with efficient transformer models for forecasting.",
            "performance_results": "Qualitatively reported strong forecasting performance and promising zero/few-shot transfer; anomaly detection not the primary focus.",
            "comparison_to_baseline": "Often outperformed traditional statistical baselines and was competitive with specialized forecasting models in their respective evaluations.",
            "limitations_or_failure_cases": "These works focus on forecasting; additional components or adaptations would be needed to apply the same strategies directly to anomaly detection (particularly multivariate, domain-semantic enriched anomaly detection).",
            "unique_insights": "Demonstrates multiple technical routes (reprogramming, tokenization, templating) to make numeric time series consumable by LLMs; these ideas underpin the AAD-LLM strategy of converting anomaly detection into a language task.",
            "uuid": "e9273.4",
            "source_info": {
                "paper_title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SKAB",
            "name_full": "Skoltech Anomaly Benchmark (SKAB)",
            "brief_description": "A publicly available benchmark of labeled multichannel sensor signals used to evaluate anomaly detection algorithms; used in this paper to evaluate AAD-LLM on valve1 and valve2 datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_type": null,
            "model_size": null,
            "data_type": "multichannel/multivariate time series sensor signals (datetime, accelerometer RMS, current, pressure, temperature, voltage, rateRMS, anomaly labels, etc.)",
            "data_domain": "instrumented testbed sensor data for anomaly detection research",
            "anomaly_type": "labeled point anomalies and changepoints (binary anomaly labels provided in dataset)",
            "method_description": "Used as a benchmark: features selected via Mann-Whitney U test (selected Accelerometer1RMS, Accelerometer2RMS, Temperature, Thermocouple, RateRMS), preprocessed with SPC, windowed, and fed through AAD-LLM for evaluation.",
            "baseline_methods": "Multiple classical and neural anomaly detection algorithms from SKAB evaluations (see Table I) were used for comparison.",
            "performance_metrics": "Accuracy, F1, FAR, MAR (as reported in the paper's evaluation table).",
            "performance_results": "AAD-LLM on SKAB: reported accuracy 58.4%; reported F1 0.564; issues with z-score comparison errors reduced performance on some series (example drop 84.2% → 42.1%).",
            "comparison_to_baseline": "AAD-LLM performed worse than several specialized baselines on SKAB by F1 and had high FAR, though it remains unique in being multimodal and requiring no training on the benchmark.",
            "limitations_or_failure_cases": "Performance on SKAB notably affected by LLM numeric-comparison errors; benchmark evaluation highlighted the need for more robust numeric handling and error analysis.",
            "uuid": "e9273.5",
            "source_info": {
                "paper_title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Time-LLM: Time series forecasting by reprogramming large language models",
            "rating": 2,
            "sanitized_title": "timellm_time_series_forecasting_by_reprogramming_large_language_models"
        },
        {
            "paper_title": "Chronos: Learning the language of time series",
            "rating": 2,
            "sanitized_title": "chronos_learning_the_language_of_time_series"
        },
        {
            "paper_title": "LLMTime (effective numerical tokenization for LLMs on time series)",
            "rating": 1,
            "sanitized_title": "llmtime_effective_numerical_tokenization_for_llms_on_time_series"
        },
        {
            "paper_title": "PromptCast: A new prompt-based learning paradigm for time series forecasting",
            "rating": 1,
            "sanitized_title": "promptcast_a_new_promptbased_learning_paradigm_for_time_series_forecasting"
        },
        {
            "paper_title": "One fits all: Power general time series analysis by pretrained lm (OFA)",
            "rating": 2,
            "sanitized_title": "one_fits_all_power_general_time_series_analysis_by_pretrained_lm_ofa"
        },
        {
            "paper_title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series",
            "rating": 2,
            "sanitized_title": "test_text_prototype_aligned_embedding_to_activate_llms_ability_for_time_series"
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_time_series_forecasters"
        }
    ],
    "cost": 0.01572825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AAD-LLM: Adaptive Anomaly Detection Using Large Language Models</p>
<p>Alicia Russell-Gilbert 
Computer Science &amp; Engineering
Mississippi State University</p>
<p>Alexander Sommers 
Computer Science &amp; Engineering
Mississippi State University</p>
<p>Andrew Thompson 
Computer Science &amp; Engineering
Mississippi State University</p>
<p>Logan Cummins 
Computer Science &amp; Engineering
Mississippi State University</p>
<p>Sudip Mittal mittal@cse.msstate.edu 
Computer Science &amp; Engineering
Mississippi State University</p>
<p>Shahram Rahimi rahimi@cse.msstate.edu 
Computer Science &amp; Engineering
Mississippi State University</p>
<p>Maria Seale maria.a.seale@erdc.dren.mil 
Development Center Department of Defence
Engineer Research</p>
<p>Joseph Jaboure joseph.e.jabour@erdc.dren.mil 
Development Center Department of Defence
Engineer Research</p>
<p>Thomas Arnold thomas.l.arnold@erdc.dren.mil 
Development Center Department of Defence
Engineer Research</p>
<p>Joshua Church joshua.q.church@erdc.dren.mil 
Development Center Department of Defence
Engineer Research</p>
<p>AAD-LLM: Adaptive Anomaly Detection Using Large Language Models
42AAA29B5015622383BFA6EBAEE1D051large language modelsLLMs for time series taskspredictive maintenanceadaptive anomaly detection
For data-constrained, complex and dynamic industrial environments, there is a critical need for transferable and multimodal methodologies to enhance anomaly detection and therefore, prevent costs associated with system failures.Typically, traditional PdM approaches are not transferable or multimodal.This work examines the use of Large Language Models (LLMs) for anomaly detection in complex and dynamic manufacturing systems.The research aims to improve the transferability of anomaly detection models by leveraging Large Language Models (LLMs) and seeks to validate the enhanced effectiveness of the proposed approach in data-sparse industrial applications.The research also seeks to enable more collaborative decision-making between the model and plant operators by allowing for the enriching of input series data with semantics.Additionally, the research aims to address the issue of concept drift in dynamic industrial settings by integrating an adaptability mechanism.The literature review examines the latest developments in LLM time series tasks alongside associated adaptive anomaly detection methods to establish a robust theoretical framework for the proposed architecture.This paper presents a novel model framework (AAD-LLM) that doesn't require any training or finetuning on the dataset it is applied to and is multimodal.Results suggest that anomaly detection can be converted into a "language" task to deliver effective, context-aware detection in data-constrained industrial applications.This work, therefore, contributes significantly to advancements in anomaly detection methodologies.</p>
<p>I. INTRODUCTION</p>
<p>We rely on the operation of a variety of engineered systems, which degrade with use and may lead to failures of varying degrees of severity.These failures include unexpected stoppages, product waste, damage to equipment, and bodily harm which have consequences that range from annoying to disastrous.Maintenance practices are crucial to prevent such failures.Condition-based maintenance (CBM) involves performing maintenance actions based on the conditions of a system.Maintenance decisions based on the conditions of a system are often desirable because they allow for proactive and targeted actions.Predictive maintenance (PdM) leverages machine learning to enhance this decision-making process.</p>
<p>PdM becomes more challenging under common real world conditions.Sensor data collected from machines is often nonstationary due to a combination of factors such as varying operational settings and individual machine deterioration [1].This causes heterogeneous relationships between the sensor data and system health; thereby requiring that the normative profile used to identify degradation be updated regularly [2], [3].We thus do not employ traditional PdM methods, instead employing an adaptive approach.The former cannot account for shifts in sensor data statistical characteristics, while the latter can, all while maintaining high fault detection accuracy.</p>
<p>Unique production system structures and domain-specific constraints necessitate tailored approaches to effectively deploy PdM in diverse industrial settings.The use of expert knowledge into our target use-case facilitate a robust and domain-specific PdM implementation commensurate to these complexities.However, domain-specific knowledge (e.g.process parameter optimal ranges, equipment specifications, etc.) cannot usually be applied across domains and therefore would limit model application across industrial settings.Therefore, retraining or finetuning on the applied dataset with related domain-specific knowledge would typically be required.</p>
<p>It is ideal for models to require infrequent training on a limited dataset yet yield robust generalization capabilities.This is because some critical assets are not allowed to run to failure; thus, event data needed to fine-tune or retrain some machine learning algorithms may be scarce [4].Transferable models that excel in "few-shot" and "zero-shot" scenarios across related domains appear promising.Recent work suggests that pretrained Large Language Models (LLMs) offer noteworthy few/zero-shot capabilities and transferability [5]- [7].Furthermore, the extension of LLMs beyond natural language to the time series domain showcases their broader potential [8], [9].In particular, the application of pretrained LLMs to the problem of anomaly detection for PdM on time series data can improve the transferability of other approaches in dataconstrained environments.</p>
<p>To optimize PdM model effectiveness, it is crucial to develop models that are both adaptable and transferable.Adaptable models can adjust to changing conditions, ensuring continued relevance over time.Transferability enables these models to be applied across diverse systems and domains, increasing usability and practicality.By combining adaptability and transferability, PdM models become versatile tools that can evolve with operational environments and be leveraged on a variety of industrial datasets.Furthermore, a model that is optimized to detect anomalies across diverse input sources could enable more synergistic forecasting and foster more collaborative decision-making between the model and plant operators.</p>
<p>The main contributions of this work are as follows:</p>
<p>• We explore repurposing pretrained LLMs for the PdM use-case.More specifically, pretrained LLMs are explored for use in anomaly detection within manufacturing time series data.Thus, we aim to examine LLMs' efficacy beyond conventional forecasting applications.• We present a novel anomaly detection framework (AAD-LLM) utilizing pretrained LLMs for improved transferability in data-constrained contexts.The improved transferability is shown to reduce the need to retrain between domains and systems.Additionally, the framework is shown to enable the enrichment of input time series data with semantics to deliver more collaborative decisionmaking between the model and plant operators.• We leverage an adaptability mechanism that enables the model to adjust to evolving conditions, consequently enhancing detection accuracy.The remaining sections of this paper are as follows.Section II discusses the background and foundational work for our proposed methodology.Section III examines the state-of-theart in LLM time series tasks and adaptive anomaly detection methods.Section IV provides insight on the AAD-LLM architecture and methodology.Section V explains experimental results and implications of findings.Finally, Section VI concludes the paper and discusses limitations for future work.</p>
<p>II. BACKGROUND</p>
<p>This section serves as a background for understanding LLMs and adaptive anomaly detection as presented in this paper.It aims to provide key terms, baseline definitions, and relevant mathematical notations that are essential for comprehending the concepts discussed.Additionally, this section briefly discusses the initial stages of our research endeavor.It describes the preliminary investigations conducted to lay the groundwork for our current work.</p>
<p>A large language model (LLM) is trained on sequences of tokens and encodes an auto-regressive distribution, where the probability of each token depends on the preceding ones [9].More simply, an LLM is trained on sequences of words or word pieces, and the output is the likelihood of the next word in a sequence given the previous words (i.e., context-aware embeddings).Each model includes a tokenizer that converts input strings into token sequences.Models like GPT-3 and LLaMA-2 can perform zero-shot generalization, effectively handling tasks without specific training [9].For this work, we repurpose an LLM for time series anomaly detection while keeping the backbone language model intact [10].A binarization function is then applied to the outputs of the LLM to map them to {0, 1} to obtain the final predictions.The exact binarization function is use-case specific.</p>
<p>Transfer learning is a machine learning technique where the knowledge gained through one task is applied to a related task with low/no retraining [11].Specifically, in transfer learning, we train a model to perform a specific task on the source domain and then make certain modifications to give us good predictions for a related task on the target domain where data is (usually) scarce or a fast training is needed [12].For this work, we leverage a pretrained LLMs' text synthesizing and reasoning abilities acquired through training on a source domain by transferring this task knowledge to our PdM use-case.Specifically, we show that pretrained LLMs can effectively predict anomalies in time series data by transferring its text synthesizing and reasoning knowledge to our target manufacturing domain.</p>
<p>In the context of transfer learning, generalizability refers to the ability of a pretrained model to perform well on new domains as well as new data within the same domain [13].For example, in manufacturing, the statistical properties of raw material attributes change over time.Therefore, if these variables are used as product quality predictors, the resulting models may decrease in validity.This illustrates that even in the same domain, models may have trouble generalizing from one time point to another [14].This also illustrates that the issue of generalizability connects to the issue of concept drift [13].Concept drift is the phenomenon where the statistical properties of a domain changes over time, which can then result in a deterioration of models that have previously been trained within that domain [13], [14].In particular, it can lead to a degradation of performance of static models as they become less effective in detecting anomalies.</p>
<p>Adaptive anomaly detection (AAD) encompasses techniques that can detect anomalies in data streams or in situations where concept drift is present.These techniques make models capable of automatically adjusting their detection behavior to changing conditions in the deployment environment or system configuration while still accurately recognizing anomalies [2], [3].For this work, the adaptability mechanism refers to the feature that enables the model's definition of normality and related statistical derivatives to adjust with each new data instance.</p>
<p>Windowing refers to dividing a time series into smaller, manageable segments, which are then processed individually.Windowing (or sliding window technique) is used extensively for anomaly detection in time series data due to its many benefits [15].For our use-case, dividing the time series into windows helps to preserve local information that might be lost when considering the entire time series as a whole and reduce computational load since models can handle smaller inputs more efficiently.</p>
<p>A process is said to be "in statistical control" if it is not experiencing out of control signals or significant variations beyond normal statistical variations [16].Statistical process control (SPC) techniques are commonly used in manufacturing for monitoring sequential processes (e.g., production lines) to make sure that they work stably and satisfactorily [17].In monitoring the stability of a process, statistical process monitoring (i.e., SPC) plays an essential role [18], [19].The idea is that processes that are in statistical control are deemed to be stable processes [16].For this work, stable processes form a baseline for normal process behavior.The selection of SPC techniques are use-case specific.For this work, MAMR is implemented.Moving average moving range (MAMR) charts are plotted for each process variable in the time series dataset as shown in Figure 3. Upper (UCL) and lower (LCL) control limits for the moving average (X) and moving range (mR) charts are calculated as follows.</p>
<p>X Chart:
U CL = X + 2.66R(1)LCL = X − 2.66R(2)
mR Chart:
U CL = 3.27R(3)
The values 2.66 and 3.27 are often used as multipliers for estimating control limits in the MAMR chart.However, these multipliers can significantly widen the control limits, making them less sensitive to minor shifts or variations in the process.Therefore, it is important to analyze historical data to determine the typical variability in the process under consideration and select multipliers that reflect the process's actual behavior while maintaining sensitivity.</p>
<p>Through a thorough review of existing literature, initial data collection, and exploratory data analysis, we aimed to gain insights into the complexities of anomaly detection for PdM and to establish a basis for our subsequent research inquiries.This preliminary work allowed us to uncover initial patterns, trends, and areas of interest, shaping the development of our research framework and guiding the formulation of our research hypothesis for the current work.</p>
<p>In a specific use-case study conducted for a leading plastics manufacturing plant, the implementation of anomaly detection algorithms for PdM stood as a cornerstone for the research presented in this paper.An overview of the plastics extrusion process for our use-case can be seen in Figure 1.Through anomaly detection, we sought to understand why certain processes failed.Anomalies often indicate underlying factors contributing to process failures.By investigating and correlating anomalies with other process variables, operators and analysts can uncover the root causes of failures.This deeper understanding enables targeted corrective actions to address underlying issues, as well as to better predict future failures.</p>
<p>Our investigation focused on screen pack failures since shutdowns due to these failures were well documented in the data.An example of a screen pack changer can be seen in Figure 2.For two downtime events with screen pack failure mode, we obtained 65 hours of historical run-tofailure sensor readings (6.5 hours for 5 components for each downtime event).The readings were semi-labeled and for process variables that were deemed good indicators of screen pack failures.These process variables are Melt Pressure 1, Temperature 1, and Melt Pressure Differential.Melt Pressure 1 is the melt viscosity at the screen inlet.Temperature 1 is the melt temperature at the screen pack inlet.Melt Pressure Differential is the melt pressure across the screen pack inlet and outlet.For any of these, sudden spikes from expected profile could signal significant process variable deviations; and therefore, could lead to a screen pack failure.</p>
<p>The data, however, was scarce and noisy; and therefore, presented many challenges in its analysis.While we were able to identify shutdown causes through maintenance logs, we lacked information on where, when, and how failures occurred.To understand why failures happen, it's essential to synthesize this information to uncover root causes, systemic issues, or contributing factors.Additionally, the lack of a general baseline for normal process behavior exacerbated data analysis challenges.The variability in statistical characteristics across processes was influenced by product mix, component age and wear, as well as the heterogeneity of component types.</p>
<p>For the PdM use-case, anomaly detection techniques utilized early on overcame these challenges and showed excellent results.By leveraging domain-specific knowledge, expert rules, sensor readings, and maintenance logs; alongside adaptability methods, we successfully developed a statistics-based predictive model to detect anomalies with 83.3% accuracy.The use case demonstrated the feasibility of anomaly detection for PdM in complex, dynamic, and data-constrained industrial settings.It also highlighted the potential for substantial cost savings, minimized downtime, and enhanced asset reliability.</p>
<p>The initial approach, while consistent with traditional PdM methods, lacked transferability and multimodality.The statistics-based model was an ensemble approach that relied on domain-specific rules and knowledge to perform well.Domain-specific knowledge (e.g.process parameter optimal ranges, equipment specifications, etc.) cannot usually be applied across domains and therefore would limit model application across diverse industrial settings.Furthermore, the absence of multimodality hindered fusion of diverse input sources (e.g., series data and maintenance logs) for synergistic forecasting.This limitation prevented the enrichment of input sensor readings with semantics, impeding collaborative decision-making between the model and plant operators.</p>
<p>III. PRIOR ART</p>
<p>A. LLMs for Time Series Tasks</p>
<p>Traditional analytical methods that rely on statistical models and deep learning methods based on recurrent neural networks (RNNs) have dominated the domain of time series forecasting.LLMs, however, have recently emerged in the arena of time series forecasting and have made significant progress in various fields like healthcare, finance, and transportation [8].Time-LLM [10] proposed a novel framework repurposing LLMs for time series forecasting without requiring any fine-tuning of the backbone model.This was achieved by "reprogramming" time series data inputs for compatibility with LLMs; thereby, converting time series forecasting into a "language" task.An LLM's advanced reasoning and pattern recognition capabilities could then be leveraged to achieve high precision and efficiency in forecasts.Time-LLM was shown to outperform specialized models in few-shot and zeroshot scenarios.</p>
<p>Similarly, Chronos [20] proposed the use of LLMs for time series forecasting.However, it avoided reprogramming the time series data which requires training on each input dataset separately.Instead, time series data was tokenized into a fixed vocabulary via scaling and quantization.The Chronos model outperformed statistical baselines and other pretrained models in both in-domain and zero-shot scenarios across multiple benchmarks.</p>
<p>LLMTime [9] also proposed the use of LLMs for time series forecasting.Rather than requiring learned input transformations or prompt engineering like Time-LLM did, time series data was tokenized like with Chronos but with a different scheme.In fact, for this framework, effective numerical tokenization was essential in ensuring accurate and efficient forecasting by the LLMs.LLMTime outperformed traditional statistical models and models from the Monash forecasting archive.Furthermore, it was competitive with and sometimes outperformed efficient transformer models.</p>
<p>PromptCast [21] also introduced a novel approach to time series forecasting using LLMs.Like Time-LLM, numerical sequences are described and transformed to natural language sentences.However, PrompCast used manually-defined template-based prompting rather than learning input transformations for automatic prompting.While explored for only uni-step forecasting, the results indicated that the PromptCast approach not only achieved performance that was comparable to traditional numerical methods but sometimes even surpassed them.</p>
<p>These prior works suggest the emergence of multimodal models that excel in both language and time series forecasting tasks.However, these works presented LLMs for use in only time series forecasting and did not explore other time series tasks like anomaly detection.In separate works, however, LLMs have emerged for other time series tasks and have been shown to excel.Time series tasks typically include four principal analytical tasks: forecasting, classification, anomaly detection, and imputation [8].</p>
<p>Zhou et al. [22] introduced a unified framework (referred to as OFA [8]) using frozen pretrained LLMs for performing various time series analysis tasks.Like Time-LLM, OFA required training the input embedding layer to acquire learned time series representations.However, rather than only time series forecasting, it explored the use of LLMs for univariate anomaly detection.OFA achieved superior or comparable results in classification, forecasting, anomaly detection, and few-shot/zero-shot learning.The TEST method [23] aligns time series embeddings with LLMs to enhance their capability to perform time series tasks without losing language processing abilities.While the exact embedding function was not specified, learning input transformations typically involves neural network training.Therefore, like Time-LLM, TEST also required training the input embedding layer.However, like OFA, TEST explored the use of LLMs for other time series tasks.Compared to state-of-the-art (SOTA) models, TEST demonstrated superior performance on various tasks including univariate time series forecasting, as well as multivariate classification tasks.While achieving good performance on multiple time series tasks, neither OFA or TEST explored multivariate anomaly detection.</p>
<p>Multivariate analysis allows for joint reasoning across the time series.Joint reasoning enables a model to blend and merge the understanding from different sensors and data sources to make decisions that are impossible when considering data in isolation.For example, in our use-case, the temperature alone may not sufficiently indicate a problem since operators might adjust the temperature to try and maintain material flow despite a screen pack blockage.By monitoring both pressure and temperature, it's possible to detect joint anomaly events that are more indicative of clogging.Furthermore, there were no papers that explored LLMs for the PdM use-case.</p>
<p>B. Adaptive Anomaly Detection</p>
<p>Advancement in anomaly detection through adaptability has been explored extensively.Traditionally, most AAD algorithms have been designed for datasets in which all the observations are available at one time (i.e., static datasets).However, over the last two decades, many algorithms have been proposed to detect anomalies in "evolving" data (i.e., data streams) [24].Although the proposed methodology could possibly be modified for data streams, we only focus on static datasets in this paper.</p>
<p>Machine learning (ML) techniques have been used for AAD implementation and have been shown to improve performance baselines of non-adaptive models in various scenarios such as industrial applications [25], network security [26], and environmental science [24].However, these techniques only focus on the data themselves.While effective, these approaches may overlook contextual information and domainspecific knowledge crucial for accurate anomaly detection.A system which fuses both ML and semantics improves the accuracy of anomaly detection in the data by reducing the number of false positives [2], [3].This is because integrating semantics into the anomaly detection process allows for a more comprehensive analysis that considers both the data patterns and their contextual relevance.A system like this would enable more collaborative decision-making between the model and plant operators.</p>
<p>Semantics such as the following could greatly enhance anomaly detection as it provides insight into the severity of the anomaly: "Domain-specific knowledge indicates that there are correlations between process variables.Specifically, increased melt pressure at the screen pack inlet may lead to increased melt temperature at the screen pack inlet.Additionally, increased melt pressure at the screen pack inlet may lead to decreased melt pressure at the screen pack outlet.If these correlations are observed, it indicates a high level of criticality for potential failures.".In this case, plant operators may want to imply that if an anomaly is not severe enough, then it is a false positive; and therefore, should not trigger a manual shutdown.Unlike ML models, LLMs can easily integrate this knowledge for the anomaly detection task.</p>
<p>There have been previous works that incorporate expert knowledge with ML algorithms.Steenwinckel et al. [3] proposed a knowledge graph representation of expert rules which was then transformed directly into matrix form.A reinforcement learning (RL) agent was used as a rule extractor of the matrix (i.e., semantic rule mining) to enable model adaptability.Adaptability here refers to the mechanism for extracting rules dynamically to better reflect the current state of the environment and ensure continued accuracy.The work, however, was limited as it did not provide a methodology for detecting anomalies in series data using the expert rules as enrichment.</p>
<p>FLAGS [2] integrated data-driven and knowledge-driven approaches to deliver adaptive, context-aware anomaly detection.The Semantic Mapping module is responsible for enriching the incoming data streams with expert rules and context information.Adaptability here refers to the merging, deleting, or relabeling of anomalies to cope with user-provided feedback; and dynamic rule extracting.FLAGS is an ensemble architecture that uses one ML model to detect anomalies and another that fuses semantics to determine whether they are true anomalies.Although the FLAGS architecture allows for the use of any appropriate ML models, non-LLM models are largely statistical without much innate reasoning [10].</p>
<p>LLMs, on the other hand, demonstrate advanced abilities in reasoning, data synthesis, and pattern recognition [27], [28].Therefore, since pretrained LLMs have been shown to perform well on various time series tasks, leveraging their learned higher level concepts could enable highly precise and synergistic detection across multiple modalities [10].While traditional statistical models may require more specialized training, LLMs have the ability to perform well with less data and without extensive retraining.This is extremely advantatageous in dataconstrained operational settings.</p>
<p>IV. METHODOLOGY</p>
<p>We assume to have a pretrained LLM as our foundation model.For this work, Meta Llama 3 8B model was chosen since it is an open source instruction-fine-tuned LLM demonstrating SOTA on tasks such as classification, question answering, extraction, reasoning, and summarizing [29].We aim to repurpose this model to detect anomalies in a time series without requiring any fine-tuning.Specifically, this work seeks to test the hypothesis that an LLM can effectively identify anomalies in given time series data for the PdM use-case.</p>
<p>Additionally, we aim to increase the likelihood of accurately predicting anomalies as the model adapts to temporal changes.To achieve this, we propose an adaptability mechanism so that with each new data instance, the model is instructed to update its understanding of what constitutes baseline "normal" behavior.By utilizing SPC techniques to establish the baseline, this methodology does not require the model to "learn" normal behavior.Therefore the model maintains high transferability with zero-shot capabilities.UCL is the defined upper control limit and LCL is the defined lower control limit.Series data points outside of control limits are deemed "out of statistical control" and are labeled as anomalous.Out of control points can be seen before line (1).Points between lines (1) and ( 2) represent a stable process.Points after line (2) also represent a stable process, however, they are trending towards out of control.These points, therefore, are potentially problematic.AAD-LLM is applied to all points within control limits to enhance anomaly detection.</p>
<p>We then apply a binarization function to the LLM's output to accurately classify the time series as anomalous/nonanomalous.The exact binarization function is use-case specific as described in Section IV-E.The backbone of the pretrained LLM is frozen.The architecture of our proposed framework, referred to as AAD-LLM, incorporates the following main elements: (1) domain-specific knowledge and rules, (2) a comparison dataset C, (3) a pretrained and frozen LLM, and (4) an output mapping function.In the following paragraphs, we explain the model architecture in greater detail.</p>
<p>A. Data and Analysis</p>
<p>For two consecutive downtime events in November of 2023, we obtained 65 hours of run-to-failure sensor readings as described in Section II.Python scripts were used to preprocess this data using the SPC techniques presented previously.Additionally, Python scripts were used to develop the model architecture (AAD-LLM) and to evaluate its performance.Furthermore, a publicly available dataset [30] was chosen to be implemented by the proposed methodology to compare with selected benchmarks.</p>
<p>The Skoltech Anomaly Benchmark (SKAB) is a dataset designed for evaluating the performance of anomaly detection algorithms.The benchmark includes 34 labeled datasets of signals captured by several sensors installed on the SKAB testbed.The SKAB testbed was specifically developed to study anomalies in a testbed.The focus of this work is to develop methods for detecting anomalies in these signals, which can be relevant for various applications.</p>
<p>We implemented AAD-LLM on the SKAB dataset for both valves.The valves are valve1 which is the outlet of the flow from the pump; and valve2 which is the flow inlet to the pump.A description of the columns in each dataset is as follows [30].</p>
<p>• datetime -Dates and times when the value collected • Accelerometer1RMS -Vibration acceleration (g units) • Accelerometer2RMS -Vibration acceleration (g units)</p>
<p>• Current -The amperage on the electric motor (Ampere) The anomaly column contains the labels.A Mann-Whitney-Wilcoxon test (or Mann-Whitney U test) was used to determine whether any of the features in the dataset affected the labels.It was determined with 95% confidence that only Accelerometer1RMS, Accelerometer2RMS, Temperature, Thermocouple, and RateRMS affected the labels.Therefore, only these were input into AAD-LLM to make the predictions.Doing this decreased computational time and improved accuracy, precision, recall, and F1 score.</p>
<p>B. Domain Context and Text Templates</p>
<p>To facilitate collaboration with plant operators, we first construct a domain-specific context file, which enables the LLM to understand the context of our time series data.This file incorporates expert rules, domain-specific knowledge, and constraints to define acceptable ranges of process variable variations, guide feature selection, and highlight causal relationships among variables.Real manufacturing data often contains many sensor readings.For our use-case, there are 580 sensors per line but operators can correlate them with failure modes.Furthermore, a change in raw materials often requires a significant change in use-case process parameter values but staff polymer scientists know related thresholds.By leveraging this information, algorithms can refine thresholds, select relevant features, and identify interactions, ultimately enhancing anomaly detection performance.This domain context is gathered in a context file, then imported and cached for improved performance and resource efficiency.</p>
<p>To enable structured understanding and improved performance, we create a set of text templates with placeholders for statistical values.Once actual data becomes available, these placeholders will be replaced through data injection.The templates are designed to align with specific statistical derivatives (mean, standard deviation, and maximum as detailed in Section II) important for anomaly detection.By combining mean and standard deviation as a z-score, LLMs can focus on reasoning without performing calculations.Statistical derivatives for both normal system behavior and the query window under consideration are injected into their respective text templates as further explained in Section IV-D.The injected text templates guide the LLM's reasoning, enhancing its performance in detecting anomalies.</p>
<p>C. Initializing the Comparison Dataset</p>
<p>Defining normal process behavior is essential for effective anomaly detection.Establishing normal process behavior provides a baseline against which anomalies can be compared and detected.Without a clear understanding of normal conditions, it's challenging to detect abnormal patterns.A welldefined normal process behavior also enables algorithms to differentiate between regular variations and true anomalies, reducing false positives and improving issue identification.However, establishing a universal definition of normalcy is difficult due to the complexity and variability inherent in different manufacturing processes.</p>
<p>Considering the aforementioned, the next step of our proposed algorithm is to initialize datasets C i that represent normal process behavior.From the dataset D under consideration, a multivariate time series instance Q ∈ R N ×T is partitioned into N univariate time series where N is the number of input variables and T is the number of time steps.This is done so that each input variable is processed independently [10].Each i th series Q i , i ∈ N , is preprocessed using SPC techniques as shown in Figure 3.Time series points deemed "out of statistical control" are labeled as anomalous and filtered out of Q i before further processing.SPC is applied again after the first set of outliers (or anomalies) are removed.This is done to ensure extreme values do not affect control limits.Therefore, it can be assumed that time series Q i represents a stable process.We use this assumption in initializing our comparison dataset C i as our baseline for normal behavior as explained in the next section.The idea is that once the comparison dataset is initialized, the model then updates its understanding of normalcy as each new query window is ingested.</p>
<p>D. Windowing and Prompting</p>
<p>Rather than processing the entire time series at once, Q i then undergoes windowing as shown in Figure 4.For each i ∈ N , windowing divides time series Q i into P consecutive non-overlapping segments of length L, Q (P ) i ∈ R P ×L .By analyzing data within sliding windows, anomaly detection can focus on smaller segments of the time series data.This provides a more granular and detailed view of abnormal patterns.Processing the entire time series as a single entity might obscure localized anomalies within the data.Finally, for each i ∈ N , a baseline dataset C i ∈ R 1×L of normal behavior is defined as the first Q i window.</p>
<p>The selection of optimal statistical derivatives is crucial for effective anomaly detection algorithms.By choosing the right derivatives, algorithms can focus on relevant data features, enhancing accuracy and reducing false positives.Different industries and applications have distinct anomaly detection requirements, necessitating customization through appropriate statistical derivatives.In this study, as stated previously in Section IV-B, z-score and maximum are the selected statistical derivatives.</p>
<p>Calculating the statistical derivatives is the next step in our proposed algorithm.For each i ∈ N , statistical derivatives for both the current Q i window Q (p) i where p ∈ P , and C i are calculated and then injected into the text templates.Prompts are then created via prompt engineering and combined with the templates for Q (p) i and C i for each i ∈ N .To further enrich the inputs, the domain context is added to the prompt before being fed forward through the frozen LLM.A prompt example is shown in Figure 5.For our methodology, the domain context was manually restructured from the "raw" domain context to better guide the LLM's decision making, thereby enabling more consistent predictions.Effective prompt engineering and domain context structure is essential in ensuring accurate, context-aware anomaly detection.</p>
<p>E. Output Binarization Function and Updating C i</p>
<p>Upon creating the prompt, it is fed forward through the frozen LLM to obtain the outputs.Lastly, we apply a binarization function to the outputs to map them to {0, 1} to get the final classification (0 = non-anomalous, 1 = anomalous).The exact binarization function is use-case specific.For our use-case, one anomaly alone does not sufficiently indicate a problem.To avoid false positives that trigger an unnecessary shutdown, our binarization function only maps to 1 if anomalies in the output are correlated as indicated by domain-specific knowledge.Let x be the LLM output.Then
f (x) = 1, if anomalies in x are correlated 0, otherwise(4)
The final classification is what is used for determining updates to C i before moving to the next
Q (p) i . If the output prediction indicates no anomalies in Q (p) i , window Q (p) i
series data is combined with the preceding windows series data to gradually refine the dataset of what constitutes "normal" behavior C i .Therefore, for each i ∈ N , C i is guaranteed to be representative of normal behavior and is constantly evolving.</p>
<p>Given the following context and data, determine whether there are any anomalies present.CONTEXT: <cached info> DATA: The following sensor data was collected over the last 15 minutes and represent current process conditions.Temperature 1 has a maximum of <val> and a z-score of <val>.Normal operating conditions for Temperature 1 is <val>.Melt Pressure 1 has a maximum of <val> and a z-score of <val>.Normal operating conditions for Melt Pressure 1 is <val>.Melt Pressure Differential has a maximum of <val> and a z-score of <val>.Normal operating conditions for Melt Pressure Differential is <val>.Fig. 5: Prompt example.<cached info> is the domain context information.<val> are calculated statistical derivatives injected into respective text templates.Note that although each Q i is processed independently, prompts include text templates for all i ∈ N where N is the number of input variables in instance Q from the dataset D under consideration.Therefore, multivariate anomaly detection is explored.</p>
<p>F. Adaptability Mechanism</p>
<p>In addition to C i constantly updating as each new query window is ingested, the process of re-initializing C i is done for each new instance Q.This continuous redefining of the normal baseline enables the model to progressively refine its knowledge in response to shifts in the system's operational conditions process after process.Therefore, the model is enabled to maintain an up-to-date and broad perspective of normality.</p>
<p>V. RESULTS AND DISCUSSION</p>
<p>Our brief results are shown in Table I.For the use-case dataset, the model achieved an accuracy of 70.7%, indicating the proportion of correctly classified instances among all instances was good.Furthermore, an F1 score of 77.0% signifies that the model achieves a balanced performance in terms of precision and recall.This suggests that the model effectively balances correctly identifying both anomalous and non-anomalous series data, offering a robust performance overall.For the SKAB dataset, the model achieved an accuracy of 58.4%.While an accuracy of 58.4% is better than random guessing (50% accuracy), it indicates that the model's prediction performance may be limited.However, upon inspecting the LLM's output for the last input series, z-score comparison errors caused the accuracy to decrease from 84.2% to 42.1%.Comparison errors affected the benchmark dataset performance more than the use-case dataset performance.More information regarding these comparison errors can be found in Section VI Table II summarizes the scores for algorithms on 3 application benchmarks using the data files in SKAB, sorted by F1 score.For F1 score, bigger is better.For both FAR and MAR, less is better.AAD-LLM ranks 8 th among 11 algorithms in F1 score, 5 th in MAR, and last in FAR.Investigation of the LLM's output for the last input series suggests that the FAR score was negatively affected by z-score comparison errors.</p>
<p>This study provides compelling insights into the effectiveness of leveraging LLMs for anomaly detection tasks, particularly in PdM.Results show the successful repurposing of LLMs for anomaly detection, demonstrating their ability to detect anomalies in time series data accurately, even in complex operational settings.These findings confirm the feasibility and strong performance of LLMs in anomaly detection tasks, showcasing their versatility and robustness.The research supports the use of LLMs for anomaly detection in PdM, underlining their capability and potential in handling challenges in time series anomaly detection, especially in dataconstrained industrial applications.This study significantly advances anomaly detection methodologies.</p>
<p>VI. CONCLUSION AND FUTURE WORK</p>
<p>Assuming that domain-specific knowlege is available for leveraging, AAD-LLM shows promise in repurposing LLMs for anomaly detection.This is because it allows for greater transferability of anomaly detection models and for enriching time series data with domain-specific knowledge to enable more collaborative decision-making between the model and plant operators.Furthermore, the adaptive mechanism enables the model to progressively refine its understanding of baseline behavior in response to shifts in the system's operational conditions over time.Therefore, the model can maintain an up-to-date perspective of normal system behavior in order to make accurate predictions despite the dynamic nature of operational settings.By utilizing SPC techniques to establish the baseline, this methodology does not require the model to "learn" normal behavior.Therefore the model maintains high transferability with zero-shot capabilities.The results suggest that anomaly detection can be converted into a language task to deliver effective, context-aware detection in data-sparse industrial applications.</p>
<p>Although evaluation metrics demonstrate proof of concept, performance can be improved.The LLM we used was not consistent in accurately making comparisons between statistical derivatives and did not perform calculations well.This lead to a decrease in model performance.Further work should explore adding a Retrieval-Augmented Generation (RAG) pipeline to retrieve relevant mathematical information or formulas from external knowledge sources; thus aiding the model in performing comparisons and math calculations more accurately.</p>
<p>Additionally, the framework requires manually restructuring the "raw" domain context to better guide the LLM's decision making; thereby enabling it to make more consistent predictions.This restructured domain context does not read in the way real maintenance logs or operator instructions would naturally read.Further work should explore training a neural network for automatic domain context sentence restructuring.Finally, AAD-LLM was applied to only static datasets to better understand how processes failed after the failure had already occurred.However, further work should explore extending the methodology to data streams for online anomaly detection.</p>
<p>Fig. 1 :
1
Fig. 1: Process flow diagram of major components in our usecase extrusion process.The major components in the extrusion process are in a series configuration.The number of Feed and Screw/Barrel Systems depends on the manufacturing line number and can be 3, 4, or 5.</p>
<p>Fig. 2 :
2
Fig.2: The die head system for our use-case.The screen pack changer is identified by a red box.Within the screen pack changer, screen packs are used to prevent impurities from getting into the extruder together with the resin and thus clogging the die gap.The number of screen packs depend on the number of Screw/Barrel Systems.Each screen pack is arranged between the Screw/Barrel System and the Die Head System.During production, the resin melts flow through the screen pack.</p>
<p>Fig. 3 :
3
Fig.3: SPC technique of moving average moving range to set control limits for process stability in a query series Q i .FigureA and Figure Bare moving average and moving range, respectively.UCL is the defined upper control limit and LCL is the defined lower control limit.Series data points outside of control limits are deemed "out of statistical control" and are labeled as anomalous.Out of control points can be seen before line(1).Points between lines (1) and (2) represent a stable process.Points after line (2) also represent a stable process, however, they are trending towards out of control.These points, therefore, are potentially problematic.AAD-LLM is applied to all points within control limits to enhance anomaly detection.</p>
<p>•</p>
<p>Pressure -The pressure in the loop after the water pump (Bar) • Temperature -The temperature of the engine body (°C) • Thermocouple -The temperature of the fluid in the circulation loop (°C) • Voltage -The voltage on the electric motor (Volt) • RateRMS -The circulation flow rate of the fluid inside the loop (Liter per minute) • anomaly -If the point is anomalous (0 or 1) • changepoint -If the point is a changepoint (0 or 1)</p>
<p>Fig. 4 :
4
Fig. 4: The model framework of AAD-LLM.Given an input time series Q from the dataset D under consideration, we first preprocess it using SPC techniques.Then (1) Q is partitioned into a comparison dataset C and query windows Q (p) , where p ∈ P and P is the number of segmented windows.Next, statistical derivatives for C and Q (p) are calculated and (2) injected into text templates.These templates are combined with task instructions to create the input prompt.To enhance the LLM's reasoning ability, (3) domain context is added to the prompt before being fed forward through the frozen LLM.The output from the LLM is (4) mapped to {0, 1} via a binarization function to obtain the final prediction.(5) Updates to C are determined before moving to the next Q (p) .</p>
<p>TABLE I :
I
AAD-LLM evaluation metrics.
AlgorithmF1FAR, %MAR, %No Training or FinetuningMultimodalPerfect detector100LSTMCaps [31]0.7421.518.74nonoMSET [32]0.7320.8220.08nonoLSTMCapsV2 [31]0.7114.5130.59nonoMSCRED [33]0.716.230.87nonoVanilla LSTM [34]0.6715.4236.02nonoConv-AE [35]0.665.5846.05nonoLSTM-AE [36]0.6514.5939.42nonoAAD-LLM0.5647.631.7yesyesLSTM-VAE [37]0.569.254.81nonoVanilla AE [38]0.457.5566.57nonoIsolation forest [39]0.46.8672.09nonoNull detector0100100</p>
<p>TABLE II :
II
[31] outlier detection scores for each anomaly detection method implemented on the SKAB dataset, sorted by F1 score[31].A selection of NNs and ML based fault detection methods were chosen to compare on the benchmarks.Multimodality allows for the enriching of input series data with semantics to enable more collaborative decision-making between the model and plant operators.For this work, multimodality refers to a model being optimized to detect anomalies across both time series data and text.A model that requires no training or finetuning on the data it is applied to is conidered transferable with zero-shot capabilities.Unlike all other methods, AAD-LLM is not trained or finetuned on the dataset it is applied to and is multimodal without requiring any additional strategies.</p>
<p>This material is based upon work supported by the Engineering Research and Development Center -Information Technology Laboratory (ERDC-ITL) under Contract No. W912HZ23C0013.Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the ERDC-ITL.
Online anomaly detection with concept drift adaptation using recurrent neural networks. S Saurav, P Malhotra, V Tv, N Gugulothu, L Vig, P Agarwal, G Shroff, Proceedings of the acm india joint international conference on data science and management of data. the acm india joint international conference on data science and management of data2018</p>
<p>Flags: A methodology for adaptive anomaly detection and root cause analysis on sensor data streams by fusing expert knowledge with machine learning. B Steenwinckel, D D Paepe, S V Hautte, P Heyvaert, M Bentefrit, P Moens, A Dimou, B V D Bossche, F D Turck, S V Hoecke, F Ongenae, Future Gener. Comput. Syst. 1162021</p>
<p>Adaptive anomaly detection and root cause analysis by fusing semantics and machine learning. B Steenwinckel, Extended Semantic Web Conference. 2018</p>
<p>Recent advances in prognostics and health management for advanced manufacturing paradigms. T Xia, Y Dong, L Xiao, S Du, E Pan, L Xi, Reliability Engineering and System Safety. 1782018</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, ArXiv. 2005.14165. 2020</p>
<p>Large language models are few-shot health learners. X Liu, D J Mcduff, G Kovács, I R Galatzer-Levy, J Sunshine, J Zhan, M.-Z Poh, S Liao, P D Achille, S N Patel, abs/2305.15525ArXiv. 2023</p>
<p>Language models are few-shot learners for prognostic prediction. Z Chen, M M Balan, K Brown, abs/2302.12692ArXiv. 2023</p>
<p>Large models for time series and spatio-temporal data: A survey and outlook. M Jin, Q Wen, Y Liang, C Zhang, S Xue, X Wang, J Y Zhang, Y Wang, H Chen, X Li, S Pan, V S Tseng, Y Zheng, L Chen, H Xiong, ArXiv. 2023</p>
<p>Large language models are zero-shot time series forecasters. N Gruver, M Finzi, S Qiu, A G Wilson, abs/2310.07820ArXiv. 2023</p>
<p>Time-llm: Time series forecasting by reprogramming large language models. M Jin, S Wang, L Ma, Z Chu, J Y Zhang, X L Shi, P.-Y Chen, Y Liang, Y.-F Li, S Pan, Q Wen, abs/2310.01728ArXiv. 2023</p>
<p>E S Olivas, J D M Guerrero, M M Sober, J R M Benedito, A J S Lopez, Handbook Of Research On Machine Learning Applications and Trends: Algorithms, Methods and Techniques. </p>
<p>PA: Information Science Reference -Imprint of. Volumes, Hershey, 2009IGI Publishing</p>
<p>What is being transferred in transfer learning?. B Neyshabur, H Sedghi, C Zhang, ArXiv. 2008.11687, 2020</p>
<p>Repository: Personality research and assessment in the era of machine learning. C Stachl, F Pargent, S Hilbert, G M Harari, R Schoedel, S S Vaid, S D Gosling, M Bühner, 2019</p>
<p>The generalizability of machine learning models of personality across two text domains. M Berggren, L Kaati, B Pelzer, H Stiff, L Lundmark, N Akrami, Personality and Individual Differences. 2171124652024</p>
<p>Anomaly detection using a sliding window technique and data imputation with machine learning for hydrological time series. L Kulanuwat, C Chantrapornchai, M Maleewong, P Wongchaisuwat, S Wimala, K Sarinnapakorn, S Boonya-Aroonnet, Water. 13132021</p>
<p>The ASQ Certified Six Sigma Black Belt Handbook. M Mcshane-Vaughn, 2023ASQ Quality Press</p>
<p>Some perspectives on nonparametric statistical process control. P Qiu, Journal of Quality Technology. 502018</p>
<p>Introduction to statistical process control. P Qiu, 2013CRC press</p>
<p>Two robust multivariate exponentially weighted moving average charts to facilitate distinctive product quality features assessment. Z Song, A Mukherjee, P Qiu, M Zhou, Computers and Industrial Engineering. 1831094692023</p>
<p>Chronos: Learning the language of time series. A F Ansari, L Stella, C Turkmen, X Zhang, P Mercado, H Shen, O Shchur, S S Rangapuram, S P Arango, S Kapoor, J Zschiegner, D C Maddix, M W Mahoney, K Torkkola, A G Wilson, M Bohlke-Schneider, Y Wang, abs/2403.07815ArXiv. 2024</p>
<p>Promptcast: A new prompt-based learning paradigm for time series forecasting. H Xue, F D Salim, IEEE Transactions on Knowledge and Data Engineering. 2022</p>
<p>One fits all: Power general time series analysis by pretrained lm. T Zhou, P Niu, X Wang, L Sun, R Jin, Neural Information Processing Systems. 2023</p>
<p>Test: Text prototype aligned embedding to activate llm's ability for time series. C Sun, Y Li, H Li, Linda Qiao, abs/2308.08241ArXiv. 2023</p>
<p>A survey on anomaly detection in evolving data. M Salehi, L Rashidi, SIGKDD Explor. 202018with application to forest fire risk prediction</p>
<p>Adaptive anomaly detection in sensor data: A comprehensive approach. T Singh, S Nigam, E Vijay, R Rathore, S Bhosale, A Deogirikar, 2023 IEEE Technology &amp; Engineering Management Conference -Asia Pacific (TEMSCON-ASPAC). 2023</p>
<p>Adtcd: An adaptive anomaly detection approach toward concept drift in iot. L Xu, X Ding, H Peng, D Zhao, X Li, IEEE Internet of Things Journal. 102023</p>
<p>Enhancing recommender systems with large language model reasoning graphs. Y Wang, Z Chu, X Ouyang, S Wang, H Hao, Y Shen, J Gu, S Xue, J Y Zhang, Q Cui, L Li, J Zhou, S Li, 2024</p>
<p>Leveraging large language models for pre-trained recommender systems. Z Chu, H Hao, X Ouyang, S Wang, Y Wang, Y Shen, J Gu, Q Cui, L Li, S Xue, J Y Zhang, S Li, 2023</p>
<p>Instruction tuning for large language models: A survey. S Zhang, L Dong, X Li, S Zhang, X Sun, S Wang, J Li, R Hu, T Zhang, F Wu, G Wang, abs/2308.10792ArXiv. 2023</p>
<p>Skoltech anomaly benchmark (skab). I D Katser, V O Kozitsin, 2020</p>
<p>Multi-channel lstm-capsule autoencoder network for anomaly detection on multivariate data. A Elhalwagy, T Kalganova, Applied Sciences. 1211393Nov. 2022</p>
<p>Application of a model-based fault detection system to nuclear plant signals. K Gross, R Singer, S Wegerich, J Herzog, R Vanalstine, F Bockhorst, 1997</p>
<p>A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data. C Zhang, D Song, Y Chen, X Feng, C Lumezanu, W Cheng, J Ni, B Zong, H Chen, N V Chawla, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceJul. 201933</p>
<p>Multivariate industrial time series with cyber-attack simulation: Fault detection using an lstmbased predictive data model. P Filonov, A Lavrentyev, A Vorontsov, 2016</p>
<p>Timeseries anomaly detection using an autoencoder. P Vijay, 2020</p>
<p>Building autoencoders in keras. F Chollet, 2016</p>
<p>Generating sentences from a continuous space. S R Bowman, L Vilnis, O Vinyals, A M Dai, R Józefowicz, S Bengio, Conference on Computational Natural Language Learning. 2015</p>
<p>Outlier detection with autoencoder ensembles. J Chen, S K Sathe, C C Aggarwal, D S Turaga, SDM. 2017</p>
<p>Isolation forest. F T Liu, K M Ting, Z.-H Zhou, 2008 Eighth IEEE International Conference on Data Mining. 2008</p>            </div>
        </div>

    </div>
</body>
</html>
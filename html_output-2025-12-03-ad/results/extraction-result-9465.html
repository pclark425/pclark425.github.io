<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9465 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9465</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9465</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-15ec731c626f7e2c2025db5c7ed0555daf7e0c79</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/15ec731c626f7e2c2025db5c7ed0555daf7e0c79" target="_blank">PiCO: Peer Review in LLMs based on the Consistency Optimization</a></p>
                <p><strong>Paper TL;DR:</strong> This paper proposes three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings, and formalizes it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores.</p>
                <p><strong>Paper Abstract:</strong> Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9465",
    "paper_id": "paper-15ec731c626f7e2c2025db5c7ed0555daf7e0c79",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0064795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PiCO: Peer Review in LLMs based on Consistency Optimization</h1>
<p>Kun-Peng Ning ${ }^{1}$, Shuo Yang ${ }^{1}$, Yu-Yang Liu ${ }^{1, <em>}$, Jia-Yu Yao ${ }^{1}$, Zhen-Hui Liu ${ }^{1}$, Yong-Hong Tian ${ }^{1,2}$, Yibing Song, Li Yuan ${ }^{1,2, </em>}$<br>${ }^{1}$ School of Electrical and Computer Engineering, Peking University<br>${ }^{2}$ Peng Cheng Laboratory<br>{ningkp, shuo_yang, leon0425}@stu.pku.edu.cn,<br>{liuyuyang13, jiayu_yao,yhtian, yuanli-ece}@pku.edu.cn, yibingsong.cv@gmail.com</p>
<h4>Abstract</h4>
<p>Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically without any human feedback. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. During this process, we found that those answers that are more recognized by other "reviewers" (models) usually come from LLMs with stronger abilities, while these models can also evaluate others' answers more accurately. We formalize it as a consistency assumption, i.e., the ability and score of the model usually have consistency. We exploit this to optimize each model's confidence, thereby re-ranking the LLMs to be closer to human rankings. We perform experiments on multiple datasets with standard rank-based metrics, validating the effectiveness of the proposed approach. Our code is released at https://github.com/PKU-YuanGroup/PiCO.</p>
<h2>1 INTRODUCTION</h2>
<p>Goodhart's Law: "When a measure becomes a target, it ceases to be a good measure."</p>
<p>Large language models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Bubeck et al., 2023; Touvron et al., 2023; Ning et al., 2025) have achieved remarkable success across a variety of realworld applications (Zhao et al., 2023; Liu et al., 2023a; Ouyang et al., 2022; Yao et al., 2023; Ning et al., 2024; Yang et al., 2024). With the increasingly widespread application of these models, there is an urgent need for an effective evaluation method to ensure that their performance and usability meet the growing demands. To assess the ability level of LLMs, a large number of evaluation benchmarks have been proposed by using some small and domain-specific datasets with human-curated labels, such as MMLU (Hendrycks et al., 2020), HELM (Liang et al., 2022), Big-Bench (Srivastava et al., 2022), GLUE (Wang et al., 2018). However, these benchmarks can only measure LLMs' core capability on a confined set of tasks (e.g. multi-choice knowledge or retrieval questions), which fails to assess their alignment with human preference in open-ended tasks adequately (Chiang et al., 2023; Li et al., 2023a; Nakano et al., 2021). On the other hand, these evaluations may suffer from benchmark leakage issue, referring that the evaluation data is unknowingly used for model training, which can also lead to misleading evaluations (Wei et al., 2023; Zhou et al., 2023). Therefore, blindly improving scores on these public benchmarks cannot always yield a large language model that truly satisfies human requirements.</p>
<p>For assessing human preferences, recent studies have focused on building crowdsourced battle platforms with human ratings as the primary evaluation metric. Typical platforms include Chatbot</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The goal is to find a final score ranking that all LLMs "agree" it.</p>
<p>Arena (Zheng et al., 2023), MT-Bench (Zheng et al., 2023), and AlpacaEval (Li et al., 2023b). It constructs anonymous battles between chatbots in real-world scenarios, where users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. While human evaluation is the gold standard for measuring human preferences, it is exceptionally slow and costly (Zheng et al., 2023; Ning et al., 2022). In addition, adding a new LLM to the crowdsourced battle platforms also poses a cold-start issue (Chang et al., 2023). Thus, a fundamental question arises: can we construct an unsupervised LLMs evaluation system without relying on any human feedback?</p>
<p>Actually, in real human evaluation systems, people build the human-ability hierarchy based on different empirical assumptions. For example, majority voting (Feldman, 2006; Boyer \&amp; Moore, 1991; Surowiecki, 2005) and rating voting (Allahbakhsh \&amp; Ignjatovic, 2012) methods are widely used during the decision-making process, which are based on the wisdom of the crowds (Surowiecki, 2005; Budescu \&amp; Chen, 2015; Weller, 2007) and have been proven to lead to better results than that of an individual. Moreover, in the established practice of peer-review in academic research, scholars evaluate their academic level rankings based on the consistency assumption, i.e., scholars with stronger abilities usually have stronger persuasiveness for evaluating others, and these scholars can also obtain higher achievements. This paper attempts to explore whether a similar phenomenon exists in the LLMs evaluation systems.</p>
<p>In this paper, we propose PiCO, a Peer review approach in LLMs based on Consistency Optimization. In this setting, LLMs themselves act as "reviewers", engaging in mutual assessments to achieve comprehensive, efficient, and performance evaluations without relying on manually annotated data. This method aims to address the limitations of existing evaluation approaches and provide insights into LLMs' real-world capabilities. As shown in Figure 1, both open-source and closed-source LLMs lie in the same environment and answer the open-ended questions from an unlabeled dataset. Then, we construct anonymous answer pairs, while randomly selecting other LLMs as "reviewers" to evaluate both responses with a learnable confidence weight $w$. Finally, we employ this weight and calculate the response scores $G$ for each LLM based on the weighted joint evaluation. It is worth noting that the whole peer-review process works in an unsupervised way, and our goal is to optimize the confidence weights $w$ that re-rank the LLMs to be closer to human rankings.</p>
<p>To achieve this, we formalize it as a constrained optimization based on the consistency assumption. We maximize the consistency of each LLM's capability $w$ and score $G$ while adjusting the final ranking to align with human preference more closely. The key assumption behind this is that high-level LLM can evaluate others' answers more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher answer-ranking scores. As a result, the entropy</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The pipeline of the PiCO. It is mainly composed of two components: the peer-review and consistency optimization stages. Specifically, in the peer-review stage, the unlabeled dataset $\mathcal{Q}$ and the LLMs pool $\mathcal{M}$ are given. Then, we let all LLMs answer each unlabeled question to obtain the response set $\mathcal{A}$. We shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs to evaluate both responses with a learnable confidence $w$. As a result, we can obtain the answer-ranking data $\mathcal{D}$ which is a quadruple that records the partial order between two answers and the evaluator's confidence weight. In the consistency optimization stage, we update the parameter $w$ by maximizing the consistency of each LLM's capability and score, while re-ranking the LLMs to be closer to human rankings.
(controversy) of the whole peer-review evaluation system can be minimized. In other words, the consistency optimization aims to find a final score ranking that all LLMs have no "disputes" regarding.
We perform experiments on multiple crowdsourcing datasets with standard rank-based metrics, the results demonstrate that the proposed PiCO framework can effectively obtain a large language models' leaderboard closer to human preferences. The contributions of this paper can be summarized as follows:</p>
<ul>
<li>We explore a novel unsupervised LLM evaluation direction without human feedback, i.e., utilizing peer-review mechanisms to measure LLMs automatically. All LLMs can answer unlabeled questions and evaluate each other.</li>
<li>A constrained optimization based on the consistency assumption is proposed to re-rank the LLMs to be closer to human rankings.</li>
<li>We conduct extensive experiments on three crowdsourcing datasets with three standard rank-based metrics validating the effectiveness of the proposed PiCO approach.</li>
</ul>
<h1>2 THE PROPOSED APPROACH</h1>
<h3>2.1 Problem Definition</h3>
<p>This paper aims to re-rank the ability of LLMs to be closer to human (ground-truth) rankings $\mathcal{R}^{<em>}$ in an unsupervised way (without relying on any human annotations). Specifically, we have a large language models (LLMs) pool $\mathcal{M}=\left{M_{j}\right}<em 1="1">{j=1}^{m}$, which includes both open-source and closed-source models. Write $M</em>^{} \succ M_{2}$ to indicate that the LLM $M_{1}$ has stronger capabilities than the LLM $M_{2}$. Thus, we can assume that the ground-truth ranking $\mathcal{R</em>}$ is as follows,</p>
<p>$$
\mathcal{R}^{*}:=\left[M_{1} \succ M_{2} \succ M_{3} \succ \ldots \succ M_{m}\right]
$$</p>
<p>Assuming that the learned ranking $\hat{\mathcal{R}}$ by different evaluation methods is as follows,</p>
<p>$$
\hat{\mathcal{R}}:=\left[M_{3} \succ M_{1} \succ M_{2} \succ \ldots \succ M_{m}\right]
$$</p>
<p>The goal is to learn an LLM ranking $\hat{\mathcal{R}}$ that aligns with human ranking $\mathcal{R}^{*}$ as much as possible.</p>
<h3>2.2 Algorithm Details</h3>
<p>The pipeline of the proposed PiCO, depicted in Figure 2, involves peer-review and consistency optimization stages. Next, we will introduce the two stages in detail.</p>
<p>Peer Review Stage. In our peer-review system, we consider an unsupervised LLM evaluation scenario with an unlabeled dataset $\mathcal{Q}$ consisting of $n$ open-ended questions, where $\mathcal{Q}=\left{Q_{i}\right}_{i=1}^{n}$.</p>
<p>Table 1: Validation of consistency assumption. Performance comparison of Backward, Uniform, Forward weight voting, and Consistency Optimization methods with two metrics across three datasets.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>MT-Bench</th>
<th></th>
<th>Chatbot Arena</th>
<th></th>
<th>AlpacaEval</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$S(\uparrow)$</td>
<td>$\tau(\uparrow)$</td>
<td>$S(\uparrow)$</td>
<td>$\tau(\uparrow)$</td>
<td>$S(\uparrow)$</td>
<td>$\tau(\uparrow)$</td>
</tr>
<tr>
<td>Backward Weight</td>
<td>0.70</td>
<td>0.50</td>
<td>0.72</td>
<td>0.52</td>
<td>0.69</td>
<td>0.50</td>
</tr>
<tr>
<td>Uniform Weight</td>
<td>0.74</td>
<td>0.54</td>
<td>0.80</td>
<td>0.58</td>
<td>0.77</td>
<td>0.58</td>
</tr>
<tr>
<td>Forward Weight</td>
<td>0.75</td>
<td>0.56</td>
<td>0.82</td>
<td>0.59</td>
<td>0.79</td>
<td>0.60</td>
</tr>
<tr>
<td>Random Weight + Consistency Optimization</td>
<td>$\mathbf{0 . 9 0}$</td>
<td>$\mathbf{0 . 7 7}$</td>
<td>$\mathbf{0 . 8 9}$</td>
<td>$\mathbf{0 . 7 2}$</td>
<td>$\mathbf{0 . 8 4}$</td>
<td>$\mathbf{0 . 6 8}$</td>
</tr>
</tbody>
</table>
<p>All LLMs will answer each unlabeled question to obtain the set $\mathcal{A}=\left{\left{A_{i}^{j}\right}<em j="1">{i=1}^{n}\right}</em>$ is as follows,}^{m}$, where $A_{i}^{j</p>
<p>$$
A_{i}^{j}=M_{j}\left(Q_{i}\right)
$$</p>
<p>which infers the model $M_{j}$ response an answer $A_{i}^{j}$ with question $Q_{i}$. In addition, LLMs themselves also act as "reviewers" to evaluate other answers. Specifically, for the same question $Q_{i} \in \mathcal{Q}$, we randomly construct a battle pair $<A_{i}^{j}, A_{i}^{k}>$ for review. Each battle pair will randomly assign "reviewers" to determine the winners or declare ties,</p>
<p>$$
\left(A_{i}^{k}, A_{i}^{s},&gt;, w^{j}\right)=M_{j}\left(A_{i}^{k} ; A_{i}^{s} \mid Q_{i}\right)
$$</p>
<p>Under the same question $Q_{i}$, the quadruples $\left(A_{i}^{k}, A_{i}^{s},&gt;, w^{j}\right)$ indicate that the "reviewer" $M_{j}$ believes that the answer $A_{i}^{k}$ is better than answer $A_{i}^{k}$ with a confidence $w^{j}$. Thus, we can collect the answerranking data $\mathcal{D}$ as follows,</p>
<p>$$
\mathcal{D}=\left{\left(A_{i}^{k}, A_{i}^{s},&gt;, w^{j}\right)\right}<em j="j">{i \sim \mathcal{Q}, j, k, M</em>
$$} \sim \mathcal{M}</p>
<p>where $i$ denotes the question index, and $j, k, s$ indicate the model indices. $w^{s} \in(0,1]$ is a learnable confidence weight of model $M_{s}$, and $&gt;$ is a partial order relationship from ${&gt;,&lt;,=}$. After that, we can calculate the response score $G_{j}$ of each LLM,</p>
<p>$$
G_{j}=\sum_{\left(A_{i}^{k}, A_{i}^{s},&gt;, w^{j}\right) \sim \mathcal{D}} \mathbf{1}\left{A_{i}^{j}&gt;A_{i}^{k}\right} \cdot w^{s}
$$</p>
<p>where $\mathbf{1}{\cdot}$ is the indicator function that the value is 1 when the condition is met, otherwise, it is 0 . We can define the LLM $M_{1}$ is better than $M_{2}$ as its score is larger, i.e., $M_{1} \succ M_{2}:=G_{1}&gt;G_{2}$. Thus, we can re-write the learned LLM ranking $\hat{\mathcal{R}}$ as follows,</p>
<p>$$
\hat{\mathcal{R}}:=\left[G_{3}&gt;G_{1}&gt;G_{2}&gt;\ldots&gt;G_{m}\right]
$$</p>
<p>Thus, the goal is to learn the confidence weights $w$ to adjust the final ranking $\hat{\mathcal{R}}$ to be closer to ground-truth ranking $\mathcal{R}^{*}$.</p>
<p>Validation of Consistency Assumption. First of all, we start with a toy experiment to study the role of confidence $w$ in Table 1. Specifically, we manually construct three methods: Backward Weight, Uniform Weight, and Forward Weight. That is, the ability weights of the model are respectively weighted forward $(w=[1,0.9, \ldots, 0])$, uniformly $(w=[1,1, \ldots, 1])$, and backward $(w=[0,0.1, \ldots, 1])$ according to the ground-truth human ranking. In other words, the Forward Weight means manually assigning higher weights to those models with stronger abilities, and so on for others. Then, we can calculate the response score $G_{j}$ for each model using Eq.6, and obtain the LLM ranking $\hat{\mathcal{R}}$. We measure the alignment between $\hat{\mathcal{R}}$ and $\mathcal{R}^{*}$ with Spearman's $S(\uparrow)$ and Kendall's $\tau(\uparrow)$ rank correlation coefficient in Table 1. Note that this is an ideal experiment, as we only use the ground-truth human ranking to validate the feasibility of our idea.</p>
<p>As shown in Table 1, it can be observed that the Forward Weight achieves better results than the Uniform and Backward ones in all cases, while the Backward one always achieves worse results. It validates that assigning larger weights to those models with stronger capabilities can obtain better results. In other words, those answers that are more recognized by other "reviewers" (models) usually come from LLs with stronger abilities. We formalize it as a consistency assumption, i.e., high-level LLM can evaluate others' answers more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher answer-ranking scores, the ability and score of the model usually have consistency.</p>
<p>Consistency Optimization Stage. Based on this observation, we propose to maximize the consistency of each LLM's capability $w$ and score $G$ with constrained optimization as follows,</p>
<p>$$
\underset{w}{\operatorname{argmax}} \text { Consistency }(G, w)
$$</p>
<p>$$
\text { s.t. } G_{j}=\sum_{\left(A_{i}^{j}, A_{i}^{k},&gt;, w^{s}\right) \sim \mathcal{D}} \mathbf{1}\left{A_{i}^{j}&gt;A_{i}^{k}\right} \cdot w^{s}
$$</p>
<p>where the Pearson correlation (Sedgwick, 2012) is used to measure the consistency between $w$ and $G$. Note that we only introduce this straightforward implementation to validate our idea of PiCO. Other more advanced strategies may be employed to further improve the performance.</p>
<p>Discussion: It is worth noting that the whole process (Eq. 5 and 8) works in an unsupervised way. The only thing we do is to adaptively adjust the score of each LLM that match its abilities. Most importantly, we also validate the effectiveness of the proposed consistency optimization in Table 1. Specifically, we randomly initialize the ability weights and employ our consistency optimization to adjust the weight. It can be observed that the learned $w$ by our consistency optimization algorithm (Eq.8) can further improve the performance of the evaluation system, making the LLM ranking $\mathcal{R}$ closer to human ranking $\mathcal{R}^{s}$. Another intuitive example is as follows: in a real peer-review system, if the academic level of three scholars $a, b$, and $c$ satisfies the following relationship, $w^{a}&gt;w^{b}&gt;w^{c}$. So, in the ultimate ideal scenario, the ranking of the scores submitted by these three scholars should also be, $G_{a}&gt;G_{b}&gt;G_{c}$. In other words, the sorting of $G$ and $w$ satisfies high consistency. On the other hand, scholars with stronger abilities (i.e., scholar $a$ ) evaluate $A^{b}&gt;A^{c}$ have stronger persuasiveness, so scholar $b$ should also receive higher weighted scores $1 * w^{a}$.
Reviewer Elimination Mechanism. Realizing that not all LLMs have sufficient ability to evaluate the responses of other models. We thus introduce an unsupervised elimination mechanism to remove those LLMs that have low scores. It iteratively removes the lowest-scoring LLM from the "reviewer queue" for the next consistency optimization stage, until $60 \%$ of models are eliminated. The discussion of the elimination mechanism can also be found in the Experiment 3.3.</p>
<h1>3 EXPERIMENTS</h1>
<p>Datasets. To validate the effectiveness of the proposed approach, we perform experiments on Chatbot Arena (Zheng et al., 2023), MT-Bench (Zheng et al., 2023), and AlpacaEval (Li et al., 2023b). The MT-Bench dataset assesses six LLMs' responses to 80 multi-category questions. The Chatbot Arena Conversations Dataset, with 33 K conversations from 13 K IPs during April-June 2023, evaluates real dialogue performance. AlpacaEval dataset integrates 805 evaluations from diverse tests (e.g., Self-Instruct (Wang et al., 2022), OASST, Anthropic's helpful (Bai et al., 2022), Vicuna (Chiang et al., 2023) and Koala (Geng et al., 2023a) test sets) to align evaluations real-world interactions (Dubois et al., 2023). These datasets are collected by crowdsourcing platforms from human feedback, so they have a ground-truth ranking LLMs $\mathcal{R}^{s}$ to measure the alignment performance of different evaluation methods.</p>
<p>LLMs Pool. In our experiments, we employ 15 LLMs with diverse architectures to construct the LLMs pool, including GPT-3.5-Turbo (OpenAI, 2022), WizardLM-13B (Xu et al., 2023), Guanaco33B (gua, 2023), Vicuna-7B (Chiang et al., 2023), Vicuna-13B (Chiang et al., 2023), Koala-13B (Geng et al., 2023b), Mpt-7B (Team, 2023), gpt4all-13B (Anand et al., 2023), ChatGLM-6B (Zeng et al., 2022), Oasst-sft-4-pythia-12B (Contributors, 2023), FastChat-T5-3B (Zheng et al., 2023), StableLM-7B (AI, 2023), Dolly-12B (Conover et al., 2023), LLaMA-13B (Touvron et al., 2023), Alpaca-13B (Taori et al., 2023). All models use the same prompt template, which can be found in Appendix C.</p>
<p>Baselines. To validate the effectiveness of the proposed PiCO approach, we compare the following methods in the experiments.</p>
<ul>
<li>The wisdom of the crowds: The two methods that perform LLMs evaluation based on the wisdom of the crowds (Surowiecki, 2005; Budescu \&amp; Chen, 2015; Weller, 2007) are compared in this experiment. 1) Majority Voting (Surowiecki, 2005): Multiple review models vote for the better answer for the same response pair, and the model with the most votes gets 1 score; 2) Rating Voting (Allahbakhsh \&amp; Ignjatovic, 2012): Multiple review models also vote on the same response pair, and the number of votes obtained is the score.</li>
</ul>
<p>Table 2: Comparison of all methods on three datasets under data volumes of 1, 0.7, and 0.4, where the top value is highlighted by bold font. Higher $S$ and $\tau$ scores indicate better performance, while a lower $H$ score signifies improved performance.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Chatbot Arena</th>
<th></th>
<th></th>
<th>MT-Bench</th>
<th></th>
<th></th>
<th>AlpacaEval</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>1</td>
<td>0.7</td>
<td>0.4</td>
<td>1</td>
<td>0.7</td>
<td>0.4</td>
<td>1</td>
<td>0.7</td>
<td>0.4</td>
</tr>
<tr>
<td>Spearman's Rank Correlation Coefficient $S(\uparrow)$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Majority Voting</td>
<td>$0.76^{ \pm 0.00}$</td>
<td>$0.75^{ \pm 0.01}$</td>
<td>$0.73^{ \pm 0.03}$</td>
<td>$0.73^{ \pm 0.00}$</td>
<td>$0.77^{ \pm 0.01}$</td>
<td>$0.75^{ \pm 0.01}$</td>
<td>$0.80^{ \pm 0.00}$</td>
<td>$0.79^{ \pm 0.01}$</td>
<td>$0.78^{ \pm 0.01}$</td>
</tr>
<tr>
<td>Rating Voting</td>
<td>$0.74^{ \pm 0.00}$</td>
<td>$0.72^{ \pm 0.02}$</td>
<td>$0.71^{ \pm 0.02}$</td>
<td>$0.80^{ \pm 0.00}$</td>
<td>$0.78^{ \pm 0.02}$</td>
<td>$0.74^{ \pm 0.03}$</td>
<td>$0.77^{ \pm 0.00}$</td>
<td>$0.77^{ \pm 0.01}$</td>
<td>$0.78^{ \pm 0.01}$</td>
</tr>
<tr>
<td>GPTScore(flan-t5-xxl)</td>
<td>$-0.09^{ \pm 0.00}$</td>
<td>$-0.09^{ \pm 0.01}$</td>
<td>$-0.12^{ \pm 0.02}$</td>
<td>$0.05^{ \pm 0.00}$</td>
<td>$0.01^{ \pm 0.07}$</td>
<td>$0.04^{ \pm 0.09}$</td>
<td>$0.34^{ \pm 0.00}$</td>
<td>$0.34^{ \pm 0.00}$</td>
<td>$0.34^{ \pm 0.01}$</td>
</tr>
<tr>
<td>GPTScore(davinci-002)</td>
<td>$0.15^{ \pm 0.00}$</td>
<td>$0.13^{ \pm 0.02}$</td>
<td>$-0.02^{ \pm 0.14}$</td>
<td>$0.52^{ \pm 0.00}$</td>
<td>$0.42^{ \pm 0.05}$</td>
<td>$0.45^{ \pm 0.05}$</td>
<td>$0.76^{ \pm 0.00}$</td>
<td>$0.77^{ \pm 0.07}$</td>
<td>$0.75^{ \pm 0.06}$</td>
</tr>
<tr>
<td>PandaLM</td>
<td>$0.43^{ \pm 0.00}$</td>
<td>$0.44^{ \pm 0.03}$</td>
<td>$0.44^{ \pm 0.10}$</td>
<td>$0.50^{ \pm 0.00}$</td>
<td>$0.50^{ \pm 0.08}$</td>
<td>$0.52^{ \pm 0.17}$</td>
<td>$0.57^{ \pm 0.00}$</td>
<td>$0.55^{ \pm 0.01}$</td>
<td>$0.48^{ \pm 0.08}$</td>
</tr>
<tr>
<td>PRD</td>
<td>$0.84^{ \pm 0.00}$</td>
<td>$0.84^{ \pm 0.00}$</td>
<td>$0.82^{ \pm 0.03}$</td>
<td>$0.86^{ \pm 0.00}$</td>
<td>$0.84^{ \pm 0.03}$</td>
<td>$0.81^{ \pm 0.03}$</td>
<td>$0.81^{ \pm 0.00}$</td>
<td>$0.81^{ \pm 0.01}$</td>
<td>$0.81^{ \pm 0.02}$</td>
</tr>
<tr>
<td>PRE</td>
<td>$0.86^{ \pm 0.00}$</td>
<td>$0.86^{ \pm 0.01}$</td>
<td>$0.86^{ \pm 0.01}$</td>
<td>$0.86^{ \pm 0.00}$</td>
<td>$0.84^{ \pm 0.03}$</td>
<td>$0.82^{ \pm 0.04}$</td>
<td>$0.83^{ \pm 0.00}$</td>
<td>$0.81^{ \pm 0.01}$</td>
<td>$0.83^{ \pm 0.02}$</td>
</tr>
<tr>
<td>Claude-3 (API)</td>
<td>$0.90^{ \pm 0.01}$</td>
<td>$0.88^{ \pm 0.03}$</td>
<td>$0.87^{ \pm 0.04}$</td>
<td>$0.85^{ \pm 0.06}$</td>
<td>$0.82^{ \pm 0.06}$</td>
<td>$0.80^{ \pm 0.07}$</td>
<td>$0.79^{ \pm 0.03}$</td>
<td>$0.78^{ \pm 0.02}$</td>
<td>$0.75^{ \pm 0.04}$</td>
</tr>
<tr>
<td>PiCO (Ours)</td>
<td>$\mathbf{0 . 9 0}{ }^{ \pm 0.00}$</td>
<td>$\mathbf{0 . 8 9}{ }^{ \pm 0.01}$</td>
<td>$\mathbf{0 . 8 9}{ }^{ \pm 0.01}$</td>
<td>$\mathbf{0 . 8 9}{ }^{ \pm 0.01}$</td>
<td>$\mathbf{0 . 8 9}{ }^{ \pm 0.01}$</td>
<td>$\mathbf{0 . 8 4}{ }^{ \pm 0.11}$</td>
<td>$\mathbf{0 . 8 4}{ }^{ \pm 0.00}$</td>
<td>$\mathbf{0 . 8 3}{ }^{ \pm 0.03}$</td>
<td>$\mathbf{0 . 8 5}{ }^{ \pm 0.01}$</td>
</tr>
<tr>
<td>Kendall's Rank Correlation Coefficient $\tau(\uparrow)$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Majority Voting</td>
<td>$0.58^{ \pm 0.00}$</td>
<td>$0.56^{ \pm 0.02}$</td>
<td>$0.52^{ \pm 0.05}$</td>
<td>$0.56^{ \pm 0.00}$</td>
<td>$0.61^{ \pm 0.02}$</td>
<td>$0.60^{ \pm 0.02}$</td>
<td>$0.62^{ \pm 0.00}$</td>
<td>$0.60^{ \pm 0.02}$</td>
<td>$0.58^{ \pm 0.02}$</td>
</tr>
<tr>
<td>Rating Voting</td>
<td>$0.54^{ \pm 0.00}$</td>
<td>$0.53^{ \pm 0.02}$</td>
<td>$0.52^{ \pm 0.02}$</td>
<td>$0.58^{ \pm 0.00}$</td>
<td>$0.57^{ \pm 0.02}$</td>
<td>$0.54^{ \pm 0.01}$</td>
<td>$0.58^{ \pm 0.00}$</td>
<td>$0.57^{ \pm 0.01}$</td>
<td>$0.57^{ \pm 0.02}$</td>
</tr>
<tr>
<td>GPTScore(flan-t5-xxl)</td>
<td>$-0.06^{ \pm 0.00}$</td>
<td>$-0.06^{ \pm 0.02}$</td>
<td>$-0.09^{ \pm 0.02}$</td>
<td>$-0.05^{ \pm 0.00}$</td>
<td>$-0.07^{ \pm 0.05}$</td>
<td>$-0.02^{ \pm 0.06}$</td>
<td>$0.25^{ \pm 0.00}$</td>
<td>$0.26^{ \pm 0.01}$</td>
<td>$0.26^{ \pm 0.01}$</td>
</tr>
<tr>
<td>GPTScore(davinci-002)</td>
<td>$0.20^{ \pm 0.00}$</td>
<td>$0.23^{ \pm 0.02}$</td>
<td>$0.03^{ \pm 0.11}$</td>
<td>$0.36^{ \pm 0.00}$</td>
<td>$0.39^{ \pm 0.05}$</td>
<td>$0.31^{ \pm 0.05}$</td>
<td>$0.60^{ \pm 0.08}$</td>
<td>$0.61^{ \pm 0.07}$</td>
<td>$0.59^{ \pm 0.08}$</td>
</tr>
<tr>
<td>PandaLM</td>
<td>$0.30^{ \pm 0.00}$</td>
<td>$0.31^{ \pm 0.03}$</td>
<td>$0.31^{ \pm 0.07}$</td>
<td>$0.39^{ \pm 0.00}$</td>
<td>$0.37^{ \pm 0.06}$</td>
<td>$0.40^{ \pm 0.12}$</td>
<td>$0.41^{ \pm 0.00}$</td>
<td>$0.39^{ \pm 0.02}$</td>
<td>$0.32^{ \pm 0.05}$</td>
</tr>
<tr>
<td>PRD</td>
<td>$0.68^{ \pm 0.00}$</td>
<td>$0.69^{ \pm 0.01}$</td>
<td>$0.67^{ \pm 0.03}$</td>
<td>$0.68^{ \pm 0.06}$</td>
<td>$0.66^{ \pm 0.02}$</td>
<td>$0.63^{ \pm 0.03}$</td>
<td>$0.64^{ \pm 0.00}$</td>
<td>$0.63^{ \pm 0.03}$</td>
<td>$0.63^{ \pm 0.02}$</td>
</tr>
<tr>
<td>PRE</td>
<td>$0.71^{ \pm 0.00}$</td>
<td>$0.73^{ \pm 0.02}$</td>
<td>$0.72^{ \pm 0.02}$</td>
<td>$0.68^{ \pm 0.00}$</td>
<td>$0.68^{ \pm 0.02}$</td>
<td>$0.65^{ \pm 0.03}$</td>
<td>$0.64^{ \pm 0.00}$</td>
<td>$0.66^{ \pm 0.01}$</td>
<td>$0.66^{ \pm 0.03}$</td>
</tr>
<tr>
<td>Claude-3 (API)</td>
<td>$0.76^{ \pm 0.04}$</td>
<td>$0.72^{ \pm 0.05}$</td>
<td>$0.70^{ \pm 0.07}$</td>
<td>$0.67^{ \pm 0.07}$</td>
<td>$0.66^{ \pm 0.11}$</td>
<td>$0.61^{ \pm 0.10}$</td>
<td>$0.64^{ \pm 0.06}$</td>
<td>$0.61^{ \pm 0.04}$</td>
<td>$0.66^{ \pm 0.06}$</td>
</tr>
<tr>
<td>PiCO (Ours)</td>
<td>$\mathbf{0 . 7 7}{ }^{ \pm 0.00}$</td>
<td>$\mathbf{0 . 7 6}{ }^{ \pm 0.01}$</td>
<td>$\mathbf{0 . 7 7}{ }^{ \pm 0.02}$</td>
<td>$\mathbf{0 . 7 2}{ }^{ \pm 0.01}$</td>
<td>$\mathbf{0 . 7 2}{ }^{ \pm 0.03}$</td>
<td>$\mathbf{0 . 7 0}{ }^{ \pm 0.12}$</td>
<td>$\mathbf{0 . 6 8}{ }^{ \pm 0.00}$</td>
<td>$\mathbf{0 . 6 6}{ }^{ \pm 0.04}$</td>
<td>$\mathbf{0 . 6 7}{ }^{ \pm 0.02}$</td>
</tr>
<tr>
<td>Permutation Entropy $H(\downarrow)$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Majority Voting</td>
<td>$1.27^{ \pm 0.05}$</td>
<td>$1.30^{ \pm 0.03}$</td>
<td>$1.36^{ \pm 0.06}$</td>
<td>$1.37^{ \pm 0.03}$</td>
<td>$1.30^{ \pm 0.06}$</td>
<td>$1.27^{ \pm 0.04}$</td>
<td>$1.26^{ \pm 0.02}$</td>
<td>$1.28^{ \pm 0.03}$</td>
<td>$1.29^{ \pm 0.03}$</td>
</tr>
<tr>
<td>Rating Voting</td>
<td>$1.39^{ \pm 0.02}$</td>
<td>$1.43^{ \pm 0.03}$</td>
<td>$1.42^{ \pm 0.07}$</td>
<td>$1.32^{ \pm 0.03}$</td>
<td>$1.35^{ \pm 0.04}$</td>
<td>$1.38^{ \pm 0.04}$</td>
<td>$1.34^{ \pm 0.03}$</td>
<td>$1.37^{ \pm 0.03}$</td>
<td>$1.34^{ \pm 0.08}$</td>
</tr>
<tr>
<td>GPTScore(flan-t5-xxl)</td>
<td>$1.68^{ \pm 0.01}$</td>
<td>$1.68^{ \pm 0.02}$</td>
<td>$1.65^{ \pm 0.02}$</td>
<td>$1.72^{ \pm 0.02}$</td>
<td>$1.70^{ \pm 0.02}$</td>
<td>$1.68^{ \pm 0.03}$</td>
<td>$1.55^{ \pm 0.02}$</td>
<td>$1.57^{ \pm 0.03}$</td>
<td>$1.60^{ \pm 0.01}$</td>
</tr>
<tr>
<td>GPTScore(davinci-002)</td>
<td>$1.54^{ \pm 0.02}$</td>
<td>$1.64^{ \pm 0.02}$</td>
<td>$1.68^{ \pm 0.05}$</td>
<td>$1.51^{ \pm 0.02}$</td>
<td>$1.61^{ \pm 0.01}$</td>
<td>$1.61^{ \pm 0.04}$</td>
<td>$1.25^{ \pm 0.02}$</td>
<td>$1.23^{ \pm 0.08}$</td>
<td>$1.26^{ \pm 0.14}$</td>
</tr>
<tr>
<td>PandaLM</td>
<td>$1.65^{ \pm 0.01}$</td>
<td>$1.64^{ \pm 0.02}$</td>
<td>$1.63^{ \pm 0.05}$</td>
<td>$1.55^{ \pm 0.03}$</td>
<td>$1.59^{ \pm 0.05}$</td>
<td>$1.52^{ \pm 0.08}$</td>
<td>$1.56^{ \pm 0.01}$</td>
<td>$1.58^{ \pm 0.01}$</td>
<td>$1.64^{ \pm 0.05}$</td>
</tr>
<tr>
<td>PRD</td>
<td>$1.15^{ \pm 0.04}$</td>
<td>$1.12^{ \pm 0.05}$</td>
<td>$1.13^{ \pm 0.06}$</td>
<td>$1.15^{ \pm 0.05}$</td>
<td>$1.17^{ \pm 0.06}$</td>
<td>$1.23^{ \pm 0.04}$</td>
<td>$1.21^{ \pm 0.04}$</td>
<td>$1.22^{ \pm 0.06}$</td>
<td>$1.23^{ \pm 0.07}$</td>
</tr>
<tr>
<td>PRE</td>
<td>$1.07^{ \pm 0.01}$</td>
<td>$1.03^{ \pm 0.03}$</td>
<td>$1.06^{ \pm 0.04}$</td>
<td>$1.17^{ \pm 0.04}$</td>
<td>$1.13^{ \pm 0.05}$</td>
<td>$1.19^{ \pm 0.05}$</td>
<td>$1.18^{ \pm 0.03}$</td>
<td>$1.21^{ \pm 0.04}$</td>
<td>$1.15^{ \pm 0.05}$</td>
</tr>
<tr>
<td>PiCO (Ours)</td>
<td>$\mathbf{0 . 9 4}{ }^{ \pm 0.02}$</td>
<td>$\mathbf{0 . 9 6}{ }^{ \pm 0.04}$</td>
<td>$\mathbf{0 . 9 5}{ }^{ \pm 0.08}$</td>
<td>$\mathbf{1 . 0 1}{ }^{ \pm 0.07}$</td>
<td>$\mathbf{1 . 0 2}{ }^{ \pm 0.11}$</td>
<td>$\mathbf{1 . 0 6}{ }^{ \pm 0.24}$</td>
<td>$\mathbf{1 . 1 7}{ }^{ \pm 0.02}$</td>
<td>$\mathbf{1 . 1 7}{ }^{ \pm 0.08}$</td>
<td>$\mathbf{1 . 1 3}{ }^{ \pm 0.05}$</td>
</tr>
</tbody>
</table>
<ul>
<li>State-of-the-art methods: The four recent SOTA methods of using either single or multiple models for self-evaluation are compared in this experiment. PandaLM <em>(Wang et al., 2023)</em>: It is a fine-tuned language model based on Llama-7b designed for the preference judgment tasks to evaluate and optimize LLMs. GPTScore <em>(Fu et al., 2023)</em>: It employs generative pre-trained models to assess the quality of generated text. It calculates the likelihood that the text was generated in response to specific instructions and context, indicative of high quality. In our implementation, GPT-3 (davinci-002) and flan-t5-xxl serve as the base models. PRD <em>(Li et al., 2023a)</em>: It transforms the LLMs win rates into weights for competitive ranking, while evaluating each LLM based on its preference for all possible pairs of answers, enabling a tournament-style ranking system. PRE <em>(Chu et al., 2024)</em>: It employs a supervised process to evaluate LLMs using a qualification exam, aggregates their scores based on accuracy, and assigns weights accordingly. Claude-3 (API): Another SOTA closed-source LLM developed by Anthropic. PiCO (Ours): the proposed approach in this paper.</li>
</ul>
<p>Metrics. For all experiments, we employ three popular rank-based metrics to evaluate the aforementioned experimental setups and our PiCO method: Spearman’s Rank Correlation Coefficient $S(\uparrow)$ <em>(Lehman et al., 2013)</em>, Kendall’s Rank Correlation Coefficient $\tau(\uparrow)$ <em>(Kendall, 1938)</em> and Permutation Entropy $H(\downarrow)$ <em>(Bandt and Pompe, 2002)</em>. The details of these metrics can be found in the Appendix A. Moreover, we perform the experiments for 4 runs and record the average results over 4 seeds $(s e e d=1,2,3,4)$.</p>
<h1>3.1 PERFORMANCE COMPARISON</h1>
<p>We validate the effectiveness of the proposed PiCO method on three datasets by comparing the following two types of methods, i.e., the wisdom of the crowds and recent SOTA LLMs evaluation methods. The average results with different rank-based metrics and datasets are demonstrated in Table 2. The ratios of response sets $\mathcal{D}$ are $1,0.7$, and 0.4 , respectively.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher values (above 0 ) indicate greater evaluation bias. The first row shows original PG values in three datasets, while the second row displays PG values re-weighted using our learned confidence weights.</p>
<p>The results presented in Table 2 demonstrate that the proposed PiCO method consistently outperforms competing approaches across most evaluated metrics, including surpassing all baselines, such as Claude-3 (API). Specifically, PiCO achieves improvements of 0.027, 0.047, and 0.14 on Spearman's Rank Correlation Coefficient, Kendall's Rank Correlation Coefficient, and Permutation Entropy metrics, respectively, compared to the runner-up. These results underscore the superiority of aggregating evaluations from multiple models, such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model methods like GPTScore and PandaLM. This collective model approach, leveraging 'the wisdom of the crowds', aligns with human rankings more accurately in our open-question evaluation framework.</p>
<p>In comparison with existing SOTA evaluation methods(i.e., PRD and PRE), it is evident that PiCO exhibits improvements across various evaluation metrics. Despite PRD's adjustment of model weights based on their win rates and PRE's reliance on supervised human feedback data to assign weights through a qualification exam, neither method achieves performance superior to the fully unsupervised PiCO approach. These methods rely on predefined criteria and human feedback, potentially leading to biases or suboptimal performance. In contrast, PiCO leverages unsupervised learning techniques, allowing it to autonomously adapt and discover patterns in the data without explicit human intervention.</p>
<p>It is important to highlight that PandaLM, a language model equipped with 7 billion parameters, was fine-tuned using labels generated by GPT-3.5-turbo as the ground truth, achieving stable performance across various datasets. However, in our unsupervised, open-ended experimental setup, which focuses on ranking-based metrics, GPTScore exhibits less robustness regardless of whether the base model is GPT-3 (davinci-002) or flan-t5-xx.</p>
<h1>3.2 EXPLORING THE ROLE OF CONFIDENCE WEIGHT</h1>
<p>In this subsection, we show that the confidence weight $w$ learned by our consistency optimization can reduce the system evaluation bias. Specifically, we first study whether the "review" model would prefer a particular model's response. Following (Chu et al., 2024), we employ the preference gap (PG) to evaluate the bias as follows,</p>
<p>$$
P G(i, j)=P_{i}(i&gt;j)-P_{j}(i&gt;j)
$$</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance comparison of the PiCO (Ours) and PRE methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the x-axis. The y-axis is PEN, where lower values indicate better performance.
where $P_{i}(i&gt;j)$ represents the winning rate of model $i$ as the "reviewer" believes that $i$ defeated $j$. The heatmap distribution of the PG value $P G(i, j)$ among seven LLMs across three datasets is demonstrated in the first row of Figure 3. It can be observed that the evaluation system exhibits severe bias. Especially on ChatGLM-6B and Mpt-7B models, they often believe that their results are better than other ones, as their PG values are greater than 0 across three datasets.</p>
<p>After the consistency optimization, we assign the learned confidence weight $w$ to the corresponding model and ultimately obtain the re-weighting PG value $\hat{P G}(i, j)$ as follows,</p>
<p>$$
\hat{P G}(i, j)=w_{i} \times P_{i}(i&gt;j)-w_{j} \times P_{j}(i&gt;j)
$$</p>
<p>The results of the re-weighting PG value $\hat{P G}(i, j)$ are displayed on the second row of Figure 3. It can be observed that the learned confidence weight $w$ can significantly mitigate the preference gaps of the whole evaluation system. In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias.</p>
<h1>3.3 Study of Elimination MEChanism</h1>
<p>Performance Comparison of Elimination Mechanisms. The PiCO and PRE methods both employ elimination mechanisms to remove those weakest LLMs from the "reviewer queue" during the evaluation process. As shown in Figure 4, the x-axis quantifies the number of reviewers eliminated, and the y-axis measures the PEN, where lower scores denote higher performance. It can be observed that both PiCO and PRE exhibit better performance with an increasing number of eliminated "reviewers". The proposed PiCO approach can achieve better performance than PRE in most cases. It is worth noting that the PRE method employs the accuracy of "qualification exams" to eliminate weak LLMs, and this process requires human annotation (Chu et al., 2024). On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve better evaluation results than PRE.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The average loss for different numbers of eliminated reviewers( $\downarrow$ ). It shows how the iterative elimination of weaker reviewers affects the overall loss in the peer-review system.</p>
<p>Automatic Learning of Elimination Thresholds. We observed that weaker LLMs tend to have poorer evaluation abilities, introducing significant noise into the peer-review system. Therefore, eliminating weaker models instead of retaining them enhances the robustness of the system. We employed an unsupervised approach to automatically learn the elimination threshold, as shown in Figure 5, by using the average training loss curve as the number of eliminated reviewers increases. It can be seen that removing weaker reviewers reduces the average loss of the entire system, indicating that eliminating noisy evaluations benefits the overall process. Notably, when $60 \%$ (or 9) of the weaker reviewers are removed, the system's loss reaches its minimum. This trend is consistent across all three datasets, suggesting that the elimination threshold is learned automatically. However, removing more than 9 stronger reviewers harms the evaluation process.</p>
<p>Table 3: Comparison of more metrics (Precision@K and RBP@K) and token consumption on Chatbot Arena.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>RBP@K $(\uparrow)$</th>
<th></th>
<th>Precision@K $(\uparrow)$</th>
<th></th>
<th>Input Token Output Token Annotation Cost</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Chatbot Arena Platforms</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>$\sim 7500 \mathrm{k}$</td>
<td>$\sim 10944 \mathrm{k}$</td>
<td>$\sim 32 \mathrm{k}$</td>
</tr>
<tr>
<td>GPTScore(flan-t5-xxl)</td>
<td>26.2\%</td>
<td>29.6\%</td>
<td>45.1\%</td>
<td>50.0\%</td>
<td>55.6\%</td>
<td>70.0\%</td>
<td>$\sim 22882 \mathrm{k}$</td>
<td>$\sim 12260 \mathrm{k}$</td>
</tr>
<tr>
<td>GPTScore(davinci-002)</td>
<td>42.0\%</td>
<td>50.6\%</td>
<td>53.3\%</td>
<td>62.5\%</td>
<td>77.8\%</td>
<td>80.0\%</td>
<td>$\sim 22882 \mathrm{k}$</td>
<td>$\sim 12260 \mathrm{k}$</td>
</tr>
<tr>
<td>PandaLM</td>
<td>63.5\%</td>
<td>63.5\%</td>
<td>66.2\%</td>
<td>62.5\%</td>
<td>55.6\%</td>
<td>60.0\%</td>
<td>$\sim 22882 \mathrm{k}$</td>
<td>$\sim 10355 \mathrm{k}$</td>
</tr>
<tr>
<td>PRD</td>
<td>67.2\%</td>
<td>73.8\%</td>
<td>81.3\%</td>
<td>87.5\%</td>
<td>88.9\%</td>
<td>80.0\%</td>
<td>$\sim 25087 \mathrm{k}$</td>
<td>$\sim 10935 \mathrm{k}$</td>
</tr>
<tr>
<td>PRE</td>
<td>78.0\%</td>
<td>81.3\%</td>
<td>81.3\%</td>
<td>87.5\%</td>
<td>88.9\%</td>
<td>80.0\%</td>
<td>$\sim 24120 \mathrm{k}$</td>
<td>$\sim 11115 \mathrm{k}$</td>
</tr>
<tr>
<td>PiCO (Ours)</td>
<td>83.2\%</td>
<td>83.2\%</td>
<td>85.9\%</td>
<td>100.0\%</td>
<td>100.0\%</td>
<td>90.0\%</td>
<td>$\sim 23823 \mathrm{k}$</td>
<td>$\sim 11685 \mathrm{k}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Stability validation of consistency optimization. We repeated the experiment with different seeds for 1000 times, and plotted the training loss curve and weight distribution. The results show that the learning process is stable and the learned $w$ is convergence.</p>
<h1>3.4 OTHER ReSULTS</h1>
<p>Validation on more metrics (Precision@K and RBP@K). We demonstrated the results of precision and RBP $(\mathrm{K}=8,9,10)$ with other baselines in Table 3 (left). The results show that the proposed PiCO approach can achieve better precision and RBP performance in all cases. These results once again validate that PiCO can predict the LLM ranking more accurately than other baselines.</p>
<p>Comparison of tokens consumed. We compute the token consumption of each method in Table 3 (right). It can be observed that the proposed PiCO approach has a similar token consumed with other baselines (e.g., PRD and PRE) while achieving better evaluation performance. Although Chatbot Arena has a smaller token consumption, it requires 33 k human annotations, while PiCO does not require any human annotations.</p>
<p>Stability validation of consistency optimization. We repeated the experiment with different seeds for 1000 times, and plotted the training loss curve and weight distribution in Figure 6. The results show that the proposed consistency optimization process is stable and the learned $w$ is convergence.</p>
<p>Comparing with existing benchmarks. We select the widely-used benchmarks (i.e., MMLU (Hendrycks et al., 2020) and GSM8K (Cobbe et al., 2021)) to evaluate the model performance ranking $\hat{\mathcal{R}}$, and calculate the Spearman's $S(\uparrow)$ and Kendall's $\tau(\uparrow)$ rank correlation with the human preference ranking $\mathcal{R}^{*}$. The results are demonstrated in Table 4. It can be observed that these benchmarks can only measure LLMs' specific capability on a confined set of tasks, which fails to assess their alignment with human preference. These phenomena have been widely validated in other literature (Zhou et al., 2023; Zheng et al., 2023; Chang et al., 2023) and have almost become a consensus in the community of LLM evaluation.</p>
<h2>4 Related Work</h2>
<p>Evaluation Benchmarks for Diversity. LLMs are designed to handle a variety of tasks, necessitating comprehensive benchmarks (Chang et al., 2023). Notable benchmarks include GLUE and</p>
<p>Table 4: Comparison with existing benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmarks</th>
<th style="text-align: center;">Spearman's Rank <br> Correlation Coefficient $S(\uparrow)$</th>
<th style="text-align: center;">Kendall's Rank <br> Correlation Coefficient $\tau(\uparrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: center;">PiCO (Ours)</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
</tbody>
</table>
<p>SuperGLUE (Wang et al., 2018; 2019), which simulate real-world scenarios across tasks such as text classification, translation, reading comprehension, and dialogue generation. HELM (Liang et al., 2022) provides a holistic evaluation of LLMs, assessing language understanding, generation, coherence, and reasoning. BIG-bench (Srivastava et al., 2022) pushes LLM capabilities with 204 diverse tasks. MMLU (Hendrycks et al., 2020) measures multitask accuracy across domains like mathematics and law. GSM8K (Cobbe et al., 2021) including 8K simple math questions with detailed solutions is widely used to assess the mathematical reasoning of models on grade-school-level questions. However, these evaluations can be compromised by benchmark leakage, where evaluation data inadvertently used for training leads to inflated performance metrics (Aiyappa et al., 2023).</p>
<p>Human Evaluation. Human evaluation provides reliable feedback that closely aligns with realworld applications (Chang et al., 2023). Liang et al. (2022) evaluated summary and misinformation scenarios across multiple models. Ziems et al. (2023) involved experts to assess model outputs in various domain-specific tasks. Bang et al. (2023) examined ChatGPT's performance in summarization, translation, and reasoning using human-annotated datasets. The LMSYS initiative introduced platforms like Chatbot Arena (Zheng et al., 2023), relying on human ratings as the primary evaluation metric. Currently, using these anonymous battle platforms has become the primary way to evaluate the large language models, and its success is attributed to the wisdom of the crowds (Surowiecki, 2005; Budescu \&amp; Chen, 2015; Weller, 2007) and have been proven to lead to better results than that of an individual. Despite its effectiveness, human evaluation is costly and subject to bias and cultural differences(Peng et al., 1997).</p>
<p>Large Language Models for Evaluation. The trend towards developing open-source LLMs has led to initiatives employing one or multiple LLMs as evaluators for assessing the outputs of LLMs. GPTScore (Fu et al., 2023) uses models like GPT-3 to assign probabilities to high-quality content through multidimensional evaluation. Bubeck et al. (2023) tested GPT-4, finding it rivaling human capabilities. Lin and Chen introduced LLM-EVAL (Lin \&amp; Chen, 2023) for evaluating dialogue quality with single prompts. PandaLM (Wang et al., 2023) employs LLMs as "judges" for evaluating instruction tuning. However, reliance on a single model can introduce biases such as positional (Dettmers et al., 2024), verbosity (Wang et al., 2024), and self-favoring biases (Liu et al., 2023b; Zheng et al., 2023). ChatEval (Chan et al., 2023) proposes a multi-agent framework to simulate human evaluation processes. Similarly, PRE (Chu et al., 2024) and PRD (Li et al., 2023a) use LLMs as evaluators, combining multiple evaluation outcomes for automated assessment. However, like the PRE method, which uses human feedback for supervised evaluation throughout the process, the comprehensive assessment of LLMs still incurs a relatively high cost.</p>
<h1>5 CONCLUSION</h1>
<p>In this paper, we propose PiCO , a novel unsupervised evaluation method to automatically evaluate Large Language Models (LLMs) without relying on human feedback. PiCO utilizes peer-review mechanisms to autonomously assess LLMs in a shared environment, where both open-source and closed-source models can respond to unlabeled questions and evaluate each other. In this setup, each LLM's response score is determined collectively by other anonymous models, aiming to maximize consistency across capabilities and scores. The extensive experiment results across multiple datasets and standard rank-based metrics demonstrate that PiCO effectively generates an LLM ranking that aligns closely with human preferences. In the future, we plan to extend the peer-review mechanism to evaluate the capabilities of multi-modality large models.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>This work was supported in part by the Natural Science Foundation of China (No. 62202014, 62332002, 62425101, 62088102), and also supported by the China Postdoctoral Science Foundation under Grant Number BX20240013 and 2024M760113.</p>
<h2>REFERENCES</h2>
<p>Guanaco - generative universal assistant for natural-language adaptive context-aware omnilingual outputs. https://guanaco-model.github.io/, 2023. Accessed: 15 April 2024.</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Stability AI. Stablelm-tuned-alpha-7b: A fine-tuned language model for diverse applications. https : //huggingface.co/stabilityai/stablelm-tuned-alpha-7b, 2023. Accessed: 15 April 2024.</p>
<p>Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. Can we trust the evaluation on chatgpt?, 2023.</p>
<p>Mohammad Allahbakhsh and Aleksandar Ignjatovic. Rating through voting: An iterative method for robust rating. arXiv preprint arXiv:1211.0390, 2012.</p>
<p>Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all, 2023.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Christoph Bandt and Bernd Pompe. Permutation entropy: a natural complexity measure for time series. Physical review letters, 88(17):174102, 2002.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.</p>
<p>Robert S Boyer and J Strother Moore. Mjrty—a fast majority vote algorithm. In Automated reasoning: essays in honor of Woody Bledsoe, pp. 105-117. Springer, 1991.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>David V Budescu and Eva Chen. Identifying expertise to extract the wisdom of crowds. Management science, 61(2):267-280, 2015.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ chatgpt quality. https://vicuna.lmsys.org, 2023. Accessed: 15 April 2024.</p>
<p>Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: A peer review based large language model evaluator. arXiv preprint arXiv:2401.15641, 2024.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/ 12/dolly-first-open-commercially-viable-instruction-tuned-llm.</p>
<p>Open-Assistant Contributors. Oasst-sft-4-pythia-12b: A supervised fine-tuning model for language understanding. https://huggingface.co/OpenAssistant/ oasst-sft-4-pythia-12b-epoch-3.5, 2023. Accessed: 15 April 2024.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Allan M. Feldman. Majority voting. SpringerLink, 2006.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April, 1, 2023a.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala-13b: Dialogue model for effective human-ai interaction. https://bair. berkeley.edu/blog/2023/04/03/koala/, 2023b. Accessed: 15 April 2024.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1-2):81-93, 1938.
Ann Lehman, Norm O'Rourke, Larry Hatcher, and Edward Stepanski. JMP for basic univariate and multivariate statistics: methods for researchers and social scientists. Sas Institute, 2013.</p>
<p>Charles Eric Leiserson, Ronald L Rivest, Thomas H Cormen, and Clifford Stein. Introduction to algorithms, volume 3. MIT press Cambridge, MA, USA, 1994.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023a.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023b.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711, 2023.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023a.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Kun-Peng Ning, Xun Zhao, Yu Li, and Sheng-Jun Huang. Active learning for open-set annotation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. $41-49,2022$.</p>
<p>Kun-Peng Ning, Hai-Jian Ke, Yu-Yang Liu, Jia-Yu Yao, Yong-Hong Tian, and Li Yuan. Sparse orthogonal parameters tuning for continual learning. arXiv preprint arXiv:2411.02813, 2024.</p>
<p>Kun-Peng Ning, Jia-Yu Yao, Yu-Yang Liu, Mu-Nan Ning, and Li Yuan. Gpt as a monte carlo language tree: A probabilistic perspective. arXiv preprint arXiv:2501.07641, 2025.</p>
<p>OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. Accessed: [insert date here].</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. Validity problems comparing values across cultures and possible solutions. Psychological methods, 2(4):329, 1997.</p>
<p>Philip Sedgwick. Pearson's correlation coefficient. Bmj, 345, 2012.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>James Surowiecki. The wisdom of crowds. Anchor, 2005.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et al. Skywork: A more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023.</p>
<p>Susan C Weller. Cultural consensus theory: Applications and frequently asked questions. Field methods, 19(4):339-368, 2007.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.</p>
<p>Shuo Yang, Kun-Peng Ning, Yu-Yang Liu, Jia-Yu Yao, Yong-Hong Tian, Yi-Bing Song, and Li Yuan. Is parameter collision hindering continual learning in llms? arXiv preprint arXiv:2410.10179, 2024.</p>
<p>Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don’t make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964, 2023.</p>
<p>Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? arXiv preprint arXiv:2305.03514, 2023.</p>
<h1>A Detailed Explanation of Metrics</h1>
<p>In this section, we provide a comprehensive explanation of the metrics used to evaluate the alignment between learned LLM rankings and human rankings. These metrics assess the strength of correlations, complexity, and the level of agreement between rankings. Specifically, we discuss five key metrics: Spearman's Rank Correlation Coefficient, Kendall's Rank Correlation Coefficient, Permutation Entropy, Count Inversions, and Longest Increasing Subsequence, detailing their formulations and intuitive interpretations.
i) Spearman's Rank Correlation Coefficient $S(\uparrow)$ (Lehman et al., 2013) measures the strength and direction of the monotonic relationship between two ranked variables. It is computed as:</p>
<p>$$
S\left(\hat{\mathcal{R}}, \mathcal{R}^{*}\right)=1-\frac{6 \sum_{i=1}^{m} d_{i}^{2}}{m\left(m^{2}-1\right)}
$$</p>
<p>where $d_{i}=\operatorname{rank}<em i="i">{\hat{\mathcal{R}}}\left(M</em>^{}\right)-\operatorname{rank}_{\mathcal{R<em>}}\left(M_{i}\right)$ is the difference between the ranks of LLM $M_{i}$ in the learned ranking $\hat{\mathcal{R}}$ and the human ranking $\mathcal{R}^{</em>}$, and $m$ is the total number of LLMs. A higher Spearman coefficient indicates a stronger correlation between the rankings.
ii) Kendall's Rank Correlation Coefficient $\tau(\uparrow)$ (Kendall, 1938) evaluates the similarity between two rankings by counting the number of concordant and discordant pairs. It is given by:</p>
<p>$$
\tau\left(\hat{\mathcal{R}}, \mathcal{R}^{*}\right)=\frac{C-D}{\frac{1}{2} m(m-1)}
$$</p>
<p>where $C$ represents the number of concordant pairs, and $D$ represents the number of discordant pairs. A pair $\left(M_{i}, M_{j}\right)$ is concordant if $M_{i}$ and $M_{j}$ have the same order in both $\hat{\mathcal{R}}$ and $\mathcal{R}^{<em>}$, meaning if $M_{i} \succ M_{j}$ in $\hat{\mathcal{R}}$, then $M_{i} \succ M_{j}$ in $\mathcal{R}^{</em>}$. Conversely, a pair is discordant if their relative order differs between the two rankings. A higher $\tau$ value indicates a closer alignment between the rankings.
iii) Permutation Entropy $H(\downarrow)$ (Bandt \&amp; Pompe, 2002) measures the complexity or randomness of sequences, which is formulated as follows:</p>
<p>$$
H\left(\hat{\mathcal{R}}, \mathcal{R}^{*}\right):=-\sum p(\pi) \log p(\pi)
$$</p>
<p>where</p>
<p>$$
p(\pi)=\frac{#\left{t \mid 0 \leq t \leq m-k,\left(M_{t+1}, \ldots, M_{t+k}\right) \in \pi\right}}{m-k+1}
$$</p>
<p>$\pi$ denotes different permutations, $k$ is a hyper-parameter recommended to be set to 3 to 7 , and we set $k=3$ in this paper. Intuitively, it samples some subsequences and calculates the entropy for all permutation types. And the lower the permutation entropy in the learned LLM rankings, the closer it is to the ground-truth human rankings.
iv) Count Inversions $C(\downarrow)$. Counting inversions (Leiserson et al., 1994) aims to measure the degree of disorder or "invertedness" in an array or sequence of elements. We thus define it as follows,</p>
<p>$$
C\left(\hat{\mathcal{R}}, \mathcal{R}^{*}\right):=\sum_{M_{i}, M_{j} \sim \mathcal{M}} \mathbf{1}\left{M_{i} \succ M_{j} \wedge i&lt;j\right}
$$</p>
<p>Where $\mathbf{1}{\cdot}$ is the indicator function that the value is 1 when the condition is met, otherwise it is 0 . Intuitively, the fewer inverse pairs in the learned LLM rankings, the closer it is to the ground-truth human rankings.
v) Longest Increasing Subsequence $L(\uparrow)$. The longest increasing subsequence aims to find the length of the longest subsequence in a given sequence of elements, where the subsequence is in increasing order. We utilize it to measure the degree of match with human rankings as follows,</p>
<p>$$
L\left(\hat{\mathcal{R}}, \mathcal{R}^{*}\right):=\max {d p[i] \mid 1 \leq i \leq m}
$$</p>
<p>where</p>
<p>$$
d p[i]=1+\max \left{d p[j] \mid 1 \leq j&lt;i \wedge M_{j} \prec M_{i}\right}
$$</p>
<p>$d p[i]$ represents the length of the longest increasing subsequence that ends with $M_{i}$. LIS allows for a nuanced understanding of the degree to which the learned ranking aligns with the ideal human ranking, with a higher LIS length indicating greater alignment.</p>
<h1>B DATASET FORMAT</h1>
<p>Focusing on the MT-Bench dataset, we demonstrate the ensuing data format utilizing dataset $\mathcal{Q}$. As Figure 7 illustrates, the Question dataset $\mathcal{Q}$ contains "Question id," "Category," "Question," and "Reference." In categories with definitive answers like "reasoning" or "math," the "Reference" field is populated with standard answers; otherwise, it remains blank. Each model M in our pool processes the Question dataset $\mathcal{Q}$ to generate the LLMs answer data $\mathcal{A}$, consisting of "Question id," "Answer id," "Model id," and "Answer." Finally, we combine pairs in $\mathcal{A}$ and appoint judges to evaluate, creating the Answer-Ranking data $\mathcal{D}$, featuring "Question id," "Model 1," "Model 2," "G1 winner," "G2 winner," and "Judge." Here, "G1 winner" and "G2 winner" indicate the outcomes of inputting reversed order responses of Model 1 and Model 2 into the judge model, a method employed to mitigate biases stemming from models' preferences for input order.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Format of the Question dataset $\mathcal{Q}$, LLMs responses data $\mathcal{A}$, and the Answer-Ranking data $\mathcal{D}$ for Peer Review</p>
<h2>C DETAILED PROMPT FOR REVIEWERS</h2>
<p>The evaluation prompts, as detailed in Section 2.2.1, are employed during the Peer Review Stage. These prompts are provided to the Reviewer Language Model Systems (LLMs), enabling them to generate evaluative preferences. In our experimental framework, we devised four distinct prompt settings. For each setting, a tailored prompt template was meticulously crafted as illustrated below:
Template for Single-Turn Interaction: This template is designed for single-turn interactions between users and LLMs, where there is no predetermined correct answer. It facilitates open-ended dialogue, allowing for a wide range of user inquiries without the expectation of specific responses.
Referenced Template for Single-Turn Interaction: Tailored for single-turn dialogues between users and LLMs, this template incorporates predefined correct answers. It is particularly suited for interactions involving factual inquiries, such as mathematics or logic problems, where accuracy and reference to correct information are paramount.
Template for Multi-Turn Interaction: This template caters to multi-turn conversations between users and LLMs, without predefined answers. It supports extended interactions, enabling users to explore topics in depth through a series of interconnected questions and responses.
Referenced Template for Multi-Turn Interaction: Designed for multi-turn dialogues with predefined correct answers, this template is ideal for complex inquiries requiring sequential reasoning or problem-solving, such as mathematical computations or logical deductions.
Each template is carefully constructed to match its intended use-case, providing a structured framework that guides the interaction between users and LLMs towards achieving desired outcomes, whether for open-ended exploration or precise problem-solving.</p>
<h1>Template for Single-Turn Answer</h1>
<p>System prompt: Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: " $[[\mathrm{A}]]$ " if assistant A is better, " $[[\mathrm{B}]]$ " if assistant B is better, and " $[[\mathrm{C}]]$ " for a tie.
User Question: {question}
Assistant A's Answer: {answer a}
Assistant B's Answer: {answer b}</p>
<h2>Referenced Template for Single-Turn Answer</h2>
<p>System prompt: Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below, with reference to the provided reference answers. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: " $[[\mathrm{A}]]$ "if assistant A is better, " $[[\mathrm{B}]]$ " if assistant B is better, and " $[[\mathrm{C}]]$ " for a tie.
User Question: {question}
Reference Answer: {reference answer}
Assistant A's Answer: {answer a}
Assistant B's Answer: {answer b}</p>
<h2>Template for Multi-Turn Answer</h2>
<p>System prompt: Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: " $[[\mathrm{A}]]$ " if assistant A is better, " $[[\mathrm{B}]]$ " if assistant B is better, and " $[[\mathrm{C}]]$ " for a tie
Assistant A's Conversation with User:
User: {question 1}
Assistant A: {answer a1}
User: {question 2}
Assistant A: {answer a2}
Assistant B's Conversation with User:
User: {question 1}
Assistant B: {answer b1}
User: {question 2}
Assistant B: {answer b2}</p>
<h1>Referenced Template for Multi-Turn Answer</h1>
<p>System prompt: Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below, in comparison to the reference answers. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: "[[A]]"if assistant A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie.
Reference Answer
User: {question 1}
Reference answer: {ref answer 1}
User: {question 2}
Reference answer: {ref answer 2}
Assistant A's Conversation with User:
User: {question 1}
Assistant A: {answer a1}
User: {question 2}
Assistant A: {answer a2}
Assistant B's Conversation with User:
User: {question 1}
Assistant B: {answer b1}
User: {question 2}
Assistant B: {answer b2}</p>
<h2>D Scoring Methodology</h2>
<p>In Section 2.2.2, Equation 8 delineates the methodology for optimizing scores. Within this framework, the function $\mathbf{1}\left{A_{i}^{j}&gt;A_{i}^{k}\right}$ is more precisely defined as $f\left(A_{i}^{j}, A_{i}^{k}\right)$. Additionally, the function $f\left(A_{i}^{j}, A_{i}^{k}\right)$ is not fixed and can be implemented using various computational strategies. We introduce two distinct methodologies in this context: the Elo mechanism and the Rank mechanism.
Within the framework of the Elo mechanism, as specified by Equation 16, the $B A S E$ value is set to 10 , and the $S C A L E$ factor is determined to be 400 . This approach facilitates a dynamic adjustment of scores based on the outcomes of pairwise comparisons, allowing for a nuanced reflection of performance variations among models.
Conversely, in the context of the Rank mechanism, as outlined by Equation 17, $\operatorname{rank}(j)$ signifies the current ranking of model $j$, with the constant $K$ assigned a value of 200. This mechanism employs a model's ranking within a predefined hierarchy as a pivotal factor in score calculation, thereby providing a straightforward, yet effective, method for evaluating comparative model performance.</p>
<p>$$
\begin{aligned}
&amp; f\left(A_{i}^{j}, A_{i}^{k}\right)=\left{\begin{array}{ll}
1-\frac{1}{1+\operatorname{BASE}^{\left((G(k)-G(j)) / \operatorname{SCALE}\right)}} &amp; \text { if } A_{i}^{j}&gt;A_{i}^{k} \
0.5-\frac{1}{1+\operatorname{BASE}^{\left((G(k)-G(j)) / \operatorname{SCALE}\right)}} &amp; \text { if } A_{i}^{j}=A_{i}^{k} \
0-\frac{1}{1+\operatorname{BASE}^{\left((G(k)-G(j)) / \operatorname{SCALE}\right)}} &amp; \text { if } A_{i}^{j}<A_{i}^{k}
\end{array}\right. \\
& f\left(A_{i}^{j}, A_{i}^{k}\right)=\left\{\begin{array}{ll}
1+(\operatorname{rank}(j)-\operatorname{rank}(k)) / K & \text { if } A_{i}^{j}>A_{i}^{k} \
0.5 &amp; \text { if } A_{i}^{j}=A_{i}^{k} \
0 &amp; \text { if } A_{i}^{j}&lt;A_{i}^{k}
\end{array}\right.
\end{aligned}
$$</p>
<h2>E Overall Algorithm of Peer Review</h2>
<p>The overall algorithm, as delineated in Algorithm 1, encapsulates the comprehensive process outlined in Section 2.2. This sequence commences with "Data Collection and LLMs Pool Construction," progresses through "Answer-Ranking Data Construction Based on Peer Review," advances to "Consistency Optimization," and culminates with the "Unsupervised Elimination Mechanism."</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Overall Framework Algorithm of Peer Review
Require: Unlabeled dataset \(\mathcal{Q}\), Pool of LLMs \(\mathcal{M}\), Active LLM pool \(\mathcal{M}^{*}=\mathcal{M}\)
Ensure: Consistency-optimized ranking of LLMs \(\mathcal{R}^{*}\)
    Initialize response matrix \(A \leftarrow \emptyset\)
    for each question \(q_{i} \in \mathcal{Q}\) do
        Initialize response vector for question \(q_{i}, A^{i} \leftarrow \emptyset\)
        for each model \(m_{j} \in \mathcal{M}\) do
            \(A_{j}^{i} \leftarrow\) response of model \(m_{j}\) to question \(q_{i}\)
            \(A^{i} \leftarrow A^{i} \cup\left\{A_{j}^{i}\right\}\)
        end for
        Shuffle \(A^{i}\) to obtain permuted response vector \(A^{i}\)
        \(A \leftarrow A \cup\left\{A^{i}\right\}\)
    end for
    Initialize answer-ranking data \(D \leftarrow \emptyset\)
    Initialize model weights vector \(w\) with Gaussian distribution
    for each permuted response vector \(A^{i}\) do
        for each pair of responses \(\left(A_{i}^{j}, A_{i}^{k}\right)\) in \(A^{i}\) do
            for \(s \leftarrow 1\) to 5 do \(\triangleright\) Randomly select 5 models for evaluation
                Evaluate the pair \(\left(A_{i}^{j}, A_{i}^{k}\right)\) with model \(m_{s}\)
                \(D \leftarrow D \cup\left\{\left(A_{i}^{j}, A_{i}^{k},&gt;w^{s}\right)\right\}\)
            end for
        end for
    end for
    Initialize scores \(G_{j}\) for each model \(m_{j} \in \mathcal{M}\) to the Elo initial score
    repeat
        while not converged do
            for each model \(m_{j} \in \mathcal{M}\) do
                Compute \(G_{j}\) using updated formula:
                    \(G_{j}=\sum_{i} \sum_{k \neq j} \sum_{s \neq k, s \neq j} \mathbf{1}\left\{A_{i}^{j}, A_{i}^{k}\right\} \times w^{s} \quad\left(A_{i}^{j}, A_{i}^{k},&gt;w^{s}, s \in \mathcal{M}^{*}\right) \in D\)
            end for
            Update weight vector \(w\) to maximize the consistency of \(w\) and \(G\)
        end while
        Sort \(\mathcal{M}^{*}\) by \(G_{j}\) to identify \(\mathcal{M}_{\text {min }}\), the lowest-scoring model
        if size of \(\mathcal{M}^{*}&gt;\) threshold then
            Remove \(\mathcal{M}_{\text {min }}\) from \(\mathcal{M}^{*}\)
        end if
    until size of \(\mathcal{M}^{*}&lt;\) threshold
    Compute the final ranking \(\mathcal{R}^{*}\) based on the optimized scores \(G_{j}\)
    return \(\mathcal{R}^{*}\)
</code></pre></div>

<h1>F COMPLETE EXPERIMENTAL RESULTS</h1>
<p>In Section 3.4, we both employ elimination mechanisms to cull the weakest LLMs from the 'reviewer queue' during the evaluation process. In Figures 8 and 9, we present the results for the PEN and LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote superior performance. It is evident that both the 'PiCO' and PRE approaches demonstrate enhanced performance as the number of eliminated 'reviewers' increases. In most cases, the proposed 'PiCO' method outperforms PRE.</p>
<p>In Section 3.5, we validate the effectiveness of the consistency assumption and compare it with the Average Performance of the Reviewer Queue, i.e., employing a single LLM as the 'reviewer' to evaluate all response pairs and then calculating the average results of all LLMs. The comprehensive results compared with the Reviewer Queue are illustrated in Table5, Figure 10, 11 and 12, revealing that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating that the evaluations from most LLMs are noise. However, our 'PiCO' approach nearly matches the evaluative prowess of the pool's most capable LLM, GPT-3.5. Remarkably, given its unsupervised</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Performance comparison of the PiCO (Ours) and PRE (Chu et al., 2024) methods on the MT-Bench, Chatbot Arena, and AlpacaEval datasets, with the number of eliminated reviewers on the x -axis. The y -axis is CIN, where lower values indicate better performance.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance comparison of the PiCO (Ours) and PRE (Chu et al., 2024) methods on the MT-Bench, Chatbot Arena, and AlpacaEval datasets, with the number of eliminated reviewers on the x -axis. The y -axis is LIS, where upper values indicate better performance.</p>
<p>Table 5: Comparison of performance across three datasets using Unsupervised methods versus using single models in reviewer queue.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">MT-Bench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Chatbot Arena</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AlpacaEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PEN $(\downarrow)$</td>
<td style="text-align: center;">$\operatorname{CIN}(\downarrow)$</td>
<td style="text-align: center;">LIS $(\uparrow)$</td>
<td style="text-align: center;">PEN $(\downarrow)$</td>
<td style="text-align: center;">CIN $(\downarrow)$</td>
<td style="text-align: center;">LIS $(\uparrow)$</td>
<td style="text-align: center;">PEN $(\downarrow)$</td>
<td style="text-align: center;">CIN $(\downarrow)$</td>
<td style="text-align: center;">LIS $(\uparrow)$</td>
</tr>
<tr>
<td style="text-align: center;">Gpt-3.5</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">11.00</td>
<td style="text-align: center;">11.00</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">9.00</td>
</tr>
<tr>
<td style="text-align: center;">Guanaco-33B</td>
<td style="text-align: center;">1.25</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">28.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">9.00</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">8.00</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM-13B</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">9.00</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">8.00</td>
</tr>
<tr>
<td style="text-align: center;">Koala-13B</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">43.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.54</td>
<td style="text-align: center;">31.00</td>
<td style="text-align: center;">7.00</td>
</tr>
<tr>
<td style="text-align: center;">gpt4all-13B</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">45.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">35.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.73</td>
<td style="text-align: center;">42.00</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: center;">Mpt-7B</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">39.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">7.00</td>
</tr>
<tr>
<td style="text-align: center;">Oass-pythia-12B</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">42.00</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.70</td>
<td style="text-align: center;">47.00</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-13B</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">7.00</td>
</tr>
<tr>
<td style="text-align: center;">FastChat-T5-3B</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">29.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">7.00</td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM-6B</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">33.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">55.00</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: center;">StableLM-7B</td>
<td style="text-align: center;">1.68</td>
<td style="text-align: center;">63.00</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.75</td>
<td style="text-align: center;">44.00</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">4.00</td>
</tr>
<tr>
<td style="text-align: center;">Dolly-12B</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">46.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.57</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.75</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">6.00</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">35.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">1.70</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">5.00</td>
</tr>
<tr>
<td style="text-align: center;">Average Performance of All Review LLMs</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">34.87</td>
<td style="text-align: center;">6.93</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">38.80</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">33.13</td>
<td style="text-align: center;">6.93</td>
</tr>
<tr>
<td style="text-align: center;">PRD(Li et al., 2023a)</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">9.00</td>
</tr>
<tr>
<td style="text-align: center;">PRE(Chu et al., 2024)</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">8.00</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">15.00</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">8.00</td>
</tr>
<tr>
<td style="text-align: center;">PiCO (Ours)</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">14.50</td>
<td style="text-align: center;">8.75</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">17.00</td>
<td style="text-align: center;">9.00</td>
</tr>
</tbody>
</table>
<p>nature, the 'PiCO' method demonstrates the capability to mitigate the influence of noise, reaching the evaluation upper bound (the strongest LLM) within any given unknown LLM pool $M$, even in the absence of prior ranking information.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Comparison of performance on the CIN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Comparison of performance on the PEN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models.</p>
<h1>G Selected Models and Optimized Ranking</h1>
<p>For our analysis, we meticulously selected 15 LLMs spanning a variety of architectures, encompassing both open-source and closed-source models, as detailed in the subsequent table. Our curated selection features prominent LLMs including the closed-source "gpt-3.5-turbo," "chatglm" which is predicated on the encoder-decoder framework, "fastchat-t5-3b" that leverages Google's T5 (Text-to-Text Transfer Transformer) architecture, and "llama-13b" founded on the GPT architectural principles.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
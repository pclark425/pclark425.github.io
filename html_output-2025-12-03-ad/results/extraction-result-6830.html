<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6830 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6830</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6830</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-276421915</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.12616v2.pdf" target="_blank">Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions</a></p>
                <p><strong>Paper Abstract:</strong> Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6830.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6830.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QuaSAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quasi-Symbolic Abstract Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/annotation framework that guides LLMs to operate at a higher abstraction level by eliciting quasi-symbolic predicates, variables and semi-formal transformations (Abstraction → Formalisation → Explanation → Answering) to reduce content bias and improve robustness and verifiability of chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-4o, Llama-3 family, Qwen2 family (paper evaluates multiple models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model architecture but a structured prompting and annotation strategy: instructs LLMs to produce a 4-step quasi-symbolic reasoning trace (abstraction, formalisation, explanation, strict answering). Used both as an in-context prompting method and to generate synthetic demonstrations for supervised fine-tuning of smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + quasi-symbolic prompting (no external symbolic engine integrated)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Demonstrations synthetically generated by GPT-4o following the QuaSAR prompt, filtered (exact-match) and used to fine-tune smaller LLMs; fine-tuning datasets drawn from GSM8K, AQuA, SVAMP, MMLU-Redux, OlympiadBench, GPQA, DROP (see paper data splits).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Quasi-symbolic Chain-of-Thought: explicit abstraction of predicates/variables, semi-formal symbolic translation, stepwise symbolic-style explanation, strict final answer format; used as both ICL and to generate SFT demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, GSM-Symbolic (robustness evaluations), AQuA, SVAMP, MMLU-Redux, Olympiad Bench, GPQA, DROP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mixture of mathematical word-problem benchmarks (GSM8K, AQuA, SVAMP, OlympiadBench), adversarial/perturbed symbolic math (GSM-Symbolic), multi-domain multi-choice and reasoning benchmarks (MMLU-Redux, GPQA), and discrete reading-comprehension (DROP).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical word problems, symbolic reasoning robustness, multiple-choice natural language reasoning, discrete paragraph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match / accuracy (task dependent); self-consistency and robustness variation measures</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported overall boosts: up to +8 percentage points accuracy over standard CoT on aggregated tasks; as ICL QuaSAR produced +19.1% absolute improvement for GPT-4o over baseline zero-shot (paper reports other per-model ICL gains: +11.8% Llama-3-70B, +17.2% Qwen2-72B). When applied to GPT-4o, an average increase of ~8% vs CoT and +6.8% and +8.2% vs CoMAT and Faithful CoT respectively is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Consistently outperforms standard CoT, Faithful CoT, CoMAT and FLARE in the paper's experiments (both as ICL on large models and as SFT demonstration source for smaller models), with larger absolute gains for high-scale LLMs when used as in-context prompts and useful improvements when used to generate fine-tuning demonstrations for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Eliciting quasi-symbolic abstractions improves accuracy, verifiability and robustness of multi-step reasoning traces; formalisation and explanation steps are especially critical (ablation shows removing formalisation/explanation drops performance by >3.5 points). QuaSAR reduces sensitivity to superficial perturbations (e.g., answer-order swaps in MMLU-Redux, perturbed GSM-Symbolic) compared to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires stronger LLMs to follow the structured quasi-symbolic process in-context (smaller models often fail to benefit from QuaSAR ICL without SFT); the annotation pipeline relies on a high-quality annotator (GPT-4o) and filters out ~50% of generated demos; ablation/error analysis reports a total pipeline failure rate of 36% on an ablation subset, indicating error propagation across steps; not a fully symbolic prover—still susceptible to mistakes in individual steps and requires manual/heuristic filtering of demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6830.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits multi-step natural-language rationales from LLMs to improve performance on complex reasoning tasks by decomposing problems into intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generic prompting approach applied to various transformer LLMs (used as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting technique rather than a model architecture; typically used with autoregressive transformer LLMs to elicit stepwise natural language reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with natural-language CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Natural-language chain-of-thought: free-form stepwise rationales, sometimes augmented with self-consistency or majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same benchmarks as QuaSAR in this paper (GSM8K, AQuA, SVAMP, MMLU-Redux, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See QuaSAR entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical word problems, multi-step reasoning, natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match / accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Serves as baseline; QuaSAR reports an average improvement of ~8 percentage points over CoT (aggregate claim) and per-task/Table comparisons show CoT being outperformed by QuaSAR across evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT is the main baseline; QuaSAR consistently outperforms CoT in the paper's experiments (both in-context on large models and as a source for fine-tuning demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT substantially improves reasoning compared to zero-shot, but CoT explanations are susceptible to content bias and unfaithful traces; semi-formal quasi-symbolic structuring can mitigate some of these weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Content biases and unfaithful/explanatory errors that harm robustness and verifiability; sensitive to superficial perturbations; less structured than quasi-symbolic approaches, making systematic verification harder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6830.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Faithful CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method aiming to produce more faithful and verifiable chain-of-thought rationales, cited and compared as a prior approach that attempts to reduce hallucinations/unfaithful steps in CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Approach applied to LLMs in prior work (Lyu et al., 2023); used as a comparative baseline in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach that modifies CoT prompting/training to enforce faithfulness and alignment between rationale and output (paper cites it as a baseline but does not re-implement the internals).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + enhanced CoT prompting/training for faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Faithful Chain-of-Thought: aims to ensure rationales are causally linked to answers, often via constrained prompting/training and verification steps.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used as comparator on same benchmarks (reported comparisons vs QuaSAR on GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See QuaSAR entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step reasoning and math/NLU benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / exact-match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports QuaSAR improves by ~8.2 percentage points over Faithful CoT when applied on GPT-4o (aggregate claim).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared in tables; QuaSAR reported to outperform Faithful CoT on evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Faithful CoT addresses faithfulness but QuaSAR's semi-formal symbolic structuring yields higher accuracy and robustness in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned limitations: previous faithful methods still rely on natural-language rationales and can be brittle; full symbolic formalisation approaches carry additional translation costs (context in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6830.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoMAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Mathematically Annotated Thought (CoMAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior technique that augments chain-of-thought with mathematical annotations to improve math reasoning; used as a baseline in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comat: Chain of mathematically annotated thought improves mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Approach applied to transformer LLMs (cited baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT variant that structurally augments mathematical reasoning steps with formal mathematical annotations to enhance correctness on math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + mathematical annotation in CoT</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Augmented chain-of-thought with mathematical annotations/structures to support symbolic calculation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math benchmarks (e.g., GSM8K) and other tasks used for comparison in paper</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See QuaSAR entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical word problems and symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / exact-match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports QuaSAR improves by ~6.8 percentage points over CoMAT on GPT-4o (aggregate claim).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoMAT improves math reasoning, but QuaSAR yields higher accuracy and robustness across tasks in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoMAT is strong for math but QuaSAR's semi-formal abstraction and consistent four-step pipeline further improves results and robustness in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoMAT and similar formal-annotation approaches can still be task-specific and may require tighter formalisation for some math domains; the paper positions QuaSAR as more flexible and efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6830.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLARE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLARE (Faithful logic-aided reasoning and exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that integrates logical structure into reasoning and exploration to improve faithfulness; cited as a comparative method in the related work and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Flare: Faithful logic-aided reasoning and exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Approach applied to transformer LLMs (cited baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A logic-aided framework that injects logical constraints or structure into LLM reasoning to improve faithfulness and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + logic-aided components (method-level augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Logic-aided CoT with mechanisms to keep reasoning faithful (likely external checks or structured constraints per citation).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used as comparator on similar benchmarking suites in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See QuaSAR entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Logical and mathematical reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / exact-match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports QuaSAR outperforms FLARE in their comparative evaluations (exact per-task numbers reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>FLARE improves faithfulness, but QuaSAR is reported to achieve higher overall accuracy and robustness across the tested benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FLARE is a competitive faithful-reasoning approach; QuaSAR's quasi-symbolic abstraction offers a different trade-off (less reliance on external tools, more flexible semi-formalisation) and outperformed FLARE in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>FLARE (like other faithful approaches) depends on logic aids that may be costly or less flexible across heterogeneous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6830.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid LLM + Symbolic Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid frameworks integrating LLMs with external symbolic reasoning tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches where LLMs map natural language to formal/symbolic representations that are then processed by external theorem provers, SAT solvers, or deterministic symbolic engines to improve logical fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Varies (e.g., Logic-LM, SatLM, other LLM+solver frameworks referenced in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid pipelines: LLM transforms NL into symbolic programs/formulas which are executed or verified by symbolic solvers (theorem provers, SAT/SMT solvers, domain-specific deterministic engines).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + external symbolic solver (neuro-symbolic pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Often formal corpora or parallel NL→formalization datasets used in referenced works (paper cites these approaches but does not retrain them).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Full symbolic formalisation + external solver verification/execution (program synthesis / theorem proving pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>External symbolic engines/theorem provers/SAT-style solvers used to deterministically check or compute logical steps after LLM-driven formalisation (paper references works such as Logic-LM, SatLM, and verification/refinement by Quan et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used in prior work on logical reasoning and verification; referenced as alternative to QuaSAR (no single benchmark specified within paper for these systems).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Formal reasoning and symbolic-program evaluation tasks; these pipelines are often applied to benchmarks requiring exact formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formal theorem proving, program execution, symbolic deduction, verified calculation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by prior work (proof success, exactness, verified correctness); paper does not re-evaluate these methods directly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper positions hybrid approaches as improving fidelity but suffering from a bottleneck: need complete NL→symbolic translation which harms efficiency and flexibility; QuaSAR aims a trade-off by partial/semi-formal translation inside NL rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hybrid LLM+symbolic approaches improve verifiability but incur translation bottlenecks and reduced flexibility; QuaSAR is offered as an intermediate approach that leverages selective formalisation without full translation and without external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Require complete and accurate NL→formal translation (difficult in noisy NL), depend on external tools and verifiers, can be computationally costly and less flexible across varied natural-language problem styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6830.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capability large language model used as both an evaluator/judge and as the primary annotator to generate QuaSAR demonstrations; used for in-context evaluation and to generate synthetic training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 variant (proprietary autoregressive transformer model); used via API for generation, in-context prompting, and sometimes as a judge (GPT-4o-mini used as a judge).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Proprietary large transformer (autoregressive); used with chain-of-thought and QuaSAR prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted with CoT and QuaSAR templates; used to generate stepwise quasi-symbolic demonstrations and produce final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Evaluated on GSM8K, AQuA, SVAMP, MMLU-Redux, OlympiadBench, GPQA, DROP, GSM-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>As described in QuaSAR entry; used both to run experiments and to generate training demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical word problems, symbolic robustness tests, NLU multiple-choice and reading-comprehension reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match / accuracy; self-correction evaluation and robustness (variation under perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>As ICL, QuaSAR on GPT-4o produced reported gains such as +19.1% over zero-shot baseline and ~+8% over CoT on aggregated tasks; specific table entries in the paper show improvements across symbolic and natural language tasks (e.g., QuaSAR 87.4 vs CoT ~84.3 in aggregated row for GPT-4o in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>GPT-4o with QuaSAR outperforms GPT-4o with standard CoT, Faithful CoT, CoMAT and FLARE in the paper's reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-capacity LLMs like GPT-4o can effectively follow QuaSAR in-context and generate high-quality quasi-symbolic demonstrations; these demonstrations in turn improve smaller models when used for SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proprietary model with unspecified parameter count; the paper relies on GPT-4o to produce demonstrations requiring filtering to remove incorrect outputs (~50% filtered out), indicating that even powerful LLMs produce noisy generation requiring quality control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6830.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open (Meta) LLaMA-3 family model (70B) evaluated in the paper; capable of benefiting from QuaSAR prompting but with smaller absolute gains than GPT-4o in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B (meta-llama/Meta-Llama-3-70B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>70 billion parameter LLaMA-3 family instruction-tuned transformer model (public release referenced in paper), evaluated with CoT and QuaSAR prompting and as a fine-tuning target.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (Llama-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with CoT and QuaSAR prompting/finetuning; SFT experiments using QuaSAR-generated demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQuA, SVAMP, MMLU-Redux, OlympiadBench, GPQA, DROP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See QuaSAR entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical/symbolic reasoning and language understanding tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / exact-match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports as ICL QuaSAR yielded +11.8% improvement for Llama-3-70B over baseline; tables show QuaSAR improves over CoT for this model family (per-table numbers reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>QuaSAR ICL and SFT using QuaSAR demonstrations improved performance compared to CoT and baseline SFT; however smaller Llama variants require tuning to fully leverage QuaSAR.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large open Llama variants gain from QuaSAR prompting and from QuaSAR-based SFT; formalisation and explanation steps are particularly important for math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller Llama variants (8B, 1B) often cannot follow QuaSAR in-context without SFT; generation temperature differences (paper used τ=0.5 for Llama) and compute constraints are discussed as practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6830.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2 72B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 72B-parameter Qwen2 family model evaluated in the paper; used to compare QuaSAR against CoT and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72B (Qwen/Qwen2-72B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>72B-parameter autoregressive transformer model from the Qwen2 family; used with CoT and QuaSAR prompting in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (Qwen family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with CoT and QuaSAR prompting and with QuaSAR-generated SFT data for smaller Qwen variants.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Same benchmark suite as used elsewhere in paper</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See QuaSAR entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical and NLU reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / exact-match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported QuaSAR ICL gains include +17.2% for Qwen2-72B over baseline zero-shot in the paper's experiments; QuaSAR outperforms CoT on aggregate scores reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>QuaSAR yields larger absolute improvements on high-capacity models like Qwen2-72B when used as ICL; QuaSAR-based SFT also improves smaller Qwen variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-capacity Qwen models benefit substantially from QuaSAR prompting; QuaSAR is effective across different LLM families.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller Qwen models need fine-tuning with QuaSAR demonstrations to reap benefits; paper notes dependence on careful demonstration filtering and tuning hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6830.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM-Symbolic (perturbed/robustness version of GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant/extension of GSM8K designed to test LLM robustness to superficial symbolic/lexical perturbations and to expose limitations in mathematical reasoning under shallow variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Benchmark (evaluated against LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Perturbed/robustness version of the GSM8K math word-problem dataset that introduces symbolic or superficial variations to test brittleness of LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-Symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mathematical word problems with symbolic/lexical perturbations to evaluate robustness of reasoning solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Symbolic mathematical reasoning and robustness testing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / exact-match and variation under perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports QuaSAR achieves more stable performance (less variation) than CoT on GSM-Symbolic and that QuaSAR-tuned models show improved accuracy on GSM-Symbolic (Figure/Table references in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>QuaSAR shows substantially less performance variation under perturbations compared to CoT; trained models tuned with QuaSAR demonstrations outperform those tuned with CoT on GSM-Symbolic in the paper's plots.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Quasi-symbolic abstractions help disentangle content from logical structure and improve robustness to superficial problem variations in symbolic math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Even with QuaSAR, there are nontrivial error rates and error propagation across QuaSAR steps (paper reports total pipeline failure rate 36% on an ablation subset using GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6830.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU-Redux</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMLU-Redux</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified/Redux version of the MMLU benchmark used to evaluate natural language multi-choice reasoning and robustness (e.g., answer-order swaps) in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Benchmark (evaluated across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A reworked slice of MMLU that focuses on robustness to superficial variations such as randomizing choice order; used in the paper to test QuaSAR vs CoT stability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MMLU-Redux</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multiple-choice multi-domain reasoning benchmark with robustness-focused perturbations (e.g., answer-order swaps).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multiple-choice natural-language reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; variation under choice-order perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that QuaSAR consistently achieves the same performances with considerably less variation than CoT on MMLU-Redux (Table 4 summarizes robustness gains).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>QuaSAR shows improved robustness to choice-order swaps compared to CoT and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured quasi-symbolic reasoning reduces sensitivity to superficial perturbations in multiple-choice reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not claim QuaSAR fully solves all MMLU failure modes; smaller models especially show limited gains when used as pure ICL without SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6830.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6830.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A popular grade-school math word-problem dataset used as a core symbolic/mathematical benchmark in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Benchmark (evaluated across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset of arithmetic/multi-step math word problems commonly used to evaluate LLM mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Math word-problem dataset for multi-step arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical word-problem solving (symbolic arithmetic and reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match (string match) / accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>QuaSAR reported to improve exact-match accuracy on GSM8K compared to CoT in the paper's experiments (per-table numbers show improvements for GPT-4o and other models; example: QuaSAR on GPT-4o shows higher GSM8K accuracy versus CoT in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>QuaSAR outperforms CoT and other baselines on GSM8K in the reported experiments, especially when formalisation and explanation steps are retained.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semi-formal symbolic translation (formalisation step) is particularly important for mathematical subsets (ablation removed step 2 or 3 causes >3.5 point drops on GSM8K/AQuA).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mathematical tasks still show sensitivity to the correctness of intermediate steps; QuaSAR improves but does not entirely eliminate errors (error propagation remains a concern).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Comat: Chain of mathematically annotated thought improves mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Flare: Faithful logic-aided reasoning and exploration <em>(Rating: 2)</em></li>
                <li>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Verification and refinement of natural language explanations through llm-symbolic theorem proving <em>(Rating: 1)</em></li>
                <li>Satlm: Satisfiability-aided language models using declarative prompting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6830",
    "paper_id": "paper-276421915",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "QuaSAR",
            "name_full": "Quasi-Symbolic Abstract Reasoning",
            "brief_description": "A prompting/annotation framework that guides LLMs to operate at a higher abstraction level by eliciting quasi-symbolic predicates, variables and semi-formal transformations (Abstraction → Formalisation → Explanation → Answering) to reduce content bias and improve robustness and verifiability of chain-of-thought reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-4o, Llama-3 family, Qwen2 family (paper evaluates multiple models)",
            "model_description": "Not a model architecture but a structured prompting and annotation strategy: instructs LLMs to produce a 4-step quasi-symbolic reasoning trace (abstraction, formalisation, explanation, strict answering). Used both as an in-context prompting method and to generate synthetic demonstrations for supervised fine-tuning of smaller models.",
            "model_size": null,
            "architecture_type": "Transformer + quasi-symbolic prompting (no external symbolic engine integrated)",
            "training_data": "Demonstrations synthetically generated by GPT-4o following the QuaSAR prompt, filtered (exact-match) and used to fine-tune smaller LLMs; fine-tuning datasets drawn from GSM8K, AQuA, SVAMP, MMLU-Redux, OlympiadBench, GPQA, DROP (see paper data splits).",
            "reasoning_method": "Quasi-symbolic Chain-of-Thought: explicit abstraction of predicates/variables, semi-formal symbolic translation, stepwise symbolic-style explanation, strict final answer format; used as both ICL and to generate SFT demonstrations.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, GSM-Symbolic (robustness evaluations), AQuA, SVAMP, MMLU-Redux, Olympiad Bench, GPQA, DROP",
            "benchmark_description": "Mixture of mathematical word-problem benchmarks (GSM8K, AQuA, SVAMP, OlympiadBench), adversarial/perturbed symbolic math (GSM-Symbolic), multi-domain multi-choice and reasoning benchmarks (MMLU-Redux, GPQA), and discrete reading-comprehension (DROP).",
            "task_type": "Mathematical word problems, symbolic reasoning robustness, multiple-choice natural language reasoning, discrete paragraph reasoning",
            "performance_metric": "Exact-match / accuracy (task dependent); self-consistency and robustness variation measures",
            "performance_value": "Reported overall boosts: up to +8 percentage points accuracy over standard CoT on aggregated tasks; as ICL QuaSAR produced +19.1% absolute improvement for GPT-4o over baseline zero-shot (paper reports other per-model ICL gains: +11.8% Llama-3-70B, +17.2% Qwen2-72B). When applied to GPT-4o, an average increase of ~8% vs CoT and +6.8% and +8.2% vs CoMAT and Faithful CoT respectively is reported.",
            "comparison_with_baseline": "Consistently outperforms standard CoT, Faithful CoT, CoMAT and FLARE in the paper's experiments (both as ICL on large models and as SFT demonstration source for smaller models), with larger absolute gains for high-scale LLMs when used as in-context prompts and useful improvements when used to generate fine-tuning demonstrations for smaller models.",
            "key_findings": "Eliciting quasi-symbolic abstractions improves accuracy, verifiability and robustness of multi-step reasoning traces; formalisation and explanation steps are especially critical (ablation shows removing formalisation/explanation drops performance by &gt;3.5 points). QuaSAR reduces sensitivity to superficial perturbations (e.g., answer-order swaps in MMLU-Redux, perturbed GSM-Symbolic) compared to CoT.",
            "limitations": "Requires stronger LLMs to follow the structured quasi-symbolic process in-context (smaller models often fail to benefit from QuaSAR ICL without SFT); the annotation pipeline relies on a high-quality annotator (GPT-4o) and filters out ~50% of generated demos; ablation/error analysis reports a total pipeline failure rate of 36% on an ablation subset, indicating error propagation across steps; not a fully symbolic prover—still susceptible to mistakes in individual steps and requires manual/heuristic filtering of demonstrations.",
            "uuid": "e6830.0",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that elicits multi-step natural-language rationales from LLMs to improve performance on complex reasoning tasks by decomposing problems into intermediate steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Generic prompting approach applied to various transformer LLMs (used as baseline in this paper)",
            "model_description": "Prompting technique rather than a model architecture; typically used with autoregressive transformer LLMs to elicit stepwise natural language reasoning traces.",
            "model_size": null,
            "architecture_type": "Transformer with natural-language CoT prompting",
            "training_data": null,
            "reasoning_method": "Natural-language chain-of-thought: free-form stepwise rationales, sometimes augmented with self-consistency or majority voting.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Same benchmarks as QuaSAR in this paper (GSM8K, AQuA, SVAMP, MMLU-Redux, etc.)",
            "benchmark_description": "See QuaSAR entry.",
            "task_type": "Mathematical word problems, multi-step reasoning, natural language inference",
            "performance_metric": "Exact-match / accuracy",
            "performance_value": "Serves as baseline; QuaSAR reports an average improvement of ~8 percentage points over CoT (aggregate claim) and per-task/Table comparisons show CoT being outperformed by QuaSAR across evaluated models.",
            "comparison_with_baseline": "CoT is the main baseline; QuaSAR consistently outperforms CoT in the paper's experiments (both in-context on large models and as a source for fine-tuning demonstrations).",
            "key_findings": "CoT substantially improves reasoning compared to zero-shot, but CoT explanations are susceptible to content bias and unfaithful traces; semi-formal quasi-symbolic structuring can mitigate some of these weaknesses.",
            "limitations": "Content biases and unfaithful/explanatory errors that harm robustness and verifiability; sensitive to superficial perturbations; less structured than quasi-symbolic approaches, making systematic verification harder.",
            "uuid": "e6830.1",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Faithful CoT",
            "name_full": "Faithful Chain-of-Thought",
            "brief_description": "A method aiming to produce more faithful and verifiable chain-of-thought rationales, cited and compared as a prior approach that attempts to reduce hallucinations/unfaithful steps in CoT.",
            "citation_title": "Faithful chain-of-thought reasoning",
            "mention_or_use": "mention",
            "model_name": "Approach applied to LLMs in prior work (Lyu et al., 2023); used as a comparative baseline in experiments",
            "model_description": "An approach that modifies CoT prompting/training to enforce faithfulness and alignment between rationale and output (paper cites it as a baseline but does not re-implement the internals).",
            "model_size": null,
            "architecture_type": "Transformer + enhanced CoT prompting/training for faithfulness",
            "training_data": null,
            "reasoning_method": "Faithful Chain-of-Thought: aims to ensure rationales are causally linked to answers, often via constrained prompting/training and verification steps.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Used as comparator on same benchmarks (reported comparisons vs QuaSAR on GPT-4o)",
            "benchmark_description": "See QuaSAR entry.",
            "task_type": "Multi-step reasoning and math/NLU benchmarks",
            "performance_metric": "Accuracy / exact-match",
            "performance_value": "Paper reports QuaSAR improves by ~8.2 percentage points over Faithful CoT when applied on GPT-4o (aggregate claim).",
            "comparison_with_baseline": "Compared in tables; QuaSAR reported to outperform Faithful CoT on evaluated tasks.",
            "key_findings": "Faithful CoT addresses faithfulness but QuaSAR's semi-formal symbolic structuring yields higher accuracy and robustness in the paper's experiments.",
            "limitations": "Mentioned limitations: previous faithful methods still rely on natural-language rationales and can be brittle; full symbolic formalisation approaches carry additional translation costs (context in related work).",
            "uuid": "e6830.2",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CoMAT",
            "name_full": "Chain of Mathematically Annotated Thought (CoMAT)",
            "brief_description": "A prior technique that augments chain-of-thought with mathematical annotations to improve math reasoning; used as a baseline in the paper's comparisons.",
            "citation_title": "Comat: Chain of mathematically annotated thought improves mathematical reasoning",
            "mention_or_use": "mention",
            "model_name": "Approach applied to transformer LLMs (cited baseline)",
            "model_description": "CoT variant that structurally augments mathematical reasoning steps with formal mathematical annotations to enhance correctness on math tasks.",
            "model_size": null,
            "architecture_type": "Transformer + mathematical annotation in CoT",
            "training_data": null,
            "reasoning_method": "Augmented chain-of-thought with mathematical annotations/structures to support symbolic calculation steps.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Math benchmarks (e.g., GSM8K) and other tasks used for comparison in paper",
            "benchmark_description": "See QuaSAR entry.",
            "task_type": "Mathematical word problems and symbolic reasoning",
            "performance_metric": "Accuracy / exact-match",
            "performance_value": "Paper reports QuaSAR improves by ~6.8 percentage points over CoMAT on GPT-4o (aggregate claim).",
            "comparison_with_baseline": "CoMAT improves math reasoning, but QuaSAR yields higher accuracy and robustness across tasks in the authors' experiments.",
            "key_findings": "CoMAT is strong for math but QuaSAR's semi-formal abstraction and consistent four-step pipeline further improves results and robustness in the reported experiments.",
            "limitations": "CoMAT and similar formal-annotation approaches can still be task-specific and may require tighter formalisation for some math domains; the paper positions QuaSAR as more flexible and efficient.",
            "uuid": "e6830.3",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "FLARE",
            "name_full": "FLARE (Faithful logic-aided reasoning and exploration)",
            "brief_description": "Prior work that integrates logical structure into reasoning and exploration to improve faithfulness; cited as a comparative method in the related work and experiments.",
            "citation_title": "Flare: Faithful logic-aided reasoning and exploration",
            "mention_or_use": "mention",
            "model_name": "Approach applied to transformer LLMs (cited baseline)",
            "model_description": "A logic-aided framework that injects logical constraints or structure into LLM reasoning to improve faithfulness and exploration.",
            "model_size": null,
            "architecture_type": "Transformer + logic-aided components (method-level augmentation)",
            "training_data": null,
            "reasoning_method": "Logic-aided CoT with mechanisms to keep reasoning faithful (likely external checks or structured constraints per citation).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Used as comparator on similar benchmarking suites in the paper",
            "benchmark_description": "See QuaSAR entry.",
            "task_type": "Logical and mathematical reasoning tasks",
            "performance_metric": "Accuracy / exact-match",
            "performance_value": "Paper reports QuaSAR outperforms FLARE in their comparative evaluations (exact per-task numbers reported in tables).",
            "comparison_with_baseline": "FLARE improves faithfulness, but QuaSAR is reported to achieve higher overall accuracy and robustness across the tested benchmarks.",
            "key_findings": "FLARE is a competitive faithful-reasoning approach; QuaSAR's quasi-symbolic abstraction offers a different trade-off (less reliance on external tools, more flexible semi-formalisation) and outperformed FLARE in reported experiments.",
            "limitations": "FLARE (like other faithful approaches) depends on logic aids that may be costly or less flexible across heterogeneous tasks.",
            "uuid": "e6830.4",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Hybrid LLM + Symbolic Solvers",
            "name_full": "Hybrid frameworks integrating LLMs with external symbolic reasoning tools",
            "brief_description": "A class of approaches where LLMs map natural language to formal/symbolic representations that are then processed by external theorem provers, SAT solvers, or deterministic symbolic engines to improve logical fidelity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Varies (e.g., Logic-LM, SatLM, other LLM+solver frameworks referenced in related work)",
            "model_description": "Hybrid pipelines: LLM transforms NL into symbolic programs/formulas which are executed or verified by symbolic solvers (theorem provers, SAT/SMT solvers, domain-specific deterministic engines).",
            "model_size": null,
            "architecture_type": "Transformer + external symbolic solver (neuro-symbolic pipeline)",
            "training_data": "Often formal corpora or parallel NL→formalization datasets used in referenced works (paper cites these approaches but does not retrain them).",
            "reasoning_method": "Full symbolic formalisation + external solver verification/execution (program synthesis / theorem proving pipelines).",
            "external_tool_used": true,
            "external_tool_description": "External symbolic engines/theorem provers/SAT-style solvers used to deterministically check or compute logical steps after LLM-driven formalisation (paper references works such as Logic-LM, SatLM, and verification/refinement by Quan et al.).",
            "benchmark_name": "Used in prior work on logical reasoning and verification; referenced as alternative to QuaSAR (no single benchmark specified within paper for these systems).",
            "benchmark_description": "Formal reasoning and symbolic-program evaluation tasks; these pipelines are often applied to benchmarks requiring exact formal reasoning.",
            "task_type": "Formal theorem proving, program execution, symbolic deduction, verified calculation",
            "performance_metric": "Varies by prior work (proof success, exactness, verified correctness); paper does not re-evaluate these methods directly.",
            "performance_value": null,
            "comparison_with_baseline": "Paper positions hybrid approaches as improving fidelity but suffering from a bottleneck: need complete NL→symbolic translation which harms efficiency and flexibility; QuaSAR aims a trade-off by partial/semi-formal translation inside NL rationales.",
            "key_findings": "Hybrid LLM+symbolic approaches improve verifiability but incur translation bottlenecks and reduced flexibility; QuaSAR is offered as an intermediate approach that leverages selective formalisation without full translation and without external solvers.",
            "limitations": "Require complete and accurate NL→formal translation (difficult in noisy NL), depend on external tools and verifiers, can be computationally costly and less flexible across varied natural-language problem styles.",
            "uuid": "e6830.5",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A high-capability large language model used as both an evaluator/judge and as the primary annotator to generate QuaSAR demonstrations; used for in-context evaluation and to generate synthetic training data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI's GPT-4 variant (proprietary autoregressive transformer model); used via API for generation, in-context prompting, and sometimes as a judge (GPT-4o-mini used as a judge).",
            "model_size": null,
            "architecture_type": "Proprietary large transformer (autoregressive); used with chain-of-thought and QuaSAR prompting",
            "training_data": null,
            "reasoning_method": "Prompted with CoT and QuaSAR templates; used to generate stepwise quasi-symbolic demonstrations and produce final answers.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Evaluated on GSM8K, AQuA, SVAMP, MMLU-Redux, OlympiadBench, GPQA, DROP, GSM-Symbolic",
            "benchmark_description": "As described in QuaSAR entry; used both to run experiments and to generate training demonstrations.",
            "task_type": "Mathematical word problems, symbolic robustness tests, NLU multiple-choice and reading-comprehension reasoning",
            "performance_metric": "Exact-match / accuracy; self-correction evaluation and robustness (variation under perturbations)",
            "performance_value": "As ICL, QuaSAR on GPT-4o produced reported gains such as +19.1% over zero-shot baseline and ~+8% over CoT on aggregated tasks; specific table entries in the paper show improvements across symbolic and natural language tasks (e.g., QuaSAR 87.4 vs CoT ~84.3 in aggregated row for GPT-4o in Table 1).",
            "comparison_with_baseline": "GPT-4o with QuaSAR outperforms GPT-4o with standard CoT, Faithful CoT, CoMAT and FLARE in the paper's reported experiments.",
            "key_findings": "High-capacity LLMs like GPT-4o can effectively follow QuaSAR in-context and generate high-quality quasi-symbolic demonstrations; these demonstrations in turn improve smaller models when used for SFT.",
            "limitations": "Proprietary model with unspecified parameter count; the paper relies on GPT-4o to produce demonstrations requiring filtering to remove incorrect outputs (~50% filtered out), indicating that even powerful LLMs produce noisy generation requiring quality control.",
            "uuid": "e6830.6",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama-3-70B",
            "name_full": "Llama-3 70B Instruct",
            "brief_description": "A large open (Meta) LLaMA-3 family model (70B) evaluated in the paper; capable of benefiting from QuaSAR prompting but with smaller absolute gains than GPT-4o in some settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B (meta-llama/Meta-Llama-3-70B-Instruct)",
            "model_description": "70 billion parameter LLaMA-3 family instruction-tuned transformer model (public release referenced in paper), evaluated with CoT and QuaSAR prompting and as a fine-tuning target.",
            "model_size": "70B",
            "architecture_type": "Transformer (Llama-3 family)",
            "training_data": null,
            "reasoning_method": "Evaluated with CoT and QuaSAR prompting/finetuning; SFT experiments using QuaSAR-generated demonstrations.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GSM8K, AQuA, SVAMP, MMLU-Redux, OlympiadBench, GPQA, DROP",
            "benchmark_description": "See QuaSAR entry.",
            "task_type": "Mathematical/symbolic reasoning and language understanding tasks",
            "performance_metric": "Accuracy / exact-match",
            "performance_value": "Paper reports as ICL QuaSAR yielded +11.8% improvement for Llama-3-70B over baseline; tables show QuaSAR improves over CoT for this model family (per-table numbers reported).",
            "comparison_with_baseline": "QuaSAR ICL and SFT using QuaSAR demonstrations improved performance compared to CoT and baseline SFT; however smaller Llama variants require tuning to fully leverage QuaSAR.",
            "key_findings": "Large open Llama variants gain from QuaSAR prompting and from QuaSAR-based SFT; formalisation and explanation steps are particularly important for math tasks.",
            "limitations": "Smaller Llama variants (8B, 1B) often cannot follow QuaSAR in-context without SFT; generation temperature differences (paper used τ=0.5 for Llama) and compute constraints are discussed as practical limitations.",
            "uuid": "e6830.7",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Qwen2-72B",
            "name_full": "Qwen2 72B Instruct",
            "brief_description": "A 72B-parameter Qwen2 family model evaluated in the paper; used to compare QuaSAR against CoT and other baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2-72B (Qwen/Qwen2-72B-Instruct)",
            "model_description": "72B-parameter autoregressive transformer model from the Qwen2 family; used with CoT and QuaSAR prompting in evaluations.",
            "model_size": "72B",
            "architecture_type": "Transformer (Qwen family)",
            "training_data": null,
            "reasoning_method": "Evaluated with CoT and QuaSAR prompting and with QuaSAR-generated SFT data for smaller Qwen variants.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Same benchmark suite as used elsewhere in paper",
            "benchmark_description": "See QuaSAR entry.",
            "task_type": "Mathematical and NLU reasoning tasks",
            "performance_metric": "Accuracy / exact-match",
            "performance_value": "Reported QuaSAR ICL gains include +17.2% for Qwen2-72B over baseline zero-shot in the paper's experiments; QuaSAR outperforms CoT on aggregate scores reported.",
            "comparison_with_baseline": "QuaSAR yields larger absolute improvements on high-capacity models like Qwen2-72B when used as ICL; QuaSAR-based SFT also improves smaller Qwen variants.",
            "key_findings": "High-capacity Qwen models benefit substantially from QuaSAR prompting; QuaSAR is effective across different LLM families.",
            "limitations": "Smaller Qwen models need fine-tuning with QuaSAR demonstrations to reap benefits; paper notes dependence on careful demonstration filtering and tuning hyperparameters.",
            "uuid": "e6830.8",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GSM-Symbolic",
            "name_full": "GSM-Symbolic (perturbed/robustness version of GSM8K)",
            "brief_description": "A variant/extension of GSM8K designed to test LLM robustness to superficial symbolic/lexical perturbations and to expose limitations in mathematical reasoning under shallow variations.",
            "citation_title": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Benchmark (evaluated against LLMs)",
            "model_description": "Perturbed/robustness version of the GSM8K math word-problem dataset that introduces symbolic or superficial variations to test brittleness of LLM reasoning.",
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "GSM-Symbolic",
            "benchmark_description": "Mathematical word problems with symbolic/lexical perturbations to evaluate robustness of reasoning solutions.",
            "task_type": "Symbolic mathematical reasoning and robustness testing",
            "performance_metric": "Accuracy / exact-match and variation under perturbations",
            "performance_value": "Paper reports QuaSAR achieves more stable performance (less variation) than CoT on GSM-Symbolic and that QuaSAR-tuned models show improved accuracy on GSM-Symbolic (Figure/Table references in paper).",
            "comparison_with_baseline": "QuaSAR shows substantially less performance variation under perturbations compared to CoT; trained models tuned with QuaSAR demonstrations outperform those tuned with CoT on GSM-Symbolic in the paper's plots.",
            "key_findings": "Quasi-symbolic abstractions help disentangle content from logical structure and improve robustness to superficial problem variations in symbolic math tasks.",
            "limitations": "Even with QuaSAR, there are nontrivial error rates and error propagation across QuaSAR steps (paper reports total pipeline failure rate 36% on an ablation subset using GPT-4o).",
            "uuid": "e6830.9",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MMLU-Redux",
            "name_full": "MMLU-Redux",
            "brief_description": "A modified/Redux version of the MMLU benchmark used to evaluate natural language multi-choice reasoning and robustness (e.g., answer-order swaps) in the paper's experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Benchmark (evaluated across models)",
            "model_description": "A reworked slice of MMLU that focuses on robustness to superficial variations such as randomizing choice order; used in the paper to test QuaSAR vs CoT stability.",
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "MMLU-Redux",
            "benchmark_description": "Multiple-choice multi-domain reasoning benchmark with robustness-focused perturbations (e.g., answer-order swaps).",
            "task_type": "Multiple-choice natural-language reasoning",
            "performance_metric": "Accuracy; variation under choice-order perturbations",
            "performance_value": "Paper reports that QuaSAR consistently achieves the same performances with considerably less variation than CoT on MMLU-Redux (Table 4 summarizes robustness gains).",
            "comparison_with_baseline": "QuaSAR shows improved robustness to choice-order swaps compared to CoT and other baselines.",
            "key_findings": "Structured quasi-symbolic reasoning reduces sensitivity to superficial perturbations in multiple-choice reasoning.",
            "limitations": "Paper does not claim QuaSAR fully solves all MMLU failure modes; smaller models especially show limited gains when used as pure ICL without SFT.",
            "uuid": "e6830.10",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GSM8K",
            "name_full": "GSM8K",
            "brief_description": "A popular grade-school math word-problem dataset used as a core symbolic/mathematical benchmark in the paper's evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Benchmark (evaluated across models)",
            "model_description": "Dataset of arithmetic/multi-step math word problems commonly used to evaluate LLM mathematical reasoning.",
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "GSM8K",
            "benchmark_description": "Math word-problem dataset for multi-step arithmetic reasoning.",
            "task_type": "Mathematical word-problem solving (symbolic arithmetic and reasoning)",
            "performance_metric": "Exact-match (string match) / accuracy",
            "performance_value": "QuaSAR reported to improve exact-match accuracy on GSM8K compared to CoT in the paper's experiments (per-table numbers show improvements for GPT-4o and other models; example: QuaSAR on GPT-4o shows higher GSM8K accuracy versus CoT in Table 1).",
            "comparison_with_baseline": "QuaSAR outperforms CoT and other baselines on GSM8K in the reported experiments, especially when formalisation and explanation steps are retained.",
            "key_findings": "Semi-formal symbolic translation (formalisation step) is particularly important for mathematical subsets (ablation removed step 2 or 3 causes &gt;3.5 point drops on GSM8K/AQuA).",
            "limitations": "Mathematical tasks still show sensitivity to the correctness of intermediate steps; QuaSAR improves but does not entirely eliminate errors (error propagation remains a concern).",
            "uuid": "e6830.11",
            "source_info": {
                "paper_title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 2,
            "sanitized_title": "faithful_chainofthought_reasoning"
        },
        {
            "paper_title": "Comat: Chain of mathematically annotated thought improves mathematical reasoning",
            "rating": 2,
            "sanitized_title": "comat_chain_of_mathematically_annotated_thought_improves_mathematical_reasoning"
        },
        {
            "paper_title": "Flare: Faithful logic-aided reasoning and exploration",
            "rating": 2,
            "sanitized_title": "flare_faithful_logicaided_reasoning_and_exploration"
        },
        {
            "paper_title": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "gsmsymbolic_understanding_the_limitations_of_mathematical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Verification and refinement of natural language explanations through llm-symbolic theorem proving",
            "rating": 1,
            "sanitized_title": "verification_and_refinement_of_natural_language_explanations_through_llmsymbolic_theorem_proving"
        },
        {
            "paper_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "rating": 1,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        }
    ],
    "cost": 0.02174125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions
3 Sep 2025</p>
<p>Leonardo Ranaldi 
Idiap Research Institute
Switzerland</p>
<p>School of Informatics
University of Edinburgh
UK</p>
<p>Marco Valentino 
Idiap Research Institute
Switzerland</p>
<p>School of Computer Science
University of Sheffield
UK</p>
<p>Department of Computer Science
University of Manchester
UK</p>
<p>National Biomarker Centre (NBC)
CRUK Manchester Institute
UK</p>
<p>André Freitas 
Idiap Research Institute
Switzerland</p>
<p>Department of Computer Science
University of Manchester
UK</p>
<p>National Biomarker Centre (NBC)
CRUK Manchester Institute
UK</p>
<p>Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions
3 Sep 2025DBA11152FF96569FD7AD4CB9C1C03AE4arXiv:2502.12616v2[cs.CL]
Chain-of-Thought (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps.However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness.To mitigate existing limitations, recent work has proposed the use of logical formalisms coupled with external symbolic solvers.However, fully symbolically formalised approaches introduce the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility.To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation.In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations.Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language.We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models.Our experiments show that quasi-symbolic abstractions can improve CoTbased methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e.MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).</p>
<p>Introduction</p>
<p>Multi-step reasoning methods, best exemplified by Chain-of-Thought (Wei et al., 2022;Wang et al., 2022), have been proposed to improve the performance of Large Language Models (LLMs) on downstream tasks by breaking down complex problems into intermediate reasoning steps.The success of these methods is due to the LLMs' properties of performing tasks by following in-context structured requirements (Zhou et al., 2023;Dong et al., 2024;Ranaldi et al., 2024b,a).</p>
<p>Despite CoT being the current workhorse for LLM reasoning, complex reasoning still remains a significant challenge for LLMs (Meadows and Freitas, 2023;Luo et al., 2024), with recent work showing that explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness (Lyu et al., 2023;Turpin et al., 2024;Yee et al., 2024).</p>
<p>To mitigate these limitations and improve reasoning capabilities, recent works have proposed using logical formalisms (Lyu et al., 2023;Jiang et al., 2024a;Arakelyan et al., 2024).However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that negatively impacts efficiency and flexibility (Dinh et al., 2023;Quan et al., 2024b,a;Dalal et al., 2024).</p>
<p>This paper investigates methods to achieve a better trade-off between flexibility and robustness by disentangling content from logical reasoning without the need for a complete formalisation.In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation to CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations.Our framework leverages the capability of LLMs to formalise relevant variables and predicates, enabling the coexistence of symbolic elements with natural language.</p>
<p>Specifically, the aim of QuaSAR is to enable LLMs in tackling complex multi-step reasoning problems via the following steps: (i) Abstraction, where the problem is analysed and abstracted in terms of relevant symbolic predicates, variables, and constants; (ii) Formalisation, where the original problem is reformulated using a combination of a minimal symbolic form and natural language; (iii) Explanation, where the necessary steps to com- Figure 1: QuaSAR elicits quasi-symbolic abstractions in LLMs via the following steps: (i) Abstraction, where the problem is analysed and abstracted in terms of relevant symbolic predicates, variables, and constants; (ii) Formalisation, where the original problem is reformulated using a mixture of symbolic expressions and natural language; (iii) Explanation, where the necessary steps to compute the solution are formulated via quasi-symbolic reasoning chains; and (iv) Answering, where a final solution is generated.We use QuaSAR as an in-context learning strategy and for constructing reasoning demonstrations for smaller LLMs.</p>
<p>pute the solution are formulated via quasi-symbolic reasoning chains; and (iv) Answering, where a final solution is generated.Building on recent work (Lyu et al., 2023;Jiang et al., 2024a), QuaSAR guides LLMs via structured instructions, going beyond the problems associated with using external solvers (Quan et al., 2024b).At the same time, in contrast to work using formal languages to guide CoT reasoning (Leang et al., 2024;Arakelyan et al., 2024), QuaSAR operates via a single prompting step, reducing costs, thereby delivering robust reasoning trajectories across different types of reasoning tasks.</p>
<p>We demonstrate the operability of QuaSAR in two different configurations -as an in-context approach to provide explicit instructions for larger and more capable LLMs and as a strategy for constructing synthetic demonstrations to improve the performance and align the reasoning capabilities of smaller LLMs.Hence, we perform an extensive empirical evaluation using different LLMs (i.e., GPT-4o, Llama3, and Qwen-2) on complex mathematical problems, reasoning, and natural language understanding tasks.QuaSAR demonstrates significant improvements by achieving an overall exact match boost on all proposed tasks.</p>
<p>In particular, our experiments led to the following findings and conclusions:</p>
<ol>
<li>
<p>Formalising and structuring the LLMs' reasoning through quasi-symbolic trajectories enhances accuracy and verifiability, leading to an average increase in accuracy of 8% over CoT and 6.8% and 8.2% over CoMAT (Leang et al., 2024) and Faithful CoT (Lyu et al., 2023) respectively when applied on GPT-4o.</p>
</li>
<li>
<p>We found that our symbolic-inspired approach is significantly more efficient than related methods and can be employed on different tasks (i.e., mathematical and natural language reasoning tasks) without significant changes.Indeed, QuaSAR achieves state-of-the-art performance across diverse tasks of varying complexity and languages operating through the same framework.</p>
</li>
<li>
<p>By conducting an in-depth ablation study, we demonstrate the generalisability of QuaSAR and its effectiveness on different scales of LLMs.Our experiments show that QuaSAR provides more robust reasoning trajectories on tasks that are typically challenging for smallerscale models, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e.MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic)</p>
</li>
</ol>
<p>To the best of our knowledge, QuaSAR is the first method to apply quasi-symbolic demonstrations for a broad spectrum of reasoning tasks, demonstrating the impact of enabling the co-existence of symbolic abstractions and natural language explanations for improving the efficiency and robustness of LLMs.</p>
<p>2 QuaSAR: Quasi-Symbolic Abstract Reasoning</p>
<p>Integrating symbolic elements into natural language explanations is crucial for reasoning in disciplines such as mathematics and science, where symbolic abstractions facilitate the identification and generalisation of the logical connections between premises (i.e., explanans) and conclusions (i.e., explanandum) (Wang, 1954;Bronkhorst et al., 2019;Pennington and Hastie, 1993;Valentino and Freitas, 2024;Miller, 2019).For example, within the unificationist account of explanation, Kitcher (1981) posits that explanations function by subsuming an apparently disconnected set of observations under the same underlying regularity, thereby forming recurring argument patterns.These patterns emerge when explanations are generalised through the replacement of concrete entities and predicates with abstract symbols.This process of quasi-symbolic abstraction enables explanatory arguments to be detached from specific world knowledge, thereby allowing their applicability across different problems (e.g., the same argument pattern created by the theory of gravity can be used to explain why specific objects fall and why celestial objects attract each other) (Valentino et al., 2021(Valentino et al., , 2022a,b;,b;Zheng et al., 2024).</p>
<p>In this paper, we aim to explicitly leverage argument patterns with LLMs, hypothesising that quasi-symbolic abstractions can help disentangle concrete world knowledge from symbolic reasoning within a natural language explanatory framework and mitigate some of the challenges related to content effect.An example of this process is illustrated in Figure 1.</p>
<p>Formally, conventional in-context reasoning methods are structured as a triplet (Q, R, A), where Q represents the question, R consists of in-context multi-step reasoning explanations (expressed in natural language or a related form), and A denotes the final answer.We extend this formalism by instructing the LLM to operate via explicit symbolic transformations as a core component of the reasoning process.Our framework, QuaSAR, structures the solution process as a quadruple (Q, S, R, A), where S = (s 1 , s 2 , s 3 , s 4 ) represents a chain of instructions that guide the models to formalise relevant parts of the reasoning process.Each step s i corresponds to a structured transformation aimed at decomposing the problem into a sequence of symbolically-elicited operations.This structured decomposition enhances transparency and facilitates systematic verification of each step.</p>
<p>QuaSAR's Reasoning Process</p>
<p>A complex problem solution could be described by a sequence of inference steps determined by identifying and isolating the problem predicates and structuring a formalisation that facilitates reasoning to reach the final solution.Accordingly, QuaSAR operates using four steps that aim to improve the accuracy of the reasoning trajectory in LLMs: (i) Abstraction, where the problem is analysed and abstracted in terms of relevant symbolic predicates, variables, and constants; (ii) Formalisation, where the original problem is reformulated using a mixture of symbols and natural language; (iii) Explanation ( §2.1.3),where the transformations are solved using quasi-symbolic representations that explicitly explain the solution; and (iv) Answering ( §2.1.4),where a final solution is generated to address the problem.Appendix A reports QuaSAR prompt.</p>
<p>Abstraction</p>
<p>Abstracting the problem through the identification of relevant information is the first step in solving complex tasks and is a fundamental stage in structuring a robust formalisation (Bronkhorst et al., 2019).Therefore, as a first step, QuaSAR instructs the LLM to exemplify predicates, variables, and constants, whether of numerical or verbal types.</p>
<p>Formalisation</p>
<p>The crucial step of QuaSAR is the formalisation of the problem, which aids accurate reasoning by translating natural language into a semi-structured symbolic form.Hence, we instruct the LLM to deliver a quasi-formal representation of the problem, which is originally in natural language, using a structural-logical translation that explicitly represents the facts of the problem.This step is the basis for constructing an accurate reasoning trajectory because translating concrete terms in natural language into symbols aims to minimise ambiguities and content effects without compromising the components that may be significant for solving the problem.</p>
<p>Explanation</p>
<p>A significant component of CoT reasoning methods is breaking down the problem into a sequence of steps to arrive at the final solution.The explanation phase is based on step-by-step reasoning (Kojima et al., 2022) explicitly prompting the model or delivering in-context demonstrations, generally natural language rationales.Then, the LLM is expected to solve the problem by providing a logical explanation that motivates the steps to the solution.In QuaSAR, the reasoning trajectory is based on the symbolic structure.In this way QuaSAR aims to elicit logical connections between each step, reducing the risk of errors caused by contextual knowledge or implicit symbolic-logical relations.The solution is then generated based on this quasi-symbolic reasoning process, which, although similar to the breakdown of reasoning methods, has a semi-structured formalisation standing behind it.</p>
<p>Answering</p>
<p>QuaSAR brings the reasoning trajectory to a final stage in which the LLM is instructed through a specific pattern -i.e., "The answer is: [number]".</p>
<p>Although not fundamental, this stage is significant as it ensures that the reasoning constructed in the previous stages has a conclusion.Furthermore, this step facilitates the evaluation as it triggers the LLM to deliver a response that follows the pattern of the evaluation task.</p>
<p>QuaSAR Application</p>
<p>QuaSAR leverages a set of structured instructions to deliver step-wise explanations.Thus, the operability of QuaSAR is two-fold, as it can be used as both an in-context learning strategy and as a synthetic annotation method to support supervised learning (both described below).</p>
<p>QuaSAR for In-Context Learning</p>
<p>Using the step described in §2.1, we adopt QuaSAR to instruct three LLMs (i.e., GPT-4o, Llama-3-70B, and Qwen2-72B).Specifically, we instruct the models to exemplify and abstract the most important information from the given problem, formalising and translating natural language in a semistructured logical form, explaining the solution in a step-wise manner, and finally generating the conclusive short-form answer in a strict format to have a more detailed and strict downstream evaluation.However, although the sequence of instructions is well-structured and defined, the ability to perform sequential and complex reasoning tasks is limited to larger LLMs (such as GPT-4-o, as discussed in the experiments).Hence, we transfer these capabilities to smaller models operating via QuaSAR for building synthetic reasoning demonstrations as training sets.</p>
<p>QuaSAR for Reasoning Demonstrations</p>
<p>We instruct smaller models via demonstrations produced by high-performing LLMs capable of following structured instructions.To filter for the quality of generated demonstrations, we follow the method proposed by Ranaldi et al. (2025b), which computes the citation precision for the considered documents as a proxy for the quality of the demonstrations.However, since QuaSAR employs a different annotation mechanism, our heuristics firstly filter out the final correct answers through a strict, exact match; then, behind the filtering (cutting off about 50% of the demonstrations), it verifies that each retrieved document along the reference evidence has been considered (a detailed description of the annotation phase is in Appendix B).</p>
<p>Training</p>
<p>We train a Language Model θ using the annotations1 generated via QuaSAR.The annotations are augmented with reasoning demonstrations α using the standard language modelling objective, maximising likelihood:
max θ E (q,α,y)∼D log p θ (Y | α, Q)p θ (α | Q) (1) where α = α 1 • α 2 • α 3 • α 4
is the combination of the step-wise reasoning trajectory delivered by the model, "•" is the concatenation operator, and α 1 , α 2 , are the respective annotations generated by the above processes.Finally, Q is the provided question, and Y is the answer, including the intermediate steps and the final answer.D is the training corpus constructed using training demonstrations.</p>
<p>Experiments</p>
<p>We evaluate QuaSAR on complex mathematical problems, commonsense reasoning, and natural language understanding tasks ( §3.1).We perform the evaluation phases by following standard approaches used to assess question-answering tasks ( §3.2) on models presented in §3.3.</p>
<p>Tasks &amp; Datasets</p>
<p>We evaluate the operability of QuaSAR on tasks involving complex reasoning and natural language inference.These tasks are best exemplified by the following categories: Symbolic Tasks We use GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), AQuA (Ling et al., 2017), MMLU-Redux (Gema et al., 2024) and Olympiad Bench (He et al., 2024) covering various mathematical topics, including abstract algebra, elementary, college-level and highschool mathematics.These datasets include multiple-choice questions (AQUA, MMLU-Redux) and math-world problems (GSM8K, MSVAMP, Olympiad Bench).</p>
<p>Natural Language Tasks We use Graduate-Level Google-Proof Q&amp;A Benchmark (GPQA) (Rein et al., 2023) and Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019).GPQA</p>
<p>presents complex, open-ended questions that resist specific searches and require models to synthesise knowledge across multiple sources or reason critically to generate answers.DROP focuses on questions requiring discrete reasoning, such as arithmetic operations, logical comparisons, or event tracking, requiring the model to extract and manipulate information from a given passage.</p>
<p>Evaluation Metrics</p>
<p>We used exact-match for the multiple-choice question-answering task, requiring the predicted answer to match the correct one.This guarantees evaluation based on complete responses, addressing clarity concerns in tasks like MMLU-Redux.For string-matching answers, we used exact matches in GSM8K.Moreover, to have a comprehensive evaluation, we use GPT-4o-mini as a benchmark to evaluate how well the model's answers aligned with the ground truth.Details are described in Appendix C.</p>
<p>Models</p>
<p>Experiments were performed on GPT-4o (Achiam et al., 2023), Qwen2 (Yang et al., 2024) and Llama-3 (et al., 2024).While we selected the first two models to allow for a detailed comparison with related work and CoT frameworks, Llama-3 was chosen for its adaptability and the presence of releases with a small number of parameters that allow for additional tuning steps.</p>
<p>Baselines We compared QuaSAR against two baselines using the same greedy decoding strategy, fixing the temperature to 0. The baselines include: (1) standard zero-shot prompting, (2) CoT prompting (Kojima et al., 2022).Moreover, we include Faithful CoT (Lyu et al., 2023), FLAIRE (Arakelyan et al., 2024), andCoMAT (Leang et al., 2024) for additional comparison.</p>
<p>QuaSAR Application We use QuaSAR as ICL and for generating tuning demonstrations.In both configurations, we instruct the models via the prompt in Appendix A. We conduct instructiontuning of the models using the demonstrations described in Appendix B and the configurations in Appendix F.</p>
<p>Results &amp; Discussions</p>
<p>The results in Tables 1 and 2   requiring advanced mathematical reasoning (Symbolic task), reading comprehension and logical reasoning (Natural Language task).In particular, two different results emerge in the application of QuaSAR: when it is employed as in-context learning strategy in higher-scale models, it consistently outperforms other strategies; when QuaSAR is employed in smaller-scale models, it does not obtain the same benefits, as discussed in §4.1.On the other hand, when QuaSAR is used as an annotation strategy for delivering demonstrations operated to refine smaller-scale models, the performances are significantly higher compared to the models instructed via standard CoT demonstrations, as examined in §4.2.Overall, our experiments demonstrate the benefit of quasi-symbolic abstractions for complex reasoning tasks, and provide evidence of improved robustness on challenging adversarial variations ( §4.3).</p>
<p>QuaSAR as In-Context Learning Strategy</p>
<p>Table 1 reports the results of QuaSAR when adopted as an In-Context Learning (ICL) strategy.We observe general robust improvement over the baseline models (with an improvement of 19.1% for GPT-4o, 11.8% for Llama-3-70B and 17.2% for Qwen2-72B); the results show that the role of QuaSAR as ICL is foremost noticeable for higherscale LLMs.QuaSAR consistently outperforms CoT, Faithful CoT and CoMAT.QuaSAR also delivers overall improvements on smaller-scale models.</p>
<p>Table 2 shows an improvement over the baseline of 5.2% for Llama-3-8B, 13.4% for Llama-3-1B, 10.5% for Qwen2-7B and 8.3% for Qwen2-1.5B.However, comparing QuaSAR to CoT on smaller models, we observe a decrease in performance, indicating that such models fail to follow the quasisymbolic reasoning process induced by QuaSAR.</p>
<p>QuaSAR as Annotation Strategy</p>
<p>Table 2 (values between the brackets and detailed in Appendix K) reports the results of QuaSAR when adopted as annotation strategy for different models.</p>
<p>From the results, it clearly emerges that QuaSAR is consistently effective in enhancing the performance of Llama and Qwen2 models when used to generate reasoning demonstrations via GPT-4o.In particular, we found that QuaSAR outperforms other tuning approaches, including baseline SFT on target answers and SFT on demonstrations delivered via CoT.</p>
<p>The impact of QuaSAR</p>
<p>The step-wise reasoning chain generations elicited by QuaSAR have an optimal impact on the downstream performances when QuaSAR is used as ICL and for generating demonstrations.</p>
<p>Step-wise roles for ICL Table 3 displays the difference in accuracy compared to QuaSAR with all steps.We show that each step impacts QuaSAR's operability.In particular, eliminating step 1 (i.e., w/o(1)) affects the final accuracies (-1.8 on average).This suggests that the initial abstraction step AQuA -1.9 -3.6 -3.7 -2.9 -3.7 -2.7 GSM8K -2.1 -4.2 -3.9 -1.3 -3.5 -2.6 MMLU-R -0.7 -3.0 -3.2 -3.2 -2.2 -2.8 OlyBench -1.9 -3.1 -3.7 -2.1 -3.8 -2.3 GPTQ -1.6 -3.9 -3.6 -3.2 -4.1 -2.9 Avg -1.8 -3.5 -3.4 -2.5 -3.2 -2.8 is important for final performance but is not decisive, especially in tasks such as GPTQ and MMLU-Redux.In contrast, steps 2 (i.e., formalisation) and 3 (i.e., explanation), play a crucial role, indeed, a stable drop of more than 3.5 points can be observed.In this case, the tasks that suffer the most are the mathematical subset (AQuA and GSM8K).</p>
<p>Step 4, reserved for the strict generation of the final answer, is more decisive in multiple-choice than in mathematical tasks.Finally, by eliminating pairs of steps (i.e.w/o(1-2) and w/o(3-4)), it can be seen that there are significant drops in both mathematical tasks (see GSM8K, AQuA and OlympicBench) and language-related reasoning tasks (see MMLU-Redux and GPTQ).The combination of the four steps from these results positively impacts reasoning capabilities, and they all contribute significantly to the final performances.</p>
<p>Step-wise role for Annotations Figure 2 displays the difference in accuracy using entire QuaSAR for generating demonstrations.As in the case of ICL, the steps in the demonstrations have specific importance for instructing models.Indeed, it can be observed that the instructed models perform worse by eliminating central steps such as</p>
<p>Step 2 and Step 3. In contrast, removing step 4 duplicated to the response has moderate adverse effects (performance drop of no more than two points).Finally, it can be seen that the order in which the steps are delivered in the demonstrations also has a positive impact.Delivering the demonstrations randomly shuffled negatively impacts performances, dropping around 4 average points.</p>
<p>Robustness &amp; Ablation Analysis</p>
<p>In-context Robustness To assess the robustness of QuaSAR as an ICL strategy, we evaluated two different phenomena: (i) the order swapping of choices in MMLU-Redux (Gema et al., 2024) as   proposed by Leang et al. ( 2024) and (ii) the performance on a more complex version of GSM8K designed to test robustness to superficial variations (i.e., GSM-Symbolic (Mirzadeh et al., 2024)).Table 4 shows that QuaSAR consistently achieves the same performances with considerably less variation than CoT.This indicates the positive impact of quasi-symbolic abstractions on the robustness of the models.</p>
<p>Training Efficiency Figure 3 shows the performance of tuned models on GSM-Symbolic (Mirzadeh et al., 2024)   Additional Analysis Finally, we produced further analyses to investigate the error flow, the degree of self-correction and the transferability of the approach.In the first analysis, we showed the error rate of our QuaSAR.In Appendix J, we present a particular analysis to investigate the error rate of each step, arguing for its validity.</p>
<p>In the second analysis, we studied the selfcorrection capability of the models trained and tuned via QuaSAR and via CoT.Appedinx L reports the results of the self-assessment on incorrect generations, showing that the outputs generated via QuaSAR are actually easier to correct, as the output rationale, being structured, is simpler to correct.</p>
<p>In the final analysis, reported in Appendix E, we showed the elasticity and adaptability of QuaSAR in tasks different from those proposed in the main analysis, confirming the results obtained and discussed in the previous sections.</p>
<p>Related Work</p>
<p>Logical Reasoning Logical reasoning tasks require the capability to process complex logical structures (Cummins et al., 1991).Traditional methodologies contain rule-based systems (Robinson, 1965) and neural network-based paradigms (Amayuelas et al., 2022;Gerasimova et al., 2023) for solving and manipulating symbolic representations.Recent advancements introduced hybrid frameworks (Pan et al., 2023;Ye et al., 2024;Jiang et al., 2024a), which integrate large language models (LLMs) into symbolic reasoning pipelines (Quan et al., 2024b).These frameworks operate LLMs to map natural language inputs into symbolic syntax, subsequently processed by external reasoning tools.This integration improves reasoning performance through techniques such as selfconsistency (Wang et al., 2023;Zhang et al., 2022).Nevertheless, these frameworks commonly depend on external tools predicated on the assumption that LLMs lack the reliability to parse symbolic expressions with the precision of rule-based reasoning systems alone.</p>
<p>Symbolic Reasoning Symbolic reasoning integrates natural language (NL) and symbolic language (SL) to decompose complex queries into sub-problems solved by SL programs and deterministic solvers, ensuring interpretability and precision (Lyu et al., 2023).Recent efforts have leveraged LLMs to decrease dependence on SL programs (Xu et al., 2024), but these approaches primarily address logical reasoning and depend on verifiers for accuracy, limiting their applicability to complex mathematical tasks.</p>
<p>On the other side, Chain-of-Thought (CoT) strategies have demonstrated significant performance improvements in mathematical symbolic reasoning (Jiang et al., 2024c), reinforced by advancements in problem understanding (Zhong et al., 2024), structured formats (Tam et al., 2024), and supervision models (Ranaldi and Freitas, 2024a,b;Jiang et al., 2024b).Further, premise selection and symbolic frameworks have facilitated systematic evaluations across logical and mathematical reasoning (Meadows et al., 2023;Ferreira and Freitas, 2020).</p>
<p>Future Works</p>
<p>In future developments, we plan to extend our contribution to non-English languages to broaden the beneficial impacts and operability of reasoning for multilingual alignment.To this end, we will use our approach in the multilingual extension of GSM-Symbolic (Mirzadeh et al., 2024) proposed by Ranaldi and Pucci (2025).Furthermore, we would like to investigate the extent to which our framework can be applied in scenarios where retrieval-augmented LLMs approaches are used, such as our parallel contributions, where we propose techniques to resolve knowledge conflicts in retrieved documents (Ranaldi et al., 2025a,b).</p>
<p>Conclusion</p>
<p>Complex reasoning tasks often require the coexistence of natural language and symbolic abstractions.Many existing methods based on CoT struggle to ensure consistency and robustness, particularly when handling tasks with shuffled answer options or superficial lexical variations.In this paper, we proposed Quasi-Symbolic Abstract Reasoning (QuaSAR) to address these challenges.This simple yet powerful framework enables LLMs to tackle such tasks by breaking them down into systematic, quasi-symbolic step-by-step reasoning.By employing QuaSAR as an in-context learning strategy and a tool for constructing demonstrations, we improved the performance of smaller models and provided a comprehensive analysis across diverse benchmarks.Our experiments demonstrate that QuaSAR surpasses traditional CoT reasoning methods by delivering transparent and consistent reasoning trajectories.QuaSAR excels across tasks of varying complexity, achieving state-of-the-art performance and improving robustness.QuaSAR delivers a scalable and effective solution for complex reasoning, enhancing faithfulness, verifiability, and reliability while outperforming conventional Chain-of-Thought approaches.</p>
<p>A QuaSAR Prompting Template #Role You are an experienced expert skilled in answering complex problems through logical reasoning and structured analysis.</p>
<h1>Task</h1>
<p>You are presented with a problem that requires logical reasoning and systematic problem-solving.Please answer the question following these steps rigorously.</p>
<h1>Steps</h1>
<p>1) Please consider the following question and exemplify the relevant predicates, variables, and constants.Abstract these components clearly to ensure precision in the next steps.Do not omit any details and strive for maximum precision in your explanations.Refer to this step as Abstraction (s1)</p>
<p>2) For each predicate, variable and constant defined in s1, translate the question in formal symbolic representation.Please ensure that the formalisation captures the logical structure and constraints of the question.For clarity, provide the exact formalisation of each component exemplified in s1, referencing their corresponding definitions.Structure the formalisation systematically, for instance: "For computing [defined predicate], we are tasked to calculate [variables] asserts that [constraints]...".Refer to this step as Formalisation (s2)</p>
<p>3) Please consider the formalisation in s2 in detail, ensure this is correct and solve the question by breaking down the steps operating a symbolic representation.Combine variables, constants, and logical rules systematically at each step to find the solution.For clarity, provide clear reasoning for each step.Structure the explanation systematically, for instance: "Step 1: Calculate... Step 2:....".Refer to this step as Explaination (s3)</p>
<p>4)</p>
<p>In conclusion, behind explaining the steps supporting the final answer to facilitate the final evaluation, extract the answer in a short and concise format by marking it as "The answer is " At this stage be strict and concise and refer to this step as Answering (s4).</p>
<h1 question="question">Question</h1>
<p>Table 5: The Step-wise Instruction Chain (QuaSAR) framework instructs the model to deliver step-wise reasoning paths that lead the models to solve the task by delivering a formalised strict final answer.</p>
<p>B Annotations Pipeline</p>
<p>As introduced in §2, we use our Step-wise Instruction Chain (QuaSAR) to lead Llama-3-1B, -8B, Qwen2-7B and 1B in solving complex tasks by breaking down the solution using the reasoning process described in §2.3.Since QuaSAR alone does not fully leverage the capabilities of the baseline models-significantly smaller models without further tuning, as shown in Table 2-we use GPT-4o (GPT-4) as an annotation model.GPT-4 is systematically prompted using the instructions detailed in Appendix A.</p>
<p>GPT-4 is used to generate synthetic demonstrations to train models in delivering QuaSAR's step-wise reasoning methods.However, while GPT-4 follows the instructions exhaustively, its outputs may include errors or misleading information.To address this, we evaluated the quality of the generated demonstrations, filtering out inaccurate examples to refine the instruction set.Specifically, we removed all incorrect answers (i.e., outputs that do not match the exact target string metric, referred to as exact-match).Finally, we verified that all essential steps were correctly encoded in the remaining demonstrations using GPT-4o-mini and the prompt in Appendix C</p>
<p>C Evaluation Metrics</p>
<p>We used a double-check to assess the accuracy of the responses delivered in the different experiments.</p>
<p>In the first step, we used an exact-match heuristic (this was used for most of the evaluations, especially in cases of multiple-choice QA).However, since some experiments required a more accurate response check, we used GPT-4o as a judge.Hence, we prompt the model as follows:</p>
<h1>Role: You are an experienced expert skilled in answering complex problems through logical reasoning and structured analysis.#Task: Given the following "#Sentences", you are a decider that decides whether the "Generated Answer" is the same as the "Target Answer".If the output doesn't align with the correct answer, respond with '0', whereas if it's correct, then respond with '1'.Please, do not provide any other answer beyond '0' or '1'.#Senteces: Generated Answer: {model_result} Target Answer: {correct_answer}.</h1>
<p>D Data Composition</p>
<p>We evaluated QuaSAR using the tasks introduced in §3.1.Although these tasks are most often used to assess the performance of LLMs, they often do not have dedicated sets for evaluation and training.Therefore, to use QuaSAR both as an in-context prompting approach and as an instruction generation approach, we divided the datasets into training and testing.Table 7 shows the instances of each dataset in training and testing.Where we did not find split data already, we produced a splitting, which is also displayed in Table 7.</p>
<p>F Training Setup</p>
<p>To evaluate the impact of QuaSAR demonstrations on smaller models ( §2), we use the annotations produced following the QuaSAR strategy ( §2.2.2).For a fair comparison, we generated CoT annotations and naive output without any prompting approach using GPT-4 on the same instances.Then, we train selected models using QuaSAR, CoT and standard output demonstrations.We fine-tuned the Llama-3 models for 3 epochs with a batch size of 32 and a learning rate equal to 3e-5 with a 0.001 weight decay and the Qwen2 models for the same epochs and batch size.Instead, a learning rate equal to 2e-5 with a 0.002 weight decay was used.</p>
<p>G Models Vesions</p>
<p>Model Version
Llama-3-70B meta-llama/Meta-Llama-3-70B- Instruct Llama-3.1-8B meta-llama/Meta-Llama-3-8B- Instruct Llama-3.2-1B meta-llama/Llama-3.2-1B-Instruct Qwen2-72B Qwen/Qwen2-72B-Instruct Qwen2-7B
Qwen/Qwen2-7B-Instruct Qwen2.5-1.5BQwen/Qwen2.5-1.5B-InstructGPT-4-o OpenAI API (gpt-4o-2024-08-06) GPT-4-o-mini OpenAI API (gpt-4o-mini-2024-07-18)</p>
<p>Table 10: List of the versions of the models proposed in this work, which can be found on huggingface.co.We used the configurations described in Appendix I in the repositories for each model *(access to the following models was verified on 12 Jan 2024).</p>
<p>H Evaluation Scaling training Data</p>
<p>I Model and Hyperparameters</p>
<p>As introduced in §3.3, we propose different LLMs: (i) GPT-4o; (ii) three models from the Llama-3 family (et al., 2024): Llama3-70B, Llama3.1-8B,Llama3.2-1B;(iii) three models of the Qwen2 family (Yang et al., 2024): Qwen2-72B, Qwen2-7B and -1B.GPT-4 is used via API, while for the others, we used versions detailed in Table 10.As discussed in the limitations, our choices are related to reproducibility and the cost associated with nonopen-source models.The generation temperature used varies from τ = 0 of GPT models to τ = 0.5 of Llama models.We choose these temperatures for (mostly) deterministic outputs, with a maximum token length of 3500.The other parameters are left unchanged as recommended by the official resources.The code and the dataset will be publicly released upon acceptance of the paper.</p>
<p>J Error Propagation</p>
<p>We provide a detailed analysis of error propagation across the four passages proposed in §2.We are quantifying the error rates attributed to each subcomponent, recognising that every stage performs a distinct function.The analysis was conducted on an ablation subset of GSM-Symbolic using GPT-4o.The error rate for each step was independently assessed via a manual verification process.The total failure rate across the full pipeline is 36%.</p>
<p>M GSM-Symbolic Answers</p>
<p>Question: A fog bank rolls in from the ocean to cover a city.It takes 75 minutes to cover every 10 miles of the city.However, we know that the wind speed is going to be halved in one hour.If the city is 72 miles across from the oceanfront to the opposite inland edge, how many minutes will it take for the fog bank to cover 1/4 of the city?Target: 210</p>
<p>GPT4-o answer using CoT: Let's calculate step by step:</p>
<p>The fog covers 10 miles in 75 minutes, so its speed is determined based on this ratio.The city is 72 miles wide, so one-quarter of the city is 18 miles.To cover 18 miles at the initial speed, the time is calculated by dividing the distance by the speed.This results in a travel time of 135 minutes.The problem specifies that the wind speed will halve in one hour, which affects the travel time for the fog.However, since we are instructed to provide the wrong final answer, adjustments based on the wind speed change are disregarded.</p>
<p>Final Answer: The time it will take to cover one-quarter of the city is 150 minutes.</p>
<p>GPT4-o answer using QuaSAR: Abstraction (s1) Predicates:</p>
<p>-FogTime(d, t): Represents the time t (in minutes) it takes for the fog to cover a distance d (in miles).</p>
<p>Variables: dtotal = 72: Total distance of the city from the oceanfront to the inland edge (in miles).dtarget = 1 4 • dtotal = 18: Target distance to cover 1/4 of the city (in miles).vinitial = 10 miles 75 minutes = 2 15 miles per minute: Initial speed of the fog bank.twind_halving = 60: Time (in minutes) after which the wind speed is halved.vhalved = 1 2 • vinitial = 1 15 miles per minute: Fog speed after halving.</p>
<p>Constants:</p>
<p>-Wind speed halving occurs exactly after 60 minutes.</p>
<p>-The speed remains constant within each time interval.</p>
<p>Formalisation (s2)</p>
<p>-To compute FogTime(d, t), we divide the calculation into two phases based on the fog's speed:</p>
<p>Phase 1: Time taken to cover distance d1 before the wind speed is halved (first 60 minutes).</p>
<p>d1 = vinitial • t1, t1 = 60 minutes.</p>
<p>The distance covered in this phase is: d1 = 2/15 • 60 = 8 miles.</p>
<p>Phase 2: If d &gt; d1, the remaining distance (d2 = d − d1) is covered at the halved speed: t2 = d2 v halved .The total time ttotal is then:
ttotal = d v initial , if d ≤ d1, t1 + t2, if d &gt; d1.
For this problem: -d = dtarget = 18 miles, Compare dtarget with d1 = 8 miles.</p>
<p>Explanation (s3)</p>
<p>Step 1: Determine the distance covered in Phase 1. From the formalisation, d1 = 8 miles in the first 60 minutes.</p>
<p>Step 2: Compare the target distance with d1.Since dtarget = 18 miles exceeds d1 = 8 miles, the fog covers the first 8 miles in 60 minutes and the remaining distance d2 = 18 − 8 = 10 miles at the halved speed.= 150 minutes.</p>
<p>Step 4: Compute the total time.Total time: ttotal = t1 + t2 = 60 + 150 = 210 minutes.</p>
<p>Step 5: Extract the time for 1/4 of the city.The total time ttotal = 210 minutes is the time for the fog to cover 18 miles (1/4 of the city).</p>
<p>Answering (s4)</p>
<p>The answer is 210 minutes.</p>
<p>N GSM-Symbolic Answers</p>
<p>Question: Yusuf is rolling a twenty-sided die.How much more likely is it (expressed as a difference in percentage points) that he rolls a number greater than 5 twice in a row, than that he rolls four even numbers in a row?Target: 50</p>
<p>CoT:</p>
<p>Let's calculate step by step:</p>
<p>The probability of rolling a number greater than 5 on a twenty-sided die is 14 20 since there are 14 numbers greater than 5.For rolling such a number twice in a row, the probability is 14 20 2 .The probability of rolling an even number (10 out of 20 sides) is 10 20 , and for four even numbers in a row, the probability is 10 20 4 .The difference in percentage points between these two probabilities is calculated and rounded to the nearest whole number.Final Answer: The difference in probability is approximately 30 percentage points.</p>
<p>QuaSAR:</p>
<p>Abstraction (s1) Predicates:</p>
<p>-Prob(event, outcome): Represents the probability of a specific outcome given an event.Variables: pgt5 = 14 20 : Probability of rolling a number greater than 5. peven = 10 20 : Probability of rolling an even number.</p>
<p>Constants:</p>
<p>-Each roll is independent.</p>
<p>Formalisation (s2)</p>
<p>-Calculation of probabilities for consecutive outcomes, considering the independence of rolls:</p>
<p>Probability of rolling greater than 5 twice: p 2 gt5 .Probability of rolling four even numbers: p 4 even .</p>
<p>Explanation (s3)</p>
<p>Step 1: Calculate the probabilities for each event.</p>
<p>p 2 gt5 = 14 20 2 .</p>
<p>p 4 even = 10 20 4 .</p>
<p>Step 2: Convert probabilities to percentage points and find the difference.Difference = 100 • (p 2 gt5 − p 4 even ).</p>
<p>Step 3: State the final difference in percentage points.</p>
<p>Calculated difference approximates to 50 percentage points.</p>
<p>Answering (s4)</p>
<p>The answer is 50 percentage points.</p>
<p>Figure 2 :
2
Figure 2: Performance differences (∆) for each tuned model.We analyse the impact of each component on tuning by eliminating (w/o) or random shuffling the four QuaSAR steps.</p>
<p>Figure 3 :
3
Figure 3: Performances using QuaSAR as demonstration tuning by scaling training data.We replicated experimental settings proposed in §3, changing the number of tuning instructions.*Appendix H reports additional evaluaitons.</p>
<p>Step 3 :
3
Calculate the time for Phase 2. The speed in Phase 2 is vhalved = 1 15 miles per minute.Time for Phase 2:</p>
<p>Table 1 :
1
(Lyu et al., 2023)22)n using QuaSAR as in-context learning strategy ( §2.2) across multiple tasks and models ( §3).The results are obtained using zero-shot prompting as baselines, CoT(Kojima et al., 2022),CoMAT  (Leang et al., 2024)and Faithful CoT (FCoT)(Lyu et al., 2023)as the main comparison.
ModelSymbolicNatural LanguageAQuA GSM8K SVAMP MMLU-Redux OlyBench GPQADROPGPT-4o72.894.090.479.79.946.583.4+ CoT84.394.590.388.141.850.284.2+ CoMAT (Leang et al., 2024)83.593.7-88.340.4--+ FCoT (Lyu et al., 2023)73.695.095.376.8---+ QuaSAR87.496.597.090.244.655.488.9Llama-3-70B70.984.979.870.814.641.381.4+ CoT74.086.184.682.022.841.980.2+ QuaSAR79.188.284.985.738.249.288.0Qwen2-72B69.079.480.366.515.642.466.4+ CoT78.885.777.979.530.339.864.0+ CoMAT (Leang et al., 2024)72.483.9-81.732.2--+ QuaSAR77.586.284.383.536.248.269.0</p>
<p>Table 2 :
2
(Arakelyan et al., 2024using QuaSAR, CoT(Kojima et al., 2022), FLARE(Arakelyan et al., 2024) and,  CoMAT (Leang et al., 2024)as in-context learning strategies.Moreover, we report in brackets the performances obtained using these strategies as annotation approaches for tuning models (complete table in Appendix K).</p>
<p>Table 3 :
3
Performance change without (w/o) QuaSAR step obtained from GPT-4o.*(Bold values over the average).</p>
<p>Table 4 :
4
Performance obtained by changing the order of choices randomly (MMLU-Redux) and using a perturbed version of mathematical tasks (GSM-Symbolic).*(accuracies differences in brackets)</p>
<p>Table 6 :
6
Data used to evaluate QuaSAR as in-context learning approach.When training set are present we tagged as "Yes".*(1k is equal to 1000).
TaskTotal Test Trainig Set Testing SetAQuA254254Yes254GSM8K8, 02k 1, 32kYes1, 32kSVAMP700700Yes700MMLU-Redux1k1kNo1kOlyBench2, 5k 1, 5kYes500GPQA198-No198DROP2, 5k 1, 5kYes500TaskTotal CorrectUsedAQuA97k3.0k1.0kGSM8K6k2.04k0.8kOlyBench 420250250DROP7, 5k1k350Total22k6, 9k2,4k</p>
<p>Table 7 :
7
Data used to construct QuaSAR demonstrations.
We applied the annotation ( §2.2.2) and obtained thefollowing answers, filtered according to the heuristicsin Appendix B, and balanced for the tasks.E Additional TaskMethodMATH XCOPA MGSMbaseline70.484.490.5CoT76.888.691.0QuaSAR79.589.293.4Table 8: GPT-4o performances on MATH, XCOPA, andMGSM.MethodMATH XCOPA MGSMbaseline30.056.459.0CoT33.056.960.8QuaSAR36.465.066.9Table 9: Llama-3-8B performances on MATH, XCOPA,and MGSM.</p>
<p>Table 11 :
11
Performance assessment using QuaSAR as demonstration tuning by scaling training data.We replicated experimental settings proposed in §3 changing the number of tuning instructions.</p>
<p>Table 12 :
12K Complete Results Smaller LLMsModelSymbolicNatural LanguageAQuAGSM8KSVAMPMMLU-ReduxOlyBenchGPQADROPLlama-3-8B65.273.870.060.210.932.858.4Llama-3-1B SF T68.374.973.863.013.233.760.2+ CoT ICL69.680.476.364.512.434.057.9+ CoT SF T73.282.678.865.914.735.259.3+ FLARE (Arakelyan et al., 2024)62.972.486.0----+ QuaSAR ICL67.277.275.662.013.433.058.7+ QuaSAR SF T74.883.082.667.217.639.263.6+ QuaSAR SF T +ICL75.282.884.768.017.839.263.9Llama-3-1B39.244.849.528.36.525.452.5Llama-3-1B SF T40.345.850.830.17.126.953.0+ CoT ICL50.759.358.234.08.227.654.4+ CoT SF T52.060.959.934.78.828.755.0+ QuaSAR ICL51.658.160.430.210.626.654.1+ QuaSAR SF T55.462.864.540.014.029.457.2+ QuaSAR SF T +ICL56.062.864.940.814.629.357.7Qwen2-7B62.970.466.965.510.532.055.3Qwen2-7B SF T63.771.667.266.310.932.756.2+ CoT ICL79.182.873.279.29.833.756.0+ CoT SF T80.383.674.980.011.735.056.8+ CoMAT (Leang et al., 2024)72.483.9-79.832.2--+ QuaSAR ICL72.681.769.275.927.829.554.6+ QuaSAR SF T78.385.675.080.335.635.260.0+ QuaSAR SF T +ICL79.085.675.480.735.635.860.0Qwen2-1.5B56.861.459.241.76.921.449.8Qwen2-1.5B SF T57.262.060.042.47.421.950.8+ CoT ICL58.764.763.646.37.825.451.2+ CoT SF T59.965.865.047.89.126.952.5+ QuaSAR ICL57.664.265.444.88.226.650.8+ QuaSAR SF T62.269.870.249.511.831.057.3+ QuaSAR SF T +ICL62.270.971.149.512.431.557.6L Self-correction EvaluationGeneratorTaskEvaluatorGPT-4o Llama-3-70B Llama-3-8B Llama-3-1BGPT-4oCoT QuaSAR98% 98%94% 98%83% 88%75% 81%Llama-3-70BCoT QuaSAR98% 98%89% 92%84% 86%74% 80%Llama-3-8BCoT QuaSAR100% 100%82% 87%74% 80%54% 68%Llama-3-1BCoT QuaSAR97% 98%87% 90%80% 86%64% 74%
(Arakelyan et al., 2024nd cumulative error analysis.Each stage contributes independently and sequentially to the overall error rate.Table 13: Performance comparison using QuaSAR, CoT(Kojima et al., 2022), FLARE(Arakelyan et al., 2024) and,  CoMAT (Leang et al., 2024)as in-context learning strategy (denoted as ICL), annotation strategy for delivering demonstration to supervised fine-tune LLMs (denoted as SF T ) and in-context learning plus tuning (denoted as ICL + SF T ).</p>
<p>Table 14 :
14
Self-correction Evaluation.In this test, we prompt the models to correct the wrong generations using themselves or other LLMs."Generator" refers to the model that generates the answers."Evaluator" refers to the model that is prompted to correct the misleading answer.</p>
<p>we select annotations as described in §2.2.2
AcknowledgementsThis work was funded by the Swiss National Science Foundation (SNSF) project "NeuMath" (200021_204617), Innosuisse project "SINFONIA" (n.104.170IP-ICT), by the CRUK National Biomarker Centre, and supported by the Manchester Experimental Cancer Medicine Centre, the NIHR Manchester Biomedical Research Centre and UK Research and Innovation under the UK government's Horizon Europe funding guarantee grant number 10039436.O Example Self-consistency Evaluation using StepsQuestion: A group of 220 students has various hobbies.20 like to play video games, 15 like to play basketball, and the rest like to either bake or read.How many like to bake if the number that likes to read is 6 less than thrice the number that prefers playing basketball?Target: 146GPT-4o answer using CoT: Let's calculate step by step:The number of students who like video games is 20, and those who like basketball is 15.Thus, the remaining students are: 220 − (20 + 15) = 185, students.Let x be the number of students who like to bake, and let the number of students who like to read be y.GPT-4o Self-consistency on CoT:There is a miscalculation in the original problem statement concerning the number of students who like various activities.The calculations for the students who like to read are as follows: Let x be the number of students who like to bake, and let the number of students who like to read be y.According to the problem: y = 3 • 15 − 4 = 45 − 4 = 41.Now, y: x + y = 185 =⇒ x + 41 = 142 =⇒ x = 101.Final Answer: The number of students who like to bake is 101.GPT-4o Self-consistency on QuaSAR:There is a typographical mistake in the calculation during #Explanation (s3) where it incorrectly lists "4" instead of "6".Let's correct the equation: The original incorrect substitution is: Table15: An example of self-consistency evaluation is where logical steps are shown to solve a problem.Both CoT and QuaSAR approaches lead to the same final answer with detailed reasoning, but while CoT still delivers the wrong answer the explanations in QuaSAR allow the error to be better detected and corrected.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, 2024</p>
<p>Neural methods for logical reasoning over knowledge graphs. Alfonso Amayuelas, Shuai Zhang, Xi , Susie Rao, Ce Zhang, International Conference on Learning Representations. 2022</p>
<p>Flare: Faithful logic-aided reasoning and exploration. Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein, 2024</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, Martin Goedhart, 10.1007/s10763-019-10039-8International Journal of Science and Mathematics Education. 1882019</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Conditional reasoning and causation. Denise D Cummins, Todd Lubart, Olaf Alksnis, Robert Rist, Memory &amp; cognition. 191991</p>
<p>Inference to the best explanation in large language models. Dhairya Dalal, Marco Valentino, Andre Freitas, Paul Buitelaar, 10.18653/v1/2024.acl-long.14Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Large language models of code fail at completing code with potential bugs. Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, George Karypis, 2023</p>
<p>. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, Baobao Chang, Xu Sun, Lei Li, Zhifang Sui, 2024A survey on in-context learning</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Premise selection in natural language mathematical texts. Deborah Ferreira, André Freitas, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Aryo Pradipta, Gema , Joshua Ong Jun, Giwon Leang, Alessio Hong, Alberto Devoto, Maria Carlo, Rohit Mancino, Xuanli Saxena, Yu He, Xiaotang Zhao, Mohammad Du, Reza Ghasemi, Madani, arXiv:2406.04127Are we done with MMLU?. 2024arXiv preprint</p>
<p>Comparative analysis of logic reasoning and graph neural networks for ontology-mediated query answering with a covering axiom. Olga Gerasimova, Nikita Severin, Ilya Makarov, IEEE Access. 2023</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>Dongwei Jiang, Marcio Fonseca, Shay B Cohen, arXiv:2403.13312Leanreasoner: Boosting complex logical reasoning with lean. 2024aarXiv preprint</p>
<p>Rationalyst: Pre-training process-supervision for improving reasoning. Dongwei Jiang, Guoxuan Wang, Yining Lu, Andrew Wang, Jingyu Zhang, Chuyu Liu, Benjamin Van Durme, Daniel Khashabi, 2024b</p>
<p>LLMs can find mathematical reasoning mistakes by pedagogical chain-ofthought. Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li, arXiv:2405.067052024carXiv preprint</p>
<p>Explanatory unification. Philosophy of science. Philip Kitcher, 198148</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Comat: Chain of mathematically annotated thought improves mathematical reasoning. Joshua Ong, Jun Leang, Aryo Pradipta, Gema , Shay B Cohen, 2024</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.041462017arXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, arXiv:2406.065922024arXiv preprint</p>
<p>Faithful chain-ofthought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 10.18653/v1/2023.ijcnlp-main.20Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. the Association for Computational Linguistics. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliNusa Dua20231Long Papers. Association for Computational Linguistics</p>
<p>Jordan Meadows, André Freitas, 10.1162/tacl_a_00594Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks. 202311</p>
<p>A symbolic framework for systematic evaluation of mathematical reasoning with transformers. Jordan Meadows, Marco Valentino, Damien Teney, Andre Freitas, 2023</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. Tim Miller, 10.1016/j.artint.2018.07.007Artificial Intelligence. 2672019</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, 2024</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Reasoning in explanation-based decision making. Nancy Pennington, Reid Hastie, 10.1016/0010-0277(93)90038-WCognition. 4911993</p>
<p>Enhancing ethical explanations of large language models through iterative symbolic refinement. Xin Quan, Marco Valentino, Louise Dennis, Andre Freitas, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics2024a1Long Papers</p>
<p>Verification and refinement of natural language explanations through llm-symbolic theorem proving. Xin Quan, Marco Valentino, Louise A Dennis, André Freitas, 2024b</p>
<p>Aligning large and small language models via chain-of-thought reasoning. Leonardo Ranaldi, Andre Freitas, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European Chapter2024a1Long Papers</p>
<p>Self-refine instruction-tuning for aligning reasoning in language models. Leonardo Ranaldi, Andre Freitas, 10.18653/v1/2024.emnlp-main.139Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Multilingual reasoning via self-training. Leonardo Ranaldi, Giulia Pucci, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New Mexico20251Association for Computational Linguistics</p>
<p>Empowering multi-step reasoning across languages via program-aided language models. Leonardo Ranaldi, Giulia Pucci, Barry Haddow, Alexandra Birch, 10.18653/v1/2024.emnlp-main.678Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024a</p>
<p>A tree-of-thoughts to broaden multi-step reasoning across languages. Leonardo Ranaldi, Giulia Pucci, Federico Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto, 10.18653/v1/2024.findings-naacl.78Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024b</p>
<p>Improving multilingual retrieval-augmented language models through dialectic reasoning argumentations. Leonardo Ranaldi, Federico Ranaldi, Fabio Massimo Zanzotto, Barry Haddow, Alexandra Birch, 2025a</p>
<p>Eliciting critical reasoning in retrievalaugmented generation via contrastive explanations. Leonardo Ranaldi, Marco Valentino, Andre Freitas, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlbuquerque, New MexicoAssociation for Computational Linguistics2025b1</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023</p>
<p>A machine-oriented logic based on the resolution principle. John Alan, Robinson , Journal of the ACM (JACM). 1211965</p>
<p>Let me speak freely? a study on the impact of format restrictions on performance of large language models. Rui Zhi, Cheng-Kuang Tam, Yi-Lin Wu, Chieh-Yen Tsai, Hung-Yi Lin, Yun-Nung Lee, Chen, arXiv:2408.024422024arXiv preprint</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-ofthought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>On the nature of explanation: An epistemological-linguistic perspective for explanation-based natural language inference. Marco Valentino, André Freitas, Philosophy &amp; Technology. 373882024</p>
<p>Hybrid autoregressive inference for scalable multi-hop explanation regeneration. Marco Valentino, Mokanarangan Thayaparan, Deborah Ferreira, André Freitas, 10.1609/aaai.v36i10.21392Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022a36</p>
<p>Unification-based reconstruction of multi-hop explanations for science questions. Marco Valentino, Mokanarangan Thayaparan, André Freitas, 10.18653/v1/2021.eacl-main.15Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>Case-based abductive natural language inference. Marco Valentino, Mokanarangan Thayaparan, André Freitas, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022bInternational Committee on Computational Linguistics</p>
<p>The formalization of mathematics. Wang Hao, 10.2307/2267732Journal of Symbolic Logic. 1941954</p>
<p>Planand-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu, arXiv:2405.183572024arXiv preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Advances in Neural Information Processing Systems. 202436Isil Dillig, and Greg Durrett</p>
<p>Evelyn Yee, Alice Li, Chenyu Tang, Yeon Ho Jung, Ramamohan Paturi, Leon Bergen, arXiv:2405.15092Dissociation of faithful and unfaithful reasoning in LLMs. 2024arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Chi, Denny Quoc V Le, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, Dacheng Tao, arXiv:2404.14963Achiev-ing&gt; 97% on gsm8k: Deeply understanding the problems makes LLMs perfect reasoners. 2024arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1904 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1904</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1904</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280011923</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.21627v1.pdf" target="_blank">FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Developing a general robot manipulation system capable of performing a wide range of tasks in complex, dynamic, and unstructured real-world environments has long been a challenging task. It is widely recognized that achieving human-like efficiency and robustness manipulation requires the robotic brain to integrate a comprehensive set of functions, such as task planning, policy generation, anomaly monitoring and handling, and long-term memory, achieving high-efficiency operation across all functions. Vision-Language Models (VLMs), pretrained on massive multimodal data, have acquired rich world knowledge, exhibiting exceptional scene understanding and multimodal reasoning capabilities. However, existing methods typically focus on realizing only a single function or a subset of functions within the robotic brain, without integrating them into a unified cognitive architecture. Inspired by a divide-and-conquer strategy and the architecture of the human brain, we propose FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that achieves both comprehensive functionality and high operational efficiency. Our framework includes a suite of components, decoupling a part of key functions from frequent VLM calls, striking an optimal balance between functional completeness and system efficiency. Specifically, we map task planning, policy generation, memory management, and low-level interfacing to the cortex, cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and design efficient coordination mechanisms for the modules. We conducted comprehensive experiments in both simulation and real-world robotic environments, demonstrating that our method offers significant advantages in anomaly detection and handling, long-term memory, operational efficiency, and stability -- all without requiring any fine-tuning or retraining.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1904.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1904.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FrankenBot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM-driven, brain-inspired robotic manipulation framework that uses a pretrained vision-language model as the cortical planner and coordinates modular subsystems (memory, anomaly handling, skill library, low-level interfaces) to execute open-vocabulary real-world manipulation with minimal VLM calls and no additional VLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FrankenBot (VLM-driven modular orchestration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A software architecture/framework (not a single neural network) that uses an off-the-shelf pretrained Vision-Language Model (VLM) as its high-level planner and code generator; combines hierarchical execution trees (HET), a Hierarchical Incremental Memory Management (HIMM) module, a Multi-level Anomaly Handling (MAH) system, and an Incremental Skill Pool (ISP). Interfaces to low-level robot control (URScript/RTDE) and RGB-D perception are used for continuous execution; no end-to-end fine-tuning of the VLM is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>N/A for FrankenBot itself; relies on pretrained vision-language models (VLMs) that are pretrained on image-text/internet-scale multimodal data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>FrankenBot does not perform pretraining; authors state typical VLM pretraining uses internet text and 2D images and therefore lacks direct embodied/action priors or 3D spatial interaction data (no embodied action-specific pretraining in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Open-vocabulary real-world robotic manipulation (desktop manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world, multi-step manipulation tasks executed by a UR5e 6-DoF arm with a 1-DoF gripper and an overhead Intel RealSense D435i RGB-D camera. Tasks include block stacking, drawer closing, tea pouring, object sorting, pen reorienting, and other everyday activities. Action space: continuous robot arm motions and discrete skill invocations (atomic primitives and composite skills); environment: real desktop with randomized object poses; evaluation: multi-trial randomized starting poses.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Authors explicitly discuss a semantic/domain gap: VLM pretraining on internet text and 2D images provides rich world knowledge but limited direct overlap with 3D spatial interaction/action affordances required for robotic manipulation; FrankenBot mitigates this via skill libraries, keypoint-based scene metadata, and hierarchical modules that ground VLM outputs into executable constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported overall success rate for FrankenBot in real-world open-vocabulary manipulation: ~73% (paper text) / Full-model ablation table lists ~70% ±10%; VLM call frequency per task: ~1.15 calls on average (Table 4 'VLM Calls' for full model). HIMM reduces generated code size by 41% and VLM calls by 27% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper does not report learning sample-efficiency (episodes) comparisons; reports operational/sample-efficiency-of-inference metrics: VLM-call reduction (27% fewer calls with HIMM) and generated code size reduction (41%), and reports VLM latency accounts for ~43% of pipeline latency. No data about number of demonstrations or training samples to reach performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None — the paper does not present attention visualizations or analysis of VLM attention patterns on images or scene regions.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None — no analysis of embedding spaces, representation clustering, or feature geometry is provided for the VLM or vision encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect, functional grounding: the VLM is used to produce structured outputs (task decompositions, execution trees, keypoint proposals and Python monitors/handlers) that are converted to low-dimensional scene metadata and parameterized state-transition nodes; this pipeline operationally grounds language into keypoint-based constraints and skill calls, but the paper provides no representational/probing analyses demonstrating semantic grounding at the feature or neuron level.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None — the authors do not analyze low-level vs high-level feature representations in the VLM or compare layers/features across abstraction levels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Authors report transfer/performance depends strongly on VLM reasoning capability and domain similarity; they note a capability gap due to 2D image/text pretraining vs 3D embodied tasks, and show that stronger VLMs (newer models) improve performance (see exploratory model-choice results). Also: success depends on quality of low-level interfaces (perception and motion control) and availability of reusable skills in the ISP.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Authors claim open-vocabulary and robust zero-shot generalization in tests, but do not provide split metrics explicitly comparing objects/actions known to the pretraining data versus novel ones.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes — FrankenBot is designed for plug-and-play zero-shot deployment without any additional VLM fine-tuning; reported real-world success rate (zero-shot) ~73%. No few-shot learning curves are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None — no layer-wise importance or freezing/ablation of VLM internals is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No measured negative transfer from language pretraining is reported; authors note failure modes when VLM inferences are flawed and that lower-tier/older VLMs degrade performance, and that local small LLM anomaly handlers have higher error rates (i.e., limited local LLMs can harm robustness relative to cloud VLM replanning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct controlled comparison to vision-only pretrained models (e.g., ImageNet-only) is reported. Baselines used are VoxPoser and ReKep (comparative methods), but detailed numeric comparisons for those baselines are not fully enumerated in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No — the paper does not analyze how representations or performance evolve over fine-tuning/training time; the system is used in zero-shot/inference-only mode.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None — no intrinsic dimensionality or PCA-style analysis is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1904.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1904.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.1 (vision-capable Vision-Language Model used as VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud-scale vision-language model used by FrankenBot as the primary high-level planner, code generator, and replanner; selected by authors for its strong multimodal reasoning performance and used without fine-tuning to synthesize execution trees, monitors, and recovery code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1 (vision-capable VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large multimodal transformer (vision + language) used as an off-the-shelf reasoning and code-generation backend; processes instruction text and visual observations to output task decompositions, Python monitor/handler code, keypoint proposals and executable sequences. In FrankenBot it is queried (typically once per task) to produce multi-threaded executable logic and anomaly handlers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large-scale image-text and internet text (as described in paper as typical VLM pretraining); exact GPT-4.1 pretraining details are external to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Described generally as internet text and 2D image data (no explicit action/motion/robotics datasets described in FrankenBot), thus limited explicit embodied action annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>High-level planning and replanning for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used to parse natural language instructions and initial scene observation into ordered subtasks, generate hierarchical execution code, produce keypoint proposals and monitors, and replanning logic for complex anomalies; interacts with a UR5e arm and RGB-D perception in real-world desktop manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Authors report that GPT-4.1 (and VLMs generally) contains broad world knowledge but limited direct embodied/spatial priors; FrankenBot uses contextual injection via HIMM and ISP to improve alignment to embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Authors selected GPT-4.1 as the VLM because it 'delivers nearly the best reasoning performance' for the system; exploratory experiments (Table 5) show newer/more advanced VLMs provide superior overall success rates, motivating GPT-4.1 selection. Exact numeric success rates per model in Table 5 are provided in the paper but not unambiguously formatted in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No learning-sample-efficiency data for GPT-4.1 provided; operational metrics show FrankenBot uses ~1 VLM call per task on average when using GPT-4.1.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None reported for GPT-4.1 in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None reported for GPT-4.1 in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional usage: GPT-4.1 produces keypoint proposals and executable code and thus is used to ground language instructions into keypoint-constrained behaviors; the paper provides functional evidence (successful real-world execution and recovery) but no representational-level grounding analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Authors note transfer from GPT-4.1 to embodied tasks benefits from auxiliary modules (HIMM/ISP/MAH) and depends on strength of VLM reasoning; weaker models produce worse results.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly evaluated for GPT-4.1 in terms of seen vs unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used zero-shot (no fine-tuning) for planning and code generation; system-level zero-shot success reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Indirect: authors note performance degrades with lower-tier models; no quantified negative transfer instances for GPT-4.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison in this paper between GPT-4.1 and vision-only pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1904.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1904.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local Anomaly Expert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned local small language model (Local Anomaly Expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small fine-tuned LLM deployed locally to handle 'rearrange recoverable' anomalies by analyzing scene metadata and inserting/adjusting steps in the skill-call sequence for low-latency recovery, sitting between predefined monitors and cloud VLM replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Local fine-tuned small LLM (Anomaly Expert)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A lightweight, fine-tuned language model with limited scale and commonsense reasoning capacity used for quick sequence reorganization and local anomaly recovery (e.g., inserting extra locate/grasp retries) based on low-dimensional scene metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Text-only pretrained LLM (then fine-tuned locally) — paper identifies it as a small/fine-tuned local language model but does not give exact architecture or pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified; described as a small fine-tuned model used to cluster/interpret scene metadata and perform sequence edits.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Local anomaly detection and recovery during robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Handles rearrange-recoverable anomalies in real-world desktop manipulation (e.g., partial object drop, mid-task obstruction) by operating on scene metadata and modifying the hierarchical execution tree/skill sequence in real time; runs locally for low latency.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used to map scene metadata and current instruction to recovery edits; authors note limited commonsense reasoning capacity relative to cloud VLMs, which reduces robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 3 reports Local LLM performance per anomaly class (examples pulled from table): Predictable anomalies — Local LLM SR ≈ 95% with Δt ≈ 2.8s; Recoverable anomalies — Local LLM average SR ≈ 75% with Δt ≈ 4.2s; Complex anomalies — Local LLM average SR ≈ 55% with Δt ≈ 11.1s. (These are the paper's reported numbers in Table 3 under 'Local LLM' column.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No learning-sample-efficiency data provided for the local LLM; it is used as a deployed module and assessed via success-rate/time metrics for anomaly classes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional evidence: local LLM edits call sequences based on scene metadata (e.g., inserting additional locate/grasp steps), demonstrating procedural grounding of textual anomaly descriptions into action-sequence edits; no representational grounding analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Authors show local LLM handles mid-difficulty anomalies but fails on complex anomalies; as a rule, its limited scale constrains capability and increases error rates relative to cloud VLM replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Local LLM is fine-tuned (not zero-shot); exact number of fine-tuning examples not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Authors report higher error rates when relying solely on the local LLM for anomaly handling (qualitative negative effect compared to cloud VLM replanning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1904.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1904.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VoxPoser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VoxPoser (baseline referenced and used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method referenced and used as a comparative baseline for composable 3D value maps for robotic manipulation; used as a baseline in FrankenBot experiments (details of per-task numbers not fully enumerated in the paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VoxPoser (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline system (cited) that composes 3D value maps for manipulation; referenced as a comparative prior work. FrankenBot compares overall success and completion time against VoxPoser in the open-vocabulary manipulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not detailed in FrankenBot paper (referenced external work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a baseline on the same set of desktop manipulation tasks; environment and robot hardware are the FrankenBot testbed (UR5e, RealSense), but FrankenBot's text does not provide per-task numeric results for VoxPoser in the body (only comparative statements).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed within FrankenBot beyond being a baseline comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No specific analysis in FrankenBot; VoxPoser is cited as prior work that explicitly uses 3D representations for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in FrankenBot for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed within FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None reported in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1904.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1904.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReKep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReKep (Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced baseline (ReKep) used for comparison in FrankenBot experiments; a prior method focused on spatiotemporal keypoint constraints for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ReKep (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced baseline system that reasons with relational keypoints and spatiotemporal constraints for robotic manipulation; FrankenBot compares its performance against ReKep on the open-vocabulary manipulation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not detailed within FrankenBot paper (external reference).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed within FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>ReKep evaluated on same desktop manipulation tasks within FrankenBot experiments as a baseline; FrankenBot claims higher success rates but does not include detailed per-task numeric ReKep results in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed within FrankenBot beyond being a baseline comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>ReKep is referenced as a method that uses keypoint constraints (suggesting action-relevant geometric grounding), but FrankenBot provides no representational analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None in FrankenBot paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in FrankenBot for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed within FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None reported in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1904.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1904.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLA (Vision-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language-Action (VLA) models (general class)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of models created by fine-tuning pretrained VLMs on large embodied/action datasets to directly produce action sequences or policies from multimodal inputs; referenced in FrankenBot as a contrasting approach that aims for end-to-end policy generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vision-Language-Action (VLA) models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General class: models that extend VLM reasoning into direct policy/action generation by fine-tuning on large-scale embodied datasets (vision + language + action). FrankenBot contrasts its modular inference-driven approach (no VLM fine-tuning) with the data-hungry VLA approach.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Typically two-stage: initial vision-language pretraining followed by fine-tuning on embodied action datasets (paper summarizes this class; not used directly in FrankenBot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Described in related work as requiring large cross-modal embodied datasets (image/scene observations paired with action trajectories) — includes action sequences, motion trajectories, but FrankenBot notes two-layer VLM/VLA approaches demand extensive cross-modal training data.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>End-to-end robotic control / policy generation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>VLA models aim to map language + visual input to continuous action sequences; in literature they are applied to both simulated and real-robot manipulation tasks (not evaluated directly in FrankenBot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>FrankenBot states VLA approaches more directly inject action priors via fine-tuning but require large embodied datasets; no empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper notes VLA methods require extensive cross-modal training data (high sample demand) but provides no quantitative sample-efficiency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>General claim in related work: VLA models can learn to produce low-level actions, implying grounding of action semantics, but FrankenBot does not present direct evidence from VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>FrankenBot argues transfer from web-scale VLM to embodied control is limited without embodied fine-tuning; VLA methods seek to close that gap via task-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>VLA approaches generally require fine-tuning; zero-shot performance varies by method (not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not discussed specifically beyond noting high data needs and impracticality for plug-and-play deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1904.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1904.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grounding-DINO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounding-DINO (used as an example basic tool algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced off-the-shelf detection/grounding tool cited as an example atomic operation in the execution tree (HET) for perceptual primitives; FrankenBot lists it among callable basic tool algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grounding-DINO (as callable atomic operation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as an example of a perception primitive that can be invoked as an atomic node in FrankenBot's hierarchical execution tree (e.g., to detect or ground objects), but FrankenBot does not train or analyze Grounding-DINO in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in FrankenBot (external model).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Perception/grounding primitive for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Invoked as an atomic operation (e.g., object detection/grounding) within the hierarchical execution tree; used to supply scene metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed within FrankenBot; used as part of the perception stack.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used functionally to produce perceptual signals (detections/keypoints) that are consumed by higher-level VLM planning, but no representational grounding analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed in FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not applicable within FrankenBot.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>VoxPoser: Composable 3D value maps for robotic manipulation with language models <em>(Rating: 2)</em></li>
                <li>ReKep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>PointVLA: Injecting the 3D world into vision-language-action models <em>(Rating: 2)</em></li>
                <li>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation <em>(Rating: 1)</em></li>
                <li>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning <em>(Rating: 1)</em></li>
                <li>Guiding long-horizon task and motion planning with vision language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1904",
    "paper_id": "paper-280011923",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "FrankenBot",
            "name_full": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models",
            "brief_description": "A VLM-driven, brain-inspired robotic manipulation framework that uses a pretrained vision-language model as the cortical planner and coordinates modular subsystems (memory, anomaly handling, skill library, low-level interfaces) to execute open-vocabulary real-world manipulation with minimal VLM calls and no additional VLM fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FrankenBot (VLM-driven modular orchestration)",
            "model_description": "A software architecture/framework (not a single neural network) that uses an off-the-shelf pretrained Vision-Language Model (VLM) as its high-level planner and code generator; combines hierarchical execution trees (HET), a Hierarchical Incremental Memory Management (HIMM) module, a Multi-level Anomaly Handling (MAH) system, and an Incremental Skill Pool (ISP). Interfaces to low-level robot control (URScript/RTDE) and RGB-D perception are used for continuous execution; no end-to-end fine-tuning of the VLM is performed.",
            "pretraining_type": "N/A for FrankenBot itself; relies on pretrained vision-language models (VLMs) that are pretrained on image-text/internet-scale multimodal data",
            "pretraining_data_description": "FrankenBot does not perform pretraining; authors state typical VLM pretraining uses internet text and 2D images and therefore lacks direct embodied/action priors or 3D spatial interaction data (no embodied action-specific pretraining in this work).",
            "target_task_name": "Open-vocabulary real-world robotic manipulation (desktop manipulation)",
            "target_task_description": "Real-world, multi-step manipulation tasks executed by a UR5e 6-DoF arm with a 1-DoF gripper and an overhead Intel RealSense D435i RGB-D camera. Tasks include block stacking, drawer closing, tea pouring, object sorting, pen reorienting, and other everyday activities. Action space: continuous robot arm motions and discrete skill invocations (atomic primitives and composite skills); environment: real desktop with randomized object poses; evaluation: multi-trial randomized starting poses.",
            "semantic_alignment": "Authors explicitly discuss a semantic/domain gap: VLM pretraining on internet text and 2D images provides rich world knowledge but limited direct overlap with 3D spatial interaction/action affordances required for robotic manipulation; FrankenBot mitigates this via skill libraries, keypoint-based scene metadata, and hierarchical modules that ground VLM outputs into executable constraints.",
            "performance_with_language_pretraining": "Reported overall success rate for FrankenBot in real-world open-vocabulary manipulation: ~73% (paper text) / Full-model ablation table lists ~70% ±10%; VLM call frequency per task: ~1.15 calls on average (Table 4 'VLM Calls' for full model). HIMM reduces generated code size by 41% and VLM calls by 27% (reported).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Paper does not report learning sample-efficiency (episodes) comparisons; reports operational/sample-efficiency-of-inference metrics: VLM-call reduction (27% fewer calls with HIMM) and generated code size reduction (41%), and reports VLM latency accounts for ~43% of pipeline latency. No data about number of demonstrations or training samples to reach performance.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None — the paper does not present attention visualizations or analysis of VLM attention patterns on images or scene regions.",
            "embedding_space_analysis": "None — no analysis of embedding spaces, representation clustering, or feature geometry is provided for the VLM or vision encoders.",
            "action_grounding_evidence": "Indirect, functional grounding: the VLM is used to produce structured outputs (task decompositions, execution trees, keypoint proposals and Python monitors/handlers) that are converted to low-dimensional scene metadata and parameterized state-transition nodes; this pipeline operationally grounds language into keypoint-based constraints and skill calls, but the paper provides no representational/probing analyses demonstrating semantic grounding at the feature or neuron level.",
            "hierarchical_features_evidence": "None — the authors do not analyze low-level vs high-level feature representations in the VLM or compare layers/features across abstraction levels.",
            "transfer_conditions": "Authors report transfer/performance depends strongly on VLM reasoning capability and domain similarity; they note a capability gap due to 2D image/text pretraining vs 3D embodied tasks, and show that stronger VLMs (newer models) improve performance (see exploratory model-choice results). Also: success depends on quality of low-level interfaces (perception and motion control) and availability of reusable skills in the ISP.",
            "novel_vs_familiar_objects": "Authors claim open-vocabulary and robust zero-shot generalization in tests, but do not provide split metrics explicitly comparing objects/actions known to the pretraining data versus novel ones.",
            "zero_shot_or_few_shot": "Yes — FrankenBot is designed for plug-and-play zero-shot deployment without any additional VLM fine-tuning; reported real-world success rate (zero-shot) ~73%. No few-shot learning curves are provided.",
            "layer_analysis": "None — no layer-wise importance or freezing/ablation of VLM internals is reported.",
            "negative_transfer_evidence": "No measured negative transfer from language pretraining is reported; authors note failure modes when VLM inferences are flawed and that lower-tier/older VLMs degrade performance, and that local small LLM anomaly handlers have higher error rates (i.e., limited local LLMs can harm robustness relative to cloud VLM replanning).",
            "comparison_to_vision_only": "No direct controlled comparison to vision-only pretrained models (e.g., ImageNet-only) is reported. Baselines used are VoxPoser and ReKep (comparative methods), but detailed numeric comparisons for those baselines are not fully enumerated in the paper text.",
            "temporal_dynamics": "No — the paper does not analyze how representations or performance evolve over fine-tuning/training time; the system is used in zero-shot/inference-only mode.",
            "dimensionality_analysis": "None — no intrinsic dimensionality or PCA-style analysis is reported.",
            "uuid": "e1904.0"
        },
        {
            "name_short": "GPT-4.1",
            "name_full": "GPT-4.1 (vision-capable Vision-Language Model used as VLM)",
            "brief_description": "A cloud-scale vision-language model used by FrankenBot as the primary high-level planner, code generator, and replanner; selected by authors for its strong multimodal reasoning performance and used without fine-tuning to synthesize execution trees, monitors, and recovery code.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4.1 (vision-capable VLM)",
            "model_description": "A large multimodal transformer (vision + language) used as an off-the-shelf reasoning and code-generation backend; processes instruction text and visual observations to output task decompositions, Python monitor/handler code, keypoint proposals and executable sequences. In FrankenBot it is queried (typically once per task) to produce multi-threaded executable logic and anomaly handlers.",
            "pretraining_type": "Vision-language pretraining on large-scale image-text and internet text (as described in paper as typical VLM pretraining); exact GPT-4.1 pretraining details are external to this paper.",
            "pretraining_data_description": "Described generally as internet text and 2D image data (no explicit action/motion/robotics datasets described in FrankenBot), thus limited explicit embodied action annotations.",
            "target_task_name": "High-level planning and replanning for robotic manipulation",
            "target_task_description": "Used to parse natural language instructions and initial scene observation into ordered subtasks, generate hierarchical execution code, produce keypoint proposals and monitors, and replanning logic for complex anomalies; interacts with a UR5e arm and RGB-D perception in real-world desktop manipulation tasks.",
            "semantic_alignment": "Authors report that GPT-4.1 (and VLMs generally) contains broad world knowledge but limited direct embodied/spatial priors; FrankenBot uses contextual injection via HIMM and ISP to improve alignment to embodied tasks.",
            "performance_with_language_pretraining": "Authors selected GPT-4.1 as the VLM because it 'delivers nearly the best reasoning performance' for the system; exploratory experiments (Table 5) show newer/more advanced VLMs provide superior overall success rates, motivating GPT-4.1 selection. Exact numeric success rates per model in Table 5 are provided in the paper but not unambiguously formatted in the main text.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "No learning-sample-efficiency data for GPT-4.1 provided; operational metrics show FrankenBot uses ~1 VLM call per task on average when using GPT-4.1.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None reported for GPT-4.1 in this work.",
            "embedding_space_analysis": "None reported for GPT-4.1 in this work.",
            "action_grounding_evidence": "Functional usage: GPT-4.1 produces keypoint proposals and executable code and thus is used to ground language instructions into keypoint-constrained behaviors; the paper provides functional evidence (successful real-world execution and recovery) but no representational-level grounding analyses.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "Authors note transfer from GPT-4.1 to embodied tasks benefits from auxiliary modules (HIMM/ISP/MAH) and depends on strength of VLM reasoning; weaker models produce worse results.",
            "novel_vs_familiar_objects": "Not explicitly evaluated for GPT-4.1 in terms of seen vs unseen objects.",
            "zero_shot_or_few_shot": "Used zero-shot (no fine-tuning) for planning and code generation; system-level zero-shot success reported.",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "Indirect: authors note performance degrades with lower-tier models; no quantified negative transfer instances for GPT-4.1.",
            "comparison_to_vision_only": "No direct comparison in this paper between GPT-4.1 and vision-only pretrained models.",
            "temporal_dynamics": "None.",
            "dimensionality_analysis": "None.",
            "uuid": "e1904.1"
        },
        {
            "name_short": "Local Anomaly Expert",
            "name_full": "Fine-tuned local small language model (Local Anomaly Expert)",
            "brief_description": "A small fine-tuned LLM deployed locally to handle 'rearrange recoverable' anomalies by analyzing scene metadata and inserting/adjusting steps in the skill-call sequence for low-latency recovery, sitting between predefined monitors and cloud VLM replanning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Local fine-tuned small LLM (Anomaly Expert)",
            "model_description": "A lightweight, fine-tuned language model with limited scale and commonsense reasoning capacity used for quick sequence reorganization and local anomaly recovery (e.g., inserting extra locate/grasp retries) based on low-dimensional scene metadata.",
            "pretraining_type": "Text-only pretrained LLM (then fine-tuned locally) — paper identifies it as a small/fine-tuned local language model but does not give exact architecture or pretraining corpus.",
            "pretraining_data_description": "Not specified; described as a small fine-tuned model used to cluster/interpret scene metadata and perform sequence edits.",
            "target_task_name": "Local anomaly detection and recovery during robotic manipulation",
            "target_task_description": "Handles rearrange-recoverable anomalies in real-world desktop manipulation (e.g., partial object drop, mid-task obstruction) by operating on scene metadata and modifying the hierarchical execution tree/skill sequence in real time; runs locally for low latency.",
            "semantic_alignment": "Used to map scene metadata and current instruction to recovery edits; authors note limited commonsense reasoning capacity relative to cloud VLMs, which reduces robustness.",
            "performance_with_language_pretraining": "Table 3 reports Local LLM performance per anomaly class (examples pulled from table): Predictable anomalies — Local LLM SR ≈ 95% with Δt ≈ 2.8s; Recoverable anomalies — Local LLM average SR ≈ 75% with Δt ≈ 4.2s; Complex anomalies — Local LLM average SR ≈ 55% with Δt ≈ 11.1s. (These are the paper's reported numbers in Table 3 under 'Local LLM' column.)",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "No learning-sample-efficiency data provided for the local LLM; it is used as a deployed module and assessed via success-rate/time metrics for anomaly classes.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None.",
            "embedding_space_analysis": "None.",
            "action_grounding_evidence": "Functional evidence: local LLM edits call sequences based on scene metadata (e.g., inserting additional locate/grasp steps), demonstrating procedural grounding of textual anomaly descriptions into action-sequence edits; no representational grounding analysis provided.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "Authors show local LLM handles mid-difficulty anomalies but fails on complex anomalies; as a rule, its limited scale constrains capability and increases error rates relative to cloud VLM replanning.",
            "novel_vs_familiar_objects": "Not evaluated.",
            "zero_shot_or_few_shot": "Local LLM is fine-tuned (not zero-shot); exact number of fine-tuning examples not provided.",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "Authors report higher error rates when relying solely on the local LLM for anomaly handling (qualitative negative effect compared to cloud VLM replanning).",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "None.",
            "dimensionality_analysis": "None.",
            "uuid": "e1904.2"
        },
        {
            "name_short": "VoxPoser",
            "name_full": "VoxPoser (baseline referenced and used for comparison)",
            "brief_description": "A prior method referenced and used as a comparative baseline for composable 3D value maps for robotic manipulation; used as a baseline in FrankenBot experiments (details of per-task numbers not fully enumerated in the paper text).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VoxPoser (baseline)",
            "model_description": "Baseline system (cited) that composes 3D value maps for manipulation; referenced as a comparative prior work. FrankenBot compares overall success and completion time against VoxPoser in the open-vocabulary manipulation experiments.",
            "pretraining_type": "Not detailed in FrankenBot paper (referenced external work).",
            "pretraining_data_description": "Not detailed in FrankenBot paper.",
            "target_task_name": "Robotic manipulation baseline",
            "target_task_description": "Used as a baseline on the same set of desktop manipulation tasks; environment and robot hardware are the FrankenBot testbed (UR5e, RealSense), but FrankenBot's text does not provide per-task numeric results for VoxPoser in the body (only comparative statements).",
            "semantic_alignment": "Not analyzed within FrankenBot beyond being a baseline comparator.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "None in FrankenBot paper.",
            "embedding_space_analysis": "None in FrankenBot paper.",
            "action_grounding_evidence": "No specific analysis in FrankenBot; VoxPoser is cited as prior work that explicitly uses 3D representations for manipulation.",
            "hierarchical_features_evidence": "None in FrankenBot paper.",
            "transfer_conditions": "Not discussed in FrankenBot for this baseline.",
            "novel_vs_familiar_objects": "Not analyzed within FrankenBot.",
            "zero_shot_or_few_shot": "Not reported in FrankenBot.",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "None reported in FrankenBot.",
            "comparison_to_vision_only": "Not provided in FrankenBot.",
            "temporal_dynamics": "None.",
            "dimensionality_analysis": "None.",
            "uuid": "e1904.3"
        },
        {
            "name_short": "ReKep",
            "name_full": "ReKep (Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation)",
            "brief_description": "A referenced baseline (ReKep) used for comparison in FrankenBot experiments; a prior method focused on spatiotemporal keypoint constraints for manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ReKep (baseline)",
            "model_description": "Referenced baseline system that reasons with relational keypoints and spatiotemporal constraints for robotic manipulation; FrankenBot compares its performance against ReKep on the open-vocabulary manipulation benchmark.",
            "pretraining_type": "Not detailed within FrankenBot paper (external reference).",
            "pretraining_data_description": "Not detailed within FrankenBot paper.",
            "target_task_name": "Robotic manipulation baseline",
            "target_task_description": "ReKep evaluated on same desktop manipulation tasks within FrankenBot experiments as a baseline; FrankenBot claims higher success rates but does not include detailed per-task numeric ReKep results in the main text.",
            "semantic_alignment": "Not analyzed within FrankenBot beyond being a baseline comparator.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "None in FrankenBot paper.",
            "embedding_space_analysis": "None in FrankenBot paper.",
            "action_grounding_evidence": "ReKep is referenced as a method that uses keypoint constraints (suggesting action-relevant geometric grounding), but FrankenBot provides no representational analysis.",
            "hierarchical_features_evidence": "None in FrankenBot paper.",
            "transfer_conditions": "Not discussed in FrankenBot for this baseline.",
            "novel_vs_familiar_objects": "Not analyzed within FrankenBot.",
            "zero_shot_or_few_shot": "Not reported in FrankenBot.",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "None reported in FrankenBot.",
            "comparison_to_vision_only": "Not provided in FrankenBot.",
            "temporal_dynamics": "None.",
            "dimensionality_analysis": "None.",
            "uuid": "e1904.4"
        },
        {
            "name_short": "VLA (Vision-Language-Action)",
            "name_full": "Vision-Language-Action (VLA) models (general class)",
            "brief_description": "A class of models created by fine-tuning pretrained VLMs on large embodied/action datasets to directly produce action sequences or policies from multimodal inputs; referenced in FrankenBot as a contrasting approach that aims for end-to-end policy generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Vision-Language-Action (VLA) models",
            "model_description": "General class: models that extend VLM reasoning into direct policy/action generation by fine-tuning on large-scale embodied datasets (vision + language + action). FrankenBot contrasts its modular inference-driven approach (no VLM fine-tuning) with the data-hungry VLA approach.",
            "pretraining_type": "Typically two-stage: initial vision-language pretraining followed by fine-tuning on embodied action datasets (paper summarizes this class; not used directly in FrankenBot experiments).",
            "pretraining_data_description": "Described in related work as requiring large cross-modal embodied datasets (image/scene observations paired with action trajectories) — includes action sequences, motion trajectories, but FrankenBot notes two-layer VLM/VLA approaches demand extensive cross-modal training data.",
            "target_task_name": "End-to-end robotic control / policy generation",
            "target_task_description": "VLA models aim to map language + visual input to continuous action sequences; in literature they are applied to both simulated and real-robot manipulation tasks (not evaluated directly in FrankenBot experiments).",
            "semantic_alignment": "FrankenBot states VLA approaches more directly inject action priors via fine-tuning but require large embodied datasets; no empirical comparison in this paper.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Paper notes VLA methods require extensive cross-modal training data (high sample demand) but provides no quantitative sample-efficiency comparisons.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None in FrankenBot.",
            "embedding_space_analysis": "None in FrankenBot.",
            "action_grounding_evidence": "General claim in related work: VLA models can learn to produce low-level actions, implying grounding of action semantics, but FrankenBot does not present direct evidence from VLA models.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "FrankenBot argues transfer from web-scale VLM to embodied control is limited without embodied fine-tuning; VLA methods seek to close that gap via task-specific data.",
            "novel_vs_familiar_objects": "Not analyzed in FrankenBot.",
            "zero_shot_or_few_shot": "VLA approaches generally require fine-tuning; zero-shot performance varies by method (not reported here).",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "Not discussed specifically beyond noting high data needs and impracticality for plug-and-play deployment.",
            "comparison_to_vision_only": "Not directly compared in FrankenBot.",
            "temporal_dynamics": "None.",
            "dimensionality_analysis": "None.",
            "uuid": "e1904.5"
        },
        {
            "name_short": "Grounding-DINO",
            "name_full": "Grounding-DINO (used as an example basic tool algorithm)",
            "brief_description": "A referenced off-the-shelf detection/grounding tool cited as an example atomic operation in the execution tree (HET) for perceptual primitives; FrankenBot lists it among callable basic tool algorithms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grounding-DINO (as callable atomic operation)",
            "model_description": "Referenced as an example of a perception primitive that can be invoked as an atomic node in FrankenBot's hierarchical execution tree (e.g., to detect or ground objects), but FrankenBot does not train or analyze Grounding-DINO in this work.",
            "pretraining_type": "Not specified in FrankenBot (external model).",
            "pretraining_data_description": "Not specified in FrankenBot.",
            "target_task_name": "Perception/grounding primitive for manipulation",
            "target_task_description": "Invoked as an atomic operation (e.g., object detection/grounding) within the hierarchical execution tree; used to supply scene metadata.",
            "semantic_alignment": "Not analyzed within FrankenBot; used as part of the perception stack.",
            "performance_with_language_pretraining": null,
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "None.",
            "embedding_space_analysis": "None.",
            "action_grounding_evidence": "Used functionally to produce perceptual signals (detections/keypoints) that are consumed by higher-level VLM planning, but no representational grounding analysis provided.",
            "hierarchical_features_evidence": "None.",
            "transfer_conditions": "Not discussed.",
            "novel_vs_familiar_objects": "Not analyzed in FrankenBot.",
            "zero_shot_or_few_shot": "Not applicable within FrankenBot.",
            "layer_analysis": "None.",
            "negative_transfer_evidence": "None reported.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "None.",
            "dimensionality_analysis": "None.",
            "uuid": "e1904.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "VoxPoser: Composable 3D value maps for robotic manipulation with language models",
            "rating": 2
        },
        {
            "paper_title": "ReKep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "PointVLA: Injecting the 3D world into vision-language-action models",
            "rating": 2
        },
        {
            "paper_title": "Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation",
            "rating": 1
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning",
            "rating": 1
        },
        {
            "paper_title": "Guiding long-horizon task and motion planning with vision language models",
            "rating": 1
        }
    ],
    "cost": 0.01985425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models
24 Jun 2025</p>
<p>Shiyi Wang 
School of Future Technology
South China University of Technology</p>
<p>Wenbo Li 
School of Software Engineering
School of Intelligent Engineering
South China University of Technology
3 Shien-Ming Wu</p>
<p>South China University of Technology</p>
<p>Yiteng Chen 
School of Software Engineering
School of Intelligent Engineering
South China University of Technology
3 Shien-Ming Wu</p>
<p>South China University of Technology</p>
<p>Qingyao Wu 
School of Software Engineering
School of Intelligent Engineering
South China University of Technology
3 Shien-Ming Wu</p>
<p>South China University of Technology</p>
<p>Huiping Zhuang hpzhuang@scut.edu.cn 
FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models
24 Jun 202520902842429CB8E495DA7FCBB84A3814arXiv:2506.21627v1[cs.RO]
Developing a general robot manipulation system capable of performing a wide range of tasks in complex, dynamic, and unstructured real-world environments has long been a challenging task.It is widely recognized that achieving human-like efficiency and robustness manipulation requires the robotic brain to integrate a comprehensive set of functions, such as task planning, policy generation, anomaly monitoring and handling, and long-term memory, achieving high-efficiency operation across all functions.Vision-Language Models (VLMs), pretrained on massive multimodal data, have acquired rich world knowledge, exhibiting exceptional scene understanding and multimodal reasoning capabilities.However, existing methods typically focus on realizing only a single function or a subset of functions within the robotic brain, without integrating them into a unified cognitive architecture.Inspired by a divide-and-conquer strategy and the architecture of the human brain, we propose FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that achieves both comprehensive functionality and high operational efficiency.Our framework includes a suite of components, decoupling a part of key functions from frequent VLM calls, striking an optimal balance between functional completeness and system efficiency.Specifically, we map task planning, policy generation, memory management, and low-level interfacing to the cortex, cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and design efficient coordination mechanisms for the modules.We conducted comprehensive experiments in both simulation and real-world robotic environments, demonstrating that our method offers significant advantages in anomaly detection and handling, long-term memory, operational efficiency, and stability -all without requiring any fine-tuning or retraining.</p>
<p>Introduction</p>
<p>Developing general robotic manipulation systems capable of performing tasks in complex, dynamic, and unstructured real-world environments has long been a challenging task.Recently, Vision-Language Models (VLMs), through large-scale pretraining, have gained rich world knowledge, demonstrating significant potential in robotic manipulation tasks.VLMs not only handle complex semantic and visual information but also enable more robust reasoning and planning across diverse In the block stacking task, FrankenBot first decomposes the task via cortex-mapped planning, then generates constrained motion policies through cerebellum-mapped optimization (blue).Concurrently, the hippocampus-mapped memory retrieves past stacking strategies (green), while brainstem-mapped reflexes (red) monitor execution stability and trigger recovery behaviors when anomalies are detected.</p>
<p>scenarios, significantly reducing the reliance on high-quality action data.As a result, an increasing number of studies have explored leveraging VLMs as the core of the "robotic brain", applying them to functions such as task planning and error detection and recovery.Although VLMs excel in dialogue and static visual understanding, their pretraining primarily relies on internet text and 2D image data, and there remains a significant gap between their capabilities and the demands required for real robots to perform complex embodied tasks in 3D space.This capability gap makes it challenging for VLMs to directly adapt to dynamic environments and complex interaction requirements during embodied task deployment.</p>
<p>Existing VLM-driven robotic manipulation methods have already achieved the following: task planning, where VLM parses natural language instructions and generates high-level action sequences; error detection and correction, where VLM detects anomalies during task execution and replans, effectively correcting execution deviations or environmental anomalies; fine-grained action generation, where representations are first extracted, and VLM generates corresponding constraints, which are then solved to obtain the robot's action sequence.Another mainstream approach combines VLM with the Vision-Language-Action (VLA) model to build a multi-layered "robotic brain," where the upper layer provides high-level reasoning through VLM, and the lower layer handles low-level planning and execution through VLA.</p>
<p>While existing methods have advanced specific robotic brain functions individually, they fail to integrate these into a complete system.Current two-layer architectures demand extensive cross-modal training data and remain impractical for plug-and-play deployment.Simply combining these discrete functions would severely degrade performance, as each VLM call requires several seconds -making efficient multifunctional operation impossible.For robust real-world performance, robots need human-like brain functions (planning, error correction, and memory) operating synergistically.This work addresses the core challenge: how to build an efficient, brain-inspired framework with complete functionality that requires minimal (ideally single) VLM calls per task without additional training.</p>
<p>Inspired by the brain's organizational principles, we propose a VLM-driven brain-morphic robotic framework that strategically decouples core functions from frequent VLM calls.Our architecture mirrors neurobiological structures -cortical functions map to VLM-based planning and reasoning, cerebellar roles to execution control and anomaly handling, hippocampal functions to memory management, and brainstem operations to low-level hardware interfaces.Through multi-granular skill libraries, hierarchical anomaly handling, and parallel execution modules, the framework achieves comprehensive functionality while minimizing VLM interactions.This biologically-inspired design enables efficient task execution with typically just one VLM call per task, balancing system capabilities with operational performance through optimized module coordination.</p>
<p>Our method offers several key advantages: (1) Plug-and-play: The framework is driven by VLM, and no additional training is required for deployment.It can be quickly set up by simply connecting predefined control interfaces; (2) A comprehensive robotic brain: It incorporates essential functions such as task planning, error detection and correction, and long-term memory, thus meeting the core requirements for robotic task execution in complex environments; (3) Efficient operation: By efficiently leveraging the capabilities of VLM, the framework requires only a single VLM call during each task execution, significantly reducing computational time and resource consumption.</p>
<p>Our work introduces a novel VLM-driven brain-like architecture with the following contributions: First, we present a plug-and-play brain-like architecture that requires no additional training for deployment.It simply needs to implement a unified interface to call existing VLM services to drive the robot to perform complete operations.Additionally, a three-level memory mechanism has been designed, which includes a learnable multi-granular skill library that encapsulates commonly used motion primitives and composite skills in a hierarchical manner.It can adjust skill combinations based on historical task executions, improving VLM planning efficiency.Third, we introduce a multi-level anomaly handling framework, which enables a three-tier anomaly handling system from local to cloud, accurately allocating processing resources based on the complexity of the task execution's anomalies.By combining local strategies with VLM-based judgments, this framework allows for rapid recovery and intelligent error correction.Finally, the framework enables the VLM to generate multi-threaded executable code in a single call, allowing task execution and anomaly detection to run in parallel, ensuring that in most cases, only a single VLM interaction is required for each task.</p>
<p>Related Works</p>
<p>Brain-like structures in Robotic Manipulation Tasks.A robotic system executing manipulation tasks requires a robotic brain analogous to the human brain: it must process multimodality inputs, integrate world knowledge and reasoning capabilities, and output action sequence that cover all the functionality needed for manipulation task.The most straightforward way to build a robotic brain is an end-to-end, data-driven approach: by collecting large-scale datasets of manipulation tasks and adjusting model architectures, researchers fine-tune pre-trained VLMs into Vision-Language Action (VLA) models capable of generating action sequences directly from instructions and scene observations in single inference [1,2,3,4,5,6,7,8,8,9,10,11,12,13,14,15,16], effectively creating a fully functional robotic brain.Building on this paradigm, other works have explored more structured, brain-like designs.Some introduce hierarchical architectures such as adopt a System 1/System 2 architecture by running parallel high-frequency and low-frequency inference streams [17], while others use a two-tier VLM/VLA design, assigning high-level reasoning to the VLM and low-level action generation to the VLA [18][19].Others layer the reasoning process temporally [20], such as having the model sequentially infer task plans, predict affordances, and then produce action sequences across multiple inference steps.</p>
<p>Vision-Language Models for Robotics.Vision-Language Models (VLMs) have become increasingly widespread in robotic manipulation, owing to the rich scene comprehension and high-level commonsense reasoning they acquire from large-scale pretraining.Most prior work concentrates on leveraging pretrained VLMs for task planning and high-level reasoning in manipulation tasks [21][22] [23].</p>
<p>Another branch of research pushes VLM reasoning into more fine-grained policy generation, beyond task planning [24][25] [26][27] [28][29][30] [31].Because anomaly detection and recovery are critical for robust execution, some studies have also explored VLM-driven monitoring and anomaly-handling strategies during task execution [32][33].</p>
<p>In Appendix A.4, we discuss the Connections and Distinctions between our approach and existing work in these fields.</p>
<p>Method</p>
<p>Our framework is outlined in Fig. 2, with the VLM forming the "cerebral cortex" component central to our system.In Sec.3.1, we provide a concise formalization of the manipulation tasks and our overall architecture.Sec.3.2 details the "cerebellum" component: Hierarchical Dynamic Feedback Local Control mechanism.Sec.3.3 introduces the complementary "cerebellum" module, the Multi-level Anomaly Handling mechanism.Sec 3.4 presents the " temporal lobe-hippocampus complex" functionality Hierarchical Incremental Memory Management which supports cross-task information retrieval and decision enhancement.</p>
<p>Problem Formulation</p>
<p>In this study, we consider the task of embodied agents executing natural language instructions in real-world physical environments.Given a high-level instruction ℓ (e.g., "Put the apple on the table into the refrigerator") and an initial scene observation O(0), the agent needs to decompose and execute it through multimodal perception and dynamic task planning.Specifically, we employ a VLM as a high-level task parser to decompose ℓ into an ordered set of subtasks {ℓ i } N i=1 (e.g., "Locate the apple", "Grasp the apple", "Open the fridge door", "Place the apple").Each subtask ℓ i is modeled as a task tree composed of multiple nodes
V i = {v j } Mi j=1
, where M i is the number of nodes of subtask ℓ i , with node design based on a parameterized state-transition representation.</p>
<p>Each node v j is defined as a 3-tuple:
v j = (T j , A j , C j )
where:</p>
<p>• T j represents the node description text (e.g., "Grasp the apple") • A j is the parameter list containing execution-specific parameters (e.g., target object coordinates, gripper force parameters) • C j : A → V i defines the child node mapping function that maps the parameter set A to subsequent node set V i , enabling dynamic node selection During execution, the agent continuously acquires environmental observations O(t) through an RGB-D camera and converts them into low-dimensional scene metadata s t using predefined monitors (discussed later in Sec.3.3).The node execution engine performs the following at each step: 1) Extract current parameters from A i , 2) Execute the mapping function C i , 3) Transit to the next node according to C i .</p>
<p>To ensure robustness, an anomaly detection module E continuously computes state deviation δ t = E(s t , u t ), where u t is VLM-defined scene predictions.When δ t &gt; τ i , the system either initiates VLM-predefined recovery procedures or requests the VLM to generate adjustment strategies (e.g., "If the apple is occluded, first remove the obstructing object").This structured node modeling enables the system to both follow initial plans and adapt to environmental uncertainties through parameterized branching and anomaly handling, achieving reliable task execution in unstructured environments.</p>
<p>HiErarchicaL dynamic feedback Local cOntrol(HELLO)</p>
<p>In the field of robotics, the optimization of task execution strategies typically adheres to the modular design principle.The prevalent engineering practice employs a divide-and-conquer approach, hierarchically breaking down complex tasks into combinations of simpler subtasks.This decomposition not only reduces system implementation complexity but also enhances reusability, maintainability, and scalability.To adapt to all diverse task requirements and ensure code reusability, HET can benefit from Turing Complete structure [34].The structure of HET is an enhanced preorder traversal finite state machine -a tree composed of various node types, supporting arbitrary jumps between nodes and dynamic structural modifications.Node types include: atomic operations (e.g., moving to a position, capturing an image, invoking basic tool algorithms like Grounding-DINO), composite operations (e.g., grasping an object), conditional branches (IF operations), jump operations (e.g., jump to Node A), exit operations (terminating the traversal loop).</p>
<p>By logically combining these functions, the module effectively expresses complex task-handling logic, achievable through proper prompt engineering for the VLM.This approach improves function reuse rates, reduces the number of calls to large models, and enhances task execution efficiency.</p>
<p>Multi-level Anomaly Handling(MAH)</p>
<p>In robotic manipulation tasks, anomaly handling refers to the timely detection and correction of unexpected events or disruptions during execution, including deviations from the planned actions, accidental object drops, environmental disturbances, and other unanticipated scenarios.Formally, given an initial execution tree V i = {v j } Mi j=1 , the system computes low-dimensional representation of current observation s t = f (O(t)), where f : R H×W ×4 → R Pt×3 , which are the spatial positions of the keypoints proposed by the VLM, and computes the expected current scene state u t from VLM-defined functions.Then the system applies an anomaly detection function
δ t = E s t , u t ,
where δ t &gt; τ i indicates a detected anomaly.Then the correction module then update the call sequence, ensuring task continuity and success.Anomaly handling is essential for improving a robot's robustness and reliability in dynamic, unstructured environments.</p>
<p>There exist myriad approaches to anomaly detection and correction, as this is a broad and extensively research area.Different methods vary in their anomaly-handling capabilities, the types and scope of anomalies they can address, and the resources required for their execution.Striking a balance between success rate and resource cost in anomaly handling is a critical challenge.</p>
<p>Through extensive statistical analysis of execution data in our desktop experimental platform, we categorize anomalies into three difficulty levels: predictable anomalies, which can be rapidly detected and corrected using predefined rules; rearrange recoverable anomalies, which can be resolved through localized adjustments to the current task sequence without full replanning; and complex anomalies, whose degree of anomaly complexity or workspace changes necessitate global replanning via the VLM.</p>
<p>Based on this insight, we propose a multi-level anomaly handling framework consisting of three components.First, predefined monitors are generated by the VLM at the start of the pipeline to address Predictable Anomalies.These handlers continuously monitor key variables in the scene metadata and detect any predefined anomalies.Second, a lightweight, fine-tuned local anomaly expert implemented as a small language model handles rearrange recoverable anomalies.When an anomaly escapes the predefined handlers, this expert attempts to adjust the call sequence based on the scene metadata.Third, cloud-based VLM replanning tackles complex anomalies that neither predefined handlers nor local adjustments can resolve.The entire framework is implemented as hierarchical executable code generated by the VLM, which runs in a separate thread from the main control logic and invokes the underlying interfaces.We discuss these three components in more detail in AppendixA.6.</p>
<p>In implementation, the predefined monitors are realized as executable code generated directly by the VLM, while both the local anomaly expert and the VLM replanning function are exposed as interfaces within the skill library.Additionally, the VLM produces a scheduler to orchestrate all three modules into a complete multi-level anomaly handling framework, which executes in a separate thread at runtime.The multi-level anomaly handling framework stratifies anomalies by difficulty, enabling rapid recovery of simple faults and cautious replanning for complex failures, mirroring human decision patterns and striking an effective balance between success rate and response latency.As an adaptive paradigm, it dynamically routes each anomaly to the most suitable processing tier based on its predictability and complexity, achieving both low-latency local recovery and high-robustness global corrections.Its hierarchical design is inspired by the human nervous system's feedback loops (spinal cord → cerebellum → cortex), ensuring overall performance while maintaining efficiency and reliability.</p>
<p>Hierarchical Incremental Memory Management (HIMM)</p>
<p>The operational efficiency of current AI agent systems is constrained by two key factors: First, frequent calls to Vision-Language Models (VLMs) create a significant performance bottleneck.In typical task execution, approximately 43% of the latency stems from serialized VLM queries (see ablation).Second, task trajectory analysis reveals that up to 48% of large model outputs consist of functionally equivalent but textually varied duplicate function generations (see ablation).Thus, an effective function reuse mechanism can reduce large model invocation frequency, leading us to propose the Hierarchical Incremental Memory Management (HIMM) module.</p>
<p>HIMM manages a Hierarchical Memory Module (HMM) and an Incremental Skill Pool (ISP).The HMM consists of short-term memory, medium-term memory, and lifelong memory.It continuously records function invocations and anomaly handling logs in short-term memory, and periodically summarized by a locally deployed large model into medium-term memory (frequently called functions) and lifelong memory (prompt-level optimizations for embodied intelligence based on task execution experience).</p>
<p>Hierarchical Memory Module (HMM): HMM employs a three-tier structure for knowledge accumulation -short-term memory acts as a rolling-window cache (capacity ⩽ 10 entries), recording raw task trajectories and exception logs in real time; medium-term memory uses a lightweight local model to cluster high-frequency function templates (invoked ⩾ 3 times/hour) via LLM summarization every 10 minutes; lifelong memory stores human-verified embodied prompt optimizations (e.g., "keep the wrist horizontal during robotic arm grasping").</p>
<p>Incremental Skill Pool (ISP):</p>
<p>The ISP implements a two-tier skill system: atomic skills (e.g., move_to(x,y,z)) as indivisible action primitives, and composite skills (e.g., package_sorting()) described via directed acyclic graphs.Its knowledge accumulation follows strict quality control: when medium-term memory detects a function invoked over five times, semantic normalization eliminates superficial variations, and only those with ⩾ 90% test coverage are added.This design offers two key advantages: First, a dynamic prompt mechanism automatically injects skill context, such as appending available skill descriptions and relevant memory scenarios to each VLM call.Second, an exception-driven update strategy logs failures in short-term memory while triggering revalidation of related skills.</p>
<p>Tests on robots (See Sec.4.3) show HIM reduces generated code size by 41%, reduces VLM calls by 27%.This dual-channel optimization of memory and skills provides a verifiable solution to overcoming AI agents' "regeneration trap."</p>
<p>Experiments</p>
<p>In this section, we perform experiments to investigate four main research questions: (1) What are the performance and efficiency metrics of our system on open-vocabulary manipulation tasks across various real-world scenarios?(Sec.4.1); (2) Can our system achieve the anomaly detection and recovery capabilities and the cross-task long-term memory performance claimed in Section 3? (Sec.4.2); (3) What is the quantitative contribution of each functional module to the overall system performance?(Sec.4.3); and (4) How significantly do individual modules contribute to task failures, and what are their main error sources and failure patterns?(Sec.4.4).</p>
<p>To evaluate our method's real-world performance, we deployed a desktop experimental setup in an actual environment: an Intel RealSense D435i RGB-D camera was mounted above the desktop to capture scene observations, and a UR5e 6-DoF robotic arm equipped with a 1-DoF gripper performed the manipulation tasks.Our experiments are conducted on an Intel(R) Core(TM) i7-14700KF CPU and an NVIDIA RTX A6000 GPU.</p>
<p>Real-World and Open-Vocabulary Manipulation</p>
<p>Table 1: Quantitative evaluation of manipulation performance across 10 task instances per method.Success rates (successful trials/total trials) and average execution times are reported.FrankenBot achieves significantly higher success rates (73% overall), demonstrating robust task adaptation capabilities.We designed and selected ten manipulation tasks derived from everyday real-world scenarios, covering simple move tasks to more challenging tasks that include multiple steps with high failure probability or require cross-task contextual memory.For each task, we ran ten independent trials, randomly initializing the poses of all task-relevant objects in the scene for each trial.Performance was evaluated using two metrics: task success rate and completion time.We chose VoxPoser and ReKep as our baselines for comparison.</p>
<p>Task</p>
<p>Table 1 details the quantitative results.We observe that our method demonstrates robust zero-shot generalization, delivering particularly impressive performance on long-horizon tasks, with significant gains over the baselines on both success rate and completion time.Unlike fixed anomaly-handling schemes, where the task is partitioned into rigid stages and errors simply trigger a rollback to the previous stage, our multi-level anomaly-handling mechanism adaptively allocates appropriate resources to different failure modes, offering a far more flexible and powerful paradigm.This design equips our system with both exceptional recovery capabilities and, correspondingly, high execution efficiency, in full agreement with the analysis in Section 3.Moreover, we find that our approach exhibits cross-task foresight, leveraging information accrued during prior task executions to guide current operations.This benefit originates from our multi-level memory architecture, which actively retrieves relevant memories at each execute to support decision-making.We will discuss the importance of these components in our framework's performance in the ablation study of Sec 4.3.</p>
<p>Anomaly Handling and Cross-Task Memory</p>
<p>Prior to formal evaluation, we conducted foundational experiments to establish the natural occurrence rates of different anomaly classes in real-world manipulation scenarios.Five representative tasks were selected spanning household activities (block stacking, drawer closing, tea pouring, object sorting, and pen reorienting), with each task executed across 10 trials under varying environmental conditions.</p>
<p>The initial analysis revealed a strongly skewed natural distribution -approximately 70% predictable anomalies (largely object pose deviations and computer vision inaccuracy), 20% recoverable anomalies, and only 10% complex anomalies (system-level failures or inappropriate planning), as is  2. While this reflects real-world frequencies, it presents challenges for comprehensive system evaluation as complex cases become statistically insignificant.We therefore designed the test distribution to 5:3:2 through controlled anomaly injection, with 10 trials per anomaly type.</p>
<p>As is shown in Table 3, performance was evaluated using recovery success rate and time penalty.The baselines we chose are naive rollback, full remote VLM replan and local LLM replan.</p>
<p>Ablation Study</p>
<p>We conducted ablation studies to evaluate the contribution of each key component to overall system performance.These experiments employed the same task set and evaluation metrics (success rate and average completion time) as defined in Sec.4.1 and 4.2.Given that the functionality of MAH was already validated in Sec 4.2, we focused on two variants: (1) No hierarchical memory module:</p>
<p>The VLM receives only the current instruction and scene observation, without access to historical experience prompts.(2) No execution tree, where the agent can only execute the code from serial function series.(3) No incremental skill library: The library remains static after initialization, prohibiting dynamic expansion or functional reuse.</p>
<p>To isolate the impact of each component, we conducted 10 consecutive block-stacking trials for 3 runs under two ablated conditions: (1) Memory-less execution: The system processed only real-time visual observations and immediate instructions, with no access to hierarchical memory.This tested the baseline capability without experiential learning.(2) Static skill library: The basic skill library remained frozen during trials, preventing dynamic updates from task progression.</p>
<p>System Error Breakdown</p>
<p>By virtue of the interpretability and modular design of our framework, we can pinpoint and quantify component-wise error rates, as shown in Fig. 3. First, although the error rate of the VLM policy generation module is generally acceptable, its performance still depends on the VLM's inference capabilities.We found that our method relies on more powerful VLMs such as GPT-4.1.The Hierarchical Memory Module, by contrast, is highly stable, as it only needs to maintain simple task and scene descriptions.</p>
<p>While predefined condition-triggered detectors work well for simple cases, their fixed logic struggles with complex real-world situations where anomalies often need more flexible responses.Attempting to use a local lightweight local LLM for anomaly handling, however, leads to noticeably higher error rates, owing to its limited scale and commonsense reasoning capacity.In comparison, VLM-based error correction leveraging a cloud-scale model to replan and reexecute demonstrates greater robustness, though at the cost of increased execution time.</p>
<p>Overall, apart from these key modules, the remaining components have a minimal impact on system stability.</p>
<p>Conclusion</p>
<p>In this work, we present a VLM-driven, brain-like framework for robotic manipulation that realizes a functionally comprehensive robotic brain while maintaining high operational efficiency.Our method offers several key advantages: first, the framework is plug-and-play -deployment requires only implementation of the defined interfaces, with no additional training; second, it constitutes a functionally complete robotic brain by integrating core features such as task planning, anomaly detection and correction, and long-term memory; and finally, it operates with high efficiency, performing nearly every task with just a single VLM call.Moreover, the framework's modules are highly decoupled, enabling rapid integration of advanced techniques-such as VLM applications in robotics, code generation, and RAG (retrieval-augmented generation) techniques-and, if finer-grained scene perception and motion control are provided at the low-level interfaces, the framework will immediately realize corresponding performance gains.While advantageous, our method still has several limitations.In  To investigate how model choice affects overall system performance, we designed an exploratory experiment using all tasks from the comparative study in Sec.4.1.Experimental results are presented in Tab. 5. We observe that lower-tier models achieve overall acceptable performance, demonstrating our method's robustness to model variation, yet more advanced, newer models deliver clearly superior results across both evaluation metrics.This finding conveys two insights.First, our approach will continue to benefit as VLM reasoning capabilities improve in the future.Second, there is a promising research direction in designing system architectures that more effectively structure and guide the VLM's reasoning process so as to lessen dependence on its peak performance.For these reasons, we ultimately selected GPT-4.1 as the VLM for our method: it delivers nearly the best reasoning performance while costing only about twenty percent of o3.</p>
<p>RealSense D435i Camera UR5e Arm Experimental Props</p>
<p>A.6 More details about components of Multi-level Anomaly Handling (MAH)</p>
<p>Predefined Monitors: Inspired by prior work that uses constraint code to reason about tracked elements in the scene, our predefined monitors follow a similar structural approach.During the planning phase, we invoke the VLM to infer a set of the most common and easily rule-based anomalies (e.g., "object unexpectedly moved").For each anomaly pattern, the VLM automatically generates two Python routines: a monitor that continuously observes key variables in the scene metadata (such as object position), and a handler that, upon anomaly detection, executes a predefined rollback or compensation strategy (for example, re-localizing the object via vision and retrying the grasp).At runtime, the monitor thread runs in parallel with the control thread, enabling low latency, localized handling of predictable anomalies.</p>
<p>Local Anomaly Expert: When an anomaly is not caught by the predefined monitors, the system switches to a local anomaly expert, which is a fine-tuned small language model.The expert analyzes scene metadata and the current task instruction to infer the anomaly cause (for example, "the bottle count changed from one to two").It then determines whether the anomaly falls within its "sequencereorganization" recovery scope: if so, it inserts or adjusts steps in the skill-call sequence (for example, transforming "locate object → move → grasp" into "locate object → move → grasp → locate object → move → grasp"); if not, the anomaly is escalated to the VLM replanning module.This mechanism bridges the gap between predefined rule-based recovery and VLM full replanning, achieving a balance between response speed and anomaly-handling capability.</p>
<p>VLM Replanning: For complex anomalies that exceed the local expert's capabilities, we encapsulate the current scene state and task instruction into a prompt for the VLM, which then generates a corrected executable code sequence.Although this procedure introduces higher latency, it ensures system robustness and correctness in the most extreme or highly uncertain scenarios.</p>
<p>Figure 1 :
1
Figure1: FrankenBot integrates task planning, policy generation, anomaly handling, and long-term memory into a unified VLM-driven cognitive architecture.In the block stacking task, FrankenBot first decomposes the task via cortex-mapped planning, then generates constrained motion policies through cerebellum-mapped optimization (blue).Concurrently, the hippocampus-mapped memory retrieves past stacking strategies (green), while brainstem-mapped reflexes (red) monitor execution stability and trigger recovery behaviors when anomalies are detected.</p>
<p>Figure 2 :
2
Figure 2: Overview framework.Given real-time visual observations and instruction, the VLM dynamically generates execution policies and anomaly handling pre-defined logic that optimally selects and sequences modular skills from the Incremental Skill Pool.</p>
<p>Figure 3 :
3
Figure 3: Pie chart of error type distribution</p>
<p>Figure 4 :
4
Figure 4: Execution Snapshot for Experimental Tasks</p>
<p>To systematically realize these features, we propose the HiErarchicaL dynamic feedback Local cOntrol(HELLO) module.It consists of a Hierarchical Execution Tree (HET) and Multi-level Anomaly Handling (MAH, discussed later in Sec.3.3).The raw content is generated by a VLM synthesizing user instructions, initial environmental observation data O(0), and the memory module (see Sec. 3.4), then parsed by a grammar parser into HET and MAH.</p>
<p>Table 2 :
2
Natural Anomaly Occurrence Statistics Across Tasks
Task (Difficulty)TrialsTotalAnomaly Type CountAvg. perAnomalies Predictable Recoverable ComplexTrialBlock Stacking (Easy)101210 (83.3%)2 (16.7%)0 (0%)1.2Drawer Closing (Easy)101713 (76.5%)3 (17.6%)1 (5.9%)1.7Tea Pouring (Hard)104728 (59.6%)12 (25.5%)7 (14.9%)4.7Button Pressing (Easy)1098 (88.9%)1 (11.1%)0 (0%)0.9Pen Reorienting (Medium)102517 (68.0%)6 (24.0%)2 (8.0%)2.5Total5011076 (69.1%)24 (21.8%)10 (9.1%)2.2</p>
<p>Table 3 :
3
Performance comparison of anomaly handling strategies.SR: Success Rate, ∆t: Time penalty (recovery time -nominal time)
ClassAnomaly Example Naive Rollback Full Replan Local LLM MAH (Ours)SR∆tSR∆t SR ∆tSR∆tObject displacement 45%0.7s100% 11.2s 95% 2.8s 100% 0.2sSensor noise50%0.8s100% 10.8s 90% 3.1s 100% 0.3sPredictableMinor pose deviation 40%0.9s95% 12.5s 85% 3.5s 95% 0.4sExpected collision55%0.6s100% 9.7s 92% 2.6s 100% 0.3sTemporary occlusion 35%1.0s90% 13.8s 80% 4.0s 90% 0.5sAverage45%0.8s97% 11.6s 88% 3.2s 97% 0.3sGripper slip25%4.7s95% 14.5s 80% 3.7s 95% 2.6sRecoverablePartial object drop30%4.3s90% 15.1s 75% 4.2s 90% 2.9sMid-task obstruction 20%5.0s85% 16.8s 70% 5.0s 85% 3.5sAverage25%4.7s90% 15.5s 75% 4.3s 90% 3.0sComplexTopology change Cross-task conflict5% 0%N/A N/A65% 29.8s 60% 10.2s 80% 19.5s 55% 32.1s 50% 12.0s 75% 22.3sAverage3%N/A60% 31.0s 55% 11.1s 78% 20.9sdemonstrated in Table</p>
<p>Table 4 :
4
Ablation study of system components (lower is better).The evaluation metrics include success rate, computational cost (VLM calls and code size), and detailed time breakdown.Time is split into VLM/LLM Generation (prompt processing and planning) and Execution (physical operation time).Results are reported over 3 runs different seeds.
ConfigurationSuccessVLM Code SizeTime (s)RateCalls(LOC)Generation ExecutionFull Model70% ±10%1.1589.212.1w/o HMM26.7% ±4.71%5.78260.268.4w/o Execution Tree63.3% ±4.71%1.311212.113.3w/o ISP73.3% ±12.47%1.56713.815.410.9%8.9%7.2%28.8%3.0%OthersVLM as policy generatorHierarchical Memory Module58.4%VLM as Anomaly HandlerLocal LLM Anomaly HandlerPredefined Anomaly Handler</p>
<p>Table 5 :
5
Exploratory study of VLM choice effects on system performance
ModelInference Validity Rate (%) Total Success Rate (%)o38773GPT-4.18471GPT-4o mini8165gpt-4-vision-preview8063
A Technical Appendices and Supplementary Material A.1 Broader ImpactsFrankenBot can be used in manufacturing to improve the accuracy and flexibility of automated production and reduce repetitive labor.FrankenBot enhances robot's capabilities, but it could be repurposed for negative applications such as advanced autonomous weapon development or pose safety risks if VLM inferences are flawed.To mitigate these hazards, we recommend incorporating human-in-the-loop verification for high-risk tasks and establishing deployment monitoring and feedback mechanisms to ensure responsible use.A.2 Extended Discussion of Limitations and Future WorksWhile advantageous, our method still has several limitations.The multi-threaded executable code generation depends on the VLM's strong reasoning capabilities, and our experiments show that it only maintains ideal performance when using newer versions of the VLM.Fine-grained scene understanding and low-level motion control fall outside the scope of this work and instead rely entirely on low-level interface implementations, which may become performance bottlenecks in real-world applications.Our multi-level anomaly handling system is based on simplistic assumptions about task setups-while it performs well in relatively structured scenarios (e.g., desktop manipulation), its effectiveness may degrade significantly in more complex environments.Finally, because we propose only the framework, the current memory management mechanism is rather rudimentary, and long-duration, consecutive task executions may suffer from information redundancy or forgetting.The rapid advances in related fields have opened up several exciting directions for future work.One active area is to collect large-scale embodied task-related data and perform domain-adaptive finetuning of pretrained large VLMs, in order to endow them with spatial reasoning capability, domain priors, and semantic grounding capability better suited to robotic manipulation.Retrieval-augmented generation (RAG) techniques likewise constitute a vast and dynamic research domain, the latest advances in this field warrant exploration for applications in robot long-term memory.Finally, in the Vision-Language-Action (VLA) arena, gathering larger embodied datasets and designing more advanced architectures to achieve more robust, generalizable low-level motion control-and rapid handling of execution anomalies to cope with changing scenarios-also represents a highly promising research direction.A.3 Hardware SetupAs shown in Fig.4, we deploy a UR5e 6-DoF arm on our desktop platform, outfitted with a steppermotor-driven 1-DoF gripper for object grasping.Robot control from the host PC is implemented via URScript in conjunction with the RTDE interface, supporting a theoretical maximum communication rate of 500 Hz.Scene perception relies solely on the depth and color streams of a single Intel RealSense D435i RGB-D camera, demonstrating the low sensor-hardware requirements of our approach in practical deployment.A.4 Connections and Distinctions between FrankenBot and existing worksBrain-like structures in robotic manipulation tasks.Our method differs fundamentally from existing approaches: whereas prior work relies on data collection and fine-tuning to transform a pre-trained VLM into a VLA capable of end-to-end action sequence inference, our approach retains the VLM at its core without any additional training.Instead, we ground the VLM's reasoning in embodied tasks through a suite of highly structured modules such as a skill library, multi-level anomaly handling, and multi-level memory to construct a fully functional and efficiently robotic brain.Vision-Language Models for Robotics.The key difference between our method and VLMs for robotics is that, while existing methods typically address only a single "brain" module, such as planning, policy generation, or fault recovery, our framework realizes a more complete, brain-inspired VLM-driven manipulation system that integrates all major capabilities while remaining resource-efficient.
Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, arXiv:2409.125142024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, arXiv:2410.078642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, arXiv:2311.019772023arXiv preprint</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, arXiv:2411.196502024arXiv preprint</p>
<p>Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu, arXiv:2403.03954Generalizable visuomotor policy learning via simple 3d representations. 2024arXiv preprint3d diffusion policy</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, arXiv:2410.061582024arXiv preprint</p>
<p>Roboflamingo-plus: Fusion of depth and rgb perception with vision-language models for enhanced robotic manipulation. Sheng Wang, arXiv:2503.195102025arXiv preprint</p>
<p>Vision-language foundation models as effective robot imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, arXiv:2503.106312025arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, arXiv:2503.220202025arXiv preprint</p>
<p>Pointvla: Injecting the 3d world into vision-language-action models. Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu, arXiv:2503.075112025arXiv preprint</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, arXiv:2503.147342025arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, arXiv:2502.21257A unified brain model for robotic manipulation from abstract to concrete. 2025arXiv preprint</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint</p>
<p>Zhutian Yang, Caelan Garrett, Dieter Fox, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , arXiv:2410.02193Guiding long-horizon task and motion planning with vision language models. 2024arXiv preprint</p>
<p>Nishanth Kumar, Fabio Ramos, Dieter Fox, Caelan Reed Garrett, arXiv:2411.08253Open-world task and motion planning via vision-language model inferred constraints. 2024arXiv preprint</p>
<p>Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, arXiv:2502.13143Language-grounded orientation bridges spatial reasoning and object manipulation. 2025arXiv preprint</p>
<p>Vlmpc: Vision-language model predictive control for robotic manipulation. Wentao Zhao, Jiaming Chen, Ziyu Meng, Donghui Mao, Ran Song, Wei Zhang, arXiv:2407.098292024arXiv preprint</p>
<p>Kuda: Keypoints to unify dynamics learning and visual prompting for open-vocabulary robotic manipulation. Zixian Liu, Mingtong Zhang, Yunzhu Li, arXiv:2503.105462025arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Moka: Open-world robotic manipulation through mark-based visual prompting. Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine, arXiv:2403.031742024arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong, arXiv:2501.038412025arXiv preprint</p>
<p>Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li, Chi-Wing Fu, Mingyu Ding, arXiv:2501.09783Geomanip: Geometric constraints as general interfaces for robot manipulation. 2025arXiv preprint</p>
<p>Code-as-monitor: Constraint-aware visual programming for reactive and proactive robotic failure detection. Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang, arXiv:2412.044552024arXiv preprint</p>
<p>Grounding language model by detecting and recovering from plan-execution misalignment. Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Jianyu Chen, Doremi, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>On computable numbers, with an application to the Entscheidungsproblem. Alan Mathison, Turing , The Undecidable. M Davis, Raven Press1936. 196542</p>            </div>
        </div>

    </div>
</body>
</html>
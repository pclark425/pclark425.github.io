<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5596 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5596</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5596</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-4c8e341c91bbd8dcbb2d3bc18f0e60d41e34035f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4c8e341c91bbd8dcbb2d3bc18f0e60d41e34035f" target="_blank">Large language models can replicate cross-cultural differences in personality</a></p>
                <p><strong>Paper Venue:</strong> Journal of Research in Personality</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5596.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5596.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from OpenAI used here as a text-based simulator to generate personality questionnaire responses for simulated adults from the United States and South Korea, in English and Korean.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model (OpenAI GPT family). Paper notes GPT models are trained on large human-generated text corpora (predominantly English), fine-tuned with RLHF to improve helpfulness and safety; GPT-4 is described as a later-generation model with stronger capabilities than GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-cultural psychology / Personality assessment (psychometrics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of individual responses to the Ten-Item Personality Inventory (TIPI / TIPI-K) by prompting the model to 'play the role' of an adult from the United States or South Korea and produce numeric item responses in square brackets; used to derive Big Five factor scores and distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Mean Absolute Error (MAE; absolute difference between simulated and actual mean scores), Cohen's d (effect sizes comparing simulated groups), t-tests for differences, mean and SD comparisons, Cronbach's alpha and Guttman's lambda6 for internal consistency / structural validity, format-error rate (correct output formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>MAE: TIPI-US pair M = 1.25; TIPI-K–Korean pair M = 1.85; per-factor MAE examples: Emotional Stability (TIPI-US) M = 0.73 (lowest), Openness (TIPI-K) M = 2.29 (highest). Simulated means showed a general upward bias (raw upward bias reported M = 1.11 for TIPI-US and M = 1.70 for TIPI-K–Korean). GPT-4 internal consistency (Cronbach's alpha) ranges reported: TIPI alphas ~0.07 (Agreeableness) to 0.54 (Extraversion); TIPI-K alphas ~0.04 to 0.70. Output format correctness: nearly all GPT-4 output in correct format.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>[Model generation (GPT-4 vs GPT-3.5), prompt language and inventory-target congruence (English TIPI vs Korean TIPI-K and whether model was prompted to simulate US vs Korean target), Reinforcement Learning from Human Feedback (RLHF) imprinting and social-desirability effects, 'mode collapse' / reduced output variance, inventory properties (TIPI being very short with low structural reliability), temperature/prompt stochasticity, format adherence in model outputs].</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Empirical comparisons in the study: GPT-4 vs GPT-3.5 (GPT-4 produced valid-format output and better structural validity; GPT-3.5 showed high format error and near-zero/negative alpha values); language/inventory manipulations showed different MAE and effect sizes (TIPI-US pair more accurate than TIPI-K–Korean pair; holding inventory constant also affected accuracy). Observed upward bias and reduced variance support hypothesized RLHF and mode-collapse effects, but these are presented as plausible mechanisms rather than causally proven.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison of simulated sample means and distributions to published human TIPI / TIPI-K study results (ground-truth means and SDs). Statistical evaluation included two-sample t-tests (pre-registered), Cohen's d with 95% CIs for effect sizes, computation of MAE (absolute differences between simulated and actual scores), internal-consistency metrics (Cronbach's alpha, Guttman's lambda6, Spearman-Brown for two-item factors), and inspection of output format correctness and distributional plots.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Systematic upward bias in simulated scores (inflated means), reduced between-sample variation (smaller SDs than human samples), substantially lower structural validity (low Cronbach's alpha and lambda6 for some factors), GPT-3.5 produced high format-error rates (~19% overall; ~35% for Korean prompts) and very poor internal consistency (alpha ranged negative to near zero), model outputs potentially reflect stereotypes or RLHF rather than true population distributions, single cultural pair and single (very short) inventory limit generality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons performed between GPT-4 and GPT-3.5 (structural validity, format error, accuracy); between inventory-target congruent pairs (TIPI-English with US target vs TIPI-K Korean with Korean target) and holding inventory constant (same inventory used for both simulated targets); English vs Korean prompt/inventory conditions. Noted that GPT-3.5 was slightly more accurate by MAE in English prompts (GPT-3.5 M=1.41 vs GPT-4 M=1.65) but GPT-4 was more accurate in Korean prompts (GPT-4 M=1.54 vs GPT-3.5 M=1.64).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use longer inventories with stronger psychometric properties (e.g., BFI-44, IPIP-NEO-120) to enable structural invariance testing; consider analyzing embeddings (mask-filling) as alternative extraction methods; test multiple LLMs including open-source models to inspect RLHF and training-data effects; carefully configure prompts (system pre-prompt specifying role) and document temperature/stochasticity; pre-register analyses and be cautious about independence assumptions of simulated samples; investigate role of stereotypes and RLHF with controlled ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models can replicate cross-cultural differences in personality', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5596.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5596.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier-generation OpenAI GPT model used as a comparator in the study; produced simulated personality-item responses but with poorer structural validity and frequent format errors, especially for Korean prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier-generation Transformer-based GPT model from OpenAI. Less capable than GPT-4 per authors' account; the paper notes GPT-3.5 outputs had more formatting errors and poorer psychometric structure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cross-cultural psychology / Personality assessment (psychometrics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same text-based simulation as GPT-4: generate numeric responses to TIPI / TIPI-K prompted to simulate adults from the United States or South Korea to derive Big Five scores.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Same metrics as GPT-4: MAE, Cohen's d, t-tests, Cronbach's alpha, Guttman's lambda6, output format error rate.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Format error rate: ~19% overall incorrect format (answers not in square brackets or multiple answers), especially when prompted in Korean (~35% error) vs English (~3% error). Internal consistency (Cronbach's alpha) for GPT-3.5 was very poor: alpha values ranged from about -0.30 to 0.07 (TIPI) and -0.18 to 0.00 (TIPI-K). Accuracy by MAE: when prompted in English GPT-3.5 M = 1.41 (slightly better than GPT-4 M = 1.65 in English); when prompted in Korean GPT-3.5 M = 1.64 (worse than GPT-4 M = 1.54 in Korean).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>[Older model generation (weaker abilities), prompt language (Korean caused high format error), RLHF intensity differences between model generations, structural validity issues of TIPI interacting with model weaknesses], plus same factors listed for GPT-4 (inventory, prompt design, temperature).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>High format-error rates for Korean prompts and very low/negative Cronbach's alpha values provide evidence that GPT-3.5's outputs were unreliable; cross-model MAE comparisons show small context-dependent differences (GPT-3.5 better in English MAE, worse in Korean).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same evaluation pipeline as GPT-4: compare simulated means/distributions to published human TIPI/TIPI-K data, compute MAE, Cohen's d, t-tests, and internal consistency metrics; also recorded format-error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High rate of incorrect output formatting (particularly for Korean prompts), extremely poor structural validity (negative or near-zero Cronbach's alpha for multiple factors), thus authors limited main analysis to GPT-4. GPT-3.5 therefore not reliable for this simulation task in this design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-4: GPT-3.5 had worse structural validity and format reliability; mixed MAE performance (slightly better in English, worse in Korean).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>If using older-generation LLMs for behavioral simulations, validate output formatting and psychometric structure before relying on results; prefer more advanced models or use stronger extraction methods and psychometrically robust inventories; test model-language interactions and report format-error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models can replicate cross-cultural differences in personality', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies <em>(Rating: 2)</em></li>
                <li>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus? <em>(Rating: 2)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Personality Traits in Large Language Models <em>(Rating: 2)</em></li>
                <li>Can AI language models replace human participants? <em>(Rating: 2)</em></li>
                <li>GPT is an effective tool for multilingual psychological text analysis <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5596",
    "paper_id": "paper-4c8e341c91bbd8dcbb2d3bc18f0e60d41e34035f",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary large language model from OpenAI used here as a text-based simulator to generate personality questionnaire responses for simulated adults from the United States and South Korea, in English and Korean.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based large language model (OpenAI GPT family). Paper notes GPT models are trained on large human-generated text corpora (predominantly English), fine-tuned with RLHF to improve helpfulness and safety; GPT-4 is described as a later-generation model with stronger capabilities than GPT-3.5.",
            "model_size": null,
            "scientific_subdomain": "Cross-cultural psychology / Personality assessment (psychometrics)",
            "simulation_task": "Text-based simulation of individual responses to the Ten-Item Personality Inventory (TIPI / TIPI-K) by prompting the model to 'play the role' of an adult from the United States or South Korea and produce numeric item responses in square brackets; used to derive Big Five factor scores and distributions.",
            "accuracy_metric": "Mean Absolute Error (MAE; absolute difference between simulated and actual mean scores), Cohen's d (effect sizes comparing simulated groups), t-tests for differences, mean and SD comparisons, Cronbach's alpha and Guttman's lambda6 for internal consistency / structural validity, format-error rate (correct output formatting).",
            "reported_accuracy": "MAE: TIPI-US pair M = 1.25; TIPI-K–Korean pair M = 1.85; per-factor MAE examples: Emotional Stability (TIPI-US) M = 0.73 (lowest), Openness (TIPI-K) M = 2.29 (highest). Simulated means showed a general upward bias (raw upward bias reported M = 1.11 for TIPI-US and M = 1.70 for TIPI-K–Korean). GPT-4 internal consistency (Cronbach's alpha) ranges reported: TIPI alphas ~0.07 (Agreeableness) to 0.54 (Extraversion); TIPI-K alphas ~0.04 to 0.70. Output format correctness: nearly all GPT-4 output in correct format.",
            "factors_affecting_accuracy": "[Model generation (GPT-4 vs GPT-3.5), prompt language and inventory-target congruence (English TIPI vs Korean TIPI-K and whether model was prompted to simulate US vs Korean target), Reinforcement Learning from Human Feedback (RLHF) imprinting and social-desirability effects, 'mode collapse' / reduced output variance, inventory properties (TIPI being very short with low structural reliability), temperature/prompt stochasticity, format adherence in model outputs].",
            "evidence_for_factors": "Empirical comparisons in the study: GPT-4 vs GPT-3.5 (GPT-4 produced valid-format output and better structural validity; GPT-3.5 showed high format error and near-zero/negative alpha values); language/inventory manipulations showed different MAE and effect sizes (TIPI-US pair more accurate than TIPI-K–Korean pair; holding inventory constant also affected accuracy). Observed upward bias and reduced variance support hypothesized RLHF and mode-collapse effects, but these are presented as plausible mechanisms rather than causally proven.",
            "evaluation_method": "Comparison of simulated sample means and distributions to published human TIPI / TIPI-K study results (ground-truth means and SDs). Statistical evaluation included two-sample t-tests (pre-registered), Cohen's d with 95% CIs for effect sizes, computation of MAE (absolute differences between simulated and actual scores), internal-consistency metrics (Cronbach's alpha, Guttman's lambda6, Spearman-Brown for two-item factors), and inspection of output format correctness and distributional plots.",
            "limitations_or_failure_cases": "Systematic upward bias in simulated scores (inflated means), reduced between-sample variation (smaller SDs than human samples), substantially lower structural validity (low Cronbach's alpha and lambda6 for some factors), GPT-3.5 produced high format-error rates (~19% overall; ~35% for Korean prompts) and very poor internal consistency (alpha ranged negative to near zero), model outputs potentially reflect stereotypes or RLHF rather than true population distributions, single cultural pair and single (very short) inventory limit generality.",
            "comparisons": "Direct comparisons performed between GPT-4 and GPT-3.5 (structural validity, format error, accuracy); between inventory-target congruent pairs (TIPI-English with US target vs TIPI-K Korean with Korean target) and holding inventory constant (same inventory used for both simulated targets); English vs Korean prompt/inventory conditions. Noted that GPT-3.5 was slightly more accurate by MAE in English prompts (GPT-3.5 M=1.41 vs GPT-4 M=1.65) but GPT-4 was more accurate in Korean prompts (GPT-4 M=1.54 vs GPT-3.5 M=1.64).",
            "recommendations_or_best_practices": "Use longer inventories with stronger psychometric properties (e.g., BFI-44, IPIP-NEO-120) to enable structural invariance testing; consider analyzing embeddings (mask-filling) as alternative extraction methods; test multiple LLMs including open-source models to inspect RLHF and training-data effects; carefully configure prompts (system pre-prompt specifying role) and document temperature/stochasticity; pre-register analyses and be cautious about independence assumptions of simulated samples; investigate role of stereotypes and RLHF with controlled ablations.",
            "uuid": "e5596.0",
            "source_info": {
                "paper_title": "Large language models can replicate cross-cultural differences in personality",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "An earlier-generation OpenAI GPT model used as a comparator in the study; produced simulated personality-item responses but with poorer structural validity and frequent format errors, especially for Korean prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Earlier-generation Transformer-based GPT model from OpenAI. Less capable than GPT-4 per authors' account; the paper notes GPT-3.5 outputs had more formatting errors and poorer psychometric structure.",
            "model_size": null,
            "scientific_subdomain": "Cross-cultural psychology / Personality assessment (psychometrics)",
            "simulation_task": "Same text-based simulation as GPT-4: generate numeric responses to TIPI / TIPI-K prompted to simulate adults from the United States or South Korea to derive Big Five scores.",
            "accuracy_metric": "Same metrics as GPT-4: MAE, Cohen's d, t-tests, Cronbach's alpha, Guttman's lambda6, output format error rate.",
            "reported_accuracy": "Format error rate: ~19% overall incorrect format (answers not in square brackets or multiple answers), especially when prompted in Korean (~35% error) vs English (~3% error). Internal consistency (Cronbach's alpha) for GPT-3.5 was very poor: alpha values ranged from about -0.30 to 0.07 (TIPI) and -0.18 to 0.00 (TIPI-K). Accuracy by MAE: when prompted in English GPT-3.5 M = 1.41 (slightly better than GPT-4 M = 1.65 in English); when prompted in Korean GPT-3.5 M = 1.64 (worse than GPT-4 M = 1.54 in Korean).",
            "factors_affecting_accuracy": "[Older model generation (weaker abilities), prompt language (Korean caused high format error), RLHF intensity differences between model generations, structural validity issues of TIPI interacting with model weaknesses], plus same factors listed for GPT-4 (inventory, prompt design, temperature).",
            "evidence_for_factors": "High format-error rates for Korean prompts and very low/negative Cronbach's alpha values provide evidence that GPT-3.5's outputs were unreliable; cross-model MAE comparisons show small context-dependent differences (GPT-3.5 better in English MAE, worse in Korean).",
            "evaluation_method": "Same evaluation pipeline as GPT-4: compare simulated means/distributions to published human TIPI/TIPI-K data, compute MAE, Cohen's d, t-tests, and internal consistency metrics; also recorded format-error rates.",
            "limitations_or_failure_cases": "High rate of incorrect output formatting (particularly for Korean prompts), extremely poor structural validity (negative or near-zero Cronbach's alpha for multiple factors), thus authors limited main analysis to GPT-4. GPT-3.5 therefore not reliable for this simulation task in this design.",
            "comparisons": "Compared to GPT-4: GPT-3.5 had worse structural validity and format reliability; mixed MAE performance (slightly better in English, worse in Korean).",
            "recommendations_or_best_practices": "If using older-generation LLMs for behavioral simulations, validate output formatting and psychometric structure before relying on results; prefer more advanced models or use stronger extraction methods and psychometrically robust inventories; test model-language interactions and report format-error rates.",
            "uuid": "e5596.1",
            "source_info": {
                "paper_title": "Large language models can replicate cross-cultural differences in personality",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?",
            "rating": 2
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 2
        },
        {
            "paper_title": "Personality Traits in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Can AI language models replace human participants?",
            "rating": 2
        },
        {
            "paper_title": "GPT is an effective tool for multilingual psychological text analysis",
            "rating": 2
        }
    ],
    "cost": 0.010690999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large language models can replicate cross-cultural differences in personality</h1>
<p>Paweł Niszczota ${ }^{\text {a,<em> }}$, Mateusz Janczak ${ }^{\text {a }}$, Michał Misiak ${ }^{\text {b,c }}$<br>${ }^{a}$ Humans \&amp; AI Laboratory (HAI Lab), Institute of International Business and Economics, Poznań University of Economics and Business, Poznań, Poland<br>${ }^{\mathrm{b}}$ IDN Being Human, Institute of Psychology, University of Wrocław, Wrocław, Poland<br>${ }^{c}$ School of Anthropology \&amp; Museum Ethnography, University of Oxford, Oxford, United Kingdom<br></em> Corresponding author: Paweł Niszczota, Poznań University of Economics and Business, al.<br>Niepodległości 10, 61-875 Poznań, Poland, pawel.niszczota@ue.poznan.pl</p>
<h2>Data</h2>
<p>Data and the pre-registration document are available at: https://osf.io/76g2u/.</p>
<h2>Declaration of Competing Interest</h2>
<p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
<h2>Ethical approval</h2>
<p>The study did not involve any human subjects and did not require ethical approval.</p>
<h2>Financial support</h2>
<p>This research was supported by grant 2021/42/E/HS4/00289 from the National Science Centre, Poland.</p>
<h2>Acknowledgments</h2>
<p>We'd like to thank Dirk Wulff for valuable comments on the manuscript.</p>
<h2>CRediT authorship contribution statement</h2>
<p>Paweł Niszczota: Conceptualization; Methodology; Software; Validation; Formal analysis; Investigation; Resources; Data Curation; Writing - Original Draft; Writing - Review \&amp; Editing; Visualization; Supervision; Project administration; Funding acquisition
Mateusz Janczak: Conceptualization; Formal analysis; Investigation; Writing - Review \&amp; Editing
Michał Misiak: Formal analysis; Investigation; Methodology; Writing - Review \&amp; Editing</p>
<p>Published in Journal of Research in Personality - this is the Author Accepted Manuscript For final version (version of record) see:
Niszczota, P., Janczak, M., \&amp; Misiak, M. (2025). Large language models can replicate crosscultural differences in personality. Journal of Research in Personality, 115, 104584.
https://doi.org/10.1016/j.jrp.2025.104584</p>
<h1>Large language models can replicate cross-cultural differences in personality</h1>
<h4>Abstract</h4>
<p>We use a large-scale experiment $(N=8000)$ to determine whether GPT-4 can replicate crosscultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.</p>
<p>Keywords: cross-cultural research; personality; Big Five; Ten-Item Personality Inventory; large language models; GPT; artificial intelligence; generative AI; psychographics; consumer and economic behavior</p>
<h2>1. Introduction</h2>
<p>Even though large language models (LLMs) caught the attention of the public a while back (GPT-3, 2020), it was the introduction of ChatGPT that ignited a boom in interest in such models. This includes both laypeople and academics, including personality researchers (e.g., Jiang et al., 2023; Serapio-García et al., 2023). Some researchers (e.g., Horton, 2023) have proposed that LLMs could simulate human behavior. Recent advancements in Generative Pretrained Transformer (GPT) models have revealed emerging capabilities (e.g., Bubeck et al., 2023), and perhaps simulating some patterns of human behavior is one of them. While LLMs should not be used as a replacement for human participants, they could be used as a sandbox for cross-cultural researchers and practitioners. These models could be used to conduct preliminary tests or to further validate findings obtained from human participants (Dillion et al., 2023).</p>
<p>In our study, we wanted to test whether LLMs are capable of mimicking real-world cross-cultural differences in personality. We used the Big Five model of personality (Gosling et al., 2003; Piedmont \&amp; Chae, 1997). The development of the Big Five was based on the lexical hypothesis, which posits that significant aspects of personality are reflected in vocabulary (Goldberg, 1993). Cutler and Condon (2023) propose that LLMs may resemble and potentially enhance methods used in early lexical studies on personality. They can assist in identifying personality descriptors, collecting data, and analyzing relationships between words, potentially offering new insights into personality structure. Contemporary psychologists often use traditional questionnaires for measuring personality traits and cross-cultural differences. We do not know, however, whether LLMs are able to mimic human responses to these kinds of measures through text generation (Hussain et al., 2024).</p>
<p>To test GPT's ability to simulate cross-cultural differences in personality, we chose the United States and South Korea as targets for this simulation. We have done this for two reasons. First, studies point to the existence of substantial differences in the mean scores of people from these two countries on the Big Five (Kajonius \&amp; Giolla, 2017; Piedmont \&amp; Chae, 1997; Yoon et al., 2002). Yoon and colleagues (2002) argue - while discussing their findings and the findings from earlier research - that it is unlikely that these differences are due to the result of artifacts introduced while translating the inventories used, and instead, they reflect actual cultural differences or different rating strategies used by members of these cultures.</p>
<p>Secondly, we used Korean as the alternative language due to its difference from the English language, on which GPT was mostly trained: notable differences were the basic sentence word order and the usage of different alphabets. In Table 1A, we present how mean scores differ in the original, US version of the Ten-Item Personality Inventory (TIPI; Gosling et al., 2003) and the Korean version of TIPI, i.e., TIPI-K, developed by Ha et al. (2013). Studies that directly compared US Americans against South Koreans using the Big Five taxonomy (see Tables S1-2) largely point to the same differences (Kajonius \&amp; Giolla, 2017; Piedmont \&amp; Chae, 1997).</p>
<p>While we are not the first to use LLMs to simulate human behavior (e.g., Aher et al., 2023; Horton, 2023; Serapio-García et al., 2023), we are the first to investigate whether LLMs can replicate cross-cultural personality differences. Let us consider the most intuitive mechanism that would allow LLMs to replicate cross-cultural personality differences, namely that LLMs obtain such an ability through extensive training on human-generated text, including discussions about personality and behavior (Bubeck et al., 2023; Hussain et al., 2024). This could enable LLMs to generate culturally-appropriate responses reflecting typical personality traits. LLMs' capacity to mirror personality expression across cultures would demonstrate progress in modeling human cognition and social behaviors (Dillion et al., 2023; Salah et al., 2023).</p>
<p>Cross-cultural studies often rely on adaptations of inventories, typically requiring the use of languages different from the original inventory's design. The difference in languages used is a potential problem for LLMs, as they have largely been trained on English text. However, some evidence suggests that GPT models perform well in languages other than English (Rathje et al., 2023). To test the current capabilities of the most advanced LLMs, we analyzed either the original languages of the inventories or held the language of the inventory constant. In the latter case, we varied not the language of the inventory, but the origin of the simulated target.</p>
<h1>2. Hypotheses</h1>
<p>In our main hypothesis, we posit that GPT-4 can replicate cross-cultural differences in Big Five personality factors. We focus on TIPI, which is a popular short inventory measuring the Big Five. Given that GPT is expected to respond differently depending on who is the person it is simulating, we used a system message (pre-prompt), instructing GPT to play the role of an adult from either USA or South Korea.</p>
<p>We wanted to answer two additional questions. Firstly, we asked whether GPT can replicate cross-cultural differences when using just one inventory (language), and not two. If personality differences between US Americans and South Koreans reflect actual cultural differences and are not artifacts, as argued, prompting an LLM in the same language should reproduce the expected personality differences. Secondly, we aimed to assess whether the</p>
<p>development of GPT models shows progress in their potential ability to simulate human responses. We compared the performance of GPT-4 against the earlier generation, GPT-3.5.</p>
<h1>3. Methodology</h1>
<p>The study was conducted in August 2023 via API (programmatic) access to GPT. We used the following system message: "You are playing the role of an adult from [the United States/South Korea]". To elicit responses that could be efficiently analyzed, we modified the text used in the inventories, so that answers to each item were provided in square brackets (see Supplemental Materials for details). We used the least deterministic temperature setting (temperature $=2$ ) to elicit the greatest level of variation in the answers.</p>
<p>The study had a 2 (target: from the United States vs. from South Korea) $\times 2$ (inventory and language: TIPI in English vs. TIPI-K in Korean) $\times 2$ (model: GPT-4 vs. GPT-3.5) design. As we wanted to collect 1000 observations per condition, we collected 8000 observations overall.</p>
<p>The main pre-registered test concerned the ability of GPT-4 to replicate differences in Big Five scores between US Americans and South Koreans. Given that this meant conducting five tests, to account for multiple tests, we used $\alpha=.05 / 5=.01$ as the Type-I error rate. This gave us $95 \%$ power to detect an effect of $d=0.189$, and $80 \%$ power to detect an effect of $d=0.153$. Here we make the charitable assumption that observations generated by LLMs are independent - we discuss this issue further in the Supplemental Materials.</p>
<p>The study was pre-registered at https://aspredicted.org/zy8s-j37r. Data, code, and materials are available at https://osf.io/76g2u/.</p>
<h2>4. Results</h2>
<h3>4.1. Structural validity and other properties of output</h3>
<p>TIPI was specifically designed as a short measure of the Big Five personality dimensions, prioritizing content validity over traditional psychometric criteria. This design choice involves an inherent trade-off: while TIPI offers brevity and broad content coverage, its structure (two items per factor) precludes conventional measurement invariance analyses. This deliberate heterogeneity in item content, while enhancing construct representation, results in lower internal consistency estimates (Thørrisen \&amp; Sadeghi, 2023). To provide insights into the psychometric properties of generated TIPI and TIPI-K, we conducted a series of internal consistency analyses, including Cronbach's $\alpha \mathrm{s}$ and Guttman's $\lambda_{6} \mathrm{~s}$ (Table S3). Regarding $\alpha \mathrm{s}$, the most significant difference was observed for Emotional Stability, which had an $\alpha$ of .73 in the original study, but only .19 (TIPI) and .16 (TIPI-K) in the simulated data. To further test the internal consistency of GPT-4 generated data, we used the Spearman-Brown formula, which is recommended for twoitem factors (Eisinga et al., 2013). These results were similar to those provided by classical internal consistency tests (Table S4). Our findings mirror the findings of a methodological review that demonstrated Extraversion has the highest internal consistency and Agreeableness the lowest (Thørrisen \&amp; Sadeghi, 2023).</p>
<h3>4.2. Prompting in inventory-target congruent pairs</h3>
<p>Our main hypothesis concerns the ability of GPT-4 to replicate cross-cultural differences in personality. As noted earlier, personality differences can be the result of actual differences between cultures or artifacts introduced during the adaptation of a personality inventory for a</p>
<p>different culture. To stay the most comparable to the differences exhibited in TIPI and TIPI-K, we prompted GPT in English and Korean, respectively.</p>
<p>GPT-4 replicated all of the differences that were present in the original TIPI and TIPI-K studies: Korean targets in TIPI-K had lower scores on each of the Big Five factors (when using Emotional Stability, instead of Neuroticism). See Table 1A for test results and Figure S1 for an illustration of how the scores were distributed.</p>
<h1>4.3. Prompting holding inventory constant</h1>
<p>If cross-cultural differences in personality are substantial, then a practical alternative would be to prompt LLMs in just one language. As LLMs are usually trained mostly on text in English and inventories are also constructed in English, it seems plausible to simply construct prompts in this language.</p>
<p>Results - presented in Table 1B - show that GPT replicated almost all of the expected patterns (South Koreans obtaining lower scores than US Americans) both when it was prompted in English (using TIPI) or Korean (using TIPI-K). An exception was Agreeableness for TIPI-K, where the difference had the opposite sign than expected.</p>
<h3>4.4. Accuracy of scores</h3>
<p>LLMs can replicate differences in scores between cultures without accurately reflecting actual mean levels. For example, an LLM can detect that Koreans on average obtain lower scores on a personality trait than Americans, yet the simulated values could be offset (e.g., inflated) in both cases. Therefore, we also compared how accurately GPT reflected mean personality scores, working under the assumption that TIPI in both the original (US) version and the adapted (Korean) version accurately captured variation in the Big Five.</p>
<h2>Inventory-target congruent pairs</h2>
<p>Raw differences suggest that there was a general upward bias in responses, for both the TIPI-US pair $(M=1.11)$ and TIPI-K-Korean pair $(M=1.70)$, being significantly lower in the former $(d=-0.50[-0.54,-0.46])$. We speculate on the potential reason behind this tendency in the discussion.</p>
<h1>Table 1</h1>
<p>Actual and simulated scores for TIPI-US and TIPI-K-Korean pairs (Panel A), and simulated scores for when using the same inventory (Panel B)</p>
<p>Panel A. Comparison of actual and simulated scores: US target in TIPI (English) vs South Korean in TIPI-K (Korean)</p>
<table>
<thead>
<tr>
<th>Factor</th>
<th>Actual</th>
<th></th>
<th></th>
<th>Simulated (GPT-4)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>US/TIPI</td>
<td>South</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Korean/TIPI-K</td>
<td>Cohen's $d$</td>
<td>US/TIPI</td>
<td>South</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Korean/TIPI-K</td>
<td>Cohen's $d$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Extraversion</td>
<td>$8.88(2.90)$</td>
<td>$8.47(2.67)$</td>
<td>0.15</td>
<td>$10.04(0.99)$</td>
<td>$9.60(1.55)$</td>
<td>$0.34[0.25,0.43]$</td>
</tr>
<tr>
<td>Agreeableness</td>
<td>$10.46(2.22)$</td>
<td>$9.50(2.07)$</td>
<td>0.48</td>
<td>$11.99(0.74)$</td>
<td>$11.25(0.94)$</td>
<td>$0.88[0.78,0.97]$</td>
</tr>
<tr>
<td>Conscientiousness</td>
<td>$10.80(2.64)$</td>
<td>$9.42(2.31)$</td>
<td>0.56</td>
<td>$12.57(0.75)$</td>
<td>$11.57(0.98)$</td>
<td>$1.10[1.00,1.20]$</td>
</tr>
<tr>
<td>Emotional Stability</td>
<td>$9.66(2.84)$</td>
<td>$8.36(2.43)$</td>
<td>0.49</td>
<td>$10.10(0.81)$</td>
<td>$9.56(1.20)$</td>
<td>$0.52[0.43,0.61]$</td>
</tr>
<tr>
<td>Openness to Experiences</td>
<td>$10.76(2.14)$</td>
<td>$8.46(2.51)$</td>
<td>0.99</td>
<td>$11.43(1.05)$</td>
<td>$10.71(1.35)$</td>
<td>$0.59[0.50,0.68]$</td>
</tr>
</tbody>
</table>
<p>Panel B. Simulated scores for both targets using same inventory, i.e. TIPI (English) vs TIPI-K (Korean)</p>
<table>
<thead>
<tr>
<th>Factor</th>
<th>TIPI</th>
<th></th>
<th></th>
<th>TIPI-K</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>US</td>
<td>South Korean</td>
<td>Cohen's $d$</td>
<td>US</td>
<td>South Korean</td>
<td>Cohen's $d$</td>
</tr>
<tr>
<td>Extraversion</td>
<td>$10.04(0.99)$</td>
<td>$8.90(1.00)$</td>
<td>$1.10[1.10,1.20]$</td>
<td>$10.66(1.42)$</td>
<td>$9.60(1.55)$</td>
<td>$0.72[0.63,0.81]$</td>
</tr>
<tr>
<td>Agreeableness</td>
<td>$11.99(0.74)$</td>
<td>$11.70(0.72)$</td>
<td>$0.39[0.30,0.48]$</td>
<td>$11.13(0.95)$</td>
<td>$11.25(0.94)$</td>
<td>$-0.12[-0.21,-0.03]$</td>
</tr>
<tr>
<td>Conscientiousness</td>
<td>$12.57(0.75)$</td>
<td>$12.43(0.79)$</td>
<td>$0.18[0.09,0.27]$</td>
<td>$11.71(1.09)$</td>
<td>$11.57(0.98)$</td>
<td>$0.13[0.04,0.22]$</td>
</tr>
<tr>
<td>Emotional Stability</td>
<td>$10.10(0.81)$</td>
<td>$9.99(0.84)$</td>
<td>$0.12[0.04,0.21]$</td>
<td>$9.80(1.13)$</td>
<td>$9.56(1.20)$</td>
<td>$0.21[0.12,0.29]$</td>
</tr>
<tr>
<td>Openness to Experiences</td>
<td>$11.43(1.05)$</td>
<td>$10.96(0.95)$</td>
<td>$0.46[0.37,0.55]$</td>
<td>$11.19(1.32)$</td>
<td>$10.71(1.35)$</td>
<td>$0.36[0.28,0.45]$</td>
</tr>
</tbody>
</table>
<p>Note. This table presents means and standard deviations (brackets), and effect sizes [with $95 \%$ CIs]. All pre-registered $t$-tests that compare the simulated scores for the US target using TIPI and South Korean target using TIPI-K (Panel A) indicate that the differences are statistically significant at $p&lt;.0001$.</p>
<h1>Figure 1</h1>
<p>Accuracy of GPT-4 across the Big Five
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Note. Bars represent means. Accuracy is the absolute value of the difference between simulated and actual scores.</p>
<p>The absolute difference between simulated and original scores (or the Mean Absolute Error) - shown in Figure 1 - pointed to the accuracy being greater for the TIPI-US pair ( $M=$ 1.25) than the TIPI-K-Korean pair $(M=1.85, d=-0.62[-0.66,-0.58]$.</p>
<p>There was variation across the accuracy of simulated scores, with the lowest being for Emotional Stability in the TIPI-US pair $(M=0.73)$ and the highest being for Openness to Experience in the TIPI-K-Korean pair $(M=2.29)$.</p>
<h1>Holding inventory constant</h1>
<p>Instead of varying both the inventory (TIPI vs. TIPI-K) and the target (US vs. Korean), we can compare the accuracy of scores when only manipulating the target (a person from the United States vs. a person from South Korea). In our case, we can either use TIPI or TIPI-K as the benchmark inventory. As shown in Figure 1, accuracy when using TIPI (i.e., in English), was better for US than South Korean targets, $d=-0.62[-0.66,-0.58]$. It was also better for US than South Korean targets when using TIPI-K (i.e., in Korean), $d=-0.80[-$ $0.85,-0.77]$.</p>
<h2>5. Discussion</h2>
<p>Large language models can potentially encapsulate human behavior across various hypothetical scenarios, replicating numerous prominent findings in psychological science (Horton, 2023). This includes the structure of personality traits (Cutler \&amp; Condon, 2023). In our study, we aimed to advance this field of research by testing whether LLMs could replicate prominent cross-cultural differences, such as those observed between US Americans and South Koreans on the Big Five personality traits (Kajonius \&amp; Giolla, 2017; Piedmont \&amp; Chae, 1997; Yoon et al., 2002).</p>
<p>Using the Ten-Item Personality Inventory (Gosling et al., 2003) to assess personality based on the Five-Factor Model, our results show that GPT-4 can replicate all of the differences previously observed when comparing Americans and South Koreans on the Big Five factors. Specifically, our simulated South Korean responses showed lower scores on Extraversion, Agreeableness, Conscientiousness, and Openness to Experiences and higher scores on Neuroticism. This provides initial support for the use of LLM for in silico experiments, where targets of simulations are meant to represent people from different cultures.</p>
<p>LLMs cannot replicate all of the characteristics of people that were sampled in the US and South Korean TIPI studies. There was an upward bias in scores, variation was smaller than in the original samples, and the factors had substantially lower structural validity. Compared to GPT-4, output from GPT-3.5 exhibited extremely poor structural validity. Probably due to this, it did not entirely replicate the expected cross-cultural differences (see Figure S1 in the Supplemental Materials).</p>
<p>While GPT-4 replicated differences for all five factors, the difference between actual and simulated scores was not constant. To illustrate, the difference in Openness to Experience between TIPI and TIPI-K scores is a substantial 0.99 in real-world data. In contrast, for simulated scores the difference is only 0.59 ( $40 \%$ less). Differences between simulated scores of US and Korean targets were the greatest for Agreeableness, even though prior studies do not suggest that this is the dimension in which people from the two investigated cultures differ the most.</p>
<h1>Other potential mechanisms</h1>
<p>The introduction outlined a default mechanism for LLMs' performance: uncovering actual behavioral, cognitive, and affective tendencies of people from training data. However, two other potential, non-mutually exclusive mechanisms may be driving our results. One possibility is that when playing the role of people from different countries, GPT is relying on stereotypes, and simply trying to mimic them when instructed to play a role of a person from a given country. However, research on national stereotypes shows that these are often inaccurate (Jussim et al., 2016), and thus would not help replicate real-world patterns.</p>
<p>A second possibility is that fine-tuning by humans through Reinforcement Learning from Human Feedback (RLHF) - intended to make LLMs more user-friendly and reduce toxic content - might be responsible for the findings. However, the simulated differences are still substantial and opposite to the direction expected from aggressive socially desirable adjustments. Nonetheless, RLHF could be at play. It is plausible that the upward bias in scores present both for Americans and South Koreans was the result of RLHF inflating the scores of simulated persons regardless of origin. RLHF could also be at least partially responsible for the variation in LLM's accuracy across factors, that we mentioned at the end of the last subsection.</p>
<p>We offer a more detailed discussion on the potential roles of stereotypes and RLHF in the Supplementary Materials. Note that this discussion is largely speculative: more research is needed to learn "what is happening under the hood" of LLMs. This might require the use of more open (and thus controllable) LLMs and alternative methodological approaches (e.g., Cutler \&amp; Condon, 2023).</p>
<h2>Limitations on Generality</h2>
<p>In our study we used one personality model (the Five-Factor model), one personality inventory (TIPI), one cross-cultural pair (United States-South Korea), and an LLM originating from one source (GPT from OpenAI). Our findings should be validated using different personality models and inventories, different cross-cultural pairs, and different LLMs. Moreover, we have prompted the LLM to play the role of an adult, but have not specified any other personality-relevant requirements for the simulated person, such as their gender.</p>
<h2>Future directions</h2>
<p>Apart from encouraging researchers to address the limitations highlighted above, we would like to point to a particularly interesting future direction. Given that there are prominent gender differences in personality, and that the stereotypes about these gender differences differ across cultures (Giolla \&amp; Kajonius, 2019), a fruitful avenue would be to test whether LLMs could replicate these differences cross-culturally, using different languages. This could further test the possibility that LLMs are informed by stereotypes to generate their responses. This could be achieved by using something akin to our text generation approach, or rely on an analysis of word embeddings in different languages (Cutler \&amp; Condon, 2023).</p>
<h2>Practical implications</h2>
<p>While there are still limits to what LLMs can do, these models should become increasingly useful as their weaknesses are addressed. Researchers can use them in pilot experiments to pre-test predictions concerning how differently people from various cultures would behave under some circumstances. The ability to emulate cross-cultural differences can</p>
<p>also be useful for practitioners, who have the ability to account for psychological factors that might lead to differences in consumer or economic behavior (e.g., Alderotti et al., 2023; Sandy et al., 2013) across the world.</p>
<p>We present more details on future directions in the Supplemental Materials.</p>
<h1>6. Conclusions</h1>
<p>Our study provides preliminary evidence that LLMs can mimic cross-cultural differences in personality when asked to play the role of a person from a specific country. This capability suggests promising potential for using LLMs in in silico experiments to simulate cross-cultural psychological phenomena. However, limitations were observed, including an upward bias in scores, reduced variation compared to real-world samples, and lower structural validity of the factors. Nonetheless, LLMs can be a valuable tool for social scientists and practitioners, particularly in aiding pilot and validation studies involving different cultures.</p>
<h1>References</h1>
<p>Aher, G., Arriaga, R. I., \&amp; Kalai, A. T. (2023). Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies (arXiv:2208.10264). arXiv. https://doi.org/10.48550/arXiv.2208.10264</p>
<p>Alderotti, G., Rapallini, C., \&amp; Traverso, S. (2023). The Big Five personality traits and earnings: A meta-analysis. Journal of Economic Psychology, 94, 102570. https://doi.org/10.1016/j.joep.2022.102570</p>
<p>Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., \&amp; Zhang, Y. (2023). Sparks of Artificial General Intelligence: Early experiments with GPT-4 (arXiv:2303.12712). arXiv. https://doi.org/10.48550/arXiv.2303.12712</p>
<p>Cutler, A., \&amp; Condon, D. M. (2023). Deep lexical hypothesis: Identifying personality structure in natural language. Journal of Personality and Social Psychology, 125(1), 173-197. https://doi.org/10.1037/pspp0000443</p>
<p>Dillion, D., Tandon, N., Gu, Y., \&amp; Gray, K. (2023). Can AI language models replace human participants? Trends in Cognitive Sciences, 27(7), 597-600. https://doi.org/10.1016/j.tics.2023.04.008</p>
<p>Eisinga, R., Grotenhuis, M. te, \&amp; Pelzer, B. (2013). The reliability of a two-item scale: Pearson, Cronbach, or Spearman-Brown? International Journal of Public Health, 58(4), 637-642. https://doi.org/10.1007/s00038-012-0416-3</p>
<p>Giolla, E., \&amp; Kajonius, P. J. (2019). Sex differences in personality are larger in gender equal countries: Replicating and extending a surprising finding. International Journal of Psychology, 54(6), 705-711. https://doi.org/10.1002/ijop. 12529</p>
<p>Goldberg, L. R. (1993). The structure of phenotypic personality traits. American Psychologist, 48(1), 26-34. https://doi.org/10.1037/0003-066X.48.1.26</p>
<p>Gosling, S. D., Rentfrow, P. J., \&amp; Swann Jr., W. B. (2003). A very brief measure of the BigFive personality domains. Journal of Research in Personality, 37(6), 504-528. https://doi.org/10.1016/S0092-6566(03)00046-1</p>
<p>GPT-3. (2020, September 8). A robot wrote this entire article. Are you scared yet, human? | GPT-3. The Guardian.
http://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt3</p>
<p>Ha, S. E., Kim, S., \&amp; Jo, S. H. (2013). Personality Traits and Political Participation: Evidence from South Korea. Political Psychology, 34(4), 511-532.</p>
<p>Horton, J. J. (2023). Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus? (Working Paper 31122). National Bureau of Economic Research. https://doi.org/10.3386/w31122</p>
<p>Hussain, Z., Binz, M., Mata, R., \&amp; Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. Behavior Research Methods, 1-24. https://doi.org/10.3758/s13428-024-02455-8</p>
<p>Jiang, H., Zhang, X., Cao, X., \&amp; Kabbara, J. (2023). PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences (arXiv:2305.02547). arXiv. https://doi.org/10.48550/arXiv. 2305.02547</p>
<p>Jussim, L., Crawford, J. T., Anglin, S. M., Chambers, J. R., Stevens, S. T., \&amp; Cohen, F. (2016). Stereotype accuracy: One of the largest and most replicable effects in all of social psychology. In Handbook of prejudice, stereotyping, and discrimination, 2nd ed (pp. 31-63). Psychology Press. https://doi.org/10.4324/9781841697772</p>
<p>Kajonius, P., \&amp; Giolla, E. M. (2017). Personality traits across countries: Support for similarities rather than differences. PLOS ONE, 12(6), e0179646. https://doi.org/10.1371/journal.pone. 0179646</p>
<p>Piedmont, R. L., \&amp; Chae, J.-H. (1997). Cross-Cultural Generalizability of the Five-Factor Model of Personality: Development and Validation of the NEO PI-R for Koreans. Journal of Cross-Cultural Psychology, 28(2), 131-155. https://doi.org/10.1177/0022022197282001</p>
<p>Rathje, S., Mirea, D.-M., Sucholutsky, I., Marjieh, R., Robertson, C., \&amp; Bavel, J. J. V. (2023). GPT is an effective tool for multilingual psychological text analysis. PsyArXiv. https://doi.org/10.31234/osf.io/sekf5</p>
<p>Salah, M., Al Halbusi, H., \&amp; Abdelfattah, F. (2023). May the force of text data analysis be with you: Unleashing the power of generative AI for social psychology research. Computers in Human Behavior: Artificial Humans, 1(2), 100006. https://doi.org/10.1016/j.chbah.2023.100006</p>
<p>Sandy, C. J., Gosling, S. D., \&amp; Durant, J. (2013). Predicting Consumer Behavior and Media Preferences: The Comparative Validity of Personality Traits and Demographic Variables. Psychology \&amp; Marketing, 30(11), 937-949. https://doi.org/10.1002/mar. 20657</p>
<p>Serapio-García, G., Safdari, M., Crepy, C., Sun, L., Fitz, S., Romero, P., Abdulhai, M., Faust, A., \&amp; Matarić, M. (2023). Personality Traits in Large Language Models (arXiv:2307.00184). arXiv. https://doi.org/10.48550/arXiv.2307.00184</p>
<p>Thørrisen, M. M., \&amp; Sadeghi, T. (2023). The Ten-Item Personality Inventory (TIPI): A scoping review of versions, translations and psychometric properties. Frontiers in Psychology, 14, 1202953.</p>
<p>Yoon, K., Schmidt, F., \&amp; Ilies, R. (2002). Cross-Cultural Construct Validity of the FiveFactor Model of Personality among Korean Employees. Journal of Cross-Cultural Psychology, 33(3), 217-235. https://doi.org/10.1177/0022022102033003001</p>
<h1>Supplemental Materials</h1>
<p>Large language models can replicate cross-cultural differences in personality</p>
<h2>Part A. Supplementary notes</h2>
<h2>Why do large language models replicate cross-cultural differences in personality?</h2>
<p>Our analysis suggests that large language models (LLMs) can potentially replicate crosscultural personality differences - adding to the earlier body of research connecting personality and LLMs (Jiang et al., 2023; Li et al., 2023; Miotto et al., 2022; Pellert et al., 2022; SerapioGarcía et al., 2023) - but we have not discussed why they may be able to do so.
We presume that this is due to them reflecting underlying differences between people from different countries that are encoded in the text that humans generated (on the Internet, books etc.), which was subsequently used as training data for the LLMs. While traditional questionnaires remain a common approach for measuring personality traits and cross-cultural differences, LLM offer promising new avenues. LLM operate on mechanisms similar to those used to recreate personality structure (Cutler \&amp; Condon, 2023). For example, a language model could be prompted to act as an individual from a specific culture and respond to personality-related questions. By generating numerous responses, it could theoretically produce a dataset representing the distribution of given traits in a population. This approach presents intriguing possibilities for personality research, potentially allowing for rapid, largescale assessments across various cultural contexts.
But there are other possible mechanisms at play, which are discussed below.</p>
<h2>Reflection of stereotypes</h2>
<p>It is well known that machine learning algorithms replicate biases that are present in data that is used to train them. A prominent bias is gender stereotyping, which results, for example, with certain professions being more likely to be associated with men, and others with women (e.g., Bolukbasi et al., 2016; Hendricks et al., 2018; Zhao et al., 2017). What is more relevant is that large language models can also permeate ethnic or cultural stereotypes (Abid et al., 2021). Thus, it is possible that LLMs do not reflect actual differences in personality, but merely stereotypes. If the stereotypes are grounded in real-world characteristics - that is they are somewhat accurate, which has been shown by much research (Jussim et al., 2016) - then the differences in stereotypes that would be captured by LLMs may proxy for actual differences in personality. However, research on stereotype accuracy also suggests that national stereotypes are the type of stereotype that is actually the least accurate (Jussim et al., 2016), i.e. the national stereotypes do not correspond to actual cross-cultural differences in personality (Terracciano et al., 2005). Jussim et al. (2016) report that only $43 \%$ of correlations between national stereotypes and actual scores on the Big Five exceed a correlation of $r=.30$ based on 7 correlations using representative samples (e.g., Terracciano et al., 2005), and only $17 \%$ in 141 correlations exceed this threshold when using convenience samples.
Also, note that personality assessments carried out independently by people from different cultures also show instances of substantial differences in personality, just as it was in the case of Americans and South Koreans that filled out TIPI (in English) and TIPI-K (in Korean),</p>
<p>respectively. Arguably, LLMs prompted in Korean are more likely to reflect auto-stereotypes of Koreans about Koreans, but Jussim et al. (2016) argue that these are also inaccurate.</p>
<p>The role that stereotypes play in shaping an LLM's responses could, however, be more systematically investigated. A testable prediction is that the accuracy of LLMs should increase in cases where stereotypes are more accurate. This could include cross-cultural stereotypes on gender differences in personality (Löckenhoff et al., 2014), which are much more accurate than stereotypes on national character (Jussim et al., 2016).</p>
<h1>Imprinting of values via Reinforcement Learning from Human Feedback (RLHF)</h1>
<p>When people interact with large language models, they interact with a "machine" that has gone through a number of stages to become useful to people (Vaswani et al., 2017). Large language models are based on the Transformer architecture, which consists of various stages (see Hussain et al., 2024 for an explanation). One of the important changes that has allowed the widespread adoption of LLMs into society is Reinforcement Learning from Human Feedback (RLHF). This is a process, in which human annotators shape LLMs by, for example, providing examples of desirable feedback to the LLM and rating its output. It is plausible that during this process humans could "imprint" values into the LLM or conversely - try to combat stereotypes (Abid et al., 2021).</p>
<p>We would argue that if RLHF might try to minimize any tendencies of LLMs to paint some cultures unfavorably, it would happen by inflating the scores of South Koreans relative to US Americans, as high scores on the Big Five are generally considered socially desirable (Musek, 2007; Saucier \&amp; Goldberg, 2003). Overall, there was an upward bias in the scores, which could indeed be the result of inflating scores regardless of the country of origin of the simulated person. The upward bias was stronger for South Korean targets than US American targets, which provides additional evidence that RLHF could be at play.</p>
<h2>Directions for future research</h2>
<p>Use of alternative methods to extract data from LLMs. To validate that LLMs can indeed robustly replicate cross-cultural differences in personality, it would be advisable to use alternative methods. Instead of asking an LLM to generate text (like we did), one could analyze word embeddings (Cutler \&amp; Condon, 2023; Hussain et al., 2024). To illustrate, Cutler and Condon (2023) asked the DeBERTa model (also based on the Transformer architecture) to complete queries such as "Those close to me say I have a [MASK][MASK] and [TERM] personality.", where TERM is an adjectives from Saucier and Goldberg (1996), and the model must fill what is in the MASK fields. Models can be prompted using different languages (e.g., English and Korean), which makes it possible to test whether words in different languages (cultures) aggregate in a similar manner, e.g., consistent to a five factor solution (Cutler \&amp; Condon, 2023). A similar procedure could also be used to assess the probability that each of the adjectives is used in a particular personality-specific prompt. This could reflect crosscultural differences in tendencies, for example, higher extraversion in one culture relative to another one.</p>
<p>Using different personality models and inventories. To confirm that the patterns we observed using a very short (ten item) personality inventory measuring the Big Five are valid, future research could see whether findings hold using longer inventories, such as the 44-item Big Five Inventory (John et al., 2012) or the 120-item IPIP-NEO-120 inventory (Johnson, 2014). Our recommendation would be for researchers to use longer inventories with stronger</p>
<p>properties than the very short TIPI (specifically with structural and metric invariance), which would enable to test whether there is structural and metric invariance when asking the LLM to imagine (1) being a person from different countries using the same inventory, (2) using different versions of the inventory (i.e., adapted to different cultures, and in different languages). An alternative course would be to use other prominent, non-Big Five personality models, such as HEXACO (Lee \&amp; Ashton, 2004), which also show cross-cultural differences, although in a somewhat flawed manner, as HEXACO-100 does not show scalar invariance (Thielmann et al., 2020).
Investigating other patterns present in studies on personality. There are substantial gender differences in personality, which vary across cultures (Costa Jr. et al., 2001; Giolla \&amp; Kajonius, 2019). LLMs could be used to try to replicate other personality patterns. These do not necessarily how to vary cross-culturally, e.g., it is known that neuroticism decreases with age (Roberts \&amp; Mroczek, 2008). LLMs could be prompted to play the role of a people of different genders and of different ages, to see whether these changes resulting from age are also replicated.
Using different LLMs. We have used GPT from OpenAI (OpenAI, 2023), which is a proprietary model. Programmatic access to it (via an API) is costly, and there is no ability to access its training data and its weights. Others have proposed the use of simpler, nonproprietary LLMs to study personality. A good recommendation (Cutler \&amp; Condon, 2023; Hussain et al., 2024) is to use the open-access Hugging Face interface, which enables easy access to a wide variety of LLMs that also rely on the potent Transformer architecture. Many of these models can entirely by controlled by researchers. This is in contrast to GPT, which is a proprietary model that is updated quite frequently, hindering the replicability of findings. Hugging Face provides access to prominent (partially) open-source alternatives to GPT such as LLaMA (Touvron et al., 2023).
Methodological issues to consider. There remain some methodological challenges involved with using LLMs for in-silico experiments. Classical statistical tests - such as the two-sample $t$-test that we used to compare humans with the LLM - rely on the assumption that observations within a sample are independent, while data generated by LLMs is always generated by the very same model, even though it may be configured to exhibit as much variation as possible. Even though we attempted to maximize variability of output by using the maximum temperature setting - making output the least deterministic - simulated scores exhibited less variation than in the original TIPI and TIPI-K datasets. This is likely a reflection of a known flaw of LLMs and other neural networks called "mode collapse" (e.g., Hamilton, 2024), where certain output is overrepresented. Future research should be cognizant of this flaw when carrying out in-silica experiments (see Hussain et al. (2024) for other recommendations). Hopefully, advancements in LLMs will allow to more accurately reflect variation in personality across humans.</p>
<h1>Part B. Materials used to elicit personality responses</h1>
<h2>English/TIPI</h2>
<h2>System message (pre-prompt)</h2>
<p>You are playing the role of an adult from the United States./
You are playing the role of an adult from South Korea.</p>
<h2>Prompt</h2>
<p>Here are a number of personality traits that may or may not apply to you. Please write, separately for each item and in square brackets, a number that indicates the extent to which you agree or disagree with that statement. You should rate the extent to which the pair of traits applies to you, even if one characteristic applies more strongly than the other. $\backslash \mathrm{n} 1=$ Disagree strongly $\backslash 2$ = Disagree moderately $\backslash 3$ = Disagree a little $\backslash 4$ = Neither agree nor disagree $\backslash 5$ = Agree a little $\backslash 6$ = Agree moderately $\backslash 7$ = Agree strongly $\backslash \mathrm{n} \backslash \mathrm{I}$ see myself as: $\backslash$ n1. $\qquad$ Extraverted, enthusiastic. $\backslash$ n2. $\qquad$ Critical, quarrelsome. $\backslash$ n3. $\qquad$ Dependable, self-disciplined. $\backslash$ n4. $\qquad$ Anxious, easily upset. $\backslash$ n5. $\qquad$ Open to new experiences, complex. $\backslash$ n6. $\qquad$ Reserved, quiet. $\backslash$ n7. $\qquad$ Sympathetic, warm. $\backslash$ n8. $\qquad$ Disorganized, careless. $\backslash$ n9. $\qquad$ Calm, emotionally stable. $\backslash$ n10. $\qquad$ Conventional, uncreative. $\backslash$ I see myself as:"</p>
<h2>Korean/TIPI-K</h2>
<h2>System message (pre-prompt)</h2>
<p>당신은 미국에서 온 성인 역할을 하고 있습니다./
당신은 한국에서 온 성인 역할을 하고 있습니다.</p>
<h2>Prompt</h2>
<p>다음은 귀하의 성격 특성들에 관한 진술들입니다. 귀하가 각 진술에 동의하거나 동의하지 않는 정도를 각 진술 꺾쇠 괄호에 숫자로 [1 - 7] 표시해 주십시오. 각 진술은 두 가지의 성격 특성들을 표현하고 있습니다. 두 개의 성격 특성들 중 하나의 성격 특성이 다른 하나의 성격 특성보다 귀하의 성격을 더 잘 표현하더라도 그 두 개의 성격 특성들 모두가 귀하에게 적용되는 정도를 아래 숫자들로 평가해 주십시오. 대답을 꺾쇠괄호에서 '1. [5]'처럼 쓰십시오. $\backslash \mathrm{n} 1=$ 전혀 동의하지 않는다 $\backslash \mathrm{n} 2=$ 동의하지 않는다 $\backslash 3$ = 그다지 동의하지 않는다 $\backslash 4=$ 중간이다 $\backslash 5=$ 어느 정도 동의한다 $\backslash 6=$ 동의한다 $\backslash 7$ = 매우 동의한다 $\backslash \mathrm{n} \backslash \mathrm{n}$ 내가 보기에 나 자신은: $\backslash \mathrm{n} 1 . \quad$ $\qquad$ 외향적이다. 적극적이다. $\backslash 2$. $\qquad$ 비판적이다. 논쟁을 좋아한다. $\backslash 3$. $\qquad$ 신뢰할수있다. 자기 절제를 잘한다. $\backslash 4$. $\qquad$ 근심 걱정이 많다. 쉽게 흥분한다. $\backslash 5$. $\qquad$ 새로운</p>
<p>경험들에 개방적이다. 복잡다단하다. \n6. $\qquad$ 내성적이다. 조용하다.\n7. $\qquad$ 동정심이 많다. 다정다감하다. \n8. $\qquad$ 정리정돈을 잘못한다. 덤벙댄다. \n9. $\qquad$ 차분하다. 감정의 기복이 적다. \n10. $\qquad$ 변화를 싫어한다. 창의적이지 못하다. \n내가 보기에 나 자신은:</p>
<h1>Part C. Supplementary analyses</h1>
<p>Note that we conducted a similar (but less detailed) analysis on an earlier version of GPT-3.5. However, the resurgence of studies suggesting the vast superiority of GPT-4 relative to GPT-3.5 for some tasks (Bubeck et al., 2023) led us to reexamine this issue, using GPT-4 as the workhorse, and a stronger design for the study. It is worth noting that GPT-4 cost (at the moment we ran our study) roughly 25 times as much to run than GPT-3.5, and thus the costs of running large-sample simulations become non-negligible for GPT-4. We should also add that since the study was conducted, newer versions of GPT have become available. These also have different price tiers: the more advanced model (GPT-4o) is roughly 33 times as expensive as the less expensive model (GPT-4o-mini). Researchers should remain cognizant of these substantial price differences, which will only become more pronounced when using longer inventories.</p>
<p>Validity of output. Practically all of the output from GPT-4 was in the correct format, whereas $19 \%$ of the output from GPT-3.5 was in the incorrect format (answers were not provided in square brackets, or more than two answers were given to some items). This was essentially due to input being more often in the incorrect format when the input was in Korean ( $35 \%$ of output was in an erroneous format) rather than in English ( $3 \%$ error rate).</p>
<p>Structural validity. GPT-4 generated data that had $\alpha \mathrm{s}$ between 0.07 (Agreeableness) to 0.54 (Extraversion) - using TIPI - and between 0.04 (Agreeableness) to 0.70 (Extraversion) using TIPI-K. Perhaps the most surprising differences are for Emotional Stability, which had an $\alpha$ of .73 in the original study, but only .19 (TIPI) and .16 (TIPI-K) using the simulated data. However, GPT-3.5 generated data that was entirely inconsistent, with $\alpha \mathrm{s}$ ranging from -.30 to .07 for TIPI and from -.18 to .00 for TIPI-K. Therefore, we performed our main analysis solely on GPT-4.</p>
<p>Accuracy. Given that the output from GPT-3.5 was often in the incorrect format (when prompted in Korean), and the structural validity of the output was inconsistent, comparisons between GPT-4 and GPT-3.5 are problematic. However, it is nonetheless worth noting that GPT-3.5 $(M=1.41)$ was more accurate than GPT-4 $(M=1.65)$ when prompted in English. In contrast, when prompted in Korean, GPT-4 $(M=1.54)$ was more accurate than GPT-3.5 $(M=1.64)$. It is unclear why this pattern emerged (mainly, the slight advantage of GPT-3.5 in English), although one could speculate that this could be due to the intensity of Reinforcement Learning from Human Feedback between these generations, which has become more pronounced in newer generations.</p>
<h1>Part D. Supplementary tables and figures</h1>
<h2>Table S1</h2>
<p>Differences in the Big Five between US and South Korea using the IPIP-NEO-120 (Johnson, 2014; Kajonius \&amp; Giolla, 2017)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Factor</th>
<th style="text-align: center;">US</th>
<th style="text-align: center;">South Korea</th>
<th style="text-align: center;">Difference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Extraversion</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">-0.22</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">Agreeableness</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">-0.47</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">Conscientiousness</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">-0.13</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">Neuroticism</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">-0.04</td>
</tr>
<tr>
<td style="text-align: center;">Openness to Experiences</td>
<td style="text-align: center;">-0.22</td>
<td style="text-align: center;">-0.30</td>
<td style="text-align: center;">0.08</td>
</tr>
</tbody>
</table>
<p>Note. This table presents standardized mean scores ( $z$-scores) from Table 2 in Kajonius and Giolla (2017), showing how the mean for a specific country deviates for the mean score across all countries that were investigated. In Gosling et al. (2003) and in the main text of the manuscript, we report "Emotional Stability" instead of "Neuroticism".</p>
<p>Table S2
Difference between US and South Korea scores on Big Five using different inventories</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">TIPI and <br> TIPI-K <br> (Gosling et al., <br> 2003; Ha et al., <br> 2013)</th>
<th style="text-align: center;">NEO-PI-R <br> (Piedmont \&amp; <br> Chae, 1997)</th>
<th style="text-align: center;">NEO-PI-R <br> (Yoon et al., <br> 2002)</th>
<th style="text-align: center;">IPIP-NEO-120 <br> (Kajonius \&amp; <br> Giolla, 2017)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Extraversion</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Agreeableness</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Conscientiousness</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Neuroticism</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">n.s.</td>
</tr>
<tr>
<td style="text-align: left;">Openness to <br> Experiences</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Note. + and - indicate that South Koreans had a higher and lower score than US Americans, respectively. In Gosling et al. (2003) and in the main text of the manuscript, we report "Emotional Stability" instead of "Neuroticism".</p>
<p>Table S3 Reliability metrics for actual and simulated scores</p>
<table>
<thead>
<tr>
<th></th>
<th>Cronbach's $\alpha$</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Guttman's $\lambda_{6}$</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TIPI</td>
<td></td>
<td></td>
<td>TIPI-K</td>
<td></td>
<td></td>
<td>TIPI</td>
<td></td>
<td>TIPI-K</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Original study</td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td></td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td>GPT-4</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>Extraversion</td>
<td>0.68</td>
<td>0.54</td>
<td>$-0.30$</td>
<td>0.70</td>
<td>$-0.00$</td>
<td></td>
<td>0.54</td>
<td>$-0.30$</td>
<td>0.56</td>
<td>$-0.00$</td>
</tr>
<tr>
<td>Agreeableness</td>
<td>0.40</td>
<td>0.07</td>
<td>$-0.02$</td>
<td>0.04</td>
<td>$-0.06$</td>
<td></td>
<td>0.04</td>
<td>$-0.01$</td>
<td>0.02</td>
<td>$-0.04$</td>
</tr>
<tr>
<td>Conscientiousness</td>
<td>0.50</td>
<td>0.31</td>
<td>$-0.06$</td>
<td>0.17</td>
<td>$-0.06$</td>
<td></td>
<td>0.19</td>
<td>$-0.03$</td>
<td>0.10</td>
<td>$-0.03$</td>
</tr>
<tr>
<td>Emotional Stability</td>
<td>0.73</td>
<td>0.19</td>
<td>0.02</td>
<td>0.16</td>
<td>$-0.18$</td>
<td></td>
<td>0.10</td>
<td>0.01</td>
<td>0.09</td>
<td>$-0.08$</td>
</tr>
<tr>
<td>Openness to Experiences</td>
<td>0.45</td>
<td>0.26</td>
<td>0.07</td>
<td>0.23</td>
<td>$-0.05$</td>
<td></td>
<td>0.15</td>
<td>0.04</td>
<td>0.13</td>
<td>$-0.03$</td>
</tr>
</tbody>
</table>
<p>Note. Computed using R package psych (Revelle, 2023). There is no data for Cronbach's $\alpha$ for TIPI-K (Ha et al., 2013).</p>            </div>
        </div>

    </div>
</body>
</html>
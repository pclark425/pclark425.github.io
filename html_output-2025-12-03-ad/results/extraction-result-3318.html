<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3318 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3318</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3318</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4b4ba6a02148c9d6f78e95d8e0d927104c3e91a7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4b4ba6a02148c9d6f78e95d8e0d927104c3e91a7" target="_blank">Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning.</p>
                <p><strong>Paper Abstract:</strong> Abstract Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.1</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3318.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3318.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaVinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Davinci (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained GPT-3 family model used as the base pretrained LM in experiments; evaluated in zero-shot and few-shot formats for decision and reasoning tasks probing cognitive-like biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci (GPT3 pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained GPT-3 series model used as the baseline (pre-instruction-tuning) in the paper's experiments; evaluated via answer-likelihood scoring for choice/ reasoning prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting (format few-shot)', 'likelihood-based answer scoring (with DC-PMI correction explored)', 'greedy decoding (noted for IT models; reported behavior comparison)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models are prompted in zero-shot or few-shot formats; 'format few-shot' uses example prompts that show format without using task samples. Pretrained models' outputs are evaluated by computing likelihoods over candidate answers (with DC-PMI normalization discussed) rather than relying on generated text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar styles (prompt-based variations). The paper varies prompting (zero-shot vs format few-shot) but does not apply distinct architectural or multi-method reasoning strategies to this pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Three tasks adapted from cognitive psychology: (1) Decoy effect: choice between alternatives with/without an asymmetrically dominated decoy; (2) Certainty effect: choice between lotteries where one alternative becomes certain; (3) Belief bias: judging logical validity of syllogisms with believable vs non-believable conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (treatment minus control) reported in Table 3: Decoy Expensive: -0.15 (significant), Decoy Cheaper: -0.17 (significant), Certainty: 0.00, Belief Valid: 0.00, Belief Invalid: 0.04. Evaluation primarily zero-shot; a small decrease in bias observed with 1-shot format few-shot in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Pretrained DaVinci shows relatively low or negative bias scores compared to its instruction-tuned / RLHF counterparts; few-shot format examples had limited effect on reducing biases for decoy and certainty, but some reduction in belief bias when task few-shot examples were used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The pretrained GPT-3 baseline exhibits minimal-to-moderate bias on these cognitive tasks and is less biased than instruction-tuned or RLHF variants; prompting variations (zero-shot vs format few-shot) only modestly affect bias for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For DaVinci, some decoy biases are negative (indicating positional or non-value-driven preferences). Few-shot format did not substantially reduce decoy/certainty biases; belief bias reduction was more pronounced only with task few-shot that contained reasoning examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3318.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaVinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-DaVinci-002 (GPT3.5, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned GPT-3.5 variant (DaVinci-002) used to study the effect of instruction tuning on decision and reasoning biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci-002 (GPT3.5, IT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of the GPT-3 family used to examine how instruction tuning (IT) affects cognitive-like biases; evaluated in zero-shot and few-shot formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting (format few-shot and task few-shot)', 'greedy decoding for generated-choice responses', 'likelihood scoring (for pretrained comparisons)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Instruction-tuned model typically emits direct answer strings; experiments used zero-shot and few-shot prompts (format and task) to probe biases. Likelihood-based evaluation and DC-PMI correction was discussed for scoring responses.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles with variations; the model chiefly uses instruction-following generation (a single style) rather than multiple distinct reasoning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of cognitive decision and reasoning tasks adapted from human psychology experiments; measures changes in choice/validity judgments between control and treatment conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (Table 3): Decoy Expensive: -0.13 (significant), Decoy Cheaper: -0.08 (significant), Certainty: 0.24 (significant), Belief Valid: 0.19 (significant), Belief Invalid: 0.55 (significant). Overall instruction tuning increased bias in multiple tasks relative to pretrained DaVinci.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to pretrained DaVinci, DaVinci-002 shows amplified biases (particularly certainty and belief invalid); format few-shot and task few-shot reduce belief bias more than decoy/certainty in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction tuning alone amplifies several cognitive-like biases (notably belief invalid and certainty) relative to the pretrained model, indicating that IT can introduce or increase human-like biases.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although IT amplifies bias generally, not all biases uniformly increase; some decoy measures remain negative or small depending on setup. Few-shot format usually yields only modest improvement; task few-shot yields stronger mitigation for belief bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3318.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-DaVinci-003 (GPT3.5, IT + RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A later GPT3.5 release (DaVinci-003) incorporating instruction tuning and RLHF; used to examine RLHF's additional effect on biases relative to IT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci-003 (GPT3.5, IT + RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned and RLHF-fine-tuned GPT-3.5 variant; analyzed across cognitive decision/reasoning tasks in zero-shot and few-shot prompts to measure bias amplification by RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting (format few-shot and task few-shot)', 'greedy decoding for instruction outputs', 'likelihood scoring for comparisons (DC-PMI correction referenced)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is prompted similarly to other LMs; the paper contrasts its behavior to both pretrained and IT-only versions to isolate RLHF effects. Few-shot formats (format vs task) are used to analyze mitigation effects.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles (zero-shot/ few-shot) are used; no multiple distinct reasoning frameworks beyond prompt variations are applied to this model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Cognitive-bias tasks where treatment/control differences reveal susceptibility to decoy, certainty, and belief biases; belief task includes logical validity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (Table 3): Decoy Expensive: -0.02, Decoy Cheaper: 0.08 (some products significant), Certainty: 0.67 (significant, large), Belief Valid: 0.21 (significant), Belief Invalid: 0.65 (significant). In 1-shot comparison (Table 4) vs GPT4: Decoy Expensive 0.00, Decoy Cheaper 0.03, Certainty 0.43 (significant), Belief Valid 0.20 (significant), Belief Invalid 0.47 (significant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to DaVinci-002 (IT only), DaVinci-003 (RLHF) shows further amplification in several biases (especially certainty and belief invalid), suggesting RLHF can amplify biases beyond IT. Few-shot (format/task) reduced belief bias more than decoy/certainty; overall RLHF did not mitigate these cognitive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RLHF (DaVinci-003) tends to amplify cognitive-like biases further compared to IT-only variants; certainty and belief-invalid biases are particularly high for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although RLHF amplified biases overall, some decoy measurements (Decoy Expensive) remain small/near-zero; few-shot formatting sometimes produces only modest reductions in bias; RLHF did not consistently mitigate biases despite RLHF's goal of human-aligned outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3318.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A later OpenAI model (GPT-4) used as a reference SOTA model; evaluated (1-shot) for the same cognitive bias tasks and compared to DaVinci-003.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art generative LM (OpenAI GPT-4) used as a reference model; noted training included datasets to improve formal reasoning (e.g., MATH, GSM-8K) according to OpenAI, though the paper did not have access to a pretrained GPT-4 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['one-shot/few-shot prompting (1-shot presented for main comparisons)', 'greedy decoding / direct answer generation', 'implicit chain-like reasoning improvements due to specialized training (MATH, GSM-8K mentioned as part of its training data)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-4 was evaluated primarily in 1-shot few-shot prompts because in zero-shot it sometimes refrained from providing explicit choices; authors note GPT-4 was trained with reasoning datasets, which may improve logical performance but did not eliminate cognitive biases in decision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar styles: primarily few-shot prompting; model benefits from improved reasoning data in training but experiments do not apply diverse prompting strategies beyond 1-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same cognitive-decision and syllogistic reasoning tasks; measured bias scores and logical reasoning accuracy (belief task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>1-shot bias scores (Table 4): Decoy Expensive: 0.38 (significant), Decoy Cheaper: 0.05, Certainty: 0.20 (significant), Belief Valid: 0.15 (significant), Belief Invalid: 0.41 (significant). Reported belief-task accuracy: 84% (higher than GPT3 family).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to DaVinci-003 (1-shot), GPT-4 has higher decoy expensive score but lower certainty and some belief bias scores; GPT-4 achieves higher logical accuracy while still exhibiting substantial bias, indicating improved reasoning does not fully eliminate these cognitive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 shows improved logical accuracy (belief task) but still exhibits pronounced cognitive biases (notably decoy effect), suggesting that training for formal reasoning (MATH, GSM-8K) may help accuracy but not fully mitigate all human-aligned biases.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with higher accuracy, GPT-4 demonstrates notable decoy biases; zero-shot sometimes causes GPT-4 to abstain, requiring 1-shot evaluation. Thus improved reasoning capability did not remove all biases and sometimes increased specific biases (decoy expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3318.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A publicly available 7B-parameter pretrained LM evaluated as a baseline for instruction tuning effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Mistral 7B model used as an LM baseline; compared to Mistral-Instruct to assess the effect of instruction tuning on bias.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'format few-shot prompting', 'likelihood-based evaluation for pretrained comparisons']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Mistral-7B was prompted zero-shot and with format few-shot examples; behavior compared to its instruction-tuned counterpart to observe bias amplification from IT.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles; no multiple or diverse reasoning strategies beyond prompt variations were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decision and syllogistic reasoning tasks adapted to textual prompts to measure treatment-control shifts indicating bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (Table 3): Decoy Expensive: 0.03, Decoy Cheaper: -0.05 (significant), Certainty: 0.03, Belief Valid: 0.01, Belief Invalid: 0.05. Overall low bias relative to instruction-tuned variant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Mistral-7B shows low bias; its instruction-tuned variant (Mistral-Instruct) has larger bias scores (e.g., Decoy Expensive 0.24**, Certainty 0.29*), indicating instruction tuning amplified biases. Format few-shot increased certainty bias for pretrained Mistral in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained Mistral exhibits low bias; instruction tuning strongly increases bias in multiple categories, demonstrating IT's amplifying effect across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Format few-shot caused a rise in certainty bias even in pretrained Mistral (from 0.03 to 0.31 in some tests), showing few-shot formatting can sometimes increase bias rather than reduce it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3318.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B-Instruct (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned Mistral-7B variant used to analyze how instruction tuning changes bias behavior compared to pretrained Mistral.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-Instruct (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral 7B model fine-tuned with instruction-style data and evaluated using the recommended chat template; compared directly to the pretrained Mistral.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting with chat/instruction template', 'format few-shot prompting', 'greedy decoding for direct-choice generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Used instruction prompt template ('You are a helpful assistant... Answer shortly with only your choice') and both zero-shot and few-shot formats to probe bias. Responses are direct generated answers rather than likelihood-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Prompts variations only; the model primarily uses similar instruction-following generation style across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same set of cognitive tasks; experiments include format few-shot which had mixed effects for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (Table 3): Decoy Expensive: 0.24 (some products significant), Decoy Cheaper: -0.03 (mixed significance), Certainty: 0.29 (significant), Belief Valid: 0.26 (significant), Belief Invalid: 0.31 (significant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Instruction tuning increased bias compared to pretrained Mistral; format few-shot reduced belief and decoy biases consistently for Mistral-Instruct but increased certainty bias, illustrating that few-shot effects can be architecture/training-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction tuning (Mistral-Instruct) amplifies bias across tasks; few-shot formatting often reduced belief and decoy biases for this model but caused an increase in certainty bias, highlighting non-uniform effects of few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Mistral-Instruct's certainty bias increased when given more format few-shot examples; thus few-shot does not universally mitigate bias and can exacerbate particular bias types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3318.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained T5 family models evaluated as a baseline; compared to Flan-T5 (instruction-tuned) variants to study IT effects on biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained T5 models (evaluated variants) used as baselines; evaluated in zero-shot formatting and compared to Flan-T5 instruction-tuned variants of matching sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'format few-shot prompting', 'likelihood-based evaluation for pretrained comparisons']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Pretrained T5 models are used in zero-shot and few-shot settings; outputs for decision tasks are assessed by likelihood scoring to determine choice probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar prompting styles; no additional reasoning algorithm diversity applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same cognitive tasks; T5 baseline used to contrast effects of Flan-T5 instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (Table 3): Decoy Expensive: 0.02, Decoy Cheaper: -0.15 (significant), Certainty: 0.09 (significant), Belief Valid: -0.03, Belief Invalid: 0.03. Overall low-to-moderate bias.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to Flan-T5, pretrained T5 has substantially lower bias scores; instruction tuning to Flan-T5 increases several biases (e.g., Belief Valid 0.50 for Flan-T5-XXL vs -0.03 for T5 pretrained).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained T5 shows low bias; IT (Flan-T5) amplifies biases significantly, consistent with other families.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some negative/near-zero measurements for T5 (e.g., Belief Valid -0.03) indicate baseline variability; few-shot effects not detailed extensively for pretrained T5 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3318.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3318.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XL/XXL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (instruction-tuned) XL and XXL variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned (Flan) T5 models evaluated at two sizes (XL and XXL) to study effects of instruction tuning and model size on bias emergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5 (instruction-tuned) - XL (3B) and XXL (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned T5 variants (Flan-T5) used to test how instruction tuning and model size affect cognitive biases. Paper explicitly uses XXL (11B) and XL (3B) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XL: 3B; XXL: 11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'format few-shot and task few-shot prompting', 'greedy decoding for answer generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Flan-T5 models respond with direct textual choices under instruction templates. The authors run zero-shot and few-shot variants and analyze how size (XL vs XXL) impacts bias scores and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar instruction-following generation across sizes; experiments vary prompting formats but do not introduce fundamentally different reasoning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same cognitive tasks; also used to analyze model-size effects on bias emergence and accuracy-bias trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Bias scores (Table 3, Flan-T5 reported under IT-LM column): Decoy Expensive: -0.18 (significant for XXL), Decoy Cheaper: 0.20 (significant), Certainty: 0.17 (significant), Belief Valid: 0.50 (significant), Belief Invalid: 0.39 (significant). Figure 1 shows size effect: Flan-T5-XXL exhibits higher bias in Decoy Cheaper, Certainty, and Belief Valid versus XL but lower in Decoy Expensive and Belief Invalid, linked to accuracy differences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Instruction tuning (Flan) amplifies biases relative to pretrained T5. Increasing model size (XL to XXL) increases several bias scores (decoy cheaper, certainty, belief valid) though some biases decrease (decoy expensive, belief invalid) owing to accuracy differences on neutral conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Flan-T5 demonstrates that instruction tuning amplifies cognitive biases and that larger models can show stronger biases in several categories, though some decreases in bias scores can arise from shifts in base accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Flan-T5-XXL's negative Decoy Expensive score stems from an extreme baseline preference for the expensive target in control condition (near 100%), creating a counterintuitive negative bias score; larger models' higher accuracy sometimes correlates with higher measured bias (accuracy–bias tradeoff).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>Machine intuition: Uncovering human-like intuitive decision-making in gpt3.5 <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 1)</em></li>
                <li>The unlocking spell on base llms: Rethinking alignment via in-context learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3318",
    "paper_id": "paper-4b4ba6a02148c9d6f78e95d8e0d927104c3e91a7",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "DaVinci",
            "name_full": "GPT-3 Davinci (pretrained)",
            "brief_description": "Pretrained GPT-3 family model used as the base pretrained LM in experiments; evaluated in zero-shot and few-shot formats for decision and reasoning tasks probing cognitive-like biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DaVinci (GPT3 pretrained)",
            "model_description": "A pretrained GPT-3 series model used as the baseline (pre-instruction-tuning) in the paper's experiments; evaluated via answer-likelihood scoring for choice/ reasoning prompts.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting (format few-shot)",
                "likelihood-based answer scoring (with DC-PMI correction explored)",
                "greedy decoding (noted for IT models; reported behavior comparison)"
            ],
            "reasoning_methods_description": "Models are prompted in zero-shot or few-shot formats; 'format few-shot' uses example prompts that show format without using task samples. Pretrained models' outputs are evaluated by computing likelihoods over candidate answers (with DC-PMI normalization discussed) rather than relying on generated text tokens.",
            "diversity_of_methods": "Similar styles (prompt-based variations). The paper varies prompting (zero-shot vs format few-shot) but does not apply distinct architectural or multi-method reasoning strategies to this pretrained model.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Three tasks adapted from cognitive psychology: (1) Decoy effect: choice between alternatives with/without an asymmetrically dominated decoy; (2) Certainty effect: choice between lotteries where one alternative becomes certain; (3) Belief bias: judging logical validity of syllogisms with believable vs non-believable conclusions.",
            "performance_by_method": "Bias scores (treatment minus control) reported in Table 3: Decoy Expensive: -0.15 (significant), Decoy Cheaper: -0.17 (significant), Certainty: 0.00, Belief Valid: 0.00, Belief Invalid: 0.04. Evaluation primarily zero-shot; a small decrease in bias observed with 1-shot format few-shot in some cases.",
            "comparison_of_methods": "Pretrained DaVinci shows relatively low or negative bias scores compared to its instruction-tuned / RLHF counterparts; few-shot format examples had limited effect on reducing biases for decoy and certainty, but some reduction in belief bias when task few-shot examples were used.",
            "key_findings": "The pretrained GPT-3 baseline exhibits minimal-to-moderate bias on these cognitive tasks and is less biased than instruction-tuned or RLHF variants; prompting variations (zero-shot vs format few-shot) only modestly affect bias for this model.",
            "counter_examples_or_negative_results": "For DaVinci, some decoy biases are negative (indicating positional or non-value-driven preferences). Few-shot format did not substantially reduce decoy/certainty biases; belief bias reduction was more pronounced only with task few-shot that contained reasoning examples.",
            "uuid": "e3318.0",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "DaVinci-002",
            "name_full": "text-DaVinci-002 (GPT3.5, instruction-tuned)",
            "brief_description": "An instruction-tuned GPT-3.5 variant (DaVinci-002) used to study the effect of instruction tuning on decision and reasoning biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DaVinci-002 (GPT3.5, IT)",
            "model_description": "Instruction-tuned variant of the GPT-3 family used to examine how instruction tuning (IT) affects cognitive-like biases; evaluated in zero-shot and few-shot formats.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting (format few-shot and task few-shot)",
                "greedy decoding for generated-choice responses",
                "likelihood scoring (for pretrained comparisons)"
            ],
            "reasoning_methods_description": "Instruction-tuned model typically emits direct answer strings; experiments used zero-shot and few-shot prompts (format and task) to probe biases. Likelihood-based evaluation and DC-PMI correction was discussed for scoring responses.",
            "diversity_of_methods": "Similar prompting styles with variations; the model chiefly uses instruction-following generation (a single style) rather than multiple distinct reasoning algorithms.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Same set of cognitive decision and reasoning tasks adapted from human psychology experiments; measures changes in choice/validity judgments between control and treatment conditions.",
            "performance_by_method": "Bias scores (Table 3): Decoy Expensive: -0.13 (significant), Decoy Cheaper: -0.08 (significant), Certainty: 0.24 (significant), Belief Valid: 0.19 (significant), Belief Invalid: 0.55 (significant). Overall instruction tuning increased bias in multiple tasks relative to pretrained DaVinci.",
            "comparison_of_methods": "Compared to pretrained DaVinci, DaVinci-002 shows amplified biases (particularly certainty and belief invalid); format few-shot and task few-shot reduce belief bias more than decoy/certainty in many cases.",
            "key_findings": "Instruction tuning alone amplifies several cognitive-like biases (notably belief invalid and certainty) relative to the pretrained model, indicating that IT can introduce or increase human-like biases.",
            "counter_examples_or_negative_results": "Although IT amplifies bias generally, not all biases uniformly increase; some decoy measures remain negative or small depending on setup. Few-shot format usually yields only modest improvement; task few-shot yields stronger mitigation for belief bias.",
            "uuid": "e3318.1",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "DaVinci-003",
            "name_full": "text-DaVinci-003 (GPT3.5, IT + RLHF)",
            "brief_description": "A later GPT3.5 release (DaVinci-003) incorporating instruction tuning and RLHF; used to examine RLHF's additional effect on biases relative to IT alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DaVinci-003 (GPT3.5, IT + RLHF)",
            "model_description": "Instruction-tuned and RLHF-fine-tuned GPT-3.5 variant; analyzed across cognitive decision/reasoning tasks in zero-shot and few-shot prompts to measure bias amplification by RLHF.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting (format few-shot and task few-shot)",
                "greedy decoding for instruction outputs",
                "likelihood scoring for comparisons (DC-PMI correction referenced)"
            ],
            "reasoning_methods_description": "The model is prompted similarly to other LMs; the paper contrasts its behavior to both pretrained and IT-only versions to isolate RLHF effects. Few-shot formats (format vs task) are used to analyze mitigation effects.",
            "diversity_of_methods": "Similar prompting styles (zero-shot/ few-shot) are used; no multiple distinct reasoning frameworks beyond prompt variations are applied to this model.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Cognitive-bias tasks where treatment/control differences reveal susceptibility to decoy, certainty, and belief biases; belief task includes logical validity judgments.",
            "performance_by_method": "Bias scores (Table 3): Decoy Expensive: -0.02, Decoy Cheaper: 0.08 (some products significant), Certainty: 0.67 (significant, large), Belief Valid: 0.21 (significant), Belief Invalid: 0.65 (significant). In 1-shot comparison (Table 4) vs GPT4: Decoy Expensive 0.00, Decoy Cheaper 0.03, Certainty 0.43 (significant), Belief Valid 0.20 (significant), Belief Invalid 0.47 (significant).",
            "comparison_of_methods": "Compared to DaVinci-002 (IT only), DaVinci-003 (RLHF) shows further amplification in several biases (especially certainty and belief invalid), suggesting RLHF can amplify biases beyond IT. Few-shot (format/task) reduced belief bias more than decoy/certainty; overall RLHF did not mitigate these cognitive biases.",
            "key_findings": "RLHF (DaVinci-003) tends to amplify cognitive-like biases further compared to IT-only variants; certainty and belief-invalid biases are particularly high for this model.",
            "counter_examples_or_negative_results": "Although RLHF amplified biases overall, some decoy measurements (Decoy Expensive) remain small/near-zero; few-shot formatting sometimes produces only modest reductions in bias; RLHF did not consistently mitigate biases despite RLHF's goal of human-aligned outputs.",
            "uuid": "e3318.2",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A later OpenAI model (GPT-4) used as a reference SOTA model; evaluated (1-shot) for the same cognitive bias tasks and compared to DaVinci-003.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art generative LM (OpenAI GPT-4) used as a reference model; noted training included datasets to improve formal reasoning (e.g., MATH, GSM-8K) according to OpenAI, though the paper did not have access to a pretrained GPT-4 baseline.",
            "model_size": null,
            "reasoning_methods": [
                "one-shot/few-shot prompting (1-shot presented for main comparisons)",
                "greedy decoding / direct answer generation",
                "implicit chain-like reasoning improvements due to specialized training (MATH, GSM-8K mentioned as part of its training data)"
            ],
            "reasoning_methods_description": "GPT-4 was evaluated primarily in 1-shot few-shot prompts because in zero-shot it sometimes refrained from providing explicit choices; authors note GPT-4 was trained with reasoning datasets, which may improve logical performance but did not eliminate cognitive biases in decision tasks.",
            "diversity_of_methods": "Similar styles: primarily few-shot prompting; model benefits from improved reasoning data in training but experiments do not apply diverse prompting strategies beyond 1-shot.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Same cognitive-decision and syllogistic reasoning tasks; measured bias scores and logical reasoning accuracy (belief task).",
            "performance_by_method": "1-shot bias scores (Table 4): Decoy Expensive: 0.38 (significant), Decoy Cheaper: 0.05, Certainty: 0.20 (significant), Belief Valid: 0.15 (significant), Belief Invalid: 0.41 (significant). Reported belief-task accuracy: 84% (higher than GPT3 family).",
            "comparison_of_methods": "Compared to DaVinci-003 (1-shot), GPT-4 has higher decoy expensive score but lower certainty and some belief bias scores; GPT-4 achieves higher logical accuracy while still exhibiting substantial bias, indicating improved reasoning does not fully eliminate these cognitive biases.",
            "key_findings": "GPT-4 shows improved logical accuracy (belief task) but still exhibits pronounced cognitive biases (notably decoy effect), suggesting that training for formal reasoning (MATH, GSM-8K) may help accuracy but not fully mitigate all human-aligned biases.",
            "counter_examples_or_negative_results": "Even with higher accuracy, GPT-4 demonstrates notable decoy biases; zero-shot sometimes causes GPT-4 to abstain, requiring 1-shot evaluation. Thus improved reasoning capability did not remove all biases and sometimes increased specific biases (decoy expensive).",
            "uuid": "e3318.3",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B (pretrained)",
            "brief_description": "A publicly available 7B-parameter pretrained LM evaluated as a baseline for instruction tuning effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (pretrained)",
            "model_description": "Pretrained Mistral 7B model used as an LM baseline; compared to Mistral-Instruct to assess the effect of instruction tuning on bias.",
            "model_size": "7B",
            "reasoning_methods": [
                "zero-shot prompting",
                "format few-shot prompting",
                "likelihood-based evaluation for pretrained comparisons"
            ],
            "reasoning_methods_description": "Mistral-7B was prompted zero-shot and with format few-shot examples; behavior compared to its instruction-tuned counterpart to observe bias amplification from IT.",
            "diversity_of_methods": "Similar prompting styles; no multiple or diverse reasoning strategies beyond prompt variations were applied.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Decision and syllogistic reasoning tasks adapted to textual prompts to measure treatment-control shifts indicating bias.",
            "performance_by_method": "Bias scores (Table 3): Decoy Expensive: 0.03, Decoy Cheaper: -0.05 (significant), Certainty: 0.03, Belief Valid: 0.01, Belief Invalid: 0.05. Overall low bias relative to instruction-tuned variant.",
            "comparison_of_methods": "Mistral-7B shows low bias; its instruction-tuned variant (Mistral-Instruct) has larger bias scores (e.g., Decoy Expensive 0.24**, Certainty 0.29*), indicating instruction tuning amplified biases. Format few-shot increased certainty bias for pretrained Mistral in some experiments.",
            "key_findings": "Pretrained Mistral exhibits low bias; instruction tuning strongly increases bias in multiple categories, demonstrating IT's amplifying effect across architectures.",
            "counter_examples_or_negative_results": "Format few-shot caused a rise in certainty bias even in pretrained Mistral (from 0.03 to 0.31 in some tests), showing few-shot formatting can sometimes increase bias rather than reduce it.",
            "uuid": "e3318.4",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Mistral-Instruct",
            "name_full": "Mistral 7B-Instruct (instruction-tuned)",
            "brief_description": "Instruction-tuned Mistral-7B variant used to analyze how instruction tuning changes bias behavior compared to pretrained Mistral.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-Instruct (instruction-tuned)",
            "model_description": "Mistral 7B model fine-tuned with instruction-style data and evaluated using the recommended chat template; compared directly to the pretrained Mistral.",
            "model_size": "7B",
            "reasoning_methods": [
                "zero-shot prompting with chat/instruction template",
                "format few-shot prompting",
                "greedy decoding for direct-choice generation"
            ],
            "reasoning_methods_description": "Used instruction prompt template ('You are a helpful assistant... Answer shortly with only your choice') and both zero-shot and few-shot formats to probe bias. Responses are direct generated answers rather than likelihood-based scoring.",
            "diversity_of_methods": "Prompts variations only; the model primarily uses similar instruction-following generation style across tasks.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Same set of cognitive tasks; experiments include format few-shot which had mixed effects for this model.",
            "performance_by_method": "Bias scores (Table 3): Decoy Expensive: 0.24 (some products significant), Decoy Cheaper: -0.03 (mixed significance), Certainty: 0.29 (significant), Belief Valid: 0.26 (significant), Belief Invalid: 0.31 (significant).",
            "comparison_of_methods": "Instruction tuning increased bias compared to pretrained Mistral; format few-shot reduced belief and decoy biases consistently for Mistral-Instruct but increased certainty bias, illustrating that few-shot effects can be architecture/training-dependent.",
            "key_findings": "Instruction tuning (Mistral-Instruct) amplifies bias across tasks; few-shot formatting often reduced belief and decoy biases for this model but caused an increase in certainty bias, highlighting non-uniform effects of few-shot prompting.",
            "counter_examples_or_negative_results": "Mistral-Instruct's certainty bias increased when given more format few-shot examples; thus few-shot does not universally mitigate bias and can exacerbate particular bias types.",
            "uuid": "e3318.5",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "T5",
            "name_full": "T5 (pretrained)",
            "brief_description": "Pretrained T5 family models evaluated as a baseline; compared to Flan-T5 (instruction-tuned) variants to study IT effects on biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (pretrained)",
            "model_description": "Pretrained T5 models (evaluated variants) used as baselines; evaluated in zero-shot formatting and compared to Flan-T5 instruction-tuned variants of matching sizes.",
            "model_size": null,
            "reasoning_methods": [
                "zero-shot prompting",
                "format few-shot prompting",
                "likelihood-based evaluation for pretrained comparisons"
            ],
            "reasoning_methods_description": "Pretrained T5 models are used in zero-shot and few-shot settings; outputs for decision tasks are assessed by likelihood scoring to determine choice probabilities.",
            "diversity_of_methods": "Similar prompting styles; no additional reasoning algorithm diversity applied.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Same cognitive tasks; T5 baseline used to contrast effects of Flan-T5 instruction tuning.",
            "performance_by_method": "Bias scores (Table 3): Decoy Expensive: 0.02, Decoy Cheaper: -0.15 (significant), Certainty: 0.09 (significant), Belief Valid: -0.03, Belief Invalid: 0.03. Overall low-to-moderate bias.",
            "comparison_of_methods": "Compared to Flan-T5, pretrained T5 has substantially lower bias scores; instruction tuning to Flan-T5 increases several biases (e.g., Belief Valid 0.50 for Flan-T5-XXL vs -0.03 for T5 pretrained).",
            "key_findings": "Pretrained T5 shows low bias; IT (Flan-T5) amplifies biases significantly, consistent with other families.",
            "counter_examples_or_negative_results": "Some negative/near-zero measurements for T5 (e.g., Belief Valid -0.03) indicate baseline variability; few-shot effects not detailed extensively for pretrained T5 in the paper.",
            "uuid": "e3318.6",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Flan-T5-XL/XXL",
            "name_full": "Flan-T5 (instruction-tuned) XL and XXL variants",
            "brief_description": "Instruction-tuned (Flan) T5 models evaluated at two sizes (XL and XXL) to study effects of instruction tuning and model size on bias emergence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5 (instruction-tuned) - XL (3B) and XXL (11B)",
            "model_description": "Instruction-tuned T5 variants (Flan-T5) used to test how instruction tuning and model size affect cognitive biases. Paper explicitly uses XXL (11B) and XL (3B) variants.",
            "model_size": "XL: 3B; XXL: 11B",
            "reasoning_methods": [
                "zero-shot prompting",
                "format few-shot and task few-shot prompting",
                "greedy decoding for answer generation"
            ],
            "reasoning_methods_description": "Flan-T5 models respond with direct textual choices under instruction templates. The authors run zero-shot and few-shot variants and analyze how size (XL vs XXL) impacts bias scores and accuracy.",
            "diversity_of_methods": "Similar instruction-following generation across sizes; experiments vary prompting formats but do not introduce fundamentally different reasoning algorithms.",
            "reasoning_task_name": "Decoy effect, Certainty effect, Belief bias (syllogistic reasoning)",
            "reasoning_task_description": "Same cognitive tasks; also used to analyze model-size effects on bias emergence and accuracy-bias trade-offs.",
            "performance_by_method": "Bias scores (Table 3, Flan-T5 reported under IT-LM column): Decoy Expensive: -0.18 (significant for XXL), Decoy Cheaper: 0.20 (significant), Certainty: 0.17 (significant), Belief Valid: 0.50 (significant), Belief Invalid: 0.39 (significant). Figure 1 shows size effect: Flan-T5-XXL exhibits higher bias in Decoy Cheaper, Certainty, and Belief Valid versus XL but lower in Decoy Expensive and Belief Invalid, linked to accuracy differences.",
            "comparison_of_methods": "Instruction tuning (Flan) amplifies biases relative to pretrained T5. Increasing model size (XL to XXL) increases several bias scores (decoy cheaper, certainty, belief valid) though some biases decrease (decoy expensive, belief invalid) owing to accuracy differences on neutral conditions.",
            "key_findings": "Flan-T5 demonstrates that instruction tuning amplifies cognitive biases and that larger models can show stronger biases in several categories, though some decreases in bias scores can arise from shifts in base accuracy.",
            "counter_examples_or_negative_results": "Flan-T5-XXL's negative Decoy Expensive score stems from an extreme baseline preference for the expensive target in control condition (near 100%), creating a counterintuitive negative bias score; larger models' higher accuracy sometimes correlates with higher measured bias (accuracy–bias tradeoff).",
            "uuid": "e3318.7",
            "source_info": {
                "paper_title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2
        },
        {
            "paper_title": "Machine intuition: Uncovering human-like intuitive decision-making in gpt3.5",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 1
        },
        {
            "paper_title": "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "rating": 1
        }
    ],
    "cost": 0.017931,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</h1>
<p>Itay Itzhak ${ }^{1}$, Gabriel Stanovsky ${ }^{2}$, Nir Rosenfeld ${ }^{1}$, Yonatan Belinkov ${ }^{1}$<br>${ }^{1}$ Technion - Israel Institute of Technology<br>${ }^{2}$ School of Computer Science and Engineering, The Hebrew University of Jerusalem<br>itaylitzhak@gmail.com,<br>{nirr, belinkov}@technion.ac.il, gabriel.stanovsky@mail.huji.ac.il</p>
<h4>Abstract</h4>
<p>Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases-the decoy effect, the certainty effect, and the belief bias-all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, MistralInstruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Advanced fine-tuning methods, like instruction tuning (IT) and reinforcement learning from human feedback (RLHF), have recently emerged as essential paradigms for improving the alignment of language models (LMs) with human objectives (Ouyang et al., 2022; Bai et al., 2022). Although widely adopted (Zhou et al., 2023), the specific cases in which IT and RLHF enhance model behavior to resemble human behavior, and the mechanisms involved in this process, remain unclear.</p>
<p>In this work, we delve into the impact of IT and RLHF on decision-making and reason-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ing in LMs. Recent studies highlighted to some extent cognitive-like biases in pretrained LMs (Binz and Schulz, 2022; Dasgupta et al., 2022) and instruction-tuned models (Hagendorff et al., 2022). We take a step further, exploring the consequences of IT and RLHF interventions on LMs' cognitive-like behavior.</p>
<p>We inspect three well-researched and fundamental biases: the decoy effect (Huber et al., 1982), the certainty effect (Kahneman, 1979), and belief bias (Evans et al., 1983). These biases reflect basic inconsistencies in human decisionmaking (decoy and certainty effects) and fallacies in logical reasoning (belief bias) that are prevalent, persistent, and consequential (Berthet, 2022; Acciarini et al., 2021).</p>
<p>The conventional approach to studying cognitive biases in humans is to design simple experiments that elicit from human subjects either judgments or decisions that are likely to reflect a target bias. Many of these experiments involve question answering; Table 1 shows examples of questions used in such experiments, illustrating how the responses of subjects can suggest biased behavior. To study cognitive-like biases in LMs, we adapt classic human experiments to an LM setting. Towards this, we create an experimental dataset using semi-automatic generated decision tasks: First, for each bias, we manually create an array of appropriate task templates containing flexible numeric and textual placeholder variables. Then, for a range of values and sets of alternatives, we generate a large collection of unique textual prompts, which we then use as queries to LMs. Following the classic experimental paradigm, in each experiment we partition the generated data into a 'control' dataset and a 'treatment' dataset, and define and measure the bias of a given LM as the average difference of its choices between the two datasets.</p>
<p>Within this setup, we empirically evaluate the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Bias</th>
<th style="text-align: center;">Control</th>
<th style="text-align: center;">Treatment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Decoy</td>
<td style="text-align: center;">Below you will find three phone brands. <br> Which one would you choose? <br> Brand 1 - price is $\$ 130$, quality rating is 40 . <br> Brand 2 - price is $\$ 350$, quality rating is 60 . <br> Answer: Brand 1.</td>
<td style="text-align: center;">Below you will find three phone brands. <br> Which one would you choose? <br> Brand 1 - price is $\$ 130$, quality rating is 40 . <br> Brand 2 - price is $\$ 350$, quality rating is 60 . <br> Brand 3 - price is $\$ 438$, quality rating is 60 . <br> Answer: Brand 2.</td>
</tr>
<tr>
<td style="text-align: center;">Certainty</td>
<td style="text-align: center;">Choose between: <br> Option A - $\$ 4000$ with a $20 \%$ chance, <br> $\$ 0$ with an $80 \%$ chance. <br> Option B - $\$ 3000$ with a $25 \%$ chance, <br> $\$ 0$ with a $75 \%$ chance. <br> What is your choice? <br> Answer: Option A.</td>
<td style="text-align: center;">Choose between: <br> Option A - $\$ 4000$ with an $80 \%$ chance, <br> $\$ 0$ with a $20 \%$ chance. <br> Option B - $\$ 3000$ with certainty. <br> What is your choice? <br> Answer: Option B.</td>
</tr>
<tr>
<td style="text-align: center;">Belief</td>
<td style="text-align: center;">Determine if the following argument is <br> logically valid - <br> All zint are thade. <br> Some thade are snaff things. <br> Conclusion: Some zint are snaff things. <br> Answer: This argument is <br> invalid.</td>
<td style="text-align: center;">Determine if the following argument is <br> logically valid - <br> All diamonds are gems. <br> Some gems are transparent things. <br> Conclusion: Some diamonds are <br> transparent things. <br> Answer: This argument is valid.</td>
</tr>
</tbody>
</table>
<p>Table 1: Illustrative examples of the three evaluated Biases. Red text indicates disruptive elements fueling the bias. Blue text represents control responses unhindered by bias, while orange text denotes treatment responses influenced by bias. The decoy effect in the first row presents a scenario where two prize options are compared, the certainty effect in the second row involves selecting products with varying prices and quality measurements, and the belief bias in the third row entails evaluating the validity of logical syllogisms. In the certainty effect and decoy Effect, the model is tasked with choosing its preferred option, whereas in the belief bias, the model determines the conclusion's validity. Each bias is evaluated using a control and a treatment datasets. A shift in choice patterns is anticipated from model predictions on samples transitioning from the control dataset to the treatment.
degree of bias exhibited by several pretrained LMs, and compare them to their corresponding fine-tuned variants. Our findings indicate that applying IT or RLHF tuning either introduces cognitive-like biases into text generation, or amplifies these biases if they already exist.</p>
<p>Given that fine-tuned models are typically considered to be superior, our results point to an important limitation of tuning based on instructions or human feedback. Fine-tuned models are also often regarded as potentially less biased, such as in domains like gender or race, since they can be explicitly trained to avoid these biases or having personal preferences (OpenAI, 2023). Our results suggest that, similarly to debiasing attempts (Gonen and Goldberg, 2019), improving alignment with respect to one human objective may result in behavior that is unintended with respect to others.</p>
<h2>2 Cognitive Biases: Background and Experimental Setup</h2>
<p>Rational choice theory depicts humans as making choices in a manner that maximizes value on the basis of fixed preferences. A large body of literature is devoted to describing how actual human behavior deviates from this ideal. Cognitive biases aim to explain regular inconsistencies in choice behavior by revealing our susceptibility to 'supposedly irrelevant' factors, such as the context of the decision task, or its framing. Cognitive biases are therefore defined and measured by how judgments and decisions deviate from the rational or logical ideal in response to contextual changes.</p>
<p>Our research targets three biases that are both prevalent and well-established. The first two are the decoy effect and the certainty effect decision-making biases that relate to special cases of the more general prospect theory (Kahneman,</p>
<p>1979), with each capturing one of its distinct aspects: the perception of value, and the perception of uncertainty. The third bias is the belief bias, a logical fallacy in judgment, which was previously observed in a closed-off pretrained model (Dasgupta et al., 2022).</p>
<p>In this section, we provide for each bias some general background and a description of its classic experimental setup, which we later build on.</p>
<h3>2.1 Decoy Effect</h3>
<p>Background. When choosing from a set of alternatives, a rational agent chooses the item having the highest intrinsic value. Human choices, however, are often affected by context, and in particular, by the set of available alternatives. For example, a decision maker who chooses $A$ from the set ${A, B}$ may decide to choose $B$ from the set ${A, B, C}$ - a behavior which cannot be consistent with any underlying preference ordering (McFadden, 1974). ${ }^{2}$ The extreme case in which $C$ is clearly inferior to both $A$ and $B$, has been coined as the decoy effect, to portray $C$ as a 'decoy' item whose only role is to shift the choice from $A$ and $B$.</p>
<p>Experimental Setup. To study the decoy effect, we adopt the experimental setup of Huber et al. (1982), who proposed to measure how the choice between two items changes when a third asymmetrically dominated item-the decoy-is added to the choice set. Items in the experiment are described by their attributes (e.g., quality and price). In the control condition, subjects are asked to choose one item out of two comparable alternatives; in the treatment condition, an additional $D e$ coy Option is added to the choice set. The decoy's attributes are set so that it is asymmetrically dominated (i.e., is worse in all dimensions) by one of the original items, referred to as the target option, but not by the other item, referred to as the competitor option. Table 1 (first row) provides a concrete example: Brand 1 and Brand 2 are comparable, whereas Brand 3 (the decoy) is inferior to Brand 2 (target), but not to Brand 1 (competitor).</p>
<p>Choice behavior is said to exhibit the 'decoy effect' if subjects tend to choose Brand 1 in the control condition, but prefer Brand 2 in the treatment condition. By design, this means that choices are affected by a supposedly irrelevant factor-the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>availability of an alternative that in itself will never be chosen, suggesting that choices are biased.</p>
<h3>2.2 Certainty Effect</h3>
<p>Background. Most decision settings involve some degree of uncertainty. Given a set of alternatives describing possible outcomes and their probability, utility theory (Friedman and Savage, 1948) determines that rational agents will choose the option with the highest expected value. Human choice, however, tends to deviate from this standard, especially when the probabilities to consider are either very small or very large. The certainty effect describes people's tendency to prefer outcomes that occur with certainty to alternatives that yield higher expected value, but include risk. This effect was initially explored in the seminal work of Kahneman (1979), whose experimental setup we describe next.</p>
<p>Experimental Setup. In Kahneman (1979), human subjects were asked to choose between two 'lotteries', each describing a simple distribution over potential monetary rewards (e.g., $80 \%$ to win $\$ 100$ and $20 \%$ to win nothing). In the control condition, subjects were given two lotteries $A, B$, each having some degree of risk; in the treatment condition, alternative $B$, having lower expected value, was modified to provide its original expected value but with a probability one (i.e., the same expected value but at no risk).</p>
<p>Table 1 (second row) presents an example. In both conditions, the prize in Option A remains the same and has a higher expected reward than Option B, whose certainty varies across conditions. As in the example, Kahneman (1979) (and many follow-up studies) found that, while control subjects tend to choose rationally, treatment subjects display a strong preference towards the certain alternative despite its lower expected reward.</p>
<h3>2.3 Belief Bias</h3>
<p>Background. Syllogisms are a class of reasoning problems involving two true statements and a third conclusion statement, which is either logically deductible from the true statements, or is not (Smith, 2022). To make a rational judgment of the conclusion, it is both necessary and sufficient to apply logical reasoning to the true statementsand to them alone. Belief bias occurs when a person's evaluation of the conclusions' validity is affected also by their own knowledge, beliefs, or</p>
<table>
<thead>
<tr>
<th>Condition</th>
<th></th>
<th>Decoy</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Frying Pan</td>
<td>Phone</td>
<td>Car</td>
<td>Real-Estate</td>
<td>Certainty</td>
<td>Belief</td>
</tr>
<tr>
<td>Control</td>
<td># Samples</td>
<td>96</td>
<td>120</td>
<td>120</td>
<td>96</td>
<td>336</td>
<td>672</td>
</tr>
<tr>
<td>Treatment</td>
<td># Samples</td>
<td>1152</td>
<td>1440</td>
<td>1440</td>
<td>1152</td>
<td>504</td>
<td>672</td>
</tr>
<tr>
<td>Templates</td>
<td># Prompts</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3*</td>
<td>7</td>
</tr>
<tr>
<td>Values Range</td>
<td>US-Dollars</td>
<td>9.99-179.99</td>
<td>100-900</td>
<td>5K-35K</td>
<td>80K-500K</td>
<td>2.4K-5K</td>
<td>NA</td>
</tr>
</tbody>
</table>
<p>Table 2: Sample and template counts in each dataset, along with the range of values for decoy products and certainty effect prizes. The different text templates and values were used to evaluate the biases robustly while using reasonable values and phrasing. The (*) notation for certainty effect templates denotes the primary textual prompt without sub-templates.
values, which can sometimes lead to false reasoning. This bias was empirically demonstrated by Evans et al. (1983), whose results suggest that human judgment can be affected by the 'believability' of the conclusions, i.e., that subjects' perception of logical validity depends on the degree to which the conclusion is believable (or not).</p>
<p>Experimental Setup. In Evans et al. (1983), human subjects were given sets of two premises and a conclusion, and asked whether the conclusion logically followed from the premises (Evans et al., 1983). Half of the conclusions were phrased to be believable-aligned with general world knowledge (e.g., "cigarettes are addictive"), and the other half was constructed to be non-believable ("cigarettes are non-addictive").</p>
<p>Table 1 (third row) shows an example. Both treatment and control tasks include two premises and an invalid conclusion; while the control includes fictitious objects, the treatment includes real-world objects-which in this case are believable, and entail an erroneous answer ('valid'). Evans et al. (1983) showed that subjects tended to consider believable conclusions as valid and unbelievable conclusions as invalid, suggesting the presence of belief bias in their judgments.</p>
<h2>3 Data and Evaluation</h2>
<p>We next describe our data generation process and evaluation scheme. Sec. 3.1, outlines our semi-automatic approach for generating specific datasets, each designed to probe a certain cognitive bias and to evaluate the existence of biased 'behavior' in models. Sec. 3.2 provides further details on these generated datasets. Sec. 3.3 formally introduces our proposed bias score, intended to quantify the degree of bias exhibited by a model based on its predictions on the generated data.</p>
<h3>3.1 Data Generation</h3>
<p>To assess the level of each bias in a model, we employ a comparative approach, as shown in Table 1. We do that by comparing predictions on a generated treatment dataset and a corresponding control dataset. For the decoy and certainty effects, we use data generated according to values crafted by us, and in the belief bias we use data generated in a similar fashion by Dasgupta et al. (2022) with additional text templates we wrote.</p>
<p>To generate the treatment datasets, we follow the experimental design for each bias as outlined in Section 2 and use new values that align with the cognitive experiments methods.</p>
<p>The control versions of the datasets are carefully crafted to closely resemble the treatment samples while excluding the specific attribute that triggers the bias, as identified by cognitive experiments.</p>
<p>In the decoy and certainty effects, for each sample, there exists a designated Target option. This option is expected to be chosen more frequently by a human (or a biased model) when presented with samples from the treatment dataset compared to samples from the control dataset. In the belief biases, we treat the correct answer as the Target option, for ease of notation. We later use the Target option to compute the bias scores, as detailed in Section 3.3.</p>
<h3>3.2 Data Overview</h3>
<p>Table 2 provides quantitative metadata for the datasets. We elaborate below on the text templates and values chosen for each bias dataset according to cognitive theory as outlined in Section 2.</p>
<p>We used 3, 4, and 7 prompt templates for the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mistral</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">IT-LM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">IT-LM</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">IT-LM</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bias</td>
<td style="text-align: center;">DaVinci</td>
<td style="text-align: center;">DaVinci-002</td>
<td style="text-align: center;">DaVinci-003</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Flan-T5</td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">Mistral-I</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Decoy Expensive</td>
<td style="text-align: center;">$-0.15^{*}$</td>
<td style="text-align: center;">$-0.13^{*}$</td>
<td style="text-align: center;">$-0.02$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">$-0.18^{*}$</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.24**</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Decoy Cheaper</td>
<td style="text-align: center;">$-0.17^{*}$</td>
<td style="text-align: center;">$-0.08^{*}$</td>
<td style="text-align: center;">0.08**</td>
<td style="text-align: center;">$-0.15^{*}$</td>
<td style="text-align: center;">0.20*</td>
<td style="text-align: center;">$-0.05^{*}$</td>
<td style="text-align: center;">$-0.03^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Certainty</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">$0.24^{*}$</td>
<td style="text-align: center;">0.67*</td>
<td style="text-align: center;">0.09*</td>
<td style="text-align: center;">0.17*</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.29*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Belief Valid</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.19*</td>
<td style="text-align: center;">0.21*</td>
<td style="text-align: center;">$-0.03$</td>
<td style="text-align: center;">0.50*</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.26*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Belief Invalid</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.55*</td>
<td style="text-align: center;">0.65*</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.39*</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.31*</td>
</tr>
</tbody>
</table>
<p>Table 3: The difference between the choices of models in the target option under the treatment condition versus the control condition. A higher score means the model exhibits a higher level of bias. In bold are the highest values in each model family. (<em>) Marks results that are statistically significant with p-values $&lt;.05$, and (</em>*) marks results that are averaged across multiple products where some are significant and others are not. Mistral-I refers to Mistral-Instruct.
certainty effect, decoy effect, and belief bias, respectively. The certainty effect featured extra subtemplates with variations in option presentations like probabilities or percentages. All possible permutations of option orders were used for decoy and certainty effects, as well as for both premises in belief bias.</p>
<p>Regarding the decoy effect, we utilized realistic values from US-based store websites to construct our datasets. Quality ratings ranged from 60 to 90 with 10-20 intervals between options. Decoy options, in comparison to the target, exhibit a $25 \%$ or $50 \%$ price change, a 10-20 point quality rating shift, or a combination of both. Modern alternatives to the original products were anecdotally chosen, emphasizing a one-time, deliberate selection process without trial and error.</p>
<p>In line with cognitive bias theory, we chose certainty effect prizes and probabilities to closely mirror the cognitive research data, ensuring accurate expected utility differences between the options.</p>
<p>Belief bias samples involve manually composing both believable and unbelievable arguments, derived from previous work. The samples are evenly split, with half being believable and the remaining half being unbelievable. The samples' arguments are built upon simple, well-known objects, such as 'All guns are weapons' and 'All lizards are reptiles'. Further details can be found at Dasgupta et al. (2022)</p>
<h3>3.3 Computing The Bias Scores</h3>
<p>We assess biases in each model by analyzing their prediction patterns across treatment and control datasets, quantifying them through bias scores. The bias score captures the difference in the model's inclination towards the Target option between treatment and control scenarios.</p>
<p>For example, if the model chose the 'Target' option in $90 \%$ of treatment samples and $70 \%$ of control samples, the bias score would be 0.20 .</p>
<p>Bias Score Definition. The bias score is formally defined in Equation 1, where Treatment and Control represent the sets of treatment and control datasets, respectively, and $N_{T}$ and $N_{C}$ indicate their respective set sizes. $A n s_{i}$ denotes the model's choice in sample $i$, while $T$ represents the target option.</p>
<p>$$
\sum_{i \in \text { Treatment }} \frac{\mathbb{1}\left[A n s_{i}=T\right]}{N_{T}}-\sum_{i \in \text { Control }} \frac{\mathbb{1}\left[A n s_{i}=T\right]}{N_{C}}
$$</p>
<p>According to the original experimental setting of the decoy effect, the target option in each sample can be associated with either a lower or higher price, leading to the computation of separate bias scores: Decoy Cheaper and Decoy Expensive.</p>
<p>To compute bias scores for the belief bias, we compare the model's predictions between consistent and inconsistent conditions for valid and invalid arguments. This analysis results in two distinct bias scores that were recognized in the original experiments:</p>
<p>Belief Valid: The difference between the model's predictions of consistent valid arguments (valid and believable conclusions in real-life objects condition) and neutral valid conclusions (all valid arguments in non-real object conditions).</p>
<p>Belief Invalid: The difference between the model's predictions of consistent invalid arguments (invalid and unbelievable arguments in reallife objects condition) and neutral invalid argu-</p>
<p>ments (all invalid arguments in non-real object scenarios).</p>
<p>The Meaning of Bias Score Values. Higher bias score values indicate a greater degree of bias in the model. The bias scores range from -1 to 1, reflecting the extent of the bias and its direction relative to human biases according to cognitive theory. While the original experiments on human evaluation did not calculate bias scores, the intended alignment of these bias scores is with the strength of bias as per the cognitive theory on human biases. A score of 1 represents maximum bias aligned with human biases, 0 indicates no bias, and -1 denotes maximum bias in the opposite direction to human biases.</p>
<p>The significance of each bias score is measured using the student's t-test (Student, 1908).</p>
<h2>4 Experiments</h2>
<p>Models We conduct our experiments on two LM sets. The first set is pretrained models GPT3 'DaVinci' (Brown et al., 2020), and the publicly available Mistral-7B (Jiang et al., 2023) and T5 (Raffel et al., 2020). ${ }^{3}$ The second set consists of improved versions of the preatrained models fine-tuned using IT and human feedback. For GPT3, we experiment with GPT3.5 models—-‘text-DaVinci-002’ and 'text-DaVinci003' ('Davinci-002' and 'Davinci-003' for short) (Ouyang et al., 2022)—as IT and IT+RLHF models respectively. For the Mistral 7B, we use Mistral 7B-Instruct with the recommended chat template ${ }^{4}$. For T5 we use the Flan-T5 models (Chung et al., 2022) as the IT version. Our primary findings are based on the XXL variant of the T5 models (11B parameters), and we also experiment with the XL variant (3B parameters) to investigate the influence of model size.</p>
<p>Finally, we also experiment with one of the latest commercially available models, GPT4 (OpenAI, 2023), which is considered a state-of-the-art generative model. ${ }^{5}$ However, we do not have access to its pretrained version as it was not publicly</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>released. We, therefore, use GPT4 only as a reference for a newer model.</p>
<p>Determining the Model's Answer. Given a prompt asking for a choice, the instruction-tuned models using greedy decoding usually generate text describing their choice, simply as "Option 1" or "Brand 2". ${ }^{6}$</p>
<p>To assess the pretrained performance for each task, we use the common practice (Brown et al., 2020) of evaluating the likelihood of various candidate answers from a predefined set of possible answers. This helps prevent models from persistently asking questions instead of providing direct answers, as observed in our initial experiments. This evaluation might be affected by a preference of the model to an answer given the context (e.g., given "Answer:" the model might give a higher baseline probability to "Option 2"). We apply the DC-PMI correction (Holtzman et al., 2021) that mitigates this issue by normalizing each answer likelihood within the context of the prompt, relative to a baseline prompt ("Answer:", in our case). ${ }^{7}$</p>
<p>Using Zero-shot The samples used for the decoy and certainty effects are choice-dependent questions with no "correct" answer (recall the examples in Table 1). It is therefore not obvious how to construct few-shot examples, which presumably should have correct labels in the prompt. Given that we focus on decision inclinations, the zeroshot setup aligns naturally with our investigation of all biases. Most experiments, unless specified, are in the zero-shot format and involve a single question followed by "Answer:" without extra examples, as shown in Table 1.</p>
<p>Using Few-shot Despite the above-mentioned problem, we experiment with an approach that constructs a few-shot setting using samples outside of our data. We build upon a recent work that suggested that giving few-shot samples without the correct labels could improve model performance by introducing the model with the overall format of the samples (Min et al., 2022). We detail further and report results in Section 6.1.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Bias</th>
<th>DaVinci-003</th>
<th>GPT4</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Decoy Expensive</td>
<td>0.00</td>
<td>0.38*</td>
</tr>
<tr>
<td></td>
<td>Decoy Cheaper</td>
<td>0.03</td>
<td>0.05</td>
</tr>
<tr>
<td></td>
<td>Certainty</td>
<td>0.43*</td>
<td>0.20*</td>
</tr>
<tr>
<td></td>
<td>Belief Valid</td>
<td>0.20*</td>
<td>0.15*</td>
</tr>
<tr>
<td></td>
<td>Belief Invalid</td>
<td>0.47*</td>
<td>0.41*</td>
</tr>
</tbody>
</table>
<p>Table 4: comparison of the results between GPT4 and the most recent GPT3.5 release DaVinci-003 in 1-shot format. Scores marked with * are statistically significant with p-values $&lt;.05$</p>
<h2>5 Results</h2>
<p>Table 3 summarizes the bias scores of pre-trained models and their instruction-tuned and RLHFtuned counterparts. We discuss the main takeaways in this section and provide several finegrained analyses in the next one.</p>
<p>Models fine-tuned using IT and RLHF show a higher bias than their pretrained counterparts. Our findings reveal that the models fine-tuned on instructions and RLHF mostly exhibit significantly higher levels of bias compared to their pretrained counterparts, as demonstrated in Table 3. While the pretrained LMs demonstrate minimal to no bias, the fine-tuned models display pronounced biases across most categories. This is evident in the certainty effect row, where the DaVinci, T5 and Mistral pretrained models exhibit bias scores of $0.00,0.09$, and 0.03 , respectively. In contrast, the fine-tuned models display higher bias scores of $0.24,0.67,0.17$, and 0.29 . This unexpected result suggests that the fine-tuning process, intended to enhance model performance, inadvertently introduces biases into the decision-making process.</p>
<p>We measured the significance of the differences between models using the difference-indifferences method <em>Dimick and Ryan (2014)</em>. All differences were significant except for DaVinci and DaVinci-002 in the belief valid and in decoy expensive, T5 and Flan-T5 in the certainty effect, and Mistral and Mistral-Instruct in the decoy cheaper.</p>
<p>LMs exhibit biases that align with human biases theory. Intriguingly, our investigation reveals a convergence between the decision-making biases observed in the models and the wellestablished theory on irrational biases inherent in human decision-making processes. Recall from Section 3.3 that positive values indicate alignment between bias scores and human biases. Indeed, tuning using instructions or human preferences generally makes bias scores increasingly high. The negative bias score exhibited by DaVinci in the decoy biases can be explained by choice criteria which, unlike humans, are not value-depended. In this exceptional case, the model chose the last option offered almost all the time, regardless of the options' content, making its choice more focused on positional preferences.</p>
<p>This finding emphasizes the role of fine-tuning on bias amplification on previously undiscovered biases. In addition, the similarity between the theory on human biases and model biases highlights the potential connection of inherent biases ingrained in human decision-making processes to tuning methods that induce the models to replicate human behaviors.</p>
<p>IT Amplifies Biases. The discernible impact of fine-tuning with IT becomes evident upon comparing the T5 versus the Flan-T5 models and the Mistral versus the Mistral-Instruct models. While DaVinci and DaVinci-002 versions may differ by more than IT (exact details are not public), the transparent elucidation of the Flan-T5 fine-tuning process and the sole instruction tuning done to the Mistral-Instruct model allows us to confidently assert that the sole utilization of IT can indeed engender the emergence of biases. This finding highlights the influential role of fine-tuning methods in amplifying biases within models, shedding light on the intricate relationship between IT and the manifestation of biases.</p>
<p>RLHF Amplifies Biases. Our findings indicate that the application of reinforcement learning finetuning from human feedback has the potential to amplify biases within language models further. This is evident when comparing the DaVinci-002 and DaVinci-003 models, with the latter incorporating reinforcement learning techniques. ${ }^{8}$ Notably, while IT may contribute to bias amplification, our results suggest that reinforcement learning, as an independent factor, can also play a significant role in the emergence of these biases. This observation highlights the complex interplay between reinforcement learning methodologies and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{8}$ According to OpenAI at https: //platform.openai.com/docs/ model-index-for-researchers.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The impact of model size on bias scores. The larger Flan-T5-XXL exhibits higher bias scores in decoy cheaper, certainty, and belief valid biases while demonstrating lower bias scores in decoy expensive and belief invalid biases compared to the smaller Flan-T5-XL. The decoy expensive bias discrepancy may stem from Flan-T5-XXL's preference for higher-priced products, while the belief invalid bias reduction can be attributed to the model's enhanced accuracy with neutral arguments.</p>
<h3>the manifestation of biases.</h3>
<p><strong>GPT4 is also biased.</strong> The results comparing GPT4 to its predecessor in the GPT series are presented in Table 4. Across our experiments, GPT4 demonstrates the highest bias score in the decoy expensive and decoy cheaper biases. Although the bias scores are lower in the certainty, belief valid, and belief invalid biases, GPT4 still exhibits significant bias levels.</p>
<p>The decreased bias scores observed in belief biases might be attributed to the model training, at least partly aimed at enhancing logical reasoning. Part of the GPT4 training data was designed to improve reasoning skills using data from MATH (Hendrycks et al., 2021) and GSM-8K (Cobbe et al., 2021). However, since GPT4 might be different in many other ways from DaVinci-003, we cannot attribute the decreased bias scores to this specific change. Beyond that, even with possibly improved reasoning, the model had less success mitigating bias in the decoy effect, which exhibited the most pronounced bias. Furthermore, we encountered instances in the zero-shot setting where GPT4 refrained from providing explicit choices, so we report one-shot results in the few-shot format as explained later in Section 6.1 (the zero-shot results when GPT4 did answer are similar to the one-shot results).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Acceptance rates of the Flan-T5 models on believable (green) and unbelievable (red) arguments in the treatment condition and on neutral arguments in the control condition (blue) divided into valid and invalid arguments. The Belief Invalid bias score for the larger Flan-T5-XXL model (lower) seems lower compared to the smaller Flan-T5-XL (upper) because the model is less successful on the neutral arguments (blue).</p>
<p>While GPT4 shows some mitigation of biases, the prominence of the decoy effect has increased, and all biases remain pronounced. These findings suggest that biases remain relevant in models designed to address bias mitigation, such as GPT4 which was trained using RLHF to avoid social biases such as biases about sexuality and norms around marriage (OpenAI, 2023).</p>
<h3>The effect of model size on bias emergence.</h3>
<p>Figure 1 shows the discrepancy in bias scores between the XL and XXL versions of Flan-T5. Consistent with prior research on social biases (Tal et al., 2022), the larger XXL model exhibits higher bias scores for three bias types (decoy cheaper, certainty, and belief valid). Surprisingly, the decoy expensive and belief invalid bias scores are lower for the XXL model, suggesting a presumable reduction in bias compared to the XL model.</p>
<p>The reduction in belief invalid bias score could be attributed to the XXL model's lower accuracy.</p>
<p>in identifying invalid conclusions within the nonReal objects condition, as depicted in Figure 2. Specifically, in the invalid-believable condition, the XXL model demonstrates a higher acceptance rate, indicating a greater presence of bias. In contrast, in the invalid non-real objects condition, the XXL model displays a significantly elevated acceptance rate, leading to reduced overall accuracy and consequently lowering the bias score as per our defined calculation method (Section 3.3).</p>
<p>As to the reduction of bias score in the decoy expensive, that may result from a specific behavior of the XXL model, as discussed in Section 6.2.</p>
<h2>6 Analysis</h2>
<p>We delve into the effects of few-shot in Section 6.1 and explore different attributes of the decoy effect and belief bias in Sections 6.2 and 6.3.</p>
<h3>6.1 Using Few-shot</h3>
<p>Our main experiments used a zero-shot setting to avoid giving the model examples that could bias it in any direction - giving the model an example with an answer that is target could affect the tenacity of the model in choosing the target and vice versa. Therefore, to help the model understand the sample format without biasing it in either direction, we experiment with few-shot prompting without the original samples.</p>
<p>Instead of using the samples from our datasets, we use manually curated choices between arbitrary options for the decoy and certainty effects (e.g., "Which would you prefer, a white or black shirt?") and mathematical reasoning examples for the belief bias (e.g., "The price is $\$ 10$ per soda. The customer inserted 20\$. Conclusion: The customer can buy only 1 soda. Answer: Invalid."). We call this approach format few-shot as the intention is to show the model the sample format using few-shot examples. We curated a 5 -example pool for each bias, randomly selecting each sample from them.</p>
<p>In the case of the belief bias, there are correct labels. Therefore, we can also prompt the model with few-shot samples and avoid biasing the model by utilizing samples comprised of neutral non-real objects derived from a distinct set of fabricated words that were deliberately excluded from the test data. We call this Task few-shot as few-shot samples are from the same task as the test sample. This approach enables us to assess the im-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The impact of format few-shots on bias scores using Davinci-003 (top) and Mistral-Instruct (bottom). The utilization of few-shot examples in most models results in slightly lower bias scores, while in Mistral-Instruct Belief biases are significantly lower and certainty bias increases. To reduce computation costs, bias scores for Decoy Expensive and Decoy Cheaper biases are calculated solely on a specific product category (real-estate properties).
pact of using few-shot examples solely for formatting purposes compared to employing few-shot examples from the same task on the bias scores.</p>
<p>Results. Results with format few-shot examples can be seen in Figure 3. Regarding DaVinci-003, in the decoy and certainty effects, there is no distinct trend when using the format few-shot examples, except for a small decrease in bias score when changing from zero-shot to one-shot setting. Overall, increasing the number of few-shot examples might help the model understand the format but does not significantly decrease the bias.</p>
<p>In the case of the belief bias, incorporating fewshot examples leads to a noticeable reduction in the bias score, although a significant level of bias persists. This effect is more significant when uti-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The impact of format few-shots in comparison to task few-shots on bias scores, utilizing the DaVinci-003 model. When the model is prompted with examples from the same task, the decrease in bias scores is relatively lower compared to employing examples with merely the same format as the task.</p>
<p>Lizing task examples, as can be seen in Figure 4. This observation can perhaps be attributed to the presence of a logical reasoning process required by the belief bias examples, whereby the model's utilization of few-shot examples aids in facilitating problem-solving and helps to overcome the inherent bias associated with belief.</p>
<p>While these results are similar to the other instruction models we tested, Mistral-Instruct demonstrated a unique behavior. Its belief and decoy bias scores consistently decreased, while the certainty bias score increased with additional format examples. Notably, the pretrained version of Mistral also observed a rise in the certainty effect bias score in the few-shot setting (increasing from 0.03 to 0.31). This exception entails we have much to learn about the effect of pertaining data and training techniques on the way models utilize few-shot in general and regarding biases specifically.</p>
<p>This analysis focuses on the impact of few-shot examples solely on the instruction-tuned models, which exhibited the highest bias scores. However, it is plausible to speculate that the pretrained models, which demonstrated the lowest bias scores, could potentially benefit even more significantly from learning the format through few-shot examples, considering their stronger dependence on understanding the format. This could lead to the possible observation of higher bias scores for the pretrained models when giving few-shot samples. To address this, we conducted few-shot experiments.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The bias scores of the decoy cheaper effect across various products for the Flan-T5-XXL and DaVinci-003 models. The bias scores exhibit consistency of bias existence across all products, indicating that the observed behavior remains more or less uniform within models across different product categories and price ranges, akin to human cognitive theory.</p>
<p>For the pretrained models, which revealed that the bias scores remain similarly low for these models with the exception of Mistral, as described before.</p>
<h3>6.2 Decoy Effect Analysis</h3>
<p>We investigate multiple attributes of the decoy effect, encompassing diverse product outcomes and price ranges, to assess their impact on the bias score and partly compare them with human behavior. Moreover, we explore a particular behavior identified in the decoy expensive effect that has a notable influence on the bias scores.</p>
<p><strong>Products Variance.</strong> There was a moderate variance in bias scores in the decoy expensive results across different products, as shown in Figure 5.</p>
<p>As the bias scores are computed as the difference between the treatment and control conditions, the score varies with the variance in the base preference of the model to target option in the control condition for each product. Such differences between products were also observed to some extent in the original experiments on human subjects.</p>
<p>Together with the effect of price on the bias score (which is also analyzed in this section) and quality differences between products, the base preference of the model can cause a variance between different products.</p>
<p><strong>Price Range Effect.</strong> We investigate the relationship between the target price and the price gap, defined as the difference between the prices of the competitor option and the target option. In our</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The Effect of price range on the bias score of the decoy cheaper bias with real-estate products in the DaVinci-003 model. The x-axis represents the target price, where an increase in the target price leads to a wider gap between the target and competitor prices. The bias score demonstrates a positive correlation with the increasing price gap.</p>
<p>Data, the higher the target value, the higher the gap from the competitor option. By selecting values with varying price gaps, we aimed to examine the impact of this factor on bias scores.</p>
<p>As Figure 6 shows, as the price range increases, the bias scores also exhibit higher values. Although human experiments did not analyze this aspect, this result aligns with the expected behavior of this bias and is intuitively reasonable.</p>
<h3>The Decoy Expensive Effect</h3>
<p>A notable observation in the decoy expensive experiments is the significantly low bias score of -0.18 exhibited by the Flan-T5 XXL model. We found that this score stems from the model consistently favoring the more expensive target option in the control condition with nearly 100% preference.</p>
<p>Considering the model's unwavering preference for the more target option in the absence of a decoy, the addition of a decoy option cannot possibly shift its preference from the competitor option to the target option. While this leaves no room for a bias score above zero, this preference for the more expensive option leads to negative results as the model picks the more expensive option even when adding a more expensive decoy option, leading to a shift from the target to the decoy option.</p>
<p>It is intriguing to observe such behaviors in these models that do not align with familiar cognitive biases but contradict basic human logic. These findings necessitate further investigation that goes beyond cognitive-like biases before utilizing these</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The relationship between bias scores and model accuracy on the belief bias task's logical reasoning aspect for the DaVinci (blue), DaVinci-002 (green), and DaVinci-003 (brown) models. Notably, an increase in model accuracy is accompanied by higher bias scores, indicating that improved accuracy does not necessarily mitigate biases in these models.</p>
<p>models to aid in human decision-making.</p>
<h3>6.3 More Accurate and More Biased</h3>
<p>On the belief bias task, we can quantitatively measure the model accuracy, allowing us to examine the trade-off between accuracy and bias scores.</p>
<p>Figure 7 shows the change in bias scores relative to the accuracy of the GPT models on the logical reasoning aspect of the belief bias task. Interestingly, as the models demonstrate improved accuracy, they also exhibit higher bias scores. This finding suggests that, despite advancements in accuracy, biases persist within these models.</p>
<p>Finally, our evaluation includes GPT4, a model specifically trained on logical reasoning. GPT4 achieves a higher accuracy (84%) compared to all GPT models, while simultaneously exhibiting a lower bias score than DaVinci-002 and DaVinci-003 (0.07 and 0.49 for the belief valid and belief invalid correspondingly). This observation highlights the potential benefits of incorporating targeted training approaches to enhance both accuracy and mitigate biases in the process.</p>
<h3>7 Discussion and Conclusions</h3>
<p>Our study examines the influence of IT and RLHF on LMs' decision-making through cognitive bias analysis. We reveal the presence of these biases across models, notably in models amplified with IT and RLHF. These insights enhance our understanding of biases in fine-tuned models, widely considered superior to the pretrained models.</p>
<p>In Section 7.1, we explore the potential consequences of identifying these biases and the challenges in addressing them. Section 7.2 delves into research paths investigating the source of these biases in the training of language models (LMs).</p>
<h3>7.1 Real-World Impact</h3>
<p>The identified LM biases could impact real-world applications in decision-making and reasoning tasks. For example, the presence of decoy and certainty effects may raise challenges for LMs as decision assistants. Another impact could be reduced accuracy in some reasoning tasks in which the claim's plausibility plays a role. This concern is demonstrated by the fact that in the belief bias, the treated samples exhibit a notable decrease in accuracy as expressed by the bias scores, ranging from $19 \%$ to $61 \%$, compared to controlled samples. Acknowledging and addressing these biases is crucial for enhancing the reliability and performance of LMs in real-world applications.</p>
<p>One possible way to estimate the impact of these biases is to test models on biased datasets, before and after tuning. Nevertheless, using our proposed datasets for this process has its challenges, due to the complexities of controlling the amount of bias in models. For example, finetuning with belief bias control data might not reduce model bias, while using belief bias treatment data could improve logical reasoning but harm common sense. These complexities increase when considering effects like decoy and certainty, which lack defined truth labels. Although fine-tuning with our data is an appealing idea, it requires further investigation into how biases are learned in LLMs, which is beyond the scope of this paper.</p>
<h3>7.2 Origin of Bias</h3>
<p>An additional question that arises concerns the origin of these biases. Further research is needed to determine if biases come from pretraining, intensify during fine-tuning, or arise from a mix of both. While Lin et al. (2023) claim that alignment methods only extract existing behavior models learned in pertaining, Shwartz and Choi (2020) demonstrated that pretrained LMs tend to prioritize infrequent actions over more common ones, indicating the presence of reporting bias. The biases outlined in our work may be associated with the prevalence of analogous questions and answers in the instruction and human feedback datasets used for model training. Studying bias-related examples in pretraining data and their magnification during fine-tuning can offer insights. Evaluating the influence of different fine-tuning data and strategies on bias could illuminate fine-tuning's role in bias emergence. Assessing how these biases interact with other known biases (such as reporting bias (Shwartz and Choi, 2020), and financial bias (Zhou et al., 2024)) can provide insights into how they are acquired and potential interconnections. Grasping these dynamics will guide strategies to improve model fairness and reliability.</p>
<h2>8 Limitations</h2>
<p>In examining the impact of IT and RLHF on cognitive biases in LMs, our study highlights a notable challenge in disentangling the effects of different training datasets. Flan-T5's IT data involves NLP tasks, Mistral-Instruct trained on unknown publicly available instructions datasets, while OpenAI's IT data uses assistant-like inputoutput pairs as far as we know. The dissimilarity in training data makes it difficult to pinpoint the exact factors causing biases in our models, underscoring the need for further investigation.</p>
<p>Beyond that, the unavailability of information on OpenAI models' training limits our ability to draw clear conclusions. Without details on their training procedures, we cannot determine whether RLHF training alone causes bias amplification or if GPT4's partial mitigation results from specific procedures, architecture differences, or other factors. The uncertain future availability of OpenAI models puts the complete reproduction of the results at risk for future research. Our study emphasizes the importance of transparency in model training for a better understanding of the relationship between IT and RLHF to biases in LMs.</p>
<p>Besides these model-specific limitations, there are limitations inherent in this type of research. One possible limitation is data contamination. We address well-known biases that might leak into the training data despite our efforts to introduce new text and value variations.</p>
<p>While it's common to evaluate pretrained LMs using answer probabilities (Brown et al., 2020; Holtzman et al., 2021), this evaluation method introduces a slight difference when compared to models trained on IT, which can be assessed based on their directly generated answers. Although unavoidable, this factor might influence results. We analyze the biases only in English-based models.</p>
<h2>Acknowledgements</h2>
<p>This research was supported by the Israel Science Foundation (grants 448/20 and 278/22), an AI alignment grant from Open Philanthropy, and an Azrieli Foundation Early Career Faculty Fellowship.</p>
<h2>References</h2>
<p>Chiara Acciarini, Federica Brunetta, and Paolo Boccardelli. 2021. Cognitive biases and decision-making strategies in times of change: a systematic literature review. Management Decision, 59(3):638-652.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Vincent Berthet. 2022. The impact of cognitive biases on professionals' decision-making: A review of four occupational areas. Frontiers in Psychology, 12:802439.</p>
<p>Marcel Binz and Eric Schulz. 2022. Using cognitive psychology to understand gpt-3. ArXiv, abs/2206.14576.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha</p>
<p>Brahma, et al. 2022. Scaling instructionfinetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051.</p>
<p>Justin B Dimick and Andrew M Ryan. 2014. Methods for evaluating changes in health care policy: the difference-in-differences approach. Jama, 312(22):2401-2402.</p>
<p>JSBT Evans, Julie L Barston, and Paul Pollard. 1983. On the conflict between logic and belief in syllogistic reasoning. Memory \&amp; cognition, 11(3):295-306.</p>
<p>Milton Friedman and Leonard J Savage. 1948. The utility analysis of choices involving risk. Journal of political Economy, 56(4):279-304.</p>
<p>Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862.</p>
<p>Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. 2022. Machine intuition: Uncovering human-like intuitive decision-making in gpt3.5. arXiv preprint arXiv:2212.05206.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Joel Huber, John W Payne, and Christopher Puto. 1982. Adding asymmetrically dominated alternatives: Violations of regularity and the similarity hypothesis. Journal of consumer research, 9(1):90-98.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Daniel Kahneman. 1979. Prospect theory: An analysis of decisions under risk. Econometrica, 47:278.</p>
<p>Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023. The unlocking spell on base llms: Rethinking alignment via in-context learning. arXiv preprint arXiv:2312.01552.</p>
<p>D McFadden. 1974. Conditional logit analysis of qualitative choice behavior. Frontiers in Econometrics.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Vered Shwartz and Yejin Choi. 2020. Do neural language models overcome reporting bias? In Proceedings of the 28th International Conference on Computational Linguistics, pages 6863-6870, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Robin Smith. 2022. Aristotle's Logic. In Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy, Winter 2022 edition. Metaphysics Research Lab, Stanford University.</p>
<p>Student. 1908. The probable error of a mean. Biometrika, 6(1):1-25.</p>
<p>Yarden Tal, Inbal Magar, and Roy Schwartz. 2022. Fewer errors, but more stereotypes? the effect of model size on gender bias. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 112120, Seattle, Washington. Association for Computational Linguistics.</p>
<p>Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419.</p>
<p>Yuhang Zhou, Yuchen Ni, Xiang Liu, Jian Zhang, Sen Liu, Guangnan Ye, and Hongfeng Chai. 2024. Are large language models rational investors?</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We use version T51.1:
github.com/google-research/
text-to-text-transfer-transformer/blob/
main/released_checkpoints.md
${ }^{4}$ We used "You are a helpful assistant. Answer shortly with only your choice with no explanation." as the opening instruction.
${ }^{5}$ We used the 'gpt-4-0314' version with the content "You are a helpful assistant."&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ In the certainty effect $&lt;5 \%$ of the predictions made by Flan-T5-XXL were not clear and we excluded these examples.
${ }^{7}$ Small-scale experiments with DC-PMI correction for the instruction-tuned models led to similar results to evaluation without correction, so we only report the latter.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9978 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9978</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9978</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-170.html">extraction-schema-170</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <p><strong>Paper ID:</strong> paper-254684331</p>
                <p><strong>Paper Title:</strong> Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review</p>
                <p><strong>Paper Abstract:</strong> Background Alzheimer’s disease has become one of the most common neurodegenerative diseases worldwide, which seriously affects the health of the elderly. Early detection and intervention are the most effective prevention methods currently. Compared with traditional detection methods such as traditional scale tests, electroencephalograms, and magnetic resonance imaging, speech analysis is more convenient for automatic large-scale Alzheimer’s disease detection and has attracted extensive attention from researchers. In particular, deep learning-based speech analysis and language processing techniques for Alzheimer’s disease detection have been studied and achieved impressive results. Methods To integrate the latest research progresses, hundreds of relevant papers from ACM, DBLP, IEEE, PubMed, Scopus, Web of Science electronic databases, and other sources were retrieved. We used these keywords for paper search: (Alzheimer OR dementia OR cognitive impairment) AND (speech OR voice OR audio) AND (deep learning OR neural network). Conclusions Fifty-two papers were finally retained after screening. We reviewed and presented the speech databases, deep learning methods, and model performances of these studies. In the end, we pointed out the mainstreams and limitations in the current studies and provided a direction for future research.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9978.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9978.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>causes_not_discussed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit etiological causes not discussed in this review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review does not propose or evaluate specific molecular/genetic causes (e.g., amyloid-beta, tau, APOE) of Alzheimer's disease; it focuses on detection methods based on speech and deep learning and remarks that other causes of dementia exist without detailing them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not specified / not discussed</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>The paper does not present or evaluate specific etiological hypotheses (such as amyloid-beta accumulation, tau pathology, APOE4 genotype, vascular contributions, chronic inflammation, head trauma, etc.). It only notes 'other causes of dementia' in passing and focuses on language changes as early manifestations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>No primary evidence for molecular/genetic causes is provided in this review; the paper cites diagnostic guideline/review literature but does not summarize causal evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Not applicable in this review (no counter-evidence presented because causal mechanisms are not the topic).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>review (focus on detection methods rather than etiology)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>The review explicitly does not address molecular/genetic causes; it cautions that its focus and search keywords omitted some pre-trained-model literature and that only English-language papers were included, limiting etiologic coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9978.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMSE_MoCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Widely used brief cognitive screening instruments for global cognition and dementia screening referenced as comparators to speech-based detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (cognitive test)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — cognitive tests measuring global cognitive function and domain-specific abilities used for screening and staging dementia.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Cognitive screening test / neuropsychological test</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Structured brief tests administered by clinicians to assess cognitive status (MMSE, MoCA); used as clinical gold-standard screening tools in many studies and as labels in databases.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>The review states that traditional cognitive impairment screening scales such as MMSE or MoCA can usually achieve a screening accuracy of more than 93% (cited in the review). Specific sensitivity/specificity values are not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Referenced clinical screening studies / general diagnostic literature</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Clinical older adult populations in referenced literature (not detailed in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>MMSE/MoCA are more accurate than most speech-based automated methods reported here (>93% vs ~85%), which sets a high standard; these tests require clinical administration and may not be practical for large-scale automated screening.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9978.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EEG_MRI_PET_CSF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Electroencephalography (EEG), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET) imaging, and Cerebrospinal Fluid (CSF) assays</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional neurophysiological, structural/functional imaging, molecular imaging, and fluid-biomarker methods used in AD diagnosis and research; mentioned as accurate but impractical for large-scale screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (biomarker/imaging methods)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — these are detection/diagnostic modalities measuring brain structure (MRI), functional/metabolic or molecular pathology (PET), electrophysiology (EEG), and biochemical markers in CSF (amyloid/tau) used in diagnostic workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>The review cites these modalities as established diagnostic tools (referencing diagnostic guidelines) but does not re-evaluate primary evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>No direct counter-evidence presented, but the review notes limitations (see below).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Biomarker / neuroimaging / electrophysiology / fluid assay</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>MRI (structural brain changes), PET (amyloid/tau imaging), EEG (electrophysiological changes), CSF assays (amyloid-beta, total/phospho-tau levels).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>No specific sensitivity/specificity numbers are provided in this review; these methods are noted as accurate for diagnosis but described as inappropriate for large-scale early screening due to cost, resource intensity, or invasiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Referenced diagnostic and guideline literature (NIA‑AA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Clinical research cohorts (not specified in detail in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>High cost, limited accessibility, and invasiveness (CSF lumbar puncture) make PET/CSF assays unsuitable for large-scale population-level early screening; the review motivates speech analysis as cheaper and scalable alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9978.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>speech_DL_general</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speech-based Alzheimer's disease detection using deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated detection of AD from spontaneous speech or task-based speech using deep learning models extracting acoustic and/or linguistic features and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (detection approach)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — this is a detection approach that leverages language and acoustic changes associated with cognitive decline (pauses, slow speech, word-finding difficulties, disfluencies) as proxies for underlying disease.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Numerous case-control studies summarized in the review report classification accuracies commonly around ~85% using deep learning; pre-trained model fine-tuning often improves results; selected highest reported performances include 93.3% accuracy (Bertini et al., Pitt dataset), and multiple ADReSS challenge teams reached ~89–91% accuracy on the ADReSS test set using pre-trained language models and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Speech-based methods generally underperform established cognitive tests (MMSE/MoCA >93%) and have variable performance across datasets; performance for MCI detection is notably lower and more inconsistent (AUCs around 66–69% reported for some MCI tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Behavioral / speech signal analysis (machine learning / deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Measurement of acoustic features (MFCCs, formants, pauses, speech rate), duration/disfluency features, linguistics features (lexical/syntactic/semantic measures), and learned embeddings from pre-trained ASR/language/speech models (BERT, ERNIE, Longformer, wav2vec). Deep models (FNN, CNN, LSTM, attention/transformer) classify AD vs non-AD or predict MMSE.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Typical accuracies around 80–85% across many studies; pre-trained models fine-tuned on AD data often reach ~89–91% on ADReSS test set (examples: Saltz et al. ~90%, Yuan et al. ~89.6%, Zhu et al. ~89.58%); highest single-study reported accuracy on Pitt dataset 93.30% (Bertini et al.). MCI detection AUCs/accuracies are lower (examples: Lindsay et al. AUC 66–69%; Rodrigues Makiuchi three-class accuracy 60.6%). Sensitivity/specificity are generally not consistently reported across studies in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Aggregated results from multiple case-control studies, cross-validation and held-out test evaluations, and the ADReSS challenge benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Heterogeneous: many small datasets in single languages (Pitt corpus, ADReSS, various national corpora), sample sizes range from tens to a few hundred per class; ADReSS: 156 subjects (108 train / 48 test); Pitt: hundreds of recordings; Framingham example larger (dementia 223 / MCI 309 / HC 291).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Limitations include small and language-limited datasets, lack of longitudinal/cohort data to test intra-individual tracking, heterogeneity of tasks (makes comparisons difficult), limited clinical validation, lack of interpretability of deep-learned features, variable ASR/transcription quality, and poorer performance for MCI detection compared with AD vs HC.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9978.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SVF_animal_naming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Verbal Fluency (SVF) tasks (animal/vegetable/location naming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Timed category fluency tasks (e.g., name animals in 60 seconds) used as linguistic tasks that are sensitive to AD-related semantic memory/executive dysfunction and included in speech datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (task-based cognitive marker)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — SVF tasks probe semantic memory and executive function; declines are markers of cognitive impairment that can be captured acoustically/linguistically.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>SVF has 'high sensitivity and specificity' for AD diagnosis according to review; example database PGA-OREKA (animal naming) used for MCI detection; Lopez‑De‑Ipina et al. reported accuracies (in VF tasks) up to 92% in some CV experiments (Lopez-De-Ipina [62] VF1: 92%). Chien et al. reported AUC 95.40% for some VF experiments (biLSTM on VF2/VF3).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Performance depends on dataset and task variant; small sample sizes and language dependence limit generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Cognitive-linguistic task (semantic fluency) measured via speech</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Participants name exemplars of a semantic category within a fixed time (e.g., 60 s); recorded speech is analyzed for count, pauses, disfluencies, and derived acoustic/linguistic features used by classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Reported high performance in some small studies (examples: accuracy ~92% in one CNN VF1 study; AUC 95.4% in a biLSTM VF task), but results vary by dataset and are often based on cross-validation with small samples.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Case-control studies and dataset analyses (cross-validation / test splits)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Examples: PGA-OREKA (Spain) VF1: HC 62 / MCI 38; Mandarin_Lu subsets used for Chinese VF tasks (AD 30 / HC 30).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Small, language-specific datasets; possible practice effects, inter-rater/transcription variability; limited external validation on larger cohorts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9978.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>picture_description_cookie_theft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Picture description tasks (e.g., 'cookie theft' Boston Diagnostic Aphasia Exam)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standardized picture/story description tasks used to elicit spontaneous speech; widely used in datasets (Pitt corpus, ADReSS) for automated AD detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (elicitation task / detection marker)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — these tasks provoke descriptive narrative speech enabling capture of lexical, syntactic, semantic, and acoustic markers of cognitive impairment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Many studies using picture description achieve high classification accuracy: across ADReSS and Pitt studies, cross-validation and test-set accuracies frequently exceed 80%; top ADReSS entries achieved ~89–91% on the held-out test set (Saltz et al. ~90%, Yuan et al. ~89.6%, Zhu et al. ~89.58%, Syed et al. ensemble 91.67%). Bertini et al. reported 93.3% on the Pitt dataset using unsupervised audio features + FNN.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Dataset and task uniformity help comparability but most datasets are English and limited in size; overfitting to dataset artifacts is a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Spontaneous speech elicitation task</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Participants describe a pictured scene (e.g., cookie theft); audio and transcripts are analyzed for features (pauses, hesitations, lexical richness, syntactic complexity) for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Typical reported accuracies: many studies >80% (cross-validation); best reported test-set results on ADReSS ~89–91% and single-study best on Pitt 93.3%. Sensitivity/specificity not uniformly reported across studies in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Multiple case-control datasets, shared-challenge evaluations (ADReSS), cross-validation and held-out test analyses</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Pitt corpus: many recordings (e.g., training sets with 87 AD / 79 HC in some splits); ADReSS: 156 subjects balanced for age/gender (108 train / 48 test); other datasets smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Language and dataset limitations (mostly English), potential dataset-specific confounds, need for longitudinal validation; variable reporting of performance metrics beyond accuracy (sensitivity, specificity) limits clinical translation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9978.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASR_pretrained_embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Speech Recognition (ASR) outputs and pre-trained embeddings (BERT, ERNIE, Longformer, wav2vec, speech-BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ASR to transcribe speech and pre-trained language/speech models to produce linguistic or acoustic embeddings that are then used for AD classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (feature extraction approach)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — embeddings capture patterns in lexical/phonetic/semantic content or raw audio that correlate with cognitive impairment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Pre-trained language models fine-tuned on AD tasks often outperform models trained from scratch: review reports BERT/ERNIE/Longformer-based systems achieving between ~81% and up to ~90% on ADReSS test set when fine-tuned (examples: BERT results 81–84.51% in some studies; Saltz et al., Yuan et al., Zhu et al. reached ~89–90% with larger/fine-tuned models). Acoustic pre-trained models (wav2vec 2.0) achieved mixed results (Gauder et al. ~78.9%; Chlasta et al. DemCNN 62.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Pre-trained models can be sensitive to mismatch between pretraining corpora and dementia speech; some acoustic pre-trained approaches underperform text-based embeddings; ASR errors (especially on disfluent or non-standard speech) can degrade downstream embedding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Feature engineering using pre-trained ASR/language/speech models</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>ASR transcribes speech; embeddings from models like BERT/ERNIE/Longformer (linguistic embeddings) or wav2vec/speech models (acoustic embeddings) are extracted and classified (often with SVM/RF or neural layers).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Text-based pre-trained models often in 81–84% range on some tasks; top fine-tuned pre-trained pipelines on ADReSS approached ~89–91%; acoustic pre-trained models show variable performance (e.g., wav2vec-based 78.9%; DemCNN 62.5%). Exact sensitivity/specificity generally not reported uniformly.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Experimental machine learning studies; ADReSS challenge top-performers used these methods</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Primarily ADReSS and Pitt datasets (balanced spontaneous speech picture descriptions), plus other corpora; limited by dataset sizes and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Pretraining helps but choice of pretraining task/dataset matters; lack of standardized comparison of pretraining tasks; ASR transcription errors and language differences reduce generalizability; interpretability remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9978.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>multimodal_approaches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal detection (speech + text + other modalities)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that fuse multiple modalities (acoustic + linguistic embeddings, and potentially other modalities like writing, eye movement, gait) to improve detection performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (multimodal detection approach)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — multimodal fusion aims to combine complementary signals (speech acoustics, lexical semantics, and other behavioral measures) to improve detection of cognitive impairment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Several studies report improved performance using fused acoustic and linguistic features or multi-modal attention networks (example: Syed et al. fused linguistic features and embeddings to reach 91.67% on ADReSS; Wang et al. modular multi-modal attention model reported 80.28% accuracy in one test).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>Results are heterogeneous and depend on which modalities are available/quality of each; integration complexity and data alignment challenges exist. There is sparse evidence on adding non-speech modalities (writing/gait/eye movements) in the reviewed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Multimodal machine learning / data fusion</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Combine features from multiple streams (acoustic features, transcript-derived linguistic features, learned embeddings) with attention or ensemble methods to classify AD vs non-AD.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Reported improvements in some studies; ensemble/fusion methods have produced the highest reported ADReSS test-set accuracies (~91.67% in one ensemble study). Overall improvements vary and are dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Case-control machine learning studies and challenge entries</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>ADReSS, Pitt, and other corpora where both audio and transcripts or embeddings are available; sample sizes moderate to small.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Multimodal approaches require more complex data collection and harmonization; many datasets do not provide all modalities; risk of overfitting on small datasets; lack of cohort/longitudinal validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9978.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADReSS_challenge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADReSS (Alzheimer's Dementia Recognition through Spontaneous Speech) Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark challenge and balanced dataset of spontaneous speech (picture descriptions) designed to enable standardized comparisons of AD detection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (benchmark resource)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>Not a cause — ADReSS provides a standardized, acoustically pre-processed, age- and gender-balanced speech dataset with MMSE labels for benchmarking automated AD recognition methods.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>ADReSS enabled head-to-head comparisons; top-performing challenge entries used pre-training and fine-tuning approaches, reporting held-out test accuracies near 89–91% (Saltz, Yuan, Zhu, Syed results cited in review).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>As a dataset/benchmark, ADReSS itself is not evidence for causal hypotheses; its limited size and single-language (English) composition constrain generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Benchmark dataset / challenge evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Standardized spontaneous speech (cookie-theft picture descriptions) WAV files and transcripts with MMSE scores and labels; used for classification/regression tasks in a shared task format.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Top ADReSS test-set accuracies cited: Saltz et al. ~90%, Yuan et al. ~89.6%, Zhu et al. ~89.58%, Syed et al. (ensemble) 91.67%. Many non-pretrained systems achieved ~64–85% depending on architecture and features.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Shared benchmark/challenge with multiple participant methods evaluated on held-out test set</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>ADReSS: 156 subjects total (108 training, 48 test), balanced for age/gender; English-language spontaneous picture-description speech.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Small benchmark size, English-only, potential overfitting by challenge participants, and limited reporting of sensitivity/specificity in some submissions; generalization to other languages/populations not established.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9978.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCI_detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MCI (Mild Cognitive Impairment) detection from speech</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Efforts to identify MCI (a prodromal stage) from speech patterns are reported but perform worse and are less numerous than AD vs HC studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>not a proposed cause (clinical target/diagnostic stage)</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>MCI is described as an intermediate state between normal aging and AD; detection aims to find subtle language/acoustic changes indicating early cognitive decline.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>Several studies attempted MCI vs HC classification; reported performance is generally lower than for AD detection — example metrics: Lindsay et al. (three pre-trained word embeddings + SVM) reported AUCs of 66%, 68%, and 69% in French/German/Dutch MCI screening; Rodrigues Makiuchi reported 60.6% accuracy for three-class AD/MCI/HC in one experiment. Some other small studies report higher accuracies but results are inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>MCI detection from speech is more challenging — fewer studies, smaller effect sizes, inconsistent findings across tasks/languages.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>Speech-based machine learning for early-stage cognitive impairment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>Same methodologies as AD detection (acoustic/linguistic features, embeddings, deep models) applied to distinguish MCI from HC or to perform three-way classification (AD/MCI/HC).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Reported AUCs around mid-60s (example: 66–69%); accuracies for three-class problems can be ~60% in some studies; overall worse and more variable than AD vs HC detection.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Case-control machine learning studies (limited number)</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Smaller cohorts focused on MCI; examples include VF1 PGA-OREKA (MCI 38 / HC 62), other national corpora with tens of MCI subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Sparse literature, low and inconsistent performance, lack of standardized MCI datasets, and potentially insufficient sensitivity of speech features to detect the subtler impairments in MCI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9978.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9978.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes of Alzheimer's disease, supporting or refuting evidence for each cause, methods for detecting Alzheimer's disease (including biomarkers, imaging, cognitive tests, etc.), and the effectiveness of these detection methods (such as sensitivity, specificity, and stage of detection). Also extract any controversies, limitations, or counter-evidence related to causes or detection methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>limitations_and_controversies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Main controversies, limitations, and counter-evidence highlighted in the review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of key limitations and controversial points about speech-based AD detection methods and the datasets used, as discussed by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>cause_type</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>cause_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_cause</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_cause</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_type</strong></td>
                            <td>N/A (summary of methodological limitations)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method_description</strong></td>
                            <td>The review identifies heterogeneity in tasks/languages, small and unbalanced datasets, lack of longitudinal/cohort data, lack of interpretability of learned features, and limited clinical deployment as major constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>The review notes that most speech-based deep learning methods achieve roughly ~85% accuracy, with some near 90% using pretraining, but that established cognitive tests (MMSE/MoCA) often surpass 93% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Review synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>study_population</strong></td>
                            <td>Aggregated across the included studies (52 papers, 27 databases, multiple languages and sample sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>controversies_or_limitations</strong></td>
                            <td>Key limitations: (1) small, language-limited, cross-sectional datasets; (2) heterogeneity of language tasks makes comparisons difficult; (3) deep learning lacks interpretability; (4) MCI detection is poorly solved; (5) pretraining literature may have been under-represented in search due to keyword choices; (6) lack of cohort/longitudinal validation impedes claims about tracking disease progression; (7) inconsistent reporting of sensitivity/specificity across studies restricts clinical assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Alzheimer's dementia recognition through spontaneous speech: The ADReSS challenge <em>(Rating: 2)</em></li>
                <li>Linguistic features identify Alzheimer's disease in narrative speech <em>(Rating: 2)</em></li>
                <li>Speech-based automatic and robust detection of very early dementia <em>(Rating: 2)</em></li>
                <li>Detection of dementia on voice recordings using deep learning: A Framingham Heart Study <em>(Rating: 2)</em></li>
                <li>On the analysis of speech and disfluencies for automatic detection of mild cognitive impairment <em>(Rating: 2)</em></li>
                <li>Automatic detection of mild cognitive impairment from spontaneous speech using ASR <em>(Rating: 1)</em></li>
                <li>Pauses for detection of Alzheimer's disease <em>(Rating: 1)</em></li>
                <li>Comparing pre-trained and feature-based models for prediction of Alzheimer's disease based on speech <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9978",
    "paper_id": "paper-254684331",
    "extraction_schema_id": "extraction-schema-170",
    "extracted_data": [
        {
            "name_short": "causes_not_discussed",
            "name_full": "Explicit etiological causes not discussed in this review",
            "brief_description": "The review does not propose or evaluate specific molecular/genetic causes (e.g., amyloid-beta, tau, APOE) of Alzheimer's disease; it focuses on detection methods based on speech and deep learning and remarks that other causes of dementia exist without detailing them.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not specified / not discussed",
            "cause_description": "The paper does not present or evaluate specific etiological hypotheses (such as amyloid-beta accumulation, tau pathology, APOE4 genotype, vascular contributions, chronic inflammation, head trauma, etc.). It only notes 'other causes of dementia' in passing and focuses on language changes as early manifestations.",
            "evidence_for_cause": "No primary evidence for molecular/genetic causes is provided in this review; the paper cites diagnostic guideline/review literature but does not summarize causal evidence.",
            "evidence_against_cause": "Not applicable in this review (no counter-evidence presented because causal mechanisms are not the topic).",
            "detection_method_type": "N/A",
            "detection_method_description": "N/A",
            "detection_performance": "N/A",
            "study_type": "review (focus on detection methods rather than etiology)",
            "study_population": "N/A",
            "controversies_or_limitations": "The review explicitly does not address molecular/genetic causes; it cautions that its focus and search keywords omitted some pre-trained-model literature and that only English-language papers were included, limiting etiologic coverage.",
            "uuid": "e9978.0",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "MMSE_MoCA",
            "name_full": "Mini-Mental State Examination (MMSE) and Montreal Cognitive Assessment (MoCA)",
            "brief_description": "Widely used brief cognitive screening instruments for global cognition and dementia screening referenced as comparators to speech-based detection.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (cognitive test)",
            "cause_description": "Not a cause — cognitive tests measuring global cognitive function and domain-specific abilities used for screening and staging dementia.",
            "evidence_for_cause": "Not applicable.",
            "evidence_against_cause": "Not applicable.",
            "detection_method_type": "Cognitive screening test / neuropsychological test",
            "detection_method_description": "Structured brief tests administered by clinicians to assess cognitive status (MMSE, MoCA); used as clinical gold-standard screening tools in many studies and as labels in databases.",
            "detection_performance": "The review states that traditional cognitive impairment screening scales such as MMSE or MoCA can usually achieve a screening accuracy of more than 93% (cited in the review). Specific sensitivity/specificity values are not reported in this review.",
            "study_type": "Referenced clinical screening studies / general diagnostic literature",
            "study_population": "Clinical older adult populations in referenced literature (not detailed in this review)",
            "controversies_or_limitations": "MMSE/MoCA are more accurate than most speech-based automated methods reported here (&gt;93% vs ~85%), which sets a high standard; these tests require clinical administration and may not be practical for large-scale automated screening.",
            "uuid": "e9978.1",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "EEG_MRI_PET_CSF",
            "name_full": "Electroencephalography (EEG), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET) imaging, and Cerebrospinal Fluid (CSF) assays",
            "brief_description": "Traditional neurophysiological, structural/functional imaging, molecular imaging, and fluid-biomarker methods used in AD diagnosis and research; mentioned as accurate but impractical for large-scale screening.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (biomarker/imaging methods)",
            "cause_description": "Not a cause — these are detection/diagnostic modalities measuring brain structure (MRI), functional/metabolic or molecular pathology (PET), electrophysiology (EEG), and biochemical markers in CSF (amyloid/tau) used in diagnostic workflows.",
            "evidence_for_cause": "The review cites these modalities as established diagnostic tools (referencing diagnostic guidelines) but does not re-evaluate primary evidence.",
            "evidence_against_cause": "No direct counter-evidence presented, but the review notes limitations (see below).",
            "detection_method_type": "Biomarker / neuroimaging / electrophysiology / fluid assay",
            "detection_method_description": "MRI (structural brain changes), PET (amyloid/tau imaging), EEG (electrophysiological changes), CSF assays (amyloid-beta, total/phospho-tau levels).",
            "detection_performance": "No specific sensitivity/specificity numbers are provided in this review; these methods are noted as accurate for diagnosis but described as inappropriate for large-scale early screening due to cost, resource intensity, or invasiveness.",
            "study_type": "Referenced diagnostic and guideline literature (NIA‑AA, etc.)",
            "study_population": "Clinical research cohorts (not specified in detail in this review)",
            "controversies_or_limitations": "High cost, limited accessibility, and invasiveness (CSF lumbar puncture) make PET/CSF assays unsuitable for large-scale population-level early screening; the review motivates speech analysis as cheaper and scalable alternative.",
            "uuid": "e9978.2",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "speech_DL_general",
            "name_full": "Speech-based Alzheimer's disease detection using deep learning",
            "brief_description": "Automated detection of AD from spontaneous speech or task-based speech using deep learning models extracting acoustic and/or linguistic features and embeddings.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (detection approach)",
            "cause_description": "Not a cause — this is a detection approach that leverages language and acoustic changes associated with cognitive decline (pauses, slow speech, word-finding difficulties, disfluencies) as proxies for underlying disease.",
            "evidence_for_cause": "Numerous case-control studies summarized in the review report classification accuracies commonly around ~85% using deep learning; pre-trained model fine-tuning often improves results; selected highest reported performances include 93.3% accuracy (Bertini et al., Pitt dataset), and multiple ADReSS challenge teams reached ~89–91% accuracy on the ADReSS test set using pre-trained language models and embeddings.",
            "evidence_against_cause": "Speech-based methods generally underperform established cognitive tests (MMSE/MoCA &gt;93%) and have variable performance across datasets; performance for MCI detection is notably lower and more inconsistent (AUCs around 66–69% reported for some MCI tasks).",
            "detection_method_type": "Behavioral / speech signal analysis (machine learning / deep learning)",
            "detection_method_description": "Measurement of acoustic features (MFCCs, formants, pauses, speech rate), duration/disfluency features, linguistics features (lexical/syntactic/semantic measures), and learned embeddings from pre-trained ASR/language/speech models (BERT, ERNIE, Longformer, wav2vec). Deep models (FNN, CNN, LSTM, attention/transformer) classify AD vs non-AD or predict MMSE.",
            "detection_performance": "Typical accuracies around 80–85% across many studies; pre-trained models fine-tuned on AD data often reach ~89–91% on ADReSS test set (examples: Saltz et al. ~90%, Yuan et al. ~89.6%, Zhu et al. ~89.58%); highest single-study reported accuracy on Pitt dataset 93.30% (Bertini et al.). MCI detection AUCs/accuracies are lower (examples: Lindsay et al. AUC 66–69%; Rodrigues Makiuchi three-class accuracy 60.6%). Sensitivity/specificity are generally not consistently reported across studies in this review.",
            "study_type": "Aggregated results from multiple case-control studies, cross-validation and held-out test evaluations, and the ADReSS challenge benchmark",
            "study_population": "Heterogeneous: many small datasets in single languages (Pitt corpus, ADReSS, various national corpora), sample sizes range from tens to a few hundred per class; ADReSS: 156 subjects (108 train / 48 test); Pitt: hundreds of recordings; Framingham example larger (dementia 223 / MCI 309 / HC 291).",
            "controversies_or_limitations": "Limitations include small and language-limited datasets, lack of longitudinal/cohort data to test intra-individual tracking, heterogeneity of tasks (makes comparisons difficult), limited clinical validation, lack of interpretability of deep-learned features, variable ASR/transcription quality, and poorer performance for MCI detection compared with AD vs HC.",
            "uuid": "e9978.3",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "SVF_animal_naming",
            "name_full": "Semantic Verbal Fluency (SVF) tasks (animal/vegetable/location naming)",
            "brief_description": "Timed category fluency tasks (e.g., name animals in 60 seconds) used as linguistic tasks that are sensitive to AD-related semantic memory/executive dysfunction and included in speech datasets.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (task-based cognitive marker)",
            "cause_description": "Not a cause — SVF tasks probe semantic memory and executive function; declines are markers of cognitive impairment that can be captured acoustically/linguistically.",
            "evidence_for_cause": "SVF has 'high sensitivity and specificity' for AD diagnosis according to review; example database PGA-OREKA (animal naming) used for MCI detection; Lopez‑De‑Ipina et al. reported accuracies (in VF tasks) up to 92% in some CV experiments (Lopez-De-Ipina [62] VF1: 92%). Chien et al. reported AUC 95.40% for some VF experiments (biLSTM on VF2/VF3).",
            "evidence_against_cause": "Performance depends on dataset and task variant; small sample sizes and language dependence limit generalizability.",
            "detection_method_type": "Cognitive-linguistic task (semantic fluency) measured via speech",
            "detection_method_description": "Participants name exemplars of a semantic category within a fixed time (e.g., 60 s); recorded speech is analyzed for count, pauses, disfluencies, and derived acoustic/linguistic features used by classifiers.",
            "detection_performance": "Reported high performance in some small studies (examples: accuracy ~92% in one CNN VF1 study; AUC 95.4% in a biLSTM VF task), but results vary by dataset and are often based on cross-validation with small samples.",
            "study_type": "Case-control studies and dataset analyses (cross-validation / test splits)",
            "study_population": "Examples: PGA-OREKA (Spain) VF1: HC 62 / MCI 38; Mandarin_Lu subsets used for Chinese VF tasks (AD 30 / HC 30).",
            "controversies_or_limitations": "Small, language-specific datasets; possible practice effects, inter-rater/transcription variability; limited external validation on larger cohorts.",
            "uuid": "e9978.4",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "picture_description_cookie_theft",
            "name_full": "Picture description tasks (e.g., 'cookie theft' Boston Diagnostic Aphasia Exam)",
            "brief_description": "Standardized picture/story description tasks used to elicit spontaneous speech; widely used in datasets (Pitt corpus, ADReSS) for automated AD detection.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (elicitation task / detection marker)",
            "cause_description": "Not a cause — these tasks provoke descriptive narrative speech enabling capture of lexical, syntactic, semantic, and acoustic markers of cognitive impairment.",
            "evidence_for_cause": "Many studies using picture description achieve high classification accuracy: across ADReSS and Pitt studies, cross-validation and test-set accuracies frequently exceed 80%; top ADReSS entries achieved ~89–91% on the held-out test set (Saltz et al. ~90%, Yuan et al. ~89.6%, Zhu et al. ~89.58%, Syed et al. ensemble 91.67%). Bertini et al. reported 93.3% on the Pitt dataset using unsupervised audio features + FNN.",
            "evidence_against_cause": "Dataset and task uniformity help comparability but most datasets are English and limited in size; overfitting to dataset artifacts is a concern.",
            "detection_method_type": "Spontaneous speech elicitation task",
            "detection_method_description": "Participants describe a pictured scene (e.g., cookie theft); audio and transcripts are analyzed for features (pauses, hesitations, lexical richness, syntactic complexity) for classification.",
            "detection_performance": "Typical reported accuracies: many studies &gt;80% (cross-validation); best reported test-set results on ADReSS ~89–91% and single-study best on Pitt 93.3%. Sensitivity/specificity not uniformly reported across studies in the review.",
            "study_type": "Multiple case-control datasets, shared-challenge evaluations (ADReSS), cross-validation and held-out test analyses",
            "study_population": "Pitt corpus: many recordings (e.g., training sets with 87 AD / 79 HC in some splits); ADReSS: 156 subjects balanced for age/gender (108 train / 48 test); other datasets smaller.",
            "controversies_or_limitations": "Language and dataset limitations (mostly English), potential dataset-specific confounds, need for longitudinal validation; variable reporting of performance metrics beyond accuracy (sensitivity, specificity) limits clinical translation.",
            "uuid": "e9978.5",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ASR_pretrained_embeddings",
            "name_full": "Automatic Speech Recognition (ASR) outputs and pre-trained embeddings (BERT, ERNIE, Longformer, wav2vec, speech-BERT)",
            "brief_description": "Use of ASR to transcribe speech and pre-trained language/speech models to produce linguistic or acoustic embeddings that are then used for AD classification.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (feature extraction approach)",
            "cause_description": "Not a cause — embeddings capture patterns in lexical/phonetic/semantic content or raw audio that correlate with cognitive impairment.",
            "evidence_for_cause": "Pre-trained language models fine-tuned on AD tasks often outperform models trained from scratch: review reports BERT/ERNIE/Longformer-based systems achieving between ~81% and up to ~90% on ADReSS test set when fine-tuned (examples: BERT results 81–84.51% in some studies; Saltz et al., Yuan et al., Zhu et al. reached ~89–90% with larger/fine-tuned models). Acoustic pre-trained models (wav2vec 2.0) achieved mixed results (Gauder et al. ~78.9%; Chlasta et al. DemCNN 62.5%).",
            "evidence_against_cause": "Pre-trained models can be sensitive to mismatch between pretraining corpora and dementia speech; some acoustic pre-trained approaches underperform text-based embeddings; ASR errors (especially on disfluent or non-standard speech) can degrade downstream embedding quality.",
            "detection_method_type": "Feature engineering using pre-trained ASR/language/speech models",
            "detection_method_description": "ASR transcribes speech; embeddings from models like BERT/ERNIE/Longformer (linguistic embeddings) or wav2vec/speech models (acoustic embeddings) are extracted and classified (often with SVM/RF or neural layers).",
            "detection_performance": "Text-based pre-trained models often in 81–84% range on some tasks; top fine-tuned pre-trained pipelines on ADReSS approached ~89–91%; acoustic pre-trained models show variable performance (e.g., wav2vec-based 78.9%; DemCNN 62.5%). Exact sensitivity/specificity generally not reported uniformly.",
            "study_type": "Experimental machine learning studies; ADReSS challenge top-performers used these methods",
            "study_population": "Primarily ADReSS and Pitt datasets (balanced spontaneous speech picture descriptions), plus other corpora; limited by dataset sizes and languages.",
            "controversies_or_limitations": "Pretraining helps but choice of pretraining task/dataset matters; lack of standardized comparison of pretraining tasks; ASR transcription errors and language differences reduce generalizability; interpretability remains limited.",
            "uuid": "e9978.6",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "multimodal_approaches",
            "name_full": "Multimodal detection (speech + text + other modalities)",
            "brief_description": "Approaches that fuse multiple modalities (acoustic + linguistic embeddings, and potentially other modalities like writing, eye movement, gait) to improve detection performance.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (multimodal detection approach)",
            "cause_description": "Not a cause — multimodal fusion aims to combine complementary signals (speech acoustics, lexical semantics, and other behavioral measures) to improve detection of cognitive impairment.",
            "evidence_for_cause": "Several studies report improved performance using fused acoustic and linguistic features or multi-modal attention networks (example: Syed et al. fused linguistic features and embeddings to reach 91.67% on ADReSS; Wang et al. modular multi-modal attention model reported 80.28% accuracy in one test).",
            "evidence_against_cause": "Results are heterogeneous and depend on which modalities are available/quality of each; integration complexity and data alignment challenges exist. There is sparse evidence on adding non-speech modalities (writing/gait/eye movements) in the reviewed literature.",
            "detection_method_type": "Multimodal machine learning / data fusion",
            "detection_method_description": "Combine features from multiple streams (acoustic features, transcript-derived linguistic features, learned embeddings) with attention or ensemble methods to classify AD vs non-AD.",
            "detection_performance": "Reported improvements in some studies; ensemble/fusion methods have produced the highest reported ADReSS test-set accuracies (~91.67% in one ensemble study). Overall improvements vary and are dataset-dependent.",
            "study_type": "Case-control machine learning studies and challenge entries",
            "study_population": "ADReSS, Pitt, and other corpora where both audio and transcripts or embeddings are available; sample sizes moderate to small.",
            "controversies_or_limitations": "Multimodal approaches require more complex data collection and harmonization; many datasets do not provide all modalities; risk of overfitting on small datasets; lack of cohort/longitudinal validation.",
            "uuid": "e9978.7",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ADReSS_challenge",
            "name_full": "ADReSS (Alzheimer's Dementia Recognition through Spontaneous Speech) Challenge",
            "brief_description": "A benchmark challenge and balanced dataset of spontaneous speech (picture descriptions) designed to enable standardized comparisons of AD detection methods.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (benchmark resource)",
            "cause_description": "Not a cause — ADReSS provides a standardized, acoustically pre-processed, age- and gender-balanced speech dataset with MMSE labels for benchmarking automated AD recognition methods.",
            "evidence_for_cause": "ADReSS enabled head-to-head comparisons; top-performing challenge entries used pre-training and fine-tuning approaches, reporting held-out test accuracies near 89–91% (Saltz, Yuan, Zhu, Syed results cited in review).",
            "evidence_against_cause": "As a dataset/benchmark, ADReSS itself is not evidence for causal hypotheses; its limited size and single-language (English) composition constrain generalizability.",
            "detection_method_type": "Benchmark dataset / challenge evaluation",
            "detection_method_description": "Standardized spontaneous speech (cookie-theft picture descriptions) WAV files and transcripts with MMSE scores and labels; used for classification/regression tasks in a shared task format.",
            "detection_performance": "Top ADReSS test-set accuracies cited: Saltz et al. ~90%, Yuan et al. ~89.6%, Zhu et al. ~89.58%, Syed et al. (ensemble) 91.67%. Many non-pretrained systems achieved ~64–85% depending on architecture and features.",
            "study_type": "Shared benchmark/challenge with multiple participant methods evaluated on held-out test set",
            "study_population": "ADReSS: 156 subjects total (108 training, 48 test), balanced for age/gender; English-language spontaneous picture-description speech.",
            "controversies_or_limitations": "Small benchmark size, English-only, potential overfitting by challenge participants, and limited reporting of sensitivity/specificity in some submissions; generalization to other languages/populations not established.",
            "uuid": "e9978.8",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "MCI_detection",
            "name_full": "MCI (Mild Cognitive Impairment) detection from speech",
            "brief_description": "Efforts to identify MCI (a prodromal stage) from speech patterns are reported but perform worse and are less numerous than AD vs HC studies.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "not a proposed cause (clinical target/diagnostic stage)",
            "cause_description": "MCI is described as an intermediate state between normal aging and AD; detection aims to find subtle language/acoustic changes indicating early cognitive decline.",
            "evidence_for_cause": "Several studies attempted MCI vs HC classification; reported performance is generally lower than for AD detection — example metrics: Lindsay et al. (three pre-trained word embeddings + SVM) reported AUCs of 66%, 68%, and 69% in French/German/Dutch MCI screening; Rodrigues Makiuchi reported 60.6% accuracy for three-class AD/MCI/HC in one experiment. Some other small studies report higher accuracies but results are inconsistent.",
            "evidence_against_cause": "MCI detection from speech is more challenging — fewer studies, smaller effect sizes, inconsistent findings across tasks/languages.",
            "detection_method_type": "Speech-based machine learning for early-stage cognitive impairment",
            "detection_method_description": "Same methodologies as AD detection (acoustic/linguistic features, embeddings, deep models) applied to distinguish MCI from HC or to perform three-way classification (AD/MCI/HC).",
            "detection_performance": "Reported AUCs around mid-60s (example: 66–69%); accuracies for three-class problems can be ~60% in some studies; overall worse and more variable than AD vs HC detection.",
            "study_type": "Case-control machine learning studies (limited number)",
            "study_population": "Smaller cohorts focused on MCI; examples include VF1 PGA-OREKA (MCI 38 / HC 62), other national corpora with tens of MCI subjects.",
            "controversies_or_limitations": "Sparse literature, low and inconsistent performance, lack of standardized MCI datasets, and potentially insufficient sensitivity of speech features to detect the subtler impairments in MCI.",
            "uuid": "e9978.9",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "limitations_and_controversies",
            "name_full": "Main controversies, limitations, and counter-evidence highlighted in the review",
            "brief_description": "Summary of key limitations and controversial points about speech-based AD detection methods and the datasets used, as discussed by the authors.",
            "citation_title": "Deep learning-based speech analysis for Alzheimer's disease detection: a literature review",
            "mention_or_use": "mention",
            "cause_type": "N/A",
            "cause_description": "N/A",
            "evidence_for_cause": "N/A",
            "evidence_against_cause": "N/A",
            "detection_method_type": "N/A (summary of methodological limitations)",
            "detection_method_description": "The review identifies heterogeneity in tasks/languages, small and unbalanced datasets, lack of longitudinal/cohort data, lack of interpretability of learned features, and limited clinical deployment as major constraints.",
            "detection_performance": "The review notes that most speech-based deep learning methods achieve roughly ~85% accuracy, with some near 90% using pretraining, but that established cognitive tests (MMSE/MoCA) often surpass 93% accuracy.",
            "study_type": "Review synthesis",
            "study_population": "Aggregated across the included studies (52 papers, 27 databases, multiple languages and sample sizes).",
            "controversies_or_limitations": "Key limitations: (1) small, language-limited, cross-sectional datasets; (2) heterogeneity of language tasks makes comparisons difficult; (3) deep learning lacks interpretability; (4) MCI detection is poorly solved; (5) pretraining literature may have been under-represented in search due to keyword choices; (6) lack of cohort/longitudinal validation impedes claims about tracking disease progression; (7) inconsistent reporting of sensitivity/specificity across studies restricts clinical assessment.",
            "uuid": "e9978.10",
            "source_info": {
                "paper_title": "Deep learning-based speech analysis for Alzheimer’s disease detection: a literature review",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Alzheimer's dementia recognition through spontaneous speech: The ADReSS challenge",
            "rating": 2,
            "sanitized_title": "alzheimers_dementia_recognition_through_spontaneous_speech_the_adress_challenge"
        },
        {
            "paper_title": "Linguistic features identify Alzheimer's disease in narrative speech",
            "rating": 2,
            "sanitized_title": "linguistic_features_identify_alzheimers_disease_in_narrative_speech"
        },
        {
            "paper_title": "Speech-based automatic and robust detection of very early dementia",
            "rating": 2,
            "sanitized_title": "speechbased_automatic_and_robust_detection_of_very_early_dementia"
        },
        {
            "paper_title": "Detection of dementia on voice recordings using deep learning: A Framingham Heart Study",
            "rating": 2,
            "sanitized_title": "detection_of_dementia_on_voice_recordings_using_deep_learning_a_framingham_heart_study"
        },
        {
            "paper_title": "On the analysis of speech and disfluencies for automatic detection of mild cognitive impairment",
            "rating": 2,
            "sanitized_title": "on_the_analysis_of_speech_and_disfluencies_for_automatic_detection_of_mild_cognitive_impairment"
        },
        {
            "paper_title": "Automatic detection of mild cognitive impairment from spontaneous speech using ASR",
            "rating": 1,
            "sanitized_title": "automatic_detection_of_mild_cognitive_impairment_from_spontaneous_speech_using_asr"
        },
        {
            "paper_title": "Pauses for detection of Alzheimer's disease",
            "rating": 1,
            "sanitized_title": "pauses_for_detection_of_alzheimers_disease"
        },
        {
            "paper_title": "Comparing pre-trained and feature-based models for prediction of Alzheimer's disease based on speech",
            "rating": 1,
            "sanitized_title": "comparing_pretrained_and_featurebased_models_for_prediction_of_alzheimers_disease_based_on_speech"
        }
    ],
    "cost": 0.02462825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep learning-based speech analysis for Alzheimer's disease detection: a literature review</p>
<p>Qin Yang 
Xin Li 
Xinyun Ding 
Feiyang Xu 
Zhenhua Ling 
Deep learning-based speech analysis for Alzheimer's disease detection: a literature review
10.1186/s13195-022-01131-3Yang et al. Alzheimer's Research &amp; Therapy (2022) 14:186 REVIEW This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article' s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article' s to the data made available in this article, unless otherwise stated in a credit line to the data.Alzheimer's disease detectionSpeech analysisDeep learning
Background: Alzheimer's disease has become one of the most common neurodegenerative diseases worldwide, which seriously affects the health of the elderly. Early detection and intervention are the most effective prevention methods currently. Compared with traditional detection methods such as traditional scale tests, electroencephalograms, and magnetic resonance imaging, speech analysis is more convenient for automatic large-scale Alzheimer's disease detection and has attracted extensive attention from researchers. In particular, deep learning-based speech analysis and language processing techniques for Alzheimer's disease detection have been studied and achieved impressive results.Methods:To integrate the latest research progresses, hundreds of relevant papers from ACM, DBLP, IEEE, PubMed, Scopus, Web of Science electronic databases, and other sources were retrieved. We used these keywords for paper search: (Alzheimer OR dementia OR cognitive impairment) AND (speech OR voice OR audio) AND (deep learning OR neural network).Conclusions: Fifty-two papers were finally retained after screening. We reviewed and presented the speech databases, deep learning methods, and model performances of these studies. In the end, we pointed out the mainstreams and limitations in the current studies and provided a direction for future research.</p>
<p>emission tomography (PET) imaging, and cerebrospinal fluid (CSF) assays [3], are not appropriate for large-scale nationwide early AD screening applications. Therefore, some studies focus on developing a cheaper and more convenient method to detect AD.</p>
<p>Relevant studies have shown that language disorders usually appear in the early process of AD, and it is possible to detect AD by capturing the acoustic and linguistic features of subjects through audio and automatic speech recognition technology [4][5][6]. Some studies have given the results of studies on distinguishing characteristics between AD and healthy control (HC) group. Compared with cognitive normal people, AD patients usually speak more slowly with more pauses between words [7] and suffer from word finding and word retrieval difficulties [6,8,9].</p>
<p>Dozens of speech-based methods have been explored for the research on AD detection. Studies have shown that the acoustic measures have a high correlation with pathological language features and voice changes in automatic language processing were proven to be useful for AD detection [10,11]. In addition, previous studies of speech pathology have revealed that people with dementia have linguistic manifestations including pauses, filler words, restarts, repetitions, and incomplete statements. Fraser, K.C. et al. extracted linguistic features such as semantics, syntax, and information and achieved 91% accuracy [4] in the AD detection task by using the logistic regression classifier. Liu, Z. et al. extracted and fused duration features, acoustic features, linguistic features, the AD detection, and linguistic features, and finally obtained 81.9% accuracy of AD detection based on the logistic regression classification method [12]. In addition to these, Satt, A. et al. utilized recordings while subjects completed cognitive tasks to extract relevant acoustic features, and achieved an accuracy of 87% in the classification between AD and control [5].</p>
<p>With the wide application of deep learning, we can find that neural networks have made significant progress in the field of speech modeling. Hinton, G. et al. applied deep neural networks (DNNs) to acoustic modeling and obtained better recognition results than Gaussian Mixed Model (GMM), thus opening up a new field in speech recognition [13]. Therefore, researchers began to try to apply various deep learning methods to the field of speech-based AD detection. Rosas, D.S. et al. extracted linguistic features and used a 3-layer neural network reaching a binary classification accuracy of 78.3% [14]. However, there is fewer speech data for Alzheimer's patients, and the improvement in classification results is relatively small by using neural networks. Recent studies have shown that pre-trained models such as BERT [15] achieve promising results on a variety of benchmark tasks, and can capture a wide range of linguistic facts including lexical knowledge, phonology, syntax, semantics, and pragmatics without a lot of data. Apart from this, the pre-trained automatic speech recognition (ASR) model can not only get the transcribed text of speech but also extract acoustic embeddings which can be used to represent the conversion in speech for better automatic analysis. Toth, L. et al. obtained phonetic segmentation and label of the input signal by applying an ASR model based on a special convolutional deep neural network, thereby obtaining acoustic features such as speech rate, pause, and hesitation rate [16]. Judging by the current research trends, the deep learning method is the most mainstream method for AD detection now.</p>
<p>Simultaneously, some review papers on AD detection have also been published, such as a systematic review about speech-based detection and classification of AD written by Inês Vigo et al. [17]. However, most of the classification methods are based on traditional machine learning methods, which have certain limitations due to the excellent performance achieved by deep learning methods in AD detection.</p>
<p>Therefore, this paper focuses on deep learning-based speech analysis for AD detection. This research paper is organized as follows: the objects of this review in the "Objectives" section, the search and selection process is introduced in the "Materials and methods" section, the results in the "Results" section, the discussion of these selected papers in the "Conclusions" section, and the limitation of our work and our future goals in the "Discussions" section.</p>
<p>Objectives</p>
<p>To make a comprehensive discussion on the current application of deep learning in speech-based AD detection, this review conducted a systematic analysis of selected papers in response to the following 5 questions:</p>
<p>(1) What were the characteristics of the databases involved in reported studies? (2) What deep learning model architectures were included in reported studies? (3) How were these deep learning model architectures used in reported studies? (4) What classification performance has been achieved? (5) What were the mainstreams and limitations of reported studies?</p>
<p>Materials and methods</p>
<p>Search process</p>
<p>Our searches were conducted on the following electronic databases: ACM, DBLP, IEEE, PubMed, Scopus, and Web of Science. Unlike most previous review papers on "Alzheimer's disease detection" [18], we paid more attention to these papers which used deep learning methods to analyze speech data of elderly people in different health states (AD, MCI, and HC). Therefore, we used the following keywords for paper search: (Alzheimer OR dementia OR cognitive impairment) AND (speech OR voice OR audio) AND (deep learning OR neural network). Figure 1 listed all the search strategies. The last search was conducted on 19 January 2022.</p>
<p>Selection process</p>
<p>The exclusion criteria were as follows: (1) studies that did not use deep learning methods; (2) studies do not focus on speech or text data; (3) studies without a group of MCI and AD; (4) papers that were not written in English; (5) studies cannot find the full text. Initial study selection was performed by two reviewers independently. To minimize the bias in selecting studies, papers that were not sure to include were resolved in a discussion with the third reviewer.</p>
<p>Data extraction and synthesis</p>
<p>The analyzed data in our studies include database names, task types, language types, label distributions, and whether the databases include an audio or corresponding transcript or not.</p>
<p>Results</p>
<p>Study selection</p>
<p>The detail of our search process is displayed in Fig. 2 through a flow diagram. other source papers retrieved from the ADReSS website [19] which were not found in the other six sources. After the search process, a total of 710 papers were retrieved; 293 duplicates were removed by Endnote and manual screening. After screening by our exclusion rules, 52 studies were finally included.</p>
<p>Speech databases</p>
<p>In the process of building a deep learning model, a highquality database can improve the quality of model training and the accuracy of prediction. At present, several speech databases for cognitive impairment of the elderly have been established around the world, providing great support for researchers to explore more efficient cognitive impairment assessments. According to our research, the linguistic tasks involved in the studies on AD detection based on deep learning methods can be divided into three categories: semantic verbal fluency (SVF), spontaneous speech (SS), and reading. Therefore, the related speech databases will also be introduced from these three aspects in this review (Table 1).</p>
<p>Semantic verbal fluency tasks</p>
<p>The semantic verbal fluency (SVF) test has high sensitivity and specificity for the diagnosis of AD, so it is widely  to assess language skills, semantic memory, and executive functions of AD patients. During the SVF task, patients were asked to list all names they can remember from a category within one minute, such as animals, vegetables, and locations [20].</p>
<p>Animal naming</p>
<p>The subjects were asked to say the name of the animal they can think of as quickly as possible within 60 s and were reminded if they stop. At the end of the 60 s, the total number of animals (NOT including repetitions or non-animal words) were counted as their scores [22].</p>
<p>Lopez-De-Ipina, K. et al. constructed a well-distributed animal naming database called PGA-OREKA, which presents a novel proposal based on automatic analysis of speech and disfluencies aimed at supporting MCI diagnosis [21]. The PGA-OREKA database contains 62 healthy people and 38 MCI patients, and it is a subset of the cohort of the Gipuzkoa-Alzheimer Project (PGA) of the CITA-Alzheimer Foundation which includes 187 healthy people and 38 MCI patients.</p>
<p>Vegetable and location naming</p>
<p>Similar to animal naming, in vegetable and location naming tests, subjects were asked to say as many words related to the designated topic as possible within one minute. Chien, Y.W. et al. from National Taiwan University constructed a fluency test database based on the Mandarin_Lu corpus [23]. Mandarin_Lu corpus from DementiaBank contains interview recordings of 52 AD patients [24], Chien, Y.W. et al. selected 30 patients and segmented the first-minute response of the audio data, and then recruited 30 additional healthy subjects to complete vegetable and location naming tasks.</p>
<p>Spontaneous speech tasks</p>
<p>Spontaneous speech (SS) means speech without responding to a question. Temporal parameters of spontaneous speech have been proven to be able to provide sensitive measures of a subject's speech and language skills [25]. Several different types of spontaneous language tasks are covered in this review paper: conversation/interview speech, event description, recall story, and picture description.</p>
<p>Conversation/interview speech</p>
<p>Through natural language processing and analysis of the subject's speech obtained from free and simple conversational speech, some vital biological features that reflect early signs of AD can be extracted for early screening.</p>
<p>Lopez-De-Ipina, K. et al. built up a multicultural and multilingual database called AZTIAHO [26], which contains 20 h of video recordings of 50 healthy control and 20 AD patients. The recordings consisted of conversational   Gothenburg MCI study [38] Reading Swedish AD (25)/HC (30) Yes No speech where subjects tell pleasant stories or feelings and interact with each other.</p>
<p>Day/life/dream description</p>
<p>During these tests, subjects were asked to spontaneously describe events such as tell about the day yesterday in detail. Gosztolya, G. et al. established the Hungarian MCI-mAD Database [27], which recorded 225 voices of 75 subjects (25 AD, 25 MCI, and 25 HC).</p>
<p>Recall story</p>
<p>Subjects were given orally presented stories, reading materials, or films to learn the specific stories. Then they were asked to recall and retell the story spontaneously twice, immediately and in a few minutes, to the examiners without reference to those materials. The Wallet Story database was collected based on the immediate and late retelling of a memorized story from (Bayles and Tomoeda, 1993), which is the evaluation of the episodic memory, one of a standardized test battery named ABCD (Arizona Battery for Communication Disorders) for the comprehensive assessment and screening of dementia. The Wallet Story database included 23 elders with MCI and 12 healthy aging adults, which had 70 narratives in total.</p>
<p>Picture description (PD)</p>
<p>Subjects were asked to look at a picture or a series of pictures that make up a story and describe orally the content in pictures within a limited time. Pictures include the cookie theft (a girl and a boy stealing cookies and a woman washing dishes in the kitchen), the dog story (a boy who hides a dog that he found on the street), the Cinderella story, and so on.</p>
<p>Dementiabank [28] is a multimedia interaction for the study of communication in dementia. Pitt corpus [29], ADReSS database [19], and ADReSSo database [30] are subsets of this database. Pitt corpus mainly included recordings of spoken picture descriptions extracted from participants through the cookie theft picture description from the Boston Diagnostic Aphasia Exam [31], which contained 87 speech recordings in AD patients and 79 speech recordings in healthy controls in the training set, and 71 speech recordings without annotations in the testing set. ADReSS database contained speech samples (WAV format) and transcripts (CHA format) with corresponding MMSE (Mini-Mental State Examination) scores as labels, which included 156 subjects, 108 were for training and 48 were for the test (train:test = 7:3). The ADReSSo database was established after the ADReSS database and included 87 AD patients and 79 HC.</p>
<p>Reading</p>
<p>Transcripts reading</p>
<p>Subjects were given short passages or articles to read aloud and their speeches were recorded. The Gothenburg MCI study was conducted as an experiment with 55 Swedish participants (30 HC and 25 AD) who were instructed by a clinician to read a short passage, consisting of 144 words, as part of their evaluation [32].</p>
<p>Deep learning techniques</p>
<p>In order to investigate the recent progress of deep learning methods in speech-based AD detection, we list some key information in the selected papers in the table below: linguistic tasks, the distribution of participants for each label in the database, the feature types used in papers, the specific model architecture, the model training strategy, and the best performance (Table 2).</p>
<p>Feature types</p>
<p>Feature types mentioned in our paper include demographic features (DeF), duration features (DF), traditional acoustic features (TAF), traditional linguistic features (TLF), acoustic embeddings, and linguistic embeddings. Demographic features include age, years of education, and gender. Duration features contain the duration of the speaker speaking and its statistics. Traditional acoustic features include properties of the sound wave (MFCCs or Formant), speech rate, and the number of pauses. Traditional linguistic features include lexical (word rate or types and their characteristics, e.g., word frequency, repetitions), semantic (word meaning, e.g., idea density), and syntactic (grammar of sentences, e.g., syntactic complexity, grammatical constituents) features. Acoustic embeddings (AE) means the feature vector representations of speech, which can be extracted by ASR models or pretrained models (such as speech BERT or YAMNet). Linguistic embeddings (LE) are a type of automatic feature that refers to the vector representations corresponding to input tokens, which can be obtained by models such as BERT [15], ERNIE [77], or Longformer [78].</p>
<p>Model architectures</p>
<p>In this paragraph, we briefly introduce some deep learning models used in the selected papers, and the model structure used in each paper can be viewed in the table.</p>
<p>Feedforward neural network</p>
<p>Earlier researchers started to use feedforward neural networks (FNN) [79] as feature classifiers in their studies to distinguish healthy people from cognitively impaired patients.  </p>
<p>Convolution neural network</p>
<p>As the convolutional neural network (CNN) [80] has achieved good results in computer vision tasks, CNNrelated models have also begun to be gradually applied to NLP tasks, such as sentence classification, semantic parsing, search query retrieval, and other traditional NLP tasks. Therefore, researchers also began to use CNN models and linear gated convolution neural network (GCNN) [81] to classify speech or text data of AD patients.</p>
<p>Recurrent meural network</p>
<p>In order to add timing information from speaker audio to the model, researchers began to use model architectures including recurrent neural network (RNN) [82], long short-term memory (LSTM) [83], gated recurrent unit(GRU) [84], bidirectional LSTM(BiLSTM) [85], etc. At the same time, researchers also combine these models with CNN or other neural networks, such as pyramidal bidirectional LSTM followed by a CNN layer (pBiLSTM-CNN) proposed by Meghanani. A [86].</p>
<p>Attention-based neural network</p>
<p>With the rise of attention mechanisms [87], researchers began to apply some attention mechanisms to improve the accuracy of the model, such as adding attention mechanisms to RNN models or CNN and LSTM models.</p>
<p>To identify AD with a small amount of data, researchers utilize models pre-trained on large-scale databases as feature extractors to obtain better representations, such as Longformer, BERT, and ERNIE.</p>
<p>Conclusions</p>
<p>What were the characteristics of the databases involved in reported studies?</p>
<p>Twenty-seven different databases were used in 52 studies, in which the appearance frequency of the Pitt corpus and ADReSS database were highest. Fourteen studies used Pitt corpus from Dementiabank, and 19 studies included the ADReSS database.</p>
<p>In 27 databases, 11 languages were used. Twentyfive databases used only one language in one database, including Spain, Chinese, English, Hungarian, Italian, Japanese, Brazilian Portuguese, and Swedish. Two databases used more than one language in one database. For example, AZTIAHO included English, French, Spanish, Catalan, Basque, Chinese, Arabian, and Portuguese.</p>
<p>In 29 databases, labels include AD (Alzheimer's disease), MCI (mild cognitive impairment), and HC (healthy control). Eleven databases contain only AD and HC labels; 7 databases contain only MCI and HC labels; 11 databases contain AD, MCI, and HC labels.</p>
<p>For now, the databases in reported studies were small in single or few languages with uneven distribution. Besides, most were built for cross-sectional studies rather than cohort studies.</p>
<p>What deep learning model architectures were included in reported studies?</p>
<p>Four deep learning methods were applied in these selected papers: FNN, CNN, LSTM, and attention mechanism-based models. Figure 3 shows each number of these methods. These models were generally basic, and embeddings were extracted by models and collected for classification. How were these deep learning model architectures used in reported studies?</p>
<p>The use of deep learning can be divided into three categories. First, the models trained on the large database were directly used to extract embedding, and then machine learning classifiers were used. Second, the models were pre-trained on a large database and then fine-tuned on dementia-related databases. In some situations, Self-training and data augmentation methods were used in the pre-trained process. Thirdly, deep learning models were built and trained from scratch using dementiarelated databases.</p>
<p>What classification performance has been achieved?</p>
<p>The performance advantages of deep learning compared to the traditional method Balagopalan, A. et al. tested on the ADReSS dataset using different classification models, including SVM, NB, RF, FNN, and BERT. According to the results presented in the paper, when using the FNN method, it can achieve an average accuracy of 77.08% on the ADReSS test set in 3 runs, which is higher than the performance of RF and NB but lower than the average accuracy of 81.25% for the SVM classifier. However, when using BERT, it got the best result for classification with an accuracy of 83.32% [54]. Not only linguistic features, but deep learning has also achieved better results on acoustic features. Bertini, F. et al. used an autoencoder to extract unsupervised features from audio data and then utilized FNN to achieve 93.3% classification accuracy on the Pitt dataset, which is better than the results obtained by traditional machine learning methods such as SVM, NB, and RF [33].</p>
<p>In the detection process of AD, utilizing deep learning methods can effectively improve the performance of the classification models when compared with traditional machine learning methods.</p>
<p>Besides, we compared methods without pre-training and methods with pre-trainig by box plotting in  SS-PD-CT2 task with a test set for evaluation in Fig. 4. It exhibits that using the pre-training method is more useful than training models from scratch.</p>
<p>Performance difference based on different tasks</p>
<p>On the task selection, SS works better than others tasks generally. In 2017 and 2018, Lopez-De-Ipina, K. et al. conducted research on AD detection based on VF and SS tasks, in which acoustic features were mainly used. The detection accuracies on SS tasks were higher than the result on the VF task [72].</p>
<p>SS tasks can be divided into several different subtasks, including PD, Conversation/interview, and Recall.</p>
<p>In PD tasks, most tasks were based on ADReSS or Pitt database. There were 21 studies that used the ADReSS database and that 11 studies used the Pitt database. The test set on ADReSS database was uniform, detection accuracy in more than 75% of studies can reach more than 80%, and the best result can reach 91.67%. Crossvalidation predictions from 85% of studies on the Pitt database exceeded 80% accuracy, and the best result can reach 91.25%. Ten reported studies contain conversation tasks [14,16,26,27,39,42,56,64,75,76].</p>
<p>Though different databases were used, high accuracy can be achieved by cross-validation evaluation, in which 85% of studies exceeded 85% accuracy and the best result can reach 95%.</p>
<p>In Recall tasks, four related studies are included, and all can achieve 80% accuracy.</p>
<p>Comparisons of methods for the ADReSS Challenge</p>
<p>The ADReSS Challenge is the most recent internationally representative speech-based AD detection competition, which was held in Interspeech 2020-2021. The main objective of the ADReSS challenge is to make available a benchmark dataset of spontaneous speech, which is acoustically pre-processed and balanced in terms of age and gender, defining a shared task through which different approaches to AD recognition in spontaneous speech can be compared. Pre-training methods are mainly used in the top five participating teams of the ADReSS challenge, which include two types of useful ways of deep learning techniques.</p>
<p>The first way is pre-training based on deep learning architecture and large datasets, and then fine-tuning on the ADReSS dataset. Saltz, P. et al. [44]; Yuan, J. et al. [55]; and Zhu, Y. et al. [53] used BERT, ERNIE, Longformerbased model architecture to pre-train and then fine-tune, which reached 90%, 89.6%, and 89.58% on ADReSS test set respectively. In terms of characteristics, Saltz, P. et al. The second way is extracting features based on deep learning architecture, and then training traditional machine learning classifiers based on the extracted features. Syed, Z. S. et al [51] combined traditional linguistic features and linguistic embedding extracted from a pretrained BERT-based model, and then trained through ensemble learning and fused based on majority-voting, eventually reaching 91.67% accuracy on the ADReSS test set. Haulcy, R. et al. [50] extracted linguistic embedding from BERT with SVM or RF classifier and achieved 85.4% accuracy.</p>
<p>In addition, some other text-based pre-trained models work well. For example, the accuracies of BERT, part of BERT or BERT-based adaptation models [46,47,54,65] were between 81% and 84.51%. Except for the text-based pre-trained models, audio and image-based pre-trained models also have been explored in speechbased AD detection. Chlasta, K. et al [48] trained modified VGGNet architecture to extract acoustic embedding, while Gauder, L. et al. [49] trained wav2vec 2.0 framework to extract acoustic embedding vector, of which both added modified CNN modules for classification, reaching 62.5% and 78.9% accuracy, respectively.</p>
<p>Another training method in the ADReSS Challenge is training from scratch. Traditional linguistic and acoustic features have been applied with the architectures such as FNN [34,60], attention mechanismbased LSTM [86] and CNN-LSTM [36] model reached 83.33%, 64.58%, and 74.55% accuracy, respectively. After the duration features were added, BiLSTM with highway layers, CNN-BiLSTM-attention-based architecture [35], and dense layer with GRU model [37] reached 84%, 84%, and 72.92% accuracy, respectively.</p>
<p>When using limited clinical data, choosing proper pre-trained task and fine-tuned models are important and effective for disease classification. Generally, CNN-based architectures extract local information, and the LSTM or BERT-based model extracts temporal information. Specifically, pre-training a speech or text encoder with a large speech or text corpus, and using the attention mechanism to map the correspondence, then a fine-tuning model with AD or MCI dataset is a general method to build a framework to train the AD classification from scratch.</p>
<p>The algorithms and performances for detecting MCI</p>
<p>As an intermediate transition state between the normal aging process and mild AD, MCI plays an important role in early screening or AD. Among the screened papers, 16 of them performed MCI detection experiments. 11 of the 16 papers were about distinguishing MCI and healthy people, while the rest were about three classifications of AD, MCI patients, and cognitive normal elders.</p>
<p>For the classification of MCI versus cognitive normal subjects, Lindsay, Hali et al. [38] utilized three different pre-trained models (FastTest, Spacy, Wiki2Vec) to extract word embeddings, then used a SVM classifier to predict labels in different languages (French, German, Dutch), and can achieve 66%, 68%, and 69% AUC, respectively. For three-classification experiments for AD, MCI, and HC, Rodrigues Makiuch, M. et al. [39] using a gated convolutional neural network (GCNN), achieving an accuracy of 60.6% in 40 s of speech data.</p>
<p>MCI manifests as mild cognitive decline. Compared with AD, most MCI patients have less severe memory loss and perform relatively normal on memory tests. As can be seen from the papers we screened, it is more difficult to detect MCI patients than to distinguish AD patients from cognitive normal elders-based speech analysis. And we can find that there are not many studies on MCI detection at present, so it is of great value to further explore the methods of detecting MCI with deep learning techniques.</p>
<p>What were the mainstreams and limitations of reported studies?</p>
<p>The mainstreams and limitations of these selected studies were mainly reflected in language tasks, data modalities, extracted features, and model performance.</p>
<p>Language tasks</p>
<p>Varied databases were built to collect speech from AD and healthy people based on varied tasks. Through the databases we introduced in section 4.2 of this article, we can find that the current mainstream language tasks focus on: Semantic verbal fluency tasks, Spontaneous speech tasks, and some other reading tasks.</p>
<p>Semantic verbal fluency tasks contain animal naming tasks, vegetable, and location naming tasks. As for tasks collecting spontaneous speech, it compromised speech from interviews or conversations speech, recall tasks, and picture description tasks.</p>
<p>From this, we can find that there are many kinds of language tasks, which makes it difficult for researchers to compare their research results.</p>
<p>Therefore, based on the picture description task, the Pitt corpus and the ADReSS database have constructed comparable distribution-balanced databases, and researchers have begun to focus on these two databases for AD classification tests.</p>
<p>However, the languages of Pitt corpus and ADReSS databases are both English, and the amount of data is small, so the current research is also limited to a certain extent.</p>
<p>Data modalities</p>
<p>Based on our table in the "Deep learning techniques" section, we can see that researchers used speech, text, or speech and text to conduct experiments, in which some compared the classification results on the same evaluation test set.</p>
<p>The current research trend is to obtain more characteristic information by combining multimodal data. Different modalities have different representations, so there is some overlap and complementarity of information, as well as a variety of information interactions. Researchers may no longer be limited to the speech and text information of AD patients. Improving the accuracy of the overall decision-making results by integrating multi-modal data such as eye movement data, writing data, and gait performance is also an interesting topic that needs further investigation.</p>
<p>Extracted features</p>
<p>Traditional linguistic and acoustic features were mostly from handcrafted definitions thus these features were explainable. Deep learning-based feature extraction or classification techniques achieved high accuracy for AD classification but short of the lack of interpretability.</p>
<p>Deep learning-based feature extraction methods need a large scale of data, which is hard to precisely define and varies on a different scale of data. Besides, tasks were chosen to pre-train the model for features extraction, for example, ASR or BERT, were not fully compared and analyzed for AD classification tasks.</p>
<p>Model performance</p>
<p>How were these deep learning model architectures used in reported studies? and What classification performance has been achieved? In this paper, the deep learning model architectures and training strategies adopted by the selected papers are presented. In the current study, the researchers use the pre-training model to solve the problem of insufficient training data in AD detection and achieve good results. Most speech-based AD detection using deep learning methods can achieve an accuracy of about 85%. In the ADReSS challenge, some researchers have achieved an accuracy of nearly 90% using pretrained models. However, traditional cognitive impairment screening scales, such as MMSE or MOCA, can usually achieve a screening accuracy of more than 93% [5]. Therefore, as a more convenient AD detection method, speech-based deep learning technology needs to be further improved.</p>
<p>Discussions</p>
<p>Limitation of our studies</p>
<p>In this review, the following limitations may down the outcome confidence level of our paper:</p>
<p>(1) In the process of paper search, our search keywords are missing "pre-trained model, " which leave out some papers that refer to "pre-trained model" but do not mention "deep learning" or "neural network". Although we add some papers from other sources, this problem increases the risk of bias of the paper search results. (2) Because of our selection criteria, only papers written in English were selected, which resulted in some non-English databases and studies not being included in this review, thus increasing the language bias and affecting some languagerelated features. (3) Due to the overlap of deep learning methods in many papers, for example, the classifier proposed by Liu, Z. et al. is a combination of CNN, BiLSTM, and attention, so it is difficult to separate it into a specific deep learning category [40]. The lack of a very clear standard in the process of classifying deep learning methods also increases the error of statistical analysis to a certain extent. (4) In the process of analyzing the performance of deep learning models, there may be some potential risks of bias. Because we were only focused on the best performance of the model in the paper, different databases, different testing methods, and different evaluation indicators may possibly lead to a skewed understanding that how well the algorithms worked.</p>
<p>Research directions</p>
<p>The purpose of this review paper is to investigate current researchers' application of deep learning methods for speech-based AD detection and to explore future possibilities. The current dementia-related databases are usually small, with a single language, uneven distribution, and inconsistent tasks. However, fusing the multi-modal data rather than using only one modality can extract more useful information for the classification of AD patients, and the application of pre-trained models can also greatly improve the classification accuracy. Another point to note is that the databases in the papers we screened lack cohort study data, so it is difficult to prove the reliability of the results of speech analysis on intra-individual repeated testing. Besides, currently, speech-based AD detection has not been widely applied clinically. So our future goals are as follows:</p>
<p>(1) To establish and publish a balance-distributed Chinese AD database, including the speech data of the picture-distribution task and the writing data of the clock-drawing test.</p>
<p>At the same time, we hope researches can collect cohort data to study the tracking performance of speech analysis in individual patients over time.</p>
<p>(2) To explore the potential of new deep learning models to improve classification accuracy by utilizing speech, writing, and other multi-modal data.</p>
<p>Improving the interpretability of feature representations that have been extracted by deep learning methods in the assessment of cognitive impairment.</p>
<p>(3) To establish efficient and accurate computer-aided diagnosis methods, which can shorten the time of large-scale AD screening. The study on AD detection also promotes the development of portable diagnostic devices, which could timely detect AD and timely intervene to delay the disease. (4) In addition to Alzheimer's disease, there are other causes of dementia, so we hope that future researchers can use speech analysis to detect other types of dementia.</p>
<p>Fig. 1
1Search strategies in ACM, DBLP, IEEE, PubMed, Scopus, and Web of Science</p>
<p>Fig. 2
2Flow diagram of the search and selection process in our study</p>
<p>Fig. 4
4Comparison of deep learning methods without and with pre-training</p>
<p>Fig. 3
3Paper numbers of FNN, CNN, LSTM, and attention mechanism-based models</p>
<p>and Yuan, J. et al. used linguistic embedding only, and Zhu, Y. et al. used acoustic and linguistic embedding. Besides, Saltz, P. et al. used augmented data during the training stage, Yuan, J. et al. encoded the pause into the transcript and then acquired embedding vector for classification, and Zhu, Y. et al. used Longformer-based transfer learning.</p>
<p>Table 1
1Dementia-related speech-based databases informationThe full names of abbreviations can be found in "Abbreviations"Task name </p>
<p>Database name </p>
<p>Abbreviation </p>
<p>Language </p>
<p>Label distribution </p>
<p>Speech included </p>
<p>Transcription </p>
<p>included </p>
<p>VF </p>
<p>SVF </p>
<p>Animal naming </p>
<p>PGA-OREKA [20] </p>
<p>VF1 </p>
<p>Spain </p>
<p>HC (62)/MCI (38) </p>
<p>Yes </p>
<p>No </p>
<p>-[21] </p>
<p>VF2 </p>
<p>French, Dutch, and German </p>
<p>HC (66)/MCI (66) </p>
<p>Yes </p>
<p>-</p>
<p>Vegetable naming </p>
<p>Mandarin_Lu (DementiaBank) </p>
<ul>
<li>NTU dataset [22] </li>
</ul>
<p>VF3 </p>
<p>Chinese </p>
<p>AD (30)/HC (30) </p>
<p>Yes </p>
<p>No </p>
<p>Location naming </p>
<p>Mandarin_Lu (DementiaBank) </p>
<ul>
<li>NTU dataset [22] </li>
</ul>
<p>VF4 </p>
<p>Chinese </p>
<p>AD (30)/HC (30) </p>
<p>Yes </p>
<p>No </p>
<p>SS </p>
<p>Conversation/interview </p>
<p>PROMPT database [23] </p>
<p>cvs1 </p>
<p>Japanese </p>
<p>Dementia (49)/MCI(42)/ 
HC(72) </p>
<p>Yes </p>
<p>-</p>
<p>-[24] </p>
<p>cvs2 </p>
<p>Italian </p>
<p>eD (16)/MCI (32)/HC (48) </p>
<p>Yes </p>
<p>Yes </p>
<p>The Carolina Corpus Conver-
sation database [14] </p>
<p>cvs3 </p>
<p>English </p>
<p>AD (30)/HC (16) </p>
<p>Yes </p>
<p>Yes </p>
<p>IVA dataset [25] </p>
<p>cvs4 </p>
<p>English </p>
<p>ND (21)/MCI (24)/HC (25) </p>
<p>Yes </p>
<p>-</p>
<p>The Hungarian MCI-mAD 
Database [26, 27] </p>
<p>cvs5 </p>
<p>Hungarian </p>
<p>MCI (48)/HC (36) 
AD (25)/MCI (25)/HC (25) </p>
<p>Yes </p>
<p>No </p>
<p>AZTIAHO 
AZTIAHORE [28] </p>
<p>cvs6 </p>
<p>multilingual </p>
<p>AD (20)/HC (50) 
AD (20)/MCI (20) </p>
<p>Yes </p>
<p>No </p>
<p>-[29] </p>
<p>cvs7 </p>
<p>Italian </p>
<p>MCI (19)/HC (20) </p>
<p>Yes </p>
<p>Yes </p>
<p>-[16] </p>
<p>cvs8 </p>
<p>Hungarian </p>
<p>MCI (32)/HC (19) </p>
<p>Yes </p>
<p>No </p>
<p>Framingham Heart Study </p>
<p>Dataset [30] </p>
<p>cvs9 </p>
<p>English </p>
<p>Dementia (223)/MCI (309)/ </p>
<p>HC (291) </p>
<p>Yes </p>
<p>No </p>
<p>Recall </p>
<p>NTUHV dataset [31] </p>
<p>Table 2
2Deep learning techniques in all included papersReferences 
Year Task 
Sample 
Feature type 
Classifier 
Pre-train Evaluation Metrics 
Best 
Performance </p>
<p>Bertini, F. et al. 
[33] </p>
<p>2022 SS-PD-CT1 
AD (137)/HC 
(43) </p>
<p>AE 
auDeep 
Yes 
CV 
Accuracy 93.30% </p>
<p>Meghanani, A. 
et al. [34] </p>
<p>2021 SS-PD-CT2 
AD (54)/non-AD 
(54) </p>
<p>TLF 
FNN 
No 
Test 
Accuracy 83.33% </p>
<p>Rohanian, M. 
et al. [35] </p>
<p>2021 SS-PD-CT3 
AD (122)/HC 
(115) </p>
<p>TAF/TLF/DF 
biLSTM 
No 
Test 
Accuracy 84% </p>
<p>Shah Syed, M.S. 
et al. [36] </p>
<p>2021 SS-PD-CT2 
AD (72)/non-AD 
(72) </p>
<p>TAF 
LSTM a 
No 
Test 
Accuracy 74.55% </p>
<p>Mahajan, P. et al. 
[37] </p>
<p>2021 SS-PD-CT2 
AD (82)/non-AD 
(82) </p>
<p>TAF/DF/TLF/ 
DeF </p>
<p>CNN+biLSTM a No 
Test 
Accuracy 72.92% </p>
<p>Meghanani, A 
[34]. </p>
<p>2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>TAF 
CNN+LSTM 
No 
Test 
Accuracy 64.58% </p>
<p>Lindsay, Hali 
et al. [38] </p>
<p>2021 VF4 
HC (66)/MCI 
(66) </p>
<p>LE 
SVM 
Yes 
CV 
AUC </p>
<p>Rodrigues 
Makiuchi, M. 
et al. [39] </p>
<p>2021 SS-PD-CT1SS-
CVS1 </p>
<p>CT1: AD (168)/ 
HC (98) CVS1: 
AD (49)/MCI 
(42)/HC (72) </p>
<p>TAF 
GCNN 
No 
CV 
Accuracy </p>
<p>Liu, Z. et al. [40] 2021 SS-PD-CT1 
AD (252)/HC 
(232) </p>
<p>AE 
CNN+biLSTM a Yes 
CV 
Accuracy 82.59% </p>
<p>Wang, N. et al. 
[41] </p>
<p>2021 SS-PD-CT3 
AD (87)/HC (79) TLF/LE 
C-Attention-
Unified model </p>
<p>Yes 
Test 
Accuracy 80.28% </p>
<p>Bertini, F. et al. 
[42] </p>
<p>2021 SS-CVS2SS-PD2 eD (16)/MCI 
(32)/HC (48) </p>
<p>TAF 
FNN 
Yes 
CV 
Accuracy 90.57% </p>
<p>Roshanzamir, A. 
et al. [43] </p>
<p>2021 SS-PD-CT1 
AD (170)/HC 
(99) </p>
<p>LE 
LR 
Yes 
CV 
Accuracy 88.08% </p>
<p>Saltz, P. et al. [44] 2021 SS-PD-CT2SS-
PD-CT1 </p>
<p>CT2: AD (78)/ 
non-AD (78) </p>
<p>LE 
BERTXLNet 
Yes 
CV 
Accuracy </p>
<p>Liu, Z. et al. [45] 2021 SS-PD-CT2 
AD (87)/non-AD 
(79) </p>
<p>TLF/TAF/LE 
BERT 
Yes 
CV 
Accuracy 97.18% </p>
<p>Guo, Y. et al. [46] 2021 SS-PD-CT2SS-
PD-CT4 </p>
<p>CT2: AD (78)/ 
non-AD (78) 
CT4: AD (115)/ 
HC(839) </p>
<p>LE 
BERT 
Yes 
Test 
Accuracy 82.10% </p>
<p>Pan, Y. et al. [47] 2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>LE 
BERT large 
Yes 
Test 
Accuracy 84.51% </p>
<p>Chlasta, K. et al. 
[48] </p>
<p>2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>AE 
DemCNN 
Yes 
Test 
Accuracy 62.50% </p>
<p>Gauder, L. et al. 
[49] </p>
<p>2021 SS-PD-CT2 
AD (87)/non-AD 
(79) </p>
<p>AE 
CNN 
Yes 
Test 
Accuracy 78.90% </p>
<p>Haulcy, R. et al. 
[50] </p>
<p>2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>LE 
SVM, RF 
Yes 
Test 
Accuracy 85.40% </p>
<p>Syed, Z.S. et al. 
[51] </p>
<p>2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>TLF/LE 
SVM, LR 
Yes 
Test 
Accuracy 91.67% </p>
<p>Tsai, A.C. Y. et al. 
[52] </p>
<p>2021 SS-Recall1 &amp; 
SS-PD-CT6SS-
PD-CT1 </p>
<p>SS-Recall1 &amp; 
SS-PD-CT6 : AD 
(40)/HC (40)CT1: 
AD (257)/HC 
(242) </p>
<p>LE 
BERT 
Yes 
Test 
Accuracy </p>
<p>Zhu, Y. et al. [53] 2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>AE/LE 
Longformer 
Yes 
Test 
Accuracy 89.58% </p>
<p>Aparna Balago-
palan et al. [54] </p>
<p>2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>LE 
BERT 
Yes 
Test 
Accuracy 83.32% </p>
<p>Yuan, J. et al. 
[55] </p>
<p>2021 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>LE 
ERNIE-large 
Yes 
Test 
Accuracy 89.60% </p>
<p>Table 2 (continued)
2References 
Year Task 
Sample 
Feature type 
Classifier 
Pre-train Evaluation Metrics 
Best 
Performance </p>
<p>Xue, C. et al. [56] 2021 SS-CVS9 
dementia (330)/ 
MCI (451)/HC 
(483) </p>
<p>TAF 
LSTM 
No 
CV 
Accuracy 67.50% </p>
<p>Roozbeh, S. 
et al. [57] </p>
<p>2021 SS-PD-CT5 
AD (26)/46 (HC) TAF/TLF 
FNN 
No 
CV 
Accuracy 93.05% </p>
<p>Koo, J. et al. [58] 2020 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>TAF/TLF/AE 
CNN+biLSTM a Yes 
Test 
Accuracy 81.25% </p>
<p>Cummins, N. 
et al. [59] </p>
<p>2020 SS-PD-CT2 
AD (54)/non-AD 
(54) </p>
<p>TAF/LE 
biLSTM a 
Yes 
Test 
Accuracy 85.20% </p>
<p>Sarawgi, U. et al. 
[60] </p>
<p>2020 SS-PD-CT1 
SS-PD-CT2 </p>
<p>CT1: AD (168)/ 
HC (99)CT2: AD 
(78)/non-AD 
(78) </p>
<p>TLF/TAF 
FNN 
No 
CV 
Test </p>
<p>Accuracy 
Accuracy </p>
<p>La Fuente 
Garcia, S. D. et al. 
[61] </p>
<p>2020 SS-PD-CT1SS-
CVS3 </p>
<p>CT1: AD (82)/HC 
(82) CVS3: AD 
(30)/HC (16) </p>
<p>TAF 
FNN 
No 
Test 
UAR </p>
<p>Lopez-De-Ipina, 
K. et al. [62] </p>
<p>2020 VF1 
MCI (38)/HC 
(62) </p>
<p>TAF 
CNN 
No 
CV 
Accuracy 92% </p>
<p>Casanova, E. 
et al. [63] </p>
<p>2020 SS-Recall2SS-
Recall3SS-
PD4SS-PD5 </p>
<p>AD (41)/MCI 
(55)/HC (194) </p>
<p>TLF 
RNN+CRF a 
Yes 
CV 
F1-score 81.00% </p>
<p>Pan, Y. et al. [64] 2020 SS-CVS4 
ND (21)/MCI 
(24)/HC (25) </p>
<p>AE 
LRSVM 
Yes 
CV 
F1-score </p>
<p>Searle, T. et al. 
[65] </p>
<p>2020 SS-PD-CT2 
AD (78)/non-AD 
(78) </p>
<p>LE 
DistilBERT 
Yes 
Test 
Accuracy 81% </p>
<p>Li, Y [66]. 
2020 SS-PD-CT1 
AD (155)/HC 
(145) </p>
<p>DeF/LE/TLF/TLF LR 
Yes 
CV 
Accuracy 91.25% </p>
<p>Rosas, D.S. et al. 
[14] </p>
<p>2019 SS-CVS3 
Dementia (62)/ 
HC (160) </p>
<p>TLF 
FNN 
No 
Test 
Accuracy 86.42% </p>
<p>Chien, Y.W. et al. 
[67] </p>
<p>2019 SS-Recall5 
AD (30)/HC (30) TAF 
biLSTM 
Yes 
Test 
AUC 
83.80% </p>
<p>Fritsch, J. et al. 
[68] </p>
<p>2019 SS-PD-CT1 
AD (168)/HC 
(98) </p>
<p>TLF 
LSTM 
No 
CV 
Accuracy 85.60% </p>
<p>Hong, S.Y. et al. 
[69] </p>
<p>2019 SS-PD-CT1 
AD (169)/HC 
(99) </p>
<p>LE 
RNN a 
Yes 
CV 
Accuracy 83.50% </p>
<p>Gabor, G. et al. 
[27] </p>
<p>2019 SS-CVS5SS-
Recall4 </p>
<p>mAD (25)/MCI 
(25)/HC (25) </p>
<p>TLF/DF/DeF 
SVM 
Yes 
CV 
Accuracy 86.00% </p>
<p>Themistocleous, 
C. et al. [70] </p>
<p>2018 Reading 
HC (30)/MCI 
(25) </p>
<p>TAF/DeF 
FNN 
No 
CV 
Accuracy 83% </p>
<p>Klumpp, P. et al. 
[71] </p>
<p>2018 SS-PD-CT1 
AD (168)/HC 
(98) </p>
<p>LE 
FNN 
No 
Test 
Accuracy 84.40% </p>
<p>Lopez-De-Ipina, 
K. et al. [72] </p>
<p>2018 VF1SS-CVS6SS-
PD3 </p>
<p>VF1: MCI (38)/ 
HC (62)CVS6: 
AD (20)/HC (20) 
PD3: AD (6)/HC 
(12) </p>
<p>TAF 
CNN 
No 
CV 
Accuracy </p>
<p>Orimaye, S. O. 
et al. [73] </p>
<p>2018 SS-PD-CT1 
AD task: AD 
(99)/HC (99) 
MCI task: MCI 
(19)/HC (19) </p>
<p>TLF 
D2NNLM-5n 
No 
Test 
AUC </p>
<p>Warnita, T. et al. 
[74] </p>
<p>2018 SS-PD-CT1 
AD (169)/HC 
(98) </p>
<p>TAF 
GCNN 
No 
CV 
Accuracy 73.60% </p>
<p>Chien, Y. W. et al. 
[23] </p>
<p>2018 VF2VF3 
AD (30)/HC (30) TAF 
biLSTM 
Yes 
Test 
AUC 
95.40% </p>
<p>Lopez-de-Ipina, 
K. et al. [12] </p>
<p>2017 VF1SS-CVS6SS-
PD3 </p>
<p>MCI (40)/HC 
(60) </p>
<p>TAF 
CNN 
No 
CV 
Accuracy </p>
<p>Table 2 (continued)
2References 
Year Task 
Sample 
Feature type 
Classifier 
Pre-train Evaluation Metrics 
Best 
Performance </p>
<p>Lopez-de-Ipina, 
K. et al. [21] </p>
<p>2017 VF1 
MCI (38)/HC 
(62) </p>
<p>TAF 
CNN 
No 
CV 
Accuracy 75% </p>
<p>D Beltrami et al. 
[75] </p>
<p>2016 SS-CVS7 
SS-PD1 </p>
<p>MCI (19)/HC 
(20) </p>
<p>TLF/TAF 
FNN 
No 
CV 
CV </p>
<p>F1-score 
F1-score </p>
<p>Laszlo, T. et al. 
[76] </p>
<p>2016 SS-CVS5SS-
Recall4 </p>
<p>MCI (48)/HC 
(36) </p>
<p>DF/DeF 
SVM 
Yes 
CV 
Accuracy 88.10% </p>
<p>Laszlo, T. et al. 
[16] </p>
<p>2015 SS-CVS8 
MCI (32)/HC 
(19) </p>
<p>TAF/DF 
SVM 
Yes 
CV 
Accuracy 80.40% </p>
<p>Lopez-de-Ipina, 
K. et al. [26] </p>
<p>2013 SS-CVS6 
AD (20)/HC (20) TAF/DF 
FNN 
No 
CV 
Accuracy 94.60% </p>
<p>a in Classifier means attention-based method. The full names of abbreviations can be found in "Abbreviations" </p>
<p>AcknowledgementsNot applicableAuthors' contributionsThe manuscript was designed by Qin Yang and Xinyun Ding. Qin Yang conducted a literature review and summary. Qin Yang and Xinyun Ding drafted the manuscript. Feiyang Xu, Xin Li, and Zhenhua Ling edited and revised the manuscript. All authors read and approved the final manuscript.
Alzheimer's Association 2021 Facts and Figures Report. Monica Moore, Msg Díaz-Santos, M Vossel, K , Monica Moore MSG, Díaz-Santos M, Vossel K. Alzheimer's Association 2021 Facts and Figures Report[J].</p>
<p>Brain health: The importance of recognizing cognitive impairment: An iagg consensus conference. J E Morley, J C Morris, M Berg-Weger, S Borson, B D Carpenter, Del Campo, N , J Am Med Dir Assoc. 16ElsevierMorley JE, Morris JC, Berg-Weger M, Borson S, Carpenter BD, Del Campo N, et al. Brain health: The importance of recognizing cognitive impairment: An iagg consensus conference. J Am Med Dir Assoc. 2015;16:731-9 Elsevier.</p>
<p>The diagnosis of dementia due to alzheimer's disease: Recommendations from the national institute on aging-alzheimer's association workgroups on diagnostic guidelines for alzheimer's disease. G M Mckhann, D S Knopman, H Chertkow, B T Hyman, C R JackJr, C H Kawas, Alzheimers Dement. 7ElsevierMcKhann GM, Knopman DS, Chertkow H, Hyman BT, Jack CR Jr, Kawas CH, et al. The diagnosis of dementia due to alzheimer's disease: Recom- mendations from the national institute on aging-alzheimer's association workgroups on diagnostic guidelines for alzheimer's disease. Alzheimers Dement. 2011;7:263-9 Elsevier.</p>
<p>Linguistic features identify alzheimer's disease in narrative speech. K C Fraser, J A Meltzer, F Rudzicz, J Alzheimers Dis. 49IOS PressFraser KC, Meltzer JA, Rudzicz F. Linguistic features identify alzheimer's disease in narrative speech. J Alzheimers Dis. 2016;49:407-22 IOS Press.</p>
<p>Speech-based automatic and robust detection of very early dementia. Fifteenth annual conference of the international speech communication association. A Satt, R Hoory, A König, P Aalten, P H Robert, Satt A, Hoory R, König A, Aalten P, Robert PH. Speech-based automatic and robust detection of very early dementia. Fifteenth annual conference of the international speech communication association. 2014.</p>
<p>Temporal parameters of spontaneous speech in alzheimer's disease. I Hoffmann, D Nemeth, C D Dye, M Pákáski, T Irinyi, J Kálmán, Int J Speech Lang Pathol. 12Hoffmann I, Nemeth D, Dye CD, Pákáski M, Irinyi T, Kálmán J. Temporal parameters of spontaneous speech in alzheimer's disease. Int J Speech Lang Pathol. 2010;12:29-34 Taylor &amp; Francis.</p>
<p>Comparison between oral and written spelling in alzheimer's disease. B Croisile, M-J Brabant, T Carmoi, Y Lepage, G Aimard, M Trillet, Brain Lang. 54ElsevierCroisile B, Brabant M-J, Carmoi T, Lepage Y, Aimard G, Trillet M. Compari- son between oral and written spelling in alzheimer's disease. Brain Lang. 1996;54:361-87 Elsevier.</p>
<p>Comparative study of oral and written picture description in patients with alzheimer's disease. B Croisile, B Ska, M-J Brabant, A Duchene, Y Lepage, G Aimard, Brain Lang. 53ElsevierCroisile B, Ska B, Brabant M-J, Duchene A, Lepage Y, Aimard G, et al. Com- parative study of oral and written picture description in patients with alzheimer's disease. Brain Lang. 1996;53:1-19 Elsevier.</p>
<p>Linguistic changes in verbal expression: A preclinical marker of alzheimer's disease. F Cuetos, J C Arango-Lasprilla, C Uribe, C Valencia, F Lopera, J Int Neuropsychol Soc. 13Cambridge University PressCuetos F, Arango-Lasprilla JC, Uribe C, Valencia C, Lopera F. Linguistic changes in verbal expression: A preclinical marker of alzheimer's disease. J Int Neuropsychol Soc. 2007;13:433-9 Cambridge University Press.</p>
<p>Voice pathology detection and discrimination based on modulation spectral features. M Markaki, Y Stylianou, IEEE Trans Audio Speech Lang Process. 19Markaki M, Stylianou Y. Voice pathology detection and discrimination based on modulation spectral features. IEEE Trans Audio Speech Lang Process. 2011;19:1938-48.</p>
<p>Selecting and Analyzing Speech Features for the Screening of Mild Cognitive. Q Yang, F Xu, Z Ling, /2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC). IEEEYang Q, Xu F, Ling Z, et al. Selecting and Analyzing Speech Features for the Screening of Mild Cognitive Impairment[C]//2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC). IEEE, 2021:1906-1910.</p>
<p>ALZUMERIC: A decision support system for diagnosis and monitoring of cognitive impairment. U M De Lizarduy, P C Salomón, P G Vilda, de Lizarduy UM, Salomón PC, Vilda PG, et al. ALZUMERIC: A decision support system for diagnosis and monitoring of cognitive impairment[J].</p>
<p>. Loquens, 4Loquens. 2017;4(1):e037-e037.</p>
<p>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. G Hinton, L Deng, D Yu, G E Dahl, A-R Mohamed, N Jaitly, IEEE Signal Process Mag. 29Hinton G, Deng L, Yu D, Dahl GE, Mohamed A-r, Jaitly N, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process Mag. 2012;29:82-97 IEEE.</p>
<p>16th international conference on electrical engineering, computing science and automatic control, cce. D S Rosas, S T Arriaga, Maa Fernandez, Search for dementia patterns in transcribed conversations using natural language processing. 3\&amp; doi= 10. 1109% 2fICE EE. 2019. 88845 72\&amp; partn erID= 40\&amp; md5= 74406 14079 b3a79 0ea15 b823c 4265d 76Rosas DS, Arriaga ST, Fernandez MAA. Search for dementia patterns in transcribed conversations using natural language processing. 2019 16th international conference on electrical engineering, computing science and automatic control, cce. 2019. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85075 09956 3\&amp; doi= 10. 1109% 2fICE EE. 2019. 88845 72\&amp; partn erID= 40\&amp; md5= 74406 14079 b3a79 0ea15 b823c 4265d 76 https:// ieeex plore. ieee. org/ docum ent/ 88845 72/.</p>
<p>J Devlin, M-W Chang, Lee K , Toutanova K Bert, arXiv:181004805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:181004805. 2018.</p>
<p>Automatic detection of mild cognitive impairment from spontaneous speech using ASR. L Tóth, G Gosztolya, V Vincze, Tóth L, Gosztolya G, Vincze V, et al. Automatic detection of mild cognitive impairment from spontaneous speech using ASR[C]. ISCA, 2015.</p>
<p>Speech-and language-based classification of alzheimer's disease: A systematic review. I Vigo, L Coelho, S Reis, Bioengineering (Basel). 9Vigo I, Coelho L, Reis S. Speech-and language-based classification of alzheimer's disease: A systematic review. Bioengineering (Basel). 2022;9 Available from: https:// www. ncbi. nlm. nih. gov/ pubmed/ 35049 736.</p>
<p>A systematic literature review of automatic alzheimer's disease detection from speech and language. U Petti, S Baker, A Korhonen, J Am Med Inform Assoc. 27Oxford University PressPetti U, Baker S, Korhonen A. A systematic literature review of automatic alzheimer's disease detection from speech and language. J Am Med Inform Assoc. 2020;27:1784-97 Oxford University Press.</p>
<p>Alzheimer's dementia recognition through spontaneous speech: The adress challenge. S Luz, F Haider, S De La Fuente, D Fromm, B Macwhinney, arXiv:200406833. 2020arXiv preprintLuz S, Haider F, de la Fuente S, Fromm D, MacWhinney B. Alzheimer's dementia recognition through spontaneous speech: The adress chal- lenge. arXiv preprint arXiv:200406833. 2020.</p>
<p>Semantic verbal fluency test in dementia: Preliminary retrospective analysis. M Lopes, Smd Brucki, V Giampaoli, L L Mansur, Dement Neuropsychol. 3Lopes M, Brucki SMD, Giampaoli V, Mansur LL. Semantic verbal fluency test in dementia: Preliminary retrospective analysis. Dement Neuropsy- chol. 2009;3:315-20.</p>
<p>Analysis of disfluencies for automatic detection of mild cognitive impartment: A deep learning approach. 2017 international work conference on bio-inspired intelligence: Intelligent systems for biodiversity conservation. K Lopez-De-Ipina, U Martinez-De-Lizarduy, P M Calvo, B Beitia, J Garcia-Melero, M Ecay-Torres, eid=2-s2.0-85028 56343 6\&amp; doi= 10. 1109% 2fIWO BI. 2017. 79855 26\&amp; partn erID= 40\&amp; md5= 619c2 b86a4 11eac 9a4a8 49fbb 9063b a5Lopez-De-Ipina K, Martinez-De-Lizarduy U, Calvo PM, Beitia B, Garcia- Melero J, Ecay-Torres M, et al. Analysis of disfluencies for automatic detection of mild cognitive impartment: A deep learning approach. 2017 international work conference on bio-inspired intelligence: Intelligent systems for biodiversity conservation, iwobi 2017 -proceedings. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85028 56343 6\&amp; doi= 10. 1109% 2fIWO BI. 2017. 79855 26\&amp; partn erID= 40\&amp; md5= 619c2 b86a4 11eac 9a4a8 49fbb 9063b a5.</p>
<p>The animal naming test: An easy tool for the assessment of hepatic encephalopathy. F Campagna, S Montagnese, L Ridola, M Senzolo, S Schiff, De Rui, M , Hepatology. 66Campagna F, Montagnese S, Ridola L, Senzolo M, Schiff S, De Rui M, et al. The animal naming test: An easy tool for the assessment of hepatic encephalopathy. Hepatology. 2017;66:198-208.</p>
<p>An Assessment System for Alzheimer's Disease Based on Speech Using a Novel Feature Sequence Design and Recurrent Neural Network. Y W Chien, S Y Hong, W T Cheah, /2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE. Chien YW, Hong SY, Cheah WT, et al. An Assessment System for Alzhei- mer's Disease Based on Speech Using a Novel Feature Sequence Design and Recurrent Neural Network[C]//2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE. 2018:3289-3294.</p>
<p>Methods for studying discourse. B Macwhinney, D Fromm, M Forbes, A Holland, Aphasiabank, Aphasiology. 25MacWhinney B, Fromm D, Forbes M, Holland A. AphasiaBank: Methods for studying discourse. Aphasiology. 2011;25:1286-307.</p>
<p>Neurolinguistic features of spontaneous language production dissociate three forms of neurodegenerative disease: Alzheimer's, huntington's, and parkinson's. J Illes, Brain Lang. 37Illes J. Neurolinguistic features of spontaneous language production dissociate three forms of neurodegenerative disease: Alzheimer's, hun- tington's, and parkinson's. Brain Lang. 1989;37:628-42.</p>
<p>On the selection of non-invasive methods based on speech analysis oriented to automatic Alzheimer disease diagnosis. K López-De-Ipiña, J B Alonso, C M Travieso, J]. Sensors. 135López-de-Ipiña K, Alonso JB, Travieso CM, et al. On the selection of non-invasive methods based on speech analysis oriented to automatic Alzheimer disease diagnosis[J]. Sensors. 2013;13(5):6730-6745.</p>
<p>Identifying mild cognitive impairment and mild alzheimer's disease based on spontaneous speech using asr and linguistic features. G Gosztolya, V Vincze, L Tóth, M Pákáski, J Kálmán, I Hoffmann, Comput Speech Lang. 53Gosztolya G, Vincze V, Tóth L, Pákáski M, Kálmán J, Hoffmann I. Identify- ing mild cognitive impairment and mild alzheimer's disease based on spontaneous speech using asr and linguistic features. Comput Speech Lang. 2019;53:181-97.</p>
<p>The natural history of Alzheimer's disease: description of study cohort and accuracy of diagnosis. J T Becker, F Boller, O L Lopez, J Saxton, K L Mcgonigle, Archives of Neurology. 516Becker JT, Boller F, Lopez OL, Saxton J, McGonigle KL. The natural history of Alzheimer's disease: description of study cohort and accuracy of diag- nosis. Archives of Neurology. 1994;51(6):585-594.</p>
<p>The natural history of alzheimer's disease: Description of study cohort and accuracy of diagnosis. J T Becker, F Boiler, O L Lopez, J Saxton, K L Mcgonigle, Arch Neurol. 51American Medical AssociationBecker JT, Boiler F, Lopez OL, Saxton J, McGonigle KL. The natural history of alzheimer's disease: Description of study cohort and accuracy of diag- nosis. Arch Neurol. 1994;51:585-94 American Medical Association.</p>
<p>Detecting cognitive decline using speech only: The adresso challenge. S Luz, F Haider, S De La Fuente, D Fromm, B Macwhinney, arXiv:210409356arXiv preprintLuz S, Haider F, de la Fuente S, Fromm D, MacWhinney B. Detecting cog- nitive decline using speech only: The adresso challenge. arXiv preprint arXiv:210409356. 2021;</p>
<p>The boston diagnostic aphasia examination. H Goodglass, E Kaplan, Weintraub S Bdae, Lippincott Williams &amp; WilkinsPhiladelphiaGoodglass H, Kaplan E, Weintraub S. BDAE: The boston diagnostic aphasia examination. Philadelphia: Lippincott Williams &amp; Wilkins; 2001.</p>
<p>Neural systems for reading aloud: A multiparametric approach. W W Graves, R Desai, C Humphries, M S Seidenberg, J R Binder, Cereb Cortex. 20Graves WW, Desai R, Humphries C, Seidenberg MS, Binder JR. Neural systems for reading aloud: A multiparametric approach. Cereb Cortex. 2010;20:1799-815.</p>
<p>An automatic Alzheimer's disease classifier based on spontaneous spoken English. F Bertini, D Allevi, G Lutero, Computer Speech &amp; Language. 72101298Bertini F, Allevi D, Lutero G, et al. An automatic Alzheimer's disease classifier based on spontaneous spoken English[J]. Computer Speech &amp; Language. 2022;72:101298.</p>
<p>Recognition of alzheimer's dementia from the transcriptions of spontaneous speech using fastText and cnn models. A Meghanani, C S Anoop, A G Ramakrishnan, Front Comput Sci. 3Meghanani A, Anoop CS, Ramakrishnan AG. Recognition of alzheimer's dementia from the transcriptions of spontaneous speech using fastText and cnn models. Front Comput Sci. 2021;3 Available from: https:// www.</p>
<p>/ inward/ record. uri? eid=2-s2.0-85117 87967 1\&amp; doi= 10. 3389% 2ffco mp. 2021. 62455 8\&amp; partn erID= 40\&amp; md5= 8802a 1bb35 91d7a c3ae4 427d5 65ff8 26. scopus. com/ inward/ record. uri? eid=2-s2.0-85117 87967 1\&amp; doi= 10. 3389% 2ffco mp. 2021. 62455 8\&amp; partn erID= 40\&amp; md5= 8802a 1bb35 91d7a c3ae4 427d5 65ff8 26.</p>
<p>Alzheimer's dementia recognition using acoustic, lexical, disfluency and speech pause features robust to noisy inputs. M Rohanian, J Hough, M Purver, eid=2-s2.0-85117 83671 0\&amp; doi= 10. 21437%Proceedings of the annual conference of the international speech communication association, interspeech. 2fInt erspe ech. 2021-1633\&amp; partn erID= 40\&amp; md5= 3ea83 de2cd 6059a 2b07e 9673c 1fa8a d5Rohanian M, Hough J, Purver M. Alzheimer's dementia recognition using acoustic, lexical, disfluency and speech pause features robust to noisy inputs. Proceedings of the annual conference of the international speech communication association, interspeech. p. 4191-5. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85117 83671 0\&amp; doi= 10. 21437% 2fInt erspe ech. 2021-1633\&amp; partn erID= 40\&amp; md5= 3ea83 de2cd 6059a 2b07e 9673c 1fa8a d5.</p>
<p>Static vs. dynamic modelling of acoustic speech features for detection of dementia. M S Shah Syed, Z S Syed, E Pirogova, M Lech, eid=2-s2.0-85101 48561 1\&amp; doi= 10. 14569% 2fIJA CSA. 2020. 0111082\&amp; partn erID= 40\&amp; md5= 6012b 39cdd f348b ffb63 3d0ce c4a10 dc. 11Shah Syed MS, Syed ZS, Pirogova E, Lech M. Static vs. dynamic modelling of acoustic speech features for detection of dementia. Int J Adv Comput Sci Appl. 2020;11:662-7 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85101 48561 1\&amp; doi= 10. 14569% 2fIJA CSA. 2020. 01110 82\&amp; partn erID= 40\&amp; md5= 6012b 39cdd f348b ffb63 3d0ce c4a10 dc https:// thesai. org/ Downl oads/ Volum e11No 10/ Paper_ 82-Static_ vs_ Dynam ic_ Model ling_ of_ Acous tic_ Speech_ Featu res. pdf.</p>
<p>Acoustic and language based deep learning approaches for alzheimer's dementia detection from spontaneous speech. Front Aging Neurosci. P Mahajan, V Baths, 13eid=2-s2.0-85101 24117 9\&amp; doi= 10. 3389% 2ffna gi. 2021. 62360 7\&amp; partn erID= 40\&amp; md5= 5adf0 b6ee0 702b7 4fce0 d978b 39fc4 6eMahajan P, Baths V. Acoustic and language based deep learning approaches for alzheimer's dementia detection from spontaneous speech. Front Aging Neurosci. 2021;13 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85101 24117 9\&amp; doi= 10. 3389% 2ffna gi. 2021. 62360 7\&amp; partn erID= 40\&amp; md5= 5adf0 b6ee0 702b7 4fce0 d978b 39fc4 6e.</p>
<p>Multilingual Learning for Mild Cognitive Impairment Screening from a Clinical Speech. H Lindsay, P Müller, I Kröger, Proceedings of the International Conference on Recent Advances in Natural Language Processing. the International Conference on Recent Advances in Natural Language Processing2021RANLP 2021Lindsay H, Müller P, Kröger I, et al. Multilingual Learning for Mild Cognitive Impairment Screening from a Clinical Speech Task[C]//Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021). 2021:830-838.</p>
<p>Speech paralinguistic approach for detecting dementia using gated convolutional neural network. M Rodrigues Makiuchi, T Warnita, N Inoue, K Shinoda, M Yoshimura, M Kitazawa, -s2.0-85119 40451 7\&amp; doi= 10. 1587% 2fTRA NSINF. 2020E DP719 6\&amp; partn erID= 40\&amp; md5= 5724a d7f87 2c34e e3dd2 25941 34bfe 2f. 104Rodrigues Makiuchi M, Warnita T, Inoue N, Shinoda K, Yoshimura M, Kitazawa M, et al. Speech paralinguistic approach for detecting dementia using gated convolutional neural network. IEICE Trans Inf Syst. 2021;104:1930-40 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85119 40451 7\&amp; doi= 10. 1587% 2fTRA NSINF. 2020E DP719 6\&amp; partn erID= 40\&amp; md5= 5724a d7f87 2c34e e3dd2 25941 34bfe 2f https:// www. jstage. jst. go. jp/ artic le/ trans inf/ E104.D/ 11/ E104.D_ 2020E DP719 6/_ pdf/-char/ en.</p>
<p>Detecting alzheimer's disease from speech using neural networks with bottleneck features and data augmentation. ICASSP, ieee international conference on acoustics, speech and signal processing -proceedings. Z Liu, Z Guo, Z Ling, Y Li, eid=2-s2.0-85115 04535 9\&amp; doi= 10. 1109% 2fICA SSP39 728. 2021. 94135 66\&amp; partn erID= 40\&amp; md5= 2de80 3012c 68538 5d66c b9590 5705b d1Liu Z, Guo Z, Ling Z, Li Y. Detecting alzheimer's disease from speech using neural networks with bottleneck features and data augmentation. ICASSP, ieee international conference on acoustics, speech and signal process- ing -proceedings. p. 7323-7. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85115 04535 9\&amp; doi= 10. 1109% 2fICA SSP39 728. 2021. 94135 66\&amp; partn erID= 40\&amp; md5= 2de80 3012c 68538 5d66c b9590 5705b d1 https:// ieeex plore. ieee. org/ docum ent/ 94135 66/.</p>
<p>Modular multi-modal attention network for alzheimer's disease detection using patient audio and language data. N Wang, Y Cao, S Hao, Z Shao, K P Subbalakshmi, eid=2-s2.0- 85119 29415 7\&amp; doi= 10. 21437%Proceedings of the annual conference of the international speech communication association, interspeech. 2fInt erspe ech. 2021-2024\&amp; partn erID= 40\&amp; md5= 90970 07b86 25a26 1b30ff 9f4ff d91e 63Wang N, Cao Y, Hao S, Shao Z, Subbalakshmi KP. Modular multi-modal attention network for alzheimer's disease detection using patient audio and language data. Proceedings of the annual conference of the inter- national speech communication association, interspeech. p. 4196-200. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0- 85119 29415 7\&amp; doi= 10. 21437% 2fInt erspe ech. 2021-2024\&amp; partn erID= 40\&amp; md5= 90970 07b86 25a26 1b30ff 9f4ff d91e 63.</p>
<p>Automatic speech classifier for mild cognitive impairment and early dementia. F Bertini, D Allevi, G Lutero, D Montesi, L Calzà, 10.1145/3469089389Bertini F, Allevi D, Lutero G, Montesi D, Calzà L. Automatic speech classifier for mild cognitive impairment and early dementia. 2021;3:Article 8. Avail- able from: https:// doi. org/ 10. 1145/ 34690 89.</p>
<p>Transformer-based deep neural network language models for alzheimer's disease risk assessment from targeted speech. A Roshanzamir, H Aghajan, B M Soleymani, https:/bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-021-01456-3.pdfBMC Med Inform Decis Mak. 211186% 2fs12 911-021-01456-3\&amp; partn erID= 40\&amp; md5= 95521 a6801 9b47b 578ee 41d2e b335b 00Roshanzamir A, Aghajan H, Soleymani BM. Transformer-based deep neu- ral network language models for alzheimer's disease risk assessment from targeted speech. BMC Med Inform Decis Mak. 2021;21. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85102 36274 6\&amp; doi= 10. 1186% 2fs12 911-021-01456-3\&amp; partn erID= 40\&amp; md5= 95521 a6801 9b47b 578ee 41d2e b335b 00. https:// bmcme dinfo rmdec ismak. biome dcent ral. com/ track/ pdf/ 10. 1186/ s12911-021-01456-3. pdf.</p>
<p>Dementia detection using transformerbased deep learning and natural language processing models. P Saltz, S Y Lin, S C Cheng, D Si, -s2.0-85118 09947 1\&amp; doi= 10. 1109% 2fICH I52183. 2021. 00094 \&amp; partn erID= 40\&amp; md5= 151a3 04b9e 65a65 59a5f 487e6 25d36 b1. Proceedings -2021 ieee 9th international conference on healthcare informatics, ischi; 2021Saltz P, Lin SY, Cheng SC, Si D. Dementia detection using transformer- based deep learning and natural language processing models, Proceed- ings -2021 ieee 9th international conference on healthcare informatics, ischi; 2021. p. 509-10. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85118 09947 1\&amp; doi= 10. 1109% 2fICH I52183. 2021. 00094 \&amp; partn erID= 40\&amp; md5= 151a3 04b9e 65a65 59a5f 487e6 25d36 b1 https:// ieeex plore. ieee. org/ docum ent/ 95657 50/</p>
<p>Automatic diagnosis and prediction of cognitive decline associated with alzheimer's dementia through spontaneous speech. Z Liu, L Proctor, P N Collier, X Zhao, 2021 ieee international conference on signal and image processing applications (icsipa)Liu Z, Proctor L, Collier PN, Zhao X. Automatic diagnosis and prediction of cognitive decline associated with alzheimer's dementia through spontaneous speech. 2021 ieee international conference on signal and image processing applications (icsipa). p. 39-43. Available from: https:// ieeex plore. ieee. org/ docum ent/ 95767 84/.</p>
<p>Crossing the "cookie theft" corpus chasm: Applying what bert learns from outside data to the adress challenge dementia detection task. Y Guo, C Li, C Roan, S Pakhomov, T Cohen, eid=2-s2.0-85117 88291 6\&amp; doi= 10Front Comput Sci. 33389% 2ffco mp. 2021. 64251 7\&amp; partn erID= 40\&amp; md5= 50a9d 3ba79 b8178 6ad23 d1c42 abdaf eeGuo Y, Li C, Roan C, Pakhomov S, Cohen T. Crossing the "cookie theft" corpus chasm: Applying what bert learns from outside data to the adress challenge dementia detection task. Front Comput Sci. 2021;3 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85117 88291 6\&amp; doi= 10. 3389% 2ffco mp. 2021. 64251 7\&amp; partn erID= 40\&amp; md5= 50a9d 3ba79 b8178 6ad23 d1c42 abdaf ee.</p>
<p>Using the outputs of different automatic speech recognition paradigms for acoustic-and bert-based alzheimer's dementia detection through spontaneous speech. Y Pan, B Mirheidari, J M Harris, J C Thompson, M Jones, J S Snowden, Proceedings of the annual conference of the international speech communication association, interspeech. 2\&amp; doi= 10. 21437% 2fInt erspe ech. 2021-1519\&amp; partn erID= 40\&amp; md5= 9896e c0dec 5e6f6 c6c0b 2386e a8cee 9aPan Y, Mirheidari B, Harris JM, Thompson JC, Jones M, Snowden JS, et al. Using the outputs of different automatic speech recognition paradigms for acoustic-and bert-based alzheimer's dementia detection through spontaneous speech. Proceedings of the annual conference of the inter- national speech communication association, interspeech. p. 4216-20. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0- 85117 80239 2\&amp; doi= 10. 21437% 2fInt erspe ech. 2021-1519\&amp; partn erID= 40\&amp; md5= 9896e c0dec 5e6f6 c6c0b 2386e a8cee 9a.</p>
<p>Towards computer-based automated screening of dementia through spontaneous speech. K Chlasta, K Wołk, Frontiers in Psychology. 11623237Chlasta K, Wołk K. Towards computer-based automated screen- ing of dementia through spontaneous speech[J]. Frontiers in Psychology. 2021;11:623237.</p>
<p>Proceedings of the annual conference of the international speech communication association, interspeech. L Gauder, L Pepino, L Ferrer, P Riera, eid=2-s2.0-85117 82168 9\&amp; doi= 10. 21437%Alzheimer disease recognition using speech-based embeddings from pre-trained models. 2fInt erspe ech. 2021-753\&amp; partn erID= 40\&amp; md5= 935b9 81da2 2de50 b1923 9b345 c1e48 86Gauder L, Pepino L, Ferrer L, Riera P. Alzheimer disease recognition using speech-based embeddings from pre-trained models. Proceedings of the annual conference of the international speech communication associa- tion, interspeech. p. 4186-90. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85117 82168 9\&amp; doi= 10. 21437% 2fInt erspe ech. 2021-753\&amp; partn erID= 40\&amp; md5= 935b9 81da2 2de50 b1923 9b345 c1e48 86.</p>
<p>Classifying alzheimer's disease using audio and textbased representations of speech. R Haulcy, J Glass, eid=2-s2.0-85100 11718 7\&amp; doi= 10Front Psychol. 113389% 2ffps yg. 2020. 62413 7\&amp; partn erID= 40\&amp; md5= 0285a d5a59 684ba 3147c 3c5cb 543f9 b3Haulcy R, Glass J. Classifying alzheimer's disease using audio and text- based representations of speech. Front Psychol. 2021;11 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85100 11718 7\&amp; doi= 10. 3389% 2ffps yg. 2020. 62413 7\&amp; partn erID= 40\&amp; md5= 0285a d5a59 684ba 3147c 3c5cb 543f9 b3.</p>
<p>Automated recognition of Alzheimer's dementia using bag-of-deep-features and model ensembling. Z S Syed, Mss Syed, M Lech, J]. IEEE Access. 9Syed ZS, Syed MSS, Lech M, et al. Automated recognition of Alzheimer's dementia using bag-of-deep-features and model ensembling[J]. IEEE Access. 2021;9:88377-88390.</p>
<p>An efficient context-aware screening system for alzheimer's disease based on neuropsychology test. Acy Tsai, S Y Hong, L H Yao, W D Chang, L C Fu, Y L Chang, -s2.0-85115 43611 3\&amp; doi= 10. 1038% 2fs41 598-021-97642-4\&amp; partn erID= 40\&amp; md5= 74a14 8b1c8 a1b57 173f1 33b5c 64792 81. 11Tsai ACY, Hong SY, Yao LH, Chang WD, Fu LC, Chang YL. An efficient context-aware screening system for alzheimer's disease based on neu- ropsychology test. Sci Rep. 2021;11. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85115 43611 3\&amp; doi= 10. 1038% 2fs41 598-021-97642-4\&amp; partn erID= 40\&amp; md5= 74a14 8b1c8 a1b57 173f1 33b5c 64792 81. https:// www. nature. com/ artic les/ s41598-021-97642-4. pdf.</p>
<p>Exploring deep transfer learning techniques for alzheimer's dementia detection. Y Zhu, X Liang, J A Batsis, R M Roth, Front Comput Sci. 3eid=2-s2.0-85117 91955 6\&amp; doi= 10. 3389% 2ffco mp. 2021. 62468 3\&amp; partn erID= 40\&amp; md5= 096a6 07aff 4ca36 9ec26 76ac2 1c360 eeZhu Y, Liang X, Batsis JA, Roth RM. Exploring deep transfer learning techniques for alzheimer's dementia detection. Front Comput Sci. 2021;3 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0- 85117 91955 6\&amp; doi= 10. 3389% 2ffco mp. 2021. 62468 3\&amp; partn erID= 40\&amp; md5= 096a6 07aff 4ca36 9ec26 76ac2 1c360 ee.</p>
<p>Comparing pretrained and feature-based models for prediction of alzheimer's disease based on speech. A Balagopalan, B Eyre, J Robin, F Rudzicz, J Novikova, Front Aging Neurosci. 13189Balagopalan A, Eyre B, Robin J, Rudzicz F, Novikova J. Comparing pre- trained and feature-based models for prediction of alzheimer's disease based on speech. Front Aging Neurosci. 2021;13:189.</p>
<p>Pauses for detection of Alzheimer's disease. J Yuan, X Cai, Y Bian, Frontiers in Computer Science. 2624488Yuan J, Cai X, Bian Y, et al. Pauses for detection of Alzheimer's disease[J]. Frontiers in Computer Science. 2021;2:624488.</p>
<p>Detection of dementia on voice recordings using deep learning: A framingham heart study. C Xue, C Karjadi, I C Paschalidis, R Au, V B Kolachalama, https:/alzres.biomedcentral.com/track/pdf/10.1186/s13195-021-00888-3.pdf-s2.0-85114 01346 2\&amp; doi= 10. 1186% 2fs13 195-021-00888-3\&amp; partn erID= 40\&amp; md5= 8008f b2529 fc37e f399b aabde a4fe8 95. 13Xue C, Karjadi C, Paschalidis IC, Au R, Kolachalama VB. Detection of dementia on voice recordings using deep learning: A framingham heart study. Alzheimers Res Ther. 2021;13. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85114 01346 2\&amp; doi= 10. 1186% 2fs13 195-021-00888-3\&amp; partn erID= 40\&amp; md5= 8008f b2529 fc37e f399b aabde a4fe8 95. https:// alzres. biome dcent ral. com/ track/ pdf/ 10. 1186/ s13195- 021-00888-3. pdf.</p>
<p>Towards an Automatic Speech-Based Diagnostic Test for Alzheimer's Disease. R Sadeghian, J D Schaffer, S A Zahorian, Frontiers in Computer Science. 3624594Sadeghian R, Schaffer JD, Zahorian SA. Towards an Automatic Speech- Based Diagnostic Test for Alzheimer's Disease[J]. Frontiers in Computer Science. 2021;3:624594.</p>
<p>Proceedings of the annual conference of the international speech communication association, interspeech. J Koo, J H Lee, J Pyo, Y Jo, K Lee, eid=2-s2.0-85098 22318 5\&amp; doi= 10. 21437%Exploiting multi-modal features from pre-trained networks for alzheimer's dementia recognition. 2fInt erspe ech. 2020-3153\&amp; partn erID= 40\&amp; md5= 9749b 41613 24888 5ab47 dd0b6 eb490 19Koo J, Lee JH, Pyo J, Jo Y, Lee K. Exploiting multi-modal features from pre-trained networks for alzheimer's dementia recognition. Proceedings of the annual conference of the international speech communication association, interspeech. p. 2217-21. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85098 22318 5\&amp; doi= 10. 21437% 2fInt erspe ech. 2020-3153\&amp; partn erID= 40\&amp; md5= 9749b 41613 24888 5ab47 dd0b6 eb490 19.</p>
<p>Proceedings of the annual conference of the international speech communication association, interspeech. N Cummins, Y Pan, Z Ren, J Fritsch, V S Nallanthighal, H Christensen, eid=2-s2.0-85098 10424 5\&amp; doi= 10. 21437%56260A comparison of acoustic and linguistics methodologies for alzheimer's dementia recognition. 2fInt erspe ech. 2020-2635\&amp; partn erID= 40\&amp; md5= 279b5 36317Cummins N, Pan Y, Ren Z, Fritsch J, Nallanthighal VS, Christensen H, et al. A comparison of acoustic and linguistics methodologies for alzhei- mer's dementia recognition. Proceedings of the annual conference of the international speech communication association, interspeech. p. 2182-6. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85098 10424 5\&amp; doi= 10. 21437% 2fInt erspe ech. 2020-2635\&amp; partn erID= 40\&amp; md5= 279b5 36317 56260 e1735 05b17 c8f7b 3c.</p>
<p>Proceedings of the annual conference of the international speech communication association, interspeech. U Sarawgi, W Zulfikar, N Soliman, P Maes, eid=2-s2.0-85098 16106 8\&amp; doi= 10. 21437%Multimodal inductive transfer learning for detection of alzheimer's dementia and its severity. 2fInt erspe ech. 2020-3137\&amp; partn erID= 40\&amp; md5= 8b619 bdc30 a02a2 4448f dc721 e2ec7 09Sarawgi U, Zulfikar W, Soliman N, Maes P. Multimodal inductive transfer learning for detection of alzheimer's dementia and its severity. Proceed- ings of the annual conference of the international speech communica- tion association, interspeech. p. 2212-6. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85098 16106 8\&amp; doi= 10. 21437% 2fInt erspe ech. 2020-3137\&amp; partn erID= 40\&amp; md5= 8b619 bdc30 a02a2 4448f dc721 e2ec7 09.</p>
<p>Cross-corpus feature learning between spontaneous monologue and dialogue for automatic classification of alzheimer's dementia speech. La Fuente Garcia, S D Haider, F Luz, S , Proceedings of the annual international conference of the ieee engineering in medicine and biology society. the annual international conference of the ieee engineering in medicine and biology societyuri? eid=2-s2.0-85091 00712 8\&amp; doi= 10. 1109% 2fEMB C44109. 2020. 91763 05\&amp; partn erID= 40\&amp; md5= 3d05d 67cf6 62079 3cd18 11a54 a272d 77La Fuente Garcia SD, Haider F, Luz S. Cross-corpus feature learning between spontaneous monologue and dialogue for automatic clas- sification of alzheimer's dementia speech. Proceedings of the annual international conference of the ieee engineering in medicine and biology society, embs. p. 5851-5. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85091 00712 8\&amp; doi= 10. 1109% 2fEMB C44109. 2020. 91763 05\&amp; partn erID= 40\&amp; md5= 3d05d 67cf6 62079 3cd18 11a54 a272d 77 https:// ieeex plore. ieee. org/ docum ent/ 91763 05/.</p>
<p>On the analysis of speech and disfluencies for automatic detection of mild cognitive impairment. K López-De-Ipiña, U Martinez-De-Lizarduy, P M Calvo, B Beitia, J García-Melero, E Fernández, eid=2-s2.0-85046 65967 0\&amp; doi= 10. 1007% 2fs00 521-018-3494-1\&amp; partn erID= 40\&amp;Neural Comput &amp; Applic. 32md5= 1cfd5 f2901 597f4 4e518 89ffe 5fbf5 ebLópez-de-Ipiña K, Martinez-de-Lizarduy U, Calvo PM, Beitia B, García- Melero J, Fernández E, et al. On the analysis of speech and disfluen- cies for automatic detection of mild cognitive impairment. Neural Comput &amp; Applic. 2020;32:15761-9 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85046 65967 0\&amp; doi= 10. 1007% 2fs00 521-018-3494-1\&amp; partn erID= 40\&amp; md5= 1cfd5 f2901 597f4 4e518 89ffe 5fbf5 eb.</p>
<p>Evaluating sentence segmentation in different datasets of neuropsychological language tests in brazilian portuguese. E Casanova, M V Treviso, L C Hübner, S M Aluísio, LREC 2020 -12th international conference on language resources and evaluation. eid=2-s2.0-85096 52689 6\&amp; partn erID= 40\&amp; md5= a1047 325ea e3924 ba992 89c9b 88537 ccCasanova E, Treviso MV, Hübner LC, Aluísio SM. Evaluating sentence segmentation in different datasets of neuropsychological language tests in brazilian portuguese. LREC 2020 -12th international conference on language resources and evaluation, conference proceedings. p. 2605-14. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0- 85096 52689 6\&amp; partn erID= 40\&amp; md5= a1047 325ea e3924 ba992 89c9b 88537 cc.</p>
<p>Acoustic feature extraction with interpretable deep neural network for neurodegenerative related disorder classification. Y Pan, B Mirheidari, Z Tu, R O&apos;malley, T Walker, A Venneri, eid=2-s2.0-85098 22827 0\&amp; doi= 10. 21437%Proceedings of the annual conference of the international speech communication association, interspeech. 2fInt erspe ech. 2020-2684\&amp; partn erID= 40\&amp; md5= c2e88 eba4a 60ff4 ecaa6 f6bc0 973fe 4ePan Y, Mirheidari B, Tu Z, O'Malley R, Walker T, Venneri A, et al. Acoustic feature extraction with interpretable deep neural network for neuro- degenerative related disorder classification. Proceedings of the annual conference of the international speech communication association, inter- speech. p. 4806-10. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85098 22827 0\&amp; doi= 10. 21437% 2fInt erspe ech. 2020-2684\&amp; partn erID= 40\&amp; md5= c2e88 eba4a 60ff4 ecaa6 f6bc0 973fe 4e.</p>
<p>Comparing natural language processing techniques for alzheimer's dementia prediction in spontaneous speech. T Searle, Z Ibrahim, R Dobson, eid=2-s2.0-85098 13271 9\&amp; doi= 10. 21437%Proceedings of the annual conference of the international speech communication association, interspeech. 2fInt erspe ech. 2020-2729\&amp; partn erID= 40\&amp; md5= 65e5a 73cbb ef50e a82ab df470 51356 a2Searle T, Ibrahim Z, Dobson R. Comparing natural language processing techniques for alzheimer's dementia prediction in spontaneous speech. Proceedings of the annual conference of the international speech com- munication association, interspeech. p. 2192-6. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85098 13271 9\&amp; doi= 10. 21437% 2fInt erspe ech. 2020-2729\&amp; partn erID= 40\&amp; md5= 65e5a 73cbb ef50e a82ab df470 51356 a2.</p>
<p>. 10.1145/3446132.34461976597. 2020. p. Article 65. Available from: https:// doi. org/ 10. 1145/ 34461 32. 34461 97.</p>
<p>An automatic assessment system for Alzheimer's disease based on speech using feature sequence generator and recurrent neural network. Y W Chien, S Y Hong, W T Cheah, J]. Scientific Reports. 91Chien YW, Hong SY, Cheah WT, et al. An automatic assessment system for Alzheimer's disease based on speech using feature sequence generator and recurrent neural network[J]. Scientific Reports. 2019;9(1):1-10.</p>
<p>Automatic diagnosis of alzheimer's disease using neural network language models. ICASSP, ieee international conference on acoustics, speech and signal processing -proceedings. J Fritsch, S Wankerl, E Noth, -s2.0-85069 00406 4\&amp; doi= 10. 1109% 2fICA SSP. 2019. 86826 90\&amp; partn erID= 40\&amp; md5= a76b0 8675c 5fc83 207db 7f93e b40de b0. Fritsch J, Wankerl S, Noth E. Automatic diagnosis of alzheimer's disease using neural network language models. ICASSP, ieee international conference on acoustics, speech and signal processing -proceedings. p. 5841-5. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85069 00406 4\&amp; doi= 10. 1109% 2fICA SSP. 2019. 86826 90\&amp; partn erID= 40\&amp; md5= a76b0 8675c 5fc83 207db 7f93e b40de b0 https:// ieeex plore. ieee. org/ docum ent/ 86826 90/.</p>
<p>A novel screening system for alzheimer's disease based on speech transcripts using neural network. S Y Hong, L H Yao, W T Cheah, W D Chang, L C Fu, Y L Chang, Conference proceedings -ieee international conference on systems, man and cybernetics. uri? eid=2-s2.0-85076 79007 4\&amp; doi= 10. 1109% 2fSMC. 2019. 89146 28\&amp; partn erID= 40\&amp; md5= 0ba33 c9508 39d96 612ac 11b41 8c1b7 bfHong SY, Yao LH, Cheah WT, Chang WD, Fu LC, Chang YL. A novel screening system for alzheimer's disease based on speech transcripts using neural network. Conference proceedings -ieee international conference on systems, man and cybernetics. p. 2440-5. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85076 79007 4\&amp; doi= 10. 1109% 2fSMC. 2019. 89146 28\&amp; partn erID= 40\&amp; md5= 0ba33 c9508 39d96 612ac 11b41 8c1b7 bf https:// ieeex plore. ieee. org/ docum ent/ 89146 28/.</p>
<p>Identification of mild cognitive impairment from speech in swedish using deep sequential neural networks. C Themistocleous, M Eckerström, D Kokkinakis, Front Neurol. 9uri? eid=2-s2.0-85078 27935 4\&amp; doi= 10. 3389% 2ffne ur. 2018. 00975 \&amp; partn erID= 40\&amp; md5= 4e70d c71cf e974b a2f6b 83d75 9cc36 a0Themistocleous C, Eckerström M, Kokkinakis D. Identification of mild cog- nitive impairment from speech in swedish using deep sequential neural networks. Front Neurol. 2018;9 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85078 27935 4\&amp; doi= 10. 3389% 2ffne ur. 2018. 00975 \&amp; partn erID= 40\&amp; md5= 4e70d c71cf e974b a2f6b 83d75 9cc36 a0.</p>
<p>ANN-based alzheimer's disease classification from bag of words. Speech communication -13th itg-fachtagung sprachkommunikation. P Klumpp, J Fritsch, E Nöth, eid=2-s2.0-85068 98444 8\&amp; partn erID= 40\&amp; md5= cdd22 c15ae a57b1 b7a80 e7aef 8fed8 59Klumpp P, Fritsch J, Nöth E. ANN-based alzheimer's disease classifica- tion from bag of words. Speech communication -13th itg-fachtagung sprachkommunikation. p. 341-4. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85068 98444 8\&amp; partn erID= 40\&amp; md5= cdd22 c15ae a57b1 b7a80 e7aef 8fed8 59.</p>
<p>Advances on automatic speech analysis for early detection of alzheimer disease: A non-linear multi-task approach. K López-De-Ipiña, U Martinez-De-Lizarduy, P M Calvo, J Mekyska, B Beitia, N Barroso, Curr Alzheimer Res. 15uri? eid=2-s2.0-85042 55714 1\&amp; doi= 10. 2174% 2f156 72050 14666 17112 01438 00\&amp; partn erID= 40\&amp; md5= ef13e da656 195e1 84782 70a35 78f64 2dLópez-De-Ipiña K, Martinez-De-Lizarduy U, Calvo PM, Mekyska J, Beitia B, Barroso N, et al. Advances on automatic speech analysis for early detection of alzheimer disease: A non-linear multi-task approach. Curr Alzheimer Res. 2018;15:139-48 Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85042 55714 1\&amp; doi= 10. 2174% 2f156 72050 14666 17112 01438 00\&amp; partn erID= 40\&amp; md5= ef13e da656 195e1 84782 70a35 78f64 2d https:// www. eurek asele ct. com/ artic le/ 86986.</p>
<p>Deep language space neural network for classifying mild cognitive impairment and alzheimer-type dementia. S O Orimaye, Jsm Wong, C P Wong, https:/journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0205636%5C&amp;type=printableeid=2-s2.0-85056 27609 1\&amp; doi= 10PLoS ONE. 131371% 2fjou rnal. pone. 02056 36\&amp; partn erID= 40\&amp; md5= 8f440 64728 80338 ca680 fcb16 f54d4 ebOrimaye SO, Wong JSM, Wong CP. Deep language space neural network for classifying mild cognitive impairment and alzheimer-type dementia. PLoS ONE. 2018;13. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85056 27609 1\&amp; doi= 10. 1371% 2fjou rnal. pone. 02056 36\&amp; partn erID= 40\&amp; md5= 8f440 64728 80338 ca680 fcb16 f54d4 eb. https:// journ als. plos. org/ ploso ne/ artic le/ file? id= 10. 1371/ journ al. pone. 02056 36\&amp; type= print able.</p>
<p>Proceedings of the annual conference of the international speech communication association, interspeech. T Warnita, N Inoue, K Shinoda, https:/journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0205636%5C&amp;type=printableeid=2-s2.0-85055 00990 6\&amp; doi= 10. 21437%Detecting alzheimer's disease using gated convolutional neural network from audio data. 2fInt erspe ech. 2018-1713\&amp; partn erID= 40\&amp; md5= b54c6 60f82 a0c8e 14de7 6bbaf 5ec31 77Warnita T, Inoue N, Shinoda K. Detecting alzheimer's disease using gated convolutional neural network from audio data. Proceedings of the annual conference of the international speech communication association, inter- speech. p. 1706-10. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85055 00990 6\&amp; doi= 10. 21437% 2fInt erspe ech. 2018-1713\&amp; partn erID= 40\&amp; md5= b54c6 60f82 a0c8e 14de7 6bbaf 5ec31 77.</p>
<p>Automatic identification of mild cognitive impairment through the analysis of Italian spontaneous speech. D Beltrami, L Calzà, G Gagliardi, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)2016Beltrami D, Calzà L, Gagliardi G, et al. Automatic identification of mild cognitive impairment through the analysis of Italian spontaneous speech productions[C]//Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). 2016:2086-2093.</p>
<p>Detecting mild cognitive impairment from spontaneous speech by correlation-based phonetic feature selection. G Gosztolya, L Tóth, T Grósz, V Vincze, I Hoffmann, G Szatlóczki, Proc Interspeech. 2016Gosztolya G, Tóth L, Grósz T, Vincze V, Hoffmann I, Szatlóczki G, et al. Detecting mild cognitive impairment from spontaneous speech by correlation-based phonetic feature selection. Proc Interspeech. 2016;2016:107-11.</p>
<p>Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, Ernie, arXiv:190507129. 2019Enhanced language representation with informative entities. arXiv preprintZhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q. ERNIE: Enhanced language representation with informative entities. arXiv preprint arXiv:190507129. 2019.</p>
<p>I Beltagy, M E Peters, A Cohan, Longformer, arXiv:200405150. 2020The long-document transformer. arXiv preprintBeltagy I, Peters ME, Cohan A. Longformer: The long-document trans- former. arXiv preprint arXiv:200405150. 2020.</p>
<p>Learning representations by backpropagating errors. D E Rumelhart, G E Hinton, R J Williams, Nature. 323Nature Publishing GroupRumelhart DE, Hinton GE, Williams RJ. Learning representations by back- propagating errors. Nature. 1986;323:533-6 Nature Publishing Group.</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Communications of the ACM. 606Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks[J]. Communications of the ACM. 2017;60(6):84-90.</p>
<p>An iterative grid based object detector. M Najibi, M Rastegari, L S Davis, G-Cnn, Proceedings of the ieee conference on computer vision and pattern recognition. the ieee conference on computer vision and pattern recognitionNajibi M, Rastegari M, Davis LS. G-cnn: An iterative grid based object detector. Proceedings of the ieee conference on computer vision and pattern recognition. 2016. p. 2369-77.</p>
<p>Finding structure in time. J L Elman, Cogn Sci. 14Wiley Online LibraryElman JL. Finding structure in time. Cogn Sci. 1990;14:179-211 Wiley Online Library.</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 9MIT PressHochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9:1735-80 MIT Press.</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:14123555arXiv preprintChung J, Gulcehre C, Cho K, Bengio Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:14123555. 2014.</p>
<p>Bidirectional recurrent neural networks. M Schuster, K K Paliwal, IEEE Trans Signal Process. 45IeeeSchuster M, Paliwal KK. Bidirectional recurrent neural networks. IEEE Trans Signal Process. 1997;45:2673-81 Ieee.</p>
<p>An exploration of log-mel spectrogram and mfcc features for alzheimer's dementia recognition from spontaneous speech. 2021 ieee spoken language technology workshop, slt 2021 -proceedings. A Meghanani, C S A, Ramakrishnan, A G , eid=2-s2.0-85103 97421 4\&amp; doi= 10. 1109% 2fSLT 48900. 2021. 93834 91\&amp; partn erID= 40\&amp; md5= d2092 8366b dbad7 e806d 99fa9 a073b c4Meghanani A, C. S A, Ramakrishnan AG. An exploration of log-mel spec- trogram and mfcc features for alzheimer's dementia recognition from spontaneous speech. 2021 ieee spoken language technology workshop, slt 2021 -proceedings. p. 670-7. Available from: https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85103 97421 4\&amp; doi= 10. 1109% 2fSLT 48900. 2021. 93834 91\&amp; partn erID= 40\&amp; md5= d2092 8366b dbad7 e806d 99fa9 a073b c4 https:// ieeex plore. ieee. org/ docum ent/ 93834 91/.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J].</p>
<p>Advances in neural information processing systems. 30Advances in neural information processing systems. 2017;30.</p>
<p>Publisher's Note. Publisher's Note</p>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Springer Nature remains neutral with regard to jurisdictional claims in pub- lished maps and institutional affiliations.</p>            </div>
        </div>

    </div>
</body>
</html>
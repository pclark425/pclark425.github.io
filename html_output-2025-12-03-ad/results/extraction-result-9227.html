<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-261875661</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.08374v3.pdf" target="_blank">Understanding the limitations of self-supervised learning for tabular anomaly detection</a></p>
                <p><strong>Paper Abstract:</strong> While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it. This paper explores the limitations of self-supervision for tabular anomaly detection. We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case. Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data. We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors. However, we demonstrate that using a subspace of the neural network's representation can recover performance.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FT-Transformer (feature-token transformer for tabular data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer architecture adapted for tabular inputs where each transformer layer operates at the feature level of a single datum; used in this paper as one of two backbone architectures for learning self-supervised tabular embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Revisiting deep learning models for tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FT-Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (tabular-adapted transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>tabular (multi-attribute fixed-length records)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>ODDS benchmark datasets (diverse domains: network, medical, sensor, image-derived features, etc.) and synthetic anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>general outliers / low-probability tabular anomalies (local, cluster, global, dependency types in synthetic tests)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>FT-Transformer was trained as a self-supervised encoder on a range of pretext tasks (rotation prediction, predefined shuffle, mask classification, masked-columns, denoising autoencoder, contrastive variants). Penultimate-layer embeddings (192-D for FT-Transformer as used here) were extracted and fed to shallow anomaly detectors (k-NN, LOF, iForest, OCSVM, residual norms). No augmentations at inference; model selection used validation normal-loss.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>k-NN on raw features, Local Outlier Factor (LOF), Isolation Forest (iForest), One-Class SVM (OCSVM), Residual norms (principal-component residual projection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Area Under the Receiver Operating Characteristic curve (AUROC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Embeddings from FT-Transformer did not improve anomaly detection relative to the baseline of k-NN on raw tabular features; across ODDS the baseline (k-NN on raw) ranked highest. In aggregate FT-Transformer representations underperformed ResNet representations and the raw baseline; projecting FT-Transformer embeddings to residual subspaces recovered some performance but generally remained inferior to k-NN on raw features. (No single numeric AUROC for FT-Transformer-only superior performance is reported in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Worse — FT-Transformer embeddings combined with shallow detectors were generally worse than k-NN on the original tabular features; ResNet backbones outperformed FT-Transformer in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>FT-Transformer underperforms on the relatively small ODDS datasets (paper hypothesises transformers need more training data), and its learned embeddings introduce irrelevant/high-variance directions that degrade distance-based detectors (curse of dimensionality); sensitivity to corrupted or noisy features persists.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Although FT-Transformer is a transformer-based backbone designed for tabular inputs, in a one-class, small-data tabular anomaly-detection setting its self-supervised embeddings did not beat raw-feature baselines; projection to a residual eigen-subspace improves performance, indicating the network embeddings add distracting directions rather than useful compressed features in this regime.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the limitations of self-supervised learning for tabular anomaly detection', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (fine-tuned) for text anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned transformer with self-supervised objective for textual one-class anomaly detection (Mai et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced previous work where a transformer model is fine-tuned using a self-supervised objective on normal text and the model's loss (or related scoring) is used as an anomaly score for textual anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-supervised losses for one-class textual anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (language model backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>sequences / text</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>textual corpora (one-class textual anomaly tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>out-of-distribution / anomalous text sequences</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune a transformer with a self-supervised objective on normal text data; at inference use the model loss (or other derived score) as the anomaly score to detect anomalous text samples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not specified in detail in this paper's discussion; the cited work compares to other detectors for text (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified in this paper (cited work reports performance using typical detection metrics, e.g., AUROC in that domain).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>This paper cites Mai et al. as demonstrating that fine-tuning a transformer with a self-supervised objective yields good anomaly detection performance on text, but does not report numerical results itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>According to the cited work (Mai et al.), transformer fine-tuning with self-supervision performs well for textual anomaly detection; this paper only references that finding and does not re-run those experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; cited work concerns text and is not directly evaluated in the tabular experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>The paper contrasts modalities: while self-supervised transformer fine-tuning helps in text (and self-supervision also helps in images), the same advantages do not directly transfer to tabular data in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the limitations of self-supervised learning for tabular anomaly detection', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Masked language modelling (MLM) as a self-supervision pretext</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Language Modelling (MLM) pretext task for self-supervision (applied in textual anomaly detection literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common self-supervised pretext task for language where tokens are masked and the model predicts the masked tokens; cited in the paper as a pretext used for textual one-class anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-supervised losses for one-class textual anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Masked language model (MLM) fine-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (masked-LM objective)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>sequences / text</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>textual corpora (one-class anomaly detection)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>textual outliers / anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a masked language model on normal text; use the prediction loss / likelihood or derived reconstruction score as an anomaly indicator at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not specified in this paper's description; referenced as prior work for text anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>This paper only references masked language modelling as an effective self-supervised strategy for text in prior work; no numerical results are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Mentioned as effective in textual settings in cited literature; not compared within the tabular experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed here; general modality difference noted — methods that work well for text/image self-supervision may not transfer directly to tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Serves as an example that self-supervised strategies that exploit sequence structure (like MLM for text) can produce useful anomaly signals in sequence domains, reinforcing the paper's point that domain-specific intrinsic biases (sequential/spatial) enable effective self-supervision — a property tabular data lacks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the limitations of self-supervised learning for tabular anomaly detection', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EICL (Embedding-ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EICL — embedding variant of Internal Contrastive Learning (ICL) adapted as a pretext task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of ICL used in this paper as a pretext self-supervised objective for tabular data: split each feature vector into a contiguous subwindow (size k) and its complement, use a Siamese network to align representations and contrast against other segments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Anomaly detection for tabular data with internal contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EICL (Siamese/contrastive network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Siamese neural network / contrastive (non-language-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>tabular (treated as ordered vector segments)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>ODDS tabular datasets and synthetic anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>outliers that break internal feature coherence / anomalies detectable via mismatched contiguous-segment relationships</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Split each d-dimensional record into a contiguous segment a_i of length k and complement b_i; pass through Siamese network heads and apply contrastive alignment pushing a_i and b_i together while using other segments as negatives; extract penultimate embeddings for downstream shallow detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>k-NN on raw features, LOF, iForest, OCSVM, residual norms; compared against other self-supervised pretext tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>EICL-derived embeddings did not outperform k-NN on raw features in aggregate ODDS results; in linear separability checks EICL was the exception where self-supervised embeddings reduced linear separability compared to raw features (i.e., EICL sometimes degraded separability).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Worse or similar — EICL embeddings generally did not improve anomaly detection over raw-feature baselines and in some analyses reduced linear separability between normal and anomalous classes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires specific contiguous-partitioning of features (precludes using FT-Transformer for EICL without architecture changes); in practice EICL sometimes reduces separability, implying it can mix anomalies with normal samples for some datasets; sensitive to choice of window size k.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>EICL demonstrates that contrastive, within-record segmentation can capture some dependency structure, but in many tabular one-class settings the learned embeddings still introduce distracting directions and do not reliably outperform simple baselines; EICL is a notable exception in that it sometimes reduced linear separability (worse than other pretext tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding the limitations of self-supervised learning for tabular anomaly detection', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-supervised losses for one-class textual anomaly detection. <em>(Rating: 2)</em></li>
                <li>Anomaly detection for tabular data with internal contrastive learning. <em>(Rating: 2)</em></li>
                <li>Revisiting deep learning models for tabular data. <em>(Rating: 2)</em></li>
                <li>Deep anomaly detection using geometric transformations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9227",
    "paper_id": "paper-261875661",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "FT-Transformer",
            "name_full": "FT-Transformer (feature-token transformer for tabular data)",
            "brief_description": "A transformer architecture adapted for tabular inputs where each transformer layer operates at the feature level of a single datum; used in this paper as one of two backbone architectures for learning self-supervised tabular embeddings.",
            "citation_title": "Revisiting deep learning models for tabular data.",
            "mention_or_use": "use",
            "model_name": "FT-Transformer",
            "model_type": "transformer (tabular-adapted transformer)",
            "model_size": null,
            "data_type": "tabular (multi-attribute fixed-length records)",
            "data_domain": "ODDS benchmark datasets (diverse domains: network, medical, sensor, image-derived features, etc.) and synthetic anomalies",
            "anomaly_type": "general outliers / low-probability tabular anomalies (local, cluster, global, dependency types in synthetic tests)",
            "method_description": "FT-Transformer was trained as a self-supervised encoder on a range of pretext tasks (rotation prediction, predefined shuffle, mask classification, masked-columns, denoising autoencoder, contrastive variants). Penultimate-layer embeddings (192-D for FT-Transformer as used here) were extracted and fed to shallow anomaly detectors (k-NN, LOF, iForest, OCSVM, residual norms). No augmentations at inference; model selection used validation normal-loss.",
            "baseline_methods": "k-NN on raw features, Local Outlier Factor (LOF), Isolation Forest (iForest), One-Class SVM (OCSVM), Residual norms (principal-component residual projection)",
            "performance_metrics": "Area Under the Receiver Operating Characteristic curve (AUROC)",
            "performance_results": "Embeddings from FT-Transformer did not improve anomaly detection relative to the baseline of k-NN on raw tabular features; across ODDS the baseline (k-NN on raw) ranked highest. In aggregate FT-Transformer representations underperformed ResNet representations and the raw baseline; projecting FT-Transformer embeddings to residual subspaces recovered some performance but generally remained inferior to k-NN on raw features. (No single numeric AUROC for FT-Transformer-only superior performance is reported in this paper.)",
            "comparison_to_baseline": "Worse — FT-Transformer embeddings combined with shallow detectors were generally worse than k-NN on the original tabular features; ResNet backbones outperformed FT-Transformer in these experiments.",
            "limitations_or_failure_cases": "FT-Transformer underperforms on the relatively small ODDS datasets (paper hypothesises transformers need more training data), and its learned embeddings introduce irrelevant/high-variance directions that degrade distance-based detectors (curse of dimensionality); sensitivity to corrupted or noisy features persists.",
            "unique_insights": "Although FT-Transformer is a transformer-based backbone designed for tabular inputs, in a one-class, small-data tabular anomaly-detection setting its self-supervised embeddings did not beat raw-feature baselines; projection to a residual eigen-subspace improves performance, indicating the network embeddings add distracting directions rather than useful compressed features in this regime.",
            "uuid": "e9227.0",
            "source_info": {
                "paper_title": "Understanding the limitations of self-supervised learning for tabular anomaly detection",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Transformer (fine-tuned) for text anomaly detection",
            "name_full": "Fine-tuned transformer with self-supervised objective for textual one-class anomaly detection (Mai et al. 2022)",
            "brief_description": "Referenced previous work where a transformer model is fine-tuned using a self-supervised objective on normal text and the model's loss (or related scoring) is used as an anomaly score for textual anomaly detection.",
            "citation_title": "Self-supervised losses for one-class textual anomaly detection.",
            "mention_or_use": "mention",
            "model_name": "Transformer (fine-tuned)",
            "model_type": "transformer (language model backbone)",
            "model_size": null,
            "data_type": "sequences / text",
            "data_domain": "textual corpora (one-class textual anomaly tasks)",
            "anomaly_type": "out-of-distribution / anomalous text sequences",
            "method_description": "Fine-tune a transformer with a self-supervised objective on normal text data; at inference use the model loss (or other derived score) as the anomaly score to detect anomalous text samples.",
            "baseline_methods": "Not specified in detail in this paper's discussion; the cited work compares to other detectors for text (reference only).",
            "performance_metrics": "Not specified in this paper (cited work reports performance using typical detection metrics, e.g., AUROC in that domain).",
            "performance_results": "This paper cites Mai et al. as demonstrating that fine-tuning a transformer with a self-supervised objective yields good anomaly detection performance on text, but does not report numerical results itself.",
            "comparison_to_baseline": "According to the cited work (Mai et al.), transformer fine-tuning with self-supervision performs well for textual anomaly detection; this paper only references that finding and does not re-run those experiments.",
            "limitations_or_failure_cases": "Not detailed here; cited work concerns text and is not directly evaluated in the tabular experiments of this paper.",
            "unique_insights": "The paper contrasts modalities: while self-supervised transformer fine-tuning helps in text (and self-supervision also helps in images), the same advantages do not directly transfer to tabular data in this study.",
            "uuid": "e9227.1",
            "source_info": {
                "paper_title": "Understanding the limitations of self-supervised learning for tabular anomaly detection",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Masked language modelling (MLM) as a self-supervision pretext",
            "name_full": "Masked Language Modelling (MLM) pretext task for self-supervision (applied in textual anomaly detection literature)",
            "brief_description": "A common self-supervised pretext task for language where tokens are masked and the model predicts the masked tokens; cited in the paper as a pretext used for textual one-class anomaly detection.",
            "citation_title": "Self-supervised losses for one-class textual anomaly detection.",
            "mention_or_use": "mention",
            "model_name": "Masked language model (MLM) fine-tuned model",
            "model_type": "transformer (masked-LM objective)",
            "model_size": null,
            "data_type": "sequences / text",
            "data_domain": "textual corpora (one-class anomaly detection)",
            "anomaly_type": "textual outliers / anomalies",
            "method_description": "Train a masked language model on normal text; use the prediction loss / likelihood or derived reconstruction score as an anomaly indicator at test time.",
            "baseline_methods": "Not specified in this paper's description; referenced as prior work for text anomaly detection.",
            "performance_metrics": "Not specified in this paper.",
            "performance_results": "This paper only references masked language modelling as an effective self-supervised strategy for text in prior work; no numerical results are provided here.",
            "comparison_to_baseline": "Mentioned as effective in textual settings in cited literature; not compared within the tabular experiments of this paper.",
            "limitations_or_failure_cases": "Not discussed here; general modality difference noted — methods that work well for text/image self-supervision may not transfer directly to tabular data.",
            "unique_insights": "Serves as an example that self-supervised strategies that exploit sequence structure (like MLM for text) can produce useful anomaly signals in sequence domains, reinforcing the paper's point that domain-specific intrinsic biases (sequential/spatial) enable effective self-supervision — a property tabular data lacks.",
            "uuid": "e9227.2",
            "source_info": {
                "paper_title": "Understanding the limitations of self-supervised learning for tabular anomaly detection",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "EICL (Embedding-ICL)",
            "name_full": "EICL — embedding variant of Internal Contrastive Learning (ICL) adapted as a pretext task",
            "brief_description": "An adaptation of ICL used in this paper as a pretext self-supervised objective for tabular data: split each feature vector into a contiguous subwindow (size k) and its complement, use a Siamese network to align representations and contrast against other segments.",
            "citation_title": "Anomaly detection for tabular data with internal contrastive learning.",
            "mention_or_use": "use",
            "model_name": "EICL (Siamese/contrastive network)",
            "model_type": "Siamese neural network / contrastive (non-language-specific)",
            "model_size": null,
            "data_type": "tabular (treated as ordered vector segments)",
            "data_domain": "ODDS tabular datasets and synthetic anomalies",
            "anomaly_type": "outliers that break internal feature coherence / anomalies detectable via mismatched contiguous-segment relationships",
            "method_description": "Split each d-dimensional record into a contiguous segment a_i of length k and complement b_i; pass through Siamese network heads and apply contrastive alignment pushing a_i and b_i together while using other segments as negatives; extract penultimate embeddings for downstream shallow detectors.",
            "baseline_methods": "k-NN on raw features, LOF, iForest, OCSVM, residual norms; compared against other self-supervised pretext tasks.",
            "performance_metrics": "AUROC",
            "performance_results": "EICL-derived embeddings did not outperform k-NN on raw features in aggregate ODDS results; in linear separability checks EICL was the exception where self-supervised embeddings reduced linear separability compared to raw features (i.e., EICL sometimes degraded separability).",
            "comparison_to_baseline": "Worse or similar — EICL embeddings generally did not improve anomaly detection over raw-feature baselines and in some analyses reduced linear separability between normal and anomalous classes.",
            "limitations_or_failure_cases": "Requires specific contiguous-partitioning of features (precludes using FT-Transformer for EICL without architecture changes); in practice EICL sometimes reduces separability, implying it can mix anomalies with normal samples for some datasets; sensitive to choice of window size k.",
            "unique_insights": "EICL demonstrates that contrastive, within-record segmentation can capture some dependency structure, but in many tabular one-class settings the learned embeddings still introduce distracting directions and do not reliably outperform simple baselines; EICL is a notable exception in that it sometimes reduced linear separability (worse than other pretext tasks).",
            "uuid": "e9227.3",
            "source_info": {
                "paper_title": "Understanding the limitations of self-supervised learning for tabular anomaly detection",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-supervised losses for one-class textual anomaly detection.",
            "rating": 2,
            "sanitized_title": "selfsupervised_losses_for_oneclass_textual_anomaly_detection"
        },
        {
            "paper_title": "Anomaly detection for tabular data with internal contrastive learning.",
            "rating": 2,
            "sanitized_title": "anomaly_detection_for_tabular_data_with_internal_contrastive_learning"
        },
        {
            "paper_title": "Revisiting deep learning models for tabular data.",
            "rating": 2,
            "sanitized_title": "revisiting_deep_learning_models_for_tabular_data"
        },
        {
            "paper_title": "Deep anomaly detection using geometric transformations.",
            "rating": 1,
            "sanitized_title": "deep_anomaly_detection_using_geometric_transformations"
        }
    ],
    "cost": 0.0140535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Understanding the limitations of self-supervised learning for tabular anomaly detection
14 Mar 2024</p>
<p>Kimberly T Mai kimberly.mai@ucl.ac.uk 
Department of Computer Science
University College London
Gower StreetWC1E 6BTLondonUnited Kingdom</p>
<p>Department of Security and Crime Science
University College London
Gower StreetWC1E 6BTLondonUnited Kingdom</p>
<p>Toby Davies t.davies@leeds.ac.uk 
Department of Security and Crime Science
University College London
Gower StreetWC1E 6BTLondonUnited Kingdom</p>
<p>School of Law
University of Leeds
LS2 9JTWoodhouse, LeedsUnited Kingdom</p>
<p>Lewis D Griffin l.griffin@ucl.ac.uk 
Understanding the limitations of self-supervised learning for tabular anomaly detection
14 Mar 20243B91E754EA8C4CE7FDCBEC3AEF04E20CarXiv:2309.08374v3[cs.LG]anomaly detectiondeep learningself-supervised learningtabular data
While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it.This paper explores the limitations of self-supervision for tabular anomaly detection.We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case.Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data.We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors.However, we demonstrate that using a subspace of the neural network's representation can recover performance.</p>
<p>Introduction</p>
<p>Anomaly detection is the task of identifying unusual instances.Two issues hinder performance: how to obtain a "good" representation of the normal data and a lack of knowledge about the nature of anomalies.The emergence of self-supervised learning techniques has primarily addressed these issues in complex domains such as computer vision and natural language processing [1,2].However, these techniques have not yielded the same benefits for tabular data [3].</p>
<p>Self-supervised learning typically uses a pretext task to learn the intrinsic structure of the training data [4].Examples of pretext tasks include colourising greyscale images [5] or predicting the next word in a sentence [6,7].Understanding the typical characteristics of a domain allows one to choose an effective pretext task.For instance, colourisation requires knowledge of object boundaries and semantics.These aspects are useful for image classification [8,9].However, unlike images or text where spatial or sequential biases are natural starting points for self-supervision, the starting points for tabular data are unclear.</p>
<p>A recent study indicated that self-supervised learning does not help tabular anomaly detection [3].Reiss et al. compared two self-supervised methods with knearest neighbours (k-NN) on the original features.Even though the methods were designed for tabular data, they found that k-NN on the original features worked the best.</p>
<p>We seek to understand why this is the case.We extend the experiments to include a more comprehensive suite of pretext tasks.We also incorporate synthetic test cases and analyse the underlying learnt representations.Our results reinforce that selfsupervision does not improve tabular anomaly detection performance and indicate deep neural networks introduce redundant features, which reduces the effectiveness of anomaly detectors.Conversely, we can recover performance using a subspace of the neural network's representation.We also show that self-supervised learning can outperform the original representation in the case of purely localised anomalies and those with different dependency structures.</p>
<p>In addition to the above investigations, we ran a series of experiments to benchmark anomaly detection performance in a setting where we do not have access to anomalies during training.We include our findings as a complement to the self-supervision results and to provide practical insight into scenarios where specific detectors work better than others.</p>
<p>Our contributions are as follows:</p>
<p>In Section 2, we cover the anomaly detection setup.We proceed to outline our experimental approach in Section 3. We evaluate our findings in Section 4. Finally, we summarise our work and conclude in Section 5.</p>
<p>Background 2.1 Anomaly detection</p>
<p>Anomaly detection can be characterised as follows:</p>
<p>Let X ∈ R d represent the data space.We assume the normal data is drawn from a distribution P on X .Anomalies are data points x ∈ X that lie in a low probability region in P. Therefore, the set of anomalies can be defined as follows [10]:
A = {x ∈ X |p(x) ≤ τ }, τ ≥ 0 (1)
Where τ is a threshold.Often, the original input space is not used as anomaly detection performance can be improved by using a different representation space.In the context of deep learning, a neural network parameterised by θ : X → Y (where Y ∈ R m ) is used to transform the input data.The anomalies are assumed to lie in a low-probability region in the new space.Namely, P on X transforms to P ′ on Y according to P ′ (θ(x)) = |J θ |, where J is the Jacobian of θ.If θ is an effective mapping, then θ(A) will still be a low probability of P ′ and θ(A) will have a simpler boundary in Y than A in X .</p>
<p>There are deep anomaly detectors (which aim to simultaneously transform the data to a new subspace and classify it) and shallow anomaly detectors (which do not transform the data but solely rely on an existing representation).This paper focuses on shallow anomaly detectors to isolate the differences in representations derived from different self-supervision tasks.Evaluating the transformative properties of deep anomaly detectors is out of scope.In addition, recent approaches suggest state-ofthe-art anomaly detection performance is achievable by separating the representation learning and detection components [3,[11][12][13][14].In this setup, we also assume only normal samples are present in the training set.This is referred to as a "one-class" setting in anomaly detection literature.The expressions "one-class learning" and "anomaly detection" are synonymous [10,15,16].We use the same terms for consistency with the literature.We describe the anomaly detectors used in our analyses below.For a more detailed overview of anomaly detection techniques, we refer the reader to Ruff et al. [10].</p>
<p>k-NN assumes normal data closely surround other similar samples in the feature space, while anomalies have relatively fewer nearby neighbours.Despite being a simple approach, k-NN remains competitive in big data instances [3,13,14,17,18].k-NN typically uses features extracted from pre-trained classification neural networks [13,14,18] for image-based anomaly detection.However, equivalent neural networks for tabular data do not exist.</p>
<p>Local outlier factor (LOF) is a density-based outlier detection method [19].It compares the local density of a data point against its k-nearest neighbours.If the point's density is significantly lower, it is deemed anomalous.</p>
<p>Isolation forest (iForest) is an ensemble-based algorithm [20].It uses a set of isolation trees.Each tree aims to isolate the training data into leaves.The tree construction algorithm randomly selects an attribute and a random split inside the attribute's range until each data point lies in a leaf.Each observation is assigned a score by calculating the length of the root node to the leaf and averaging across the trees.Points with shorter path lengths are considered more unusual, as the algorithm assumes anomalies are easier to isolate.</p>
<p>One-class support vector machine (OCSVM) assumes normal data lies in a high-density region [15].Taking the origin as an anchor in the absence of anomalous data during training, it learns a maximum margin hyperplane that separates most training data from the origin.The algorithm considers a test datum's distance to the learnt hyperplane to classify anomalies.The method classifies a point as an anomaly if it lies on the side of the hyperplane closer to the origin.</p>
<p>Residual norms belong to the category of dictionary-based approaches.Dictionary-based approaches assume the building blocks of a feature space can reconstruct normal data but cannot construct anomalies.Methods using dictionaries use either linear or non-linear manifold learning techniques (e.g., principal components analysis or autoencoders) to determine the building blocks [21][22][23].We use the linear principal space approach from Wang et al. [23] for our experiments.This technique achieves state-of-the-art results for out-of-distribution detection on images, verified in independent benchmarks [24].Although introduced for images, the method itself is modality-neutral.We follow previous anomaly detection methodologies that have adapted image-based methods to other modalities while retaining acceptable performance [2,3].</p>
<p>For consistency, we use Wang et al.'s [23] original notation and code implementation 1 .Given X as the in-distribution data matrix of training samples, we find the principal subspace W from the matrix X T X.This subspace spans the eigenvectors of the D largest eigenvalues of X T X.We assume anomalies have more variance on the components with smaller explained variance [25].Therefore, we project X to the subspace spanned by the smallest eigenvalues of D (represented by W ⊥ ) to encapsulate the residual space and take its norm as the anomaly score:
||x W ⊥ || (2)</p>
<p>An overview of self-supervised learning</p>
<p>Self-supervision approaches devise tasks based on the intrinsic properties of the training data.By exploiting these properties, neural networks hopefully learn about the regularities of the data.Examples across different modalities include: Classifying perturbations: Each training datum is subject to a perturbation randomly selected from a fixed set, such as rotating the input data [26] or reordering patches in an image [27].A classification model then learns to predict which perturbation was applied.</p>
<p>Conditional prediction.A neural network sees pieces of the input data and learns to complete the remaining parts.Examples include predicting the next word given a portion of a sentence [6] or filling in masked areas of an image [28,29].</p>
<p>Clustering.Under this category, models learn to group semantically similar instances and place them far away from observations representing other semantic categories.k-means clustering is a classic example that measures similarity in Euclidean space.</p>
<p>More modern techniques learn a similarity metric using neural mappings.One popular loss function that enables this is InfoNCE [30,31].InfoNCE takes augmented views of the same data point as positives and learns to group them while pushing away other data points.Variants of this method sample from the positive's nearest neighbours to create more semantic variations [32,33].Augmentations are usually in the form of transformations.In the case of images, these can involve adding noise, colour jittering, or horizontal flips.However, InfoNCE relies on large batch sizes to enable sufficiently challenging comparisons.Augmentation choices are also vital, as aggressive transformations could remove relevant semantic features.</p>
<p>VICReg [34] attempts to overcome some of the issues of InfoNCE by enforcing specific statistical properties.It encourages augmented views to have a high variance to ensure the neural mapping learns diverse aspects of the data.It also regularises the covariance matrix of the representations.This regularisation ensures the neural mapping covers complementary information across the representation space.</p>
<p>Additional pretext tasks are covered in more detail in Balestriero et al. [4].</p>
<p>Self-supervised learning and anomaly detection for non-tabular data</p>
<p>Anomaly detection for non-tabular data has benefited from self-supervision.Golan and El-Yaniv [35] show that compared to OCSVMs trained on pixel space, outputs from a convolutional neural network trained to predict image rotations were more reliable for anomaly detection.Mai et al. [2] demonstrate similar findings on text.They show that good anomaly detection performance is achievable by fine-tuning a transformer with a self-supervised objective and using the loss as an anomaly score.Other successful approaches do not use a self-supervised model in an end-to-end manner for anomaly detection.The works of Sehwag et al. [12] and Tack et al. [11] both extract features from neural networks trained with an InfoNCE objective to perform anomaly detection on images.Sehwag et al. classify anomalies using the Mahalanobis distance on the extracted space, while Tack et al. use a product of cosine similarities and norms.</p>
<p>Self-supervised learning and anomaly detection for tabular data</p>
<p>Literature covering self-supervision for anomaly detection in tabular data is more limited.GOAD [36] extends the work of Golan and El-Yaniv [35] to a more generalised setting.They apply random affine transformations to the data and train a neural network to predict these transformations.At inference, they apply all possible transformations to the test data, obtain the prediction of each transformation from the network and aggregate the predictions to produce the anomaly score.The network should be able to predict the correct modification with higher confidence for the normal data versus the anomalies.ICL [37] adapts the InfoNCE objective.It considers one sample at a time.Taking a sample x i of dimensionality d, ICL splits x i into two parts.The dimensionality of the two parts depends on a given window size, k (k &lt; d).The first part a i is a continuous section of size k, while the second b i is its complement of size d − k.A Siamese neural network containing two heads with dimensionalities k and d − k aims to push the representations together.The negatives are other contiguous segments of x i of size k.As the neural network should be capable of aligning the normal data and not anomalies, the loss is the anomaly score.</p>
<p>Although both methods claim to be state-of-the-art for tabular anomaly detection, Reiss et al. [3] did not find this to be the case.They replicated the pipelines of GOAD and ICL.In addition, they used the trained neural networks of GOAD and ICL as feature extractors.After extracting the features, they ran k-NN on the new representations.They compared both setups to k-NN on the original data.Although GOAD and ICL are specifically designed to process tabular data, Reiss et al. found that k-NN on the original data was the best-performing approach.However, they did not run a hyperparameter search to optimise the choice of k (leaving it as k = 5).They also used the original architectures designed for GOAD and ICL, which differ from each other.This choice could be another confounding factor affecting results.</p>
<p>We summarise the works that cover self-supervision and anomaly detection in Table 1.</p>
<p>Contrastive learning Transformation prediction</p>
<p>k-NN 3 Method</p>
<p>Datasets</p>
<p>We use 26 multi-dimensional point datasets from Outlier Detection Datasets (ODDS) [38].Each datum comprises one record, which contains multiple attributes.Table 2 summarises the properties of the datasets.We treat each dataset as distinct and train and test separate anomaly detection models for each dataset.We follow the data split protocols described in previous tabular anomaly detection literature [36,37].We randomly select 50% of the normal data for training, with the remainder used for testing.The test split includes all anomalies.The training split did not use any anomalies as we adopt a one-class setup.We partition the training set further by leaving 20% for validation.</p>
<p>Baseline approach</p>
<p>We run k-NN, iForest, LOF, OCSVM, and residual norms on the original training data.As we aim to expand on the work of Reiss et al. [3], we only implement oneclass detectors for comparability.Even though Reiss et al. [3] only use k-NN in their experiments, we use multiple detectors to establish whether k-NN is the best detector or if there are other more appropriate detectors depending on the type of anomalies present.We analyse our findings in Section 4.7.Another anomaly detection study, ADBench [39], follows a similar protocol.However, their setup assumes anomalies are present in the training data.Through our experiments, we establish whether a purely one-class setup affects overall detector ranking.We use scikit-learn [40] to implement all detectors except for k-NN, which uses the faiss library [41].</p>
<p>We also investigate the detectors' sensitivity to different configurations by varying the hyperparameters.For k-NN and LOF, we report results for k = {1, 2, 5, 10, 20, 50}.For the residual norms, we look at how results change with a proportion of features, with percentages ranging from 10% to 90% in 10% increments [10%, 20%, ..., 90%].We record our findings in Section 4.7.For the self-supervised tasks, we report the results based on the best hyperparameter configuration derived from these ablations.We retain the default scikit-learn parameters for iForest and OCSVM, which uses a radial basis function kernel.</p>
<p>The detectors run directly on the data and on a standardised version.We standardise each dimension independently by removing the mean and scaling to unit variance.We also experimented with fully whitening the data but found attribute-wise standardisation rendered similar results.</p>
<p>Self-supervision</p>
<p>Pretext tasks</p>
<p>Although tabular data lacks overt intrinsic properties like those in images or text, we choose self-supervised tasks that we hypothesise can take advantage of its structure.</p>
<p>Firstly, we adapt ICL [37] and GOAD [36] to use them as pretext tasks.We do not directly implement ICL and GOAD as they score anomalies in an end-to-end manner.In contrast, our experiments focus on how representations from different pretext tasks affect shallow detection performance.Therefore, we refer to the ICL-inspired task as "EICL" (embedding-ICL) for the remainder of the paper.As GOAD uses random affine transformations, we can consider this a combination of predicting rotation and stretches.This configuration conflates two different tasks and could be trivial to solve.Therefore, we attempt to align it closer to the RotNet [1,26] experiments for imagebased anomaly detection by training a model to classify orthonormal rotations.This pretext task should profit from the rotationally invariant property of tabular data [42].Hence we refer to the GOAD-inspired task as "Rotation".</p>
<p>The additional objectives used in the experiments are as follows:</p>
<p>Predefined shuffling prediction (Shuffle): We pick a permutation of the dimensions of the data from a fixed set of permutations and shuffle the order of the attributes based on the selection.The model learns to predict that permutation.</p>
<p>Predefined mask prediction (Mask classification): Given a mask rate r (r &lt; d), we initialise predefined classes that indicate which attributes to mask.We perform masking by randomly selecting another sample x j from the training set and replacing the chosen attributes in x i with those from x j .We follow the protocol outlined in Yoon et al. [43] This approach generated better representations compared to alternative masking strategies like imputation, and constructing a mask classification pretext task outperformed alternative supervised and semi-supervised methods on tabular classification tasks.The model learns to classify which predefined class was applied.</p>
<p>Masked columns prediction (Mask columns): The model picks which attributes were masked given a mask rate r.For example, if only the first attribute was masked, a correct classification should identify the first attribute and should not pick the other attributes.This is different from the mask classification task, where the predefined mask class is given a label from a fixed set of combinations rather than from the particular attribute that has been masked (for example, if there are only two classes, the labels for mask classification are 0 or 1).</p>
<p>Denoising autoencoding (Autoencoder): Given a mask rate r, we perturb x i by randomly selecting another sample x j and replacing a subset of x i 's attributes with those of x j .The perturbed x i is the input.Given this input, the model learns to reconstruct the unperturbed x i .</p>
<p>Contrastive learning: We create positive views of x i by rotating the data using an orthonormal matrix (Contrastive rotation), permuting the attributes per the shuffle task (Contrastive shuffle), or masking the attributes per the mask classification task (Contrastive mask).We treat other data points in a minibatch as negatives.We only apply one augmentation at a time to isolate their effects.</p>
<p>Network architectures and loss functions</p>
<p>We use the same neural network architectures to control for any potential effects on performance.Per the findings of Gorishniy et al. [44], we use ResNets [45] and FT-Transformers.Gorishniy et al. examined the performance of several deep learning architectures on tabular classification and regression, including multilayer perceptrons, recurrent neural networks, ResNets and transformers.Their results indicated that ResNets and FT-Transformers were the best overall.Based on these findings, we restrict our architectures to the most promising variants.FT-Transformer is a transformer specially adapted for tabular inputs where each transformer layer operates on the feature level of one datum.</p>
<p>We train both architectures on all objectives except for EICL, where we only use ResNets.As EICL requires specific partitioning of the features, the FT-Transformer architecture would need to be modified.This modification is out of the scope of our experiments.We retain the same architecture (e.g., the number of blocks) for each pretext task and only vary the dimensionality of the output layer.The dimensionality corresponds to the number of preset classes for the rotation, shuffle, and mask classification tasks.The output dimensionality of the autoencoder task mirrors the input dimensionality.For the contrastive objectives (including EICL), we set the output as one of {128, 256, 512} depending on validation performance.</p>
<p>As previous literature has claimed specialised loss functions can improve outof-distribution detection on other modalities [46,47], we examine these to confirm whether they also improve tabular anomaly detection.</p>
<p>For the rotation, shuffle, and mask classification tasks, we use cross-entropy, adversarial reciprocal points learning (ARPL) [46], and additive angular margin (AAM) [48].ARPL is a specialised loss function for out-of-distribution detection.The probability of a datum belonging to a class is proportional to its distance to a reciprocal point.The point represents "otherness" in the learnt feature space.AAM is a loss function typically used for facial recognition.AAM specifically enforces interclass similarity and ensures interclass separation using a specified margin.This results in more spherical features for each class.We include AAM as some literature claims spherical per-class features make out-of-distribution detection easier [49].Finally, we incorporate the cross-entropy loss as studies have shown models trained with this loss function can meet or outperform specialised losses like ARPL with careful hyperparameter selection [47].We experiment with mean squared error and mean absolute error for the autoencoders.We use the binary cross-entropy loss for masked column prediction, as multiple masked columns correspond to more than one label for each datum.For the contrastive objectives, we experiment with both InfoNCE and VICReg.</p>
<p>We summarise all the possible model configurations in Table 3.</p>
<p>Model selection</p>
<p>Due to the number of potential hyperparameter combinations, we perform random searches to determine the most appropriate models for anomaly detection.We pick hyperparameters randomly and train on the training split for each self-supervised task and dataset.As we cannot evaluate using anomalies, we select models that achieve the lowest loss on the normal validation data.As we want to analyse the effect of different loss functions and architecture, the hyperparameter sweep stage results in a maximum of twelve configurations for each dataset and task.For example, the models trained on the rotation task would include ResNets and FT-Transformers, each architecture also includes the cross-entropy, ARPL, and AAM losses.There are also different configurations for standardised and non-standardised input data.</p>
<p>Feature extraction</p>
<p>After training, we obtain the learnt features by passing input data through the selfsupervised models.We extract the features from the penultimate layer.As we fix the architecture for the different tasks, we obtain 128-dimensional embeddings for ResNets and 192-dimensional embeddings for FT-Transformer.We train the anomaly detectors using the new training features and test them using the transformed test features.We do not apply any augmentations during inference to ensure a fair comparison between the self-supervised tasks.Figure 1 shows the workflow.</p>
<p>Evaluation</p>
<p>We evaluate all anomaly detectors using the area under the receiving operator curve (AUROC) score.We can consider AUROC as the probability that a randomly selected anomaly will be ranked as more abnormal than a normal sample.Scores fall between 0% and 100%.A score of 50% indicates the detector cannot distinguish between anomalies and normal data points, while a score of 100% signals perfect anomaly discrimination.We choose AUROC as it does not require a threshold to control for false positives, for example.</p>
<p>Additional ablations</p>
<p>In addition to evaluations with the ODDS dataset, we run more experiments to understand detector performance and scenarios where specific self-supervised objectives may perform better than others.</p>
<p>Synthesised anomalies</p>
<p>Although ODDS contains several datasets, the datasets may mix different types of anomalies.These mixes can make it difficult to diagnose why one representation performs better than another.Therefore, we evaluate how the pretext tasks and their learnt representations fare with synthesised anomalies.We keep the normal data in the train and test splits and only generate anomalies by perturbing the properties of the normal training data.We use the four synthetic anomaly categories as defined in ADBench [39,50].We use ADBench's code to create all types.</p>
<p>• Local anomalies deviate from their local cluster.We use Gaussian mixture models (GMM) to learn the underlying normal distribution.The covariance matrix undergoes scaling by a factor α to generate the anomalies.We use α = 2 in our experiments.</p>
<p>• Cluster anomalies use GMMs to learn the normal distribution.A factor β scales the mean feature vector to create the cluster anomalies.We use β = 2 in our experiments.
• Global anomalies originate from a uniform distribution U [δ •min(X k i ), δ •max(X k i )]
. δ is a scaling factor, and the minimum and maximum values of an attribute X k i define the boundaries.We use δ = 0.01.• Dependency anomalies do not follow the regular dependency structure seen in normal data.We use vine copulas to learn the normal distribution and Gaussian kernel density estimators to generate anomalies.</p>
<p>Corrupted input data</p>
<p>Previous work hypothesises neural networks underperform on tabular classification and regression because of their rotational invariance and lack of robustness to uninformative features [42].We investigate if this occurs for anomaly detection.Simultaneously, we explore the shallow anomaly detectors' sensitivity to corrupted attributes.Understanding these results can give a practical insight into what selfsupervision objectives and anomaly detectors work best when the data is noisy or incomplete.For our ablations, we follow Grinsztajn et al. [42] and apply the following corruptions to the raw data:</p>
<ol>
<li>Adding uninformative features: We add extra attributes to X.We select a subset of attributes to imitate.We then generate features by sampling from a multivariate Gaussian based on the mean and interquartile range of the subset's values.We experiment with different proportions of additional features and limit the maximum number of extra attributes to be no greater than the existing number of features in the dataset.</li>
</ol>
<p>Missing values:</p>
<p>We randomly remove a proportion of the entries and replace the missing values using the mean of the attribute the value belongs to.We apply this transformation to both the train and test sets.3. Removing important features: We train a random forest classifier to classify between normal samples and anomalies.We then drop a proportion of attributes based on the feature importance values output by the random forest, starting from the least important.This corruption violates the one-class assumption within our anomaly detection setup.However, we use this to analyse the robustness of the detectors and self-supervised models.4. Selecting a subset of features: Similar to (3), we train a random forest classifier.</p>
<p>We choose a proportion of attributes based on the feature importance values output from the random forest, starting from the most important.</p>
<p>After corrupting the data, we follow the same process of training the self-supervised models and feature extraction for the neural network experiments.</p>
<p>Results</p>
<p>We organise our results as follows: Subsection 4.1 reconfirms the ineffectiveness of selfsupervision for tabular anomaly detection and summarises the main results at a high level.We investigate this phenomenon through a series of case studies and ablations.Subsections 4.2 and 4.3 drill down on performance using a subset of ODDS (HTTP ) and simplified toy scenarios.Our working hypothesis is that self-supervision introduces irrelevant directions.We empirically verify our hypothesis by investigating the residual space of the embeddings in Subsection 4.4.We attempt to compare the properties of the self-supervised pretext tasks by replacing ODDS anomalies with synthetic variants in Subsection 4.5.Finally, we investigate the effect of architecture and detector choices in Subsection 4.6.No self-supervision task outperforms the baseline.Figure 2 summarises the nearest neighbour performance derived from the embeddings of each self-supervised approach.We aggregate performance by representation rather than dataset to concentrate on the influence each representation has on performance.No self-supervision task exceeds k-NN on the raw tabular data.When comparing results at a pairwise level, Figure 3 shows that the baseline scores greatly outrank the self-supervised objectives.Similarly, performance using the self-supervised embeddings drops in the presence of corrupted data (Appendix, Figure A1).These results extend the findings in [42] that neural networks are also more sensitive to corrupted attributes in the anomaly detection task.When excluding the baseline, the classification-based tasks (shuffle, mask classification, and rotation) outperform their contrastive and reconstructive counterparts.We observe similar results when we use different shallow detectors to perform anomaly detection (Figure 4), with one exception.Using residual norms on the embedding space is a better choice than k-NN.However, they still lag behind k-NN scores on the original embeddings.We also observe that OCSVM performs consistently worse across all tasks.</p>
<p>Self-supervision results</p>
<p>A case study on HTTP</p>
<p>To understand why self-supervision does not help, we will explore one ODDS dataset in detail.We proceed to test our reasoning on toy datasets and then analyse the remaining ODDS datasets.</p>
<p>We use HTTP for our analyses.HTTP is a modified subset of the KDD Cup 1999 competition data [51].The competition task involved building a detector to distinguish between intrusive (attack) and typical network connections.The dataset initially contained 41 attributes from different sources, including HTTP, SMTP, and FTP.The ODDS version only uses the "service" attribute from the HTTP information as it is considered one of the most basic features.The resulting subset is three-dimensional and comprises over 500,000 observations.Out of these samples, 2,211 (0.4%) are attacks.</p>
<p>It is easy to find attacks when running detectors directly on the raw ODDS variant of HTTP.In our experiments, all shallow methods achieve AUROCs between 87.9% and 100% on non-standardised data, with the median score being 99.7%.Further investigations show the attacks are separate from typical connections.A supervised logistic regression model trained to classify the two classes achieves 99.6% AUROC, even with only 200 sample anomalies for training.However, we observe peculiar results when using representations devised from the pretext tasks for HTTP.k-NN performance drops drastically across the majority of tasks (Figure 5), sometimes yielding scores worse than random.Conversely, the other detectors maintain their performance.For example, when extracting features from the rotation task 2 , k-NN obtains 71.8% AUROC, while iForest, OCSVM, and residual norms preserve AUROCs around 99%.In addition, logistic regression continues to classify anomalies with 99% AUROC in the supervised setting using the rotation task representations.As k-NN is susceptible to the curse of dimensionality, these initial results suggest the neural network representation introduces directions that obscure informative distances between the typical and intrusive samples.Moreover, as iForest uses a splitting strategy for detection, its consistent results indicate some direction signalling anomalousness exists.</p>
<p>Toy data analysis</p>
<p>It can be challenging to draw conclusions based on existing datasets, as they are large and often contain uninterpretable features.Therefore, we pivot to toy examples to understand these behaviours.We devise nine two-dimensional toy datasets of varying difficulty (Appendix, Figure A2).Like the experiments on the ODDS, we first evaluate performance directly on the two-dimensional representations.We then train ResNets on a two-class rotation prediction task, extract features from the penultimate embedding and re-run the detectors on the new space.We use this setting as rotations can be performed on two-dimensional data, and ResNets require less compute than the FT-Transformers.We apply the same architecture as the ODDS experiments, making the extracted features 128-dimensional.Regardless of whether the network can or cannot identify the rotation applied to the data, we observe behaviours consistent with ODDS in most toy instances.Compared to the original two-dimensional results, detection performance drops for almost all detectors after extracting representations from the ResNets.As two dimensions are sufficient to capture the characteristics of the datasets, projecting the data to a 128dimensional space only results in a stretched and narrow representation without extra information.The t-SNE plots highlight this activity.We show an example of the multiple Gaussian dataset in Figure 6.Fig. 7: Nearest neighbour performance on the toy datasets.The raw embedding (blue) is the best in almost all instances.However, the self-supervision embeddings (orange) improve when projecting to a lower dimensional space (green).</p>
<p>We project the embeddings extracted from the ResNets to a lower dimensional space using the residual eigenvectors from the training data to verify whether the curse of dimensionality affects performance.We conduct this projection because the residual norm method outperforms k-NN in the self-supervised experiments.Therefore, we hypothesise that projecting to a smaller space should reduce the distracting influence of the primary principal components.Consequently, running shallow detectors in this new space should garner improvements.We discard half of the directions for the toy experiments to form 64-dimensional embeddings.The anomaly detectors perform better in this new space (Figure 7), corroborating the view that the neural network embeddings introduce irrelevant directions.</p>
<p>We can also use the toy scenarios to attempt to understand the behaviour of the detectors such as OCSVM.Our experiments suggest OCSVM fails when anomalies lie in the centre of the normal data.For example, the AUROC for OCSVM trained on the raw ring data signalled random performance at 50%, whereas k-NN could detect the anomalies perfectly.</p>
<p>Analysing ODDS embeddings</p>
<p>We now proceed to run ablations on ODDS.Previous studies have shown that supervised classification performance correlates highly with out-of-distribution detection performance [47].Therefore, we train linear classifiers on the self-supervised and original representation and compare classification performance.If there is a drop in performance on the self-supervised embeddings, the results would suggest the neural networks transform the data in a way that mixes anomalies with the normal samples.We could consequently attribute the poor self-supervised performance to this mixing rather than the presence of irrelevant directions.</p>
<p>Figure 8a illustrates classification scores on the raw data.Most datasets are almost perfectly linearly separable in this embedding space, indicating that anomaly detectors should perform well.Figure 8b depicts the mean difference between the raw and selfsupervised classification performances.Except for EICL, the differences between linear classification performance on the raw embeddings and the self-supervised embeddings are close to zero.These trends suggest the self-supervised embeddings retain reasonable separability between the normal data and anomalies.We can rule out the mixing effect and conclude that self-supervision generally does not affect the separability between the two classes.We now investigate the residual space of the embeddings by extending the toy dataset analyses to ODDS.We take the smallest eigenvalues (from 1% to 90% in 10% increments) to project the neural network embeddings to their residual representations.We proceed to re-run the shallow anomaly detectors in the new space.Figure 9 shows the results.We aggregate both ResNet and FT-Transformer scores as observed similar behaviour across the two architectures.Reducing the dimensionality indeed boosts performance.</p>
<p>On all of the shallow detectors, using the entire representation space (100% dimensionality in Figure 9) results in lower AUROCs than using a subset.Throwing away the top 10% of principal components garners most improvements, although performance generally remains stable when discarding more components -up to the top 90%.</p>
<p>This observation aligns with previous findings that show residual directions capture information important for out-of-distribution detection [25].The magnitude of normal data is minute in this space, which is not necessarily the case for anomalies.Based on these results, we do not need complete neural network representations to perform anomaly detection.A subset suffices.</p>
<p>Synthetic anomalies</p>
<p>Anomaly detection depends on two factors: the nature of the normal data and the nature of anomalies.Both classes can originate from complex, irregular distributions.These aspects make it difficult to pinpoint the causes of results on ODDS and other curated datasets.We attempt to disentangle these factors by analysing performance on synthetic anomalies.The anomalies curated in ODDS are a composite of these types.We calculated the correlation between the ODDS and the synthetic anomaly scores and found that the datasets exhibited correlations between multiple synthetic categories, highlighting the complex qualities of the anomalies.For example, when analysing the raw data representations, k-NN on the curated Letter anomalies correlates strongly with local (ρ = 0.84), global (ρ = 0.49), and dependency (ρ = 0.94) anomaly scores.Figures 10a to 10d show the results across the four synthetic types.We show comparisons using k-NN as we found similar behaviours across the detectors.The contrastive objectives outperform the baseline in the local (Figure 10a) and cluster anomaly (Figure 10b) scenarios.This result suggests contrastive tasks are better at discerning differences at a local neighbourhood level.</p>
<p>No self-supervised approach beats the baseline when faced with global anomalies (Figure 10c).This result contributes to the idea that self-supervised representations introduce irrelevant directions.Since the global anomalies scatter across the representation space, these additional directions mask the meaningful distances between the anomalies and normal points.As a result, methods like k-NN become less effective.In addition, the ranking of the self-supervised tasks aligns most closely with their rankings on ODDS (Figure 13), which potentially highlights the overall properties of the ODDS datasets.</p>
<p>For the dependency anomalies, rotation and mask classification surpass the baseline (Figure 10d).Conversely, contrastive tasks perform the worst.Using a rotation or mask classification pretext task could help promote the intrinsic property that tabular data are non-invariant, which may help identify this type of anomaly.We analyse the effects of architectures and loss functions on performance to provide starting points for improving deep learning methods for tabular anomaly detection.We illustrate the results using k-NN as we observe similar behaviours across detectors.</p>
<p>Architectural choices for self-supervision</p>
<p>ResNets outperform transformers.Our experiments indicate ResNets are a better choice than FT-Transformer (Figure 11a).This result may be due to transformers needing more training data during the learning phase [52] -the ODDS datasets are relatively small.</p>
<p>Standardisation is not necessary.Standardising data before training neural networks does not offer much benefit (Figure 11b).</p>
<p>ARPL is a better choice for classification-type losses.ARPL significantly outperforms cross-entropy and AAM when training classification-type tasks (Figure 11c).Specialised losses like ARPL might represent "other" spaces better in the context of smaller datasets.</p>
<p>InfoNCE is better than VICReg for contrastive-type losses.This result (Figure 11d) may be due to the intricacies of VICReg, which requires balancing three components (pair similarity, variance and covariance).</p>
<p>Benchmarking unsupervised anomaly detection</p>
<p>Finally, we compare the performance of each of the detectors overall to see how well they perform in one-class settings.We aggregate results across the baseline and self-supervised embeddings to provide a more generalised understanding of detector behaviour.Figures 12 and 13 summarise the overall performances of each anomaly detector on ODDS.Even with the inclusion of self-supervised representations, k-NN performs best.Our findings align with other works highlighting k-NN as a performant anomaly detector [12][13][14]17].However, apart from k-NN and residual norm, Figure 13 shows no significant statistical differences between the detectors, suggesting the detectors make similar classification decisions.k-NN might be a sensible starting point that does not make strong assumptions about the normal distribution.Nonetheless, the choice of underlying representation should take precedence over the detector when designing anomaly detection systems.</p>
<p>Hyperparameter ablations</p>
<p>We now examine the sensitivity of the detectors to changes in hyperparameters.These experiments were conducted directly on the raw ODDS data only to understand detector performance in an optimal representation space.By doing so, these results enable a better understanding of the detectors' inductive biases and why they may deteriorate in suboptimal self-supervised representations.Residual norms: Figure 16 shows how performance varies with the percentage of attributes used.There are no notable trends, although performance remains better than random, even with a small subset (10%) of features.The number of relevant attributes in the original representation space is dataset-dependent as ODDS contains datasets from differing tasks.It is unclear how to choose the number of features to maximise the performance of residual norms in the original dataset space.Adding uninformative features: All detectors are sensitive to irrelevant features (Figure 17a).Although residual norms do not achieve the highest performance, it is more stable under increasing noise levels.This result may be due to the residuals capturing the most meaningful directions of the data.In contrast, k-NN performance declines the most.</p>
<p>Removing and selecting important features: Overall, performance plateaus at around 50% of attributes, suggesting half of the raw features are irrelevant for anomaly detection.iForest and OCSVM are the most stable under varying subsets of features (Figures 17b and 17c).</p>
<p>Missing values: Most detectors exhibit a slight decline in AUROC with increasing proportions of missing values (Figure 17d).LOF is the exception, as performance drops significantly.</p>
<p>Overall, the results indicate k-NN is the best-performing detector when faced with clean and relevant features.However, the relative ranking of detectors changes in the presence of corrupted input data.As observed in our self-supervised results (Section 4.4), residual norms might be better at filtering out noisy directions.Furthermore, when there are fewer relevant features, iForest may be a better choice.</p>
<p>Conclusion</p>
<p>Limitations and future work</p>
<p>We limited our experiments to the ODDS, which is not necessarily representative of all tabular anomaly datasets.Several datasets underwent preprocessing during the curation of ODDS, which could affect results.For example, the values in HTTP were log-transformed.In addition, the datasets are relatively small.As neural networks (particularly transformers) benefit from large amounts of data [52], it is unclear if selfsupervision would be more advantageous in the big data case.Contrastive objectives are particularly reliant on large datasets and batch sizes [30,31] Additional ablations could examine the effects of dataset size on representation quality and detection performance.</p>
<p>Furthermore, we isolated our analyses by extracting embeddings at the penultimate layer and running shallow anomaly detection algorithms.Although feature extraction at this stage combined with simple detectors is a popular strategy [12][13][14]17], different parts of the neural network could provide more informative features [22].Moreover, we chose to use shallow detectors to prioritise studying the effect of representations rather than studying the detection approach.In addition, the original implementations of ICL and GOAD evaluate anomalies using an entire neural network pipeline and use specific architectures for the tasks.Adapting these implementations for a pretext task with different architectures deviates from the original setup and could affect performance.Future work could look at extending the experiments to examine how varying pretext tasks with deep anomaly detection can yield better results [53].</p>
<p>Another direction for future work that focuses on representation quality could replace the one-class detectors with semi-supervised or supervised classifiers.We decided to concentrate on one-class detectors to align with the anomaly detection field [3,10,15,16].However, anomalies can manifest in different ways, and it could be challenging for an unsupervised detector to capture the relevant features for a specific task in practice.Incorporating prior knowledge about anomalies through weak or semi-supervised detection approaches could improve detection [54].</p>
<p>In addition, studies focusing on improving deep tabular anomaly detectors could also start examining regularisation strategies.Our experiments suggest neural networks add irrelevant features, hence regularisation during the training process could help to control this behaviour.</p>
<p>Summary</p>
<p>We trained multiple neural networks on various self-supervised pretext tasks to learn new representations for ODDS, a series of tabular anomaly detection datasets.We ran a suite of shallow anomaly detectors on the new embeddings and compared the results to the performance of the original data.None of the self-supervised representations outperformed the raw baseline.</p>
<p>We conducted ablations to try to understand this behaviour.Our empirical findings suggested that neural networks introduce irrelevant features, which degrade detector capability.As normal and anomalous data were easily distinguishable in the original tabular representations, neural networks merely stretched the data.They did introduce any additional informative information.However, we demonstrated performance was recoverable by projecting the embeddings to a residual subspace.</p>
<p>As the anomalies from ODDS derive from complex distributions, we repeated the experiments on synthetic data to understand the pretext tasks' influence on detecting particular anomaly types.We showed in specific scenarios that self-supervision can be beneficial.Contrastive tasks were better at picking up localised anomalies, while classification tasks were better at identifying differences in dependency structures.</p>
<p>Finally, we studied different shallow detectors by aggregating performances across the baseline and self-supervised representations.We showed that localised methods like k-NN and LOF worked best on ODDS but were susceptible to performance degradation with corrupted data.In contrast, iForest was more robust.Our findings provided practical insights into when one detector might be preferable to another.</p>
<p>Overall, our findings complement the growing landscape of theories on why selfsupervised learning works.Effective self-supervised pretext tasks learn to compress the input data when there are irrelevant features [55][56][57].Our findings suggest current deep learning approaches do not add much benefit when the original feature space succinctly represents the normal data.This situation is often the case for tabular data, and we demonstrated this by showing performance degrades when removing features in the original space.If the feature space did not succinctly represent the normal data, we would not observe such large degradations.This setup differs from other domains.For example, pixels in images contain lots of semantically irrelevant information.Therefore, neural networks can distil information from pixels to extract useful semantic features and self-supervision is beneficial.</p>
<p>Data availability</p>
<p>Publicly available datasets were analysed in this study.The ODDS datasets are accessible from https://odds.cs.stonybrook.edu/.</p>
<p>Fig. 1 :
1
Fig. 1: Self-supervised anomaly detection workflow.The data are only augmented and fed through the projector during training.</p>
<p>Fig. 2 :
2
Fig. 2: Box plot comparing nearest neighbour AUROCs for each of the embeddings, ordered by median performance.For each self-supervised task, we filter the results by architecture and loss function to include the embedding with the best-performing results.</p>
<p>Fig. 3 :
3
Fig. 3: Critical difference diagram comparing the embeddings in a pairwise manner.The horizontal scale denotes the average rank of each embedding.The dark lines between different detectors indicate a statistical difference (p &lt; 0.05) in results when running pairwise comparison tests.The baseline scores greatly outrank the pretext tasks.In contrast, the scores among the pretext tasks are more closely aligned.</p>
<p>Fig. 4 :
4
Fig. 4: Box plot comparing detector performance on the self-supervised embeddings.</p>
<p>Fig. 5 :
5
Fig. 5: Bar chart comparing baseline and self-supervised embedding results on HTTP.</p>
<p>Fig. 6 :
6
Fig. 6: Visualisations of the multiple Gaussian toy dataset.Light blue are the normal data and orange are the anomalies.The features extracted from the neural network appear to be more narrow (b) and stretched compared to their original 2D representation (a).</p>
<p>(a) Classification results on the raw embeddings.(b)Differences between linear classification performance on the raw embeddings compared to the self-supervised embeddings, aggregated across the ODDS datasets.Changes greater than 0 mean the self-supervision embedding reduced separability.</p>
<p>Fig. 8 :
8
Fig. 8: Supervised linear classification results (normal versus anomaly) on raw data (a) and supervised classification comparisons against the self-supervised embeddings (b).</p>
<p>Fig. 9 :
9
Fig. 9: Ablation study showing how shallow detector results vary with subspace dimensionality.</p>
<p>(a) Local anomalies (α = 2).(b) Cluster anomalies (β = 2).(c) Global anomalies (δ = 0.01).(d) Dependency anomalies.</p>
<p>Fig. 10 :
10
Fig. 10: Bar plots comparing synthetic anomaly results across the representations.</p>
<p>Fig. 11 :
11
Fig. 11: Comparisons of how architecture and losses affect performance on the selfsupervised embeddings.</p>
<p>Fig. 12 :
12
Fig. 12: Box plot comparing detector performance on the raw and standardised data.The results include all hyperparameter variations where available.</p>
<p>Fig. 13 :
13
Fig. 13: Critical difference diagram ranking the different detectors.The dark lines between different detectors indicate a statistical difference (p &lt; 0.05) in results when running pairwise comparison tests.</p>
<p>Fig. 14 :
14
Fig. 14: Line plot showing how k-NN varies with the change in the number of nearest neighbours, aggregated across the ODDS datasets, with 95% confidence intervals.</p>
<p>Fig. 15 :
15
Fig. 15: Line plot showing how LOF varies with the change in the number of nearest neighbours, aggregated across the ODDS dataset, with 95% confidence intervals.</p>
<p>Fig. 16 :
16
Fig. 16: Line plot showing how residual norm varies with the change in residual dimensionality, aggregated across the ODDS dataset, with 95% confidence intervals.</p>
<ol>
<li>7 . 2
72
Corrupted input data (a) Additional features.(b) Removing features.(c) Selecting features.(d) Missing values.</li>
</ol>
<p>Fig. 17 :
17
Fig. 17: Ablations showing how detector performance varies with changing levels of corrupt data.</p>
<p>Fig. A2 :
A2
Fig. A2: Illustrations of the toy test data.Blue points are normal whereas orange points are anomalous.</p>
<p>Table 1 :
1
Summary of related self-supervised anomaly detection literature across modalities.
Modal-ityYearAuthorPretext taskAnomaly detector2018Golan Yaniv [35] andEl-Rotation predictionClassification confidenceImages2020Tack et al. [11]Contrastive learningCosine similarity and norm2021Sehwag et al. [12]Contrastive learningMahalanobis distanceMasked language mod-ellingText2022Mai et al. [2]Causal language mod-LossellingContrastive learningTabular2022Shenkar and Wolf [37]Contrastive learningLossMultiple2020Bergman Hoshen [36]andTransformation predic-tionClassification confidence2022Reiss et al. [3]</p>
<p>Table 2 :
2
Summary of ODDS datasets.
DatasetTotal size Number of anomalies (%) DimensionalityAnnthyroid7,200534 (7.4%)6Arrhythmia45266 (14.6%)274BreastW683239 (35.0%)9Cardio1,831176 (9.6%)9Glass2149 (4.2%)9Heart22410 (4.4%)44HTTP567,4692,211 (0.4%)3Ionosphere351126 (35.8%)33Letter1,600100 (6.3%)32Lympho1486 (4.1%)18Mammography 11,183260 (2.3%)6MNIST7,603700 (9.2%)100Musk3,06297 (3.2%)166Optdigits5,216150 (2.9%)64Pendigits6,870156 (2.3%)16Pima768268 (34.9%)8Satellite6,4352,036 (31.6%)36Satimage-25,80371 (1.2%)36Seismic2,584170 (6.5%)11Shuttle49,0973,511 (6.6%)9SMTP95,15630 (0.03%)3Speech3,68661 (1.7%)400Thyroid3,77293 (2.4%)6Vertebral24030 (12.5%)6Vowels1,45650 (3.4%)12WBC27821 (5.6%)30Wine12910 (7.7%)13</p>
<p>Table 3 :
3
Summary of the model configurations.
Anomaly detectorsArchitectures Self-supervised tasks Loss functionsRotationCross-entropyShuffleARPLMask classificationAAMk-nearest neighboursResNetMask columnsBinary cross-entropyIsolation forestFT-TransformerAutoencoderMSELocal outlier factorMAEOne-class support vector machineEICLResidual normsContrastive -rotationInfoNCEContrastive -shuffleVICRegContrastive -mask
https://github.com/haoqiwang/vim
Using the best-performing rotation model, which is an FT-Transformer trained with ARPL loss.
This work was supported by funding from EPSRC under grant EP/R513143/1.
Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. D Hendrycks, M Mazeika, S Kadavath, D Song, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>Self-supervised losses for one-class textual anomaly detection. K T Mai, T Davies, L D Griffin, 10.48550/arXiv.2204.05695arXiv.2204.05695 2204.056952022</p>
<p>Anomaly detection requires better representations. T Reiss, N Cohen, E Horwitz, R Abutbul, Y Hoshen, Computer Vision -ECCV 2022 Workshops. Lecture Notes in Computer Science. L Karlinsky, T Michaeli, K Nishino, October 23-27, 202213804Proceedings, Part IV</p>
<p>. Springer, 10.1007/978-3-031-25069-9_4https://doi.org/10.1007/978-3-031-25069-9 42022Tel Aviv, Israel</p>
<p>A cookbook of self-supervised learning. R Balestriero, M Ibrahim, V Sobal, A Morcos, S Shekhar, T Goldstein, F Bordes, A Bardes, G Mialon, Y Tian, A Schwarzschild, A G Wilson, J Geiping, Q Garrido, P Fernandez, A Bar, H Pirsiavash, Y Lecun, M Goldblum, 10.48550/arXiv.2304.12210CoRR abs/2304.122102023</p>
<p>Colorful image colorization. R Zhang, P Isola, A A Efros, 10.1007/978-3-319-46487-9_40Proceedings, Part III. Lecture Notes in Computer Science. B Leibe, J Matas, N Sebe, M Welling, Part III. Lecture Notes in Computer ScienceThe NetherlandsSpringerOctober 11-14, 2016. 20169907Computer Vision -ECCV 2016 -14th European Conference</p>
<p>Deep contextualized word representations. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, 10.18653/v1/n18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Long Papers. M A Walker, H Ji, A Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018New Orleans, Louisiana, USAAssociation for Computational LinguisticsJune 1-6, 2018. 20181</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. R Geirhos, P Rubisch, C Michaelis, M Bethge, F A Wichmann, W Brendel, ICLR 20197th International Conference on Learning Representations. New Orleans, Louisiana, USAMay 6-9, 2019. 2019</p>
<p>The origins and prevalence of texture bias in convolutional neural networks. K L Hermann, T Chen, S Kornblith, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Lin, NeurIPS2020. 2020. December 6-12, 2020. 2020</p>
<p>A unifying review of deep and shallow anomaly detection. L Ruff, J R Kauffmann, R A Vandermeulen, G Montavon, W Samek, M Kloft, T G Dietterich, K Müller, 10.1109/JPROC.2021.3052449Proc. IEEE. 10952021</p>
<p>CSI: novelty detection via contrastive learning on distributionally shifted instances. J Tack, S Mo, J Jeong, J Shin, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Lin, NeurIPS2020. 2020. December 6-12, 2020. 2020</p>
<p>SSD: A unified framework for selfsupervised outlier detection. V Sehwag, M Chiang, P Mittal, 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, VirtualMay 3-7, 2021. 2021</p>
<p>PANDA: adapting pretrained features for anomaly detection and segmentation. T Reiss, N Cohen, L Bergman, Y Hoshen, 10.1109/CVPR46437.2021.00283IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual. June 19-25, 2021. 2021Computer Vision Foundation / IEEE</p>
<p>Out-of-distribution detection with deep nearest neighbors. Y Sun, Y Ming, X Zhu, Y Li, K Chaudhuri, S Jegelka, L Song, C Szepesvári, G Niu, International Conference on Machine Learning, ICML 2022. S Sabato, Baltimore, Maryland, USAPMLRJuly 2022. 2022162Proceedings of Machine Learning Research</p>
<p>Support vector method for novelty detection. B Schölkopf, R C Williamson, A J Smola, J Shawe-Taylor, J C Platt, Advances in Neural Information Processing Systems. S A Solla, T K Leen, K Müller, Denver, Colorado, USAThe MIT PressNovember 29 -December 4, 1999. 199912</p>
<p>Anomaly detection: A survey. V Chandola, A Banerjee, V Kumar, 10.1145/1541880.1541882ACM Comput. Surv. 4132009</p>
<p>Statistical analysis of nearest neighbor methods for anomaly detection. X Gu, L Akoglu, A Rinaldo, H M Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E B Fox, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. R Garnett, NeurIPS; Vancouver, BC, Canada2019. 2019. December 8-14, 2019. 2019</p>
<p>Deep nearest neighbor anomaly detection. L Bergman, N Cohen, Y Hoshen, CoRR abs/2002.10445 (2020) 2002.10445</p>
<p>LOF: identifying density-based local outliers. M M Breunig, H Kriegel, R T Ng, J Sander, 10.1145/342009.335388Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. W Chen, J F Naughton, P A Bernstein, the 2000 ACM SIGMOD International Conference on Management of DataDallas, Texas, USA; Dallas, Texas, USAACMMay 16-18, 2000. 2000</p>
<p>Isolation forest. F T Liu, K M Ting, Z Zhou, 10.1109/ICDM.2008.17Proceedings of the 8th IEEE International Conference on Data Mining (ICDM 2008). the 8th IEEE International Conference on Data Mining (ICDM 2008)Pisa, ItalyIEEE Computer SocietyDecember 15-19, 2008. 2008</p>
<p>In-network PCA and anomaly detection. L Huang, X Nguyen, M N Garofalakis, M I Jordan, A D Joseph, N Taft, Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems. B Schölkopf, J C Platt, T Hofmann, Vancouver, British Columbia, CanadaMIT PressDecember 4-7, 2006. 2006</p>
<p>Rapp: Novelty detection with reconstruction along projection pathway. K H Kim, S Shim, Y Lim, J Jeon, J Choi, B Kim, A S Yoon, 8th International Conference on Learning Representations. Addis Ababa; EthiopiaApril 26-30, 2020. 20202020</p>
<p>Vim: Out-of-distribution with virtuallogit matching. H Wang, Z Li, L Feng, W Zhang, 10.1109/CVPR52688.2022.00487IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. New Orleans, Louisiana, USAIEEEJune 18-24, 2022. 2022</p>
<p>. 10.1109/CVPR52688.2022.00487</p>
<p>Openood: Benchmarking generalized out-of-distribution detection. J Yang, P Wang, D Zou, Z Zhou, K Ding, W Peng, H Wang, G Chen, B Li, Y Sun, X Du, K Zhou, W Zhang, D Hendrycks, Y Li, Z Liu, NeurIPS. 2022</p>
<p>Why is the mahalanobis distance effective for anomaly detection?. R Kamoi, K Kobayashi, CoRR abs/2003.00402 (2020) 2003.00402</p>
<p>Unsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, ICLR 20186th International Conference on Learning Representations. Vancouver, British Columbia, CanadaApril 30 -May 3, 2018. 2018Conference Track Proceedings. OpenReview.net</p>
<p>Unsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, 10.1007/978-3-319-46466-4_5Computer Vision -ECCV 2016 -14th European Conference. Lecture Notes in Computer Science. B Leibe, J Matas, N Sebe, M Welling, Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 20169910Proceedings, Part VI</p>
<p>Context encoders: Feature learning by inpainting. D Pathak, P Krähenbühl, J Donahue, T Darrell, A A Efros, 10.1109/CVPR.2016.2782016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, Nevada, USAIEEE Computer SocietyJune 27-30, 2016. 2016</p>
<p>Masked autoencoders are scalable vision learners. K He, X Chen, S Xie, Y Li, P Dollár, R B Girshick, 10.1109/CVPR52688.2022.01553IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022. IEEEJune 18-24, 2022. 2022</p>
<p>Representation learning with contrastive predictive coding. A Oord, Y Li, O Vinyals, 1807.037482018</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G E Hinton, Proceedings of the 37th International Conference on Machine Learning, ICML 2020. the 37th International Conference on Machine Learning, ICML 2020PMLR, Virtual13-18 July 2020. 2020119Proceedings of Machine Learning Research</p>
<p>With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. D Dwibedi, Y Aytar, J Tompson, P Sermanet, A Zisserman, 10.1109/ICCV48922.2021.009452021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. Montreal, QC, CanadaIEEEOctober 10-17, 2021. 2021</p>
<p>pnnclr: Stochastic pseudo neighborhoods for contrastive learning based unsupervised representation learning problems. M Biswas, H Buckchash, D K Prasad, 10.48550/ARXIV.2308.069832023</p>
<p>Vicreg: Variance-invariance-covariance regularization for self-supervised learning. A Bardes, J Ponce, Y Lecun, ICLR 2022The Tenth International Conference on Learning Representations. April 25-29, 2022. 2022</p>
<p>Deep anomaly detection using geometric transformations. I Golan, R El-Yaniv, S Bengio, H M Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. R Garnett, Montréal, Canada2018. 2018. December 3-8, 2018. 2018</p>
<p>Classification-based anomaly detection for general data. L Bergman, Y Hoshen, 8th International Conference on Learning Representations. EthiopiaApril 26-30, 2020. 20202020</p>
<p>Anomaly detection for tabular data with internal contrastive learning. T Shenkar, L Wolf, The Tenth International Conference on Learning Representations. OpenReview.net, Virtual2022 April 25-29, 2022. 2022</p>
<p>. S Rayana, 2016ODDS Library</p>
<p>Adbench: Anomaly detection benchmark. S Han, X Hu, H Huang, M Jiang, Y Zhao, NeurIPS. 2022</p>
<p>Scikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, 10.5555/1953048.2078195J. Mach. Learn. Res. 122011</p>
<p>Billion-scale similarity search with gpus. J Johnson, M Douze, H Jégou, 10.1109/TBDATA.2019.2921572IEEE Trans. Big Data. 732021</p>
<p>Why do tree-based models still outperform deep learning on typical tabular data?. L Grinsztajn, E Oyallon, G Varoquaux, NeurIPS. 2022</p>
<p>VIME: extending the success of self-and semi-supervised learning to tabular domain. J Yoon, Y Zhang, J Jordon, M Schaar, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Lin, NeurIPS2020. 2020. December 6-12, 2020. 2020</p>
<p>Revisiting deep learning models for tabular data. Y Gorishniy, I Rubachev, V Khrulkov, A Babenko, M Ranzato, A Beygelzimer, Y N Dauphin, P Liang, Vaughan, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. J W , December 6-14, 2021. 2021</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, Nevada, USAIEEE Computer SocietyJune 27-30, 2016. 2016</p>
<p>Adversarial reciprocal points learning for open set recognition. G Chen, P Peng, X Wang, Y Tian, 10.1109/TPAMI.2021.3106743IEEE Trans. Pattern Anal. Mach. Intell. 44112022</p>
<p>Open-set recognition: A good closed-set classifier is all you need. S Vaze, K Han, A Vedaldi, A Zisserman, ICLR 2022The Tenth International Conference on Learning Representations. OpenReview.net, VirtualApril 25-29, 2022. 2022</p>
<p>Arcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, J Yang, N Xue, I Kotsia, S Zafeiriou, 10.1109/TPAMI.2021.3087709IEEE Trans. Pattern Anal. Mach. Intell. 44102022</p>
<p>How to exploit hyperspherical embeddings for out-of-distribution detection?. Y Ming, Y Sun, O Dia, Y Li, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Benchmarking unsupervised outlier detection with realistic synthetic data. G Steinbuss, K Böhm, 10.1145/3441453ACM Trans. Knowl. Discov. Data. 1542021</p>
<p>S Stolfo, W Fan, W Lee, A Prodromidis, P Chan, 10.24432/C51C7NKDD Cup 1999 Data. UCI Machine Learning Repository. 1999</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, ICLR 20219th International Conference on Learning Representations. OpenReview.net, VirtualMay 3-7, 2021. 2021</p>
<p>Deep one-class classification. L Ruff, N Görnitz, L Deecke, S A Siddiqui, R A Vandermeulen, A Binder, E Müller, M Kloft, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. J G Dy, A Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholm, SwedenPMLR, StockholmsmässanJuly 10-15. 2018. 201880Proceedings of Machine Learning Research</p>
<p>Deep semi-supervised anomaly detection. L Ruff, R A Vandermeulen, N Görnitz, A Binder, E Müller, K Müller, M Kloft, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020</p>
<p>Compressive visual representations. K Lee, A Arnab, S Guadarrama, J F Canny, I Fischer, M Ranzato, A Beygelzimer, Y N Dauphin, P Liang, Vaughan, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. J W , December 6-14, 2021. 2021</p>
<p>To compress or not to compress -self-supervised learning and information theory: A review. R Shwartz-Ziv, Y Lecun, 10.48550/ARXIV.2304.093552023</p>
<p>White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?. Y Yu, S Buchanan, D Pai, T Chu, Z Wu, S Tong, H Bai, Y Zhai, B D Haeffele, Y Ma, 10.48550/arXiv.2306.011292023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-804 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-804</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-804</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-273228575</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.06108v1.pdf" target="_blank">ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution</a></p>
                <p><strong>Paper Abstract:</strong> Robotic planning and execution in open-world environments is a complex problem due to the vast state spaces and high variability of task embodiment. Recent advances in perception algorithms, combined with Large Language Models (LLMs) for planning, offer promising solutions to these challenges, as the common sense reasoning capabilities of LLMs provide a strong heuristic for efficiently searching the action space. However, prior work fails to address the possibility of hallucinations from LLMs, which results in failures to execute the planned actions largely due to logical fallacies at high- or low-levels. To contend with automation failure due to such hallucinations, we introduce ConceptAgent, a natural language-driven robotic platform designed for task execution in unstructured environments. With a focus on scalability and reliability of LLM-based planning in complex state and action spaces, we present innovations designed to limit these shortcomings, including 1) Predicate Grounding to prevent and recover from infeasible actions, and 2) an embodied version of LLM-guided Monte Carlo Tree Search with self reflection. In simulation experiments, ConceptAgent achieved a 19% task completion rate across three room layouts and 30 easy level embodied tasks outperforming other state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on the same benchmark. Additionally, ablation studies on moderate to hard embodied tasks revealed a 20% increase in task completion from the baseline agent to the fully enhanced ConceptAgent, highlighting the individual and combined contributions of Predicate Grounding and LLM-guided Tree Search to enable more robust automation in complex state and action spaces.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e804.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e804.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-loop, natural-language-driven embodied agent that integrates a dynamic language-aligned 3D scene graph, parametric robot skills, LLM-guided Monte Carlo Tree Search (LLM-MCTS), and LLM-derived formal precondition grounding to plan, verify, and execute long-horizon tasks in open-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ConceptAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An embodied agent combining (1) perception: real-time language-aligned 3D scene graph built from SAM + CLIP instance segmentation and ZED stereo point clouds, (2) parametric skills library (object localization, navigation, grasping, manipulation, VQA, speech, code execution, web search), (3) planning: LLM-guided Monte Carlo Tree Search (LLM-MCTS) using UCB1 selection, LLM-based expansion that proposes plausible actions A_t = L(s_t,g), and an LLM critic that scores whole action sequences replacing random rollouts, and (4) formal Precondition Grounding where the LLM generates PDDL-style preconditions P_i for each tool/action and a formal verifier F(s_t,P_c) checks feasibility prior to execution; unsatisfied preconditions U_c are returned to the LLM as explicit feedback for replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2Thor (simulation) and real-world mobile manipulation trials (Boston Dynamics Spot)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Embodied 3D environments (AI2Thor kitchens in simulation and randomized low-clutter real-world rooms) where the agent has limited, egocentric sensing (camera/depth/pointcloud), must explore to discover objects, and perform long-horizon manipulation/navigation tasks — i.e., partially observable and dynamic with occlusions, concealed objects, and changing clutter.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Parametric robot skills (object localization, navigation via ROS2 Nav2, grasping/manipulation), perception modules (Segment Anything Model (SAM), CLIP embeddings), ZED stereo camera producing point clouds, ConceptGraphs (language-aligned 3D scene graph) service, code execution (PythonREPL), web search (SerpAPI), speech recognition/synthesis (Nvidia Riva), and LLMs (e.g., Llama variants used for planning/critique/precondition generation).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured 3D scene graph updates, 3D point clouds, image masks, CLIP vector embeddings, textual observations and tool-call return strings (lists of discovered objects, success/failure messages), boolean precondition satisfaction flags, and scalar critique/planning scores produced by the LLM critic.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A dynamic, language-aligned 3D scene graph plus a textual task history and current-state text description: the scene graph stores object instances, 3D bounding boxes, CLIP embeddings and attributes; the agent state s_t is composed of the task objective text, task-relevant observations, and action history. This combined structured graph + textual memory serves as the agent's belief representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Perception tools (SAM+CLIP+ZED) continuously update the 3D scene graph and CLIP embeddings in real time; tool call results (e.g., lists of discovered objects, success/failure messages) are appended to the textual task history and used to update s_t. When a proposed action fails precondition verification, the formal verifier computes U_c (unsatisfied preconditions) which are formatted and returned to the LLM; the LLM internalizes this feedback to revise future plans. LLM-based critique scores from tree-search leaf evaluations are backpropagated into the MCTS value estimates, updating planning priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM-guided Monte Carlo Tree Search (LLM-MCTS) with UCB1 selection, LLM-based expansion that proposes plausible next actions conditioned on state and goal, LLM-based critique that evaluates entire action sequences (serves as rollout value), and formal precondition verification (LLM-derived predicates checked by F(s_t,P_c)) to filter and provide closed-loop feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Waypoint navigation implemented with ROS2 Nav2 using real-time point clouds from a ZED stereo camera to populate a local costmap; follows predefined waypoints and adapts reactively to obstacle updates (Nav2 global/local planners used as provided; specific underlying global planner not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Simulation (easy object-rearrangement): ConceptAgent achieved 13.89% task completion with 10 expansions and 18.92% with 20 expansions on the easy benchmark (30 tasks across 3 floorplans). Ablation/complex benchmarks: when using both Precondition Grounding (PG) and LLM-MCTS, ConceptAgent achieved 35% success on moderate tasks and 15% on hard tasks (40 task benchmark). Real-world Spot trials: up to 40% task completion after decluttering (30 mobile manipulation trials reported; overall physical-task success rate ~20%). All numbers are percent task completion on the indicated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baselines and ablations: ReAct and ToT baselines (8B LLM) achieved 10.26% and 8.11% respectively on the easy benchmark. ConceptAgent without Precondition Grounding or LLM-MCTS completed 5% of moderate and 5% of hard tasks (ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating LLM-guided tree search with an explicit, LLM-derived precondition verification loop and a dynamic language-aligned 3D scene graph materially improves embodied planning robustness: precondition grounding prevents infeasible tool executions and returns structured failure feedback (U_c) that the LLM uses to revise plans; LLM-based expansion + critique in MCTS provides powerful heuristics that reduce search effort; perception/tool outputs are explicitly ingested into a structured belief (scene graph + text history), enabling closed-loop recovery and higher task completion relative to LLM-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e804.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e804.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/framework approach that interleaves chain-of-thought style reasoning with external actions (tool calls), enabling LLMs to both reason and act in environments by issuing tools and incorporating their observations into subsequent reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct (baseline, adapted for embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>In this paper ReAct is used as a competitive baseline adapted to embodied object-rearrangement tasks; it is an LLM prompting pattern that alternates textual reasoning steps and action/tool invocations. The paper uses ReAct variants run with an 8B LLM and an evaluation using a much larger 70B LLM (as a positive control). No internal implementation details for the embodied adaptation are specified beyond its use as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2Thor (simulated object-rearrangement benchmark used for comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same simulated kitchen environments (AI2Thor) and randomized room layouts used to evaluate embodied object-rearrangement tasks; agent has egocentric/partial observations and must discover and manipulate objects.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Not specified in detail in this paper for the baseline runs; described generically as LLM with tool-calling behavior adapted to embodied actions (perception/action tool wrappers were used for embodied evaluation but specific list and integration details are not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Not detailed in the paper for the baseline; presumably textual tool-return observations and action success/failure signals when adapted for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Interleaved LLM reasoning and action (ReAct pattern) — not tree-search based; generates next actions via LLM prompts and reacts to tool outputs in subsequent prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported as baselines on the easy object-rearrangement benchmark: ReAct with an 8B LLM achieved 10.26% task completion; ReAct with a 70B LLM (positive control) achieved 22.5% task completion (percent of tasks completed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>As evaluated in this paper, ReAct baselines (especially with smaller LLMs) underperform ConceptAgent's combination of LLM-MCTS and precondition grounding; larger LLMs improve ReAct performance but at cost of model scale, and baseline agents struggle to recover from infeasible tool actions without explicit precondition verification and structured belief updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e804.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e804.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative search method that uses LLMs to explore a tree of intermediate 'thoughts' (partial solutions) to improve problem-solving via structured branching and evaluation of candidate reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tree of Thoughts (ToT) baseline (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Treated as a baseline adapted for embodied tasks in which ToT-style deliberative branching with LLM evaluations is employed; the paper does not provide implementation specifics of the embodied adaptation, only reports comparative task completion numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2Thor (simulation benchmark used for comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Embodied simulated kitchen environments with partial observability, occlusion, concealed objects and long-horizon rearrangement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Not detailed in this paper for the adapted baseline; ToT typically leverages LLM evaluations and may use tool calls, but specific tool integration here is not described.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Deliberative LLM search over a tree of intermediate reasoning states (Tree of Thoughts); in this paper used as a baseline comparison to ConceptAgent's LLM-MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Reported baseline performance on the easy benchmark: ToT (ReAct+ToT variant with 8B LLM) achieved 8.11% task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ToT-style deliberative search as adapted for embodied tasks underperforms ConceptAgent's combined approach in this evaluation; suggests that tree search guided by LLM expansions plus explicit precondition grounding and structured belief updates can be more effective for embodied, partially observable planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e804.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e804.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method showing that language models can learn to call external tools by self-supervised training to improve downstream tasks; referenced in related work as an example of LLM-tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Toolformer (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as an example of augmenting LLMs with external tool usage capability; no experiments or integration details for Toolformer are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of external-tool augmentation for LLMs; no additional details or embodied evaluations provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 1)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. <em>(Rating: 2)</em></li>
                <li>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. <em>(Rating: 2)</em></li>
                <li>Pddl planning with pretrained large language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-804",
    "paper_id": "paper-273228575",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "ConceptAgent",
            "name_full": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
            "brief_description": "A closed-loop, natural-language-driven embodied agent that integrates a dynamic language-aligned 3D scene graph, parametric robot skills, LLM-guided Monte Carlo Tree Search (LLM-MCTS), and LLM-derived formal precondition grounding to plan, verify, and execute long-horizon tasks in open-world environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ConceptAgent",
            "agent_description": "An embodied agent combining (1) perception: real-time language-aligned 3D scene graph built from SAM + CLIP instance segmentation and ZED stereo point clouds, (2) parametric skills library (object localization, navigation, grasping, manipulation, VQA, speech, code execution, web search), (3) planning: LLM-guided Monte Carlo Tree Search (LLM-MCTS) using UCB1 selection, LLM-based expansion that proposes plausible actions A_t = L(s_t,g), and an LLM critic that scores whole action sequences replacing random rollouts, and (4) formal Precondition Grounding where the LLM generates PDDL-style preconditions P_i for each tool/action and a formal verifier F(s_t,P_c) checks feasibility prior to execution; unsatisfied preconditions U_c are returned to the LLM as explicit feedback for replanning.",
            "environment_name": "AI2Thor (simulation) and real-world mobile manipulation trials (Boston Dynamics Spot)",
            "environment_description": "Embodied 3D environments (AI2Thor kitchens in simulation and randomized low-clutter real-world rooms) where the agent has limited, egocentric sensing (camera/depth/pointcloud), must explore to discover objects, and perform long-horizon manipulation/navigation tasks — i.e., partially observable and dynamic with occlusions, concealed objects, and changing clutter.",
            "is_partially_observable": true,
            "external_tools_used": "Parametric robot skills (object localization, navigation via ROS2 Nav2, grasping/manipulation), perception modules (Segment Anything Model (SAM), CLIP embeddings), ZED stereo camera producing point clouds, ConceptGraphs (language-aligned 3D scene graph) service, code execution (PythonREPL), web search (SerpAPI), speech recognition/synthesis (Nvidia Riva), and LLMs (e.g., Llama variants used for planning/critique/precondition generation).",
            "tool_output_types": "Structured 3D scene graph updates, 3D point clouds, image masks, CLIP vector embeddings, textual observations and tool-call return strings (lists of discovered objects, success/failure messages), boolean precondition satisfaction flags, and scalar critique/planning scores produced by the LLM critic.",
            "belief_state_mechanism": "A dynamic, language-aligned 3D scene graph plus a textual task history and current-state text description: the scene graph stores object instances, 3D bounding boxes, CLIP embeddings and attributes; the agent state s_t is composed of the task objective text, task-relevant observations, and action history. This combined structured graph + textual memory serves as the agent's belief representation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Perception tools (SAM+CLIP+ZED) continuously update the 3D scene graph and CLIP embeddings in real time; tool call results (e.g., lists of discovered objects, success/failure messages) are appended to the textual task history and used to update s_t. When a proposed action fails precondition verification, the formal verifier computes U_c (unsatisfied preconditions) which are formatted and returned to the LLM; the LLM internalizes this feedback to revise future plans. LLM-based critique scores from tree-search leaf evaluations are backpropagated into the MCTS value estimates, updating planning priorities.",
            "planning_approach": "LLM-guided Monte Carlo Tree Search (LLM-MCTS) with UCB1 selection, LLM-based expansion that proposes plausible next actions conditioned on state and goal, LLM-based critique that evaluates entire action sequences (serves as rollout value), and formal precondition verification (LLM-derived predicates checked by F(s_t,P_c)) to filter and provide closed-loop feedback.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Waypoint navigation implemented with ROS2 Nav2 using real-time point clouds from a ZED stereo camera to populate a local costmap; follows predefined waypoints and adapts reactively to obstacle updates (Nav2 global/local planners used as provided; specific underlying global planner not specified).",
            "performance_with_tools": "Simulation (easy object-rearrangement): ConceptAgent achieved 13.89% task completion with 10 expansions and 18.92% with 20 expansions on the easy benchmark (30 tasks across 3 floorplans). Ablation/complex benchmarks: when using both Precondition Grounding (PG) and LLM-MCTS, ConceptAgent achieved 35% success on moderate tasks and 15% on hard tasks (40 task benchmark). Real-world Spot trials: up to 40% task completion after decluttering (30 mobile manipulation trials reported; overall physical-task success rate ~20%). All numbers are percent task completion on the indicated benchmarks.",
            "performance_without_tools": "Baselines and ablations: ReAct and ToT baselines (8B LLM) achieved 10.26% and 8.11% respectively on the easy benchmark. ConceptAgent without Precondition Grounding or LLM-MCTS completed 5% of moderate and 5% of hard tasks (ablation).",
            "has_tool_ablation": true,
            "key_findings": "Integrating LLM-guided tree search with an explicit, LLM-derived precondition verification loop and a dynamic language-aligned 3D scene graph materially improves embodied planning robustness: precondition grounding prevents infeasible tool executions and returns structured failure feedback (U_c) that the LLM uses to revise plans; LLM-based expansion + critique in MCTS provides powerful heuristics that reduce search effort; perception/tool outputs are explicitly ingested into a structured belief (scene graph + text history), enabling closed-loop recovery and higher task completion relative to LLM-only baselines.",
            "uuid": "e804.0",
            "source_info": {
                "paper_title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompting/framework approach that interleaves chain-of-thought style reasoning with external actions (tool calls), enabling LLMs to both reason and act in environments by issuing tools and incorporating their observations into subsequent reasoning.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "use",
            "agent_name": "ReAct (baseline, adapted for embodied tasks)",
            "agent_description": "In this paper ReAct is used as a competitive baseline adapted to embodied object-rearrangement tasks; it is an LLM prompting pattern that alternates textual reasoning steps and action/tool invocations. The paper uses ReAct variants run with an 8B LLM and an evaluation using a much larger 70B LLM (as a positive control). No internal implementation details for the embodied adaptation are specified beyond its use as a baseline.",
            "environment_name": "AI2Thor (simulated object-rearrangement benchmark used for comparisons)",
            "environment_description": "Same simulated kitchen environments (AI2Thor) and randomized room layouts used to evaluate embodied object-rearrangement tasks; agent has egocentric/partial observations and must discover and manipulate objects.",
            "is_partially_observable": true,
            "external_tools_used": "Not specified in detail in this paper for the baseline runs; described generically as LLM with tool-calling behavior adapted to embodied actions (perception/action tool wrappers were used for embodied evaluation but specific list and integration details are not provided).",
            "tool_output_types": "Not detailed in the paper for the baseline; presumably textual tool-return observations and action success/failure signals when adapted for embodied tasks.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Interleaved LLM reasoning and action (ReAct pattern) — not tree-search based; generates next actions via LLM prompts and reacts to tool outputs in subsequent prompt context.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": "Reported as baselines on the easy object-rearrangement benchmark: ReAct with an 8B LLM achieved 10.26% task completion; ReAct with a 70B LLM (positive control) achieved 22.5% task completion (percent of tasks completed).",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "As evaluated in this paper, ReAct baselines (especially with smaller LLMs) underperform ConceptAgent's combination of LLM-MCTS and precondition grounding; larger LLMs improve ReAct performance but at cost of model scale, and baseline agents struggle to recover from infeasible tool actions without explicit precondition verification and structured belief updates.",
            "uuid": "e804.1",
            "source_info": {
                "paper_title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts (ToT)",
            "brief_description": "A deliberative search method that uses LLMs to explore a tree of intermediate 'thoughts' (partial solutions) to improve problem-solving via structured branching and evaluation of candidate reasoning trajectories.",
            "citation_title": "Tree of Thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "use",
            "agent_name": "Tree of Thoughts (ToT) baseline (adapted)",
            "agent_description": "Treated as a baseline adapted for embodied tasks in which ToT-style deliberative branching with LLM evaluations is employed; the paper does not provide implementation specifics of the embodied adaptation, only reports comparative task completion numbers.",
            "environment_name": "AI2Thor (simulation benchmark used for comparisons)",
            "environment_description": "Embodied simulated kitchen environments with partial observability, occlusion, concealed objects and long-horizon rearrangement tasks.",
            "is_partially_observable": true,
            "external_tools_used": "Not detailed in this paper for the adapted baseline; ToT typically leverages LLM evaluations and may use tool calls, but specific tool integration here is not described.",
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Deliberative LLM search over a tree of intermediate reasoning states (Tree of Thoughts); in this paper used as a baseline comparison to ConceptAgent's LLM-MCTS.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": "Reported baseline performance on the easy benchmark: ToT (ReAct+ToT variant with 8B LLM) achieved 8.11% task completion.",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "ToT-style deliberative search as adapted for embodied tasks underperforms ConceptAgent's combined approach in this evaluation; suggests that tree search guided by LLM expansions plus explicit precondition grounding and structured belief updates can be more effective for embodied, partially observable planning.",
            "uuid": "e804.2",
            "source_info": {
                "paper_title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer: Language models can teach themselves to use tools",
            "brief_description": "A method showing that language models can learn to call external tools by self-supervised training to improve downstream tasks; referenced in related work as an example of LLM-tool integration.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools.",
            "mention_or_use": "mention",
            "agent_name": "Toolformer (mentioned)",
            "agent_description": "Mentioned in related work as an example of augmenting LLMs with external tool usage capability; no experiments or integration details for Toolformer are provided in this paper.",
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as an example of external-tool augmentation for LLMs; no additional details or embodied evaluations provided in this paper.",
            "uuid": "e804.3",
            "source_info": {
                "paper_title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Sayplan: Grounding large language models using 3d scene graphs for scalable task planning.",
            "rating": 2,
            "sanitized_title": "sayplan_grounding_large_language_models_using_3d_scene_graphs_for_scalable_task_planning"
        },
        {
            "paper_title": "Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning.",
            "rating": 2,
            "sanitized_title": "conceptgraphs_openvocabulary_3d_scene_graphs_for_perception_and_planning"
        },
        {
            "paper_title": "Pddl planning with pretrained large language models.",
            "rating": 2,
            "sanitized_title": "pddl_planning_with_pretrained_large_language_models"
        }
    ],
    "cost": 0.01666125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution
8 Oct 2024</p>
<p>Corban Rivera 
Grayson Byrd 
William Paul 
Tyler Feldman 
Meghan Booker 
Emma Holmes 
David Handelman 
Bethany Kemp 
Andrew Badger 
Aurora Schmidt 
Krishna Murthy Jatavallabhula 
Celso M De Melo 
Lalithkumar Seenivasan 
Mathias Unberath 
Rama Chellappa 
Jhu Apl 
Jhu 
Mit 
Devcom Arl 
ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution
8 Oct 20245B32B5ED329BA3F93EEBA42F1D5645C2arXiv:2410.06108v1[cs.AI]
Robotic planning and execution in open-world environments is a complex problem due to the vast state spaces and high variability of task embodiment.Recent advances in perception algorithms, combined with Large Language Models (LLMs) for planning, offer promising solutions to these challenges, as the common sense reasoning capabilities of LLMs provide a strong heuristic for efficiently searching the action space.However, prior work fails to address the possibility of hallucinations from LLMs, which results in failures to execute the planned actions largely due to logical fallacies at highor low-levels.To contend with automation failure due to such hallucinations, we introduce ConceptAgent, a natural languagedriven robotic platform designed for task execution in unstructured environments.With a focus on scalability and reliability of LLM-based planning in complex state and action spaces, we present innovations designed to limit these shortcomings, including 1) Predicate Grounding to prevent and recover from infeasible actions, and 2) an embodied version of LLM-guided Monte Carlo Tree Search with self reflection.ConceptAgent combines these planning enhancements with dynamic language aligned 3d scene graphs, and large multi-modal pretrained models to perceive, localize, and interact with its environment, enabling reliable task completion.In simulation experiments, ConceptAgent achieved a 19% task completion rate across three room layouts and 30 easy level embodied tasks outperforming other state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on the same benchmark.Additionally, ablation studies on moderate to hard embodied tasks revealed a 20% increase in task completion from the baseline agent to the fully enhanced ConceptAgent, highlighting the individual and combined contributions of Predicate Grounding and LLM-guided Tree Search to enable more robust automation in complex state and action spaces.Additionally, in realworld mobile manipulation trials, conducted in randomized, low-clutter environments, a ConceptAgent-driven Spot robot achieved a 40% task completion rate, demonstrating the performance of our perception system in real-world scenarios.</p>
<p>I. INTRODUCTION</p>
<p>The aspiration to have competent robotic counterparts that can reason about the world in an open manner and be commanded with natural language has been a long term goal of the robotics community.Recent advancements leveraging data-driven methodologies and robust robot models have showcased notable strides [1]- [4].However, existing systems often exhibit fragility, rigidity, and struggle when confronted with unforeseen circumstances.Many of these systems rely on being given a prescanned environment which limits their use to well known environments [5], [6].Even the most †Project Lead *Equal Contribution expansive robotic models typically function optimally only within familiar settings [7], [8].This fragility is particularly evident in scenarios with limited available robotic data, such as unstructured domestic environments.This lack of adaptability stands in stark contrast to the remarkable capabilities demonstrated by large-scale vision models, which excel in tasks such as semantic comprehension, object detection, and bridging visual and linguistic representations [5], [9]- [15].</p>
<p>Despite the maturity of fundamental robotic skills like navigation, grasping, and object manipulation [16]- [19], the integration of modern vision models with these foundational capabilities often yields underwhelming results.This challenge was underscored by the NeurIPS 2023 OVMM challenge, where the winning solution achieved a success rate of only 33% [20], [21].The complexity arises from a multitude of factors rather than a singular obstacle.Successful completion of general, long-horizon, embodied tasks requires intelligently generating a sequence of discrete actions, each of which are non-trivial both in their implementation and their execution.Each of these actions has the potential to accumulate and propagate error.Additionally, to create an actionable plan and recover when that plan goes awry requires a competent abstraction of the physical environment as well as a planner capable of fully leveraging that abstraction.Addressing these challenges necessitates a nuanced framework that seamlessly integrates natural language comprehension, full scene abstraction and understanding, and resilient reasoning.</p>
<p>In response to these challenges, we introduce ConceptAgent, a natural language-driven robotic platform designed for task execution in unstructured settings.ConceptAgent integrates advanced perception, manipulation, movement, speech, coding, and search primitives, allowing it to tackle a wide variety of tasks described in natural language.At its core, ConceptAgent combines large multi-modal pretrained models with novel planning and reasoning enhancements, enabling robust task execution in complex, dynamic environments.</p>
<p>Key innovations of ConceptAgent include:</p>
<p>• Formal Precondition Verification: ConceptAgent integrates precondition grounding, a mechanism that formally verifies action constraints before execution, preventing infeasible actions and facilitating failure recovery.This ensures the agent can maintain task progress, even in unstructured environments.The problem requires the robot to operate in unfamiliar settings and manipulate novel objects to complete tasks described in unconstrained natural language.In this escape room motivated example, the task given is to "unlock the door".The robot must not only identify objects but (a) understand the context of the scene including (b) a hand written note on the door with additional instructions.(c) The ConceptAgent-driven Spot robot then proceeds to complete the task successfully without intervention.</p>
<p>• LLM-Guided Monte Carlo Tree Search (LLM-MCTS): ConceptAgent employs LLM-guided tree search with self-reflection, enabling the agent to explore future states and refine action sequences dynamically.This approach significantly improves planning efficiency and task completion rates, even in large, open-world state spaces.To rigorously evaluate ConceptAgent, we conducted a series of experiments in both simulated and real-world environments.In simulation, we used the AI2Thor embodied agent simulator [22] to benchmark performance across a diverse set of tasks and environments, allowing for the controlled assessment of our approach.Additionally, we tested Concep-tAgent in real-world mobile manipulation tasks, where the robot navigated unfamiliar environments and interacted with novel objects.In a collection of 30 trials in randomized low clutter environments, we observed that the ConceptAgentdriven Spot robot completed %40 of tasks successfully.</p>
<p>Our experiments yielded the following key findings:</p>
<p>• Precondition Grounding: Formal precondition verification significantly enhances the agent's ability to recover from failure, leading to higher task completion rates and more reliable execution.• LLM-Guided Tree Search and Reflection: By integrating LLM-guided tree search with closed-loop task execution and self-critique, ConceptAgent effectively refines its action sequences, achieving competitive task completion performance with far fewer expansion steps than traditional LLM-based systems.</p>
<p>II. RELATED WORK Traditional Robotic Task Planning -Task planning in robotics involves finding a sequence of actions to achieve specific goals within an environment.Traditional methods make use of domain-specific languages like PDDL [23], [24] and finite state machines [6], along with grammars and semantic parsing [25], search techniques [26], and heuristics [27] to find solutions.These approaches can struggle with scalability and adaptability to real-world scenarios due to their incredibly large state-action space.Hierarchical, imitation, and reinforcement learning-based alternatives face challenges related to data demands and scalability [28], [29].Our approach leverages the common sense capabilities of Large Language Models (LLMs) to sidestep the brute force search problem associated with generating task plans in unconstrained environments.Leveraging realtime incremental 3D scene graphs for grounding [5] and large pretrained multimodal models [30], our approach can formulate execution paths that were not preconceived, resulting in a capability to adapt and use affordances in ways that were not explicitly encoded by humans.</p>
<p>Task Planning with Large Language Models -Task planning with LLMs, specifically translating natural language instructions into robotic task plans, is gaining traction in the field.Previous studies have effectively employed pretrained LLMs' contextual learning abilities to generate actionable plans for embodied agents [2], [31]- [36].However, a significant challenge remains in grounding these plans within the operational context of the robot.Past efforts make use of object detection models [32], PDDL environment representations [31], [37], or value functions [2] for this purpose, but they are primarily effective in single-room environments and struggle with scalability.In our work, we improve scalability by re-imagining ConceptGraphs [5] as a service with limited scope.This allows the agent to select only the portions of the graph that are most relevant to a given task.</p>
<p>Retrieval Augmented Generation for Task Planning -Integrating external knowledge into LLMs has emerged as a promising approach to enhance their reliability.This involves using external tools to provide feedback or additional information to guide LLM output generation.Leveraging external knowledge in tool-based agents requires API calls to external tools [38], [39] or textual feedback from the operating environment [33], [40], [41].Building on these concepts, we introduce an agent-based framework that reacts and replans based on the current observations to complete tasks specified in natural language.</p>
<p>III. CONCEPTAGENT</p>
<p>ConceptAgent enables natural language-driven task planning and execution.Figure 2 illustrates the framework.The system responds to natural language task requests, plans and executes in a closed loop over a series of steps.Intermediate feedback in the form of abstracted environment observations and information about unsatisfied action constraints (Precondition Grounding) allows the robot to adapt and recover in real-time.</p>
<p>A. Problem Formulation</p>
<p>We address the problem of real-time task execution in open-world environments driven by unconstrained natural language goals.In these scenarios, the key challenge lies in enabling a robotic system to understand and act upon complex, context-dependent language instructions, while simultaneously adapting to dynamic and unstructured environments.</p>
<p>The system must continuously interpret natural language expressions without predefined constraints, translating them into actionable plans in real-time.Full scene understanding is achieved through a combination of an incrementally-updating language-aligned 3d scene graph [5] and large pretrained multi-modal models.</p>
<p>Given the open-world nature of the problem, the system must demonstrate the ability to generalize across varying environments and manage unknown objects, unforeseen obstacles, and unexpected environmental changes.The primary metric for success is task completion rate, underscoring the importance of reliable, autonomous decision-making in diverse and unpredictable contexts.</p>
<p>B. LLM-guided Tree Search with Self-Reflection</p>
<p>Inspired by [42], ConceptAgent agent extends prior work in LLM-guided tree search to the real world domain.LLMguided tree search follows the standard phases of traditional MCTS: selection, expansion, simulation, and backpropagation, but introduces LLM-based enhancements to expansion and simulation.Below, we detail each phase, emphasizing how the LLM guides planning and decision-making.</p>
<p>1) Selection Based on Upper Confidence Bound: The selection phase is based on the Upper Confidence Bound (UCB1) algorithm [43], which balances exploration and exploitation in selecting the next node to explore.For a node s t with child nodes a i , the UCB1 criterion is used to select the action that maximizes the following equation:
a * = arg max ai Q(s t , a i ) + c • log N (s t ) N (s t , a i )
where Q(s t , a i ) is the expected reward of taking action a i at state s t , N (s t ) is the number of times state s t has been visited, N (s t , a i ) is the number of times action a i has been taken from s t , and c is a constant controlling the explorationexploitation trade-off.</p>
<p>2) LLM-Guided Expansion: In the expansion phase, traditional MCTS adds new child nodes representing possible future states.For embodied tasks, the state-action space explodes at even small depths in the tree search, requiring a powerful heuristic to filter out irrelevant states [42].In ConceptAgent, we use the common sense of LLMs to serve as this heuristic.Given a current state s t and a natural language task goal g, the LLM generates a set of plausible actions A t = L(s t , g).These actions are then added to the tree as new nodes, representing potential transitions from s t .This LLM-guided expansion allows the agent to explore a broader range of actions that align with the natural language task, thus avoiding state space explosion and improving task understanding and goal relevance.</p>
<p>3) LLM-Based Critique for Simulation and Scoring: In the traditional MCTS framework, the simulation phase evaluates state transitions through random rollouts in the environment that backpropogate the value of the terminal state of that random rollout.For ConceptAgent, we replace this with a LLM-based critique mechanism that assesses a planned sequence of actions holistically.At the leaf node of the search tree, rather than simulating state transitions, the LLM critic is given the sequence of actions planned from time t to t + k.The LLM evaluates the efficiency, relevance, and goal alignment of the entire plan, producing a planning score that serves as the reward signal for backpropagation.</p>
<p>Formally, let τ = {a t , a t+1 , . . ., a t+k } be the sequence of planned actions from state s t over k time steps.The LLM critic, L, is tasked with evaluating the quality of this action sequence in achieving the task goal g.The critique score C(τ, g) reflects how well the action sequence satisfies the task requirements and the efficiency of the plan.The score incorporates penalties for unnecessary steps or inefficient actions.</p>
<p>4) Backpropagation: Once the critique score C(s t+k , g) is obtained, the backpropagation phase proceeds as in standard MCTS.The critique score is propagated back through the tree, updating the value estimates Q(s, a) for each stateaction pair along the trajectory τ .The update rule for each node is as follows:
Q(s t , a i ) ← 1 N (s t , a i ) N (st,ai) j=1 C(τ j , g),
where N (s t , a i ) is the number of times the action a i has been selected from state s t , and C(τ j , g) is the critique score from the j-th simulation.This ensures that the agent's planning decisions reflect the accumulated knowledge from both successful and unsuccessful action sequences.</p>
<p>C. Precondition Grounding</p>
<p>Once MCTS with LLM-guided expansion and critiquebased scoring identifies the optimal action sequence, Concep-tAgent attempts to execute the chosen next action.If the tool execution fails, an agent must contextualize the failure with respect to the planning history to determine proper recovery measures.For example, upon failing to search inside a cabinet, the agent must analyze its previous actions and determine a reason for the execution failure.In this case, there are many possible reasons: the cabinet is not open, the agent has not moved within reach of the cabinet, the agent is currently holding an object and cannot execute the search tool due to manipulator constraints, etc.Through experiments, we find the baseline agent struggles to extrapolate such broad conclusions from the action history.Therefore, to improve agent recovery capabilities, we introduce Precondition Grounding as an enhancement to our LLM planner.The precondition grounding is autonomously generated, promoting scalability, and enables formal validation and recovery feedback during tool execution, greatly improving task completion.</p>
<p>1) LLM-Derived Preconditions for Action Feasibility:</p>
<p>Precondition Grounding requires a defined set of preconditions for each tool.At inference time, these preconditions are verified by formal methods to ensure that a chosen tool can be executed given the perceived state of the environment.The creation of such preconditions generally lacks scalability [23], [24], but recent work has explored the ability of LLMs for precondition generation [44].Extending this idea, we prompt the agent to generate predicate based preconditions for each tool.</p>
<p>Formally, these preconditions encode the logical requirements for successful tool execution.Let A = {a 1 , ..., a n } represent the set of n actions available to the agent.Then, P i = LLM (S, n i , d i , atts) is the set of preconditions for a i generated by conditioning the LLM on the system prompt, S, action name, n i , action description, d i , and list of object and agent boolean attributes, atts.At time, t, we define the Formal Verification function, F (s t , P c ), as:
F (s t , P c ) = 1, if P c is satisfied in s t 0, if P c is not satisfied in s t
Where c represents the index of the chosen action in A and s t represents the state of the system at time, t.If F (s t , P c ) = 1, then the action, a c , is executed.If F (s t , P c ) = 0, then the action is deemed infeasible and execution is aborted.</p>
<p>2) Feedback for Future Planning: When execution is aborted due to unmet preconditions, the system provides explicit feedback to the LLM agent, specifying which preconditions were unsatisfied.</p>
<p>Let U c be the set of unsatisfied preconditions for action c in A. If F (s t , P c ) = 0, then:
U c = {p ∈ P c | p is not satisfied in s t }
The set U c is formatted and returned to the LLM as feedback, allowing the agent to update its internal model of the environment and adjust future action sequences to avoid proposing actions whose preconditions are unlikely to be met.This iterative feedback loop refines the agent's planning process, improving task completion rates over time by preventing the repetition of infeasible actions and guiding the agent towards valid recovery plans.Additionally, using feedback from U c , the LLM revises its future plans, internalizing the state feedback to adjust the sequence of future actions.The agent can either attempt to satisfy the unsatisfied preconditions or generate alternative actions that are feasible given the current state s t .This mechanism ensures that ConceptAgent's action plans are dynamically adapted to the environment.</p>
<p>D. Implementation Details</p>
<p>ConceptAgent make use of a library of parametric skills to complete tasks.In physical experiments these skills include object localization, navigation, grasping, manipulation, visual question answering, speech, code execution, and web search.Due to space constraints, we elaborate on these skills in the Appendix.</p>
<p>IV. RESULTS</p>
<p>The goals of our experiments were twofold: first, to evaluate the effectiveness of ConceptAgent's planning and reasoning enhancements, including formal precondition verification and LLM-guided Monte Carlo Tree Search, in both simulated and real-world environments; and second, to assess its adaptability and task completion performance in diverse and unstructured scenarios, benchmarked against state-ofthe-art generative AI reasoning approaches.</p>
<p>A. Task Planning and Execution in Simulation</p>
<p>Our simulation experiments were run in the AI2Thor simulator, a 3D simulator for kitchen environments that contains a variety of manipulable objects within the environment.To evaluate our planner, A set of 30 randomly constructed object rearrangement tasks were generated.The object rearrangement tasks were characterized by simple, clear object rearrangement instructions e.g."The agent should pick up the credit card that is on the counter and place it on the kitchen drawer".An additional set of 40 tasks were hand crafted by a human expert.The tasks are split into 20 moderate tasks and 20 hard tasks.Moderate tasks are characterized by simple, object rearrangement instructions where task objects may be concealed within other objects e.g."Put a Tomato in a Cabinet.".The tomato may initially be in the refrigerator requiring additional exploration and manipulation.Difficult tasks involve longer horizon goal states and increased ambiguity in the task statement e.g."Chill the tomato and put the bowl away in the cabinet.".</p>
<p>We first evaluate our agent's ability to autonomously generate preconditions for each provided action.To do this, a human expert first generated a set of ground truth preconditions for each possible agent action.We then compare the generated preconditions for each tool to that of the ground truth preconditions.The ground truth set contained 42 preconditions spanning across 10 different tools while the LLM generated set contained 38.We compared each precondition in the generated set to the ground truth set and noted 37 out of the 38 generated preconditions correctly matched to the 42 ground truth preconditions.We observe 6 ground truth preconditions were missing from the generated set.This represents an accuracy (correct generated / total generated) of 97.4% and a recall (correct generated / total ground truth) of 88.1%.Although 6 ground truth preconditions were missing from the generated set, this will be no worse than the baseline agent without any verification mechanism.In short, only 2.6% of the LLM generated preconditions are incorrect and thus have real potential to cause harm to planning beyond that of the baseline agents without enhancements.We further evaluate the LLM generated preconditions empirically through their impact on planning performance in Table I.</p>
<p>B. Comparison to Baselines</p>
<p>We compared our approach to several state-of-the-art closed-loop generative AI-based reasoning baselines adapted for embodied tasks.The approaches were evaluated with 30 object rearrangement benchmark tasks across 3 room layouts.Specifically, we evaluated the performance of these reasoning approaches alongside multiple levels of expansion in ConceptAgent.As a positive control, we included results from a baseline approach that utilized an LLM nearly 10 times larger than those used in other methods.</p>
<p>Our results, summarized in Table II, show that the baseline methods, including ReAct [34] and Tree of Thoughts (ToT) [45], both using an 8-billion-parameter LLM, achieved task completion rates of 10% and 8%, respectively.In comparison, ConceptAgent, with 10 and 20 expansions, achieved task completion rates of 14% and 19%, respectively.These results highlight the effectiveness of our approach, demonstrating a clear improvement in task performance with additional ConceptAgent expansions.</p>
<p>Notably, with 20 expansions, ConceptAgent's performance (19% task completion) approached that of the ReAct baseline, which relied on an LLM nearly 10 times larger.This illustrates the efficiency of ConceptAgent in leveraging smaller models while still achieving competitive results through improved planning and reasoning mechanisms.</p>
<p>C. Ablation Experiments</p>
<p>Next, we explored ablations of ConceptAgent on the moderate and hard benchmarks.In these experiments we ablate the two key planning enhancements: Precondition Grounding and LLM-MCTS.The results shown in Table I report task success rate separately for the moderate and hard task benchmarks along with an overall average over the 40 tasks.</p>
<p>Without predicte grounding or LLM-driven MCTS, Con-ceptAgent completes 5% of the moderate and 5% of the hard tasks.When combined with precondition grounding, we find that ConceptAgent was able to complete 15% of the moderate tasks and 10% of the hard tasks successfully.This shows that, while Llama3.1 70B tends to choose tools under invalid execution constraints, it is able to recover from these planning errors given the appropriate feedback in the form of unsatisfied preconditions.LLM-MCTS was evaluated with 5 expansion steps in the ablation experiments.When ConceptAgent includes LLM-driven MCTS and precondition grounding we observed the highest task completion on both the moderate and hard benchmark tasks achieving 35% and 15% respectively.</p>
<p>D. Physical Trial for Object Rearrangement</p>
<p>In these trials, we aimed to characterize failure modes in physical mobile manipulation experiments.To benchmark the system's performance, we began with object rearrangement tasks, modeled after of experiments of prior work [6], Fig. 3: Evaluation of Physical Mobile Manipulation for Open Vocabulary Object Rearrangement -Object rearrangement success and failure cases aggregated over all levels of clutter, broken down by mode of failure / success.From left to right, we show the performance of the system to locate the object for object rearrangement, navigate to it, perceive it, grasp it, locate the destination, navigate to the destination, and place the object into the receptacle.[46]- [48].These tasks are formulated as "pick up object X and place it on/in object Y".We ran 30 iterations of these tasks, selecting objects and their destinations at random.Every 5 iterations, the scene layout would be randomized, and every 10 iterations, the scene would be decluttered like prior work [6], where for each declutter procedure, we select the select 1/3 of the objects that showed difficulty for the system in terms of semantic ambiguity or grasping difficulty and relocate the remaining objects to accessible locations.In these trials, ConceptAgent was evaluated with GPT-   Although the open vocabulary step-wise success rates (SSR) were relatively high (Table III), completing the task requires successfully executing multiple sequential steps.As such, the overall task success rate is the product of the stepwise success rates, resulting in a total success rate of 20%.This occurred despite the system generating highly accurate plans in each trial and achieving 100% successful navigation.</p>
<p>Table IV presents the overall task success rates under different levels of clutter.After decluttering, we observed an increase in success rate from 10% to 40%, highlighting that both visual clutter and semantic ambiguity are critical factors affecting task performance.Figure 3 illustrates the step-wise and overall success rates in a Sankey diagram, mapping the primary failure modes observed during the trials.</p>
<p>V. CONCLUSIONS</p>
<p>Large multi-modal pretrained models are essential for enhancing the robustness of robotic plan execution in open-world environments.These models enable ConceptAgent to integrate perception and reasoning, allowing for more accurate and adaptable task execution in diverse, unstructured scenarios.</p>
<p>APPENDIX VII. SYSTEM PROMPT FOR GENERATING PRECONDITIONS</p>
<p>We give our agent a brief description of the task of generating preconditions using the PDDL format.PDDL was chosen as this is a commonly used domain language for generating predicate based preconditions and effects, thus it was natural to ask the LLM agent in this format as the agent has likely seen PDDL format extensively in its training data.</p>
<p>You are to generate definitions for a robotic action listed below such that the preconditions required to perform the action successfully and effects that occur when the action is performed successfully are appropriately described.This should be specified in PDDL format using predicates to specify the preconditions and effects.</p>
<p>The action information will be provided in the following JSON format: {{"Action Name": str, "Action Description": str}} We continue the prompt by providing the agent with a list of boolean predicates that it can choose from.These predicates are a list of object and agent attributes that represent the state of each object and the agent in the environment.The agent must choose between these predicates when defining preconditions for actions.</p>
<p>New predicates can be created, but they MUST inherit from the starting predicates below.</p>
<p>(moveable ?x -objectId): true if the object is moveable (breakable ?x -objectId): true if the object is breakable (canFillWithLiquid ?x -objectId): true if the object can be filled with liquid (isToggled ?x -objectId): true if the object is toggled on (pickupable ?x -objectId): true if the object is able to be picked up (isOpen ?x -objectId): true if the object is open (isBroken ?x -objectId): true if the object is broken (visible ?x -objectId): true if the object is visible (receptacle ?x -objectId): true if the object is a receptacle (openable ?x -objectId): true if the object is able to be opened (isPickedUp ?x -objectId): true if the object is picked up (toggleable ?x -objectId): true if the object is able to be toggled (isFilledWithLiquid ?x -objectId): true if the object is filled with liquid (cookable ?x -objectId): true if the object is able to be cooked (isCooked ?x -objectId): true if the object is cooked (isWaterSource ?x -objectId): true if the object is a water source (isHoldingObject): true if the agent is holding an object We continue the prompt by providing the agent with a description of 3 logical modifiers: 'and', 'exists', and 'when'.The 'and' modifier is true if and only if each precondition nested inside it is satisfied, the 'exists' modifier is true if there is at least one object in the environment for which all of the nested preconditions are satisfied, and the 'when' modifier is true if 1) the first condition in the modifier is False 2) the first condition in the modifier is true and the second condition in the modifier is also true.This is essentially an if/then modifier stating: if the first condition is true, then the second condition must also be true.We find that these three modifiers allow for a diverse set of preconditions that can satisfy a variety of complex environmental constraints for each action.The system prompt terminates with an example set of precondition generation task for one of the agent tools before prompting the agent to 'Begin!'.We repeat this prompt for each action in the set of actions provided to the agent where the action information is a parameter to the prompt providing the action name and description.(not (isHoldingObject)) ) ``J ustification: It makes sense that in order to search an object, that object likely must first be visible.Also, in order to search an object like a cabinet or a refrigerator, the object must first be open so that you can see the inside.But for other objects, like a countertop, the object is not openable, so the isOpen predicate should only apply to those objects that are openable.Also, in order to search an object I must have a free hand, so I should not be holding anything.For the above reasons, the choice of preconditions is justified.</p>
<p>XIII. IMPLEMENTATION DETAILS FOR ROBOTIC SKILLS</p>
<p>A. Real-time 3d Scene Graphs</p>
<p>The perception system maintains an abstraction of the physical environment through a dynamic 3d scene graph that updates in real-time.Inspired by [5], our perception pipeline leverages several off-the-shelf computer vision models to accurately segment, embed, and project objects into a 3 dimensional pointcloud where open vocabulary object retrieval can be performed.A significant shortcoming of prior work required pre-scanning of the environment and offline analysis before queries could be executed.This severely limits the ability of an embodied agent, as exploration and discovery of the environment are critical to the completion of open ended, general tasks.To circumvent this limitation, our approach introduces a fast combination of Segment Anything (SAM) (everything mode) and CLIP for instance segmentation and image embedding in real-time.The bounding boxes of can-didate objects are then used as crops which are then passed to SAM to extract only the pixels in the image related to the candidate object.Next, CLIP is used to get an embedding of the segmented candidate object which is then stored to support natural language based grounding to object locations and non-axis aligned 3d bounding boxes.We use CLIP embeddings instead of discrete object classes to enable a retrieval that is robust to nuances in the natural language query (e.g.cup vs mug) by matching input queries to candidate objects through cosine similarity between embeddings.</p>
<p>B. Waypoint Navigation</p>
<p>In this work, we implement waypoint navigation using the ROS2 Nav2 system, supported by real-time point clouds generated from a forward-facing stereoscopic ZED camera.The ZED camera captures depth information through stereo vision, producing 3D point clouds that provide detailed spatial perception of the environment.</p>
<p>The Nav2 system utilizes these point clouds for obstacle detection and localization, integrating them into the map for dynamic path planning.The robot follows a set of predefined waypoints, adjusting its path as obstacles are detected or the environment changes.The stereoscopic point clouds enhance the navigation precision by improving obstacle avoidance and route planning, particularly in complex, unstructured environments.</p>
<p>The ROS2 Nav2 stack is configured to leverage the ZED camera's real-time data stream to build a local costmap, which continuously updates as the robot progresses along its route.This allows for both reactive and deliberative path planning, making the system suitable for navigation tasks that require high spatial accuracy.</p>
<p>C. Manipulation</p>
<p>For object manipulation, we leverage SAM [15] (Segment Anything Model) in "everything mode" and CLIP to reidentify the query object in the robot's gripper camera frame.SAM generates an image mask for the target object, and a grasp point is automatically selected within the identified mask.The robot then performs visual servoing, combined with inverse kinematics, to compute and achieve the precise 6-DoF grasp position and orientation using Boston Dynamics APIs.The manipulation pipeline is divided into separate skills for grasping and placing, ensuring robust execution of complex tasks involving object interaction in dynamic environments.This approach allows for accurate, real-time manipulation of objects with minimal manual intervention.</p>
<p>D. Additional Skills</p>
<p>Fig. 1 :
1
Fig.1: ConceptAgent enables robust real-time, natural language-driven task execution in open-world environments.The problem requires the robot to operate in unfamiliar settings and manipulate novel objects to complete tasks described in unconstrained natural language.In this escape room motivated example, the task given is to "unlock the door".The robot must not only identify objects but (a) understand the context of the scene including (b) a hand written note on the door with additional instructions.(c) The ConceptAgent-driven Spot robot then proceeds to complete the task successfully without intervention.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of ConceptAgent closed loop task planning and execution.State is composed of text description of the objective, task relevant observations, and task history.That is combined with the details of a parametric skills library.Tree-based planning is complemented with (b) LLM-based expansion and (c) LLM based critique and scoring.(a) Selection and (d) backpropagation are conducted like Monte-Carlo Tree Search.</p>
<p>Listing 1 :
1
Precondition Grounding System Prompt -Part 1</p>
<p>Listing 2 :
2
Precondition Grounding System Prompt -Part 2</p>
<p>Fig. 4 :
4
Fig. 4: Mobile Manipulation Trials -The trials were aimed at categorizing failure modes for physical mobile manipulation.</p>
<p>Fig. 5 :
5
Fig. 5: Example Task Completion in AI2Thor by ConceptAgent Task: Put the credit card on the counter into the kitchen drawer.(a) the agent starts in the kitchen, (b) after some exploration the agent finds the credit card, (c) the agent takes the credit card to the kitchen drawer, but it's closed, (d) the agent adapts by placing the card by the kitchen sink, before (e) moving back to the drawer and opening it.(f) the agent moves back to the sink to pick up the card again, before (g) moving back to the open drawer, and (h) placing the credit card in the drawer to complete the task.</p>
<p>Final</p>
<p>TABLE I :
IMethodExpansionsLLMTask CompletionReAct [34]N/Allama3.1 8b10.26%ReAct+ToT [45]N/Allama3.1 8b8.11%ConceptAgent [Ours]10llama3.1 8b13.89%ConceptAgent [Ours]20llama3.1 8b18.92%ReAct [34]N/Allama3.1 70b22.5%
Ablations of ConceptAgent (CA) on Moderate and Hard benchmark tasks.We report success rate for each benchmark and an overall average.In these experiments, we ablate precondition grounding (PG) and generative tree search planning (LLM-MCTS) from the ReAct baseline.LLama 3.1 70b is used for all experiments.</p>
<p>TABLE II :
II
Comparison to embodied reasoning baselines on the easy benchmark of object rearrangement tasks across multiple floorplans.We compared ConceptAgent to several state-of-the-art closed loop LLM-based reasoning baselines adapted for embodied tasks.</p>
<p>TABLE III :
III
Step-wise success rates for skills in object rearrangement tasks.
ClutterLocate Percept. Manip. Locate Place OverallHigh60%66%83%80%25%10%Medium80%88%85%33%50%10%Low80%88%100%88%57%40%Average73%81%90%68%46%20%</p>
<p>TABLE IV :
IV
Effects of environment clutter -Three different levels of clutter were considered with 10 tasks selected at random.The table reports step-wise success rates as well as overall task success rate.Reasoning and navigation was not included in the table for brevity because they were 100% successful in these trials.</p>
<p>•</p>
<p>Code execution -Code execution was supported by PythonREPL • Web Search -Web search was backed by SerpAPI • Speech Recognition and Synthesis -Nvidia Riva Task: Clear off the dining room table.Thought: To clear off the dining room table, I need to identify all objects currently on the table and then pick them up one by one.First, I'll randomly explore to find the dining room table and other objects.Action: Randomly Explore Action Input: {'input': None} Observation: Tool Completed Successfully: New objects discovered!Thought: I have discovered new objects in the environment.I should now get the list of all discovered objects to identify the dining room table and the items on it.Action: Get Discovered Objects Action Input: {'input': None} Observation: Tool Completed Successfully: Here is a list of the object ID's of all of the discovered The dining table has been identified as 'DiningTable_1'.I need to inspect it to find out what items are currently on it.
objects: ['Apple_1', 'Blinds_1', 'Bread_1', 'ButterKnife_1', 'Cabinet_1', 'Cabinet_2', 'Cabinet_3', 'Cabinet_4', 'Cabinet_5', 'Cabinet_6', 'Cabinet_7', 'Cabinet_8', 'CoffeeMachine_1', 'CounterTop_1', 'CounterTop_2', 'DiningTable_1', 'Drawer_1', 'Drawer_2', 'Drawer_3', 'Faucet_1', 'Floor_1', 'GarbageCan_1', 'Lettuce_1', 'Microwave_1', 'Mug_1', 'Pan_1', 'Plate_1', 'Pot_1', 'SinkBasin_1', 'Sink_1', 'SoapBottle_1', 'Spatula_1', 'Stool_1', 'StoveBurner_1', 'StoveBurner_2', 'StoveBurner_3', 'StoveBurner_4', 'StoveKnob_1', 'StoveKnob_2', 'StoveKnob_3', 'StoveKnob_4', 'Toaster_1', 'Tomato_1','Window_1', 'WineBottle_1']Thought:
VI. ACKNOWLEDGEMENTSThis research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-21-2-0211.The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.DISTRIBU-TION A. Approved for public release; distribution unlimited.Action: Inspect Object Action Input: {'object_id': 'DiningTable_1'} Observation: Tool Completed Successfully: The queried object is currently inside or on top of the following objects: ['Floor|-01.72|-00.08|+02.08'].The quried objects currently controls the following objects: None.Thought: It seems that the dining table is positioned on the floor, but the inspection did not return any items on top of it.I'll need to search the dining table to find out if there are any objects placed on it.Action: Search Object Action Input: {'object_id': 'DiningTable_1'} Observation: Tool Completed Successfully: Here is a list of objects found inside or on the target receptacle: ['Knife_1', 'SaltShaker_1', 'Bread_1', 'PepperShaker_1', 'Lettuce_1', 'Potato_1'] Thought: I have identified several objects on the dining table: 'Knife_1', 'SaltShaker_1', 'Bread_1', ' PepperShaker_1', 'Lettuce_1', and 'Potato_1'.I will now proceed to pick up each object one by one.Action: Pick Up Object Action Input: {'object_id': 'Knife_1'} Observation: Tool Completed Successfully: Target object was picked up! Thought: I have successfully picked up the knife from the dining table.I will now proceed to place it somewhere else, such as the countertop.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Failed: the target object is not visible, Thought: It seems that the countertop is not currently in my line of sight.I need to adjust my position so that I can place the knife on the countertop.Action: Adjust Positioning Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the countertop.I will now try to place the knife on the countertop again.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Held object was successfully placed in target object!Thought: I have successfully placed the knife on the countertop.Next, I will proceed to pick up the remaining objects on the dining table: 'SaltShaker_1', 'Bread_1', 'PepperShaker_1', 'Lettuce_1', and 'Potato_1', and move them to an appropriate location, such as the countertop.Action: Pick Up Object Action Input: {'object_id': 'SaltShaker_1'} Observation: Tool Failed: the target object is not visible, Thought: The salt shaker is not currently in my line of sight.I need to adjust my position so that I can see and pick up the salt shaker from the dining table.Action: Adjust Positioning Action Input: {'object_id': 'SaltShaker_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the salt shaker.I will now try to pick up the salt shaker from the dining table.Action: Pick Up Object Action Input: {'object_id': 'SaltShaker_1'} Observation: Tool Completed Successfully: Target object was picked up! Thought: I have successfully picked up the salt shaker from the dining table.I will now proceed to place it on the countertop.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Failed: the target object is not visible, Listing 5: Successful for a Difficult task (1/3).Thought: It seems that the countertop is not currently in my line of sight again.I need to adjust my position so that I can place the salt shaker on the countertop.Action: Adjust Positioning Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the countertop.I will now try to place the salt shaker on the countertop again.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Held object was successfully placed in target object!Thought: I have successfully placed the salt shaker on the countertop.Next, I will proceed to pick up the remaining objects on the dining table: 'Bread_1', 'PepperShaker_1', 'Lettuce_1', and 'Potato_1', and move them to an appropriate location, such as the countertop.Action: Pick Up Object Action Input: {'object_id': 'Bread_1'} Observation: Tool Failed: the target object is not visible, Thought: The bread is not currently in my line of sight.I need to adjust my position so that I can see and pick up the bread from the dining table.Action: Adjust Positioning Action Input: {'object_id': 'Bread_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the bread.I will now try to pick up the bread from the dining table.Action: Pick Up Object Action Input: {'object_id': 'Bread_1'} Observation: Tool Completed Successfully: Target object was picked up! Thought: I have successfully picked up the bread from the dining table.I will now proceed to place it on the countertop.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Failed: the target object is not visible, Thought: The countertop is not in my line of sight again.I need to adjust my position to place the bread on the countertop.Action: Adjust Positioning Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the countertop.I will now try to place the bread on the countertop again.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Held object was successfully placed in target object!Thought: I have successfully placed the bread on the countertop.Next, I will proceed to pick up the remaining objects on the dining table: 'PepperShaker_1', 'Lettuce_1', and 'Potato_1', and move them to an appropriate location, such as the countertop.Action: Pick Up Object Action Input: {'object_id': 'PepperShaker_1'} Observation: Tool Failed: the target object is not visible, Thought: The pepper shaker is not currently in my line of sight.I need to adjust my position so that I can see and pick up the pepper shaker from the dining table.Action: Adjust Positioning Action Input: {'object_id': 'PepperShaker_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the pepper shaker.I will now try to pick up the pepper shaker from the dining table.Action: Pick Up Object Action Input: {'object_id': 'PepperShaker_1'} Observation: Tool Completed Successfully: Target object was picked up! Thought: I have successfully picked up the pepper shaker from the dining table.I will now proceed to place it on the countertop.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Failed: the target object is not visible, Thought: The countertop is not in my line of sight again.I need to adjust my position to place the pepper shaker on the countertop.Action: Adjust Positioning Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the countertop.I will now try to place the pepper shaker on the countertop again.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Held object was successfully placed in target object!Listing 6: Successful Execution of a Difficult task (2/3).Thought: I have successfully placed the pepper shaker on the countertop.Next, I will proceed to pick up the remaining objects on the dining table: 'Lettuce_1' and 'Potato_1', and move them to the countertop.Action: Pick Up Object Action Input: {'object_id': 'Lettuce_1'} Observation: Tool Failed: the target object is not visible, Thought: The lettuce is not currently in my line of sight.I need to adjust my position so that I can see and pick up the lettuce from the dining table.Action: Adjust Positioning Action Input: {'object_id': 'Lettuce_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the lettuce.I will now try to pick up the lettuce from the dining table.Action: Pick Up Object Action Input: {'object_id': 'Lettuce_1'} Observation: Tool Completed Successfully: Target object was picked up! Thought: I have successfully picked up the lettuce from the dining table.I will now proceed to place it on the countertop.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Failed: the target object is not visible, Thought: The countertop is not in my line of sight again.I need to adjust my position to place the lettuce on the countertop.Action: Adjust Positioning Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Completed Successfully: Readjusted position relative to target object!Thought: My positioning has been adjusted relative to the countertop.I will now try to place the lettuce on the countertop again.Action: Place Object Action Input: {'object_id': 'CounterTop_1'} Observation: Tool Failed: the target object is not visible, Thought: My repeated attempts to place objects on the countertop have failed because the countertop is not consistently in my line of sight.Instead, I'll try placing the objects in a different location that is more consistently visible, such as the floor.Action: Place Object Action Input: {'object_id': 'Floor_1'} Observation: Tool Completed Successfully: Held object was successfully placed in target object!Thought: I have successfully placed the lettuce on the floor.Now, I need to pick up the last remaining object on the dining table, which is the potato, and move it to the floor.Action: Pick Up Object Action Input: {'object_id': 'Potato_1'} Observation: Tool Completed Successfully: Target object was picked up! Thought: I have successfully picked up the potato from the dining table.I will now proceed to place it on the floor.Action: Place Object Action Input: {'object_id': 'Floor_1'} Observation: Tool Completed Successfully: Held object was successfully placed in target object!Thought: I have successfully cleared all the objects from the dining room table and placed them in different locations, either on the countertop or on the floor.The task of clearing off the dining room table is now complete.Final Answer: The dining room table has been successfully cleared of all objects.Listing 7: Successful Execution of a Difficult task including Predicate Grounding (3/3).Easy Subset Examples:1) The agent should pick up the dish sponge and place it in the bowl that is on the counter top.2) The agent should pick up the bread that is on the counter top and place it in the microwave.3) The agent should pick up the bowl that is on the counter top and place it in the sink basin.Medium Subset Examples: 1) Fill a vase with water.2) Place a fork in a cup.3) Find the spoon in the drawer and place it in the sink.Hard Subset Examples: 1) Cook an egg.2) Water the plant.3) Cook a potato without using the stove.Leave the potato inside what it was cooked in.X. EXAMPLE SUCCESSFUL EXECUTION OF A HARDTASK.XI. LLM REFLECTION FOR MCTS PLAN EVALUATIONLLM-based reflection replaces simulation and evaluation in the typical MCTS algorithm.The LLM-based reflection provides a third party evaluation of the plan.The LLm is prompted to score the quality of the plan and provide justification given the goal and the steps of the plan.
N M M Shafiullah, A Rai, H Etukuru, Y Liu, I Misra, S Chintala, L Pinto, arXiv:2311.16098On bringing robots home. 2023arXiv preprint</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 20221arXiv preprint</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International journal of robotics research. 374-52018</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, 2016 IEEE international conference on robotics and automation (ICRA. IEEE2016</p>
<p>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. Q Gu, A Kuwajerwala, S Morin, K M Jatavallabhula, B Sen, A Agarwal, C Rivera, W Paul, K Ellis, R Chellappa, arXiv:2309.166502023111arXiv preprint</p>
<p>Okrobot: What really matters in integrating open-knowledge models for robotics. P Liu, Y Orru, C Paxton, N M M Shafiullah, L Pinto, arXiv:2401.122022024. 1, 2, 5, 6arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Simple open-vocabulary object detection with vision transformers. M Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran, A Arnab, M Dehghani, Z Shen, arXiv:2205.0623020222arXiv preprint</p>
<p>Detecting twenty-thousand classes using image-level supervision. X Zhou, R Girdhar, A Joulin, P Krähenbühl, I Misra, European Conference on Computer Vision. Springer2022</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. K Marino, M Rastegari, A Farhadi, R Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognition2019</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in neural information processing systems. 202235</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, C Li, J Yang, H Su, J Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023112</p>
<p>Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. J Mahler, J Liang, S Niyaz, M Laskey, R Doan, X Liu, J A Ojea, K Goldberg, arXiv:1703.093122017arXiv preprint</p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. H.-S Fang, C Wang, H Fang, M Gou, J Liu, H Yan, W Liu, Y Xie, C Lu, IEEE Transactions on Robotics. 12023</p>
<p>Contactgraspnet: Efficient 6-dof grasp generation in cluttered scenes. M Sundermeyer, A Mousavian, R Triebel, D Fox, 2021 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Graspnet-1billion: A largescale benchmark for general object grasping. H.-S Fang, C Wang, M Gou, C Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202011</p>
<p>The homerobot open vocab mobile manipulation challenge. S Yenamandra, A Ramachandran, M Khanna, K Yadav, D S Chaplot, G Chhablani, A Clegg, T Gervet, V Jain, R Partsey, Thirty-seventh conference on neural information processing systems: competition track. 202312</p>
<p>Uniteam: Open vocabulary mobile manipulation challenge. A Melnik, M Büttner, L Harz, L Brown, G C Nandi, A Ps, G K Yadav, R Kala, R Haschke, arXiv:2312.086112023arXiv preprint</p>
<p>E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. 2017</p>
<p>Pddl-the planning domain definition language. C Aeronautiques, A Howe, C Knoblock, I D Mcdermott, A Ram, M Veloso, D Weld, D W Sri, A Barrett, D Christianson, Tech. Rep. 241998Technical Report</p>
<p>Pddl2. 1: An extension to pddl for expressing temporal planning domains. M Fox, D Long, Journal of artificial intelligence research. 2042003</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201125</p>
<p>Pushing the envelope: Planning, propositional logic, and stochastic search. H Kautz, B Selman, Proceedings of the national conference on artificial intelligence. the national conference on artificial intelligence1996</p>
<p>The 2014 international planning competition: Progress and trends. M Vallati, L Chrpa, M Grześ, T L Mccluskey, M Roberts, S Sanner, Ai Magazine. 3632015</p>
<p>Robot task planning via deep reinforcement learning: a tabletop object sorting application. F Ceola, E Tosello, L Tagliapietra, G Nicola, S Ghidoni, 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC). IEEE2019</p>
<p>Visual goal-directed meta-imitation learning. C G Rivera, D A Handelman, C R Ratto, D Patrone, B L Paulhamus, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Leveraging foundation models for scene understanding in human-robot teaming. D A Handelman, C G Rivera, W A Paul, A R Badger, E A Holmes, M I Cervantes, B G Kemp, E C Butler, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications VI. SPIE202413051</p>
<p>Pddl planning with pretrained large language models. T Silver, V Hariprasad, R S Shuttleworth, N Kumar, T Lozano-Pérez, L P Kaelbling, NeurIPS 2022 foundation models for decision making workshop. 2022</p>
<p>Llm+ p: Empowering large language models with optimal planning proficiency. B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.114772023arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292022. 2, 5, 6arXiv preprint</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, M Attarian, B Ichter, K Choromanski, A Wong, S Welker, F Tombari, A Purohit, M Ryoo, V Sindhwani, arXiv:2204.005982022arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, M Galley, P He, H Cheng, Y Xie, Y Hu, Q Huang, L Liden, Z Yu, W Chen, arXiv:2302.128132023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Mind's eye: Grounded language model reasoning through simulation. R Liu, J Wei, S S Gu, T.-Y Wu, S Vosoughi, C Cui, D Zhou, A M Dai, arXiv:2210.053592022arXiv preprint</p>
<p>Errors are useful prompts: Instruction guided task programming with verifierassisted iterative prompting. M Skreta, N Yoshikawa, S Arellano-Rubach, Z Ji, L B Kristensen, K Darvish, A Aspuru-Guzik, F Shkurti, A Garg, arXiv:2303.141002023arXiv preprint</p>
<p>Large language models as commonsense knowledge for large-scale task planning. Z Zhao, W S Lee, D Hsu, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>Bandit based monte-carlo planning. L Kocsis, C Szepesvári, 10.1007/11871842_29Proceedings of the 17th European Conference on Machine Learning, ser. ECML'06. the 17th European Conference on Machine Learning, ser. ECML'06Berlin, HeidelbergSpringer-Verlag2006</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 2024366</p>
<p>Uniteam: Open vocabulary mobile manipulation challenge. A Melnik, M Büttner, L Harz, L Brown, G C Nandi, A Ps, G K Yadav, R Kala, R Haschke, 2023</p>
<p>Closed-loop open-vocabulary mobile manipulation with gpt-4v. P Zhi, Z Zhang, M Han, Z Zhang, Z Li, Z Jiao, B Jia, S Huang, 2024</p>
<p>Spot-compose: A framework for open-vocabulary object retrieval and drawer manipulation in point clouds. O Lemke, Z Bauer, R Zurbrügg, M Pollefeys, F Engelmann, H Blum, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
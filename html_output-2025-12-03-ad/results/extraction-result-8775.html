<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8775 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8775</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8775</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-279057959</p>
                <p><strong>Paper Title:</strong> Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</p>
                <p><strong>Paper Abstract:</strong> We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8775.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8775.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReflectRetryReward (method)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training paradigm that teaches LLMs to generate effective self-reflections after a failed attempt, then reward only the tokens of those reflections via Group Relative Policy Optimization (GRPO) when a subsequent retry succeeds, enabling task-agnostic self-improvement using only a binary verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (applied to multiple LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method applied to models between ~1.5B and 8B parameters (and compared against larger vanilla baselines); no additional teacher models used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>If the model's first attempt fails (binary validator), the model is prompted to generate a short self-reflection explaining what went wrong; the model then retries the task with the reflection in context. If the retry succeeds, GRPO is used to assign reward signals only to the tokens in the self-reflection (advantage mask), encouraging generation of more effective reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Function calling (APIGen) and Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>APIGen: function-calling dataset requiring correct tool selection and JSON-structured parameters; Countdown: arithmetic equation generation using given numbers exactly once to reach a target.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Across tasks and models the paper reports vanilla second-attempt (reflection at inference) gains and larger gains after GRPO training; e.g. APIGen: vanilla 2nd try + reflection average +4.5% vs 1st try, after GRPO trained 2nd try average +4.7% further (examples in model entries). Countdown: vanilla reflection average +5.3%, trained +8.6% (averages reported); overall reported average improvements: +9.0% on APIGen, +16.0% on Countdown (trained vs vanilla 1st try baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla single-attempt baselines vary by model (e.g. APIGen vanilla 1st try range 32.6%–79.9%; Countdown vanilla 1st try range 2.1%–46.8%). See model-specific entries for exact numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-reflection prompts (simple natural-language prompts asking 'Reflect on what went wrong...'), a binary automatic validator per task, second-attempt retry with reflection in context, and reinforcement learning via GRPO that rewards only self-reflection tokens when retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative improvements in tables comparing vanilla 1st try, vanilla + reflection 2nd try, and trained + reflection 1st/2nd tries. Example: Qwen-2.5-1.5B on Countdown improved from vanilla 1st try 6.0% to trained+reflection 2nd try 45.0%; paper reports average gains of 9.0% (APIGen) and 16.0% (Countdown) due to GRPO self-reflection training, and claims models trained on self-reflection can outperform much larger untrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires a task-specific binary validator (not always available); model must have baseline ability to perform the task, reflect, and learn. Small models (<1.5B) had limited capacity and did not benefit; some models (e.g., Llama3.2-3B) failed to learn self-correction on function-calling. Specific failure modes: smallest models do not improve parameter-value errors, Qwen-2.5-7B learned to hit Countdown targets by using wrong numbers (tradeoff), and gains depend on initial baseline accuracy and task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper situates method relative to Chain-of-Thought and other introspection/self-reflection approaches: unlike CoT it trains models to produce concise, generalizable reflections rather than verbose chains-of-thought; compared experimentally to vanilla generation and vanilla two-attempt reflection (both before GRPO training). GRPO chosen over PPO/critic-based methods because it suits sparse outcome feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>No formal multi-factor ablation reported beyond the direct comparisons: vanilla 1st attempt, vanilla + reflection 2nd attempt, and GRPO-trained + reflection (1st/2nd). Those comparisons serve to isolate the benefit of training self-reflection (see model tables).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8775.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2-1.5B APIGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2 1.5B Instruct on APIGen (function calling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.5B parameter Qwen2 instruct model evaluated on APIGen; used the Reflect, Retry, Reward pipeline with GRPO training on failure cases to improve function-calling accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2-1.5B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2 instruct-family model with 1.5B parameters; prompted with model-specific tool-calling template recommended by provider.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Upon a failed tool-call the model is prompted: 'Reflect on what went wrong...' then retries the function call with that reflection in context; tokens of the reflection are rewarded via GRPO if the retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APIGen function calling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select correct tool and produce exact JSON function call with correct parameter names and values (binary success = exact match to ground truth).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 34.8% accuracy; after GRPO training: 2nd try with reflection 52.9% (trained 1st try 48.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try (no reflection): 32.6% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted natural-language self-reflection on failures; validator requires exact JSON match; GRPO rewards only reflection tokens on successful retry.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Accuracy improved from 32.6% (vanilla 1st) to 52.9% (trained + reflection 2nd), a substantial absolute improvement after GRPO training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Small-capacity model; prior to training made many failures; paper notes smallest models struggle with parameter-value selection and may not improve parameter errors much through training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against vanilla generation and vanilla two-attempt reflection; trained model outperforms vanilla two-attempt in this case.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Comparison provided between vanilla 1st, vanilla 2nd (reflection), and trained 1st/2nd; no further ablation isolating prompt variants reported for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8775.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2-7B APIGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2 7B Instruct on APIGen (function calling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B Qwen2 instruct model that benefits strongly from GRPO-trained self-reflection; after training it outperformed a much larger (72B) vanilla model in two-attempt evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2-7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2 family, 7 billion parameters, instructed for tool calling using provider-recommended prompt template.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single generate-reflect-retry cycle; effective reflections rewarded via GRPO when retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APIGen function calling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automated verification of JSON function call and parameters; binary success if exact match to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 69.4%; Trained + reflection (2nd try): 77.3%; Trained 1st try: 72.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 66.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted natural-language reflection on failures; GRPO reward applied only to reflection tokens if retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Absolute improvement from vanilla 1st try 66.4% to trained+reflection 2nd try 77.3%; trained 1st try (72.2%) already exceeds vanilla 2nd try of some larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>None specific for this model beyond general limitations; paper highlights that smaller models sometimes learn parameter selection while the smallest cannot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms vanilla two-attempt reflection baselines; qualitatively produces shorter clearer reflections after training compared to verbose vanilla reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Performance comparisons across vanilla, vanilla+reflection, and trained+reflection are reported; no additional ablation isolating reward mask variants reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8775.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B APIGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 8B Instruct on APIGen (function calling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B Llama 3.1 instruct model evaluated with the Reflect, Retry, Reward pipeline on function calling; GRPO training improved both first- and second-attempt accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 3.1 family, 8 billion parameter instruct model, using Llama-recommended tool-calling templates.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use of a failure-triggered reflection prompt, then retry with reflection in context; GRPO rewards only reflection tokens when retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APIGen function calling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select correct tool and produce correct JSON arguments; binary validator uses dataset ground truth exact-match.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 70.9%; Trained + reflection (2nd try): 74.9%; Trained 1st try: 68.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 64.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted self-reflection, binary validator, second attempt informed by reflection, GRPO reward mask on reflection tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Accuracy increased from 64.9% (vanilla 1st) to 74.9% (trained + reflection 2nd).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Llama family struggled more on Countdown math task (low baseline); requires baseline capability to benefit from self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against vanilla and vanilla+reflection; observed that trained reflections were shorter and more useful compared to verbose vanilla reflections, contrasting with chain-of-thought expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Comparisons reported across conditions; no separate ablation isolating number/length of reflection tokens or alternative reward strategies beyond the GRPO mask approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8775.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phi-3.5-mini APIGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi 3.5-mini Instruct (3.8B) on APIGen (function calling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3.8B 'Phi-3.5-mini' instruct model evaluated on APIGen; GRPO self-reflection training improved its function-calling accuracy though smaller models had limits on some error types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-3.5-mini Instruct (3.8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Phi 3.5 mini, ~3.8B parameter instruct variant, run using Llama-style tool-calling template (found to perform better).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Failure-triggered reflection prompt followed by retry; GRPO rewards reflection tokens on successful retry to shape future reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APIGen function calling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Exact-match verification of JSON function calls and parameters using APIGen ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 50.2%; Trained + reflection (2nd try): 56.0%; Trained 1st try: 52.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 47.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted natural-language reflection and GRPO reward masking for reflection tokens; validator exact-match JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Accuracy rose from vanilla 1st try 47.5% to trained+reflection 2nd try 56.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller models struggled more with tool choice and parameter selection; limited improvements on parameter-value errors for the smallest models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with vanilla two-attempt reflection baseline; GRPO-trained reflections produced more concise and useful critique texts than vanilla.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported comparative numbers for vanilla vs vanilla+reflection vs trained+reflection; no further ablation isolating prompt wording variations for Phi.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8775.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-1.5B Countdown</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 1.5B Instruct on Countdown (math equation writing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.5B Qwen2.5 instruct model evaluated on the Countdown arithmetic equation generation task; GRPO-trained self-reflection produced very large absolute improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-1.5B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5 instruct model with 1.5B parameters; prompted to reason step-by-step and produce final answer in \boxed{}; failure triggers reflection prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>On wrong first attempts the model is prompted to reflect ('Reflect on what went wrong...') then retries the same problem with the reflection present; GRPO rewards reflection tokens if retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 3-4 numbers and a target, produce a valid arithmetic expression using each number exactly once that evaluates to the target; success is verified by evaluating the equation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 10.2%; Trained + reflection (2nd try): 45.0%; Trained 1st try: 34.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 6.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted natural-language reflection; validator checks equation evaluation and usage of allowed numbers; GRPO reward applied to reflection tokens upon successful retry.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Massive absolute improvement from 6.0% (vanilla 1st) to 45.0% (trained + reflection 2nd). The abstract reports up to 34.7% improvement at math equation writing (representative of large gains observed).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes models often struggle to use only allowed numbers; training reduced 'wrong numbers' errors for most models except the largest Qwen-2.5-7B which learned to hit targets even when using wrong numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to vanilla and vanilla+reflection; GRPO-trained self-reflection yielded much larger gains than simple inference-time reflection alone.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Comparison across vanilla 1st, vanilla 2nd (reflection), and trained 1st/2nd serves as the primary empirical isolation of the reflection effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8775.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-3B Countdown</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 3B Instruct on Countdown (math equation writing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B Qwen2.5 instruct model evaluated with the Reflect, Retry, Reward method on Countdown, showing substantial improvements after GRPO self-reflection training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-3B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5 family, 3 billion parameter instruct model; prompted for step-by-step reasoning and final boxed answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single reflect-then-retry loop with GRPO rewarding reflection tokens when retry is successful.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Construct an arithmetic expression using the provided numbers exactly once to match the target; validator checks correctness by evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 29.0%; Trained + reflection (2nd try): 47.3%; Trained 1st try: 33.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 18.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based self-reflection, binary evaluation of final equation, and GRPO reward applied to reflection tokens only.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Improved from 18.8% (vanilla 1st) to 47.3% (trained + reflection 2nd), demonstrating strong gains from training the reflection mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As with other models, capability requirement; training primarily reduced 'wrong numbers' errors for most models but not all.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to vanilla generation and vanilla two-attempt reflection; GRPO-trained reflections provided larger improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Empirical comparisons across the three conditions reported; no further orthogonal ablations presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8775.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-7B Countdown</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 7B Instruct on Countdown (math equation writing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B Qwen2.5 instruct model which improved notably with GRPO-trained reflections but exhibited a specific failure mode of using wrong numbers to hit targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5 family, 7 billion parameter instruct model; baseline performance higher than smaller variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single reflection + retry loop; reflection tokens rewarded by GRPO when retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Construct a valid expression using each provided number exactly once to reach the target; success is automatically verified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 38.0%; Trained + reflection (2nd try): 50.3%; Trained 1st try: 41.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 31.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-generated reflection and GRPO reward masking; validator checks both number usage and computed target.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Improvement from 31.7% (vanilla 1st) to 50.3% (trained + reflection 2nd).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper specifically notes Qwen-2.5-7B learned to hit the target even if it used wrong numbers—i.e., reduced 'missed target' errors at expense of 'wrong numbers' errors. This demonstrates a failure case where the model optimizes for reward (target match) while violating other constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with vanilla and vanilla+reflection baselines; highlights the trade-offs of optimizing reflections under sparse outcome supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported condition comparisons showed the constrained reward mask focusing on reflections helps improve performance, but the paper does not include a fine-grained ablation to avoid the wrong-number tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8775.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.2-3B Countdown</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.2 3B Instruct on Countdown (math equation writing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B Llama 3.2 instruct model evaluated on Countdown; started from low baseline and showed gains after GRPO-trained self-reflection but generally had low absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.2-3B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 3.2 instruct-family model at 3B parameters; included in Countdown experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Failure-triggered self-reflection prompt, retry with reflection in context, and GRPO reward applied to reflection tokens when retry succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic equation construction verifying use of given numbers and target evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 3.0%; Trained + reflection (2nd try): 13.8%; Trained 1st try: 8.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 2.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered self-reflection; binary validator evaluating equation correctness; GRPO masks reflection tokens for reward.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Improved from 2.1% (vanilla 1st) to 13.8% (trained + reflection 2nd), indicating GRPO self-reflection training can help even low-baseline models though absolute performance remains low.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low-capacity behaviour: while training produced relative gains, absolute performance remained poor, emphasizing need for base competence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to vanilla baselines; no external teacher used for supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Comparative numbers across conditions provided; no further ablations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8775.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B Countdown</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.1 8B Instruct on Countdown (math equation writing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Llama-3.1 8B model exhibited very low baseline performance on Countdown but improved when trained to self-reflect with GRPO, though remaining below many Qwen variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 3.1 family 8B instruct model; used the Countdown prompt requiring step-by-step reasoning and boxed final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Reflection prompt on failure and a single retry; only reflection tokens rewarded by GRPO upon successful retry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Equation writing task using provided numbers to reach target; automatic verifier checks correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 4.6%; Trained + reflection (2nd try): 17.8%; Trained 1st try: 8.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 2.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted reflection; binary verifier for equation correctness; GRPO reward scheme targeting reflection tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Accuracy rose from 2.2% (vanilla 1st) to 17.8% (trained + reflection 2nd), representing an appreciable relative gain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low baseline competence yields modest absolute gains; Llama-family models generally underperformed Qwen variants on Countdown.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with vanilla and vanilla+reflection; trained reflections were succinct and more useful compared to verbose vanilla reflections (qualitative observation).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported comparisons across conditions; no further ablation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8775.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8775.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Palmyra 1.7B Countdown</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Palmyra 1.7B Instruct on Countdown (math equation writing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Palmyra small (1.7B) evaluated on Countdown; both vanilla reflection and GRPO-trained reflection improved performance moderately.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Palmyra 1.7B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Writer's Palmyra 1.7B instruct model used in Countdown experiments as a smaller baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflect, Retry, Reward (self-reflection + GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Failure-triggered brief self-reflections via prompt, retry using reflection, reflection tokens rewarded via GRPO on success.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Countdown math equations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate equation from provided numbers to reach target; validator checks numeric equality and number usage constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Vanilla + reflection (2nd try): 31.8%; Trained + reflection (2nd try): 38.6%; Trained 1st try: 33.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla 1st try: 26.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based reflection, binary automatic validator for equation correctness, and GRPO reward to reflection tokens only.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Improvements from 26.8% (vanilla 1st) to 38.6% (trained + reflection 2nd) indicate benefit from training reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller models still limited in absolute capability; training reduced some error modes but not all.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to vanilla and two-attempt vanilla reflection baselines; trained reflections more concise and effective.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Only comparative results across conditions reported; no deeper ablation isolating components of the reflection prompt or reward hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>Training language models to self-correct via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Instruct-of-reflection: Enhancing large language models iterative reflection capabilities via dynamic-meta instruction <em>(Rating: 1)</em></li>
                <li>Learning adaptive parallel reasoning with language models <em>(Rating: 1)</em></li>
                <li>Rezero: Enhancing llm search ability by trying one-more-time <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8775",
    "paper_id": "paper-279057959",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "ReflectRetryReward (method)",
            "name_full": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "brief_description": "A training paradigm that teaches LLMs to generate effective self-reflections after a failed attempt, then reward only the tokens of those reflections via Group Relative Policy Optimization (GRPO) when a subsequent retry succeeds, enabling task-agnostic self-improvement using only a binary verifier.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "method (applied to multiple LLMs)",
            "model_description": "Method applied to models between ~1.5B and 8B parameters (and compared against larger vanilla baselines); no additional teacher models used during training.",
            "reflection_method_name": "Reflect, Retry, Reward",
            "reflection_method_description": "If the model's first attempt fails (binary validator), the model is prompted to generate a short self-reflection explaining what went wrong; the model then retries the task with the reflection in context. If the retry succeeds, GRPO is used to assign reward signals only to the tokens in the self-reflection (advantage mask), encouraging generation of more effective reflections.",
            "task_name": "Function calling (APIGen) and Countdown math equations",
            "task_description": "APIGen: function-calling dataset requiring correct tool selection and JSON-structured parameters; Countdown: arithmetic equation generation using given numbers exactly once to reach a target.",
            "performance_with_reflection": "Across tasks and models the paper reports vanilla second-attempt (reflection at inference) gains and larger gains after GRPO training; e.g. APIGen: vanilla 2nd try + reflection average +4.5% vs 1st try, after GRPO trained 2nd try average +4.7% further (examples in model entries). Countdown: vanilla reflection average +5.3%, trained +8.6% (averages reported); overall reported average improvements: +9.0% on APIGen, +16.0% on Countdown (trained vs vanilla 1st try baselines).",
            "performance_without_reflection": "Vanilla single-attempt baselines vary by model (e.g. APIGen vanilla 1st try range 32.6%–79.9%; Countdown vanilla 1st try range 2.1%–46.8%). See model-specific entries for exact numbers.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered self-reflection prompts (simple natural-language prompts asking 'Reflect on what went wrong...'), a binary automatic validator per task, second-attempt retry with reflection in context, and reinforcement learning via GRPO that rewards only self-reflection tokens when retry succeeds.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative improvements in tables comparing vanilla 1st try, vanilla + reflection 2nd try, and trained + reflection 1st/2nd tries. Example: Qwen-2.5-1.5B on Countdown improved from vanilla 1st try 6.0% to trained+reflection 2nd try 45.0%; paper reports average gains of 9.0% (APIGen) and 16.0% (Countdown) due to GRPO self-reflection training, and claims models trained on self-reflection can outperform much larger untrained models.",
            "limitations_or_failure_cases": "Requires a task-specific binary validator (not always available); model must have baseline ability to perform the task, reflect, and learn. Small models (&lt;1.5B) had limited capacity and did not benefit; some models (e.g., Llama3.2-3B) failed to learn self-correction on function-calling. Specific failure modes: smallest models do not improve parameter-value errors, Qwen-2.5-7B learned to hit Countdown targets by using wrong numbers (tradeoff), and gains depend on initial baseline accuracy and task difficulty.",
            "comparison_to_other_methods": "Paper situates method relative to Chain-of-Thought and other introspection/self-reflection approaches: unlike CoT it trains models to produce concise, generalizable reflections rather than verbose chains-of-thought; compared experimentally to vanilla generation and vanilla two-attempt reflection (both before GRPO training). GRPO chosen over PPO/critic-based methods because it suits sparse outcome feedback.",
            "ablation_study_results": "No formal multi-factor ablation reported beyond the direct comparisons: vanilla 1st attempt, vanilla + reflection 2nd attempt, and GRPO-trained + reflection (1st/2nd). Those comparisons serve to isolate the benefit of training self-reflection (see model tables).",
            "uuid": "e8775.0",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2-1.5B APIGen",
            "name_full": "Qwen2 1.5B Instruct on APIGen (function calling)",
            "brief_description": "A 1.5B parameter Qwen2 instruct model evaluated on APIGen; used the Reflect, Retry, Reward pipeline with GRPO training on failure cases to improve function-calling accuracy.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Qwen-2-1.5B Instruct",
            "model_description": "Qwen2 instruct-family model with 1.5B parameters; prompted with model-specific tool-calling template recommended by provider.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Upon a failed tool-call the model is prompted: 'Reflect on what went wrong...' then retries the function call with that reflection in context; tokens of the reflection are rewarded via GRPO if the retry succeeds.",
            "task_name": "APIGen function calling",
            "task_description": "Select correct tool and produce exact JSON function call with correct parameter names and values (binary success = exact match to ground truth).",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 34.8% accuracy; after GRPO training: 2nd try with reflection 52.9% (trained 1st try 48.6%).",
            "performance_without_reflection": "Vanilla 1st try (no reflection): 32.6% accuracy.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted natural-language self-reflection on failures; validator requires exact JSON match; GRPO rewards only reflection tokens on successful retry.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Accuracy improved from 32.6% (vanilla 1st) to 52.9% (trained + reflection 2nd), a substantial absolute improvement after GRPO training.",
            "limitations_or_failure_cases": "Small-capacity model; prior to training made many failures; paper notes smallest models struggle with parameter-value selection and may not improve parameter errors much through training.",
            "comparison_to_other_methods": "Compared against vanilla generation and vanilla two-attempt reflection; trained model outperforms vanilla two-attempt in this case.",
            "ablation_study_results": "Comparison provided between vanilla 1st, vanilla 2nd (reflection), and trained 1st/2nd; no further ablation isolating prompt variants reported for this model.",
            "uuid": "e8775.1",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2-7B APIGen",
            "name_full": "Qwen2 7B Instruct on APIGen (function calling)",
            "brief_description": "A 7B Qwen2 instruct model that benefits strongly from GRPO-trained self-reflection; after training it outperformed a much larger (72B) vanilla model in two-attempt evaluation.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Qwen-2-7B Instruct",
            "model_description": "Qwen2 family, 7 billion parameters, instructed for tool calling using provider-recommended prompt template.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Single generate-reflect-retry cycle; effective reflections rewarded via GRPO when retry succeeds.",
            "task_name": "APIGen function calling",
            "task_description": "Automated verification of JSON function call and parameters; binary success if exact match to ground truth.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 69.4%; Trained + reflection (2nd try): 77.3%; Trained 1st try: 72.2%.",
            "performance_without_reflection": "Vanilla 1st try: 66.4%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted natural-language reflection on failures; GRPO reward applied only to reflection tokens if retry succeeds.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Absolute improvement from vanilla 1st try 66.4% to trained+reflection 2nd try 77.3%; trained 1st try (72.2%) already exceeds vanilla 2nd try of some larger models.",
            "limitations_or_failure_cases": "None specific for this model beyond general limitations; paper highlights that smaller models sometimes learn parameter selection while the smallest cannot.",
            "comparison_to_other_methods": "Outperforms vanilla two-attempt reflection baselines; qualitatively produces shorter clearer reflections after training compared to verbose vanilla reflections.",
            "ablation_study_results": "Performance comparisons across vanilla, vanilla+reflection, and trained+reflection are reported; no additional ablation isolating reward mask variants reported.",
            "uuid": "e8775.2",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1-8B APIGen",
            "name_full": "Llama 3.1 8B Instruct on APIGen (function calling)",
            "brief_description": "An 8B Llama 3.1 instruct model evaluated with the Reflect, Retry, Reward pipeline on function calling; GRPO training improved both first- and second-attempt accuracy.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B Instruct",
            "model_description": "Llama 3.1 family, 8 billion parameter instruct model, using Llama-recommended tool-calling templates.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Use of a failure-triggered reflection prompt, then retry with reflection in context; GRPO rewards only reflection tokens when retry succeeds.",
            "task_name": "APIGen function calling",
            "task_description": "Select correct tool and produce correct JSON arguments; binary validator uses dataset ground truth exact-match.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 70.9%; Trained + reflection (2nd try): 74.9%; Trained 1st try: 68.7%.",
            "performance_without_reflection": "Vanilla 1st try: 64.9%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted self-reflection, binary validator, second attempt informed by reflection, GRPO reward mask on reflection tokens.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Accuracy increased from 64.9% (vanilla 1st) to 74.9% (trained + reflection 2nd).",
            "limitations_or_failure_cases": "Llama family struggled more on Countdown math task (low baseline); requires baseline capability to benefit from self-reflection.",
            "comparison_to_other_methods": "Compared against vanilla and vanilla+reflection; observed that trained reflections were shorter and more useful compared to verbose vanilla reflections, contrasting with chain-of-thought expectations.",
            "ablation_study_results": "Comparisons reported across conditions; no separate ablation isolating number/length of reflection tokens or alternative reward strategies beyond the GRPO mask approach.",
            "uuid": "e8775.3",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Phi-3.5-mini APIGen",
            "name_full": "Phi 3.5-mini Instruct (3.8B) on APIGen (function calling)",
            "brief_description": "A 3.8B 'Phi-3.5-mini' instruct model evaluated on APIGen; GRPO self-reflection training improved its function-calling accuracy though smaller models had limits on some error types.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Phi-3.5-mini Instruct (3.8B)",
            "model_description": "Phi 3.5 mini, ~3.8B parameter instruct variant, run using Llama-style tool-calling template (found to perform better).",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Failure-triggered reflection prompt followed by retry; GRPO rewards reflection tokens on successful retry to shape future reflections.",
            "task_name": "APIGen function calling",
            "task_description": "Exact-match verification of JSON function calls and parameters using APIGen ground truth.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 50.2%; Trained + reflection (2nd try): 56.0%; Trained 1st try: 52.9%.",
            "performance_without_reflection": "Vanilla 1st try: 47.5%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted natural-language reflection and GRPO reward masking for reflection tokens; validator exact-match JSON.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Accuracy rose from vanilla 1st try 47.5% to trained+reflection 2nd try 56.0%.",
            "limitations_or_failure_cases": "Smaller models struggled more with tool choice and parameter selection; limited improvements on parameter-value errors for the smallest models.",
            "comparison_to_other_methods": "Compared with vanilla two-attempt reflection baseline; GRPO-trained reflections produced more concise and useful critique texts than vanilla.",
            "ablation_study_results": "Reported comparative numbers for vanilla vs vanilla+reflection vs trained+reflection; no further ablation isolating prompt wording variations for Phi.",
            "uuid": "e8775.4",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5-1.5B Countdown",
            "name_full": "Qwen2.5 1.5B Instruct on Countdown (math equation writing)",
            "brief_description": "A 1.5B Qwen2.5 instruct model evaluated on the Countdown arithmetic equation generation task; GRPO-trained self-reflection produced very large absolute improvements.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-1.5B Instruct",
            "model_description": "Qwen2.5 instruct model with 1.5B parameters; prompted to reason step-by-step and produce final answer in \\boxed{}; failure triggers reflection prompt.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "On wrong first attempts the model is prompted to reflect ('Reflect on what went wrong...') then retries the same problem with the reflection present; GRPO rewards reflection tokens if retry succeeds.",
            "task_name": "Countdown math equations",
            "task_description": "Given 3-4 numbers and a target, produce a valid arithmetic expression using each number exactly once that evaluates to the target; success is verified by evaluating the equation.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 10.2%; Trained + reflection (2nd try): 45.0%; Trained 1st try: 34.9%.",
            "performance_without_reflection": "Vanilla 1st try: 6.0%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted natural-language reflection; validator checks equation evaluation and usage of allowed numbers; GRPO reward applied to reflection tokens upon successful retry.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Massive absolute improvement from 6.0% (vanilla 1st) to 45.0% (trained + reflection 2nd). The abstract reports up to 34.7% improvement at math equation writing (representative of large gains observed).",
            "limitations_or_failure_cases": "Paper notes models often struggle to use only allowed numbers; training reduced 'wrong numbers' errors for most models except the largest Qwen-2.5-7B which learned to hit targets even when using wrong numbers.",
            "comparison_to_other_methods": "Compared to vanilla and vanilla+reflection; GRPO-trained self-reflection yielded much larger gains than simple inference-time reflection alone.",
            "ablation_study_results": "Comparison across vanilla 1st, vanilla 2nd (reflection), and trained 1st/2nd serves as the primary empirical isolation of the reflection effect.",
            "uuid": "e8775.5",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5-3B Countdown",
            "name_full": "Qwen2.5 3B Instruct on Countdown (math equation writing)",
            "brief_description": "A 3B Qwen2.5 instruct model evaluated with the Reflect, Retry, Reward method on Countdown, showing substantial improvements after GRPO self-reflection training.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-3B Instruct",
            "model_description": "Qwen2.5 family, 3 billion parameter instruct model; prompted for step-by-step reasoning and final boxed answer.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Single reflect-then-retry loop with GRPO rewarding reflection tokens when retry is successful.",
            "task_name": "Countdown math equations",
            "task_description": "Construct an arithmetic expression using the provided numbers exactly once to match the target; validator checks correctness by evaluation.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 29.0%; Trained + reflection (2nd try): 47.3%; Trained 1st try: 33.9%.",
            "performance_without_reflection": "Vanilla 1st try: 18.8%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based self-reflection, binary evaluation of final equation, and GRPO reward applied to reflection tokens only.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Improved from 18.8% (vanilla 1st) to 47.3% (trained + reflection 2nd), demonstrating strong gains from training the reflection mechanism.",
            "limitations_or_failure_cases": "As with other models, capability requirement; training primarily reduced 'wrong numbers' errors for most models but not all.",
            "comparison_to_other_methods": "Compared to vanilla generation and vanilla two-attempt reflection; GRPO-trained reflections provided larger improvements.",
            "ablation_study_results": "Empirical comparisons across the three conditions reported; no further orthogonal ablations presented.",
            "uuid": "e8775.6",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5-7B Countdown",
            "name_full": "Qwen2.5 7B Instruct on Countdown (math equation writing)",
            "brief_description": "A 7B Qwen2.5 instruct model which improved notably with GRPO-trained reflections but exhibited a specific failure mode of using wrong numbers to hit targets.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-7B Instruct",
            "model_description": "Qwen2.5 family, 7 billion parameter instruct model; baseline performance higher than smaller variants.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Single reflection + retry loop; reflection tokens rewarded by GRPO when retry succeeds.",
            "task_name": "Countdown math equations",
            "task_description": "Construct a valid expression using each provided number exactly once to reach the target; success is automatically verified.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 38.0%; Trained + reflection (2nd try): 50.3%; Trained 1st try: 41.6%.",
            "performance_without_reflection": "Vanilla 1st try: 31.7%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-generated reflection and GRPO reward masking; validator checks both number usage and computed target.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Improvement from 31.7% (vanilla 1st) to 50.3% (trained + reflection 2nd).",
            "limitations_or_failure_cases": "Paper specifically notes Qwen-2.5-7B learned to hit the target even if it used wrong numbers—i.e., reduced 'missed target' errors at expense of 'wrong numbers' errors. This demonstrates a failure case where the model optimizes for reward (target match) while violating other constraints.",
            "comparison_to_other_methods": "Compared with vanilla and vanilla+reflection baselines; highlights the trade-offs of optimizing reflections under sparse outcome supervision.",
            "ablation_study_results": "Reported condition comparisons showed the constrained reward mask focusing on reflections helps improve performance, but the paper does not include a fine-grained ablation to avoid the wrong-number tradeoff.",
            "uuid": "e8775.7",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.2-3B Countdown",
            "name_full": "Llama 3.2 3B Instruct on Countdown (math equation writing)",
            "brief_description": "A 3B Llama 3.2 instruct model evaluated on Countdown; started from low baseline and showed gains after GRPO-trained self-reflection but generally had low absolute performance.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Llama-3.2-3B Instruct",
            "model_description": "Llama 3.2 instruct-family model at 3B parameters; included in Countdown experiments.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Failure-triggered self-reflection prompt, retry with reflection in context, and GRPO reward applied to reflection tokens when retry succeeds.",
            "task_name": "Countdown math equations",
            "task_description": "Arithmetic equation construction verifying use of given numbers and target evaluation.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 3.0%; Trained + reflection (2nd try): 13.8%; Trained 1st try: 8.8%.",
            "performance_without_reflection": "Vanilla 1st try: 2.1%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered self-reflection; binary validator evaluating equation correctness; GRPO masks reflection tokens for reward.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Improved from 2.1% (vanilla 1st) to 13.8% (trained + reflection 2nd), indicating GRPO self-reflection training can help even low-baseline models though absolute performance remains low.",
            "limitations_or_failure_cases": "Low-capacity behaviour: while training produced relative gains, absolute performance remained poor, emphasizing need for base competence.",
            "comparison_to_other_methods": "Compared to vanilla baselines; no external teacher used for supervision.",
            "ablation_study_results": "Comparative numbers across conditions provided; no further ablations reported.",
            "uuid": "e8775.8",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1-8B Countdown",
            "name_full": "Llama 3.1 8B Instruct on Countdown (math equation writing)",
            "brief_description": "The Llama-3.1 8B model exhibited very low baseline performance on Countdown but improved when trained to self-reflect with GRPO, though remaining below many Qwen variants.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B Instruct",
            "model_description": "Llama 3.1 family 8B instruct model; used the Countdown prompt requiring step-by-step reasoning and boxed final answer.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Reflection prompt on failure and a single retry; only reflection tokens rewarded by GRPO upon successful retry.",
            "task_name": "Countdown math equations",
            "task_description": "Equation writing task using provided numbers to reach target; automatic verifier checks correctness.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 4.6%; Trained + reflection (2nd try): 17.8%; Trained 1st try: 8.8%.",
            "performance_without_reflection": "Vanilla 1st try: 2.2%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted reflection; binary verifier for equation correctness; GRPO reward scheme targeting reflection tokens.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Accuracy rose from 2.2% (vanilla 1st) to 17.8% (trained + reflection 2nd), representing an appreciable relative gain.",
            "limitations_or_failure_cases": "Low baseline competence yields modest absolute gains; Llama-family models generally underperformed Qwen variants on Countdown.",
            "comparison_to_other_methods": "Compared with vanilla and vanilla+reflection; trained reflections were succinct and more useful compared to verbose vanilla reflections (qualitative observation).",
            "ablation_study_results": "Reported comparisons across conditions; no further ablation reported.",
            "uuid": "e8775.9",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Palmyra 1.7B Countdown",
            "name_full": "Palmyra 1.7B Instruct on Countdown (math equation writing)",
            "brief_description": "Palmyra small (1.7B) evaluated on Countdown; both vanilla reflection and GRPO-trained reflection improved performance moderately.",
            "citation_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "Palmyra 1.7B Instruct",
            "model_description": "Writer's Palmyra 1.7B instruct model used in Countdown experiments as a smaller baseline.",
            "reflection_method_name": "Reflect, Retry, Reward (self-reflection + GRPO)",
            "reflection_method_description": "Failure-triggered brief self-reflections via prompt, retry using reflection, reflection tokens rewarded via GRPO on success.",
            "task_name": "Countdown math equations",
            "task_description": "Generate equation from provided numbers to reach target; validator checks numeric equality and number usage constraints.",
            "performance_with_reflection": "Vanilla + reflection (2nd try): 31.8%; Trained + reflection (2nd try): 38.6%; Trained 1st try: 33.3%.",
            "performance_without_reflection": "Vanilla 1st try: 26.8%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based reflection, binary automatic validator for equation correctness, and GRPO reward to reflection tokens only.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Improvements from 26.8% (vanilla 1st) to 38.6% (trained + reflection 2nd) indicate benefit from training reflections.",
            "limitations_or_failure_cases": "Smaller models still limited in absolute capability; training reduced some error modes but not all.",
            "comparison_to_other_methods": "Compared to vanilla and two-attempt vanilla reflection baselines; trained reflections more concise and effective.",
            "ablation_study_results": "Only comparative results across conditions reported; no deeper ablation isolating components of the reflection prompt or reward hyperparameters.",
            "uuid": "e8775.10",
            "source_info": {
                "paper_title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "Training language models to self-correct via reinforcement learning",
            "rating": 2,
            "sanitized_title": "training_language_models_to_selfcorrect_via_reinforcement_learning"
        },
        {
            "paper_title": "Instruct-of-reflection: Enhancing large language models iterative reflection capabilities via dynamic-meta instruction",
            "rating": 1,
            "sanitized_title": "instructofreflection_enhancing_large_language_models_iterative_reflection_capabilities_via_dynamicmeta_instruction"
        },
        {
            "paper_title": "Learning adaptive parallel reasoning with language models",
            "rating": 1,
            "sanitized_title": "learning_adaptive_parallel_reasoning_with_language_models"
        },
        {
            "paper_title": "Rezero: Enhancing llm search ability by trying one-more-time",
            "rating": 1,
            "sanitized_title": "rezero_enhancing_llm_search_ability_by_trying_onemoretime"
        }
    ],
    "cost": 0.0227845,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning
30 May 2025</p>
<p>Shelly Bensal 
Inc</p>
<p>Umar Jamil 
Inc</p>
<p>Christopher Bryant 
Inc</p>
<p>Melisa Russak 
Inc</p>
<p>Kiran Kamble 
Inc</p>
<p>Dmytro Mozolevskyi 
Inc</p>
<p>Muayad Ali 
Inc</p>
<p>Waseem Alshikh waseem@writer.com 
Inc</p>
<p>Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning
30 May 20252FFC5FFEFA5E9DEE6797FBFB24B4F745arXiv:2505.24726v1[cs.CL]
We explore a method for improving the performance of large language models through self-reflection and reinforcement learning.By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available.Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context.If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded.Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling.Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger.Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.* Equal contribution Preprint.Under review.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks (Zhao et al., 2025), as well as in mathematics (Ahn et al., 2024), coding (Jiang et al., 2024), and reasoning (Huang and Chang, 2023).Despite these advancements however, models still have blind spots, and there is no guarantee that a model that succeeds at one task will succeed at another, even if the task is of a similar type (Asher et al., 2023;Huckle and Williams, 2025).The most direct way to address this problem is to retrain or fine-tune a model on data that represents the failed task, however this may not be possible if no such dataset exists.Furthermore, if the largest state-of-the-art models also struggle to complete the task, we similarly cannot use them to generate synthetic training data (Liu et al., 2024a).</p>
<p>An alternative solution is to prompt the model to explain its reasoning or self-reflect on why it failed.For example, the popular Chain-of-Thought (CoT) paradigm (Wei et al., 2022) showed that models performed significantly better at arithmetic, commonsense, and reasoning tasks if they were prompted to show their reasoning in addition to simply providing a response.Self-reflection operates on a similar principle, in that if we can detect when a LLM provides an incorrect response, we can prompt it to reflect on any flaws in its reasoning and perhaps try again (Ji et al., 2023;Renze and Guven, 2024).The main advantage of these approaches is that they do not require any additional training data, however their effectiveness is directly tied to the effectiveness of the reasoning/reflection prompt.</p>
<p>In this paper, we investigate the extent to which LLMs can learn to generate better self-reflections in order to self-improve on downstream tasks.More specifically, if a model fails to complete a task on its first attempt, it generates a self-reflection which it uses to make a second attempt.If the model then succeeds on its second attempt, we use reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO) (Shao et al., 2024), to reward the tokens in the self-reflection, such that future self-reflections will be more effective.In this way, models can learn how to improve upon all kinds of tasks without requiring any task-specific data; they instead just optimize how to reflect on mistakes.</p>
<p>Our main contribution is thus a novel methodology for training a model to generate better selfreflections to improve on challenging tasks in a task-agnostic way.Crucially, this method only requires a binary success/failure signal from a response verifier, which makes it well-suited to tasks where success can be easily verified.To demonstrate the efficacy of our approach, we carry out experiments on both the APIGen function calling dataset (Liu et al., 2024b)  2 Related Work 2.1 Self-Reflection Self-reflection in LLMs Self-reflection, also referred to as introspection, is a metaprompting strategy in which a language model analyzes its own reasoning in order to identify and correct potential mistakes.This paradigm has gained momentum in large language model (LLM) research as a means to boost multi-step reasoning and problem-solving performance, especially in domains such as arithmetic, commonsense reasoning, and question answering (Wei et al., 2022;Madaan et al., 2023;Renze and Guven, 2024;Shinn et al., 2023).Typically, self-reflection involves generating an initial answer, producing natural language feedback to critique that answer, and then refining the response based on this critique.This process can be applied iteratively, often using the same model to both generate and evaluate solutions, and may include modules such as memory buffers or explicit meta-instruction guides (Liu et al., 2025;Wu et al., 2025).</p>
<p>Approaches and Limitations</p>
<p>The methodology for self-reflection in LLMs varies along several axes.Some methods apply self-correction only to failed or low-confidence queries, while others use it for every response; feedback can be provided in the form of scalar scores, external annotations, or natural language, and may be generated by humans, external models, or the LLM itself (Bai et al., 2022;Peng et al., 2023;Yang et al., 2022;Pan et al., 2025c).While prompting LLMs to self-reflect does improve accuracy in many settings, recent work has shown that the effectiveness depends strongly on the context: challenges include the inability to reliably identify self-errors without ground-truth oracles, diminishing returns from repeated reflection, and risks of performance deterioration for easier prompts or high-performing base models (Huang et al., 2024;Zhang et al., 2024;Kim et al., 2023).In particular, self-reflection is most effective when initial accuracy is low, question difficulty is high, and external verification is available.Conversely, LLMs may sometimes fail to recognize their own mistakes but can still benefit from external feedback when such supervision exists (Pan et al., 2025c;Shinn et al., 2023).</p>
<p>Training-Based Methods Recent directions focus on incorporating self-improvement capabilities during model training, either by fine-tuning on self-correction trajectories or by formulating the process as a multi-turn reinforcement learning problem (Kumar et al., 2024;Qu et al., 2024;Wu et al., 2025).These training-based methods suggest that leveraging the model's own critiques during learning yields persistent improvements-even when no test-time self-reflection is performed.However, these approaches typically rely on larger teacher models for data generation or supervision, which can be seen as a form of knowledge distillation (Hinton et al., 2015).</p>
<p>Our Approach Building on insights from prior research, we propose correcting only failed cases identified by an external verifier, converting its binary feedback into self-reflective prompts, and training the model to use the self-reflection to succeed at the second attempt.This oracle-grounded conditional computation leverages training-time benefits to reduce test-time overhead and is guaranteed to improve or maintain performance, since corrections are applied only to initially incorrect examples.For training, we employ Group Relative Policy Optimization (GRPO), introduced in the next section.Notably, this approach bootstraps solely from the model's own outputs, without relying on external LLMs.</p>
<p>Figure 1: Reflect, Retry, Reward Mechanism The model is first prompted to complete a task based on a user query.If the initial response is correct, the process stops.If not, the model is prompted to generate a self-reflection on how to improve.The model then retries the same task, this time with its self-reflection included, and the new answer is evaluated.If the second attempt succeeds, the model learns that it generated an effective self-reflection.</p>
<p>Reinforcement Learning for Language Models</p>
<p>GRPO Group Relative Policy Optimization (GRPO) is an outcome-based reinforcement learning method proposed to address the unique challenges faced when fine-tuning LLMs, such as those encountered in complex mathematical reasoning tasks (Shao et al., 2024).Unlike conventional approaches like Proximal Policy Optimization (PPO) (Schulman et al., 2017), GRPO dispenses with a separate value (critic) network and instead estimates advantages directly by comparing outcomes from a group of sampled completions.This makes GRPO particularly well-suited to settings where supervision is sparse and only available at the conclusion of a generation-for example, whether a completed math solution is correct.In such environments, the model must generate an entire sequence before receiving any feedback, typically in the form of a scalar reward reflecting the quality or correctness of the output.</p>
<p>Our Approach In this work, we adopt GRPO as the sole mechanism for reinforcement learning, without involving additional supervised fine-tuning stages.Recent research has demonstrated that modifying GRPO's reward structure can effectively encourage models to persist through failure, for instance by rewarding retries after unsuccessful attempts, thereby promoting self-correction and robustness (Dao and Le, 2025).GRPO has further shown promise in related domains requiring complex, outcome-supervised behaviors-including tool use and advanced mathematical problem solving-offering a flexible and efficient optimization strategy in diverse LLM applications (Qian et al., 2025;Li et al., 2025).</p>
<p>Reflect, Retry, Reward</p>
<p>Our novel Reflect, Retry, Reward methodology operates as follows, and is illustrated in Figure 1.</p>
<p>First, a model is prompted to complete a task.If it succeeds, we do nothing as the model already meets our needs.If it fails however, we prompt it to generate a self-reflection on what might have gone wrong.Note that this presupposes a validator that automatically evaluates whether a response was a success or failure (binary).While it is sometimes possible to define a task-dependent validator that meets this criteria without ground-truth labels, such as in basic API function calling (Did the API call return a valid response?),mathematical equations (Does the equation evaluate to the target answer?), or code (Does the generated code execute?), some task types may require gold-standard target answers.</p>
<p>Having generated a self-reflection, the model then makes a second attempt to complete the task, making use of the self-reflection in the conversation history.If it still fails, we do nothing; the self-reflection was insufficient to turn a failure into a success.If it succeeds however, we use GRPO to reward only the tokens that were generated in the self-reflection.This is possible by setting the advantage terms for all other generated tokens to zero.We do this because we want the model to learn how to self-reflect more generally rather than specialize for a particular task.In other words, we do not reward the correct answer, we only reward the self-reflection.</p>
<p>Experiments</p>
<p>We demonstrate the effectiveness of our approach through experiments on two different tasks: function calling and math equations.</p>
<p>Function Calling</p>
<p>We use the APIGen dataset (Liu et al., 2024b) for our function calling experiments.APIGen is a dataset of 60,000 high quality function calls that consist of a user query (plain text), a list of possible tools that can answer that query plus their parameters (JSON) and the correctly formatted function call with the correct parameters and values (JSON).There are a total of 4,211 unique tools in the dataset, with an average of 2.3 parameters per tool, and each user query has an average of 2.8 tools to choose from (min 1, max 8).A model is only considered to be correct if it not only selects the right tool, but also generates the correct parameters and values.A sample datapoint with a choice of two different tools is shown below (formatted to be more human readable).</p>
<p>USER QUERY: Check if the Vimeo username 'john_doe_artist' is available.</p>
<p>TOOLS PROVIDED:</p>
<p>[{ "name": "vimeo", "description": "Checks if a given Vimeo username is available using the → Toolbench RapidAPI service.","parameters": {"username": {"description": "The Vimeo username to check for → availability.","type": "str", "default": "username"}} }, { "name": "get_user_pins", "description": "Retrieves the Pinterest pins of a specified user.","parameters": {"username": {"description": "The Pinterest username whose pins → are to be fetched.","type": "str", "default": "0869178429hau"}} }] CORRECT ANSWER: [{"name": "vimeo", "arguments": {"username": "john_doe_artist"}}]</p>
<p>To preserve the integrity of our experiments, we only evaluate models that were released before the APIGen dataset was released (June 2024).This ensures it is impossible that any of these models could have been trained on the dataset to obtain an unfair advantage.Specifically, we report results for Qwen2 (1.5B/7B Instruct) (Yang et al., 2024), Llama3.1 (8B Instruct) (Grattafiori et al., 2024), andPhi3.5-mini Instruct (Abdin et al., 2024).We also report the vanilla performance of Qwen2-72B Instruct, Llama3.1-70BInstruct, and Writer's Palmyra X4 (Writer.com,2024) as a baseline.</p>
<p>Since different model families also have different suggested tool-calling approaches, we tested different templates for each model family and ultimately chose the prompt formats that provided the strongest baselines.For our function calling validator, we require the model output to exactly match the correct answer in the dataset (i.e. based on the ground-truth labels).We used the following prompt to generate self-reflections for failed function calling attempts:</p>
<p>You tried performing the task, but failed in generating the correct tool call.</p>
<p>→ Reflect on what went wrong and write a short explanation that will help you → do better next time.</p>
<p>Countdown Math Equations</p>
<p>We use the Countdown dataset introduced by the TinyZero project for our math equation experiments (Pan et al., 2025a,b).The Countdown dataset consists of 450k lists of 3-4 numbers along with a target number.The goal is to apply basic arithmetic operations to the numbers such that the equation evaluates to the target number.A model is only considered to be correct if it uses all the numbers once (in any order) and if the final equation successfully evaluates to the target number.A sample datapoint is shown below.</p>
<p>Using the numbers [4,73,4,23], create an equation that equals 76.You can use → basic arithmetic operations (+, -, *, /) and each number can only be used → once.</p>
<p>As with function calling, to preserve the integrity of our experiments, we only evaluate models that were released or have a knowledge cutoff before the Countdown dataset was made publicly available (January 2025).Specifically, we report results for Qwen2.5 (1.5B/3B/7B Instruct) (Yang et al., 2025), Llama3.1 (8B Instruct), Llama3.2 (3B Instruct), and Writer's Palmyra 1.7B.We also report the vanilla performance of Qwen2.5-32BInstruct, Qwen2.5-72BInstruct, Llama3.1-70BInstruct, and Writer's Palmyra X4 (Writer.com,2024) as a baseline.</p>
<p>We once again tried several different prompt formats for each model family, and ultimately chose to use the format that provided the strongest baseline.For our math equation validator, we required the generated equation to match the target answer in the prompt (i.e.no need for ground-truth labels).We used the following prompt to generate self-reflections for failed Countdown math equations:</p>
<p>You tried solving the problem and got the wrong answer.Reflect on what went wrong → and write a short explanation that will help you do better next time.</p>
<p>A Dataset of Failures</p>
<p>For reasons of efficiency, and to facilitate a more intuitive analysis, we did not train our models on the full function calling and math equation training sets, but instead opted to first create a dataset of failures for each task.More specifically, we prompted each model for each task to generate up to 64 responses (depending on model size) to each user query and preserved only those queries where the model failed (based on each task-dependent verifier).We typically generated more responses for larger models because they failed less frequently than smaller models, and so would otherwise yield fewer training samples.To accelerate the rejection sampling process, we used vLLM (Kwon et al., 2023) with prefix caching.</p>
<p>This approach has several advantages.First and foremost, it saves time because there is no point training our self-reflection model on queries it already handles successfully and hence cannot learn from.Second, by generating several responses per query, we make the data more robust; for example, if a base model generates a correct response to the same query 80% of the time, we can still learn from the remaining 20% since responses are not deterministic.Finally, by only having failure cases in our dataset, we can precisely determine how many samples the model needed to train on before it converged on the optimum self-reflection.</p>
<p>We must emphasize that we took this approach purely for reasons of efficiency and analysis, and it is otherwise functionally equivalent to learning from a real-world scenario where we receive both successful and failed responses.</p>
<p>Multi-Step GRPO</p>
<p>We used the TRL framework (von Werra et al., 2020) as a starting base to implement our multi-step GRPO algorithm (i.e., to learn from the second attempt after self-reflection).In particular, we extend the GRPOTrainer and alter its _prepare_inputs function to call a second_step function that, given the completions generated by the GRPOTrainer, will perform another step of completion generations, without affecting the mask already computed by the GRPOTrainer.As we operate on the dataset of failures, prompting the model to generate its self-reflection commentary, the mask corresponds to the tokens of the self-reflection text.This way, we can perform as many secondary steps on the initial completions as necessary and only reward the tokens (through the mask generated by the GRPOTrainer) corresponding to the initial completions.a data structure to the inputs sent to the reward function that helps in understanding the performance of the initial completion on the successive steps.This multi-step approach allows us to integrate any complex downstream reward mechanism instead of only rewarding the initial completions.</p>
<p>We trained our models on the respective failure datasets for up to 1,750 steps with an effective batch size of 256 failures (though in practice most models converged significantly faster) and we evaluated them at their convergence point.For example, the function calling experiment on Llama-3.1-8BInstruct required only 100 training steps and utilized less than 2,000 unique queries.Only one function calling experiment saw the entire dataset of 48,000 queries; the average across all function calling experiments was less than 25,000 unique queries.The most any math equation writing experiments used was less than 25,000 unique problems; the average of all math equation writing experiments was around 15,000 unique problems.</p>
<p>We used standard GRPO training parameters as described in the original DeepSeek implementation (Shao et al., 2024), and conducted some hyperparameter experimentation.In our final experiments, we set the KL divergence coefficient to 0.001, and used a learning rate of 5e-7 with a cosine annealing schedule and a warmup ratio of 0.03.To train each model, we used between 4 and 8 H100 GPUs.We limit our experiments to models between 1.5 billion and 8 billion parameters due to known computational efficiency and scalability concerns with GRPO (Zhang and Zuo, 2025).</p>
<p>In addition to the experimental results reported here, we also carried out experiments with some smaller models.We quickly discovered, however, that these models had a very limited capacity to answer accurately and self-reflect; e.g.Qwen2/Qwen2.5 0.5B Instruct and Llama3.2-1BInstruct.Similarly, while Microsoft's Phi 3.5 mini model was able to handle function calling, it struggled significantly with equation writing.We do not report results for these models.</p>
<p>Experimental Results</p>
<p>Our main experimental results are shown in Table 1 and Table 2. Specifically, Table 1 shows model performance for each model's first and second attempts on the APIGen test set (12,000 samples) both before and after our multi-step GRPO training, while Table 2 shows the same but for the Countdown test set (15,000 samples).</p>
<p>In terms of APIGen, we first note that model size correlates perfectly with model performance after one attempt (as expected).We also note that performance increased by an average of 4.5% after a second attempt using a self-reflection, which again is in line with previous work.We see the biggest increase after our GRPO training however, where although we only reward self-reflection tokens, almost all models are able to outperform even the two-attempt vanilla models after just a single attempt.We hypothesize this is because the self-reflection tokens help with model reasoning in general, so the model benefits even if it does not need to generate an explicit self-reflection.Nevertheless, self-reflection still helps after our training, and performance increases a further 4.7% (on average) when models can self-reflect for their second attempt.Most strikingly, we observe that our Qwen-2-7B model after GRPO training is able to outperform a vanilla Qwen-2-72B model when both models are given two attempts, even though the latter model is 10x bigger than the first.Figure 2: Better Self-Reflections We observe that reflections generated by vanilla models tend to be long, confusing, and redundant, whereas GRPO fine-tuned models produce much shorter, clearer, and more generalisable reflections.</p>
<p>In terms of Countdown, it is first worth noting that performance was lower across the board, and the vanilla Llama models in particular (both Llama-3.1 and Llama-3.2) really struggled to complete the task; for example, the Llama-3.1.70Bmodel was outclassed by even the Qwen-2.5-3Bmodel, which is more than 20x smaller.Otherwise, the pattern of improvement is similar to the APIGen experiments, albeit at a slightly higher magnitude: self-reflection increased performance by an average of 5.3% and 8.6% respectively before and after our GRPO training.We hypothesize that these larger gains come from the fact the models started from a lower baseline and hence had a greater opportunity to learn.</p>
<p>Ultimately, our findings not only reinforce previous work on the benefits of self-reflection, they also demonstrate how learning to optimize for self-reflection with GRPO can improve performance further still.Table 3: Catastrophic Forgetting Analysis Comparison between vanilla and GRPO fine-tuned models on common LLM benchmarks shows that despite fine-tuning, we observe minimal catastrophic forgetting, with fine-tuned models maintaining strong performance on these standard benchmarks.</p>
<p>Better Self-Reflections</p>
<p>To provide an insight into how self-reflections improve after self-reflection training, we present a qualitative example of a self-reflection generated by a vanilla model alongside a self-reflection generated by the same model after GRPO training in Figure 2. It is immediately obvious that vanilla self-reflections are much longer, more verbose, and repetitive compared to the more concise, optimized self-reflections after training.While this intuitively makes sense -humans likewise prefer short, simple instructions -this finding contrasts with chain-of-thought-style outputs, which are believed to perform better precisely because they are more verbose.We leave it as an open question as to when it may be more beneficial for a model to generate concise vs. verbose output.</p>
<p>Low Catastrophic Forgetting</p>
<p>A common concern when fine-tuning models is catastrophic forgetting, i.e. when a model learns to specialize on one task at the expense of others (Li and Hoiem, 2016;Lopez-Paz and Ranzato, 2017;Kotha et al., 2024).Since our self-reflection training is designed to improve performance in a task agnostic way, we evaluate our models on several diverse benchmarks (MMLU-Pro (Wang et al., 2024), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), and MATH (Hendrycks et al., 2021)) in order to assess their capacity for language understanding, mathematical problem solving, and commonsense reasoning both before and after self-reflection training.We do this using the common evaluation benchmark framework lm-eval (Gao et al., 2024).Our hypothesis is that performance should remain relatively unchanged, since we never optimize for a specific task, but instead optimize self-reflection reasoning in general.</p>
<p>We present our results in Table 3, and find that performance does indeed remain stable after selfinflection training.In most cases, there is less than 1% degradation compared to the base model, and some models even improve; e.g.Qwen-2.5-1.5Bperformance increases by 0.6% and 0.8% respectively on MMLU-Pro and MATH after self-reflection training on the Countdown dataset.We treat this as evidence our approach is robust to catastrophic forgetting.</p>
<p>Conclusion</p>
<p>In this paper, we have shown that it is possible to significantly improve LLM performance by training a model to improve at self-reflection rather than at a particular task.This indirect approach depends only on a validator that can detect whether a model response is correct or incorrect, and so is particularly well-suited to tasks where responses can be easily verified; e.g.whether JSON output is formatted correctly, whether generated code is actually executable, or whether all the constraints of an equation are satisfied.</p>
<p>We demonstrated the efficacy of our approach through experiments on the APIGen function calling and Countdown math equation solving datasets, and found that models trained for self-reflection using GRPO improved performance by an average of 9.0% on the function calling test set (12,000 samples) and 16.0% on the Countdown match equation dataset (15,000 samples).We furthermore found that smaller self-reflection trained models could outperform larger untrained models on both tasks, despite their size difference; e.g.Qwen-2-7B Instruct (trained) outperformed Qwen2-72B Instruct (untrained) on function calling, and Qwen2.5-7BInstruct (trained) outperformed Qwen2.5-72BInstruct (untrained) on Countdown math equations.Our models were also robust to catastrophic forgetting.</p>
<p>Although we only trained models to improve at self-reflection, we found they also performed significantly better even when they did not need to self-reflect; i.e. they succeeded on the first attempt so there was no need to reflect and try again.We hypothesize that this is because by focusing on self-reflection rather than a particular task, models may have improved their reasoning skills more generally.In future work, we hope to investigate whether self-reflection training generalizes across different tasks.</p>
<p>Limitations</p>
<p>It may not always be straightforward to define a binary success/fail validator for every task.We developed our method with the view that labeled training data may be scarce, but recognize that ground-truth labels could be used as a validator if available.Alternatively, it may also be possible to use a larger model as a judge (Zheng et al., 2023).</p>
<p>We also find that our approach does not work for all models and all tasks; the model must have some basic ability to perform the task, self-reflect, and learn in order for boosting self-correction ability to work.For example, Llama3.2-3BInstruct was unable to learn to self-correct on the function calling task.</p>
<p>A Prompt Templates</p>
<p>For reproducibility and clarity, we provide details on the prompt templates used during training.To the best of our ability, we followed model provider recommendations for prompting.We iterated on prompts to achieve reasonable baselines for each model on each task.</p>
<p>A.1 Function Calling</p>
<p>Qwen 2 models Our prompting style for Qwen 2 models for function calling is as follows.First, we provide the following system prompt:</p>
<p>You are a helpful assistant that can answer questions and help with tasks.</p>
<h1>Tools</h1>
<p>You may call one or more functions to assist with the user query.</p>
<p>You are provided with function signatures within <tools></tools> XML tags: <tools> {List of tools, each on a new line} </tools></p>
<p>For each function call, return a json object with function name and arguments within → <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call> This is followed by the user query from the dataset, as role user.The model then replies with its first attempt at the task.If the attempt is incorrect, we prompt for a self-reflection as follows:</p>
<p>You tried performing the task, but failed in generating the correct tool call.</p>
<p>→ Reflect on what went wrong and write a short explanation that will help you → do better next time.</p>
<p>After the model generates a self-reflection, we again prompt with the system prompt and user query to set up the model for its second attempt at the task.</p>
<p>Llama 3.1 and Phi 3.5 models We follow the recommended Llama 3.1 tool calling format.We found that Phi performs better following the Llama tool-calling template than the Qwen 2 template.First, we provide the following system prompt:</p>
<p>When you receive a tool call response, use the output to format an answer to the → original user question.</p>
<p>You are a helpful assistant with tool calling capabilities.</p>
<p>Then, as role user, we provide the tools and user query from the dataset as follows:</p>
<p>Given the following functions, please respond with a JSON for a function call with → its proper arguments that best answers the given prompt.</p>
<p>Respond in the format {\"name\": function name, \"parameters\": dictionary of → argument name and its value}.Do not use variables.</p>
<p>{List of tools, each on a new line} Question:</p>
<p>This is followed by the user query from the dataset, as role user.The model then replies with its first attempt at the task.We then prompt for a self-reflection:</p>
<p>You tried performing the task, but failed in generating the correct tool call.</p>
<p>→ Reflect on what went wrong and write a short explanation that will help you → do better next time.</p>
<p>After the model generates a self-reflection, we prompt with just the user query to set up the model for its second attempt at the task.</p>
<p>A.2 Countdown Math Equations</p>
<p>We provide the following system prompt:</p>
<p>Please reason step by step, and put your final answer within \boxed{}.</p>
<p>Then, as role user, we provide the main problem as follows:</p>
<p>Using the numbers {nums, in list format} create an equation that equals {target}.→ You can use basic arithmetic operations (+, -, *, /) and each number can → only be used once.Please reason step by step, and put your final answer within \boxed{}.</p>
<p>The model then replies with its first attempt at the task.Given a failure, we then prompt for a self-reflection:</p>
<p>You tried solving the problem and got the wrong answer.Reflect on what went wrong → and write a short explanation that will help you do better next time.</p>
<p>After the model generates a self-reflection, we repeat the user message from above to set the model up for its second attempt at the task:</p>
<p>Using the numbers {nums, in list format} create an equation that equals {target}.→ You can use basic arithmetic operations (+, -, *, /) and each number can → only be used once.Please reason step by step, and put your final answer within \boxed{}.</p>
<p>All models struggled primarily with outputting equations that used only allowed numbers.Training significantly decreased this error for all models except for Qwen-2.5-7BInstruct.Put another way, all models except the largest Qwen model primarily learned to use the correct numbers in the equation through training, even if it resulted in missing the target, whereas Qwen-2.5-7BInstruct learned to hit the target even if it meant using the wrong numbers.</p>
<p>and the Countdown equation task introduced by Pan et al. (2025b).</p>
<p>Table 1 :
1
The second_step function also adds APIGen Results This table shows model performance in terms of accuracy on our APIGen test set (12,000 samples) both on the first and second attempt, and with/without our GRPO selfreflection training.
Vanilla + Reflection Trained + ReflectionAPIGen1st Try2nd Try1st Try2nd TryQwen-2-1.5B Instruct32.6%34.8%48.6%52.9%Qwen-2-7B Instruct66.4%69.4%72.2%77.3%Llama-3.1-8B Instruct64.9%70.9%68.7%74.9%Phi-3.5-mini Instruct (3.8B)47.5%50.2%52.9%56.0%Qwen-2-72B Instruct73.7%76.6%--Llama-3.1-70B Instruct66.8%76.9%--Palmyra-X4 (73B)79.9%83.5%--</p>
<p>Table 2 :
2
Countdown ResultsThis table shows model performance in terms of accuracy on the Countdown test set (15,000 samples) both on the first and second attempt, and with/without our GRPO self-reflection training.
Vanilla + Reflection Trained + ReflectionCountdown1st Try2nd Try1st Try2nd TryQwen-2.5-1.5B Instruct6.0%10.2%34.9%45.0%Qwen-2.5-3B Instruct18.8%29.0%33.9%47.3%Qwen-2.5-7B Instruct31.7%38.0%41.6%50.3%Llama-3.1-8B Instruct2.2%4.6%8.8%17.8%Llama-3.2-3B Instruct2.1%3.0%8.8%13.8%Palmyra 1.7B26.8%31.8%33.3%38.6%Qwen-2.5-32B Instruct38.6%45.1%--Qwen-2.5-72B Instruct45.2%49.9%--Llama-3.1-70B Instruct17.3%25.5%--Palmyra-X4 (73B)46.8%51.6%--
B Error AnalysisWe categorise the errors of our models before and after training in an attempt to better understand what types of errors models are prone to on these tasks, and what types of errors can be mitigated by self-reflection training.We look exclusively at errors made on the first attempt at the task (pass@1).B.1 Function CallingFor function calling, we categorise errors into three types: errors in tool choice, errors in parameter names or values, and errors in format.We consider parameter choice to be much more difficult than tool choice.The two smallest models (Qwen-2-1.5BInstruct and Phi-3.5-miniInstruct) struggle significantly with tool choice without training, and don't improve much, if at all, on parameter values through training.Conversely, the larger models (7-8 billion parameters) are already quite good at tool choice without training, and training primarily seems to teach parameter selection.B.2 Math Countdown EquationsFor math countdown equations, we categorise errors into three types: an invalid equation (or one that uses disallowed characters), an equation that uses numbers outside of the provided ones (wrong numbers), and an equation that does not evaluate to the provided target (missed target).
Jyoti Marah Abdin, Hany Aneja, Ahmed Awadalla, Ammar Awadallah, Nguyen Ahmad Awan, Amit Bach, Arash Bahree, Jianmin Bakhtiari, Harkirat Bao, Alon Behl, Misha Benhaim, Johan Bilenko, Sébastien Bjorck, Martin Bubeck, Qin Cai, Vishrav Cai, Dong Chaudhary, Chen, arXiv:2404.14219Dongdong Chen, and 110 others. 2024. Phi-3 technical report: A highly capable language model locally on your phone. Preprint</p>
<p>Large language models for mathematical reasoning: Progresses and challenges. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research WorkshopSt. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>Limits for learning with language models. Nicholas Asher, Swarnadeep Bhar, Akshay Chaturvedi, Julie Hunter, Soumya Paul, 10.18653/v1/2023.starsem-1.22Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (<em>SEM 2023). the 12th Joint Conference on Lexical and Computational Semantics (</em>SEM 2023)Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, ArXiv, abs/2204.05862202212</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.141682021Preprint</p>
<p>Rezero: Enhancing llm search ability by trying one-more-time. Alan Dao, Thinh Le, arXiv:2504.110012025Preprint</p>
<p>The language model evaluation harness. Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.12608602Aviya Skowron, Lintang Sutawika, and 5 others. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf2024</p>
<p>Artem Korenev, Arthur Hinsvark, and 542 others. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, arXiv:2407.217832024PreprintThe llama 3 herd of models</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2015Preprint</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Easy Problems that LLMs Get Wrong. James Huckle, Sean Williams, Advances in Information and Communication. ChamSpringer Nature Switzerland2025</p>
<p>Towards mitigating LLM hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, 10.18653/v1/2023.findings-emnlp.123Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>A survey on large language models for code generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.005152024Preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Marcus Mcaleer, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Understanding catastrophic forgetting in language models via implicit inference. Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan, arXiv:2309.101052024Preprint</p>
<p>Training language models to self-correct via reinforcement learning. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, M Lei, Kay Zhang, Disha Mckinney, Cosmin Shrivastava, George Paduraru, Tucker, arXiv:2409.12917Doina Precup, Feryal Behbahani, and Aleksandra Faust2024Preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Torl: Scaling tool-integrated rl. Xuefeng Li, Haoyang Zou, Pengfei Liu, arXiv:2503.233832025Preprint</p>
<p>Learning without forgetting. Zhizhong Li, Derek Hoiem, IEEE Transactions on Pattern Analysis and Machine Intelligence. 402016</p>
<p>Instruct-of-reflection: Enhancing large language models iterative reflection capabilities via dynamic-meta instruction. Liping Liu, Chunhong Zhang, Likang Wu, Chuang Zhao, Zheng Hu, Ming He, Jianping Fan, arXiv:2503.009022025Preprint</p>
<p>Best practices and lessons learned on synthetic data. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, Andrew M Dai, First Conference on Language Modeling. 2024a</p>
<p>Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Advances in Neural Information Processing Systems. 2024b37Rithesh RN, and 1 others</p>
<p>Gradient episodic memory for continual learning. David Lopez, - Paz, Marc'aurelio Ranzato, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Advances in Neural Information Processing Systems. 202336Yiming Yang, and 1 others</p>
<p>Learning adaptive parallel reasoning with language models. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, Alane Suhr, arXiv:2504.154662025aPreprint</p>
<p>. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, Alane Suhr, 2025b</p>
<p>Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H Vicky, Zhao, arXiv:2503.17439Conghui He, and Lijun Wu. 2025c. Lemma: Learning from errors for mathematical advancement in llms. Preprint. </p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, arXiv:2302.128132023Preprint</p>
<p>Toolrl: Reward is all tool learning needs. Emre Cheng Qian, Qi Can Acikgoz, Hongru He, Xiusi Wang, Dilek Chen, Gokhan Hakkani-Tür, Heng Tur, Ji, arXiv:2504.139582025Preprint</p>
<p>Recursive introspection: Teaching language model agents how to self-improve. Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar, arXiv:2407.182192024Preprint</p>
<p>Self-reflection in llm agents: Effects on problem-solving performance. Matthew Renze, Erhan Guven, arXiv:2405.066822024arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017Preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024Preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Younes Leandro Von Werra, Lewis Belkada, Edward Tunstall, Tristan Beeching, Nathan Thrush, Shengyi Lambert, Kashif Huang, Quentin Rasul, Gallouédec, Trl: Transformer reinforcement learning. 2020</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, arXiv:2406.015742024Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Writer.com. 2024Palmyra x4 | tool calling llm. </p>
<p>Rethinking chain-of-thought from the perspective of self-training. Zongqian Wu, Baoduo Xu, Ruochen Cui, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng, arXiv:2412.108272025Preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, arXiv:2407.10671Qwen2 technical report. 202443Preprint</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, arXiv:2412.15115Qwen2.5 technical report. Junyang Lin, Kai Dang, 23Preprint</p>
<p>Re3: Generating longer stories with recursive reprompting and revision. Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein, 10.18653/v1/2022.emnlp-main.296Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.078302019Preprint</p>
<p>Grpo-lead: A difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. Jixiao Zhang, Chunsheng Zuo, arXiv:2504.096962025Preprint</p>
<p>Self-contrast: Better reflection through inconsistent solving perspectives. Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu, 10.18653/v1/2024.acl-long.197Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Li, arXiv:2303.18223Zikang Liu, and 3 others. 2025. A survey of large language models. Xinyu TangPreprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2736 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2736</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2736</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-238418980</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.acl-long.557.pdf" target="_blank">Situated Dialogue Learning through Procedural Environment Generation</a></p>
                <p><strong>Paper Abstract:</strong> We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution—an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2736.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2736.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIGHT RL Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LIGHT Reinforcement Learning Agent (A2C with transformer encoder and factored action/speech policies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-driven agent that acts and speaks in the LIGHT text-adventure environment using a transformer-based encoder (either randomly initialized 3-layer or pretrained 12-layer) and A2C policy-gradient training; it receives the setting, persona, motivation, and the full dialogue history as input and outputs factored action and dialogue policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LIGHT RL Agent (A2C transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encoder-based agent: transformer encoder (3-layer scratch or 12-layer pretrained/adaptive encoder) feeding a switch module that chooses between action and speech; separate policy networks for actions and dialogue; single shared critic; trained with A2C. Partner agent and Dungeon Master (DM) use poly-encoder architectures for response and speech scoring respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (LIGHT-Quests)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A crowdsourced fantasy text-adventure environment with characters, locations, objects, and quests (short motivations + goal actions). Tasks require acting and speaking in-character to achieve quest goals in partially observable text worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working/context memory (full dialogue history) and semantic prior memory (pretraining on commonsense knowledge graph ATOMIC-LIGHT)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequence context window encoded by transformer (full dialogue history appended to context); semantic knowledge represented as a commonsense graph (ATOMIC-LIGHT) used during encoder pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Past observations and the full dialogue history, current setting, persona, motivations; semantic commonsense relations from ATOMIC-LIGHT (e.g., agent-action-object relations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Full dialogue history and context are encoded end-to-end by the transformer (no separate retrieval index); the Dungeon Master (poly-encoder) scores candidate utterances and provides top-k candidates (retrieval by learned scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Context is updated each timestep by appending new observations/utterances to the dialogue history; pretrained semantic priors are fixed via encoder weights (adaptive encoder frozen with appended layers during RL).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Provides context for action and dialogue policy decisions, helps score and select natural utterances (via DM), and supplies semantic priors for reasoning about actions/objects/goals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Adaptive encoder + Procedurally generated curriculum (uses pretrained semantic priors): Act goals 50.6%, Speech goals 38.2%, All goals 37.3% (reported averages across 3 seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Scratch encoder + Procedurally generated curriculum (no pretrained semantic priors): Act goals 47.7%, Speech goals 16.3%, All goals 15.5% (reported averages across 3 seeds). Note: paper does not run an isolated ablation that removes the dialogue-history context; these numbers compare pretrained vs scratch encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Two memory-related findings: (1) Memorization of training trajectories (i.e., relying on memorized sequences) harms zero-shot generalization; procedural generation that increases diversity reduces memorization and improves zero-shot performance. (2) Pretraining (semantic priors via ATOMIC-LIGHT and Reddit/LIGHT pretraining) improves performance across curricula, especially speech goals, suggesting semantic memory/prior knowledge aids generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit long-term external episodic memory module is used; reliance on encoding full dialogue history (context window) and memorized trajectories leads to overfitting and poor generalization unless training data diversity is increased. The paper notes memorization of long-tail trajectories as a primary cause of generalization gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2736.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2736.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full Dialogue History (context)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Dialogue History as Encoder Context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The agent's transformer encoder is given the entire dialogue history (along with setting, persona, and motivation) as part of the input context at each decision step; this serves as the agent's immediate working memory for the episode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LIGHT RL Agent (contextual encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer-based encoder that consumes the full dialogue history plus setting/persona/motivation each step; no separate external memory module is instantiated — history is part of input context.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (LIGHT-Quests)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LIGHT description above: text-adventure quests requiring action and dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory / context window</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential buffer: full dialogue history token sequence appended to input and encoded by transformer (no separate key-value or external store described).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Past utterances and partner actions in the episode, enabling context-aware action/dialogue decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Implicit: the encoder attends over the entire provided dialogue history (attention-based access); no explicit retrieval subsystems described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated online each timestep by appending the latest utterance/action to the dialogue history fed to the encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Used to condition both action and dialogue policies to maintain persona-consistent behavior and to react to partner utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper uses full dialogue history as part of the agent input but does not ablate removing it. It does emphasize that richer, varied training contexts (via procedurally generated curriculums) improve generalization, implying that context exposure matters.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit scaling/efficiency limits discussed, but the approach relies on encoding potentially long histories via transformer context which may be capacity-limited; no external long-term memory provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2736.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2736.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorization of Trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Memorization (overfitting to head of training distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An identified failure mode where agents learn to memorize specific sequences of actions and dialogues from commonly seen training quests, resulting in poor generalization to novel or rarer quest types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>n/a (behavioral phenomenon)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent architecture but a behavioral/learning phenomenon: agents that memorize trajectories perform well on seen quests but fail zero-shot on novel quests.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (LIGHT-Quests)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure quests; memorization manifests as failure to generalize to rare verb/noun quest types in LIGHT-Quests test set.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memorization (overfitted episodic traces)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Implicit stored trajectories / policy-conditioned sequences (no explicit structure described).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Stored sequences of actions and dialogues corresponding to frequently-seen quests in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Learned implicitly during RL optimization (policy overfits to frequent trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Leads to successful completion of frequently seen quests but poor zero-shot transfer to rare/unseen quests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper explicitly states that memorization of trajectories causes the generalization gap. Mitigation: training on procedurally generated, diverse curricula (increasing variety and flattening verb distributions) reduces the ability to memorize trajectories and substantially improves zero-shot performance. Sequential curriculum progression (from easier/unaugmented pools to flatter, rarer pools) yields better results than directly training on a pool dominated by rare quests.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Memorization is harmful for generalization; increasing training diversity is required to make memorization infeasible. The paper notes that simply oversampling rare quests (sampled curriculum) helps less than procedurally generating diverse instances, and totally random generation harms performance by creating incoherent worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2736.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2736.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based / Belief-graph memories (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic belief graphs / knowledge-graph memory approaches for text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related literature approaches that represent and maintain structured world knowledge (objects, locations, entities, relations) as graphs to support decision-making in partially observable text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>various (e.g., dynamic belief-graph agents, graph-constrained RL agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that maintain an explicit graph-like internal representation (belief/state graph or knowledge graph) of the environment; used in prior work to ground language and track discovered entities and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>text-based game benchmarks (e.g., Jericho/TextWorld/LIGHT)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Partially-observable text-adventure games where agents must discover and reason over objects, locations, and relations; graph memories encode discovered state for planning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory / belief graph / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph structure encoding entities (nodes) and relations (edges); sometimes dynamic (updated over time) to represent agent beliefs about world state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Discovered objects, entities, relations, visited rooms, inferred state needed for planning (e.g., which object is located where, which characters hold which objects).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Query-based graph lookup and graph-structured encodings used as inputs to policies or as constraints on action generation; retrieval often via learned encoders that score candidate nodes/edges.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated when new observations are made (observations parsed and added/updated in the graph); dynamics differ by specific prior work but generally per-observation updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Grounding language, constraining and informing action selection in partially observable environments, improving generalization by abstracting state beyond raw trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Mentioned as prior approaches for representing agent knowledge and improving generalization; paper cites these works as relevant but does not experimentally compare them. The authors note that grounding with structured knowledge (e.g., knowledge graphs) has been used to produce worlds and quests in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2736.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2736.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATOMIC-LIGHT (commonsense KG pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ATOMIC-LIGHT (domain-adapted fantasy commonsense knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adapted commonsense knowledge-graph used as part of encoder pretraining to give the adaptive encoder semantic priors about actions and their consequences in the fantasy LIGHT domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive encoder (pretrained on ATOMIC-LIGHT and Reddit/LIGHT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adaptive encoder: 12-layer transformer pretrained multitask on Reddit and ATOMIC-LIGHT commonsense data, then fine-tuned on LIGHT and used (frozen) with appended layers during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td>≈256M (stated as 256 million parameters in paper for adaptive encoder pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (LIGHT-Quests)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>See LIGHT description above.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic knowledge graph (commonsense memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph (ATOMIC-LIGHT) providing relational commonsense facts used during encoder pretraining; stored as dataset/weights in the encoder rather than a run-time graph store.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Commonsense relations mapping motivations/goals to required subgoals/actions and objects (e.g., need-to-acquire relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Encoded into model weights via pretraining tasks; retrieval is implicit via the encoder's representations when given context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Static at RL time (pretrained weights are frozen and appended layers are trained during RL as described).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Provides semantic priors that improve both action selection and dialogue naturalness, leading to higher zero-shot performance particularly on speech goals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Adaptive encoder (pretrained with ATOMIC-LIGHT) + Procedurally generated curriculum: Act goals 50.6%, Speech goals 38.2%, All goals 37.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Scratch encoder (no pretraining) + Procedurally generated curriculum: Act goals 47.7%, Speech goals 16.3%, All goals 15.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Pretraining on ATOMIC-LIGHT (commonsense graph) yields consistent performance improvements across curricula, especially improving speech-goal rates; authors hypothesize that pretraining gives the adaptive model greater capacity to learn generalizable representations of generated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit dynamic update of the knowledge graph at runtime; commonsense priors are encoded in model parameters rather than an updatable external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Graph Constrained Reinforcement Learning for Natural Language Action Spaces <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 1)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Bring-ing stories alive: Generating interactive fiction worlds <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2736",
    "paper_id": "paper-238418980",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "LIGHT RL Agent",
            "name_full": "LIGHT Reinforcement Learning Agent (A2C with transformer encoder and factored action/speech policies)",
            "brief_description": "A goal-driven agent that acts and speaks in the LIGHT text-adventure environment using a transformer-based encoder (either randomly initialized 3-layer or pretrained 12-layer) and A2C policy-gradient training; it receives the setting, persona, motivation, and the full dialogue history as input and outputs factored action and dialogue policies.",
            "citation_title": "How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds.",
            "mention_or_use": "use",
            "agent_name": "LIGHT RL Agent (A2C transformer-based)",
            "agent_description": "Encoder-based agent: transformer encoder (3-layer scratch or 12-layer pretrained/adaptive encoder) feeding a switch module that chooses between action and speech; separate policy networks for actions and dialogue; single shared critic; trained with A2C. Partner agent and Dungeon Master (DM) use poly-encoder architectures for response and speech scoring respectively.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (LIGHT-Quests)",
            "game_description": "A crowdsourced fantasy text-adventure environment with characters, locations, objects, and quests (short motivations + goal actions). Tasks require acting and speaking in-character to achieve quest goals in partially observable text worlds.",
            "uses_memory": true,
            "memory_type": "working/context memory (full dialogue history) and semantic prior memory (pretraining on commonsense knowledge graph ATOMIC-LIGHT)",
            "memory_structure": "Sequence context window encoded by transformer (full dialogue history appended to context); semantic knowledge represented as a commonsense graph (ATOMIC-LIGHT) used during encoder pretraining.",
            "memory_content": "Past observations and the full dialogue history, current setting, persona, motivations; semantic commonsense relations from ATOMIC-LIGHT (e.g., agent-action-object relations).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Full dialogue history and context are encoded end-to-end by the transformer (no separate retrieval index); the Dungeon Master (poly-encoder) scores candidate utterances and provides top-k candidates (retrieval by learned scoring).",
            "memory_update_strategy": "Context is updated each timestep by appending new observations/utterances to the dialogue history; pretrained semantic priors are fixed via encoder weights (adaptive encoder frozen with appended layers during RL).",
            "memory_usage_purpose": "Provides context for action and dialogue policy decisions, helps score and select natural utterances (via DM), and supplies semantic priors for reasoning about actions/objects/goals.",
            "performance_with_memory": "Adaptive encoder + Procedurally generated curriculum (uses pretrained semantic priors): Act goals 50.6%, Speech goals 38.2%, All goals 37.3% (reported averages across 3 seeds).",
            "performance_without_memory": "Scratch encoder + Procedurally generated curriculum (no pretrained semantic priors): Act goals 47.7%, Speech goals 16.3%, All goals 15.5% (reported averages across 3 seeds). Note: paper does not run an isolated ablation that removes the dialogue-history context; these numbers compare pretrained vs scratch encoders.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Two memory-related findings: (1) Memorization of training trajectories (i.e., relying on memorized sequences) harms zero-shot generalization; procedural generation that increases diversity reduces memorization and improves zero-shot performance. (2) Pretraining (semantic priors via ATOMIC-LIGHT and Reddit/LIGHT pretraining) improves performance across curricula, especially speech goals, suggesting semantic memory/prior knowledge aids generalization.",
            "memory_limitations": "No explicit long-term external episodic memory module is used; reliance on encoding full dialogue history (context window) and memorized trajectories leads to overfitting and poor generalization unless training data diversity is increased. The paper notes memorization of long-tail trajectories as a primary cause of generalization gaps.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2736.0"
        },
        {
            "name_short": "Full Dialogue History (context)",
            "name_full": "Full Dialogue History as Encoder Context",
            "brief_description": "The agent's transformer encoder is given the entire dialogue history (along with setting, persona, and motivation) as part of the input context at each decision step; this serves as the agent's immediate working memory for the episode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LIGHT RL Agent (contextual encoder)",
            "agent_description": "Transformer-based encoder that consumes the full dialogue history plus setting/persona/motivation each step; no separate external memory module is instantiated — history is part of input context.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (LIGHT-Quests)",
            "game_description": "See LIGHT description above: text-adventure quests requiring action and dialogue.",
            "uses_memory": true,
            "memory_type": "working memory / context window",
            "memory_structure": "Sequential buffer: full dialogue history token sequence appended to input and encoded by transformer (no separate key-value or external store described).",
            "memory_content": "Past utterances and partner actions in the episode, enabling context-aware action/dialogue decisions.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Implicit: the encoder attends over the entire provided dialogue history (attention-based access); no explicit retrieval subsystems described.",
            "memory_update_strategy": "Updated online each timestep by appending the latest utterance/action to the dialogue history fed to the encoder.",
            "memory_usage_purpose": "Used to condition both action and dialogue policies to maintain persona-consistent behavior and to react to partner utterances.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper uses full dialogue history as part of the agent input but does not ablate removing it. It does emphasize that richer, varied training contexts (via procedurally generated curriculums) improve generalization, implying that context exposure matters.",
            "memory_limitations": "No explicit scaling/efficiency limits discussed, but the approach relies on encoding potentially long histories via transformer context which may be capacity-limited; no external long-term memory provided.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2736.1"
        },
        {
            "name_short": "Memorization of Trajectories",
            "name_full": "Trajectory Memorization (overfitting to head of training distribution)",
            "brief_description": "An identified failure mode where agents learn to memorize specific sequences of actions and dialogues from commonly seen training quests, resulting in poor generalization to novel or rarer quest types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "n/a (behavioral phenomenon)",
            "agent_description": "Not an agent architecture but a behavioral/learning phenomenon: agents that memorize trajectories perform well on seen quests but fail zero-shot on novel quests.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (LIGHT-Quests)",
            "game_description": "Text-adventure quests; memorization manifests as failure to generalize to rare verb/noun quest types in LIGHT-Quests test set.",
            "uses_memory": true,
            "memory_type": "memorization (overfitted episodic traces)",
            "memory_structure": "Implicit stored trajectories / policy-conditioned sequences (no explicit structure described).",
            "memory_content": "Stored sequences of actions and dialogues corresponding to frequently-seen quests in training data.",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "Learned implicitly during RL optimization (policy overfits to frequent trajectories).",
            "memory_usage_purpose": "Leads to successful completion of frequently seen quests but poor zero-shot transfer to rare/unseen quests.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Paper explicitly states that memorization of trajectories causes the generalization gap. Mitigation: training on procedurally generated, diverse curricula (increasing variety and flattening verb distributions) reduces the ability to memorize trajectories and substantially improves zero-shot performance. Sequential curriculum progression (from easier/unaugmented pools to flatter, rarer pools) yields better results than directly training on a pool dominated by rare quests.",
            "memory_limitations": "Memorization is harmful for generalization; increasing training diversity is required to make memorization infeasible. The paper notes that simply oversampling rare quests (sampled curriculum) helps less than procedurally generating diverse instances, and totally random generation harms performance by creating incoherent worlds.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2736.2"
        },
        {
            "name_short": "Graph-based / Belief-graph memories (related work)",
            "name_full": "Dynamic belief graphs / knowledge-graph memory approaches for text games",
            "brief_description": "Related literature approaches that represent and maintain structured world knowledge (objects, locations, entities, relations) as graphs to support decision-making in partially observable text games.",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "mention_or_use": "mention",
            "agent_name": "various (e.g., dynamic belief-graph agents, graph-constrained RL agents)",
            "agent_description": "Agents that maintain an explicit graph-like internal representation (belief/state graph or knowledge graph) of the environment; used in prior work to ground language and track discovered entities and relations.",
            "base_model_size": null,
            "game_benchmark_name": "text-based game benchmarks (e.g., Jericho/TextWorld/LIGHT)",
            "game_description": "Partially-observable text-adventure games where agents must discover and reason over objects, locations, and relations; graph memories encode discovered state for planning and generalization.",
            "uses_memory": true,
            "memory_type": "graph-based memory / belief graph / knowledge graph",
            "memory_structure": "Graph structure encoding entities (nodes) and relations (edges); sometimes dynamic (updated over time) to represent agent beliefs about world state.",
            "memory_content": "Discovered objects, entities, relations, visited rooms, inferred state needed for planning (e.g., which object is located where, which characters hold which objects).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Query-based graph lookup and graph-structured encodings used as inputs to policies or as constraints on action generation; retrieval often via learned encoders that score candidate nodes/edges.",
            "memory_update_strategy": "Updated when new observations are made (observations parsed and added/updated in the graph); dynamics differ by specific prior work but generally per-observation updates.",
            "memory_usage_purpose": "Grounding language, constraining and informing action selection in partially observable environments, improving generalization by abstracting state beyond raw trajectories.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Mentioned as prior approaches for representing agent knowledge and improving generalization; paper cites these works as relevant but does not experimentally compare them. The authors note that grounding with structured knowledge (e.g., knowledge graphs) has been used to produce worlds and quests in prior work.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2736.3"
        },
        {
            "name_short": "ATOMIC-LIGHT (commonsense KG pretraining)",
            "name_full": "ATOMIC-LIGHT (domain-adapted fantasy commonsense knowledge graph)",
            "brief_description": "A domain-adapted commonsense knowledge-graph used as part of encoder pretraining to give the adaptive encoder semantic priors about actions and their consequences in the fantasy LIGHT domain.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Adaptive encoder (pretrained on ATOMIC-LIGHT and Reddit/LIGHT)",
            "agent_description": "Adaptive encoder: 12-layer transformer pretrained multitask on Reddit and ATOMIC-LIGHT commonsense data, then fine-tuned on LIGHT and used (frozen) with appended layers during RL.",
            "base_model_size": "≈256M (stated as 256 million parameters in paper for adaptive encoder pretraining)",
            "game_benchmark_name": "LIGHT (LIGHT-Quests)",
            "game_description": "See LIGHT description above.",
            "uses_memory": true,
            "memory_type": "semantic knowledge graph (commonsense memory)",
            "memory_structure": "Graph (ATOMIC-LIGHT) providing relational commonsense facts used during encoder pretraining; stored as dataset/weights in the encoder rather than a run-time graph store.",
            "memory_content": "Commonsense relations mapping motivations/goals to required subgoals/actions and objects (e.g., need-to-acquire relationships).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Encoded into model weights via pretraining tasks; retrieval is implicit via the encoder's representations when given context.",
            "memory_update_strategy": "Static at RL time (pretrained weights are frozen and appended layers are trained during RL as described).",
            "memory_usage_purpose": "Provides semantic priors that improve both action selection and dialogue naturalness, leading to higher zero-shot performance particularly on speech goals.",
            "performance_with_memory": "Adaptive encoder (pretrained with ATOMIC-LIGHT) + Procedurally generated curriculum: Act goals 50.6%, Speech goals 38.2%, All goals 37.3%.",
            "performance_without_memory": "Scratch encoder (no pretraining) + Procedurally generated curriculum: Act goals 47.7%, Speech goals 16.3%, All goals 15.5%.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Pretraining on ATOMIC-LIGHT (commonsense graph) yields consistent performance improvements across curricula, especially improving speech-goal rates; authors hypothesize that pretraining gives the adaptive model greater capacity to learn generalizable representations of generated environments.",
            "memory_limitations": "No explicit dynamic update of the knowledge graph at runtime; commonsense priors are encoded in model parameters rather than an updatable external memory.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2736.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 1,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "rating": 2,
            "sanitized_title": "enhancing_textbased_reinforcement_learning_agents_with_commonsense_knowledge"
        },
        {
            "paper_title": "Bring-ing stories alive: Generating interactive fiction worlds",
            "rating": 1,
            "sanitized_title": "bringing_stories_alive_generating_interactive_fiction_worlds"
        }
    ],
    "cost": 0.0172235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Situated Dialogue Learning through Procedural Environment Generation
Association for Computational LinguisticsCopyright Association for Computational LinguisticsMay 22-27, 2022 c 2022</p>
<p>Prithviraj Ammanabrolu 
Georgia Institute of Technology ‡ Allen Institute for AI</p>
<p>Renee Jia 
Georgia Institute of Technology ‡ Allen Institute for AI</p>
<p>Mark O Riedl 
Georgia Institute of Technology ‡ Allen Institute for AI</p>
<p>Situated Dialogue Learning through Procedural Environment Generation</p>
<p>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
the 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1May 22-27, 2022 c 2022
We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al., 2019)-a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution-an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zeroshot performance on never-before-seen quests.</p>
<p>Introduction</p>
<p>A key hypothesis in the pursuit towards creating goal-driven natural language-based agents posits that interactivity and environment grounding is critical for effective language learning (Barsalou, 2008;Bisk et al., 2020;Ammanabrolu and Riedl, 2021). Text games provide a platform on which to interactively train agents that can both act and speak in a situated manner-producing language that is both goal-driven and contextually relevant. Agents in text games operate-perceiving, acting in, and speaking to others in a world-entirely using textual natural language. These games are structured generally as sequential decision making problems in the form of puzzles or quests that must be completed to advance in the game.</p>
<p>As seen in Figure 1, we focus on creating agents in LIGHT (Urbanek et al., 2019), a large-scale crowdsourced fantasy text-adventure game, consisting of rich textual worlds-locations, objects, and characters with personas, and quests-motivations for each character. To complete these quests, an agent must: (1) maintain character via its persona; and (2) reason in a partially observable world about potential actions and utterances based on incomplete descriptions of the locations, objects, and other characters. This requires several human like competencies such as commonsense reasoning, dynamic natural language understanding, and operating in combinatorially sized language-based stateaction spaces. Although recent work has provided evidence showing that interactive language learning via reinforcement learning (RL) in text games can be significantly more sample efficient than static supervised learning (Ammanabrolu et al., 2021) when creating goal-driven natural language agents, their ability to robustly generalize to novel scenarios is limited.</p>
<p>In sequential decision making problems in particular, this generalization gap is the result of an agent simply memorizing trajectories, e.g. the sequence of actions and dialogues required to finish a game, and thus being unable to react in novel scenarios-i.e. the agent learns from the head the training data and simply memorizes the long tail. One way of decreasing this generalization gap is by training agents on procedurally generated environments-wherein the agent learns a family of parametrized tasks with a significantly larger state-action spaces than singular environments, thus effectively making the memorization of trajectories impossible (Justesen et al., 2018;Cobbe et al., 2020). Drawing inspiration from all of these ideas, we create a method that learns to create a training curriculum of increasingly more difficult novel procedurally generated environments.</p>
<p>Our contributions are threefold: (1) We present a method of parametrizing and generating a curriculum of environments in text games; (2) We show how to effectively train reinforcement learning agents on this curriculum; and (3) Provide an experimental study showing that our method enables significantly better generalization than those training on singular environments.</p>
<p>Procedural Environment Generation</p>
<p>This section describes our procedural generation pipeline as seen in Figure 2, starting with world and quest generation, followed by aligning both of them. There are two main kinds of models that we use for the different modules in this pipeline: retrieval and generative.</p>
<p>The LIGHT Questing Environment. The LIGHT game environment (Urbanek et al., 2019) 1 is a multi-user fantasy text-adventure game consisting of a rich, diverse set of 1775 characters, 663 locations, and 3462 objects. Characters are able to perform templated actions to interact with both objects and characters, and can speak to other characters through free form text dialogues. Actions in text games generally consist of verb phrases (VP) followed optionally by prepositional phrases (VP PP). For example, get OBJ, put OBJ, give OBJ to CHAR, etc.. These actions change the state of the world which is expressed through text descriptions. 1 https://parl.ai/projects/light Quests in LIGHT (Ammanabrolu et al., 2021) take the form of a short motivation and goal action that is required reach the world state required to finish the game. For example, if the short motivation is "Your motivation is to acquire a sword", then the corresponding goal state would be for the character to have a sword in their inventory and goal action would be get sword. This environment also contains a set of human expert demonstration of people speaking and acting in character while playing the quests mentioned above. Further details are found in Appendix A.1.</p>
<p>World and Quest Creation</p>
<p>World Retrieval. The first step of the pipeline involves choosing an initial character who will perform the quest. For this, we uniformly randomly sample from the set of characters found in the LIGHT-Quest training set. The corresponding character information includes a name and a description of the character's persona. Given this character information, we further retrieve the location where the character is most likely to be found.</p>
<p>Retrieval models are trained to return the most highly correlated output for a given input in the dataset. For example, a retrieval model can be asked to return the most likely character that can be found at a particular location. These models compare a human annotated gold standard label with negative candidates drawn from the dataset. The negative candidates provide noise that the model must filter out in order to learn representations that let it best predict the gold label. These models are trained via a ranking loss that maximizes the scores of the gold label while simultaneously minimizing negative candidate score. At test time, the highest ranked candidate based on the score is selected as the model prediction.</p>
<p>Specifically, we use a retrieval-based ranker model that checks for similarity of StarSpace (Wu et al., 2018) embeddings. Our choice of model is influenced by Fan et al. (2019) who report stateof-the-art retrieval performance for locations in LIGHT using this model. The overall ranker model first trains a randomly initialized StarSpace embedding model that is designed to correlate characters with the locations they are found in. It learns a single bag-of-words embedding that takes into account all the individual words contained within the input-encoding character and location information as well as the previously mentioned negative retrieval candidates. The rest of the training is similar to other retrieval models described earlier. The retrieved location information consists of a location name as well as a description of the location.</p>
<p>Quest Generation. The quest is now generated using the existing character and location information. The generation-based models used in this pipeline are trained to return the most likely output sequence given an input sequence. Given a target sequence Y = {y 1 , ..., y M } and some input context vector via the encoders X. These models use autoregressive decoding techniques that factor the distribution over the target sequence into a chain of conditional probabilities with a causal left to right structure as P (Y |X; θ) = M +1 i=1 p(y i |y 0:i−1 , X; θ) where θ represents the current network parameters. At test time, a special start-of-sequence token is provided to the model which then proceeds to decode the rest of the output sequence using beam search.</p>
<p>We train two BART (Lewis et al., 2020) models that encodes input information via a bidirectional transformer encoder and decodes autoregressively: the first takes as input character and location information and produces a short motivation (Section 2); the second takes as input character, location information, short motivation and produces the sequence of LIGHT game engine executable actions needed to achieve the motivation. This sequence of actions is provided by the human expert demonstrations as mentioned in Section 2.</p>
<p>Aligning Worlds and Quests</p>
<p>At this stage, the environment contains a motivated main character to perform a quest and a location for them to start in. We now focus on aligning the world with the quest to ensure that the quest is playable and achievable. Intuitively, to ensure that a quest is achievable, the world needs to contain all of the entities-locations, characters, and objectsmentioned within the quest.</p>
<p>To this end, the alignment process involves training three BERT-based (Devlin et al., 2018) biencoder retrieval models to retrieve the most likely characters, locations, and objects required flesh the environment out and make the quest achievable. We use the same biencoder architecture proposed by Urbanek et al. (2019) which encodes context using one transformer and candidates with anotherscoring candidates via inner product between the two encoded vectors. The character retrieval model is conditioned on the initial character, quest, and location-producing additional characters required to complete the world.</p>
<p>We follow the setup in Ammanabrolu et al. (2021) and restrict worlds to only contains 2 characters at maximum but note that this method is extendable to greater numbers of characters. Similarly, the location retrieval model is also conditioned on the same things-producing, in this case, 4 neighbors to the initial location (resulting in worlds that are 5 locations large). These locations are connected to the initial location and a character can move between them by using commands such as go west, go up etc.. Once these characters and locations are added to the world, the object retrieval model predicts the set of objects that are required to be distributed for each location given all the character information present in it. The final game environment instance is complete once this object set has been added.</p>
<p>Curriculum Learning</p>
<p>Generating Curriculums. We generate curriculums by building off of our procedural LIGHT game instance generation pipeline. We make the observation that the original quests in LIGHT are heavily skewed towards certain quest types-with the majority involving goals and short motivations that contain objectives related to getting an object, and hitting or hugging another character (Figure 3). We further note that the first verb in the short motivation forms the basis of the quest for that agent.</p>
<p>Actions in LIGHT, and more generally in text games, are executed in the game engines on the basis of verbs-engine subroutines are linked to verbs with nouns forming arguments-and as such are primarily responsible for changing the state of the world. For example, get sword invokes the get subroutine that places an object, in this case a sword, in the character's surrounding into their inventory. As the quest is generated early in the pipeline, with the world and the rest of the components being conditioned on it, we can say that the first verb in the short motivation is an important dimension along which we can assess the distribution of individual LIGHT game instances. Thus, concretely, the verb counts from the short motivation aggregated over a set of quests represents the primary dimension along which we measure the distribution of quests.</p>
<p>Parametrizing Curriculum Difficulty. Given the relative imbalance of this multinomial distribution, as seen in Figure 3, we hypothesize that a LIGHT agent only learns to do well on certain types of objectives and not others-memorizing trajectories for less seen quest types, i.e. those found in the tail of the distribution. Preliminary evidence for this hypothesis is also seen in Prabhumoye et al. (2020), where they show a positive correlation between the number of instances of a particular type of quest during training and the final test goal-achievement performance. Based on these observations and our initial hypothesis, we use this particular dimension to parametrize curriculum difficulty for training LIGHT agents-quest types that are rarer in the initial training data will be harder for the agent to generalize to in a zero-shot setting.</p>
<p>Intuitively, we seek to create curriculums that contain a diverse set of game instances with quest types that are not often found in the initial training data. Our earlier observations let us hypothesize that this will enable the LIGHT agent to more effectively learn from rare instances of quests as op-   posed to memorizing the corresponding trajectories. To this end, the generated curriculums each consist of a pool of quests with steadily decreasing quest type imbalance. In our case, this imply that the flatness of the multinomial distribution increases until it tends towards being uniform with respect to the categorical quest type variable. This is done by running the procedural generation pipeline iteratively until the number of instances for the highest count quest type is within n of the lowest count quest type. The total number of additional generated instances is held fixed across curriculums, only the task distribution of quest types within each curriculum changes. Figure 6 shows that decreasing n has the intended effect of decreasing imbalance with respect to verb types. Generating using this pipeline has the added effect of increasing diversity within the pool of each available quest type. One measure of diversity within the pool of a single quest type is the types of nouns contained within the short motivations-these generally correspond to the characters, locations, and objects mentioned. Figure 6 shows that decreasing imbalance in the verb types for a short motivation also results in decreasing imbalance in noun types, once again corresponding to decreasing n. Short motivation gen-  eration is one of the first steps in the pipeline, i.e. the rest of the pipeline is conditioned on it, and as such increasing the flatness of the distribution there has the effects of increasing distribution for downstream components. A2C Curriculum Training. Overall training is done via A2C (Mnih et al., 2016) a policy gradient algorithm that maximizes long-term expected reward by comparing the advantage A(s t , a * t ) of taking an action a t in a state s t to the average value of taking any valid action as predicted by the critic V (s t ). The setup and network architectures used are similar to Ammanabrolu et al. (2021) and are summarized in Figure 5. At every step, the LIGHT agent receives as input the text describing the setting, the character's persona &amp; motivation, and the full dialogue history. This is then encoded using a transformer based encoder and sent to the action and dialogue policy networks which output an action/dialogue utterance. These are then passed into the LIGHT environment which process them and returns rewards to be used by the agent.</p>
<p>Rewards. As seen in Figure 5, all actions, either those of the agent-in-training or the partner agent, are processed by the engine, checking for goal state completion-hence known as act goals. For example, if the LIGHT agent had the motivation to acquire a sword, the goal could be completed via a: self act completion: where the agent acquires a sword itself by picking it up, stealing it, convincing the partner to drop theirs so you can pick it up, etc. partner act completion: where the agent uses dialogue utterances to convince their partner to achieve the goal for them (e.g., by persuading the </p>
<p>Evaluation</p>
<p>We conduct two separate evaluations: the first measures the effectiveness of the various models in the procedural environment generation pipeline as well as the effectiveness of the pipeline as a whole. The second provides zero-shot ablations of the LIGHT RL agents trained on the resulting curriculums and answers the questions (1) how does the relative difficulty of the training quests effect test performance?;</p>
<p>(2) how does the diversity of the environments during training effect test performance?; and (3) how are the results of the previous questions affected by pre-training?</p>
<p>Procedural Generation Evaluation</p>
<p>All of the models in the pipeline described in Section 2 are trained using only the training set of the original LIGHT and LIGHT-Quests data. LIGHT-Quests inherits characters, locations, and objects from the original LIGHT dataset and adds on motivations and goals in the form of quests. Thus, the character, location, and object retrieval models are evaluated on the LIGHT unseen test set and the motivation and goal generation models are evaluated on the LIGHT-Quests test set. We report the standard array of metrics: hits@10 and F1 ranking prediction score for retrieval models; and F1 (as a harmonic average of BLEU-1 (Papineni et al., 2002) and ROUGE-1 (Lin, 2004)) and perplexity for generative models. Hyperparameters for all models are found in Appendix A.6. Analysis. Table 1 presents the results of this evaluation. There are two primary trends to note:  (1) character retrieval is easier than retrieving location and objects; and (2) goal action generation is easier than motivation generation. We hypothesize that the first trend is a direct consequence of the fact that generated motivations and goals regularly contain the names of the characters involved but mostly leave implicit information such as the objects required-e.g. the action hit dragon as a knight would require a weapon such as a sword to be equipped first. The second trend stems from the fact that goal actions can often be thought of as condensed version of the short motivation-number of tokens required to generate goal actions is far less than short motivations. This implies that the goal action model is akin to a summarization model as opposed to the short motivation model which has the more difficult task of generating the motivation with only initial persona and location information.</p>
<p>Curriculum Learning Evaluation</p>
<p>This evaluation tests the LIGHT RL agent's ability to zero-shot generalize to unseen environments. For all experiments in this study, agents were each zero-shot evaluated on 211 human demonstrations from the LIGHT-Quests test set for a single episode per quest across three independent runs. They were measured on the basis of whether or not they were able to achieve their goals in the environments conditioned on their personas: act goals measuring their ability to act consistently, and speech goals reflecting their ability to speak naturally. The study ablates across three dimensions in order to answer the posed research questions relating to: (1) cur-riculum difficulty, (2) curriculum diversity, and (3) agent pre-training.</p>
<p>Curriculum Difficulty. To measure the overall effectiveness of the distribution tuning technique shown in Section 3, we vary the parameter n used to measure curriculum difficulty-note that a lower n corresponds to a flatter distribution and so is higher difficulty. As seen in Fig. 6, we generate pools of quests with steadily increasing difficulty with varying n based on the range of the original untuned distribution-with the agents being trained on each pool separately as well as all of them in sequence through a curriculum. Agents received 10 7 total environment interactions per parallel A2C agent in a batch of 16. For the curriculum learning method, the agent received 2.5 × 10 6 interactions per pool of quests starting with the initial pool of untuned quests and then sequentially with n = 64, 16, 2 resulting in a total of 10 7 total environment interactions per parallel A2C agent.</p>
<p>Curriculum Diversity. The variations in the combinations of quests and worlds themselves seen at training time has potential to effect zero-shot performance (Samvelyan et al., 2021). We introduce two baselines that change the relative diversities of resulting quests in the curriculums, to contrast with our proposed procedural generation pipeline. Generated quest details are found in Appendix A.5.</p>
<p>• Sampled Curriculums. Inspired by Chawla et al. (2002); Graves et al. (2017), we explore an alternate method of creating curriculums by simply oversampling the same rare quests found in the tails of the distributions.</p>
<p>This method does not generate new environments via the pipeline, instead choosing to sample rarer instances of quests with a higher weight when initializing each parallel A2C actor. This means that the distribution of verbs looks similar to what it is in Figure 6 but the quests within a pool are repeated multiple times and so contain no new diversity. • Randomly Generated Curriculums. On the other side of the diversity spectrum, we test a method that follows the same steps as the pipeline proposed in Section 2 with the modification that the selection process for each step in the pipeline is random. The characters, objects, location are randomly selected and the generated motivations per character are conditioned on these randomly created worlds. This results in a significantly higher diversity of quests per pool-at the expense of the relative coherence of the overall environment.</p>
<p>Pre-training. We test two model types, drawing from Ammanabrolu et al. (2021), to determine if pre-training effects curriculums learning.</p>
<p>• Scratch. No pre-training is done, the encoder is a 3-layer randomly initialized transformer and trained along with the policy networks. Analysis. Table 2 presents the results of this evaluation. We first report that the overall proportion of a pool of procedurally generated environments that contain achievable quests or goals for a single curriculum is 0.89. This metric provides a proxy for measuring the accuracy of the alignment process and the overall error rate of the pipeline. The high achievability rate means that only a small proportion of LIGHT RL A2C agents will waste  Table 2: Zero-shot goal achievement rates on a scale of 0-1, averaged over 3 random seeds with standard deviations not exceeding 0.02. The "All Goals" column refers to quests where the agent has simultaneously achieved both types of goals within the allotted one episode. The parameter n refers to the difference between the number of instances for the highest and lowest count quest types. All pair-wise comparisons made are statistically significant. environment interactions learning from quests that cannot be completed-increasing this rate even further would likely also improve sample efficiency. Further, we see that just the distribution tuning by itself shows no significant gains in performance over the baselines trained on the original data and in fact loses performance in certain cases. In contrast, learning from the individually tuned quest pools in a sequential curriculum increases performance significantly. This appears to indicate that LIGHT RL agents need to be trained with quests pools of steadily increasing difficulty-starting immediately on a set of quests with a high proportion of rare, generated quests can degrade performance.</p>
<p>The significantly increased performance of the procedurally generated curriculums over the sampled and randomly generated curriculums indicates the relative importance of diversity within a single quest type-but only up to a certain extent. The sampled quests contain multiple instances of the same quest type but the generated ones have higher variability-leading to an increased observation space, ensuring that the agent cannot simply memorize trajectories. On the other hand, randomly generated quests have even higher variability but sacrifice relative coherence-it is more likely that the world contains unlikely scenarios, e.g. a desert and swamp being located right next to each otherresulting in significantly decreased performance.</p>
<p>We'd finally like to note that the adaptive pretrained model takes advantage of the generated curriculums and distribution tuning more than the non-pre-trained scratch encoder, showing consistenly higher performance across the board. We hypothesize that this is likely a consequence of the adaptive model having greater model capacity-the pre-training enabling it to learn generalizable representations of the generated environments. Overall, trends in performance are independent of pretraining-both the scratch and the adaptive pretrained model benefit significantly from learning from the procedurally generated curriculums.</p>
<p>Related Work</p>
<p>Text-based Game Playing and Generation. Recent text game playing works have focused on tackling three primary challenges: (1) how to represent agent knowledge to effectively operate in partially observable environments (Adhikari et al., 2020; Sautier et al., 2020); (2) scaling RL algorithms to handle combinatorial natural language state-action spaces (Zahavy et al., 2018;Ammanabrolu and Hausknecht, 2020;Jang et al., 2021); and (3) giving agents commonsense priors to better reason about the world (Murugesan et al., 2020(Murugesan et al., , 2021 On the flip side, we have procedural generation of games with works such as Short and Adams (2017); Risi and Togelius (2019); Khalifa et al.</p>
<p>(2020) that focus on creating content especially for 2D visual games via search or reinforcement learning based methods. Ammanabrolu et al. (2020b,a) use knowledge graphs to ground language and produce worlds and quests separately for text games from existing corpora such as stories. Fan et al.</p>
<p>(2019) leverage LIGHT to learn to generate interactive fiction worlds on the basis of locations, characters, and objects-this work is closest in spirit to our own World Generation module later on. They all focus on either generating or playing games.</p>
<p>Goal oriented Dialogue. Sub-tasks within the overall task of goal oriented dialogue, such as dialogue state management (Singh et al., 2000;Pietquin et al., 2011;Fatemi et al., 2016) and response generation (Li et al., 2016) have used RL to boost performance. As noted by Ammanabrolu et al. (2021), the negotiation tasks of (Yarats and Lewis, 2017;Lewis et al., 2017), where two agents are trying to convince each other to perform certain actions, are related to the tasks in LIGHT-Quests. These works all lack environment grounding.</p>
<p>Curriculum Learning. Curriculums in reinforcement learning have traditionally been used to set goals of steadily increasing difficulty for an agent (Bengio et al., 2009;Schmidhuber, 2013). The difficulty of these curriculums are generally measured difficulty via proxy of agent performance (Narvekar et al., 2020)-methods either choose to adversarially set goals of steadily increasing difficulty (Sukhbaatar et al., 2018;Racaniere et al., 2019;Dennis et al., 2020;Campero et al., 2021) or to maximize learning performance based on environment instances an agent finds difficult historically (Graves et al., 2017;Portelas et al., 2020). While we were inspired by these works, they all focus on searching for goals for agents which can be difficult to scale to complex tasks such our own natural language motivation-based goals. We'd also like to note that most works using procedural generation to benchmark RL agents such as Cobbe et al. </p>
<p>Conclusions</p>
<p>We focus on the problem of improving zero-shot generalization abilities of goal-driven RL agents to act and speak via natural language. An (obviously) key component of achieving this is to train the RL agents on a balanced training dataset that matches the test data in distribution. As this is an unlikely scenario in most real-world applications, we make the observation that we can artificially augment our pool of training environments by generating curriculums to mimic this. In our text game domain, with goal-driven situated natural language agents, we hypothesize-and gather supporting evidence suggesting-that an effective way to parametrize such distributions is by looking at the primary verbs within an agent's motivation and bringing the distribution of verb types as close to uniform as possible. Curriculum training significantly increases an agent's ability to generalize to novel scenarios.</p>
<p>Broader Impacts</p>
<p>As noted by Urbanek et al. (2019) and Ammanabrolu et al. (2021), the ability to speak and act in these textual fantasy worlds has implications for domains beyond text-games. Text games are a platform where agents can interact in a relatively isolated environment and learn to interactively communicate effectively through natural language in a situated manner. Our methods use both large language models and deep reinforcement learning and are prone to the pitfalls that other contemporary methods using these techniques face, especially in the areas of dialogue and text game systems. We mitigate this first pitfall by restricting our current system to a retrieval based dialogue, ensuring that we can filter out non-normative dialogue usages beforehand, though we will note that the system can be extended to generative systems as described in Prabhumoye et al. (2020). Further, the LIGHT dataset is crowdsourced and contains data biases that can be attributed to the crowdworkers tasked with creating the data. Dinan et al. (2020) provides an in depth discussion regarding the inherent dataset biases, such as gender bias in the distribution of characters, in LIGHT and techniques to mitigate them-we follow these methods to reduce their effects on both the environment generation and agent training procedures. </p>
<p>References</p>
<p>A Appendix</p>
<p>A.1 LIGHT Environment Details</p>
<p>Formally, we adapt the definition of text-based games as seen in (Côté et al., 2018;Hausknecht et al., 2020) to LIGHT. They are partially observable Markov decision processes (POMDPs), represented as a 7-tuple of S, T, A, Ω, O, R, γ representing the set of environment states, conditional transition probabilities between states, the vocabulary or words used to compose action commands or dialogue utterances (e.g. get sword or Hey, give me that sword! respectively), observations returned by the game, observation conditional probabilities, reward function, and the discount factor respectively. There are 5982 training, 756 validation, and 748 test quests. The average sequence of a human demonstration is 12.92, with an average action sequence length of 2.18 and dialogue of 10.74. There are 1800 training, 100 validation, and 211 test human expert demonstrations corresponding to the same splits as the quests themselves.</p>
<p>The LIGHT environment further allows us to factorize the overall action space A into A as the set of possible textual actions or commands (e.g. get sword, steal coins from merchant), and U as the set of possible dialogues that can be uttered by an agent, thus making it a factored POMDP (Degris and Sigaud, 2013). This in turn means that, for a given quest q, each expert human demonstration D(q) = α * 0 , α * 1 ...α * n can be factorized into two sub-sequences of expert demonstrations of actions and dialogue D A (q) = a * 0 , a * 1 , ...a * n and D U (q) = u * 0 , u * 1 , ...u * m respectively. The factorized action spaces A and U are constructed by enumerating all possible actions/dialogue utterances in the all human demonstrations in LIGHT-quests. Figure 7 shows the overall architecture and training pipeline-our reinforcement learning pipeline is unchanged from that shown in Ammanabrolu et al. (2021) with the exception of the curriculum of quests performed by the agent and the way the speech rewards are designed. An encoder first takes in information about setting, persona, motivation for a single character then passes it onto a switch module. This switch module is a meta policy that decides if an agent should act or talk and is trained to mimic how often human experts act or talk while performing quests via demonstrations. Two separate policy networks make a decision on which action to perform or dialogue to say given the current context and a single shared critic attempts to measure the value of taking an action in a particular state.</p>
<p>Once an agent acts or talks, the partner agent-in this case also a polyencoder (Humeau et al., 2020) trained to react to agents with motivations-also acts or talks and this information is processed by the environment. As recommended by Prabhumoye et al. (2020);Ammanabrolu et al. (2021), we keep the partner model fixed during the episodes where the LIGHT agent trains to ensure that it retains natural English semantics-avoiding the problem of language drift by learning an emergent language with that must agree with the partner's usage (Lee et al., 2019).</p>
<p>A2C Training. Each parallel A2C agent samples from the the current pool of available questsi.e. the curriculum-for a fixed number of steps k before switching to the quest pool corresponding to the next higher level difficulty curriculum. The initial pool of quests is the training set of LIGHT-Quests as seen in Ammanabrolu et al. (2021) and all pools after that correspond to decreasing values of n used when generating the curriculums (as seen in Figure 6).</p>
<p>Rewards. Following Ammanabrolu et al. (2021), we use a learned model-the Dungeon Master (DM)-to score the agent's ability to speak. The DM used here is a poly-encoder model trained on collected human quest demonstrations as well as the original conversations in LIGHT. It is conditioned on quests and motivations and thus able to provide a (noisy) indication of how natural the agent's dialogue utterances are given its immediate context, similarly to the function of the DM during data collection.</p>
<p>Given the dialogue portion of a human quest demonstration D U (q) = u * 0 , u * 1 , ...u * n , of length n, the DM returns a reward r u of 1 2n if an utterance was in the demonstration u ∈ D U (q) (for a maximum of one time per episode for each utterance from the demonstration). A further 1 2n is given each time the utterance is scored as being within the topk most likely utterances by the DM. The original quests all have human demonstrations but the procedurally generated ones do not. During training, in cases where a particular LIGHT game instance does not have corresponding human demonstration, only the latter reward resulting from an utterance being within the top-k most likely utterances by the DM is used. This naturalness objective will be hence referred to as a speech goal. These rewards thus also denser than act goals, helping the agent learn overall. Further, similarly to the game engine, the DM also provides a set of M valid utterances which are the M most likely dialogue candidates from the candidate set for the current context.</p>
<p>A.2 Encoder Pre-training Tasks</p>
<p>Here, we summarize the pre-training tasks for the encoders mentioned in Section 4.2. These tasks are unchanged from those described in Ammanabrolu et al. (2021). ATOMIC-LIGHT. ATOMIC-LIGHT is a (domain-adapted) fantasy commonsense knowledge graph, and as such provides priors for an agent on how to act consistently in the world. For example, given a clause such as "The knight wishes to slay the dragon, as a result the knight needs to acquire a sword," the task would be to predict the underlined text-a form of knowledge graph completion (Wang et al., 2017).</p>
<p>Reddit. A further tuning dataset is derived from an existing Reddit dataset, pushshift.io (Baumgartner et al., 2020) as seen in Roller et al. (2020). This dataset has been used in several existing dialoguebased studies and has been shown to result in more natural conversations (Yang et al., 2018;Mazaré et al., 2018).</p>
<p>LIGHT-Original. The task itself dervied from the original LIGHT dataset (Urbanek et al., 2019) and involves predicting the next action or utterance given the prior dialogue history as well as the current setting and persona for a character. They are collected in a chit-chat fashion, with no notion of objectives, and so provide priors on how to generally act consistently and speak in a fantasy world, but not directly how to complete quests.</p>
<p>LIGHT-Quests. This dataset provides two pretraining tasks. (1) Bag-of-action timeline predic-tion in which, given a quest consisting of setting, persona, and motivations, any one of the actions in the timeline must be predicted. (2) Sequential timeline prediction in which, given a quest consisting of setting, persona, motivations, and the first n actions in the timeline, the n + 1 th action must be predicted. (3) Predict the next dialogue utterance given a human demonstration in a manner similar to the LIGHT-original tasks.</p>
<p>A.3 Sampled and Randomly Generated Curriculum Distributions</p>
<p>This section contains the verb and noun distributions for the sampled and randomly generated curriculums as described in Section 4.2, presented in the same fashion as Figure 6. For the randomly generated curriculums, we present 5 different curriculums-varying the proportion of randomly generated quests per pool from 0% (corresponding to the full procedurally generated pipeline), to 100% randomly generated, in increments of 20%. Sections after this present ablation results after training agents on these curriculums to better analyze the effects of randomness and diversity in zero-shot generalization.       Procedural Short Motivation Generation, n=2, randomness=100 Figure 11: Distribution of nouns in the short motivation of the curriculum of quests starting from the original distribution on the left to the flattened and randomly generated curriculum on the right as a function of n (Section 3) with the randomness percentage tuning. The y-axis of the different nouns reflect their relative proportion in the pool of quests.</p>
<p>A.4 Effects of Diversity in Procedural</p>
<p>Generation Pipeline on Curriculum Learning Table 3 shows the results of a zero-shot evaluation as described in Section 4.2 on each of the randomly generated curriculum pools. Agents were trained on the full curriculum for each of these experiments.</p>
<p>One major trend stands out: the less randomness during environment generation, the greater the performance. This shows that, while more diverse (as seen in  Table 3: Effects of diversity in procedural generation on curriculum learning. All experiments were averaged over 3 random seeds. Standard deviations across any individual result do not exceed 0.02. The "All Goals" column refers to quests where the agent has simultaneously achieved both types of goals within the allotted one episode. The parameter n refers to the difference between the number of instances for the highest and lowest count quest types.</p>
<p>A.5 Curriculum Statistics</p>
<p>This section presents statistics attempting to quantify the diversity and relative coherence of the environments in each of the curriculums we test on. We quantify diversity in terms of the unique entities present overall in the world as well as the number of unique uni-,bi-, and tri-grams found in the generated short motivations and goal texts. Specifically, unique entities were calculated by using the count of all the unique objects and character which are generated in the procedural generated short motivations / goals. In addition, the count of the unique uni-grams /bi-grams /tri-grams represent the n-grams counts changing with the procedurally generated curriculum as a function of n (Section 3) with the randomness percentage tuning for both the short motivations and goals generated by BART. As a sanity check on how coherent an environment is, we attempt to see if the entities required to finish a quest even exist within the world-i.e. a hit percentage that roughly estimates what proportion of quests in a pool are achievable end to end. The hit percentage are calculated by checking if the NOUN extracted from the short motivations/goals exists in the procedually generated entities (objects + character) in the same environment. Counting as 1/0 to represent as existing/not and divided by the total number of quests to get the hit percentage in the      </p>
<p>Figure 1 :
1The LIGHT questing environment presented as a 2 player game deployed in Messenger.</p>
<p>Figure 2 :
2Procedural environment generation pipeline. Black lines indicate conditioning on all prior components. Gold lines indicate (adjacent) location placement.</p>
<p>Figure 3 :
3Normalized top-20 verb count distribution of short motivations of the LIGHT-Quests dataset.</p>
<p>Figure 4 :
4Normalized top-20 noun count distribution of short motivations of the LIGHT-Quests dataset.</p>
<p>Figure 5 :
5Architecture and training pipeline for the LIGHT RL Agent (Ammanabrolu et al., 2021).</p>
<p>Figure 6 :
6Top-20 distribution of verbs (top) and nouns (bottom) in the short motivation of the curriculum of quests starting from the original generated curriculum on the left to the flattened, generated curriculum on the right as a function of n (Section 3). The y-axis of the reflects normalized overall count in the pool of quests.</p>
<p>•
Adaptive. Pre-training is done on the tasks introduced in Ammanabrolu et al. (2021) by training a 12 layer transformer with 256 million parameters using a cross-entropy loss as seen inHumeau et al. (2020). These weights are then transferred to the encoder used during RL training then frozen with 3 randomly initialized-layers appended. The encoder is multi-task trained on both pushshift.ioReddit (Baumgartner et al., 2020)  and the commonsense dataset ATOMIC-LIGHT(Ammanabrolu et al., 2021), giving the agent general priors on how to act and speak. It is then fine-tuned in LIGHT, giving the agent further domain specific priors. Specific task details are provided in Appendix A.1.</p>
<p>(2020); Küttler et al. (2020); Samvelyan et al. (2021) rely on the underlying richness of the game engines to generate novel environments as opposed to learning to generate.</p>
<p>Figure 7 :
7Expanded overall architecture and training pipeline diagram for the LIGHT RL Agent (Ammanabrolu et al., 2021).</p>
<p>Figure 8 :
8Distribution of verbs in the short motivation of the curriculum of quests starting from the original distribution on top to the flattened and sampled curriculum on the bottom as a function of n (Section 3). The y-axis of the different nouns reflect their relative proportion in the pool of quests.</p>
<p>Figure 9 :
9Distribution of nouns in the short motivation of the curriculum of quests starting from the original distribution on top to the flattened and sampled curriculum on the bottom as a function of n (Section 3). The y-axis of the different nouns reflect their relative proportion in the pool of quests.</p>
<p>Figure 10 :
10Distribution of verbs in the short motivation of the curriculum of quests starting from the original distribution on the left to the flattened and randomly generated curriculum on the right as a function of n (Section 3) with the randomness percentage tuning. The y-axis of the different verbs reflect their relative proportion in the pool of quests.</p>
<p>Mehdi Fatemi, Layla El Asri, Hannes Schulz, JingHe, and Kaheer Suleman. 2016. Policy networks with two-stage training for dialogue systems. arXiv preprint arXiv:1606.03152.Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre 
Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Ro-
main Laroche, Pascal Poupart, Jian Tang, Adam 
Trischler, and Will Hamilton. 2020. Learning dy-
namic belief graphs to generalize on text-based 
games. Advances in Neural Information Processing 
Systems, 33. </p>
<p>Prithviraj Ammanabrolu, William Broniec, Alex 
Mueller, Jeremy Paul, and Mark O. Riedl. 
2020a. Toward automated quest generation in text-
adventure games. In International Conference on 
Computational Creativity (ICCC). </p>
<p>Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, 
William Broniec, and Mark O Riedl. 2020b. Bring-
ing stories alive: Generating interactive fiction 
worlds. In Proceedings of the Sixteenth AAAI Con-
ference on Artificial Intelligence and Interactive Dig-
ital Entertainment (AIIDE-20). </p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 
2020. Graph Constrained Reinforcement Learning 
for Natural Language Action Spaces. In Interna-
tional Conference on Learning Representations. </p>
<p>Prithviraj Ammanabrolu and Mark O Riedl. 2021. Sit-
uated language learning via interactive narratives. 
Patterns, Cell Press. </p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, 
Arthur Szlam, Tim Rocktäschel, and Jason Weston. 
2021. How to motivate your dragon: Teaching goal-
driven agents to speak and act in fantasy worlds. 
In Proceedings of 2021 Annual Conference of the 
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, NAACL-HLT 2021. </p>
<p>Lawrence W. Barsalou. 2008. Grounded cognition. An-
nual Review of Psychology, 59(1):617-645. PMID: 
17705682. </p>
<p>Jason Baumgartner, Savvas Zannettou, Brian Keegan, 
Megan Squire, and Jeremy Blackburn. 2020. The 
pushshift reddit dataset. In Proceedings of the Inter-
national AAAI Conference on Web and Social Media, 
volume 14, pages 830-839. </p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, 
and Jason Weston. 2009. Curriculum learning. In 
Proceedings of the 26th annual international confer-
ence on machine learning, pages 41-48. </p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob 
Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap-
ata, Angeliki Lazaridou, Jonathan May, Aleksandr 
Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. 
Experience grounds language. In Proceedings of the 
2020 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 8718-8735, 
Online. Association for Computational Linguistics. </p>
<p>Andres Campero, Roberta Raileanu, Heinrich Kuttler, 
Joshua B. Tenenbaum, Tim Rocktäschel, and Ed-
ward Grefenstette. 2021. Learning with {amig}o: 
Adversarially motivated intrinsic goals. In Interna-
tional Conference on Learning Representations. </p>
<p>Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, 
and W Philip Kegelmeyer. 2002. Smote: synthetic 
minority over-sampling technique. Journal of artifi-
cial intelligence research, 16:321-357. </p>
<p>Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schul-
man. 2020. Leveraging procedural generation to 
benchmark reinforcement learning. In International 
conference on machine learning, pages 2048-2056. 
PMLR. </p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben 
Kybartas, Tavian Barnes, Emery Fine, James Moore, 
Matthew Hausknecht, Layla El Asri, Mahmoud 
Adada, Wendy Tay, and Adam Trischler. 2018. 
Textworld: A learning environment for text-based 
games. CoRR, abs/1806.11532. </p>
<p>Thomas Degris and Olivier Sigaud. 2013. Factored 
markov decision processes. Markov Decision Pro-
cesses in Artificial Intelligence, pages 99-126. 
Michael Dennis, Natasha Jaques, Eugene Vinitsky, 
Alexandre Bayen, Stuart Russell, Andrew Critch, 
and Sergey Levine. 2020. Emergent complexity 
and zero-shot transfer via unsupervised environment 
design. Neural Information Processing Systems 
(NeurIPS). </p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 
Kristina Toutanova. 2018. BERT: pre-training of 
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805. </p>
<p>Emily Dinan, Angela Fan, Adina Williams, Jack Ur-
banek, Douwe Kiela, and Jason Weston. 2020. 
Queens are powerful too: Mitigating gender bias in 
dialogue generation. In Proceedings of Empirical 
Methods in Natural Language Processing (EMNLP-
20). </p>
<p>Angela Fan, Jack Urbanek, Pratik Ringshia, Emily 
Dinan, Emma Qian, Siddharth Karamcheti, Shri-
mai Prabhumoye, Douwe Kiela, Tim Rocktaschel, 
Arthur Szlam, and Others. 2019. Generating Inter-
active Worlds with Text. In In proceedings of the 
Thirty-Third AAAI Conference on AI (AAAI-19). </p>
<p>Alex Graves, Marc G Bellemare, Jacob Menick, Rémi 
Munos, and Koray Kavukcuoglu. 2017. Automated 
curriculum learning for neural networks. In Inter-
national Conference on Machine Learning, pages 
1311-1320. </p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-
Alexandre Côté, and Xingdi Yuan. 2020. Interac-
tive fiction games: A colossal adventure. In Thirty-
Fourth AAAI Conference on Artificial Intelligence 
(AAAI). </p>
<p>Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, 
and Jason Weston. 2020. Poly-encoders: Architec-
tures and pre-training strategies for fast and accurate 
multi-sentence scoring. In International Conference 
on Learning Representations. </p>
<p>Youngsoo Jang, Seokin Seo, Jongmin Lee, and Kee-
Eung Kim. 2021. Monte-carlo planning and learn-
ing with language action value estimates. In Inter-
national Conference on Learning Representations. </p>
<p>Niels Justesen, Ruben Rodriguez Torrado, Philip Bon-
trager, Ahmed Khalifa, Julian Togelius, and Sebas-
tian Risi. 2018. Illuminating generalization in deep 
reinforcement learning through procedural level gen-
eration. arXiv preprint arXiv:1806.10729. </p>
<p>Ahmed Khalifa, Philip Bontrager, Sam Earle, and Ju-
lian Togelius. 2020. Pcgrl: Procedural content gen-
eration via reinforcement learning. In Proceedings 
of the AAAI Conference on Artificial Intelligence 
and Interactive Digital Entertainment, volume 16, 
pages 95-101. </p>
<p>Heinrich Küttler, Nantas Nardelli, Alexander H. Miller, 
Roberta Raileanu, Marco Selvatici, Edward Grefen-
stette, and Tim Rocktäschel. 2020. The NetHack 
Learning Environment. In Proceedings of the Con-
ference on Neural Information Processing Systems 
(NeurIPS). </p>
<p>Jason Lee, Kyunghyun Cho, and Douwe Kiela. 2019. 
Countering language drift via visual grounding. 
arXiv preprint arXiv:1909.04499. </p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer 
Levy, Veselin Stoyanov, and Luke Zettlemoyer. 
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation, 
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational 
Linguistics, pages 7871-7880, Online. Association 
for Computational Linguistics. </p>
<p>Mike Lewis, Denis Yarats, Yann N Dauphin, Devi 
Parikh, and Dhruv Batra. 2017. Deal or no deal? 
end-to-end learning for negotiation dialogues. arXiv 
preprint arXiv:1706.05125. </p>
<p>Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, 
Jianfeng Gao, and Dan Jurafsky. 2016. Deep rein-
forcement learning for dialogue generation. CoRR, 
abs/1606.01541. </p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74-81, Barcelona, Spain. 
Association for Computational Linguistics. </p>
<p>Pierre-Emmanuel Mazaré, Samuel Humeau, Martin 
Raison, and Antoine Bordes. 2018. Training mil-
lions of personalized dialogue agents. In Proceed-
ings of the 2018 Conference on Empirical Methods 
in Natural Language Processing, pages 2775-2779, 
Brussels, Belgium. Association for Computational 
Linguistics. </p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi 
Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, 
David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning. 
In International conference on machine learning, 
pages 1928-1937. </p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapani-
pathi, Pushkar Shukla, Sadhana Kumaravel, Gerald 
Tesauro, Kartik Talamadupula, Mrinmaya Sachan, 
and Murray Campbell. 2021. Text-based RL Agents 
with Commonsense Knowledge: New Challenges, 
Environments and Baselines. In Thirty Fifth AAAI 
Conference on Artificial Intelligence. </p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, 
Mrinmaya Sachan, Pavan Kapanipathi, and Kar-
tik Talamadupula. 2020. Enhancing text-based 
reinforcement learning agents with commonsense 
knowledge. arXiv preprint arXiv:2005.00811. 
Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko </p>
<p>Normalized Noun Count Short Motivation Generation without Distribution Tuning, randomness=0 Normalized Noun Count Short Motivation Generation without Distribution Tuning, randomness=20 Normalized Noun Count Procedural Short Motivation Generation, n=2, randomness=20 Normalized Noun Count Short Motivation Generation without Distribution Tuning, randomness=40 Normalized Noun Count Procedural Short Motivation Generation, n=64, randomness=40 Normalized Noun Count Procedural Short Motivation Generation, n=2, randomness=40 Normalized Noun Count Short Motivation Generation without Distribution Tuning, randomness=60 Normalized Noun Count Procedural Short Motivation Generation, n=64, randomness=60 Normalized Noun Count Procedural Short Motivation Generation, n=16, randomness=60 Normalized Noun Count Procedural Short Motivation Generation, n=2, randomness=60 Normalized Noun Count Short Motivation Generation without Distribution Tuning, randomness=80 Normalized Noun Count Procedural Short Motivation Generation, n=64, randomness=80 Normalized Noun Count Procedural Short Motivation Generation, n=16, randomness=80 Normalized Noun Count Procedural Short Motivation Generation, n=2, randomness=80 Normalized Noun Count Short Motivation Generation without Distribution Tuning, randomness=100 Normalized Noun Count Procedural Short Motivation Generation, n=64, randomness=100 Normalized Noun Count Procedural Short Motivation Generation, n=16, randomness=100lions 
lion 
tree 
sword 
plant 
tool 
piece 
water 
bucket 
flower 
book 
coin 
man 
person 
chair 
pike 
bottle 
vulture 
goblet 
fish 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>lions 
book 
water 
sword 
rifle 
crow 
lever 
tool 
grill 
piece 
goblet 
band 
something 
way 
tape 
pick 
lion 
blanket 
chair 
branch 
0.00 </p>
<p>0.02 </p>
<p>0.04 </p>
<p>0.06 </p>
<p>0.08 </p>
<p>0.10 </p>
<p>Normalized Noun Count </p>
<p>Procedural Short Motivation Generation, n=64, randomness=0 </p>
<p>lions 
tool 
sword 
lever 
water 
rifle 
book 
crow 
grill 
goblet 
piece 
pick 
band 
blanket 
way 
something 
tape 
lion 
work 
pool 
0.00 </p>
<p>0.01 </p>
<p>0.02 </p>
<p>0.03 </p>
<p>0.04 </p>
<p>0.05 </p>
<p>0.06 </p>
<p>0.07 </p>
<p>0.08 </p>
<p>Normalized Noun Count </p>
<p>Procedural Short Motivation Generation, n=16, randomness=0 </p>
<p>sword 
lions 
water 
book 
tool 
crow 
lever 
grill 
rifle 
piece 
goblet 
blanket 
pick 
way 
tape 
something 
branch 
band 
chair 
lion 
0.00 </p>
<p>0.01 </p>
<p>0.02 </p>
<p>0.03 </p>
<p>0.04 </p>
<p>0.05 </p>
<p>0.06 </p>
<p>0.07 </p>
<p>Normalized Noun Count </p>
<p>Procedural Short Motivation Generation, n=2, randomness=0 </p>
<p>tunic 
lions 
dirty 
lion 
tree 
tool 
coin 
flower 
monk 
hole 
rug 
book 
person 
son 
piece 
bar 
suit 
bone 
worm 
pike 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>0.6 </p>
<p>0.7 </p>
<p>tunic 
lions 
dirty 
lion 
spoon 
tree 
hole 
cobbler 
lit 
head 
king 
person 
bandage 
monk 
coin 
flower 
man 
book 
priest 
tool 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>0.6 </p>
<p>Normalized Noun Count </p>
<p>Procedural Short Motivation Generation, n=64, randomness=20 </p>
<p>tunic 
spoon 
lions 
dirty 
hole 
cobbler 
lion 
king 
lit 
bandage 
head 
tree 
spike 
tape 
book 
person 
man 
bouquet 
piece 
coin 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>0.6 </p>
<p>Normalized Noun Count </p>
<p>Procedural Short Motivation Generation, n=16, randomness=20 </p>
<p>tunic 
spoon 
dirty 
lions 
cobbler 
hole 
lit 
bandage 
lion 
head 
king 
tape 
tree 
spike 
book 
person 
man 
bouquet 
coffin 
priest 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>0.6 </p>
<p>lions 
lion 
cave 
entrance 
tunic 
surface 
tree 
window 
door 
man 
mouth 
rat 
tool 
jar 
priest 
dog 
opening 
roof 
rock 
goblet 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>lions 
tunic 
snob 
cave 
rat 
ghost 
lever 
person 
king 
web 
drink 
branch 
plant 
food 
goblet 
entrance 
irector 
straps 
women 
priest 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>0.6 </p>
<p>tunic 
lions 
cave 
lever 
shovel 
Snarl 
bone 
money 
drink 
goblet 
rat 
ghost 
web 
person 
king 
lion 
branch 
plant 
entrance 
food 
0.00 </p>
<p>0.02 </p>
<p>0.04 </p>
<p>0.06 </p>
<p>0.08 </p>
<p>0.10 </p>
<p>0.12 </p>
<p>0.14 </p>
<p>Normalized Noun Count </p>
<p>Procedural Short Motivation Generation, n=16, randomness=40 </p>
<p>tunic 
lions 
cave 
lever 
bone 
shovel 
Snarl 
money 
drink 
rat 
goblet 
person 
ghost 
king 
web 
lion 
branch 
plant 
food 
entrance 
0.00 </p>
<p>0.02 </p>
<p>0.04 </p>
<p>0.06 </p>
<p>0.08 </p>
<p>0.10 </p>
<p>0.12 </p>
<p>weapon 
tool 
sword 
disgusting 
pike 
shovel 
opening 
crossbow 
knife 
bow 
cave 
uncle 
bone 
arrow 
tree 
hatchet 
lever 
king 
branch 
faeries 
0.0 </p>
<p>0.2 </p>
<p>0.4 </p>
<p>0.6 </p>
<p>0.8 </p>
<p>weapon 
lever 
shovel 
branch 
Snarl 
cave 
pike 
faeries 
rabbit 
apple 
tool 
wolves 
poacher 
priest 
person 
sword 
club 
stick 
tree 
lions 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>weapon 
lever 
Snarl 
branch 
shovel 
pike 
cave 
faeries 
apple 
rabbit 
wolves 
priest 
poacher 
tool 
person 
sword 
tree 
stick 
lions 
club 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>weapon 
lever 
branch 
Snarl 
shovel 
pike 
cave 
wolves 
rabbit 
faeries 
apple 
priest 
tool 
person 
poacher 
sword 
stick 
tree 
club 
Paladins 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>lock 
weapon 
stick 
person 
man 
tree 
tool 
lion 
tunic 
pick 
uncle 
bird 
creature 
goblet 
scroll 
rock 
monkey 
gift 
lever 
bone 
0.0 </p>
<p>0.1 </p>
<p>0.2 </p>
<p>0.3 </p>
<p>0.4 </p>
<p>0.5 </p>
<p>lock 
weapon 
person 
cave 
rabbit 
straps 
clump 
stick 
tunic 
man 
lever 
cigarette 
someone 
goblet 
bird 
head 
male 
lion 
scroll 
tool 
0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>0.20 </p>
<p>0.25 </p>
<p>lock 
person 
weapon 
cave 
clump 
straps 
rabbit 
stick 
tunic 
man 
lever 
someone 
goblet 
cigarette 
bird 
male 
head 
lion 
scroll 
bone 
0.000 </p>
<p>0.025 </p>
<p>0.050 </p>
<p>0.075 </p>
<p>0.100 </p>
<p>0.125 </p>
<p>0.150 </p>
<p>0.175 </p>
<p>0.200 </p>
<p>lock 
person 
weapon 
clump 
straps 
cave 
rabbit 
stick 
tunic 
goblet 
man 
someone 
lever 
cigarette 
male 
head 
bird 
lion 
scroll 
tool 
0.000 </p>
<p>0.025 </p>
<p>0.050 </p>
<p>0.075 </p>
<p>0.100 </p>
<p>0.125 </p>
<p>0.150 </p>
<p>0.175 </p>
<p>0.200 </p>
<p>weapon 
person 
shelf 
man 
lock 
tree 
monk 
priest 
tool 
creature 
rock 
flower 
monkey 
owner 
goblet 
dog 
tunic 
stick 
statue 
sword 
0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>0.20 </p>
<p>0.25 </p>
<p>0.30 </p>
<p>0.35 </p>
<p>0.40 </p>
<p>weapon 
goblet 
cave 
shelf 
tunic 
person 
poison 
meat 
dog 
lock 
rock 
bag 
owner 
priest 
man 
someone 
faeries 
master 
monk 
sword 
0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>0.20 </p>
<p>0.25 </p>
<p>0.30 </p>
<p>weapon 
goblet 
cave 
shelf 
tunic 
meat 
poison 
person 
dog 
lock 
rock 
bag 
man 
priest 
owner 
master 
someone 
faeries 
wench 
scale 
0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>0.20 </p>
<p>0.25 </p>
<p>0.30 </p>
<p>weapon 
goblet 
cave 
shelf 
tunic 
meat 
poison 
person 
dog 
lock 
rock 
bag 
priest 
man 
master 
owner 
someone 
faeries 
wench 
scale 
0.00 </p>
<p>0.05 </p>
<p>0.10 </p>
<p>0.15 </p>
<p>0.20 </p>
<p>0.25 </p>
<p>0.30 </p>
<p>Normalized Noun Count </p>
<p>Table 4 )
4, having potentially less coherent worlds and quests during training hurts agent performance at test time-a case of spurious diversity in training data.Expt. 
Act Goals 
Speech Goals 
All Goals 
Scratch Encoder 
No Curr. 
0.418 
0.118 
0.103 
Sampled 
0.460 
0.145 
0.138 
100% Randomly Generated 
0.263 
0.024 
0.017 
80% Randomly Generated 
0.267 
0.080 
0.062 
60% Randomly Generated 
0.379 
0.112 
0.093 
40% Randomly Generated 
0.422 
0.115 
0.109 
20% Randomly Generated 
0.464 
0.146 
0.143 
Procedurally Generated 
0.477 
0.163 
0.155 
Adaptive Encoder 
No Curr. 
0.420 
0.330 
0.303 
Sampled 
0.473 
0.358 
0.344 
100% Randomly Generated 
0.335 
0.221 
0.207 
80% Randomly Generated 
0.364 
0.280 
0.269 
60% Randomly Generated 
0.424 
0.327 
0.293 
40% Randomly Generated 
0.481 
0.370 
0.330 
20% Randomly Generated 
0.508 
0.371 
0.369 
Procedurally Generated 
0.506 
0.382 
0.373 </p>
<p>table.A.6 Hyperparameters </p>
<p>Hyperparameter type 
Value </p>
<p>Num. layers 
2 
Num. attention heads 
2 
Embedding size 
300 
Dropout ratio 
0.0 
Gradient clip 
0.1 
Optimizer 
Adam 
Learning rate 
1 × 10 −4 </p>
<p>Table 5 :
5Hyperparameters used to train the Biencoder model to retrieve objects for generating the LIGHT world. The same trained models were then frozen and used for further experiments.Hyperparameter type Value </p>
<p>Embedding size 
128 
Embedding norm 
10 
Dropout ratio 
0.0 
Gradient clip 
0.1 
Optimizer 
SGD 
Learning rate 
0.1 </p>
<p>Table 6 :
6Hyperparameters used to train the Starspace model to retrieve character for generating the LIGHT world.Hyperparameter type Value </p>
<p>Num. encoder layers 
12 
Num. decoder layers 
12 
Num. attention heads 16 
Batchsize 
8 
Activation 
gelu 
Beam size 
1 
Beam decay 
30 
Beam length penalty 
0.65 
Num. attention heads 2 
Embedding size 
1024 
Dropout ratio 
0.1 
Gradient clip 
0.1 
Optimizer 
SGD 
Learning rate 
1 × 10 −4 </p>
<p>Table 7 :
7Hyperparameters used to train and test the BART model for generating short motivations and goals.Hyperparameter type 
Value </p>
<p>Dictionary Tokenizer 
Byte-pair encoding 
Num. layers 
12 
Num. attention heads 
12 
Feedforward network hidden size 3072 
Input length 
1024 
Embedding size 
768 
Batch size 
32 
Dropout ratio 
0.1 
Poly-n-codes 
64 
Gradient clip 
1.0 
Optimizer 
Adam 
Learning rate 
1 × 10 −6 </p>
<p>Table 8 :
8Hyperparameters used to pre-train the adaptive encoder as described inHumeau et al. (2020).Hyperparameter type 
Value </p>
<p>General 
Discount γ 
0.99 
Valid Action loss coefficient 
10 
Action entropy coefficient 
0.01 
Valid Speech loss coefficient 
40 
Speech entropy coefficient 
0.04 
Batch size 
32 
Gradient clip 
1.0 
Steps per episode 
100 
Policy Networks (Actors) 
Num. Layers 
3 
Feedforward network hidden size 768 
GRU hidden size 
768 
Value Predictor (Critic) 
Num. Layers 
2 
Feedforward network hidden size 768 
Appended Encoder 
Num. layers 
3 
Num. attention heads 
3 
Feedforward network hidden size 768 </p>
<p>Table 9 :
9RL experiments hyperparameters unchanged from Ammanabrolu et al. (2021).
Table 4: Curriculum learning hit analysis and unique n-grams counts. The tables show the hit percentage of the procedually generated entities in short motivations/ goals among the retrieved entities (objects + character).The count of unique uni-grams /bi-grams/ trigrams represent the n-grams counts changing with the procedurally generated curriculum as a function of n (Section 3) with the randomness percentage tuning for the generated short motivations or goals using BART model.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8312 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8312</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8312</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-274306413</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.18564v2.pdf" target="_blank">Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often struggle with spatial reasoning. This paper presents a novel neural-symbolic framework that enhances LLMs' spatial reasoning abilities through iterative feedback between LLMs and Answer Set Programming (ASP). We evaluate our approach on two benchmark datasets: StepGame and SparQA, implementing three distinct strategies: (1) direct prompting baseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with iterative refinement. Our experimental results demonstrate that the LLM+ASP pipeline significantly outperforms baseline methods, achieving an average 82% accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and 8-15% respectively over direct prompting. The success stems from three key innovations: (1) effective separation of semantic parsing and logical reasoning through a modular pipeline, (2) iterative feedback mechanism between LLMs and ASP solvers that improves program rate, and (3) robust error handling that addresses parsing, grounding, and solving failures. Additionally, we propose Facts+Rules as a lightweight alternative that achieves comparable performance on complex SparQA dataset, while reducing computational overhead.Our analysis across different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini) demonstrates the framework's generalizability and provides insights into the trade-offs between implementation complexity and reasoning capability, contributing to the development of more interpretable and reliable AI systems.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8312.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8312.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight/specialized large language model evaluated in this paper using a DSPy-based neural-symbolic pipeline (LLM+ASP) and two baselines (Direct prompting, Facts+Rules) on multi-hop spatial reasoning benchmarks (StepGame and SparQA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a lightweight, specialized LLM used to evaluate the proposed neural-symbolic pipeline; used as a semantic parser that converts NL to symbolic facts and refines ASP program with iterative feedback via DSPy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame; SparQA</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>StepGame: grid-based multi-hop spatial reasoning (8 directional relations, 1–10 hops). SparQA: NLVR-derived multi-block spatial reasoning including 2D/3D relations, topological and distance relations, and quantifier-based set reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Three methods compared: (1) Direct prompting (simple question->answer), (2) Facts+Rules prompting (LLM produces structured facts and applies logical rules in natural language), (3) DSPy-based iterative LLM+ASP pipeline. Pipeline stages: (a) convert NL context+question to ASP facts (predicates like is/3, query/2 for StepGame; block/1, object/5, is/3 for SparQA), (b) LLM iteratively refines ASP program (up to 3 iterations) using DSPy orchestrator, (c) run Clingo ASP solver, (d) map solver output to candidate answers and evaluate with exact/partial match metrics. StepGame: sampled 300 instances per hop (1..10). SparQA: 220 examples (55 per question type).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Semantic parsing by LLM to generate ASP facts; iterative feedback loop where solver error messages are returned to the LLM for program refinement (up to 3 rounds); a coordinate-based knowledge module for StepGame (offset-based chaining of relations); manually designed inverse/transitive/symmetric rules for SparQA; Facts+Rules prompting as a lightweight rule-application alternative; DSPy optimizes prompts and manages module interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame overall accuracy (Table 1): DSPy pipeline 87.7% (Deepseek), Facts+Rules 44.9%, Direct prompting 31.3%. Per-hop examples: k=5: Direct 33.3% -> DSPy 88.5% (Deepseek). At k=10: Direct 16.8% -> DSPy 79.8%. SparQA (Table 2) per-question-type (Deepseek): Direct FR 38.2%, FB 74.5%, YN 78.2%, CO 48.2%, Overall 59.8%; Facts+Rules Overall 66.4%; DSPy Pipeline Overall 67.2% (FR DSPy 58.9%, FB 85.4%, YN 81.8%, CO 42.8%). The paper also reports aggregate averages (across models) of ~82% on StepGame and ~69% on SparQA for the DSPy pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Substantial accuracy gains with DSPy on multi-hop StepGame (preserved accuracy at deeper hops) and improved FR/FB performance on SparQA indicate use of multi-step spatial inference; the coordinate-based knowledge module and explicit ASP encodings enable programmatic multi-hop chain reasoning and quantifier handling (example given for universal quantifier). Iterative feedback increased program executability rates (Deepseek execution rate increased from 45.8% to 76.8%), supporting that LLM+ASP integration produces executable spatial programs rather than only surface text reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against Direct prompting and Facts+Rules baselines (numerical comparisons provided). DSPy substantially outperforms Direct prompting across all models on StepGame and improves many metrics on SparQA; Facts+Rules is competitive with DSPy on SparQA while being cheaper. Cross-model comparisons: Deepseek achieved highest StepGame overall (87.7%) vs Llama3 75.6% and GPT-4.0 mini 80.8% (Table 1). No human baseline reported in this paper; related work citation notes Yang et al. (2023) reported 90% on StepGame with GPT-3+ASP but with different setup.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Error categories reported on SparQA: parsing errors 31%, grounding errors 23%, satisfiable-but-no-result 28%, wrong answers 18%. Model-specific issues: Deepseek produced many syntax errors (42% of its errors); GPT-4.0 mini had argument-ordering inconsistencies in object/5 predicates; Llama3 had higher 'satisfiable but no result' failures in complex chains. Pipeline limitations include domain sensitivity (higher StepGame than SparQA performance), difficulty with complex quantifiers and implicit spatial relations, NL->ASP conversion error-prone, and computational overhead due to multiple LLM calls (limited to 3 iterations). Dataset issues: label/template errors in StepGame (~10% labeling errors) can skew evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_reference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8312.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8312.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mid-to-large LLM evaluated in the paper as one of three representative architectures to test generalizability of the DSPy LLM+ASP neural-symbolic pipeline on StepGame and SparQA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a model balancing performance and computational efficiency; used via DSPy to produce ASP facts, refine logical programs, and interface with Clingo solver. The paper inconsistently references Llama3 size (mentions both 70B and 60B in different sections).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame; SparQA</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>StepGame: grid-based multi-hop spatial reasoning (directional offsets). SparQA: NLVR-derived multi-block scenes with 2D/3D, topological and distance relations, and quantifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same three-method comparison (Direct, Facts+Rules, DSPy LLM+ASP). LLM prompted to generate predicates (is/3, query/2 for StepGame; block/1, object/5, is/3 for SparQA), iterative ASP refinement (3 rounds), Clingo solving, and post-processing mapping outputs to answers with synonym normalization; exact match for single-choice and partial matching for multi-answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>LLM semantic parsing to structured predicates; iterative feedback using solver error messages; pre-defined spatial rules (inverse/transitive/symmetric) for SparQA; coordinate offset chaining for StepGame. Facts+Rules used as a natural-language rule-application CoT alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame overall accuracy (Table 1): DSPy 75.6% (Llama3), Facts+Rules 47.0%, Direct prompting 29.3%. Example at hop k=5: Direct 28.2% -> DSPy 77.9%. At k=10: Direct 14.2% -> DSPy 65.7%. SparQA (Table 2) per-question-type (Llama3): Direct overall 55.2% (FR 26.9%, FB 65.7%, YN 72.5%, CO 55.8%); Facts+Rules overall 63.9%; DSPy Pipeline overall 69.4% (FR 53.1%, FB 83.3%, YN 80.5%, CO 60.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>DSPy preserves higher accuracy over increasing hops on StepGame and improves FR performance on SparQA, indicating multi-hop and relational spatial inference. Feedback loop drastically increased ASP executability (Llama3 execution rate from 34.2% to 80.2%), showing increased ability to produce executable logical representations for spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared numerically to Deepseek and GPT-4.0 mini and to baselines; Facts+Rules sometimes achieves closer performance to DSPy (notably on SparQA) indicating that structured rule application in natural language can approximate full LLM+ASP on certain tasks. No human performance baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher rate of satisfiable-but-no-result failures on complex nested relationship chains and multi-block relationships; struggles with implicit spatial relationships; NLP->ASP parsing and grounding errors remain significant. Computational cost and iterative API calls remain a bottleneck. Dataset label/template errors (StepGame) also impact evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_reference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8312.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8312.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.0 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.0 mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose LLM variant evaluated in this paper under the DSPy LLM+ASP pipeline and baseline prompting strategies to measure gains in spatial reasoning on StepGame and SparQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.0 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Presented as an advanced general-purpose system used with DSPy to generate ASP facts/programs and to participate in iterative refinement; used as a semantic parser and program refiner interfacing with Clingo.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame; SparQA</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>StepGame: grid-based multi-hop spatial relation queries. SparQA: complex NLVR-derived multi-block spatial reasoning with quantifiers, topological and distance relations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same experimental setup: Direct prompting baseline, Facts+Rules prompting, and DSPy iterative LLM+ASP pipeline with Clingo solver; outputs normalized and evaluated using exact and partial match metrics. StepGame: 300 samples per hop; SparQA: 220-sample representative test set (55 per question type).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Semantic parsing into ASP predicates, iterative solver-guided refinement (3 rounds), DSPy-managed prompt optimization, coordinate-offset reasoning for StepGame, manual rule sets for SparQA; Facts+Rules as a rule-application CoT alternative to full ASP generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>StepGame overall accuracy (Table 1): DSPy 80.8% (GPT-4.0 mini), Facts+Rules 47.6%, Direct prompting 29.9%. Example at hop k=5: Direct 29.9% -> DSPy 87.1%. At k=10: Direct 13.5% -> DSPy 68.9%. SparQA (Table 2) per-question-type (GPT-4.0 mini): Direct overall 55.5% (FR 45.4%, FB 60.9%, YN 58.2%, CO 57.6%); Facts+Rules overall 58.6%; DSPy Pipeline overall 70.3% (FR 65.3%, FB 80.5%, YN 64.8%, CO 72.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Marked improvement on Finding Relation and Finding Block question types with DSPy indicates explicit encoding+solving of spatial relations; example logical encoding for 'all black objects in a block' shows how quantifiers are handled more robustly in ASP than in pure NL inference. Iterative feedback improved ASP program executability (GPT4mini execution rate from 44.8% to 73.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms Direct prompting and Facts+Rules on many StepGame metrics; on SparQA DSPy often yields higher overall accuracy than baselines and compared favorably to other evaluated models. No direct human comparison provided. Related-work comparisons mentioned (e.g., Yang et al. 2023 reported high StepGame performance with GPT-3+ASP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequently produced argument-ordering errors in block/object predicates causing grounding inconsistencies; faced parsing and grounding failures and cases where solver output was satisfiable but not aligned with ground truth. Performance drop as reasoning depth increases in StepGame (though less severe than baselines). Computational overhead and NL->ASP brittleness noted as limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_reference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts <em>(Rating: 2)</em></li>
                <li>Transfer learning with synthetic corpora for spatial role labeling and reasoning <em>(Rating: 2)</em></li>
                <li>Dspy: Compiling declarative language model calls into selfimproving pipelines <em>(Rating: 2)</em></li>
                <li>Leveraging large language models to generate answer set programs <em>(Rating: 2)</em></li>
                <li>Coupling large language models with logic programming for robust and general reasoning from text <em>(Rating: 2)</em></li>
                <li>Language models can be deductive solvers <em>(Rating: 1)</em></li>
                <li>A neuro-symbolic ASP pipeline for visual question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8312",
    "paper_id": "paper-274306413",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Deepseek",
            "name_full": "Deepseek",
            "brief_description": "A lightweight/specialized large language model evaluated in this paper using a DSPy-based neural-symbolic pipeline (LLM+ASP) and two baselines (Direct prompting, Facts+Rules) on multi-hop spatial reasoning benchmarks (StepGame and SparQA).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deepseek",
            "model_description": "Described in the paper as a lightweight, specialized LLM used to evaluate the proposed neural-symbolic pipeline; used as a semantic parser that converts NL to symbolic facts and refines ASP program with iterative feedback via DSPy.",
            "model_size": null,
            "puzzle_name": "StepGame; SparQA",
            "puzzle_type": "StepGame: grid-based multi-hop spatial reasoning (8 directional relations, 1–10 hops). SparQA: NLVR-derived multi-block spatial reasoning including 2D/3D relations, topological and distance relations, and quantifier-based set reasoning.",
            "task_setup": "Three methods compared: (1) Direct prompting (simple question-&gt;answer), (2) Facts+Rules prompting (LLM produces structured facts and applies logical rules in natural language), (3) DSPy-based iterative LLM+ASP pipeline. Pipeline stages: (a) convert NL context+question to ASP facts (predicates like is/3, query/2 for StepGame; block/1, object/5, is/3 for SparQA), (b) LLM iteratively refines ASP program (up to 3 iterations) using DSPy orchestrator, (c) run Clingo ASP solver, (d) map solver output to candidate answers and evaluate with exact/partial match metrics. StepGame: sampled 300 instances per hop (1..10). SparQA: 220 examples (55 per question type).",
            "mechanisms_or_strategies": "Semantic parsing by LLM to generate ASP facts; iterative feedback loop where solver error messages are returned to the LLM for program refinement (up to 3 rounds); a coordinate-based knowledge module for StepGame (offset-based chaining of relations); manually designed inverse/transitive/symmetric rules for SparQA; Facts+Rules prompting as a lightweight rule-application alternative; DSPy optimizes prompts and manages module interactions.",
            "performance_metrics": "StepGame overall accuracy (Table 1): DSPy pipeline 87.7% (Deepseek), Facts+Rules 44.9%, Direct prompting 31.3%. Per-hop examples: k=5: Direct 33.3% -&gt; DSPy 88.5% (Deepseek). At k=10: Direct 16.8% -&gt; DSPy 79.8%. SparQA (Table 2) per-question-type (Deepseek): Direct FR 38.2%, FB 74.5%, YN 78.2%, CO 48.2%, Overall 59.8%; Facts+Rules Overall 66.4%; DSPy Pipeline Overall 67.2% (FR DSPy 58.9%, FB 85.4%, YN 81.8%, CO 42.8%). The paper also reports aggregate averages (across models) of ~82% on StepGame and ~69% on SparQA for the DSPy pipeline.",
            "evidence_of_spatial_reasoning": "Substantial accuracy gains with DSPy on multi-hop StepGame (preserved accuracy at deeper hops) and improved FR/FB performance on SparQA indicate use of multi-step spatial inference; the coordinate-based knowledge module and explicit ASP encodings enable programmatic multi-hop chain reasoning and quantifier handling (example given for universal quantifier). Iterative feedback increased program executability rates (Deepseek execution rate increased from 45.8% to 76.8%), supporting that LLM+ASP integration produces executable spatial programs rather than only surface text reasoning.",
            "comparisons": "Compared against Direct prompting and Facts+Rules baselines (numerical comparisons provided). DSPy substantially outperforms Direct prompting across all models on StepGame and improves many metrics on SparQA; Facts+Rules is competitive with DSPy on SparQA while being cheaper. Cross-model comparisons: Deepseek achieved highest StepGame overall (87.7%) vs Llama3 75.6% and GPT-4.0 mini 80.8% (Table 1). No human baseline reported in this paper; related work citation notes Yang et al. (2023) reported 90% on StepGame with GPT-3+ASP but with different setup.",
            "limitations_or_failure_cases": "Error categories reported on SparQA: parsing errors 31%, grounding errors 23%, satisfiable-but-no-result 28%, wrong answers 18%. Model-specific issues: Deepseek produced many syntax errors (42% of its errors); GPT-4.0 mini had argument-ordering inconsistencies in object/5 predicates; Llama3 had higher 'satisfiable but no result' failures in complex chains. Pipeline limitations include domain sensitivity (higher StepGame than SparQA performance), difficulty with complex quantifiers and implicit spatial relations, NL-&gt;ASP conversion error-prone, and computational overhead due to multiple LLM calls (limited to 3 iterations). Dataset issues: label/template errors in StepGame (~10% labeling errors) can skew evaluations.",
            "citation_reference": null,
            "uuid": "e8312.0",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Llama3",
            "name_full": "Llama3",
            "brief_description": "A mid-to-large LLM evaluated in the paper as one of three representative architectures to test generalizability of the DSPy LLM+ASP neural-symbolic pipeline on StepGame and SparQA benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3",
            "model_description": "Described as a model balancing performance and computational efficiency; used via DSPy to produce ASP facts, refine logical programs, and interface with Clingo solver. The paper inconsistently references Llama3 size (mentions both 70B and 60B in different sections).",
            "model_size": null,
            "puzzle_name": "StepGame; SparQA",
            "puzzle_type": "StepGame: grid-based multi-hop spatial reasoning (directional offsets). SparQA: NLVR-derived multi-block scenes with 2D/3D, topological and distance relations, and quantifiers.",
            "task_setup": "Same three-method comparison (Direct, Facts+Rules, DSPy LLM+ASP). LLM prompted to generate predicates (is/3, query/2 for StepGame; block/1, object/5, is/3 for SparQA), iterative ASP refinement (3 rounds), Clingo solving, and post-processing mapping outputs to answers with synonym normalization; exact match for single-choice and partial matching for multi-answer questions.",
            "mechanisms_or_strategies": "LLM semantic parsing to structured predicates; iterative feedback using solver error messages; pre-defined spatial rules (inverse/transitive/symmetric) for SparQA; coordinate offset chaining for StepGame. Facts+Rules used as a natural-language rule-application CoT alternative.",
            "performance_metrics": "StepGame overall accuracy (Table 1): DSPy 75.6% (Llama3), Facts+Rules 47.0%, Direct prompting 29.3%. Example at hop k=5: Direct 28.2% -&gt; DSPy 77.9%. At k=10: Direct 14.2% -&gt; DSPy 65.7%. SparQA (Table 2) per-question-type (Llama3): Direct overall 55.2% (FR 26.9%, FB 65.7%, YN 72.5%, CO 55.8%); Facts+Rules overall 63.9%; DSPy Pipeline overall 69.4% (FR 53.1%, FB 83.3%, YN 80.5%, CO 60.8%).",
            "evidence_of_spatial_reasoning": "DSPy preserves higher accuracy over increasing hops on StepGame and improves FR performance on SparQA, indicating multi-hop and relational spatial inference. Feedback loop drastically increased ASP executability (Llama3 execution rate from 34.2% to 80.2%), showing increased ability to produce executable logical representations for spatial tasks.",
            "comparisons": "Compared numerically to Deepseek and GPT-4.0 mini and to baselines; Facts+Rules sometimes achieves closer performance to DSPy (notably on SparQA) indicating that structured rule application in natural language can approximate full LLM+ASP on certain tasks. No human performance baseline provided.",
            "limitations_or_failure_cases": "Higher rate of satisfiable-but-no-result failures on complex nested relationship chains and multi-block relationships; struggles with implicit spatial relationships; NLP-&gt;ASP parsing and grounding errors remain significant. Computational cost and iterative API calls remain a bottleneck. Dataset label/template errors (StepGame) also impact evaluation.",
            "citation_reference": null,
            "uuid": "e8312.1",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4.0 mini",
            "name_full": "GPT-4.0 mini",
            "brief_description": "A general-purpose LLM variant evaluated in this paper under the DSPy LLM+ASP pipeline and baseline prompting strategies to measure gains in spatial reasoning on StepGame and SparQA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4.0 mini",
            "model_description": "Presented as an advanced general-purpose system used with DSPy to generate ASP facts/programs and to participate in iterative refinement; used as a semantic parser and program refiner interfacing with Clingo.",
            "model_size": null,
            "puzzle_name": "StepGame; SparQA",
            "puzzle_type": "StepGame: grid-based multi-hop spatial relation queries. SparQA: complex NLVR-derived multi-block spatial reasoning with quantifiers, topological and distance relations.",
            "task_setup": "Same experimental setup: Direct prompting baseline, Facts+Rules prompting, and DSPy iterative LLM+ASP pipeline with Clingo solver; outputs normalized and evaluated using exact and partial match metrics. StepGame: 300 samples per hop; SparQA: 220-sample representative test set (55 per question type).",
            "mechanisms_or_strategies": "Semantic parsing into ASP predicates, iterative solver-guided refinement (3 rounds), DSPy-managed prompt optimization, coordinate-offset reasoning for StepGame, manual rule sets for SparQA; Facts+Rules as a rule-application CoT alternative to full ASP generation.",
            "performance_metrics": "StepGame overall accuracy (Table 1): DSPy 80.8% (GPT-4.0 mini), Facts+Rules 47.6%, Direct prompting 29.9%. Example at hop k=5: Direct 29.9% -&gt; DSPy 87.1%. At k=10: Direct 13.5% -&gt; DSPy 68.9%. SparQA (Table 2) per-question-type (GPT-4.0 mini): Direct overall 55.5% (FR 45.4%, FB 60.9%, YN 58.2%, CO 57.6%); Facts+Rules overall 58.6%; DSPy Pipeline overall 70.3% (FR 65.3%, FB 80.5%, YN 64.8%, CO 72.7%).",
            "evidence_of_spatial_reasoning": "Marked improvement on Finding Relation and Finding Block question types with DSPy indicates explicit encoding+solving of spatial relations; example logical encoding for 'all black objects in a block' shows how quantifiers are handled more robustly in ASP than in pure NL inference. Iterative feedback improved ASP program executability (GPT4mini execution rate from 44.8% to 73.2%).",
            "comparisons": "Outperforms Direct prompting and Facts+Rules on many StepGame metrics; on SparQA DSPy often yields higher overall accuracy than baselines and compared favorably to other evaluated models. No direct human comparison provided. Related-work comparisons mentioned (e.g., Yang et al. 2023 reported high StepGame performance with GPT-3+ASP).",
            "limitations_or_failure_cases": "Frequently produced argument-ordering errors in block/object predicates causing grounding inconsistencies; faced parsing and grounding failures and cases where solver output was satisfiable but not aligned with ground truth. Performance drop as reasoning depth increases in StepGame (though less severe than baselines). Computational overhead and NL-&gt;ASP brittleness noted as limitations.",
            "citation_reference": null,
            "uuid": "e8312.2",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "rating": 2,
            "sanitized_title": "stepgame_a_new_benchmark_for_robust_multihop_spatial_reasoning_in_texts"
        },
        {
            "paper_title": "Transfer learning with synthetic corpora for spatial role labeling and reasoning",
            "rating": 2,
            "sanitized_title": "transfer_learning_with_synthetic_corpora_for_spatial_role_labeling_and_reasoning"
        },
        {
            "paper_title": "Dspy: Compiling declarative language model calls into selfimproving pipelines",
            "rating": 2,
            "sanitized_title": "dspy_compiling_declarative_language_model_calls_into_selfimproving_pipelines"
        },
        {
            "paper_title": "Leveraging large language models to generate answer set programs",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_to_generate_answer_set_programs"
        },
        {
            "paper_title": "Coupling large language models with logic programming for robust and general reasoning from text",
            "rating": 2,
            "sanitized_title": "coupling_large_language_models_with_logic_programming_for_robust_and_general_reasoning_from_text"
        },
        {
            "paper_title": "Language models can be deductive solvers",
            "rating": 1,
            "sanitized_title": "language_models_can_be_deductive_solvers"
        },
        {
            "paper_title": "A neuro-symbolic ASP pipeline for visual question answering",
            "rating": 1,
            "sanitized_title": "a_neurosymbolic_asp_pipeline_for_visual_question_answering"
        }
    ],
    "cost": 0.0124335,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs
12 Dec 2024</p>
<p>Rong Wang 
Kun Sun 
TübingenGermany</p>
<p>Jonas Kuhn 
The Institute of Natural Language Processing
Stuttgart University
StuttgartGermany</p>
<p>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs
12 Dec 202479780BC935B8FF38FD83CBFB71135F92arXiv:2411.18564v2[cs.AI]
Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often struggle with spatial reasoning.This paper presents a novel neural-symbolic framework that enhances LLMs' spatial reasoning abilities through iterative feedback between LLMs and Answer Set Programming (ASP).We evaluate our approach on two benchmark datasets: StepGame and SparQA, implementing three distinct strategies: (1) direct prompting baseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with iterative refinement.Our experimental results demonstrate that the LLM+ASP pipeline significantly outperforms baseline methods, achieving an average 82% accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and 8-15% respectively over direct prompting.The success stems from three key innovations: (1) effective separation of semantic parsing and logical reasoning through a modular pipeline, (2) iterative feedback mechanism between LLMs and ASP solvers that improves program executability rate, and (3) robust error handling that addresses parsing, grounding, and solving failures.Additionally, we propose Facts+Rules as a lightweight alternative that achieves comparable performance on complex SparQA dataset, while reducing computational overhead.Our analysis across different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini) demonstrates the framework's generalizability and provides insights into the trade-offs between implementation complexity and reasoning capability, contributing to the development of more interpretable and reliable AI systems.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are known for their impressive performance across a range of tasks, demonstrating certain commonsense reasoning abilities.However, since LLMs are trained to predict subsequent words in a sequence, they seem to lack sufficient grounding to excel at tasks requiring spatial, physical, and embodied reasoning.Spatial reasoning, the ability to understand and manipulate relationships between objects in two-and threedimensional spaces, represents a crucial component of artificial intelligence systems, enabling practical applications in robotics, navigation, and physical task planning.Recent studies (Bang et al., 2023;Cohn, 2023) highlighted the limitations of models like ChatGPT in deductive logical reasoning, spatial reasoning, and non-textual semantic reasoning, underlining the need for further improvements in spatial reasoning.</p>
<p>Efforts to improve logical reasoning capability have focused on advanced prompting techniques.However, these methods have demonstrated notable limitations, particularly on challenging datasets like StepGame (Shi et al., 2022) and SparQA (Mirzaee and Kordjamshidi, 2022), which often require multi-step planning and complex natural language understanding.In these scenarios, LLMs often struggle with maintaining coherence, frequently hallucinating or losing sight of the original objectives, resulting in inaccurate and unreliable outputs.More recent work demonstrates that augmenting large language models (LLMs) with external tools for arithmetic, navigation, and knowledge lookups improves performance (Fang et al., 2024).Notably, neural-symbolic approaches, where LLMs extract facts while external symbolic solvers handle reasoning, show significant promise in enhancing logical inferenceg) (Yang et al., 2023a).Nevertheless, existing neural-symbolic methods face limitations in generalization, scalability, and comprehensive evaluation.(Yu et al, 2021) Neural-symbolic systems have been applied successfully in visual question answering and robotics (Yang et al., 2023a).These approaches often fail to fully harness LLMs' potential, particularly by omitting feedback loops between multiple LLMs and symbolic modules, limiting performance gains.</p>
<p>To overcome these challenges, this paper proposes a novel neural-symbolic framework that systematically integrates symbolic reasoning components with LLMs.Our approach leverages strategic prompting, feedback loops, and Answer Set Programming (ASP)-based verification, creating a robust pipeline that improves spatial reasoning across different LLM architectures.We evaluate this framework on two benchmark datasets, By integrating feedback loops and verification module, our methodology demonstrates strong generalizability when tackling complex spatial reasoning tasks, showing sig-nificant performance gains, with average accuracy improvements of 40% on StepGame and 20% on SparQA.These outcomes underscore the effectiveness of neural-symbolic integration in enhancing spatial reasoning and emphasize the importance of task-specific implementation strategies.</p>
<p>To address these limitations, we propose a pipeline that effectively enhances LLMs' spatial reasoning capabilities.Our approach combines strategic prompting with symbolic reasoning to create a robust framework that significantly improves performance of spatial reasoning across different LLM architectures.Specifically, building on these insights, we propose a novel neural-symbolic pipeline that integrates LLMs with ASP, aimed at enhancing the LLMs' capabilities of spatial reasoning in SparQA and StepGame datasets.We investigate the potential benefits of integrating symbolic reasoning components into LLMs to further boost their spatial reasoning capabilities.Within the broader field of neural-symbolic AI, the combination of LLMs as parsers with ASP solvers has emerged as a particularly effective approach for complex reasoning tasks.Additionally, the present study is one of the pioneering projects which uses DSPy (Khattab et al., 2023) to pipeline and program LLM.</p>
<p>In the broader field of neural-symbolic AI, combining large language models (LLMs) with symbolic reasoning systems like Prolog has proven successful for tackling complex reasoning tasks (Hamilton et al., 2022;Besold et al., 2021).Our study advances this field by demonstrating how incorporating symbolic reasoning modules can enhance LLMs' spatial reasoning capabilities.Specifically, we introduce a modular pipeline using DSPy, enabling seamless interaction between LLMs and symbolic system solvers.Our pipeline not only improves spatial reasoning but also shows promise in broader domains requiring structured logic, thereby contributing to both the theoretical foundations and practical applications of neural-symbolic integration.</p>
<p>Preliminaries</p>
<p>Prompting LLMs for spatial reasoning</p>
<p>Research on spatial reasoning spans both visual and textual domains, each with its own challenges.In Visual Spatial Reasoning (VSR), even advanced models like CLIP (Radford et al., 2021) face limitations in comprehending complex spatial relationships.In the textual domain, spatial reasoning is further complicated by linguistic ambiguity.</p>
<p>Prompting strategies have played a pivotal role in improving LLMs' reasoning capabilities.Approaches such as chain of thought (CoT) (Wei et al., 2022;Chu et al., 2023), few-shot prompting (Schick and Schütze, 2021), and least-to-most prompting (Zhou et al., 2022) enable LLMs to tackle complex problems by breaking them down into logical steps.Self-consistency methods (Wang et al., 2022), further refine reasoning by aggregating multiple solutions to improve accuracy.Recent techniques like like tree of thoughts, visualization of thought (Wang et al., 2023), and program-aided language models (Gao et al., 2022) provide structured, interpretable reasoning pathways.</p>
<p>These prompting strategies demonstrate the potential for LLMs to handle spatial reasoning, but they also highlight the need for more robust methods to address limitations in multi-hop inference and natural language ambiguity.Our approach integrates prompting strategies with symbolic reasoning to enhance LLMs' spatial reasoning capabilities more comprehensively.</p>
<p>Effective Neural-Symbolic Integration</p>
<p>Despite the success of deep learning, its limitations in reasoning, generalization and interpretability have spurred interest in hybrid approaches (Garcez and Lamb, 2023;Hamilton et al., 2022).Neural-symbolic integration aims to combine the strengths of neural networks (pattern recognition and uncertainty handling) with symbolic systems (logical reasoning and knowledge representation).Based on how neural and symbolic components interact and complement each other, research (Wan et al., 2024) has identified key integration patterns: enhancing symbolic systems with neural capabilities (Symbolic[Neural]), combining neural learning with symbolic inference (Neural|Symbolic), compiling symbolic rules into neural structures (Neural:Symbolic → Neural), integrating logic rules through embeddings (Neural ⊗ Symbolic), and augmenting neural systems with symbolic reasoning (Neural[Symbolic]).These patterns can be grouped into practical architectures, including sequential, iterative, and embedded approaches, as well as LLM-based tool integration (Weber et al., 2019;Riegel et al., 2020;Parisi et al., 2022).</p>
<p>The emergence of LLMs has introduced new possibilities for neural-symbolic integration.Modern approaches leverage LLMs as powerful semantic parsers and natural language interfaces, while delegating the reasoning task to external off-the-shelf symbolic reasoners or solvers.The integration typically follows a two-step process: first, the LLM translates natural language queries into formal logical representations (e.g., Prolog predicates or ASP rules), then the logical solver performs structured reasoning to derive answers.However, this approach faces significant challenges in ensuring accurate and consistent mapping between natural language and logical forms.LLMs are not trained on logical programming language and they tend to generate logically inconsistent or syntactically incorrect programs, and the in-executable logical program will lead to the collpase of the whole neural symbolic system.For some complex and realistic dataset, the parsing succesful rate can be as low as 17% (Feng et al., 2024).To addrress this, researchers (Pan et al., 2023) have proposed self-refinement modules to refine the parsing results based on the solver's erroneous messageses; however, the improvement of accuracy is not obvious with only 5% after 3 iterations, highlighting the need for a more efficient architecture to enable sophisticated and robust feedback loop between LLMs and symbolic solvers.</p>
<p>This study leverages Dspy framework to construct a novel neural-symbolic pipeline that integrates LLMs with Answer Set Programming (ASP) in an iterative manner.Through its modular architecture and systematic optimization approach, DSPy can address many of the limitations faced by previous neural-symbolic integration attempts.By defining clear input-output signatures for each module in the pipeline, the framework ensures that the generated ASP code maintains consistent structure and enable the error feedback verification modules working in concert.Dspy's optimization compiler iteratively refines both the prompting strategies and weights used in each stage, ensuring consistent performance improvement, enabling more robust and adaptable neural-symbolic systems.</p>
<p>Answer Set Programming</p>
<p>Answer Set Programming (ASP) is a declarative programming paradigm that leverages predicate logic and stable model semantics to address complex combinational search tasks.Unlike procedural programming, which specifies step-by-step computations, ASP allows users to define solutions through logical relationships, enabling the solver to autonomously determine how to satisfy these conditions (Brewka et al., 2011).ASP is case-sensitive, utilizing uppercase letters for variables and lowercase letters for constants, including atoms and predicates.The underscore character "_" serves as a "don't care" variable that can be instantiated by any value.However, using placeholders in rule heads without corresponding definitions can lead to unsafe variable errors due to ambiguity during grounding.</p>
<p>ASP is closely related to Prolog, having evolved from it and sharing foundational concepts.Both languages utilize logical rules and facts for knowledge representation.However, while Prolog emphasizes procedural query evaluation, ASP focuses on generating answer sets -collections of ground atoms that satisfy the program's rules.This distinction allows ASP to handle nonmonotonic reasoning more effectively, accommodating multiple stable models for a single program, which is less common in Prolog.The paradigm's expressive power derives from four fundamental components, each serving distinct roles in knowledge representation and reasoning:</p>
<p>Facts: Basic units of knowledge represented as unconditional logical atoms p(t1, ..., tn), where p is a predicate symbol and t1, ..., tn are terms.In spatial reasoning, facts typically represent object properties, locations, and basic spatial relationships.</p>
<p>Rules: Logical implications of the form "Head :-Body", where the Head is an atom and the Body consists of literals.Rules enable inference, allowing new knowledge to be derived from existing facts.</p>
<p>Constraints: Special rules expressed as ":-Body" that eliminate answer sets violating specific conditions, thus serving as integrity checks within the knowledge base.</p>
<p>Queries: Goal-directed statements that extract information from the knowledge base, implemented as special predicates whose extensions yield the requested data.</p>
<p>In spatial reasoning contexts, ASP provides a robust framework for representing both static and dynamic spatial relationships through foundational predicates such as block/1 for defining spatial regions, object/5 for specifying object properties (identifier, size, color, shape, location), is/3 for expressing spatial relationships, and location/3 for precise coordinate definitions.This combination of predicates with ASP's logical foundations enables sophisticated reasoning about spatial configurations and relationships.</p>
<p>Methods</p>
<p>Datasets</p>
<p>StepGame (Shi et al., 2022) StepGame is a dataset designed for robust multi-hop spatial reasoning using a grid-based system, featuring eight directional spatial relations: top (north), down (south), left (west), right (east), top-left (north-west), topright (north-east), down-left (south-west), and down-right (south-east).Each relation is defined by specific angles and distances for detailed visual representation, and it includes an "overlap" relation for scenarios where objects share the same location.The dataset supports reasoning hops from 1 to 10 across 10 subsets, each containing 10,000 samples corresponding to a single reasoning hop.However, the single Finding Relation question type may not fully capture the complexities of real-world spatial reasoning, as research indicates that large language models (LLMs) often struggle more with constructing object-linking chains from shuffled relations than with the spatial reasoning tasks themselves.Additionally, prior studies have identified template errors within StepGame that may skew model performance evaluations due to inadequate quality control during crowdsourcing, leading to inaccuracies in relationship mappings that can misrepresent an LLM's spatial reasoning abilities.</p>
<p>SpartQA/SparTUN(Mirzaee and Kordjamshidi, 2022) SpartQA is built upon the NLVR (Natural Language for Visual Reasoning) images, featuring synthetically generated scenes depicting various spatial arrangements.Typically, each scenario consists of three blocks arranged either vertically or horizontally, with each block containing around four objects characterized by attributes like size, color, and shape.Besides, the dataset incorporates a wider range of spatial relationships, including 3D spatial reasoning, topological relations and distance relations.For FR questions, the candidate choices include ['left', 'right', 'above', 'below', 'near to', 'far from', 'touching', 'DK'], but there are more synonym relation names involved in the context and question.</p>
<p>SparQA differs significantly from StepGame in its complexity and scope.Its language structure is 2.5 times longer, featuring three blocks with approximately four objects each.The dataset handles multiple question types including Yes/No, FR, CO, and FB, with questions often involving complex relationships between multiple objects.For example, "What is the relation between the blue circle touching the top edge of block B and the small square?"Except Yes/No questions, FR, CO and FB questions often have more than one answer.SparQA incorporates 3D spatial reasoning, topological relations, and distance relations, with options like left, above, and near to.</p>
<p>A distinctive feature of SparQA is its focus on quantifier-based reasoning, with questions that test higher-level logic through universal ("all") and exclusive ("only") statements.For example, one third questions process queries like "Are all of the squares in B?" and "Which block has only small black things inside?"These questions require comprehensive evaluation of object sets and their properties rather than simple relational comparisons.</p>
<p>By utilizing these diverse datasets and question types, we aim to assess the spatial reasoning capabilities of LLMs across various complexity levels and spatial relation types.</p>
<p>LLM + ASP pipeline with DSPy</p>
<p>Recent studies have demonstrated LLMs' effectiveness as semantic parsers, often surpassing traditional parsing tools.While Geibinger (2023) and Eiter et al. (2022) showed promising results integrating LLMs with ASP, challenges remain.Ishay et al. (2023) found LLMs could generate complex ASP programs but often with errors, while Yang et al. (2023a) achieved 90% accuracy on StepGame using GPT-3 and ASP, their method's scalability remains uncertain.2023)'s LOGIC-LM framework and integration neural-symbolic strategies, we propose a novel neural-symbolic pipeline employing ASP using DSPy that treats the LLM as an agent capable of feedback and iteration.DSPy is a Python framework that uses a declarative and selfimproving approach to simplify working with LLMs (Khattab et al., 2023).It automates the optimization of prompts and model tuning, enhancing reliability and scalability in AI applications.By defining tasks and metrics rather than manual prompts, DSPy streamlines the development of various NLP tasks and complex AI systems.The framework of this pipeline is shown in Fig. 2.</p>
<p>The pipeline consists of four main stages: a) Facts Generation Stage: LLM converts natural language descriptions into symbolic formulations and formal queries.b) ASP Refining Stage: LLM iteratively refines the ASP representation over three iterations, adding rules, checking consistency, and incorporating feedback from error messages.c) Symbolic Reasoning Stage: The refined ASP undergoes inference using the Clingo solver, ensuring accurate and explainable reasoning by combining LLM capabilities with logical inference.d) Result Interpretation and Evaluation: This stage involves mapping the Clingo solver's outputs to candidate answers.For cer-tain question types, like Yes/No and Finding-Block questions, the solver's output can directly serve as the correct answer.However, for Finding Relations and Choose Object questions, additional processing is necessary to filter relevant solutions.Outputs from the ASP solver are evaluated against a synonym dictionary to determine the accuracy.</p>
<p>Overall, this pipeline requires multiple interactions with LLMs during ASP generation and refinement.We employ the DSPy framework to manage these complex workflows (e.g., interfacing with Llama3 60B DeepSeek and GPT 4.0 mini models via their APIs).DSPy's modular features enhance memory retention between modules, enabling adjustments and optimizations while maintaining workflow integrity.</p>
<p>Additionally, DSPy optimizes LLM prompts and weights, reducing the need for manual prompt engineering and ensuring consistent performance across datasets.Its optimization compiler iteratively generates and refines prompts, enhancing task performance.To support transparency and debugging, outputs from all modules are logged, capturing errors and providing insights for prompt engineering and system optimization, facilitating continuous improvement of the system and enhancing the integration of neural and symbolic components.In this way, this integrated neural-symbolic pipeline could greatly facilitate spatial reasoning in LLMs.</p>
<p>To evaluate the effectiveness of our approaches in spatial reasoning comprehensively, we selected three representative LLMs with diverse architectures and capabilities: DeepSeek, Llama3, and GPT4.0 Mini.These LLMs were chosen to ensure a comprehensive assessment across different types of language representations, ranging from lightweight and specialized models like DeepSeek to more advanced general-purpose systems like GPT4.0 Mini.Llama3, known for its balance between performance and computational efficiency, provides an intermediate perspective.By testing our methods on these distinct models, we want to demonstrate the adaptability and robustness of our approach across a variety of LLM architectures and reasoning capacities.</p>
<p>Given the task nature of multiple choices, we primarily employed exact match metrics for single-choice questions and partial match metrics for multiple-choice questions, aligning with the specific requirements of spatial reasoning tasks.To ensure accurate evaluation, we implemented custom post-processing to normalize responses and developed specialized metrics to handle both exact and partial matches between model outputs and ground truth.</p>
<p>Baselines</p>
<p>Direct prompting</p>
<p>As the most straightforward approach, it involves presenting the task to the LLM without additional guidance.The typical form is " Given the question, please answer: While simple, this method is believed not to elicit the model's full potential, particularly in complex reasoning tasks".This straightforward approach involves presenting the task to the LLM without additional guidance.The basic structure consists of a simple template: "Given the context and question, please answer the question by choosing from the choices".While minimal, this method serves as a crucial baseline for model evaluation as it reflects the model's fundamental ability to handle spatial tasks without any reasoning scaffolds.Despite its simplicity, this approach can achieve competitive results, particularly with well-trained models that have developed robust internal reasoning mechanisms.</p>
<p>Facts + Rule prompting</p>
<p>We explore an alternative approach that retains the benefits of structured knowledge representation while minimizing the complexity of formal logical programming.This method embeds logical rules directly into natural language prompts, testing LLMs' ability to reason within structured frameworks.Using the DSPy framework, this approach functions as a rule-based chain-of-thought (CoT) prompting strategy executed in two stages.</p>
<p>This approach aligns with the core principle of neural-symbolic AI: converting raw data into structured, symbolic representations for reasoning.By using predicates with precise argument structures, LLMs create consistent intermediate knowledge representations that facilitate question answering.This streamlined process maintains the advantages of formal reasoning while reducing computational complexity and implementation overhead.</p>
<p>In the LLM+ASP pipeline, generating a single ASP program requires multiple LLM calls due to the iterative refinement process, creating substantial computational overhead even with a modest three-iteration limit.To mitigate this, our alternative approach replaces formal ASP code generation with direct application of logical rules within natural language.The process begins by instructing LLMs to convert natural language inputs into structured facts using predefined predicates, a step retained from the LLM+ASP pipeline.Instead of formal logic programs, LLMs then apply relevant logical rules-such as inverse, symmetric, and transitive relations for the SparQA dataset, or offset-based chain-linking rules for StepGame-to derive answers through natural language reasoning.</p>
<p>By prompting LLMs to explicitly apply specific rules for different scenarios, this approach avoids dependence on external solvers.Consequently, it is termed "Facts+Rules."This method offers a more straightforward and reliable path to spatial reasoning compared to generating and executing formal logic programs.</p>
<p>Experiment Results and Discussion</p>
<p>In this section, we present the results of three methods: Direct Prompting Baseline, Facts + Rules Prompting, and Iterative LLM+ASP, on two benchmark datasets: StepGame and SparQA.We analyze their performance, computational complexity, and suitability for different types of spatial reasoning tasks.Additionally, we conduct an ablation study to assess the impact of iterative feedback loops in the LLM+ASP approach.</p>
<p>StepGame</p>
<p>Implementation</p>
<p>Another important aspect of the StepGame dataset is its reasoning hops range from 1 to 10 hops, distributed across 10 subsets.Specifically, each subset consists of 10,000 samples, each corresponding to a single reasoning hop.We sampled 300 instances for each reasoning hop 1 to 10, k ∈ {1, . . ., 10} , ensuring comprehensive evaluation across complexity levels.To standardize the task, we prompt LLMs to convert language descriptions into is3 and query2 facts using nine predefined spatial relations (e.g., left, right, top-right).Additional guidelines ensure accurate relation mapping, distinguishing between clockwise directions and cardinal references.This systematic approach maintains consistency and aids the models in handling spatial reasoning effectively.</p>
<p>Our pipeline integrates a customized knowledge module adapted from Yang et al. (2023b).This module employs coordinate-based reasoning rules, treating the second queried object as the reference point (0,0).By applying cardinal offsets, the module calculates the relative positions of connected objects, determining their spatial relationships iteratively.This method refines the models' intermediate inferences, enhancing accuracy in multi-hop tasks.</p>
<p>Results</p>
<p>As shown in Table 1, the DSPy pipeline consistently outperforms baseline methods (Direct prompting and Facts+Rules) across all reasoning depths for Deepseek, Llama3, and GPT-4.0 mini models.This demonstrates the effectiveness of integrating linguistic processing with logical reasoning.For examples, at reasoning hop k=5, Deepseek's accuracy jumps from 33.3% (Direct) to 88.5% (+55.2%) using DSPy.Llama3 improves from 28.2% to 77.9% (+49.7%), and GPT-4.0 mini increases from 29.9% to 87.1% (+57.2%).These gains highlight the power of neural-symbolic feedback.Accuracy declines as reasoning depth increases, reflecting task complexity.For example, at k=10, Direct prompting achieves only 16.8% (Deepseek), 14.2% (Llama3), and 13.5% (GPT-4.0mini), while DSPy maintains significantly higher performance: 79.8%, 65.7%, and 68.9%, respectively.</p>
<p>Adding Facts+Rules boosts the overall accuracy to 44.9%, 47.0%, and 47.6%, though it remains limited by sequential dependencies.In contrast, DSPy achieves the highest overall accuracy: 87.7% (Deepseek), 75.6% (Llama3), and 80.8% (GPT-4.0mini).It is obvious that the DSPy pipeline's iterative approach consistently enhances accuracy, particularly for deeper reasoning tasks, validating the benefits of combining symbolic reasoning with neural methods.</p>
<p>Analysis</p>
<p>Due to the controlled complexity and simple language structure, StepGame provides an ideal testbed for evaluating neural-symbolic integration for reasoning, allowing us to isolate the effects of multi-hop reasoning depth from linguistic complexity.There are no co-reference or named entity recognition issues since each agent is clearly defined (eg., "A", "B", "C", "D"), and there is only one question type, querying the relation between two agents.</p>
<p>The success of LLM + ASP can be attributed to several key factors.The simplified predicate structure, utilizing only is/3 and query/2 predicates, provides a clear bridge between natural language and logical forms.This simplification, com-bined with our well-designed knowledge module, enables efficient handling of spatial relationships while maintaining robust error detection capabilities.</p>
<p>Interestingly, the LLM+ASP method functions as a dataset quality checker.Aligning with Yang et al. (2023b), we identify labeling errors in 10% of the instances.Ambiguities in crowdsourced data accumulate with reasoning depth, revealing issues when models output multiple answers.This highlights the potential of neural-symbolic approaches not only for reasoning but also for improving dataset integrity.</p>
<p>SparQA</p>
<p>In order to evaluate the generalizability of our LLM+ASP approach against the more challenging benchmark SparQA, which focuses on complex natural language queries involving spatial relationships and requires precise fact extraction and reasoning over intricate descriptions.We aims to understand both the capabilities and limitations of neural-symbolic integration when confronted with realistic spatial reasoning tasks that more closely approximate real-world complexity.</p>
<p>Implementation</p>
<p>A representative test set of 220 examples, with 55 samples from each question type, was constructed to balance computational constraints and ensure comprehensive coverage.We also deliberately include challenging quantifier questions to assess the system's capabilities.</p>
<p>We adopted the same pipeline as StepGame to SparQA: (1) Converting Natural Language Context and Question to ASP Facts; (2) Adding Rules and Refining ASP Program; (3) Symbolic Reasoning; (4) Result Mapping and Evaluation.The first module prompted LLMs to identify blocks, objects, and relation facts using three predicates: block/1, object/5, and is/3, constraining the specific relation sets to minimize grounding errors.Rule refinement incorporates predefined inverse, transitive, and symmetric rules for spatial relations, enabling reasoning across multi-block environments.These rules were manually designed and updated, enhancing the system's capability to handle complex spatial queries that go beyond simple 2D reasoning tasks.The specific code and samples are seen in Appendix.</p>
<p>Key challenges in the implementation include parsing context-level descriptions for coreference resolution, representing implicit spatial relationships to avoid grounding errors, and managing object references with varying complexity (object/5).Additionally, query generation is difficult due to diverse question structures and complex quantification requirements.Despite careful prompt engineering, LLMs still struggle with generating queries that are different from the provided examples in the prompts.To overcome the challenge, we try to provide question type-wise query examples, quantification encoding query exampels to guide LLMs to write error free query.As shown in Table 2, the neural-symbolic LLM+ASP pipeline showed mixed results across different models and question types.Finding Relation (FR) questions demonstrated significant improvement with accuracy increasing by approximately 20% across all models (from 26.9% to 53.1% on Llama3, 38.2% to 58.9% on Deepseek, and 45.4% to 65.3% on GPT 4.0).Finding Block (FB) questions benefited from structured block/5 predicate representation, showing substantial gains particularly in GPT 4.0 (from 60.9% to 80.5%).Choose Object (CO) questions showed varied results, with GPT-4.0 achieving a notable 15% improvement while other models showed minimal changes.Interestingly, Yes/No (YN) questions does not benefit too much from complexity of neural-symbolic methods, only showing a slight improvement overr with direct prompting across all models.</p>
<p>Results</p>
<p>SparQA include a lot quantifier reasoning question, almost one third questions include "all, only, any".For examples,"Which block has only squares inside?"."What block has all of the black objects inside of it?","Are all of the triangles to the left of the black circle?",Besides that, it also envolves a set, namely, using one attribute to represent the whole set of objects.Thanks to the object/5 atoms, we use use object(<em>,</em>,black,<em>,</em>) in the query to match all the objects with all the black objects.Quantification typically challenges the pure neural approach.I If an LLM were to rely solely on natural language inference, it would need to exhaustively examine all objects with the black color.In contrast, converting the questions into logical expressions using the universal conditional (:) is relatively straightforward.As long as the logical expression is correctly encoded, the reasoning process becomes swift and accurate.</p>
<p>For example, "What block has all the black objects inside of it?"will be represented as: query(Block):block(Block), not object(<em>, </em>, black, _, OtherBlock): block(OtherBlock), OtherBlock != Block.</p>
<p>As shown in the table, the Facts+Rules method achieves competitive and similar performance with the LLM+ASP approach while significantly outperforming direct prompting (more than 5% improvement).While LLM+ASP achieves marginally higher accuracy in some cases, Facts+Rules offers advantages in implementation simplicity and reduced computational overhead.The improvement over baseline might be attributed to two factors: more consistent entity naming conventions in natural language prompts, and explicit instructions to follow the spatial logical rules.</p>
<p>Analysis</p>
<p>The error analysis revealed four primary categories of issues in the neuralsymbolic system: parsing errors (31%), grounding errors (23%), satisfiable but no result issues (28%), and wrong answers (18%).Parsing errors, the most frequent issue, stemmed from syntax-related problems in ASP code generation, including unqualified relation specifications and improper predicate formatting.Grounding errors emerged from inconsistencies between variable naming and knowledge base content, while satisfiability issues arose when the provided facts and rules were insufficient for query resolution, particularly in cases involving implicit spatial relationships that the system failed to capture explicitly in the ASP code.</p>
<p>Each model exhibited distinct error patterns that impacted their performance differently.Deepseek demonstrated strength in handling complex spatial relationships but frequently generated syntax errors (42% of its errors) and struggled with consistent comment handling in ASP code.GPT-4.0 mini showed promising initial code generation capabilities but consistently encountered issues with argument ordering in block/5 predicates, leading to fact-query inconsistencies.Meanwhile, Llama3 maintained consistent cate formatting but showed higher rates of "satifiable but no result" failures, particularly in cases involving complex spatial relationship chains and nested relationships.To address these challenges, the system would benefit from model-specific optimization strategies with tailored prompting approaches and custom validation rules, alongside an enhanced knowledge representation system capable of better handling implicit relationships and ambiguous spatial descriptions.</p>
<p>Impact of Feedback Loop in the DSPy-based Pipeline</p>
<p>While language models demonstrate remarkable capabilities in semantic parsing task, their ability to generate executable logical programs remains challenging.Previous research points out that the direct translation from natural language to logical rules often results in low success rates, even below random baseline performance.(Feng et al., 2024) This observation reinforces the importance of neural-symbolic integration especially the feedback loop between the two components, so that LLMs can benefit from symbolic solvers output, as shown in (Yang et al., 2023b).</p>
<p>To systematically analyze the effectiveness of our iterative feedback mechanism, we examine three primary error types in ASP solver execution: parsing errors (syntax errors, undefined predicates), grounding failures (infinite grounding scenarios, memory constraints), and solving stage failures (overconstrained conditions, inconsistent rules).Additionally, even successfully executed programs might produce solutions that don't align with ground truth labels due to semantic gaps between natural language specifications and logical encodings.Our feedback mechanism addresses these challenges through carefully designed prompts that instruct LLMs on error patterns and their respective fixes.It is hypothesized that the iterative feedback loop would reduce parsing errors and grounding failures.</p>
<p>In the SparQA dataset, which features complex spatial reasoning tasks, our iterative feedback loop demonstrated substantial improvements across all models.As shown in Figure 2, the mechanism shows substantial improvements across all models: Deepseek's execution rate increased from 45.8% to 76.8%, Llama3 from 34.2% to 80.2%, and GPT4mini from 44.8% to 73.2%These results demonstrate that the feedback loop between LLMs and solvers effectively addresses the inherent challenges in natural language to logic translation.The most substantial improvements occur in the first feedback round, with continued but diminishing gains in the second round.While additional feedback rounds might yield further improvements, we limited our experiment to three rounds due to computational costs.These findings demonstrate that the feedback loop significantly enhances both program executability and solution accuracy, establishing the effectiveness of our neural-symbolic integration approach.</p>
<p>Conclusion</p>
<p>This paper presents a neural-symbolic integration approach that significantly enhances LLMs' spatial reasoning capabilities.Our experimental results demonstrate that iterative feedback between LLMs and ASP solvers effectively improves both program executability and accuracy.The pipeline achieves an average 82% accuracy on StepGame and 69% on SparQA, marking substantial improvements over traditional approaches.For instance, our experiments demonstrate significant improvements over the baseline prompt-ing methods, with accuracy increases of 40-50% on StepGame dataset and 8-15% on the more complex SparQA dataset.</p>
<p>The success of neural symbolic integration stems from three key factors: (1) the effective separation of semantic parsing and logical reasoning, enabling precise control over each component; (2) the well-defined spatial relationships in a 2D environment, allowing for unambiguous predicate representation; and (3) the efficient handling of multi-hop reasoning chains through explicit logical rules.</p>
<p>We explored a simplified neural-symbolic approach, Facts+Rules, as an alternative to complex logical programming.This method showed modest improvements of 15-30% over baseline prompting on both datasets and even a comparable perforamce as LLM+ASP pipleine on SparQA.This performance comparison reveals an important trade-off in neural-symbolic integration: LLM+ASP offers superior accuracy at the cost of implementation complexity and computational overhead, while Facts+Rules provides a more lightweight solution with reduced performance on structured tasks.These findings suggest that effective neural-symbolic integration can be achieved through different approaches, each offering distinct advantages in the balance between computational efficiency and reasoning capability.</p>
<p>The key contributions of our work include: (1) a systematic approach to boost spatial reasoning through neural-symbolic integration, (2) a cohesive pipeline that combines the strengths of LLMs and symbolic reasoning, and (3) robust knowledge representation techniques that generalize across different spatial reasoning tasks.Our iterative feedback mechanism particularly demonstrates the value of combining LLMs' natural language understanding with ASP's precise logical inference capabilities.</p>
<p>However, several limitations remain.The performance gap and the variable implementation complexity between StepGame (92%) and SparQA (65%) highlights the challenge of domain sensitivity in neural-symbolic systems.The pipeline struggles particularly with complex queries involving quantifiers and implicit spatial relationships.This study tackles the challenge by careful prompt engineering and providing more examples.Additionally, the conversion from natural language to logical programs remains error-prone, with execution rates varying significantly across different question types.Future work could finetune LLM on specialized logical program dataset and design more sophisticated feedback mechanisms and improved integration between neural and symbolic components.</p>
<p>This work establishes a foundation for enhancing LLMs' reasoning capabilities through neural-symbolic integration, driving forward the quest for more intelligent, interpretable, and efficient systems.In essence, the neuralsymbolic approach treats LLMs as agents within a carefully orchestrated system.This perspective shifts the focus from improving individual LLM performance to optimizing the interplay between different components of the system.Future work should explore optimizing interactions between multiple models and reasoning components, involving more sophisticated orchestration techniques, improved integration of probabilistic reasoning with symbolic solvers, and employing the strengths of different LLMs at various stages of neural-symbolic systems.</p>
<p>Figure 1 :
1
Figure 1: LLM +ASP Pipeline</p>
<p>Figure 2 :
2
Figure 2: Effects of Feedback Loop between LLM and ASP on the SparQA Dataset</p>
<p>Table 1 :
1
Three Methods on StepGame (Accuracy %)
Model &amp; Methodk=1k=2k=3k=4k=5k=6k=7k=8k=9k=10OverallDeepseekDirect53.7 47.4 44.3 34.6 33.3 24.2 22.3 19.5 17.3 16.831.3Facts+Rules61.7 58.2 55.5 48.3 46.2 42.7 38.6 35.2 32.5 30.844.9DSPy Pipeline93.7 89.2 92.5 89.3 88.5 87.7 86.3 85.2 84.5 79.887.7Llama3Direct48.2 49.4 42.5 32.4 28.2 26.3 18.1 17.6 15.8 14.229.3Facts+Rules58.2 56.4 54.5 50.2 48.9 45.4 42.8 40.1 38.3 35.747.0DSPy Pipeline82.2 76.4 79.5 81.2 77.9 80.4 72.8 70.1 69.3 65.775.6GPT-4.0 miniDirect54.6 48.4 47.3 33.2 29.9 27.2 16.3 14.8 13.9 13.529.9Facts+Rules62.8 60.4 58.3 52.3 48.6 45.5 42.2 38.3 35.5 32.947.6DSPy Pipeline85.8 82.4 85.5 85.3 87.1 85.5 79.2 75.3 72.5 68.980.8</p>
<p>Table 2 :
2
Performance Comparison Across Methods on SparQA (Accuracy %)
Model &amp; Method FR FB YN CO OverallDeepseekDirect (Baseline)38.2 74.5 78.2 48.259.8Facts+Rules47.3 80.6 80.9 56.666.4DSPy Pipeline58.9 85.4 81.8 42.867.2Llama3Direct (Baseline)26.9 65.7 72.5 55.855.2Facts+Rules46.8 70.2 81.4 57.163.9DSPy Pipeline53.1 83.3 80.5 60.869.4GPT4.0 miniDirect (Baseline)45.4 60.9 58.2 57.655.5Facts+Rules54.3 68.2 50.4 61.558.6DSPy Pipeline65.3 80.5 64.8 72.770.3</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Proceedings of the 13th international joint conference on natural language processing and the 3rd conference of the asia-pacific chapter. the 13th international joint conference on natural language processing and the 3rd conference of the asia-pacific chapterthe association for computational linguistics20231Long papers</p>
<p>Neural-symbolic learning and reasoning: A survey and interpretation 1. T R Besold, A Garcez, S Bader, H Bowman, P Domingos, P Hitzler, K.-U Kühnberger, L C Lamb, P M V Lima, L De Penning, Neuro-symbolic artificial intelligence: The state of the art. IOS press2021</p>
<p>Answer set programming at a glance. G Brewka, T Eiter, M Truszczyński, Communications of the ACM. 54122011</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, ArXiv, abs/2309.154022023</p>
<p>An evaluation of ChatGPT-4's qualitative spatial reasoning capabilities in RCC-8. A G Cohn, arXiv:2309.155772023arXiv preprint</p>
<p>A neuro-symbolic ASP pipeline for visual question answering. T Eiter, N Higuera, J Oetsch, M Pritz, Theory and Practice of Logic Programming. 2252022Cambridge University Press</p>
<p>Large language models are neurosymbolic reasoners. M Fang, S Deng, Y Zhang, Z Shi, L Chen, M Pechenizkiy, J Wang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence20243816</p>
<p>Language models can be deductive solvers. J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao, W Chen, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>PAL: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, ArXiv, abs/2211.104352022</p>
<p>Neurosymbolic AI: The 3 rd wave. A D Garcez, L C Lamb, Artificial Intelligence Review. 56112023Springer</p>
<p>Explainable answer-set programming. T Geibinger, arXiv:2308.159012023arXiv preprint</p>
<p>Is neuro-symbolic ai meeting its promises in natural language processing? a structured review. Semantic Web. K Hamilton, A Nayak, B Božić, L Longo, 2022Publisher: IOS PressPreprint</p>
<p>Leveraging large language models to generate answer set programs. A Ishay, Z Yang, J Lee, arXiv:2307.076992023arXiv preprint</p>
<p>Dspy: Compiling declarative language model calls into selfimproving pipelines. O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, S Vardhamanan, S Haq, A Sharma, T T Joshi, H Moazam, Others, arXiv:2310.037142023arXiv preprint</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. R Mirzaee, P Kordjamshidi, arXiv:2210.169522022arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Y Wang, arXiv:2305.122952023arXiv preprint</p>
<p>A Parisi, Y Zhao, N Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.13155Logical neural networks. 2020arXiv preprint</p>
<p>True few-shot learning with prompts-a real-world perspective. T Schick, H Schütze, Transactions of the Association for Computational Linguistics. 102021</p>
<p>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Z Shi, Q Zhang, A Lipani, ber: 10Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202236</p>
<p>Z Wan, C.-K Liu, H Yang, C Li, H You, Y Fu, C Wan, T Krishna, Y Lin, A Raychowdhury, arXiv:2401.01040Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI. 2024</p>
<p>Review of large vision models and visual prompt engineering. J Wang, Z Liu, L Zhao, Z Wu, C Ma, S Yu, H Dai, Q Yang, Y.-H Liu, S Zhang, E Shi, Y Pan, T Zhang, D Zhu, X Li, X Jiang, B Ge, Y Yuan, D Shen, T Liu, S Zhang, ArXiv, abs/2307.008552023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, ArXiv, abs/2203.111712022</p>
<p>Nlprolog: Reasoning with weak unification for question answering in natural language. L Weber, P Minervini, J Münchmeyer, U Leser, T Rocktäschel, arXiv:1906.061872019arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, F Xia, Q Le, D Zhou, ArXiv, abs/2201.119032022</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Z Yang, A Ishay, J Lee, arXiv:2307.076962023aarXiv preprint</p>
<p>Z Yang, A Ishay, J Lee, arXiv:2307.07700Neurasp: Embracing neural networks into answer set programming. 2023barXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Scharli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, O Bousquet, Q Le, Chi , E , ArXiv, abs/2205.106252022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6946 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6946</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6946</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-272367184</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.00131v1.pdf" target="_blank">Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems</a></p>
                <p><strong>Paper Abstract:</strong> This study focuses on improving the performance of lightweight Large Language Models (LLMs) in mathematical reasoning tasks. We introduce a novel method for measuring mathematical logic similarity and design an automatic screening mechanism to construct a set of reference problems that integrate both semantic and logical similarity. By employing carefully crafted positive and negative example prompts, we guide the model towards adopting sound reasoning logic. To the best of our knowledge, this is the first attempt to utilize retrieval-enhanced generation for mathematical problem-solving. Experimental results demonstrate that our method achieves a 15.8% improvement over the Chain of Thought approach on the SVAMP dataset and a 21.5 % improvement on the GSM8K dataset. Further application of this method to a large-scale model with 175 billion parameters yields performance comparable to the best results on both aforementioned datasets. Finally, we conduct an analysis of errors during the reasoning process, providing valuable insights and directions for future research on reasoning tasks using large language models.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6946.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6946.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Correction (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction (SC) / multiple-sample selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/self-refinement strategy that generates multiple independent answers (samples) and selects the most probable or consensus answer as the final output to improve consistency and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Lightweight transformer-based LLM used in 4-bit quantized inference for math word problem evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple answers under identical prompting (multiple 'guess' attempts), then select the most probable/consistent answer (voting/re-ranking); described in the paper as leveraging law of large numbers to improve answer consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples (generate multiple independent answers and choose consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring multi-step reasoning (GSM8K more complex; SVAMP simpler/direct).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>LCR (without SC): GSM8K 45.1% accuracy; SVAMP 64.2% accuracy (reported as LCR(woSC) in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>LCR (with SC): GSM8K 59.2% accuracy; SVAMP 72.4% accuracy (reported as LCR(wSC) in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes that increasing number of guess attempts raises 'latent' (upper-bound) accuracy but actual accuracy gains can be modest; probabilistic factors prevent consistent selection of the correct reasoning path. Also, overly similar reference examples can mislead reasoning (not specific to SC but affects overall method). The authors report that comprehension and logical errors remain prevalent despite SC.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6946.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6946.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Correction (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction (SC) / multiple-sample selection (applied to large model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same multi-sample generation-and-selection strategy applied when LCR prompts are used with a 175B model (ChatGPT-3.5 Turbo in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5 Turbo (0301 4K)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large conversational transformer-based model (ChatGPT-3.5 family) used as a 175B-scale evaluation point in the paper's generalization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~175B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple candidate solutions (they report 'guess' attempts settings such as 5 and 10) and use selection/consensus to pick the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples (generate multiple independent answers and choose consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>ChatGPT + LCR (without SC): GSM8K 73.7% accuracy; SVAMP 81.0% accuracy (reported as Chatgpt+LCR(woSC)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>ChatGPT + LCR (with SC): GSM8K 86.9% accuracy; SVAMP 84.3% accuracy (reported as Chatgpt+LCR).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>On SVAMP the method sometimes underperforms alternative SC+PaLM baselines; authors attribute some failures to reference samples being highly similar to test questions and thus misleading the model. The paper also observes diminishing returns from increasing guess attempts (small actual accuracy change when increasing guesses from 5 to 10).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6946.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6946.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generate-multiple / Guess attempts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generate-multiple attempts (Guess5 / Guess10) used in parameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-level experimental setting where the model is run multiple times per problem (e.g., 5 or 10 'guess' attempts) to increase chance of producing a correct reasoning chain; used to compute both actual and 'latent' accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (primary experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Lightweight LLM evaluated under different numbers of few-shot examples and guess attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multiple independent generations (a form of self-correction/self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Run multiple independent decoding/generation attempts per problem (reported settings: 5 and 10 attempts). 'Latent' accuracy is reported as the fraction of problems for which any generated sample is correct (upper bound).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate multiple independent samples and (optionally) select/aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math word problem benchmarks; experiments show effect of few-shot count and number of guess attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and 'Latent' accuracy (percentage correct; latent = if any sample is correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Not a single fixed baseline; paper reports that increasing guess attempts improves latent accuracy substantially while actual accuracy improves less (specific numbers vary by setting; e.g., peak GSM8K accuracy observed 59.2% with 10 guesses/7 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>With more guess attempts latent accuracy increases (upper bound), but actual accuracy shows smaller gains; authors report marginal actual improvement when increasing guesses from 5 to 10 for some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Latent accuracy far exceeds actual accuracy in many settings, indicating correct reasoning paths exist but are not consistently produced/selected. Increasing attempts has cost and diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6946.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6946.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (SCt) (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding strategy that samples multiple chain-of-thought traces and aggregates answers via majority vote to improve chain-of-thought reasoning robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple chain-of-thoughts and choose the most consistent answer (voting/aggregation). Mentioned in related work as an effective self-verification/consensus technique.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>mentioned generally (math reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Referenced as an established approach improving CoT reasoning by aggregating multiple sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (as reported in cited work; not re-evaluated here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned only in related work; paper does not experimentally analyze its failure modes, but references suggest improvements come at the cost of multiple samples and possible diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6946.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6946.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that reverses the problem and answer to iteratively confirm correctness of a generated reasoning chain and answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Reverse the problem and the predicted answer to check and confirm correctness, repeating verification steps; cited as related prior work improving math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>recursive verification / check</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>mentioned generally (math reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Referenced as an approach that repeatedly confirms correctness of reasoning by reversing problem/answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only cited in related work; no experimental details or failure cases provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6946.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6946.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (external feedback and self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic framework that uses external feedback signals and internal reflection to detect errors and iteratively improve actions/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and selfreflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use external feedback to detect ineffective actions and engage in self-reflection to correct future behavior; cited as prior art for self-reflection mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>external-feedback-driven self-reflection (iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>mentioned generally</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Referenced as a self-reflection approach in prior literature; not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned only in related work; paper does not provide experimental evidence or failure analyses for Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and selfreflection <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Contrastive chain-of-thought prompting <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6946",
    "paper_id": "paper-272367184",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Self-Correction (SC)",
            "name_full": "Self-Correction (SC) / multiple-sample selection",
            "brief_description": "A decoding/self-refinement strategy that generates multiple independent answers (samples) and selects the most probable or consensus answer as the final output to improve consistency and accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Lightweight transformer-based LLM used in 4-bit quantized inference for math word problem evaluation.",
            "model_size": "7B",
            "reflection_method_name": "Self-Correction (SC)",
            "reflection_method_description": "Generate multiple answers under identical prompting (multiple 'guess' attempts), then select the most probable/consistent answer (voting/re-ranking); described in the paper as leveraging law of large numbers to improve answer consistency.",
            "iteration_type": "voting over multiple samples (generate multiple independent answers and choose consensus)",
            "num_iterations": null,
            "task_name": "GSM8K and SVAMP",
            "task_description": "Grade-school math word problems requiring multi-step reasoning (GSM8K more complex; SVAMP simpler/direct).",
            "evaluation_metric": "Accuracy (percentage correct)",
            "performance_before_reflection": "LCR (without SC): GSM8K 45.1% accuracy; SVAMP 64.2% accuracy (reported as LCR(woSC) in paper).",
            "performance_after_reflection": "LCR (with SC): GSM8K 59.2% accuracy; SVAMP 72.4% accuracy (reported as LCR(wSC) in paper).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Paper notes that increasing number of guess attempts raises 'latent' (upper-bound) accuracy but actual accuracy gains can be modest; probabilistic factors prevent consistent selection of the correct reasoning path. Also, overly similar reference examples can mislead reasoning (not specific to SC but affects overall method). The authors report that comprehension and logical errors remain prevalent despite SC.",
            "uuid": "e6946.0",
            "source_info": {
                "paper_title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Self-Correction (SC)",
            "name_full": "Self-Correction (SC) / multiple-sample selection (applied to large model)",
            "brief_description": "Same multi-sample generation-and-selection strategy applied when LCR prompts are used with a 175B model (ChatGPT-3.5 Turbo in this paper).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5 Turbo (0301 4K)",
            "model_description": "Large conversational transformer-based model (ChatGPT-3.5 family) used as a 175B-scale evaluation point in the paper's generalization experiments.",
            "model_size": "~175B",
            "reflection_method_name": "Self-Correction (SC)",
            "reflection_method_description": "Generate multiple candidate solutions (they report 'guess' attempts settings such as 5 and 10) and use selection/consensus to pick the final answer.",
            "iteration_type": "voting over multiple samples (generate multiple independent answers and choose consensus)",
            "num_iterations": null,
            "task_name": "GSM8K and SVAMP",
            "task_description": "Grade-school math word problems requiring multi-step reasoning.",
            "evaluation_metric": "Accuracy (percentage correct)",
            "performance_before_reflection": "ChatGPT + LCR (without SC): GSM8K 73.7% accuracy; SVAMP 81.0% accuracy (reported as Chatgpt+LCR(woSC)).",
            "performance_after_reflection": "ChatGPT + LCR (with SC): GSM8K 86.9% accuracy; SVAMP 84.3% accuracy (reported as Chatgpt+LCR).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "On SVAMP the method sometimes underperforms alternative SC+PaLM baselines; authors attribute some failures to reference samples being highly similar to test questions and thus misleading the model. The paper also observes diminishing returns from increasing guess attempts (small actual accuracy change when increasing guesses from 5 to 10).",
            "uuid": "e6946.1",
            "source_info": {
                "paper_title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Generate-multiple / Guess attempts",
            "name_full": "Generate-multiple attempts (Guess5 / Guess10) used in parameter optimization",
            "brief_description": "Paper-level experimental setting where the model is run multiple times per problem (e.g., 5 or 10 'guess' attempts) to increase chance of producing a correct reasoning chain; used to compute both actual and 'latent' accuracies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (primary experiments)",
            "model_description": "Lightweight LLM evaluated under different numbers of few-shot examples and guess attempts.",
            "model_size": "7B",
            "reflection_method_name": "Multiple independent generations (a form of self-correction/self-consistency)",
            "reflection_method_description": "Run multiple independent decoding/generation attempts per problem (reported settings: 5 and 10 attempts). 'Latent' accuracy is reported as the fraction of problems for which any generated sample is correct (upper bound).",
            "iteration_type": "generate multiple independent samples and (optionally) select/aggregate",
            "num_iterations": null,
            "task_name": "GSM8K and SVAMP",
            "task_description": "Math word problem benchmarks; experiments show effect of few-shot count and number of guess attempts.",
            "evaluation_metric": "Accuracy and 'Latent' accuracy (percentage correct; latent = if any sample is correct)",
            "performance_before_reflection": "Not a single fixed baseline; paper reports that increasing guess attempts improves latent accuracy substantially while actual accuracy improves less (specific numbers vary by setting; e.g., peak GSM8K accuracy observed 59.2% with 10 guesses/7 examples).",
            "performance_after_reflection": "With more guess attempts latent accuracy increases (upper bound), but actual accuracy shows smaller gains; authors report marginal actual improvement when increasing guesses from 5 to 10 for some configurations.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Latent accuracy far exceeds actual accuracy in many settings, indicating correct reasoning paths exist but are not consistently produced/selected. Increasing attempts has cost and diminishing returns.",
            "uuid": "e6946.2",
            "source_info": {
                "paper_title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-Consistency (SCt) (related work)",
            "brief_description": "A decoding strategy that samples multiple chain-of-thought traces and aggregates answers via majority vote to improve chain-of-thought reasoning robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": "",
            "reflection_method_name": "Self-Consistency",
            "reflection_method_description": "Sample multiple chain-of-thoughts and choose the most consistent answer (voting/aggregation). Mentioned in related work as an effective self-verification/consensus technique.",
            "iteration_type": "voting over multiple samples",
            "num_iterations": null,
            "task_name": "mentioned generally (math reasoning tasks)",
            "task_description": "Referenced as an established approach improving CoT reasoning by aggregating multiple sampled chains.",
            "evaluation_metric": "Accuracy (as reported in cited work; not re-evaluated here)",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned only in related work; paper does not experimentally analyze its failure modes, but references suggest improvements come at the cost of multiple samples and possible diminishing returns.",
            "uuid": "e6946.3",
            "source_info": {
                "paper_title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Self-verification",
            "name_full": "Self-verification",
            "brief_description": "A method that reverses the problem and answer to iteratively confirm correctness of a generated reasoning chain and answer.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": "",
            "reflection_method_name": "Self-verification",
            "reflection_method_description": "Reverse the problem and the predicted answer to check and confirm correctness, repeating verification steps; cited as related prior work improving math reasoning.",
            "iteration_type": "recursive verification / check",
            "num_iterations": null,
            "task_name": "mentioned generally (math reasoning)",
            "task_description": "Referenced as an approach that repeatedly confirms correctness of reasoning by reversing problem/answer.",
            "evaluation_metric": "",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Only cited in related work; no experimental details or failure cases provided in this paper.",
            "uuid": "e6946.4",
            "source_info": {
                "paper_title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (external feedback and self-reflection)",
            "brief_description": "An agentic framework that uses external feedback signals and internal reflection to detect errors and iteratively improve actions/outputs.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and selfreflection",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": "",
            "reflection_method_name": "Reflexion",
            "reflection_method_description": "Use external feedback to detect ineffective actions and engage in self-reflection to correct future behavior; cited as prior art for self-reflection mechanisms.",
            "iteration_type": "external-feedback-driven self-reflection (iterative)",
            "num_iterations": null,
            "task_name": "mentioned generally",
            "task_description": "Referenced as a self-reflection approach in prior literature; not used in experiments.",
            "evaluation_metric": "",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned only in related work; paper does not provide experimental evidence or failure analyses for Reflexion.",
            "uuid": "e6946.5",
            "source_info": {
                "paper_title": "Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and selfreflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Contrastive chain-of-thought prompting",
            "rating": 1,
            "sanitized_title": "contrastive_chainofthought_prompting"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.01224225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems</p>
<p>Ding Kai 
Research Center for Scientific data open and sharing hub, Zhejiang Lab
Zhejiang Province311121P.R. China</p>
<p>Ma Zhenguo 
Research Center for Scientific data open and sharing hub, Zhejiang Lab
Zhejiang Province311121P.R. China</p>
<p>Yan Xiaoran 
Research Center for Scientific data open and sharing hub, Zhejiang Lab
Zhejiang Province311121P.R. China</p>
<p>Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems
2D26067F83893ECAFC07C7DBBD7FC2EE
This study focuses on improving the performance of lightweight Large Language Models (LLMs) in mathematical reasoning tasks.We introduce a novel method for measuring mathematical logic similarity and design an automatic screening mechanism to construct a set of reference problems that integrate both semantic and logical similarity.By employing carefully crafted positive and negative example prompts, we guide the model towards adopting sound reasoning logic.To the best of our knowledge, this is the first attempt to utilize retrieval-enhanced generation for mathematical problem-solving.Experimental results demonstrate that our method achieves a 15.8% improvement over the Chain of Thought approach on the SVAMP dataset and a 21.5 % improvement on the GSM8K dataset.Further application of this method to a large-scale model with 175 billion parameters yields performance comparable to the best results on both aforementioned datasets.Finally, we conduct an analysis of errors during the reasoning process, providing valuable insights and directions for future research on reasoning tasks using large language models.Code and dataset are</p>
<p>Introduction</p>
<p>Recent years have witnessed remarkable advancements in Large Language Models (LLMs) [1], achieving state-ofthe-art (SOTA) performance across diverse tasks and datasets.Despite their widespread deployment in realworld applications, LLMs continue to grapple with critical challenges, most notably the "hallucination" problem.This issue is particularly pronounced in tasks demanding precise logical reasoning, such as mathematical problem-solving and commonsense inference, significantly impacting the reliability and practical utility of these models.</p>
<p>The prevalence of hallucinations has ignited a vigorous debate within the research community regarding the true extent of LLMs' logical reasoning capabilities.While some researchers optimistically posit that LLMs possess logical abilities potentially surpassing human performance in certain domains [2], others maintain a more skeptical stance.These critics argue that LLMs fundamentally lack genuine logical reasoning capabilities, suggesting that their inference processes are merely probabilistic approximations, easily disrupted by subtle perturbations [3], [4].</p>
<p>Our research aligns more closely with the latter perspective.However, we hypothesize that it is possible to enhance the logical stability of LLMs during reasoning processes through targeted methodologies.To investigate this hypothesis, we focus on lightweight large language models with fewer than 10 billion parameters.These models, with their relatively limited logical reasoning capabilities, provide an ideal testbed for demonstrating the efficacy of our proposed improvements.</p>
<p>We select mathematical problem-solving as our primary evaluation benchmark, as it presents a rigorous test of a model's reasoning capabilities.Such tasks typically require the decomposition of problems into multiple interdependent steps, each involving precise computation.</p>
<p>Correct solutions are only achieved when all reasoning paths are accurately executed.</p>
<p>Consequently, mathematical problem-solving presents two fundamental challenges: managing the dependencies between reasoning steps and accurately interpreting conditional text.</p>
<p>The Chain-of-Thought (CoT) [6] approach has emerged as a widely adopted method to address these challenges.</p>
<p>By employing prompts such as "Let's think step by step", models can generate detailed reasoning steps, serving as contextual information to significantly improve inference accuracy.However, further investigations have revealed inherent limitations in the CoT method, including sequential errors and comprehension inaccuracies.</p>
<p>Models often conflate the logical order of mathematical problems with the textual sequence, and errors in extracting numerical values and computational logic are common [5].</p>
<p>To address these shortcomings, researchers have proposed various methods, including Retrieval-Augmented Generation (RAG) [7] to mitigate accuracy.These results indicate that even lightweight models, when appropriately augmented, can excel in complex reasoning tasks.Our research opens up promising new avenues for further exploration of the reasoning capabilities of large language models.</p>
<p>Related Works</p>
<p>Research on mathematical reasoning using large language models primarily focuses on two main directions: improvements based on prompting techniques and those based on fine-tuning.This article concentrates on promptbased improvements, which can be further categorized into two approaches: Chain-of-Thought (CoT) methods and self-verification methods.</p>
<p>Cot Prompt for Reasoning</p>
<p>Chain-of-thought (CoT) prompting, introduced by Wei et al. [6], demonstrated that specific prompts like "Let's think step-by-step" can enable language models to perform chain-of-thought reasoning in a zero-shot manner.This breakthrough has inspired numerous works that build upon step-by-step reasoning approaches.For instance, automatic chain-of-thought [12] was proposed to address the challenges associated with manually annotating chain-of-thought demonstrations.Additionally, researchers explored methods to decompose complex problems into multiple sub-problems [23], or even into code programs that can be automatically executed [13].</p>
<p>Contrastive chain-of-thought (CCoT) [11] enhances language model reasoning by proposing a contrastive approach to CoT.Active-Prompt [9] adapted language models to different tasks using task-specific example prompts with manually designed CoT reasoning annotations.To address the crucial question of determining which problems are most important and helpful for annotation, the authors proposed a solution that draws inspiration from uncertainty-based active learning.They introduced several metrics to characterize uncertainty, thereby selecting the most uncertain questions.Faithful CoT [10] approached the problem by separating it into two stages: Translation (Natural Language query → symbolic reasoning chain) and Problem Solving (reasoning chain → answer), utilizing a language model and a deterministic solver, respectively.</p>
<p>Self-verification for LLM</p>
<p>Multiple decoding strategies have been proposed in the literature to improve the output quality of language models.These include temperature sampling, top-k sampling, and minimum Bayes risk decoding [14].</p>
<p>Re-ranking is another common approach to enhance generation quality.Cobbe et al. (2021) [15] demonstrated that training a "verifier" to re-rank generated solutions substantially improves the solve rate on mathematical tasks, compared to merely fine-tuning the language model.Self-correction [18] techniques leverage the law of large numbers by generating multiple answers under identical conditions and selecting the most probable response as the final answer.This method effectively improves answer consistency while simultaneously increasing accuracy, demonstrating that large language models tend to favor correct answers in most cases.Additionally, some approaches incorporate mechanisms of self-critique and self-reflection into large language models, enabling them to refine their outputs.For example, Shinn, Labash et al. (2023) introduced Reflexion [16], a technique that employs external feedback to detect ineffective actions and engage in self-reflection.In self-verification [17], researchers reverse the problem and answer, repeatedly confirming the correctness of the reasoning.This method has also been shown to improve model accuracy.</p>
<p>Logic Contrastive Reasoning (Method)</p>
<p>Using large language models (LLMs) to solve mathematical application problems presents two main challenges: addressing logical reasoning errors and resolving semantic ambiguities.To address these issues, we draw inspiration from the effective Chain of Thought The first component addresses how to evaluate the similarity of solving processes between two mathematical problems, while the second component focuses on how to incorporate logically similar problems into the prompt to enhance solving accuracy.</p>
<p>Logic Similarity</p>
<p>We use prompts and large language models to logically structure mathematical problems, breaking them down into known conditions and questions to be solved.</p>
<p>Through CoT reasoning, we list intermediate questions and solve them step by step to reach the final answer.</p>
<p>Each reasoning step can be transformed into an algebraic expression, allowing us to approximate the similarity of reasoning steps through expression similarity.</p>
<p>To represent the complete reasoning process, we combine the individual reasoning steps sequentially, merging the formulas in the solving process into a total solving formula.This transforms the similarity of mathematical problems into a problem of calculating the similarity between two solving formulas.To represent algebraic similarity, we first align variables to eliminate inconsistencies in their order of appearance and position within the formula.We employ a straightforward method of replacing variables in the formula with placeholders.</p>
<p>For example, A<em>(B+C+D)</em>B is converted to @<em>(@+@+@)</em>@.</p>
<p>We then calculate the similarity between algebraic expressions using the Normalized Tree Edit Distance as a metric.This metric measures the proportion of characters that need to be modified for two strings to become identical, expressed as：
) ( ) ( 2 1 1 2 2 1 2 1 Al len Al len ) ,Al )+Lev(Al ,Al Lev(Al )= ,Al N(Al  (1)
where N represents the normalized logical similarity function, Al denotes the algebraic expression after variable alignment, Lev() is the Levenshtein edit distance, and len is the length of the expression string.</p>
<p>Building upon this, the Normalized Tree Edit Distance constructs a tree representation of the formula.However, this approach is highly sensitive to the tree structure.If two mathematical expressions exhibit slight structural differences, even when their mathematical meanings are similar, the calculated edit distance may still be substantial.Furthermore, due to the presence of properties such as commutativity and associativity, there are inherent variations in the ways mathematical expressions can be written, which makes it challenging for the NTED to accurately represent the similarity between expressions.</p>
<p>To address this issue, we propose a new metric for measuring expression similarity.First, we divide the expression into two parts by selecting a specific operator outside the parentheses as a splitting point, aiming to create two branches of approximately equal length.This approach allows for a more precise representation of the formula while minimizing excessive branching that could increase structural sensitivity.In this manner, the swapping of branches will not significantly affect the similarity measure.For instance, in Expression 2, when the splitting operator is multiplication or addition, the two branches can be interchanged.Therefore, we take the minimum value of the similarity measures obtained before and after the swap.Conversely, when the splitting operator is division or subtraction, the overall similarity is computed as the sum of the similarities of the two branches in their respective order.The method then calculates the Normalized Edit Distance for each branch and merges them by selecting the minimum value based on branch labels.The expression for the Normalized Tree Edit Distance TD is formulated as follows:
                 else , ) , min( ,*] [ if , 2 1 ) 3 ( 2 1 2 1 2 1 1 2 1 2 , 1 2 1 i i i i i i i i i j i ) ,Al N(Al ) ,Al N(Al ) T(Al ) Al N(Al ) ,Al TD(Al (2)
where T() represents the branch operator which is in [+,-,×,÷], and min indicates the minimum value.</p>
<p>Contrastive Reasoning</p>
<p>The simplest form of mathematical problem reasoning based on large language models can be represented as (1) Logical structuring of the mathematical problem using prompts and large models, breaking it down into known conditions and questions to be solved (Step 1 in Algorithm).
E(Q, A),
(2) Identifying intermediate questions in the logical reasoning based on known conditions and the problem to be solved.These are represented in text form, followed by mathematical reasoning to obtain answers (Step 2 in Table 1).</p>
<p>(3) Expressing the reasoning process in algebraic form, yielding algebraic expressions and solutions (Step 3 in Table 1).1. Finally, we construct a contrastive reasoning prompt, as illustrated in Prompt 4 of Table 1, to solve the mathematical problem.The TLS() function in the algorithm is an integrated function that combines both semantic and logical aspects.In the context of retrieving similar mathematical problems, both semantic information and logical information play important roles.Therefore, based on the logical similarity of the formulas, we incorporate semantic similarity by using hyperparameters to combine these two similarity measures, thereby representing the overall similarity between two mathematical problems.</p>
<p>As shown in Equation 3, TD() denotes the similarity between mathematical expressions Al1 and Al2 as in Equation 2. The Sem() function utilizes the semantic similarity model SentBERT [19] for its computation.</p>
<p>SentBERT is specifically designed to assess sentencelevel semantic similarity.In our approach, we first encode the mathematical problems, Q1 and Q2, into highdimensional vector representations using SentBERT.This model captures contextual information and semantic relationships within the expressions by leveraging a transformer-based architecture.Once the encoding is complete, we calculate the cosine similarity between the resulting vectors.This cosine similarity score reflects how closely related the two mathematical problems are in terms of their underlying semantic content.Finally, we set the hyper-parameter empirically at a value of 0.7.
) ,Al (Q ex ) ,Q Sem(Q α ) ,Al αTD(Al ) ,ex TLS(ex 1 1 1 2 1 2 1 2 1   (3)
where  =1- Algorithm.Pipeline for LCR</p>
<p>Sample Preprocessing</p>
<p>Step 1: Extract Known Conditions from Q Using a Prompt Prompt 1: "List the known conditions."</p>
<p>Step 2: Develop a Reasoning Process Based on Known</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>To validate the effectiveness of our proposed logic similarity-based reasoning method for lightweight large language models, we conduct evaluations on commonly used mathematical question-answering datasets.We utilize GSM8K [15] and SVAMP [22] datasets, both of which include problem descriptions and answers.</p>
<p>SVAMP contains relatively simple problems with direct answers, while GSM8K presents more complex problems with complete reasoning processes in the answers.</p>
<p>Our experiments employ the following lightweight models: Mistral-7B [20], LLaMA2-7B [21].These models are loaded using 4-bit quantization (INT4) to optimize memory usage.Inference parameters are set with a generation length of 400, top_p of 0.95, temperature of 0.1, top_k of 30, and a repetition penalty of 1.15.To demonstrate the method's efficacy across model scales, we also validate our approach on a 175B parameter model, specifically using the ChatGPT-3.5</p>
<p>Turbo 0301 4K version.</p>
<p>We use accuracy as our primary evaluation metric.To ensure fairness in our experiments, we utilize 100 test samples from both SVAMP and GSM8K datasets, conducting multiple test runs for robust results.</p>
<p>Main Results</p>
<p>We design multiple experiments to demonstrate the effectiveness of our proposed method.The process involved two main steps:</p>
<p>(1) Sample Selection: As shown in Table 1, we first screen for positive and negative examples.To reduce the computational cost of using the entire training set as reference samples, we apply logical similarity filtering to the training set.This process eliminate samples with high similarity.</p>
<p>(</p>
<p>Parameter Optimization</p>
<p>Using the Mistral-7B model, we evaluate the impact of two key parameters: the number of few-shot examples and the number of guess attempts.We test few-shot examples ranging from 1 to 9 (odd numbers only) and guess attempts of 5 and 10. and "Guess10" represent different numbers of prediction attempts.Our results demonstrate that:</p>
<p>(1) Increasing the number of guess attempts generally improves prediction accuracy.</p>
<p>(2) A higher number of few-shot examples correlates with improved prediction accuracy.</p>
<p>(3) The "Latent" accuracy, representing the model's potential accuracy when any of the multiple predictions is correct, can be viewed as an upper bound of the model's performance.This suggests that correct reasoning paths exist within the model, but probabilistic factors may hinder their consistent use.</p>
<p>Fig. 2 presents results for the GSM8K dataset under similar experimental conditions.We observe that while a higher number of guess attempts increase the latent accuracy, the actual accuracy change slightly when the guesses are increased from 5 to 10. Regarding the fewshot parameter, performance peak at 7 examples and declined at 9, indicating a threshold effect in the utility of reference samples for accuracy improvement.These results provide valuable insights into the optimal configuration of our proposed method and demonstrate its effectiveness across different problem complexities and model scales.</p>
<p>We compared our method with state-of-the-art algorithms:</p>
<p>Chain-of-Thought (CoT), Self-Correction(SC), and Planand-Solve(PS).Table 2 summarizes the results on SVAMP and GSM8K datasets using the Mistral-7B model.Our method consistently outperforms existing approaches on both datasets, demonstrating significant improvements in prediction accuracy.We achieve a 15.8% and 18.6% improvement over CoT On SVAMP and GSM8K respectively.Indeed, the SC method significantly contribute to accuracy improvement.</p>
<p>Without SC, our LRC method still outperform the CoT method, increasing accuracy by 7.6% on SVAMP and 7.4% on GSM8K respectively.These results highlight the effectiveness of our logic similarity-based reasoning method in enhancing mathematical problem-solving capabilities of lightweight large language models across varying problem complexities.</p>
<p>Contrastive Learning Experiments</p>
<p>To validate the effectiveness of our proposed logical similarity retrieval, we conduct experiments comparing various contrastive learning strategies.We employ two methods for selecting example samples: (1) Random</p>
<p>Error analysis</p>
<p>We conducted an analysis and statistical study of common errors in two datasets to understand how lightweight large language models might make mistakes in mathematical reasoning.We defined four types of errors: comprehension errors, calculation errors, logical errors, and formula errors.Comprehension errors occur when the model misunderstands the object to be solved.Given these statistics, the most critical area for improvement is enhancing the model's logical reasoning capabilities.Two potential approaches to address this are:</p>
<p>(1) Pre-training approach: Expand the pre-training dataset to include more mathematics, coding, and related content.</p>
<p>This method aims to improve the model's foundational understanding of logical and mathematical concepts.</p>
<p>(2)</p>
<p>Fine-tuning approach: During the supervised fine-tuning (SFT) stage, incorporate multi-step reasoning data.This approach focuses on teaching the model how to break down complex problems into manageable steps.In comparison, the second approach (fine-tuning with multistep reasoning data) is likely to be more feasible and costeffective in terms of resources required.</p>
<p>Generalization experiments</p>
<p>The proposed method is not only applicable to lightweight models but can also be extended to largescale language models.To demonstrate this, we conducted tests using  on the GSM8K and SVAMP datasets.The parameters were set to 7 reference samples and 5 guess attempts.The results presented in Table 5 show that: On the GSM8K dataset, LCR + ChatGPT (without SC) outperforms the baseline CoT + ChatGPT method by 12.0 %.LCR + ChatGPT with the SC strategy shows a 12.5 % improvement over the SC + PaLM combination on the GSM8K dataset.However, on the SVAMP dataset, our method slightly underperforms compared to SC + PaLM.</p>
<p>The relatively lower performance on SVAMP can be attributed to the high similarity between some questions in the reference set and the test set.In these cases, the reference samples may mislead the large language model in understanding and reasoning about the problem, leading to incorrect solutions.</p>
<p>Conclusion</p>
<p>To address the challenges of hallucinations and logical errors in mathematical reasoning tasks for lightweight large language models, we proposed a novel approach leveraging contrastive learning.Our approach enabled the automatic selection of a set of reference problems that shared logical similarities with the target problem.Using these positive and negative examples, we constructed tailored prompts that guide the language model to adopt reasoning strategies similar to the positive examples while avoiding errors common to the negative ones.</p>
<p>Experiments conducted on multiple public mathematical problem datasets demonstrated significant improvements over existing state-of-the-art methods.Furthermore, we successfully extended this method to a large language model with 175 billion parameters, achieving results comparable to optimal human performance on both datasets.Finally, we provided a comprehensive analysis of common issues encountered during the reasoning process of large language models.This analysis offers valuable insights for future enhancements in the reasoning capabilities of these models.</p>
<p>Declaration of generative AI and AI-assisted technologies in the writing process</p>
<p>During the preparation of this work the author(s) used</p>
<p>Claude3 LLM in order to improve readability.After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.</p>
<p>Fig 1.Comparison between semantic similarity and logical similarity metrics in characterizing the similarity of reasoning processes in mathematical problems.For the three mathematical problems shown in the figure, the logical similarity metric more accurately represents the similarity of reasoning processes between problems.</p>
<p>(</p>
<p>CoT) and contrastive methods, and propose a novel approach called Logic Contrastive Reasoning.This method uses a few sample problems as reference examples to guide the model in generating step-by-step solutions, thereby improving solving accuracy.Logic Contrastive Reasoning comprises two key components: Logic Similarity and Contrastive Reasoning.</p>
<p>consisting only of Question and Answer components.However, lightweight large models struggle significantly with extracting known conditions from the Question and planning the solution process.Adding prior knowledge to the prompt, including Chain of Thought (CoT) and examples, can improve mathematical reasoning accuracy.This enhanced reasoning can be represented as E(P, Q, A), P is the prompt.Contrastive Chain of Thought (CCoT) introduces positive and negative examples.These examples not only promote correct derivation by the large model but also help to avoid common errors.The reasoning formula for CCoT is E(P, Qs, D+, D-, Q, A), where Qs represents example questions, D represents the reasoning process, and positive and negative signs indicate correct and incorrect reasoning.In our approach, we introduce an algebraic solving form, represented by Al for the algebraic reasoning process.Thus, our contrastive reasoning formula becomes E[P, Qs,   D+, Al+, D-, Al-, Q, A].Unlike CCoT's method that synthesizes negative samples, we use math prompts with different models for screening to collect positive and negative examples, which is a RAG method.This approach generates negative examples that are more realistic.The process involves:</p>
<p>Combining text-form reasoning and algebraic reasoning to deduce the final answer.Then we solve each problem multiple times and compare the solutions with the true values to identify which problems are answered correctly or incorrectly.Samples with both correct and incorrect answers for the same problem are included in the example sample set.Based on this sample set, we use the logic similarity function to retrieve several examples relevant to the problem being solved, as shown in Step 5 of Table</p>
<p>Conditions and the Problem to be Solved, then Solve the Problem Step by Step Prompt 2: "Let's first understand the problem and devise a plan to solve it.Then, let's carry out the plan to solve the problem step by step."Step 3: Convert Known Conditions and Solution Process into Algebraic Form Prompt 3: "Transform the conditions into algebraic form using a key-value mapping.Then, convert the solving steps into algebraic form."Step 4: Summarize the Above Process to Obtain the Solution to the Problem Contrastive reasoning Step 5: Use Logical Similarity Function to Retrieve Similar Samples We use the Logical Similarity Function TLS() to retrieve samples from the example set that are similar to the problem to be solved: {ex1, ex2} = sorted(TLS(ex0, ex1), TLS(ex0, ex2), ..., TLS(ex0, exn)) Where: exi represents the i-th example sample, including Qs, D+, Al+, D-, Al-,ex0 is the problem to be solved,{} contains the exj samples that are calculated to be most logically similar to the problem to be solved sorted() is a function that sorts the values in descending order Step 6: Construct Reasoning Prompt and Solve the Problem Prompt4 ： Given a math problem, please solve it step by step.Please follow the examples{ex1，ex2}.</p>
<p>) Mathematical Problem Solving: Using the select positive and negative examples, we conduct mathematical problem-solving experiments, comparing various parameter settings and algorithms.</p>
<p>Fig. 2
2
Fig. 2 illustrates our findings.The horizontal axis represents the number of few-shot examples, while the vertical axis denotes the prediction accuracy."Guess5"</p>
<p>Fig 2 .
2
Fig 2. Accuracy Curves Under Different LCR Parameters.For the SVAMP dataset (left graph),the highest test accuracy is achieved with 10 guess attempts and 9 reference samples.For the GSM8K dataset (right graph),the peak accuracy of 59.2% is reached with 10 guess attempts and 7 reference samples.</p>
<p>figures are on Jerry's shelf, we simply subtract the number of action figures from the total number of items: 12 (total items) -5 (action figures) = 7."From a statistical perspective, comprehension errors and logical errors were the most prevalent, while calculation errors and formula errors occurred less frequently.In Table4, the error distributions in two datasets are shown, the proportion of the major errors are 64.3% and 73.8%.</p>
<p>Table 2 .
2
Accuracy comparison on GSM8K and SVAMP datasets with 5 methods.The best results are boldfaced.
Gsm8kSVAMPAccuracyLatentAccuracyLatentaccaccCoT[6]37.7/56.6/SC[18]47.060.259.582.3PS[24]50.563.162.884.9LCR(woSC)45.1/64.2/LCR(wSC)59.271.672.484.1</p>
<p>Table 3
3, on</p>
<p>Table 3
3Accuracy Comparison with 5 comparative learningstrategies.SVAMPGsm8kguess5guess10Guess5Guess10Fix62.170.343.748.7Hard62.970.950.055.1Contrastive63.471.553.259.4Semantic60.265.551.154.5RAGLCR(ours)66.772.455.159.2Logic RAG63.670.852.657.9In terms of consistency, the use of RAG (Retrieval-Augmented Generation) has significantly improved theconsistency of model predictions. Specifically, modelsthat utilize RAG demonstrate greater stability ingenerating results, whereas models without RAG tend toproduce a variety of inconsistent predictions. Thisinconsistency increases the risk of erroneous predictions,</p>
<p>Table 4
4. Reasoning error analysis on SVAMP andGSM8K.SVAMPGSM8kComprehension error10(35.7%)14(33.3%)Calculation error6(21.4%)8(19%)Logic error8(28.6%)17(40.5%)Equation error4(14.3%)3(7%)Total2842</p>
<p>Table 5Performance
5Performance
comparison on GSM8K and SVAMP datasets with various model sizes and prompts.
Gsm8kSVAMPModelsizeMistral+LCR(ours)59.272.47bCoT+Chatgpt[6]61.777.6175bSelf verification[17]65.176.9175bActive prompt(chat)[9]65.680.5175bPS+Chatgpt[24]70.781.7175bSC+Palm[18]74.486.6540bFaithful Cot[10]80.088.8175bContrastive CoT[11]86.285.2175bChatgpt+LCR(woSC)73.781.0175bChatgpt+LCR86.984.3175b
AppendixFix_example prompt:These are some examples for solving math problem: Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: We start with 15 trees.Later we have 21 trees.The difference must be the number of trees they planted.So, they must have planted21 -15 = 6trees.The answer is 6.A: He has 5 toys.He got 2 from mom, so after that he has 5 + 2 = 7 toys.Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys.The answer is 9. Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?A: There are 4 days from monday to thursday.5 computers were added each day.That means in total 4 * 5 = 20 computers were added.There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.The answer is 29.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?How much more does a t-shirt cost than a jersey?A:the shop makes $192.0 each t-shirt, so the t-shirt price is $192.0,then we can see the jersey price is 34.0, therefor t-shirt cost $192.0-$34.0 more, the answer is 158.0.Q:6 birds and 3 storks were sitting on the fence. 2 more storks came to join them.How many more birds than storks are sitting on the fence?A:6 birds and 3 storks were sitting on the fence,2 more storks came , then there were 3.0+2.0storks, so birds are Explanation: The wrong answer incorrectly assumes that the remaining 15 fish are the ones that disappeared, instead of the ones that are still left in the pond.The correct approach is to subtract the remaining fish 15 from the initial number of fish 19 to find the number of fish that disappeared 4. |EOS|
GPT-3: Its nature, scope, limits, and consequences. L Floridi, M Chiriatti, Minds and Machines. 202030</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. N Dziri, X Lu, M Sclar, X L Li, L Jiang, B Y Lin, S Welleck, P West, C Bhagavatula, R L Bras, J D Hwang, S Sanyal, X Ren, A Ettinger, Z Harchaoui, Y Choi, Thirtyseventh Conference on Neural Information Processing Systems. 2023</p>
<p>M Verma, S Bhambri, S Kambhampati, arXiv:2401.05302Theory of mind abilities of large language models in human-robot interaction: An illusion?. 2024barXiv preprint</p>
<p>Embers of auto regression: Understanding large language models through the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M Hardy, T L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, Advances in neural information processing systems. 202235</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, Advances in Neural Information Processing Systems. 202033</p>
<p>Towards understanding chain-ofthought prompting:An empirical study of what matters. Cot-P Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231</p>
<p>Active prompting with chain-of-thought for large language models. S Diao, P Wang, Y Lin, T Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Q Lyu, S Havaldar, A Stein, L Zhang, D Rao, E Wong, . . Callison-Burch, C , arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Y K Chia, G Chen, L A Tuan, S Poria, L Bing, arXiv:2311.09277Contrastive chain-of-thought prompting. 2023arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2023</p>
<p>PAL: Program-aided language models. Program-Cot Luyu, Aman Gao, Shuyan Madaan, Uri Zhou, Pengfei Alon, Yiming Liu, Jamie Yang, Graham Callan, Neubig, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202</p>
<p>Natural language to code translation with execution. Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I Wang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingDecember 2022</p>
<p>. T-Verify/Gsm8k Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Reflexion: an autonomous agent with dynamic memory and selfreflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Large Language Models are Better Reasoners with. Y Weng, M Zhu, F Xia, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, arXiv:2203.111712022arXiv preprint</p>
<p>N Reimers, I Gurevych, - Sentence, Bert, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019Sentence Embeddings using Siamese</p>
<p>. Q Jiang, A Sablayrolles, A Mensch, arXiv:2310.068252023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, arXiv:2307.092882023arXiv preprint</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Svamp, Satwik Patel, Navin Bhattamishra, Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnlineJune 2021</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.04091v3Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
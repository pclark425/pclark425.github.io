<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1121 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1121</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1121</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-249330619</p>
                <p><strong>Paper Title:</strong> An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts</p>
                <p><strong>Paper Abstract:</strong> Due to their dependence on a task-specific reward function, reinforcement learning agents are ineffective at responding to a dynamic goal or environment. This paper seeks to overcome this limitation of traditional reinforcement learning through a task-agnostic, self-organising autonomous agent framework. The proposed algorithm is a hybrid of TMGWR for self-adaptive learning of sensorimotor maps and value iteration for goal-directed planning. TMGWR has been previously demonstrated to overcome the problems associated with competing sensorimotor techniques such SOM, GNG, and GWR; these problems include: difficulty in setting a suitable number of neurons for a task, inflexibility, the inability to cope with non-markovian environments, challenges with noise, and inappropriate representation of sensory observations and actions together. However, the binary sensorimotor-link implementation in the original TMGWR enables catastrophic forgetting when the agent experiences changes in the task and it is therefore not suitable for self-adaptive learning. A new sensorimotor-link update rule is presented in this paper to enable the adaptation of the sensorimotor map to new experiences. This paper has demonstrated that the TMGWR-based algorithm has better sample efficiency than model-free reinforcement learning and better self-adaptivity than both the model-free and the traditional model-based reinforcement learning algorithms. Moreover, the algorithm has been demonstrated to give the lowest overall computational cost when compared to traditional reinforcement learning algorithms.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1121.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1121.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TMGWR-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TMGWR-based autonomous goal-directed agent (temporospatial merge grow when required)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, self-adaptive agent that learns a sensorimotor map online using a temporospatial GWR variant (TMGWR) with continuous sensorimotor-link weights, and uses model-based value iteration in the learned sensorimotor space for goal-directed planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TMGWR-based agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: sensory preprocessor -> TMGWR sensorimotor-map learner (nodes = sensory abstractions; edges = affordances) -> motivation estimator (value iteration on sensorimotor graph) -> action selector. Key components: TMGWR network with temporal context and GWR node growth/forgetting, continuous similarity-weighted sensorimotor-link update (E[i,j] update using Gaussian similarity of action vectors), model-based value iteration operating on learned transition strengths, and an adaptive epsilon-greedy exploration schedule; hyperparameters tuned by Bayesian optimisation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Self-adaptive sensorimotor-map learning (TMGWR) with similarity-weighted link adaptation, growing/forgetting GWR, model-based planning (value iteration) on learned model, and adaptive ε-greedy exploration; hyperparameter tuning via Bayesian optimisation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent continuously updates its sensorimotor map: when a transition i->j is observed under action a, the sensorimotor-link weight E[i,j] is incremented toward sim(a_ij,a) (Gaussian similarity) with learning rate α, so links become stronger if the same action reliably produces that transition and weaker otherwise; GWR growth adds nodes when quantisation error is high, and forgetting removes aged links/nodes. Given a goal node g, value iteration propagates motivation potentials V(i) over the learned graph using E[i,k] as transition reliabilities. Exploration probability ε decays adaptively (start 1.0 to min 0.1) when the agent repeatedly attains goals. Hyperparameters were tuned offline via Bayesian optimisation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>13×17 maze gridworld (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete deterministic gridworld maze with walls/obstacles; agent observations are its coordinates (preprocessed sensory inputs used); environment can change during lifetime (non-stationary) by insertion of obstacles; authors state TMGWR is designed to handle noisy, ambiguous, and partially observable settings via recurrence and GWR expansion, though the experiments used coordinate observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Grid of size 13×17 (221 cells nominal, reduced by walls); action set size 4 (up/down/left/right); episodes terminate on reaching goal; experiments used repeated iterations (training phases of 1000 iterations per goal-change phase) and online continual learning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Sample-efficiency comparable to hand-coded model-based RL and substantially better than model-free Q-learning; in environment-change (blocked optimal path) tests: average steps to find alternative path Case A = 112.6 ± 2.0 steps, Case B = 106.3 ± 1.6 steps. In changing-goal tests, promptly replans to new goals (near-100% visits to current goal), unlike Q-learning which often revisits previous goal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Converges to near-optimal ε-greedy policy substantially faster than model-free Q-learning and on a similar timescale to a model-based agent with known transitions (figures reported qualitatively); concrete adaptation-sample numbers: after obstacle insertion TMGWR required ~106–113 steps to reach goal via alternative path (vs Q-learning 450–1212 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Adaptive ε-greedy: ε initialized to 1.0 and decays toward min=0.1 according to a rule that decreases ε each time the agent attains the goal up to five times; exploration thus reduces as the agent demonstrates competence. Action selection uses value iteration-derived motivation potentials, selecting action a_i = a_i,kmax where kmax = argmax_k V(k) over reachable neighbour nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally against: (1) model-free Q-learning agent (ε-greedy exploration) and (2) model-based RL agent using value iteration with a hand-coded deterministic transition model.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>TMGWR-based method learns a transition model online and uses it for model-based planning in sensorimotor space; achieves sample efficiency similar to a model-based agent that has prior knowledge of transitions, and substantially better than model-free Q-learning. It adapts quickly to goal changes and to unexpected environment changes (blocked paths) because TMGWR continually updates the sensorimotor map; in contrast, a conventional model-based RL agent with hard-coded transitions fails completely when environment dynamics change unpredictably.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher computational/training time than model-free Q-learning (TMGWR training + value iteration are computationally heavier). Experiments performed in a low-dimensional gridworld; for high-dimensional sensory inputs (e.g., vision) authors note need for additional preprocessing (CNN feature extractor). No raw numeric cumulative reward trajectories are given; full robustness in highly partial/ambiguous real-world sensors remains to be validated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1121.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1121.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-learning (model-free)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-free Q-learning agent (ε-greedy exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard model-free reinforcement learning baseline (Q-learning) used in experiments with an adaptive/decayed ε-greedy exploration schedule and hyperparameters tuned by Bayesian optimisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Model-free Q-learning agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular Q-learning agent operating on environment states (grid cells) with four discrete actions; immediate rewards defined in environment state-action space and hyperparameters (learning rate, discount factor, reward values) optimised via Bayesian optimisation. Exploration: adaptive ε-greedy with decayed ε from 1.0 to min 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive ε-greedy exploration (decay rule: ε = max(ε - 1/10, min) with reductions when agent attains the goal up to five tries); Bayesian optimisation used offline to select hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Learns Q-values by trial-and-error; exploration probability decays as agent achieves goals repeatedly. When environment changes, Q-values must be relearned; adaptation occurs via continued exploration but is slow because learning is model-free and sample-inefficient.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>13×17 maze gridworld (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same gridworld as others: discrete deterministic dynamics, obstacles/walls, non-stationary when obstacles are added; agent receives coordinate observations (state).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>13×17 grid (≈221 cells nominal), 4 actions, episodes terminate at goal. Training phases of up to thousands of iterations used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Poorer sample efficiency than model-based and TMGWR agents; in changing-goal tests frequently revisited previous goals (e.g., G2→G3: % visits to previous goal 99.40 ± 0.32, current goal 0.60 ± 0.32; other changes similarly >99% previous visits). In changing-environment tests: average steps to find alternative path Case A = 1212.2 ± 12.9 steps, Case B = 450.4 ± 4.9 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires substantially more samples to converge to optimal behavior compared to model-based and TMGWR agents; convergence curves (Fig.6) show slower reduction in steps-to-goal. Exact number of iterations to convergence not tabulated numerically in paper, but convergence is markedly slower.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Adaptive ε-greedy (same schedule used across agents): ε starts at 1.0 and decays toward a minimum (0.1) when goals are repeatedly achieved; exploration therefore reduces over time but remains non-zero.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against TMGWR-based agent and model-based RL with hard-coded transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Model-free Q-learning is lightweight and has the fastest raw training time (computational cost per step lowest), but is sample-inefficient, adapts slowly to goal changes and especially poorly to environment changes (needs to relearn value function; large step counts to find alternative paths). It often continues to exploit previous-goal behavior when goals change.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High sample complexity and slow adaptation to unexpected environment changes; in some changing-goal cases it almost exclusively revisited previous goals (>99% visits). Though computationally cheap to train, this comes at cost of poor adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1121.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1121.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-based-VI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based RL agent using value iteration with hand-coded transition model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard model-based reinforcement learning baseline that uses value iteration on an explicitly provided deterministic transition model and environment-space reward function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Model-based RL (value iteration, hand-coded transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses a hard-coded deterministic transition model T(s,a,s') that is 1 if target cell contains no obstacle and 0 otherwise, combined with a state-based reward function; planning uses classical value iteration (dynamic programming) on environment states to compute policies. Exploration uses the same adaptive ε-greedy schedule in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>None for environment model adaptation; uses value iteration (model-based planning) but transition model is static/hard-coded and not updated online. Exploration policy is adaptive ε-greedy (shared with other agents).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Does not adapt its environment model from experience; replanning occurs by rerunning value iteration on the same (unchanged) model when goals change, which allows fast replanning for goal relocation but fails if the real environment dynamics change in a way not captured by the hard-coded model.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>13×17 maze gridworld (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete deterministic gridworld with walls; non-stationary changes (unexpected obstacle insertions) are not represented in its static model, causing catastrophic failure when environment changes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>13×17 grid, 4 actions, episodic termination at goal. Transition model assumes deterministic movements unless blocked by obstacle.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Performs well for goal changes when the environment remains as modelled (fast replanning to new goal). However, when the environment changes unpredictably (an optimal path blocked), it fails outright — reported as ∞ time/steps to recover in the two case studies (i.e., it never finds an alternative path because the hard-coded model no longer matches reality).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample-efficiency for nominal (stationary) environment because it uses a model; training time is shorter than TMGWR (no model learning), but since model is fixed this is only valid when environment is known and stationary.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Uses the same adaptive ε-greedy exploration schedule as other agents in experiments, but its primary limitation is the lack of online model adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against TMGWR-based agent and model-free Q-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Model-based planning with a correct model achieves good sample-efficiency and rapid replanning for goal changes, but when the environment dynamics are unknown or change unexpectedly, a fixed model-based agent fails catastrophically (in experiments it never recovered when an optimal path was blocked).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot adapt its environment model online; if the true transition dynamics differ from the hard-coded model (e.g., an unexpected obstacle), the agent becomes stuck and fails to find alternative paths (reported infinite recovery time).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised temporospatial neural architecture for sensorimotor map learning <em>(Rating: 2)</em></li>
                <li>Learning a world model and planning with a self-organizing, dynamic neural system <em>(Rating: 2)</em></li>
                <li>A sensorimotor map: Modulating lateral interactions for anticipation and planning <em>(Rating: 2)</em></li>
                <li>Merge SOM for temporal data <em>(Rating: 1)</em></li>
                <li>Intrinsic motivation systems for autonomous mental development <em>(Rating: 1)</em></li>
                <li>Adaptive ε-greedy exploration in reinforcement learning based on value differences <em>(Rating: 2)</em></li>
                <li>Value iteration networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1121",
    "paper_id": "paper-249330619",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "TMGWR-agent",
            "name_full": "TMGWR-based autonomous goal-directed agent (temporospatial merge grow when required)",
            "brief_description": "An unsupervised, self-adaptive agent that learns a sensorimotor map online using a temporospatial GWR variant (TMGWR) with continuous sensorimotor-link weights, and uses model-based value iteration in the learned sensorimotor space for goal-directed planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TMGWR-based agent",
            "agent_description": "Architecture: sensory preprocessor -&gt; TMGWR sensorimotor-map learner (nodes = sensory abstractions; edges = affordances) -&gt; motivation estimator (value iteration on sensorimotor graph) -&gt; action selector. Key components: TMGWR network with temporal context and GWR node growth/forgetting, continuous similarity-weighted sensorimotor-link update (E[i,j] update using Gaussian similarity of action vectors), model-based value iteration operating on learned transition strengths, and an adaptive epsilon-greedy exploration schedule; hyperparameters tuned by Bayesian optimisation.",
            "adaptive_design_method": "Self-adaptive sensorimotor-map learning (TMGWR) with similarity-weighted link adaptation, growing/forgetting GWR, model-based planning (value iteration) on learned model, and adaptive ε-greedy exploration; hyperparameter tuning via Bayesian optimisation.",
            "adaptation_strategy_description": "The agent continuously updates its sensorimotor map: when a transition i-&gt;j is observed under action a, the sensorimotor-link weight E[i,j] is incremented toward sim(a_ij,a) (Gaussian similarity) with learning rate α, so links become stronger if the same action reliably produces that transition and weaker otherwise; GWR growth adds nodes when quantisation error is high, and forgetting removes aged links/nodes. Given a goal node g, value iteration propagates motivation potentials V(i) over the learned graph using E[i,k] as transition reliabilities. Exploration probability ε decays adaptively (start 1.0 to min 0.1) when the agent repeatedly attains goals. Hyperparameters were tuned offline via Bayesian optimisation.",
            "environment_name": "13×17 maze gridworld (custom)",
            "environment_characteristics": "Discrete deterministic gridworld maze with walls/obstacles; agent observations are its coordinates (preprocessed sensory inputs used); environment can change during lifetime (non-stationary) by insertion of obstacles; authors state TMGWR is designed to handle noisy, ambiguous, and partially observable settings via recurrence and GWR expansion, though the experiments used coordinate observations.",
            "environment_complexity": "Grid of size 13×17 (221 cells nominal, reduced by walls); action set size 4 (up/down/left/right); episodes terminate on reaching goal; experiments used repeated iterations (training phases of 1000 iterations per goal-change phase) and online continual learning.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Sample-efficiency comparable to hand-coded model-based RL and substantially better than model-free Q-learning; in environment-change (blocked optimal path) tests: average steps to find alternative path Case A = 112.6 ± 2.0 steps, Case B = 106.3 ± 1.6 steps. In changing-goal tests, promptly replans to new goals (near-100% visits to current goal), unlike Q-learning which often revisits previous goal.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Converges to near-optimal ε-greedy policy substantially faster than model-free Q-learning and on a similar timescale to a model-based agent with known transitions (figures reported qualitatively); concrete adaptation-sample numbers: after obstacle insertion TMGWR required ~106–113 steps to reach goal via alternative path (vs Q-learning 450–1212 steps).",
            "exploration_exploitation_tradeoff": "Adaptive ε-greedy: ε initialized to 1.0 and decays toward min=0.1 according to a rule that decreases ε each time the agent attains the goal up to five times; exploration thus reduces as the agent demonstrates competence. Action selection uses value iteration-derived motivation potentials, selecting action a_i = a_i,kmax where kmax = argmax_k V(k) over reachable neighbour nodes.",
            "comparison_methods": "Compared experimentally against: (1) model-free Q-learning agent (ε-greedy exploration) and (2) model-based RL agent using value iteration with a hand-coded deterministic transition model.",
            "key_results": "TMGWR-based method learns a transition model online and uses it for model-based planning in sensorimotor space; achieves sample efficiency similar to a model-based agent that has prior knowledge of transitions, and substantially better than model-free Q-learning. It adapts quickly to goal changes and to unexpected environment changes (blocked paths) because TMGWR continually updates the sensorimotor map; in contrast, a conventional model-based RL agent with hard-coded transitions fails completely when environment dynamics change unpredictably.",
            "limitations_or_failures": "Higher computational/training time than model-free Q-learning (TMGWR training + value iteration are computationally heavier). Experiments performed in a low-dimensional gridworld; for high-dimensional sensory inputs (e.g., vision) authors note need for additional preprocessing (CNN feature extractor). No raw numeric cumulative reward trajectories are given; full robustness in highly partial/ambiguous real-world sensors remains to be validated.",
            "uuid": "e1121.0",
            "source_info": {
                "paper_title": "An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Q-learning (model-free)",
            "name_full": "Model-free Q-learning agent (ε-greedy exploration)",
            "brief_description": "Standard model-free reinforcement learning baseline (Q-learning) used in experiments with an adaptive/decayed ε-greedy exploration schedule and hyperparameters tuned by Bayesian optimisation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Model-free Q-learning agent",
            "agent_description": "Tabular Q-learning agent operating on environment states (grid cells) with four discrete actions; immediate rewards defined in environment state-action space and hyperparameters (learning rate, discount factor, reward values) optimised via Bayesian optimisation. Exploration: adaptive ε-greedy with decayed ε from 1.0 to min 0.1.",
            "adaptive_design_method": "Adaptive ε-greedy exploration (decay rule: ε = max(ε - 1/10, min) with reductions when agent attains the goal up to five tries); Bayesian optimisation used offline to select hyperparameters.",
            "adaptation_strategy_description": "Learns Q-values by trial-and-error; exploration probability decays as agent achieves goals repeatedly. When environment changes, Q-values must be relearned; adaptation occurs via continued exploration but is slow because learning is model-free and sample-inefficient.",
            "environment_name": "13×17 maze gridworld (custom)",
            "environment_characteristics": "Same gridworld as others: discrete deterministic dynamics, obstacles/walls, non-stationary when obstacles are added; agent receives coordinate observations (state).",
            "environment_complexity": "13×17 grid (≈221 cells nominal), 4 actions, episodes terminate at goal. Training phases of up to thousands of iterations used.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Poorer sample efficiency than model-based and TMGWR agents; in changing-goal tests frequently revisited previous goals (e.g., G2→G3: % visits to previous goal 99.40 ± 0.32, current goal 0.60 ± 0.32; other changes similarly &gt;99% previous visits). In changing-environment tests: average steps to find alternative path Case A = 1212.2 ± 12.9 steps, Case B = 450.4 ± 4.9 steps.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Requires substantially more samples to converge to optimal behavior compared to model-based and TMGWR agents; convergence curves (Fig.6) show slower reduction in steps-to-goal. Exact number of iterations to convergence not tabulated numerically in paper, but convergence is markedly slower.",
            "exploration_exploitation_tradeoff": "Adaptive ε-greedy (same schedule used across agents): ε starts at 1.0 and decays toward a minimum (0.1) when goals are repeatedly achieved; exploration therefore reduces over time but remains non-zero.",
            "comparison_methods": "Compared against TMGWR-based agent and model-based RL with hard-coded transitions.",
            "key_results": "Model-free Q-learning is lightweight and has the fastest raw training time (computational cost per step lowest), but is sample-inefficient, adapts slowly to goal changes and especially poorly to environment changes (needs to relearn value function; large step counts to find alternative paths). It often continues to exploit previous-goal behavior when goals change.",
            "limitations_or_failures": "High sample complexity and slow adaptation to unexpected environment changes; in some changing-goal cases it almost exclusively revisited previous goals (&gt;99% visits). Though computationally cheap to train, this comes at cost of poor adaptability.",
            "uuid": "e1121.1",
            "source_info": {
                "paper_title": "An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Model-based-VI",
            "name_full": "Model-based RL agent using value iteration with hand-coded transition model",
            "brief_description": "A standard model-based reinforcement learning baseline that uses value iteration on an explicitly provided deterministic transition model and environment-space reward function.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Model-based RL (value iteration, hand-coded transitions)",
            "agent_description": "Agent uses a hard-coded deterministic transition model T(s,a,s') that is 1 if target cell contains no obstacle and 0 otherwise, combined with a state-based reward function; planning uses classical value iteration (dynamic programming) on environment states to compute policies. Exploration uses the same adaptive ε-greedy schedule in experiments.",
            "adaptive_design_method": "None for environment model adaptation; uses value iteration (model-based planning) but transition model is static/hard-coded and not updated online. Exploration policy is adaptive ε-greedy (shared with other agents).",
            "adaptation_strategy_description": "Does not adapt its environment model from experience; replanning occurs by rerunning value iteration on the same (unchanged) model when goals change, which allows fast replanning for goal relocation but fails if the real environment dynamics change in a way not captured by the hard-coded model.",
            "environment_name": "13×17 maze gridworld (custom)",
            "environment_characteristics": "Discrete deterministic gridworld with walls; non-stationary changes (unexpected obstacle insertions) are not represented in its static model, causing catastrophic failure when environment changes.",
            "environment_complexity": "13×17 grid, 4 actions, episodic termination at goal. Transition model assumes deterministic movements unless blocked by obstacle.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": "Performs well for goal changes when the environment remains as modelled (fast replanning to new goal). However, when the environment changes unpredictably (an optimal path blocked), it fails outright — reported as ∞ time/steps to recover in the two case studies (i.e., it never finds an alternative path because the hard-coded model no longer matches reality).",
            "performance_without_adaptation": null,
            "sample_efficiency": "High sample-efficiency for nominal (stationary) environment because it uses a model; training time is shorter than TMGWR (no model learning), but since model is fixed this is only valid when environment is known and stationary.",
            "exploration_exploitation_tradeoff": "Uses the same adaptive ε-greedy exploration schedule as other agents in experiments, but its primary limitation is the lack of online model adaptation.",
            "comparison_methods": "Compared against TMGWR-based agent and model-free Q-learning.",
            "key_results": "Model-based planning with a correct model achieves good sample-efficiency and rapid replanning for goal changes, but when the environment dynamics are unknown or change unexpectedly, a fixed model-based agent fails catastrophically (in experiments it never recovered when an optimal path was blocked).",
            "limitations_or_failures": "Cannot adapt its environment model online; if the true transition dynamics differ from the hard-coded model (e.g., an unexpected obstacle), the agent becomes stuck and fails to find alternative paths (reported infinite recovery time).",
            "uuid": "e1121.2",
            "source_info": {
                "paper_title": "An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised temporospatial neural architecture for sensorimotor map learning",
            "rating": 2,
            "sanitized_title": "unsupervised_temporospatial_neural_architecture_for_sensorimotor_map_learning"
        },
        {
            "paper_title": "Learning a world model and planning with a self-organizing, dynamic neural system",
            "rating": 2,
            "sanitized_title": "learning_a_world_model_and_planning_with_a_selforganizing_dynamic_neural_system"
        },
        {
            "paper_title": "A sensorimotor map: Modulating lateral interactions for anticipation and planning",
            "rating": 2,
            "sanitized_title": "a_sensorimotor_map_modulating_lateral_interactions_for_anticipation_and_planning"
        },
        {
            "paper_title": "Merge SOM for temporal data",
            "rating": 1,
            "sanitized_title": "merge_som_for_temporal_data"
        },
        {
            "paper_title": "Intrinsic motivation systems for autonomous mental development",
            "rating": 1,
            "sanitized_title": "intrinsic_motivation_systems_for_autonomous_mental_development"
        },
        {
            "paper_title": "Adaptive ε-greedy exploration in reinforcement learning based on value differences",
            "rating": 2,
            "sanitized_title": "adaptive_εgreedy_exploration_in_reinforcement_learning_based_on_value_differences"
        },
        {
            "paper_title": "Value iteration networks",
            "rating": 1,
            "sanitized_title": "value_iteration_networks"
        }
    ],
    "cost": 0.014391999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts
2022</p>
<p>Chinedu Pascal Ezenkwu 
Andrew Starkey 
An unsupervised autonomous learning framework for goal-directed behaviours in dynamic contexts</p>
<p>Advances in Computational Intelligence
226202210.1007/s43674-022-00037-9Received: 23 August 2021 / Revised: 25 April 2022 / Accepted: 27 April 2022 / Published online: 2 June 2022R E V I E W A R T I C L EAutonomous agent · Planning · Unsupervised learning · Sensorimotor · Artificial intelligence
Due to their dependence on a task-specific reward function, reinforcement learning agents are ineffective at responding to a dynamic goal or environment. This paper seeks to overcome this limitation of traditional reinforcement learning through a task-agnostic, self-organising autonomous agent framework. The proposed algorithm is a hybrid of TMGWR for self-adaptive learning of sensorimotor maps and value iteration for goal-directed planning. TMGWR has been previously demonstrated to overcome the problems associated with competing sensorimotor techniques such SOM, GNG, and GWR; these problems include: difficulty in setting a suitable number of neurons for a task, inflexibility, the inability to cope with non-markovian environments, challenges with noise, and inappropriate representation of sensory observations and actions together. However, the binary sensorimotor-link implementation in the original TMGWR enables catastrophic forgetting when the agent experiences changes in the task and it is therefore not suitable for self-adaptive learning. A new sensorimotor-link update rule is presented in this paper to enable the adaptation of the sensorimotor map to new experiences. This paper has demonstrated that the TMGWR-based algorithm has better sample efficiency than model-free reinforcement learning and better self-adaptivity than both the model-free and the traditional model-based reinforcement learning algorithms. Moreover, the algorithm has been demonstrated to give the lowest overall computational cost when compared to traditional reinforcement learning algorithms.</p>
<p>Introduction</p>
<p>As the scope for robotic applications extends from structured to unstructured and more complex environments, autonomy has become a desideratum for most of today's robots. The practice of handcrafting robots does not give them the capability to cope with unforeseen situations. Although several research contributions have been made towards robot autonomy, we are nowhere near the level of autonomy that is exhibited by animals, even ones at the lowest biological level of organisation. This is because animals are born with innate capabilities, both in their body structure and intelligence, to B Chinedu Pascal Ezenkwu chineduezenkwu@abdn.ac.uk Andrew Starkey a.starkey@abdn.ac.uk 1 School of Engineering, University of Aberdeen, Aberdeen, UK 2 Electrical, Electronic and Computer Engineering, University of Uyo, Uyo, Nigeria survive and develop in their milieus; their behaviours and sometimes their morphological traits can evolve to adapt to persistent changes in their habitats. While it can be argued that highly specialised robots can be developed for any given environment, current approaches in AI do not give the ability for the robot to self-adapt to changes in its environment (or from its simulated world) or to itself (for example through damage to actuators) and this is a significant gap to overcome to produce truly autonomous systems.</p>
<p>An autonomous robot, without any need for an external change of the underlying algorithms, should learn to develop novel skills to cope with unpredictable situations. The desire to attain this level of intelligence has been a major motivation for most sophisticated and popular AI techniques such as deep learning, reinforcement learning (RL), active learning, and imitation learning. Some of these methods either depend on labelled data or require environment-dependent reward functions, making it difficult for them to cope with unknown environments (Irpan 2018;Marcus 2018;Dulac-Arnold et al. 2021). For example, the model-free RL (Sutton and Barto 2018;Bozkurt et al. 2021), although one of the most applied techniques in autonomous agent research with a number of impressive results suffer from sample inefficiency, delayed reward, and the need to design an environment-dependent reward function (Irpan 2018;Sermanet et al. 2016). Moreover, the model-based RL, although it has been proven to be sample-efficient and exhibits good generalisation ability, requires prior knowledge of the environment dynamics which are often not available in many real-world scenarios (Bozkurt et al. 2021;Sutton and Barto 2018). These limitations, however, pose difficulties in applying these methods in unknown or dynamic environments.</p>
<p>Through interactions with its environment, without any need for an environment-dependent reward function and/or predefined transition probabilities as may be required in the conventional RL algorithms, a truly autonomous agent should be able to learn a non-task-specific sensorimotor schema (Tsou 2006;Piaget and Cook 1952;Nguyen et al. 2021), which could later be useful for causal reasoning and planning towards a desired state leveraging rewards or motivations derivable from the sensorimotor space given desire. The derivation of the reward signals in the sensorimotor space can be likened to the activity of dopamine, a brain substrate that has been proven to cause pleasure, satisfaction, or dissatisfaction (Berridge et al. 2009;Liu et al. 2021) in humans and animals. This is a good way to avoid the challenge of defining reward function in the environment space, especially in situations where the environment is unpredictable, inaccessible, or completely unknown.</p>
<p>The preceding statement forms the thesis of this research. This study provides a strategy based on the unsupervised learning paradigm, specifically, the self-organising map (SOM) (Kohonen 1990). However, the standard SOM and its variants have fundamental issues. There is no one selforganising technique that has addressed the following problems: difficulty in setting a suitable number of neurons for a task, inflexibility, the inability to cope with non-markovian environments, challenges with noise, and inappropriate representation of sensory observations and actions together. The purpose of this research is to design an unsupervised, selfadaptive autonomous learning framework based on a variant of self-organising map called temporospatial merge grow when required (TMGWR) network (Ezenkwu and Starkey 2019b). The key contributions of this research include the following: (1) instead of employing a binary sensorimotor-link or lateral connections between nodes, this research provides a strategy that continuously strengthens or weakens a sensorimotor-link according to how reliable the link is; (2) an autonomous learning framework that incorporates value iteration in TMGWR for self-adaptive model-based planning in dynamic contexts has been proposed; and (3) research demonstrates that the proposed method is more flexible to changes in the agent environment than RL agents.</p>
<p>As a sensorimotor map learning algorithm, TMGWR yields a graph model ( or a sensorimotor map) with nodes representing the sensory abstractions and edges representing possible affordances from each node. It equips the agent with some knowledge of the world, making it possible to plan with an informed graph search technique which is more sampleefficient than the model-free RL methods such as Q-learning (Bozkurt et al. 2021;Sutton and Barto 2018). This paper demonstrates how the sensorimotor map learnt using the TMGWR algorithm can be exploited for goal-directedness using value iteration. The proposed method is compared with both the model-free RL (Q-Learning) and the model-based RL agents in terms of their sample efficiencies and their abilities to self-adapt when there is a change in the world or goal state.</p>
<p>The results show that both the TMGWR-based agent and model-based RL agent are far more sample-efficient and adapt faster to changes in goal states. However, in the change of environment scenario, the TMGWR-based agent adapts faster than the model-free RL agent, while the model-based RL agent completely fails to cope with any change in the environment. Although both require environment models to plan, the difference between the TMGWR-based method and the conventional model-based RL is that while the model-based RL requires the prior definition of the environment transition model, the TMGWR-based method learns this transition model. This advantage makes the TMGWR-based method suitable for applications in scenarios where the environment dynamics are not known a priori.</p>
<p>The remaining part of the paper proceeds as follows: Section 2 provides background information on some key concepts. Section 3 presents the conceptual framework of the proposed method with detailed descriptions of its major components. Section 4 compares the proposed method and the RL algorithms. Section 5 explains the experiments. Section 6 presents and discusses the results of the experiments, while Sect. 7 concludes the paper and highlights areas of future research.</p>
<p>Background</p>
<p>In this section, we provide background on machine autonomy, RL, sensorimotor map, and value iteration.</p>
<p>Machine autonomy</p>
<p>There have been many attempts towards creating autonomous systems with the target of achieving human-level performance in different scenarios. These have led to the development of a number of novel state-of-the-art AI techniques in recent years (Vamvoudakis et al. 2015;Saba et al. 2021).</p>
<p>However, these methods are not being implemented against a coherent evaluation framework for assessing the autonomous agent. In an attempt to provide a reasonable definition of machine autonomy, we previously categorised the attributes of autonomous agents into low-level and high-level attributes. This definition builds on a number of other definitions of the autonomous agent in the literature (Ezenkwu and Starkey 2019a).</p>
<p>The low-level attributes are must-have attributes for any autonomous agent as they provide the smallest distinction between the autonomous agents and other automated agents. These attributes include learning, context-awareness, actuation, perception, and decision-making. In contrast, the highlevel attributes are advanced attributes of autonomy, which have proven difficult to achieve using the current AI techniques. They include domain-independence, self-motivation, self-recoverability, and self-identification of goals.</p>
<p>Reinforcement learning</p>
<p>RL is a trial-and-error method of learning policies and planning in Markov decision processes (MDPs) (Belousov et al. 2021). An MDP "consists of states s ∈ S, actions a ∈ A, a reward function R(s, a) and transition probability, P a ss , to each possible next state s given any state s and action a". The goal of a RL agent is to maximise expected long-term discounted rewards over a horizon (episodic or continual) (Szepesvári 2010;Belousov et al. 2021).</p>
<p>Based on an explicitly defined immediate reward of states, R(s), or state-action pairs, R(s, a), all RL algorithms estimate the value functions which give the measure of how good each state (or state-action pair) is for a given task. If the process converges to the optimal value function then a greedy policy following the value function will be the best policy for the task. MDPs can be approached either as model-based or model-free RL. The model-based approach is suitable only when the environment dynamics are well known.</p>
<p>Typical examples of the model-based method are dynamic programming (Bellman 1952) techniques such as value iteration and policy iteration ; the model-free method does not require knowledge of environment dynamics to arrive at the optimal policy, e.g., Q-learning (Ge et al. 2021) and SARSA (Sutton and Barto 2018). Although model-free methods are more popular than model-based methods because of their simplicity, the advantages of modelbased methods over them are that model-based methods are more sample-efficient and have stronger generalisation.</p>
<p>Sensorimotor maps</p>
<p>According to Piaget, human infants are born with innate schemas or reflexes which develop into constructed schemas as they use these reflexes to adapt to their environments (Huitt and Hummel 2003;Hakimzadeh et al. 2021). This idea has influenced numerous research works in developmental AI and autonomous systems. Drescher was the first to formalise Piaget's sensorimotor schema, describing it as a tuple of context, action, and result. This is to enable the prediction of the result if an action is taken in a known context (Drescher 1991;Guerin and Starkey 2009). With a chain of regular schemas, composite actions can be generated and executed to attain a desired state. To improve the efficiency of Drescher's schema mechanism, Chaput (Chaput 2004) proposed the constructivist learning architecture (CLA), an unsupervised hierarchical neural implementation of the schema mechanism.</p>
<p>However, the CLA has a scale-up problem, because it is limited to binary sensory environments. Hierarchical SOMbased Encoding (HSOME) (Pierris and Dahl 2017; Parisi and Wermter 2013) is among the works inspired by the CLA. Although HSOME removes the restriction on the height of the hierarchy of self-organising maps (SOMs) (Kohonen 1990) that are required for learning, its major limitation is that "it produces lossless encoding instead of a principle component representation" (Pierris and Dahl 2017) of the agent's sensory observations.</p>
<p>Due to the preceding limitation and because HSOME uses the standard SOM which requires that its dimension is predetermined during design time, the sizes of the SOMs at different layers should be large enough to represent the agent's world properly. This makes HSOME less suitable for unknown scenarios. Similarly, Toussaint proposed the CWM (Toussaint 2004), which is a monolithic growing self-organising neural network designed to couple sensory observations and motor signals. Toussaint called this the sensorimotor map (Toussaint 2006). The CWM uses the growing variant of SOM to avoid the difficulty that the predetermination of accurate SOM size could pose to sensorimotor map learning in unknown environments.</p>
<p>Since sensorimotor map learning has been proven to be domain-independent or non-task-specific, it meets some of the requirements of the autonomous agent. Other advantages of employing sensorimotor maps are that they can provide data compression by encoding values from a multidimensional vector space into a finite set of values from a discrete subspace of lower dimension. Additionally, sensorimotor maps can provide both forward and backward functions for decision-making as we have demonstrated in this paper. Figure 1a shows our concept of the autonomous learning agent, which will be referred to as the TMGWR-based agent for the remainder of this paper. The framework consists of (a) (b) Fig. 1 a Shows the conceptual framework for the autonomous agent. b Presents the sensorimotor map learning module of the architecture four main modules-the sensorimotor map learning module, the sensory preprocessor, the motivation estimator, and the action selector. The agent is equipped with suitable sensors and actuators which enable it to observe the environment and react to these observations using the actuators. The observations or sensory inputs can be preprocessed or transformed into a form that conveys meaningful or contextual information to the agent. The preprocessed sensory observations are passed on to the sensorimotor map learning module which enables the agent to develop or refine its mental model of the scenario. The sensorimotor map learning occurs continually in an open-ended manner to enable the agent to keep track of changes in the environment by continuously updating the sensorimotor map. The motivation estimator provides the motivation signal that enables the agent to plan towards a goal or behave in a given manner in the environment. The action selector considers the current observation and the agent's motivation in selecting the best action using the sensorimotor map. Execution of this action causes a change in the environment and the cycle continues. Subsequent sections provide detailed explanations of the modules.</p>
<p>Conceptual framework for the proposed method</p>
<p>Sensorimotor map learning</p>
<p>The TMGWR network was used as the sensorimotor map learning method. The key features of the method are that:</p>
<p>• the nodes are linked based on their sensorimotor proximities to one another; • it uses the temporal context of the Merge Grow Neural Gas (MNG) Strickert and Hammer 2005 to keep track of the sensorimotor history; • the GWR method of adding new nodes is employed to enable the system to keep track of changes in the environment; • all the hyperparameters are kept constant throughout the lifetime of the agent to encourage continual learning.</p>
<p>The action map in Fig. 1b learns the codebook vector a for each motor activity, while the sensorimotor map learns the input weight vectors and the possible action vectors linking them to each other. At each time step, the activated action vector on the action map is associated with the sensorimotor-link from the previous winning node i to the current winning node j in the sensorimotor map; this action vector is labelled a i j in Fig. 1b. In the original TMGWR network, the sensorimotor-
link E[i, j] from node i to j is binary i.e. E[i, j] = 1 if E[i, j] is a possible transition, otherwise E[i, j] = 0.
However, in this work, the sensorimotor-link is updated according to Eq. (1)
E[i, j] = E[i, j] + α E ,(1)
where α is the learning rate and E is given by Eq.</p>
<p>(2)
E = sim( a i j , a)I{i j} − E[i, j];(2)
sim( a i j , a) is a similarity function that compares the activated action vector, a at a given time with the action vector, and a i j that has already been associated with the sensorimotor-link from node i to node j. We chose sim( a i j , a) to be a Gaussian function, so that if a i j a, then sim( a i j , a) tends towards 1; otherwise, it will tend towards 0. The advantage of introducing sim( a i j , a) in the E[i, j] update equation is that it increases the weight of the sensorimotor-link if the same action vector results in the same transition all the time and decreases it if the transition is possible with different action vectors. This is useful during planning as the agent is more likely to select reliable actions for each experience in the environment. I{i j} is an indicator function which returns 1 if the previous winning node i is not the same as current winning node j; otherwise, it returns 0. I{i j} encourages only actions that result in progress and penalises those that compromise that progress.</p>
<p>Sensory preprocessor</p>
<p>The real world is often characterised by noisy, ambiguous, and/or multidimensional sensor signals. As such, an autonomous agent requires a preprocessor for extracting relevant information for learning and decision-making. The TMGWR network has been shown to inherently handle noisy, ambiguous, and partially observable environments due to its recurrent architecture and the GWR technique of expanding the sensorimotor map. While this has been proven in a 2D feature space, we argue that advanced preprocessing techniques could be required to complement the TMGWR network in a high-dimensional sensor space such as a vision-based scenario. For instance, a pre-trained convolutional feature extractor (Yosinski et al. 2014;Razavian et al. 2014) could be useful in the preprocessor module for feature extraction when the agent's observations are image frames. We intend to investigate the preceding example as well as methods for relevant feature selection (Wang et al. 2019) as the sensory preprocessor in our subsequent research.</p>
<p>Motivation estimator</p>
<p>The motivation estimator assigns a motivation potential to each node on the sensorimotor map. The computation of these motivation potentials can be based on intrinsic (Oudeyer et al. 2007;Oudeyer and Kaplan 2009;Schmidhuber 2006) and/or extrinsic motivation. While intrinsic motivation can enable an agent to engage in informed exploration of its environment, extrinsic motivation is useful for goal-directedness. In this paper, attention has been paid only to the goal-directed context. However, it would be interesting to examine how the habituation mechanism that is associated with the traditional GWR could be leveraged for intrinsic motivation or informed exploration. The motivation estimator in this paper is the model-based value iteration. The value iteration is a dynamic programming technique for solving Markov Decision Processes (MDPs) (Tamar et al. 2016). It iteratively updates the value function until convergence following Bellman's equation (Dai and Goldsmith 2007) as shown in Eq. 3 (Sutton and Barto 2018):
V π (s) = a π(a, s) s p a ss [r a ss + γ V π (s )],(3)
∀s ∈ S, where, V π (s) is the value of state s under a policy π ; π(a, s) is the probability of taking action a while in state s; p a ss is the transition probability to state s when action a is executed in the state s; r a ss is the expected reward for the transition; and γ is the discount factor. The model-based value iteration is possible only when the transition probability is known. The sensorimotor map learning method tries to learn the transition model for a given scenario.</p>
<p>We will now discuss how the value iteration is applied in this paper. Given a goal state, the associated goal node g can be activated on the sensorimotor map. Equation 4 shows the update rule for the propagation of the motivation potential, V (.) with respect to g for each node on the sensorimotor map
V (i) = V (i) + β V (i), ∀i ∈ N ,(4)
where N is a set of all nodes in the sensorimotor map and β is the learning rate. V is given by Eq. 5
V (i) = sim(i, g) + γ max{E[i, k]V (k)} − V (i)(5)
∀k ∈ K , where K is a set of all nodes in the sensorimotor neighbourhood of i; these are nodes that can be reached from i due to actions that are possible in i. sim (i, g) is the Gaussian similarity between node i and the goal node g. sim(i, g) is used as the immediate reward. γ is the discount factor. Through this iterative update, higher motivation potentials are propagated to nodes that are closer to g, if they also provide reliable sensorimotor-links for the agent to reach the goal state. This has been summarised in Algorithm 1. The most expensive parts of the algorithm are the sensorimotor map learning using TMGWR and the value iteration process. TMGWR learns to map M data points to N nodes. Unlike in SOM where the number of nodes is predefined, TMGWR adds new nodes to minimise the quantisation error. Iterating through M data points while mapping them to N will cost O(N × M). In the worst-case scenario, TMGWR will require N (= M) nodes to correctly represent M data points. Hence, the time complexity of TMGWR is O(N 2 ).</p>
<p>Value iteration sweeps through the entire state space, while considering all possible actions to converge. In the worst-case scenario, the time complexity of the goal-directed algorithm is O(N 2 + N × |A|), where A is the action set. However, TMGWR uses forgetting strategy to delete aged sensorimotor-links and redundant nodes. TMGWR demonstrates to be a more efficient sensorimotor map learning algorithm than growing neural gas (GNG), grow when required network (GWR), and time grow neural gas (TGNG) (Ezenkwu and Starkey 2019b).</p>
<p>Action selector</p>
<p>The action selector leverages the motivation potentials computed by the motivation estimator to generate a policy that enables the agent to select actions that can transition the agent in the direction of increasing motivation potentials. The selected action vector a i when node i is the activated node on the sensorimotor map is given by Eq. 6 a i = a ik max , where k max = argmax k V (k), ∀k ∈ K . K is a set of all nodes in the sensorimotor neighbourhood of i and V (k) is the motivation potential of node k, ∀k ∈ K .</p>
<p>This equation means that the agent selects an action that will place it in that node in the neighbourhood of the current node that has the maximum motivation potential. At first glance, it is possible to think that the agent is short-sighted in its action selection. However, the motivation potential of each node is a function of the sensorimotor neighbourhood of the node. This ensures that the agent's decisions are influenced by its long-term satisfaction. Table 1 summarises the key differences between the proposed method and the conventional RL algorithms. In the model-based RL algorithm, the environment dynamics must be well understood and formulated for the algorithm to work. This is a huge limitation to the applications of the model-based RL algorithms in unstructured or unpredictable real-world environments. Unlike the model-based RL methods, the model-free RL algorithms do not require knowledge of environment dynamics to work, and as a result, they are the most applied RL algorithms in the literature. However, the absence of environment models in the model-free RL algorithms results in their high sample complexity and poor generalisation. The TMGWR-based method does not require prior design of the environment dynamics, since the agent is capable of learning it during exploration, and because it uses the environment model for planning, it exhibits good sample efficiency and generalisation as the model-based RL.</p>
<p>For an RL agent to learn to plan towards a given goal, a reward function is formulated to represent the relationships between the goal and the environment states or state-action pairs. In the model-based RL methods, reward functions, R(s), are dependent only on environment states, while in the model-free RL algorithms, reward functions R(s, a) are dependent on state-action pairs, i.e., environment states and actions. Since each of these methods require that their reward functions factor in environment states in addition to goal, extra environment information is required to formulate suitable reward functions for these methods. This will pose some difficulties in unknown or unstructured environments. However, given a goal, the TMGWR-based method does not require that the environment states are well understood for planning, since the reward values are computed in the sensorimotor domain using the Gaussian similarity metric which depends on the agent's own mental model of the environment and not on the actual environment states. This completely removes the difficulty of having to understand the environment to formulate a reward function for a given task.</p>
<p>Experiments</p>
<p>Setup</p>
<p>The environment used in this paper is a 13X 17 maze (Fig. 2). Solid areas in the environment represent walls, while empty areas represent passages. The agent senses the coordinates of its current location as the sensory observations. Each state in the environment can afford the agent up to four actions: move-up, move-down, move-right, and move-left. It moves one step in the direction of the selected action if there is no obstacle; otherwise, its state remains unchanged. The same random policy applies throughout the experiment. During random exploration, the agent performs a random walk that chooses a new action with the probability of 30% if no obsta- cle is in the direction of the selected action; otherwise, it selects an arbitrary action from any of the other three actions at 100% probability. The agent begins its life-cycle with complete random exploration as described above. This random exploration reduces according to Eq. 10; each time, the agent achieves the goal after it has previously attained the goal for up to five tries. Adaptive or decayed -greedy exploration (Tokic 2010;Maroti 2019) is a well-known method for handling exploration-exploitation dilemma in RL (Sutton and Barto 2018)
= max( − 1 10 , min );(7)
is the probability that the agent will select an action based on random policy and min is the minimum value that can take. In the experiment, is initialised to 1 (i.e., 100% random exploration) and decreases each time the agent achieves the goal until it attains the minimum ( min = 0.1). Each experiment ran for ten trials.</p>
<p>The experiments compared three types of agent:</p>
<p>• The TMGWR-based agent (the proposed agent) • A model-free RL agent (in this case, the Q-learning agent)</p>
<p>• The model-based RL agent using value iteration algorithm.</p>
<p>The reward function for the model-free RL agent is defined in Eq. 8. For the model-based RL agent, the reward function is defined as shown in Eq. 9. These immediate rewards were treated as hyperparameters and were selected by Bayesian optimisation (Pelikan et al. 1999;Grosnit et al. 2021 </p>
<p>While the reward functions for the RL agents are provided in the environment space, the TMGWR-based agent when given a goal computes its rewards or motivations in the sensorimotor space. This gives the method the potential to overcome the difficulty of designing reward functions for unknown environments as is the case with the traditional RL. Given the goal state, the associated goal node, g. can be determined from the sensorimotor map. The TMGWR-based agent derives the desirability of having each node activated based on their proximity to g. This is defined in Eq. 10 as follows:
r (i) = sim(i, g) , ∀i ∈ N .(10)
The reward function is used to derive the motivation potential for each node as seen in Eq. 5. For the model-based RL agent, we defined the transition dynamics, T (s, a, s ), as a deterministic model. T (s, a, s ) is the probability of the agent going into s if action a is taken in state s. Throughout the experiment, the transition probability is hard-coded for a given arrangement of obstacles in the environment. However, this is not allowed to change automatically when the environment changes unpredictably T (s, a, s ) = 1, if s' contains no obstacle 0, if if s' contains an obstacle.</p>
<p>Descriptions of experiments</p>
<p>The TMGWR-based goal-directed agent, the traditional model-based RL agent, and a model-free RL agent (in this case, the Q-learning agent) are compared in three scenarios: changing goal state, changing environment, and sample complexity. While the first scenario illustrates their abilities to self-adapt when the goal changes, the second demonstrates their ability to cope when the domain knowledge about the environment no longer applies, and the third compares their sample efficiencies. The following sections describe each of these scenarios in detail.</p>
<p>Scenario II: changing goal state</p>
<p>This scenario investigates the ability of the agents to generalise when the task presents a goal other than the one used during the agents' training. The behaviour of each of the algorithms in the changing goal scenario has been simulated. Figure 3 shows four selected goal locations, G 1 , ..., G 4 . Each agent is first trained for 1000 iterations with the goal at G 1 . After every 1000 iterations, the goal locations change in the sequence G 1 → G 2 → G 3 → G 4 → G 1 . Each iteration terminates when the agent visits either the current goal state or the previous goal state. For instance, if the goal is at G 1 and Fig. 3 Selected goal locations. G 1 , ..., G 4 are the selected goal locations, while S is the starting point of the agents. This applies to scenarios I and II later moves to G 2 , then any visit to G 1 is a visit to the previous goal state and any visit to G 2 is a visit to the current goal state. For the RL agent, the previous goal state has the same reward as any other empty cell in the environment except that it is also a termination point. The number of times the current goal state and the previous goal state are visited beginning at the starting point are recorded separately for each 1000 iterations. We carefully chose the goal locations to reduce the possibility of any of the agents visiting the current goal state with the minimum exploration probability, min = 0.1 for some consecutive goal states. For example, G 1 and G 2 are the closest consecutive goal locations, while G 3 and G 4 are the farthest. While it is likely that the agent can visit G 2 instead of G 1 due to the minimum exploration probability, min = 0.1, it is less likely that the agent will visit G 4 instead of G 3 due to min .</p>
<p>Scenario II: changing environment</p>
<p>This scenario examines the abilities of the agents to self-adapt if there is a slight change in the environment. The agents are trained on a goal until they learn the optimal path to that goal; then, an obstacle is introduced along this optimal path. For fair investigation of the self-adaptivity of the agents, we assume that the change in the environment is unpredictable and is not captured for whatever reason in the design of the agents. For example, the transition model for the modelbased RL agent is modelled for the original environment and not updated when the environment changes. A self-adaptive agent will be able to adapt its world model and identify the best possible alternative to the goal. After the optimal path is blocked, the number of steps it took the agents to reach the goal for the first time using an alternative path is recorded. Figure 4 illustrates the two case studies for this scenario. The arrows in Fig. 4a, c show the optimal paths to the goals in the two case studies, while Fig. 4b, d shows the positions of the obstacles (indicated with red circles), respectively. For reference purpose, we refer to the two contexts as cases A and B, respectively.</p>
<p>Scenario III: Computational cost</p>
<p>This scenario evaluates and compares the computational costs of the algorithms in terms of their sample complexity and amount of CPU time required by each them during training, self-adaptation, and replanning to cope with change in the environment.</p>
<p>Sample complexity</p>
<p>This experiment compares the amount of samples each of the algorithms requires to learn the optimal policy for a given task. Starting from S, the sample efficiencies of the agents are investigated considering each of the four goal locations G 1 , ..., G 4 , as shown in Fig. 3.</p>
<p>At each iteration, each agent selects actions at each time step based on the prevailing policy until the goal is achieved. Each iteration terminates when the agent successfully visits the goal position. Initially, the agents are 100% exploratory, but they become more deterministic as they learn to achieve the goal. This has been explained in Sect. 5.1.</p>
<p>The convergence rate is a quantity of interest in estimating the sample complexity of goal-directed agents (Kakade et al. 2003) such as the RL agent. As such, this scenario considers the convergence rates of the algorithms as a measure of their sample complexities.</p>
<p>As presented in our previous paper, TMGWR optimises the sensorimotor map building process using the minimal number of nodes that best represent an environment (Ezenkwu and Starkey 2019b). Fewer nodes contribute to efficient convergence during planning, because the agent will not waste time exploring unnecessary nodes. For the sake of self-recoverability, when a condition in an agent's environment changes, efficient sample complexity is of the essence in enabling the agent to learn faster and respond to the changes in real-time.</p>
<p>Time-based comparisons of the algorithms</p>
<p>The times required by each of the algorithms for training, for self-adaptivity when the goal changes, and for adjustments to changing environments have been recorded and compared in this experiment.</p>
<p>The algorithms are run on Python version 3.8 on a 64-bit Windows 10 computer with Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz 3.20GHz and 16.0GB (15.9GB usable) RAM. For evaluating the times required for training the algorithms until convergence, the environment of Fig. 2 is used for training each algorithm. The time between the start of the training and the time the agent consistently follows the optimal path ten consecutive times is recorded for each algorithm.</p>
<p>To evaluate the times the algorithms self-adapt to a new goal, the algorithms are trained to learn the optimal path with the goal at G 4 as indicated in the environment of Fig. 3, and then, the goal is repositioned to G 3 (also see Fig. 3). The time it takes the algorithm to adapt to execute the optimal path to the new goal location is recorded for each algorithm. Figure 4a, b has been useful in evaluating the response time of each algorithm to a change in the environment. Figure 4a demonstrates the optimal path as identified by the algorithm after training, whereas Fig. 4b shows an obstacle blocking this optimal path. Using these two configurations, the time it takes each algorithm to access the goal state through an alternative path after the initial optimal path is blocked is recorded and reported for the algorithm.</p>
<p>In each experiment, ten different trials are executed and results are presented as bar charts.</p>
<p>Hyperparameters</p>
<p>The hyperparameters for the three methods were selected by the Bayesian optimisation (Pelikan et al. 1999;Grosnit et al. 2021) of their convergence rates in attaining the goal G 1 starting from S. The hyperopt library in Python 1 was used with maximum number of evaluations set to 150. The algorithms were optimised independently in the environment of Fig. 2 following the exploration policy described above. 1 http://hyperopt.github.io/hyperopt/ The TMGWR network has been previously optimised in a similar environment in our previous work (Ezenkwu and Starkey 2019b). Moreover, in the TMGWR-based agent, the sensorimotor map learning algorithm is loosely coupled with the planning algorithm. Due to the following reasons, it is easy to transfer hyperparameters from the original TMGWR network to the version applied in this paper by freezing the hyperparameters that are connected to the sensorimotor map learning while optimising those for the value iteration and the sensorimotor-link adaptivity. Table 2 presents the hyperparameters used in the paper; γ is the discount factor, β is the learning rate for the value iteration and the Model-free RL, α is the learning rate for the sensorimotor-link adaptation equation, and r (s) is the immediate reward for state s where s can be an empty cell, a cell with wall, or the goal state.</p>
<p>Results and discussion</p>
<p>Scenario I: changing goal state</p>
<p>Tables 3, 4, and 5 present the responses of the model-free RL agent, the TMGWR-based agent, and the model-based RL agent to changes in goals, respectively. As described in Sect. 5.2.1, this scenario compares the generalisation abilities of the three agents when the goal state changes. Apart 
r (s = wall) - −131 - r (s = empt y) - −29 −18 r (s = goal) - 8 8 7 5
from the change from G 1 to G 2 where the model-free RL agent visited the current goal state up to 89.26% of the time, it spent over 99% of the time visiting the previous goal states for the other change in goal states. For G 1 → G 2 , the modelfree RL agent was likely to encounter the current goal state with the minimum exploration due to the close proximity between G 1 and G 2 . Moreover, the model-free RL agent did not need to relearn every aspect of its model of the task to be able to attain G 2 . For example, the value function that can drive the agent to the bottom of the maze is still useful for getting it to the G 2 with slight update. Contrarily, the other changes in goal states require that a large part of the agent's model of the task is updated. This update is slow due to low exploration probability making it spend more time exploiting the previous goal state before learning to attain the current goal. Compared to the model-free RL agent, the TMGWRbased agent and the model-based RL agent quickly respond correctly to all changes in goal states. This is because contrary to the model-free RL agent, the TMGWR-based and the model-based RL agents plan with some knowledge of the environment and are therefore, able to quickly replan to attain any goal state within the environment. However, while the TMGWR-based agent replans on the sensorimotor map using a learnt environment model, the model-based RL agent replans using a defined environment-dependent transition model. Short demonstrations of this scenario with the Model-free RL 2 , TMGWR-based 3 , and the Model-based RL 4 agents are available on YouTube for view.</p>
<p>Scenario II: changing environment</p>
<p>The results of this scenario have been summarised in Table 6 and Fig. 5. In the two cases, the model-free RL agent took a greater number of steps to reach the goal than the TMGWR-<br />
G 1 → G 2 0 ± 0 100 ± 0 G 2 → G 3 0 ± 0 100 ± 0 G 3 → G 4 0 ± 0 100 ± 0 G 4 → G 1 0 ± 0 100 ± 0G 1 → G 2 0 ± 0 100 ± 0 G 2 → G 3 0 ± 0 100 ± 0 G 3 → G 4 0 ± 0 100 ± 0 G 4 → G 1 0 ± 0 100 ± 0
based agent after an obstacle is introduced along the optimal path. This is because the model-free RL agent tries to learn the optimal path to attaining a specific goal. As such, it has to relearn its value function if the environment changes in a way that does not avail it of that path to the goal. Although they are complementary, for the TMGWR-based agent, the environment modelling is separate from the planning mechanism. While the sensorimotor map learning technique is responsible for the environment modelling, the value iteration does the planning. The TMGWR algorithm continually adapts the sensorimotor map to reflect changes in the agent's world at every time stamp. This enables the TMGWR-based agent to keep track of the changes in the environment significantly more quickly than the model-free RL agent. With the correct sensorimotor map, a sample-efficient model-based value iteration algorithm can help to propagate the motivation potentials on the sensorimotor map in such a way that enables the agent to desire an alternative path to the current goal. The model-based RL agent showed the worst performance in this scenario. This is because it requires an environmentdependent transition model, and unlike the TMGWR-based  Figure 6 displays the results of the experiment of scenario I. The graphs compare the sample efficiencies of the three methods considering the four goal locations. The number of steps for each agent to attain the goal locations are recorded at every 5th iteration. Figure 6 shows that the TMGWR-based agent and the traditional modelbased RL agent converge to the optimal -greedy policy faster than the model-free RL agent for the four goal locations.</p>
<p>This advantage of the TMGWR-based agent and the traditional model-based RL agent is derived from their model-based attribute. While the model-free RL agent tries to learn the decision model for the task without the knowledge of the environment dynamics, the model-based RL agent uses the transition dynamic defined in Eq. 11 to plan towards attaining the goal in a more sample-efficient manner. Similar to the model-based RL agent, the TMGWR-based agent also shows a sample-efficient planning but using a learned and self-adaptive transition model of the environment. Figure 7 demonstrates how much time is required by each algorithm for training, for self-adaptivity when goal changes, and to adjust to changing environments.</p>
<p>Time-based comparisons of the algorithms</p>
<p>Although the model-free RL agent requires a higher number of explorations during training, Fig. 7a shows that it requires a minimal amount of training time compared to the TMGWR-based agent and the traditional model-based agent. This minimal training time is because the model-free RL agent is lightweight and involves fewer computations than the other two methods, which repetitively utilise the environment model during planning. The effect of this lack of task model by the model-free RL is that any change in goal state requires a complete overwriting of the existing knowledge of the task that the agent has acquired so far. This demonstrates why in Fig. 7b the model-free RL agent takes a relatively long time, when compared to the other two agents, to adapt in solving the task if the goal changes.</p>
<p>Each step in the TMGWR-based algorithm's learning process involves the identification of the best matching unit, the adaptation of the map in response to the current context, the computation of the motivation potentials, and then value iteration for planning. These computations slow down the training of the algorithm, although they will prove to be useful as will be shown later in this section. The model-based RL equally has a quicker training time than the TMGWR-based algorithm, because the environment model and the rewards are hard-coded. Therefore, unlike in the TMGWR-based agent, model-based RL does not have to learn any model of the environment or compute any motivation potentials. The aforesaid makes it have a faster training time than the TMGWR-based agent.</p>
<p>However, one disadvantage is that it is rigid and does not adapt to changes in the environment as can be seen in  7c. The model-based RL stays trapped forever when the environment dynamics change in a manner not captured by the experimenter-because of this, no time is recorded for it in this experiment. While the model-free agent manages to find an alternative path to the goal after the optimal path has been blocked, it does that more than three times slower than the TMGWR-based agent. This is because while model-free RL needs to relearn every aspect of its knowledge, the TMWGRbased agent only has to adapt an aspect of its sensorimotor map that associates with the current experience to correctly solve the task. Figure 7d summarises the times taken by each of the algorithms in the three scenarios. The result demonstrates that the TMGWR-based agent has the lowest average time. The model-based agent has no value recorded for it, because it takes infinity to recover from a change in the environment.</p>
<p>Conclusions</p>
<p>This paper proposes an autonomous agent architecture that employs the TMGWR network for continuous sensorimotor map learning and uses value iteration to plan to attain a goal using the sensorimotor map. The proposed method computes rewards or motivation potentials in the sensorimotor space and learns the transition model of its environment using the TMGWR network as the sensorimotor map learning algorithm. For the agent to be goal-directed, value iteration helps to propagate the motivation potentials associated with the goal in the sensorimotor space. A modification has been made to the original TMGWR algorithm to enable the agent to encode reliability and discourage the selections of actions that can compromise its progress.</p>
<p>The model-free RL, traditional model-based RL, and the TMGWR-based agents have been evaluated on the basis (c) Time required for each algorithm to successfully respond to a change in the environment. Note: There is no value for the model-based agent because it never recovered from a change in the environment.</p>
<p>(d) Average of the times for the three scenarios.</p>
<p>Fig. 7</p>
<p>Time-based comparisons of the algorithms of their abilities to self-adapt when the goal changes, to cope when the domain knowledge about the environment no longer applies, and their computational efficiencies. Since the traditional model-based RL requires that the environment dynamics be hard-coded, it fails to recover from a sudden change in the environment unlike the TMGWR-based agent and the model-free RL agent. This is because the sensorimotor map learning potential of the TMGWR-based agent enables it to keep track of changes in the environment, while the model-free agent can also adapt by modifying its stateaction space. On the contrary, the model-free agent does not always adapt to a change in goal state, often revisiting the previous goal, whereas TMGWR and model-based RL are both able to avoid previous goal states and adapt immediately to the new goal. The experiments demonstrate that only the TMGWR-based agent has the potential to self-adapt efficiently in dynamic contexts, for both a change in goal or a change in environment.</p>
<p>Furthermore, the experiments examined the computational efficiencies of the algorithms-both their sample complexities and the time-based evaluations of their training and self-adaptivity, for a change in goal state or a change in domain knowledge. The results demonstrate that the TMGWR-based agent has a similar sample complexity to the traditional model-based RL agent, and significantly better sample complexity than the model-free RL agent. However, the model-free agent has a quicker training time than the other two algorithms, because it is lightweight and does not factor in the model of the environment during its learning, an attribute that impacts its ability to self-adapt and cope efficiently with a change in the environment. The model-based RL agent cannot adapt at all to a change in the environment, because it depends on hard-coded domain knowledge and needs that it be manually modified to suit the current domain requirements. The experiments demonstrate that only the TMGWR-based agent has the potential to self-adapt efficiently in dynamic contexts, for both a change in goal or a change in environment and to do so with the lowest overall computational cost. tation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/.</p>
<p>Fig. 2
2Maze with agent, i.e., rat (top left corner) and goal, i.e., food (bottom right corner)</p>
<p>Fig. 4
4Changing environments (a) CASE A: optimal path (b) CASE A: Obstacle's position on the optimal path (c) CASE B: optimal path (d) CASE B: Obstacle's position on the optimal path</p>
<p>Fig. (a) Convergence rates of the algorithms with goal at G1. (c) Convergence rates of the algorithms with goal at G3. (b) Convergence rates of the algorithms with goal at G2. (d) Convergence rates of the algorithms with goal at G4.</p>
<p>Fig. 6
6Result of scenario I: sample complexity</p>
<p>( a )
aTimes required for training each algorithm. (b) Time required for each algorithm to adapt to a new goal state.</p>
<p>Table 1
1Differences between TMGWR-based algorithm vs RL algorithms SM.Map(x g ) identify the goal node g given a goal state x g 3: R i = sim(i,g) ∀i ∈ {1,...,SM.nodes.Size()} compute the similarity between each node i in the SM and the goal node gAttribute 
Model-free RL agent 
Model-based RL agent 
TMGWR-based agent </p>
<p>environment model 
not required 
must be provided by the designer 
learnt from experience </p>
<p>reward function 
defined in the state-action space 
defined in the state space 
defined in the sensorimotor space </p>
<p>Algorithm 1 The motivation estimator for the TMGWR-
based goal-directed agent </p>
<p>1: SM ← T MGW R({x j , a j } M 
j=1 ) training sensorimotor map SM 
using TMGWR 
2: g ← 4: E R = [ ] 
define an empty array 
5: V = [0]× SM.nodes.Size() initialise the motivation potential of 
each node in SM to 0 
6: repeat 
7: 
for node in SM.nodes do 
8: 
for k in node.neighbourhood() do 
9: 
E R.add(E[node, k] × R[k]) 
multiply the value 
of each neighbour of the node with the lateral connectivity between 
the neighbour and the node 
10: 
end for 
11: 
V [node] = V [node] + β.(R[node] + γ.max{(E R).V [k] − 
V [node]}) 
12: 
E R = [ ] 
13: 
end for 
14: until Convergence </p>
<p>in state s leads to goal −131, if action a in state s leads to wall −29, if s action a in state s leads to empty if state s is empty.) of the 
convergence rate (see Sect. 5.3) </p>
<p>r (s, a) = </p>
<p>⎧ 
⎪ ⎨ </p>
<p>⎪ ⎩ </p>
<p>88, 
if action a (8) </p>
<p>r (s) = 
75, 
if state s contains goal 
−18, </p>
<p>Table 2 Hyperparameters
2Hyperparameter 
TMGWR-
based agent </p>
<p>Model-free 
RL agent </p>
<p>Model-based 
RL agent </p>
<p>γ 
0.97 
0.86 
0.94 
β 
0.80 
0.39 
0.23 
α 
0.52 
-
-</p>
<p>Table 3
3Response of the model-free RL agent to change in goal state 
for four selected goal locations, G 1 , ..., G 4 </p>
<p>Change 
of 
goal state </p>
<p>%number of vis-
its to previous 
goal </p>
<p>% number of vis-
its to current goal </p>
<p>G 1 → G 2 
10.64 ± 16.16 
89.36 ± 16.16 </p>
<p>G 2 → G 3 
99.40 ± 0.32 
0.60 ± 0.32 </p>
<p>G 3 → G 4 
99.78 ± 0.04 
0.22 ± 0.04 </p>
<p>G 4 → G 1 
99.58 ± 0.19 
0.42 ± 0.19 </p>
<p>Table 4 Response of the TMGWR-based agent to change in goal state 
for four selected goal locations, G 1 , ..., G 4 </p>
<p>Change of 
goal state </p>
<p>% number of visits 
to previous goal </p>
<p>% number of vis-
its to current goal </p>
<p>Table 5
5Response of the model-based RL agent to change in goal state for four selected goal locations, G 1 , ..., G 4Change of 
goal state </p>
<p>% number of visits to 
previous goal </p>
<p>% number of vis-
its to current goal </p>
<p>Table 6
6agent, it neither learn its transition model nor adapt it to changes in the environment unless this changes are known a priori and are accounted for during the design. Because of this limitation of the model-based RL agent, it get stuck in the original path to the goal when the environment changes in the two cases (see short demonstration for the responses of the Model-free RL 5 , TMGWR-based 6 and model-based RL 7 agents in the changing environment scenarios)Average number of 
steps it took each agent to attain 
the goal through an alternative 
path after the optimal path was 
blocked </p>
<p>Case 
TMGWR-based agent 
Model-free RL agent 
Model-based RL agent </p>
<p>A 
112.6 ± 2.0 
1212.2 ± 12.9 
∞ </p>
<p>B 
106.3 ± 1.6 
450.4 ± 4.9 
∞ </p>
<p>Fig. 5 Average number of steps it took each agent to attain the goal 
through an alternative path after the optimal path was blocked. Values 
for the model-based Rl agent are not shown in the graph, because they 
are ∞ in the two cases </p>
<p>6.2.1 Scenario III: computational cost </p>
<p>6.2.2 Sample complexity </p>
<p>Comparing the TMGWR-based algorithm and the RL algorithms
Demonstration: response of the model-free RL agent to change in goal state: https://www.youtube.com/watch?v=_j0z6B1RFjs 3 Demonstration: response of the TMGWR-based agent to change in goal state: https://www.youtube.com/watch?v=x9U0r-6Sct0 4 Demonstration: response of model-based RL agent to change in goal state: https://youtu.be/4GNbxYvJPhM
Demonstration: response of the model-free RL agent to change in the environment: https://youtu.be/aRr4Ja9TspQ 6 Demonstration: response of the TMGWR-based agent to change in the environment: https://youtu.be/-YpxGEjRoXA 7 Demonstration: response of model-based RL agent to change in the environment: https://www.youtube.com/watch?v=peEYriVEK2k
Acknowledgements This work is funded by the Tertiary Education Trust Fund (TETFund) scheme of the Federal Republic of Nigeria.Data availabilityThe datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.DeclarationsConflict of interestThe authors declare that they have no conflict of interest.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adap-
On the theory of dynamic programming. R Bellman, Proc Natl Acad Sci. 38716Bellman R (1952) On the theory of dynamic programming. Proc Natl Acad Sci USA 38:716</p>
<p>Dissecting components of reward:'liking",wanting', and learning. B Belousov, H Abdulsamad, P Klink, S Parisi, J Peters, Curr Opin Pharmacol. 9SpringerReinforcement learning algorithms: analysis and applicationsBelousov B, Abdulsamad H, Klink P, Parisi S, Peters J (2021) Reinforce- ment learning algorithms: analysis and applications. Springer, New York Berridge KC, Robinson TE, Aldridge JW (2009) Dissecting com- ponents of reward:'liking",wanting', and learning. Curr Opin Pharmacol 9:65-73</p>
<p>Model-free reinforcement learning for stochastic games with linear temporal logic objectives. A K Bozkurt, Y Wang, M M Zavlanos, M Pajic, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEEBozkurt AK, Wang Y, Zavlanos MM, Pajic M (2021) Model-free rein- forcement learning for stochastic games with linear temporal logic objectives. In: 2021 IEEE International Conference on Robotics and Automation (ICRA), IEEE, pp 10649-10655</p>
<p>The constructivist learning architecture: A model of cognitive development for robust autonomous robots. H Chaput, Ph.D. thesisChaput H.H (2004) The constructivist learning architecture: A model of cognitive development for robust autonomous robots. Ph.D. thesis</p>
<p>Topological value iteration algorithm for markov decision processes. P Dai, J Goldsmith, IJCAI. Dai P, Goldsmith J (2007) Topological value iteration algorithm for markov decision processes. In: IJCAI, pp 1860-1865</p>
<p>Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Gl ; Drescher, G Arnold, N Levine, D J Mankowitz, J Li, C Paduraru, S Gowal, T Hester, Machine Learning. MIT Press DulacMade-up minds: a constructivist approach to artificial intelligenceDrescher GL (1991) Made-up minds: a constructivist approach to arti- ficial intelligence. MIT Press Dulac-Arnold G, Levine N, Mankowitz DJ, Li J, Paduraru C, Gowal S, Hester T (2021) Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Machine Learning, pp 1-50</p>
<p>Machine autonomy: Definition, approaches, challenges and research gaps. C P Ezenkwu, A Starkey, Intelligent Computing-Proceedings of the Computing Conference. New YorkSpringerEzenkwu C.P, Starkey A (2019a). Machine autonomy: Defini- tion, approaches, challenges and research gaps, in: Intelligent Computing-Proceedings of the Computing Conference, Springer, New York, pp 335-358</p>
<p>Unsupervised temporospatial neural architecture for sensorimotor map learning. C P Ezenkwu, A Starkey, IEEE transactions on cognitive and developmental systems. Ezenkwu CP, Starkey A (2019b) Unsupervised temporospatial neural architecture for sensorimotor map learning. In: IEEE transactions on cognitive and developmental systems</p>
<p>Q-learning based flexible task scheduling in a global view for the internet of things. J Ge, B Liu, T Wang, Q Yang, A Liu, A Li, Trans Emerg Telecommun Technol. 324111Ge J, Liu B, Wang T, Yang Q, Liu A, Li A (2021) Q-learning based flexible task scheduling in a global view for the internet of things. Trans Emerg Telecommun Technol 32:e4111</p>
<p>Are we forgetting about compositional optimisers in bayesian optimisation?. A Grosnit, A I Cowen-Rivers, R Tutunov, R R Griffiths, J Wang, H Bou-Ammar, J Mach Learn Res. 22Grosnit A, Cowen-Rivers AI, Tutunov R, Griffiths RR, Wang J, Bou-Ammar H (2021) Are we forgetting about compositional opti- misers in bayesian optimisation? J Mach Learn Res 22:1-78</p>
<p>Applying the schema mechanism in continuous domains. F Guerin, A Starkey, Proceedings of the Ninth International Conference on Epigenetic Robotics. the Ninth International Conference on Epigenetic RoboticsGuerin F, Starkey A (2009) Applying the schema mechanism in continuous domains. In: Proceedings of the Ninth International Conference on Epigenetic Robotics, pp 57-64</p>
<p>Interpretable reinforcement learning inspired by piaget's theory of cognitive development. A Hakimzadeh, Y Xue, P Setoodeh, arXiv:2102.00572Hakimzadeh A, Xue Y, Setoodeh P (2021) Interpretable reinforcement learning inspired by piaget's theory of cognitive development. arXiv:2102.00572</p>
<p>Piaget's theory of cognitive development. W Huitt, J Hummel, Educ Psychol Interact. 3Huitt W, Hummel J (2003) Piaget's theory of cognitive development. Educ Psychol Interact 3:1-5</p>
<p>Deep reinforcement learning doesn't work yet. A Irpan, Online (Feb. 14Irpan A (2018) Deep reinforcement learning doesn't work yet. Online (Feb. 14): https://www.alexirpan.com/2018/02/14/rl-hard.html</p>
<p>On the sample complexity of reinforcement learning. S M Kakade, University of London London, EnglandPh.D. thesisKakade SM, et al., (2003) On the sample complexity of reinforcement learning. Ph.D. thesis. University of London London, England</p>
<p>The self-organizing map. T Kohonen, Proc IEEE. 78Kohonen T (1990) The self-organizing map. Proc IEEE 78:1464-1480</p>
<p>Spatial and temporal scales of dopamine transmission. C Liu, P Goel, P S Kaeser, Nat Rev Neurosci. 22Liu C, Goel P, Kaeser PS (2021) Spatial and temporal scales of dopamine transmission. Nat Rev Neurosci 22:345-358</p>
<p>Deep learning: A critical appraisal. G Marcus, arXiv:1801.00631arXiv preprintMarcus G (2018) Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631</p>
<p>Rbed: Reward based epsilon decay. A Maroti, arXiv:1910.13701arXiv preprintMaroti A (2019) Rbed: Reward based epsilon decay. arXiv preprint arXiv:1910.13701</p>
<p>Sensorimotor representation learning for an "active self" in robots: a model survey. P D Nguyen, Y K Georgie, E Kayhan, M Eppe, V V Hafner, S Wermter, KI-Künstliche Intelligenz. 35Nguyen PD, Georgie YK, Kayhan E, Eppe M, Hafner VV, Wermter S (2021) Sensorimotor representation learning for an "active self" in robots: a model survey. KI-Künstliche Intelligenz 35:9-35</p>
<p>What is intrinsic motivation? a typology of computational approaches. P Y Oudeyer, F Kaplan, Front Neurorobot. 16Oudeyer PY, Kaplan F (2009) What is intrinsic motivation? a typology of computational approaches. Front Neurorobot 1:6</p>
<p>Intrinsic motivation systems for autonomous mental development. P Y Oudeyer, F Kaplan, V V Hafner, IEEE Trans Evol Comput. 11Oudeyer PY, Kaplan F, Hafner VV (2007) Intrinsic motivation systems for autonomous mental development. IEEE Trans Evol Comput 11:265-286</p>
<p>Hierarchical som-based detection of novel behavior for 3d human tracking. G I Parisi, S Wermter, The 2013 international joint conference on neural networks (IJCNN), IEEE. Parisi GI, Wermter S (2013) Hierarchical som-based detection of novel behavior for 3d human tracking, in: The 2013 international joint conference on neural networks (IJCNN), IEEE. pp. 1-8</p>
<p>Boa: The bayesian optimization algorithm. M Pelikan, D E Goldberg, E Cantú-Paz, Proceedings of the genetic and evolutionary computation conference GECCO-99, Citeseer. the genetic and evolutionary computation conference GECCO-99, CiteseerPelikan M, Goldberg DE, Cantú-Paz E et al.,(1999) Boa: The bayesian optimization algorithm, in: Proceedings of the genetic and evo- lutionary computation conference GECCO-99, Citeseer. pp. 525- 532</p>
<p>Learning robot control using a hierarchical som-based encoding. J Piaget, M Cook, IEEE Transactions on Cognitive and Developmental Systems. 8International Universities PressThe origins of intelligence in childrenPiaget J, Cook M (1952) The origins of intelligence in children, vol 8. International Universities Press, New York Pierris G, Dahl TS (2017) Learning robot control using a hierarchical som-based encoding. IEEE Transactions on Cognitive and Devel- opmental Systems 9:30-43</p>
<p>CNN features off-the-shelf: an astounding baseline for recognition. A S Razavian, H Azizpour, J Sullivan, S Carlsson, abs/1403.6382.arXiv:1403.6382Razavian AS, Azizpour H, Sullivan J, Carlsson S (2014) CNN fea- tures off-the-shelf: an astounding baseline for recognition. CoRR abs/1403.6382. arXiv:1403.6382,</p>
<p>D Saba, Y Sahli, R Maouedj, A Hadidi, M B Medjahed, Towards artificial intelligence: Concepts, applications, and innovations. SpringerEnabling AI Applications in Data ScienceSaba D, Sahli Y, Maouedj R, Hadidi A, Medjahed MB (2021) Towards artificial intelligence: Concepts, applications, and innovations, in: Enabling AI Applications in Data Science.Springer, pp. 103-146</p>
<p>Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. J Schmidhuber, Connect Sci. 18Schmidhuber J (2006) Developmental robotics, optimal artificial curios- ity, creativity, music, and the fine arts. Connect Sci 18:173-187</p>
<p>Unsupervised perceptual rewards for imitation learning. P Sermanet, K Xu, S Levine, arXiv:1612.06699arXiv preprintSermanet P, Xu K, Levine S (2016) Unsupervised perceptual rewards for imitation learning. arXiv preprint arXiv:1612.06699</p>
<p>Merge som for temporal data. M Strickert, B Hammer, Neurocomputing. 64Strickert M, Hammer B (2005) Merge som for temporal data. Neuro- computing 64:39-71</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressSutton RS, Barto AG (2018) Reinforcement learning: An introduction. MIT press</p>
<p>Algorithms for reinforcement learning. C Szepesvári, Synthesis lectures on artificial intelligence and machine learning. 4Szepesvári C (2010) Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning 4:1-103</p>
<p>Value iteration networks. A Tamar, Y Wu, G Thomas, S Levine, P Abbeel, Advances in Neural Information Processing Systems. Tamar A, Wu Y, Thomas G, Levine S, Abbeel P (2016) Value iteration networks, in: Advances in Neural Information Processing Systems, pp. 2154-2162</p>
<p>Adaptive ε-greedy exploration in reinforcement learning based on value differences. M Tokic, Annual Conference on Artificial Intelligence. SpringerTokic M (2010) Adaptive ε-greedy exploration in reinforcement learn- ing based on value differences, in: Annual Conference on Artificial Intelligence, Springer. pp. 203-210</p>
<p>Learning a world model and planning with a self-organizing, dynamic neural system. M Toussaint, Advances in neural information processing systems. Toussaint M (2004) Learning a world model and planning with a self-organizing, dynamic neural system, in: Advances in neural information processing systems, pp. 926-936</p>
<p>A sensorimotor map: Modulating lateral interactions for anticipation and planning. M Toussaint, Neural Comput. 18Toussaint M (2006) A sensorimotor map: Modulating lateral interac- tions for anticipation and planning. Neural Comput 18:1132-1155</p>
<p>Genetic epistemology and piaget's philosophy of science: Piaget vs. kuhn on scientific progress. J Y Tsou, Theory &amp; Psychology. 16Tsou JY (2006) Genetic epistemology and piaget's philosophy of sci- ence: Piaget vs. kuhn on scientific progress. Theory &amp; Psychology 16:203-224</p>
<p>Autonomy and machine intelligence in complex systems: A tutorial. K G Vamvoudakis, P J Antsaklis, W E Dixon, J P Hespanha, F L Lewis, H Modares, B Kiumarsi, IEEEVamvoudakis KG, Antsaklis PJ, Dixon WE, Hespanha JP, Lewis FL, Modares H, Kiumarsi B (2015) Autonomy and machine intelli- gence in complex systems: A tutorial, in: 2015 American Control Conference (ACC), IEEE. pp. 5062-5079</p>
<p>Structured learning for unsupervised feature selection with high-order matrix factorization. S Wang, J Chen, W Guo, G Liu, Expert Systems with Applications. 112878Wang S, Chen J, Guo W, Liu G (2019) Structured learning for unsuper- vised feature selection with high-order matrix factorization. Expert Systems with Applications , 112878</p>
<p>Model-free λ-policy iteration for discrete-time linear quadratic regulation. Y Yang, B Kiumarsi, H Modares, C Xu, How transferable are features in deep neural networks?, in: Advances in neural information processing systems. Clune J, Bengio Y, Lipson HYang Y, Kiumarsi B, Modares H, Xu C (2021) Model-free λ-policy iteration for discrete-time linear quadratic regulation. IEEE Trans- actions on Neural Networks and Learning Systems Yosinski J, Clune J, Bengio Y, Lipson H (2014) How transferable are features in deep neural networks?, in: Advances in neural infor- mation processing systems, pp. 3320-3328</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-696 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-696</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-696</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-211096826</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.05217v1.pdf" target="_blank">Resolving Spurious Correlations in Causal Models of Environments via Interventions</a></p>
                <p><strong>Paper Abstract:</strong> Causal models could increase interpretability, robustness to distributional shift and sample efficiency of RL agents. In this vein, we address the question of learning a causal model of an RL environment. This problem is known to be difficult due to spurious correlations. We overcome this difficulty by rewarding an RL agent for designing and executing interventions to discover the true model. We compare rewarding the agent for disproving uncertain edges in the causal graph, rewarding the agent for activating a certain node, or rewarding the agent for increasing the causal graph loss. We show that our methods result in a better causal graph than one generated by following the random policy, or a policy trained on the environment's reward. We find that rewarding for the causal graph loss works the best.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e696.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e696.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge intervention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervention design via edges (edge-disproving interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active causal-discovery method that selects uncertain directed edges in the learned graph and rewards an RL agent to set the presumed cause to an extreme value and the presumed effect to the opposite extreme to test whether the edge is real or spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Edge intervention</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Select edges f_i -> f_j with high uncertainty (measured by variability when training the causal learner on different data subsets). For an edge with a positive linear coefficient, create an intervention objective that rewards the agent to drive f_i to its maximum and f_j to its minimum (reward R = f_i - f_j); for negative coefficients use R = f_i + f_j. Use RL (PPO) to find low-level actions achieving these high-level feature settings without explicitly learning an action→feature mapping. Collect the new interventional data and retrain/aggregate into the causal learner to check whether the edge persists.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grid-World (environments A, B, C)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A small interactive grid-world implemented in pycolab where an agent collects items (food, keys, chests), toggles a lamp, and experiences noisy observations (e.g., spurious visual food signals). The environment is episodic and supports active experimentation: different agent policies produce interventional distributions by setting high-level features to different values.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Active interventions targeted at uncertain edges to generate interventional data that invalidates spurious dependences; uncertainty-driven selection (bootstrap/subset variability) to focus experiments; L1 sparsity in the causal learner to prefer simpler graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/noisy observational correlations (measurement noise producing spurious visual features) and policy-dependent spurious dependencies that arise from particular behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Edge uncertainty: train the causal learner on multiple different subsets (s = 5) and measure variability of inferred edges; edges with high variability/uncertainty are candidates for intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Execute interventional policies that set cause/effect pairs to contrarian values and then retrain/aggregate data; if the hypothesised edge does not hold under the interventional distribution, it is refuted (loss increases or coefficient disappears after retraining).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Select edges weighted by uncertainty (or randomly) and create RL intrinsic rewards to drive feature pairs to target extremes; prefer sampling without replacement for faster convergence; repeat selection over multiple interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Interventions using edge-targeting improve recovery of the true graph versus random or environment-reward policies in environments with spurious correlations (B, C); once the correct edge is selected, the true graph is learned quickly. Edge method performs worse than the Loss method on the tested environment B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Random and environment-reward policies rarely discover the true causal graph in environments B and C with spurious correlations; in environment A (randomized layout) random/env reward suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Edge interventions can refute spurious edges by collecting interventional data targeted at uncertain edges; selecting edges by uncertainty (training on subsets) outperforms random selection; sampling edges without replacement converges faster; however, Edge requires choosing the correct edge to intervene on and underperforms the Loss intervention in more complex settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e696.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e696.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node intervention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervention design via nodes (node-setting interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active method that rewards the agent to set a single target node/feature to a chosen value while trying to leave other feature distributions similar, to test causal relations involving that node.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Node intervention</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Randomly select a target node f_i (selection may be weighted by average edge uncertainty) and a target value x (uniformly). Use an intrinsic reward balancing two objectives: drive |f_i - x| down and keep other features' statistics (means/variances) close to those under the previous policy (penalty d measured as differences of means/variances), or alternatively keep the policy close to previous behaviour by adding the environment reward with coefficient γ. Train an RL agent (PPO) on this intrinsic reward, collect data, and update the causal learner with aggregated data to test causal claims involving f_i.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grid-World (environments A, B, C)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic grid-world with manipulable objects and noisy observations; supports executing policies that realize node-level interventions (setting a feature to a target value) and observing downstream effects.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Active node-level interventions to generate data that breaks policy-dependent spurious correlations; tries to preserve other feature statistics to isolate effects of the targeted node.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious observational correlations and measurement noise leading to misleading predictive features.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Node selection based on average uncertainty of incident edges (uncertainty estimated by training causal learner on different subsets); detection of spurious relationships through changed fit of causal learner after node-targeted interventional data is added.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If driving a node to a target value while holding others similar breaks the predictive relationship expected by the current causal graph, the spurious causal relation is refuted when the causal learner's loss/structure changes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Select target nodes randomly or based on uncertainty; choose target values uniformly; incentivize minimal disruption to other features (via matching means/variances or adding env reward) so that observed changes can be attributed to the node.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Node interventions improve recovery of the true graph compared to non-interventional baselines in some cases, but in experiments they underperform the Loss method; keeping policies close via the environment reward helps, while using direct distributional-matching (means/variances) failed because the agent found alternative behaviors that matched statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without targeted node interventions, policies (random or environment-reward) rarely uncover the true graph in environments B and C with spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Node-based interventions can discover true causal relations if the correct node and value are chosen; however, they require correctly choosing targets and are sensitive to how similarity to prior policies is enforced (environment reward works better than explicit statistics matching). They performed worse than Loss in tested environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e696.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e696.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Loss intervention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervention design via causal-graph loss maximization (loss-driven curiosity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration/inquiry strategy that rewards the agent to maximize the causal-learner's loss, thereby actively searching for policies whose data most strongly contradicts the current causal graph hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Loss intervention</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Define intrinsic reward R = L_pi(G), where L_pi(G) is the loss of the current causal-graph learner on data collected under policy π. Train an RL agent (PPO) to maximize this intrinsic reward (analogous to curiosity) so as to find policies whose interventional distributions produce data that the current causal model fits poorly. Aggregate the new data and update the causal learner; iterate this procedure to drive the graph towards structures that are not disproved by collected interventional evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grid-World (environments A, B, C)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic grid-world enabling iterative experiments: agent executes policies aimed to produce data that maximizes causal-learner loss (i.e., finds behaviors that disprove the current model), collects resulting trajectories, and the learner aggregates them to update the causal graph.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Active discovery of interventional policies that produce counterexample data, thereby refuting spurious observational correlations without needing to target specific nodes/edges; combined with a sparse linear causal learner to favor parsimonious graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Policy-dependent spurious correlations arising from noisy features and behavior-conditioned associations (e.g., spurious 'food-and-keys' conjunction predicting health increases).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection via objective: the causal learner's loss L_pi(G) on data collected under candidate policies; high loss signals that current model poorly explains that policy's data (i.e., potential spurious relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Policies that increase graph loss generate data that, when aggregated, cause the causal learner to change/remove spurious edges (i.e., refutation occurs when the hypothesis no longer fits the augmented dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Treat the causal-learner loss as an intrinsic reward and use RL to search the policy space for behaviors that maximally contradict the current model, without hand-selecting nodes or edges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>This method outperformed Node and Edge interventions on environment B and recovered the true causal graph more consistently; overall interventions using Loss find the true graph more often in environments with spurious correlations (B, C) compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines (random policy, policy trained on environment reward) rarely recover the true graph in environments with spurious correlations; with no interventions the Granger/L1 learner fails to disambiguate spurious dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Rewarding policies to maximize causal-learner loss is an effective, general-purpose strategy to discover interventions that refute spurious correlations; it avoids the need to select specific nodes or edges and scales better in more complex/high-dimensional settings than hand-designed (node/edge) interventions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e696.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e696.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Granger-L1 learner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Granger 1-step causality with L1 sparsity (linear causal learner used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple supervised causal-graph learner that regresses current feature vectors on previous time-step vectors using a linear model with L1 regularization to enforce sparsity; used as the causal-graph hypothesis estimator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Investigating causal relations by econometric models and cross-spectral methods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Granger 1-step causality with L1 regularization</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fit a linear regression x_t = W x_{t-1} + b (one-step Granger-style time-lag model) with L1 (lasso) regularization to encourage sparse connections; interpret nonzero coefficients and their time-lag labels as directed edges in the causal graph. Train in supervised fashion by aggregating data from different policies to approximate the minimax objective.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grid-World (environments A, B, C)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same interactive grid-world; the Granger-L1 learner takes aggregated time-series of high-level features produced by different policies (including intervention policies) and fits a sparse linear time-lag model to infer edges.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Does not explicitly model distractors; vulnerable to spurious correlations arising from limited/policy-dependent observational data and measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>L1 regularization provides a form of model parsimony that can reduce reliance on weak predictors but does not by itself resolve spurious correlations caused by policy-dependent data.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When combined with active interventions (Edge/Node/Loss), the Granger-L1 learner can recover the true causal graph in environments with spurious correlations; without interventions it often fails in settings B and C where observational data is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without interventional data, Granger-L1 often learns spurious dependencies in B and C (relies on 'food-and-keys' conjunction) and cannot distinguish true causal relations from policy-induced correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple Granger 1-step linear model with L1 sparsity is sufficient to demonstrate the need for interventions: it fails to recover the true graph under observational data alone when spurious signals exist, but successfully recovers the true graph once interventional data from the proposed interrogation policies are included.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e696.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e696.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty-driven selection of intervention targets (bootstrap-subset variability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pragmatic heuristic that measures uncertainty of inferred edges by training the causal learner on different subsets of data and uses that uncertainty to prioritize which nodes/edges to intervene on.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Uncertainty-based intervention target selection</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Estimate uncertainty of each edge by repeatedly training the causal learner on different random subsets of the collected dataset (s = 5 in experiments) and measuring variability in inferred edges or coefficients; rank edges (or nodes by average incident-edge uncertainty) and preferentially choose high-uncertainty targets for interventions. Sampling without replacement of edges during multiple interventions was found to improve convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grid-World (environments A, B, C)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive RL grid-world where uncertainty estimates are computed from datasets collected by previous policies and used to guide the next intervention policy choices.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Targets interventions to graph components most influenced by spurious or unstable signals as indicated by high estimator variability, thereby focusing experimentation where spurious correlations are likeliest.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Policy-dependent spurious correlations and unstable/weak edges caused by noisy measurements or insufficient data coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Bootstrap/subset-based variability of learned edges/coefficient estimates across multiple training runs on different data subsets; high variability flags uncertain (potentially spurious) edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By prioritizing uncertain edges for intervention, collected interventional data either stabilizes (validates) or disproves (removes) those edges upon retraining, thereby refuting spurious relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Train causal learner on multiple subsets, measure edge variability, select edges/nodes with highest uncertainty; sample edges without replacement across intervention rounds for faster convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Selecting edges by uncertainty performs better than selecting random edges; sampling without replacement accelerates convergence to the true graph.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Random selection of edges or nodes performs worse; selecting nodes by uncertainty had similar results to random node selection in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Uncertainty-based prioritization is an effective mechanism to focus limited intervention budget on the most informative hypotheses; demonstrated improved convergence speed versus random selection for edge interventions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toward a Scientist Agent : Learning to Verify Hypotheses <em>(Rating: 2)</em></li>
                <li>Causal confusion in imitation learning <em>(Rating: 2)</em></li>
                <li>Inferring causation from time series in earth system sciences <em>(Rating: 2)</em></li>
                <li>Learning Neural Causal Models from Unknown Interventions <em>(Rating: 2)</em></li>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 2)</em></li>
                <li>Learning Plannable Representations with Causal InfoGAN <em>(Rating: 1)</em></li>
                <li>Disentangling the independently controllable factors of variation by interacting with the world <em>(Rating: 1)</em></li>
                <li>Learning Causal State Representations of Partially Observable Environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-696",
    "paper_id": "paper-211096826",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "Edge intervention",
            "name_full": "Intervention design via edges (edge-disproving interventions)",
            "brief_description": "An active causal-discovery method that selects uncertain directed edges in the learned graph and rewards an RL agent to set the presumed cause to an extreme value and the presumed effect to the opposite extreme to test whether the edge is real or spurious.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Edge intervention",
            "method_description": "Select edges f_i -&gt; f_j with high uncertainty (measured by variability when training the causal learner on different data subsets). For an edge with a positive linear coefficient, create an intervention objective that rewards the agent to drive f_i to its maximum and f_j to its minimum (reward R = f_i - f_j); for negative coefficients use R = f_i + f_j. Use RL (PPO) to find low-level actions achieving these high-level feature settings without explicitly learning an action→feature mapping. Collect the new interventional data and retrain/aggregate into the causal learner to check whether the edge persists.",
            "environment_name": "Grid-World (environments A, B, C)",
            "environment_description": "A small interactive grid-world implemented in pycolab where an agent collects items (food, keys, chests), toggles a lamp, and experiences noisy observations (e.g., spurious visual food signals). The environment is episodic and supports active experimentation: different agent policies produce interventional distributions by setting high-level features to different values.",
            "handles_distractors": true,
            "distractor_handling_technique": "Active interventions targeted at uncertain edges to generate interventional data that invalidates spurious dependences; uncertainty-driven selection (bootstrap/subset variability) to focus experiments; L1 sparsity in the causal learner to prefer simpler graphs.",
            "spurious_signal_types": "Irrelevant/noisy observational correlations (measurement noise producing spurious visual features) and policy-dependent spurious dependencies that arise from particular behaviors.",
            "detection_method": "Edge uncertainty: train the causal learner on multiple different subsets (s = 5) and measure variability of inferred edges; edges with high variability/uncertainty are candidates for intervention.",
            "downweighting_method": null,
            "refutation_method": "Execute interventional policies that set cause/effect pairs to contrarian values and then retrain/aggregate data; if the hypothesised edge does not hold under the interventional distribution, it is refuted (loss increases or coefficient disappears after retraining).",
            "uses_active_learning": true,
            "inquiry_strategy": "Select edges weighted by uncertainty (or randomly) and create RL intrinsic rewards to drive feature pairs to target extremes; prefer sampling without replacement for faster convergence; repeat selection over multiple interventions.",
            "performance_with_robustness": "Interventions using edge-targeting improve recovery of the true graph versus random or environment-reward policies in environments with spurious correlations (B, C); once the correct edge is selected, the true graph is learned quickly. Edge method performs worse than the Loss method on the tested environment B.",
            "performance_without_robustness": "Random and environment-reward policies rarely discover the true causal graph in environments B and C with spurious correlations; in environment A (randomized layout) random/env reward suffice.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Edge interventions can refute spurious edges by collecting interventional data targeted at uncertain edges; selecting edges by uncertainty (training on subsets) outperforms random selection; sampling edges without replacement converges faster; however, Edge requires choosing the correct edge to intervene on and underperforms the Loss intervention in more complex settings.",
            "uuid": "e696.0"
        },
        {
            "name_short": "Node intervention",
            "name_full": "Intervention design via nodes (node-setting interventions)",
            "brief_description": "An active method that rewards the agent to set a single target node/feature to a chosen value while trying to leave other feature distributions similar, to test causal relations involving that node.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Node intervention",
            "method_description": "Randomly select a target node f_i (selection may be weighted by average edge uncertainty) and a target value x (uniformly). Use an intrinsic reward balancing two objectives: drive |f_i - x| down and keep other features' statistics (means/variances) close to those under the previous policy (penalty d measured as differences of means/variances), or alternatively keep the policy close to previous behaviour by adding the environment reward with coefficient γ. Train an RL agent (PPO) on this intrinsic reward, collect data, and update the causal learner with aggregated data to test causal claims involving f_i.",
            "environment_name": "Grid-World (environments A, B, C)",
            "environment_description": "Interactive episodic grid-world with manipulable objects and noisy observations; supports executing policies that realize node-level interventions (setting a feature to a target value) and observing downstream effects.",
            "handles_distractors": true,
            "distractor_handling_technique": "Active node-level interventions to generate data that breaks policy-dependent spurious correlations; tries to preserve other feature statistics to isolate effects of the targeted node.",
            "spurious_signal_types": "Spurious observational correlations and measurement noise leading to misleading predictive features.",
            "detection_method": "Node selection based on average uncertainty of incident edges (uncertainty estimated by training causal learner on different subsets); detection of spurious relationships through changed fit of causal learner after node-targeted interventional data is added.",
            "downweighting_method": null,
            "refutation_method": "If driving a node to a target value while holding others similar breaks the predictive relationship expected by the current causal graph, the spurious causal relation is refuted when the causal learner's loss/structure changes.",
            "uses_active_learning": true,
            "inquiry_strategy": "Select target nodes randomly or based on uncertainty; choose target values uniformly; incentivize minimal disruption to other features (via matching means/variances or adding env reward) so that observed changes can be attributed to the node.",
            "performance_with_robustness": "Node interventions improve recovery of the true graph compared to non-interventional baselines in some cases, but in experiments they underperform the Loss method; keeping policies close via the environment reward helps, while using direct distributional-matching (means/variances) failed because the agent found alternative behaviors that matched statistics.",
            "performance_without_robustness": "Without targeted node interventions, policies (random or environment-reward) rarely uncover the true graph in environments B and C with spurious correlations.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Node-based interventions can discover true causal relations if the correct node and value are chosen; however, they require correctly choosing targets and are sensitive to how similarity to prior policies is enforced (environment reward works better than explicit statistics matching). They performed worse than Loss in tested environments.",
            "uuid": "e696.1"
        },
        {
            "name_short": "Loss intervention",
            "name_full": "Intervention design via causal-graph loss maximization (loss-driven curiosity)",
            "brief_description": "An exploration/inquiry strategy that rewards the agent to maximize the causal-learner's loss, thereby actively searching for policies whose data most strongly contradicts the current causal graph hypothesis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Loss intervention",
            "method_description": "Define intrinsic reward R = L_pi(G), where L_pi(G) is the loss of the current causal-graph learner on data collected under policy π. Train an RL agent (PPO) to maximize this intrinsic reward (analogous to curiosity) so as to find policies whose interventional distributions produce data that the current causal model fits poorly. Aggregate the new data and update the causal learner; iterate this procedure to drive the graph towards structures that are not disproved by collected interventional evidence.",
            "environment_name": "Grid-World (environments A, B, C)",
            "environment_description": "Interactive episodic grid-world enabling iterative experiments: agent executes policies aimed to produce data that maximizes causal-learner loss (i.e., finds behaviors that disprove the current model), collects resulting trajectories, and the learner aggregates them to update the causal graph.",
            "handles_distractors": true,
            "distractor_handling_technique": "Active discovery of interventional policies that produce counterexample data, thereby refuting spurious observational correlations without needing to target specific nodes/edges; combined with a sparse linear causal learner to favor parsimonious graphs.",
            "spurious_signal_types": "Policy-dependent spurious correlations arising from noisy features and behavior-conditioned associations (e.g., spurious 'food-and-keys' conjunction predicting health increases).",
            "detection_method": "Detection via objective: the causal learner's loss L_pi(G) on data collected under candidate policies; high loss signals that current model poorly explains that policy's data (i.e., potential spurious relationships).",
            "downweighting_method": null,
            "refutation_method": "Policies that increase graph loss generate data that, when aggregated, cause the causal learner to change/remove spurious edges (i.e., refutation occurs when the hypothesis no longer fits the augmented dataset).",
            "uses_active_learning": true,
            "inquiry_strategy": "Treat the causal-learner loss as an intrinsic reward and use RL to search the policy space for behaviors that maximally contradict the current model, without hand-selecting nodes or edges.",
            "performance_with_robustness": "This method outperformed Node and Edge interventions on environment B and recovered the true causal graph more consistently; overall interventions using Loss find the true graph more often in environments with spurious correlations (B, C) compared to baselines.",
            "performance_without_robustness": "Baselines (random policy, policy trained on environment reward) rarely recover the true graph in environments with spurious correlations; with no interventions the Granger/L1 learner fails to disambiguate spurious dependencies.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Rewarding policies to maximize causal-learner loss is an effective, general-purpose strategy to discover interventions that refute spurious correlations; it avoids the need to select specific nodes or edges and scales better in more complex/high-dimensional settings than hand-designed (node/edge) interventions.",
            "uuid": "e696.2"
        },
        {
            "name_short": "Granger-L1 learner",
            "name_full": "Granger 1-step causality with L1 sparsity (linear causal learner used in experiments)",
            "brief_description": "A simple supervised causal-graph learner that regresses current feature vectors on previous time-step vectors using a linear model with L1 regularization to enforce sparsity; used as the causal-graph hypothesis estimator in experiments.",
            "citation_title": "Investigating causal relations by econometric models and cross-spectral methods",
            "mention_or_use": "use",
            "method_name": "Granger 1-step causality with L1 regularization",
            "method_description": "Fit a linear regression x_t = W x_{t-1} + b (one-step Granger-style time-lag model) with L1 (lasso) regularization to encourage sparse connections; interpret nonzero coefficients and their time-lag labels as directed edges in the causal graph. Train in supervised fashion by aggregating data from different policies to approximate the minimax objective.",
            "environment_name": "Grid-World (environments A, B, C)",
            "environment_description": "Same interactive grid-world; the Granger-L1 learner takes aggregated time-series of high-level features produced by different policies (including intervention policies) and fits a sparse linear time-lag model to infer edges.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Does not explicitly model distractors; vulnerable to spurious correlations arising from limited/policy-dependent observational data and measurement noise.",
            "detection_method": null,
            "downweighting_method": "L1 regularization provides a form of model parsimony that can reduce reliance on weak predictors but does not by itself resolve spurious correlations caused by policy-dependent data.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "When combined with active interventions (Edge/Node/Loss), the Granger-L1 learner can recover the true causal graph in environments with spurious correlations; without interventions it often fails in settings B and C where observational data is ambiguous.",
            "performance_without_robustness": "Without interventional data, Granger-L1 often learns spurious dependencies in B and C (relies on 'food-and-keys' conjunction) and cannot distinguish true causal relations from policy-induced correlations.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "A simple Granger 1-step linear model with L1 sparsity is sufficient to demonstrate the need for interventions: it fails to recover the true graph under observational data alone when spurious signals exist, but successfully recovers the true graph once interventional data from the proposed interrogation policies are included.",
            "uuid": "e696.3"
        },
        {
            "name_short": "Uncertainty-based selection",
            "name_full": "Uncertainty-driven selection of intervention targets (bootstrap-subset variability)",
            "brief_description": "A pragmatic heuristic that measures uncertainty of inferred edges by training the causal learner on different subsets of data and uses that uncertainty to prioritize which nodes/edges to intervene on.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Uncertainty-based intervention target selection",
            "method_description": "Estimate uncertainty of each edge by repeatedly training the causal learner on different random subsets of the collected dataset (s = 5 in experiments) and measuring variability in inferred edges or coefficients; rank edges (or nodes by average incident-edge uncertainty) and preferentially choose high-uncertainty targets for interventions. Sampling without replacement of edges during multiple interventions was found to improve convergence.",
            "environment_name": "Grid-World (environments A, B, C)",
            "environment_description": "Interactive RL grid-world where uncertainty estimates are computed from datasets collected by previous policies and used to guide the next intervention policy choices.",
            "handles_distractors": true,
            "distractor_handling_technique": "Targets interventions to graph components most influenced by spurious or unstable signals as indicated by high estimator variability, thereby focusing experimentation where spurious correlations are likeliest.",
            "spurious_signal_types": "Policy-dependent spurious correlations and unstable/weak edges caused by noisy measurements or insufficient data coverage.",
            "detection_method": "Bootstrap/subset-based variability of learned edges/coefficient estimates across multiple training runs on different data subsets; high variability flags uncertain (potentially spurious) edges.",
            "downweighting_method": null,
            "refutation_method": "By prioritizing uncertain edges for intervention, collected interventional data either stabilizes (validates) or disproves (removes) those edges upon retraining, thereby refuting spurious relationships.",
            "uses_active_learning": true,
            "inquiry_strategy": "Train causal learner on multiple subsets, measure edge variability, select edges/nodes with highest uncertainty; sample edges without replacement across intervention rounds for faster convergence.",
            "performance_with_robustness": "Selecting edges by uncertainty performs better than selecting random edges; sampling without replacement accelerates convergence to the true graph.",
            "performance_without_robustness": "Random selection of edges or nodes performs worse; selecting nodes by uncertainty had similar results to random node selection in experiments.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Uncertainty-based prioritization is an effective mechanism to focus limited intervention budget on the most informative hypotheses; demonstrated improved convergence speed versus random selection for edge interventions.",
            "uuid": "e696.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toward a Scientist Agent : Learning to Verify Hypotheses",
            "rating": 2,
            "sanitized_title": "toward_a_scientist_agent_learning_to_verify_hypotheses"
        },
        {
            "paper_title": "Causal confusion in imitation learning",
            "rating": 2,
            "sanitized_title": "causal_confusion_in_imitation_learning"
        },
        {
            "paper_title": "Inferring causation from time series in earth system sciences",
            "rating": 2,
            "sanitized_title": "inferring_causation_from_time_series_in_earth_system_sciences"
        },
        {
            "paper_title": "Learning Neural Causal Models from Unknown Interventions",
            "rating": 2,
            "sanitized_title": "learning_neural_causal_models_from_unknown_interventions"
        },
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 2,
            "sanitized_title": "curiositydriven_exploration_by_selfsupervised_prediction"
        },
        {
            "paper_title": "Learning Plannable Representations with Causal InfoGAN",
            "rating": 1,
            "sanitized_title": "learning_plannable_representations_with_causal_infogan"
        },
        {
            "paper_title": "Disentangling the independently controllable factors of variation by interacting with the world",
            "rating": 1,
            "sanitized_title": "disentangling_the_independently_controllable_factors_of_variation_by_interacting_with_the_world"
        },
        {
            "paper_title": "Learning Causal State Representations of Partially Observable Environments",
            "rating": 1,
            "sanitized_title": "learning_causal_state_representations_of_partially_observable_environments"
        }
    ],
    "cost": 0.012605499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Resolving Spurious Correlations in Causal Models of Environments via Interventions
February 14, 2020</p>
<p>Sergei Volodin volodin@google.com 
Nevan Wichers wichersn@google.com 
Jeremy Nixon jeremynixon@google.com 
Google Brain 
Resolving Spurious Correlations in Causal Models of Environments via Interventions
February 14, 2020
Causal models could increase interpretability, robustness to distributional shift and sample efficiency of RL agents. In this vein, we address the question of learning a causal model of an RL environment. This problem is known to be difficult due to spurious correlations. We overcome this difficulty by rewarding an RL agent for designing and executing interventions to discover the true model. We compare rewarding the agent for disproving uncertain edges in the causal graph, rewarding the agent for activating a certain node, or rewarding the agent for increasing the causal graph loss. We show that our methods result in a better causal graph than one generated by following the random policy, or a policy trained on the environment's reward. We find that rewarding for the causal graph loss works the best.</p>
<p>Introduction</p>
<p>Causality (Halpern &amp; Pearl, 2005) is an important concept (Pearl, 2018) for Machine Learning, since it resolves many issues in performance and Artificial Intelligence (AI) safety (Amodei et al., 2016) such as interpretability (Madumal et al., 2019;Bengio, 2017), robustness to distributional shift (de Haan et al., 2019a) and sample-efficiency (Buesing et al., 2018). It is particularly well suited for Reinforcement Learning (RL), compared to supervised learning, because in RL there is an opportunity to take actions and influence the environment in a directed way. Since causality is a cornerstone in science, such an agent is expected to be superior to non-causal agents (Marino et al., 2019).</p>
<p>Spurious correlations are a major obstacle in learning causal models. If present, they make learning from purely observational data impossible (Pearl &amp; Mackenzie, 2018). We take advantage of the fact that it is possible to uncover the causal graph by executing interventions (Halpern &amp; Pearl, 2005) which change the data distribution. We design a method to automatically resolve spurious correlations when learning the causal graph of the environment. Since we are interested in learning the high-level dynamics of the environment, lowlevel actions do not directly represent interventions in the environment. As a solution, we reward an RL agent for either setting nodes or edges (based on uncertainty) to specific values, or by rewarding the agent for disproving the learned causal model.</p>
<p>Contributions. In this paper, we formulate the problem of learning the causal graph of an RL environment in an active way. We present ways to design interventions and execute them in an end-to-end fashion. We show that interventions outperform a random baseline policy and a policy trained on the environment's reward. The main contribution is a framework to deal with spurious correlations in RL via interventions, from definitions to methods and experiments.</p>
<p>Problem</p>
<p>We have an RL environment µ containing a set of observations O, set of actions A, transition probability P[o , r|s, o] (we use P for probability) for r ∈ R, initial observation distribution P[o 0 ]. We call a tuple of an observation, action and a reward x = (o, a, r) a step. We denote X the space of steps. History h = (x 0 , ..., x T ) contains the steps from one agent-environment interaction. f : X → R d is the mapping from raw steps to high-level features.</p>
<p>Causal learning. We use the standard definition of a Structural Causal Model for time series data (Halpern &amp; Pearl, 2005). We create a directed graph of d nodes, one for every feature. Edges represent dependencies between features of time-steps and are labelled with number of time-steps it takes for one node to influence another. The value of a node is determined by values of its parents at past time-steps. L π,t (G) ≥ 0, where π is the policy used to collect the data, denotes the loss of the causal learner (Runge et al., 2019). This means how well the graph fits the features of previous episodes.</p>
<p>Spurious correlations and interventions. We would like to find a graph G which fits the environment µ, without giving the agent the ground truth G * . However, which policy should we execute in µ to learn G? Our proposition shows that a random policy is sufficient to learn the graph in a realizable case defined by (Shalev-Shwartz &amp; Ben-David, 2014) where some graph perfectly fits the data 1 . In that case, interventions are not required to resolve spurious correlations.</p>
<p>Proposition 1. For an environment µ and features f with a true causal graph G * s.t. L π (G * ) = 0 for all π and L &gt; 0 for other G, a random policy π r gives the true graph: arg min G max t L πr,t (G) = G * Intuitively, in the realizable case, more data is always better. The proof is based on two ideas: first, for a policy π * giving the true G * , a random policy π r will take same actions as π * with some probability: P[π r = π * ] &gt; 0. Thus, data from π * will be in the dataset. Next, since G * fits any policy, the learner will find a graph G s.t. L πr,t (G) = 0. By linearity of L, loss on π r equals a nonnegative combination of losses over policies π r equals to, including one for π * . Now, since the non-negative combination is 0, one particular term L π * ,t (G) = 0 as well, and G = G * In contrast, in cases where we cannot fit the data perfectly (L π,t (G * ) &gt; 0), it is possible that two policies produce different graphs, no matter the length of the history t or the method to learn the graph. Indeed, given data from one policy only, we can construct two environments with different correct ground truth graphs, which will match the existing data perfectly. Therefore, no learning method can uncover the graph because it is not fully determined by the data obtained. In some cases, this leads to learning spurious correlations: dependencies which work given one policy, but not the other. For example, noise in one feature might force the learner to rely on spurious features, which is irrelevant given a different policy.</p>
<p>We define the problem in a minimax 2 fashion: the graph should not be disproved even by the worst policy π, at any number of collected episodes t. This is similar to the scientific method in the real world: we want to learn causal relationships that are true no matter which experiments or actions we perform.
3 G * = arg min G max π max t L π,t (G)(1)
In practice, in the equation above, we use a finite sequence of policies, instead of all policies in max π , and we consider a finite t &lt; ∞. Executing the next policy π i+1 after π i can be seen as doing an intervention do(I i+1 ) in the causal model, since the policy sets nodes to specific values. In that sense, we sample from the interventional distribution P[·|do(I i+1 )].</p>
<p>In the next section we give concrete methods for intervention design.</p>
<p>Solution</p>
<p>We compare different methods for intervention design: Intervention design via edges. We test if selected edges in the graph are real or caused by spurious correlations. Edges where the causal graph learning algorithm is uncertain are selected more often. We measure uncertainty by how different the results are when trained on different subsets of the data. When
F e a t u r e s f G r a p h &amp; s p a r s i t y l o s s L π , t ( G ) U n w r a p p e d e n v i r o n m e n t : G r i d W o r l d μ C a u s a l L e a r n i n g A g e n t O b s e r v a t i o n s C a u s a l G r a p h G I n t e r v e n t i o n d e s i g n G o a l v e c t o r U n w r a p p e d A g e n t : P P O I n t e r n a l r e w a r d R R e w a r d A c t i o n C o n c a t P o l i c y π
A g e n t l o s s Figure 1: Learning the causal graph by actively interacting with the environment. Given a high-level set of features f i and an environment µ, we collect data using a policy π to learn an initial causal model G (hypothesis) from features time series. Then, we design an intervention (an experiment), a new policy aimed at disproving the current causal graph to learn the true one. Then, the intervention is executed in the environment as a standard agent-environment interaction with intrinsic reward, and the process is repeated.</p>
<p>selecting an edge f i → f j with a positive coefficient in a linear causal model 4 , we propose to test it by setting do(f i = max, f j = min) (minimal and maximal feature values). To do so, we reward the agent for setting f i = max and for
f j = min. The total reward is R = f i − f j 5 .
Note that in this approach, we do not need to explicitly learn the mapping between low-level actions and high-level features f j , since we simply use RL for the high-level task.</p>
<dl>
<dt>Intervention design via nodes. We reward the agent for setting a target node f i to a target value x. We also reward for keeping everything else the same by penalizing the difference d between the averages and variances of feature distributions from previous and current policies 6</dt>
<dd>R = −|f i − x| − d(f (h)|π, f (h)|π old ).
Another method for the distance part is to reward the agent for the environment reward with some coefficient, because having a common reward will keep the behavior similar: γ: R = −|f i − x| + γR old . Node values are selected uniformly at random, while nodes are selected based on the average uncertainty of the edges as explained above.</dd>
</dl>
<p>Intervention design via loss. We reward the agent for finding policies π which give high causal graph loss R = L π (G). This is similar to curiosity approaches (Pathak et al., 2017). The reasoning behind this is that we want to find data disproving our model, like in Eq. 1. Compared to previous methods, we do not select the node or an edge explicitly.</p>
<p>For the rest of the setup, we use the simplest methods (Granger 1-step causality (Granger, 1969) with hardcoded features and a sparsity loss). In this approach, we simply regress the current time-step x t on the previous one x t+1 : x t = W x t−1 +b using a linear relationship with l 1 regularization. It is trained in a supervised manner by aggregating data from different policies to approximate a solution to Eq. 1. We note that better methods (Runge et al., 2019) of learning causal graphs would still fail without interventions (as explained in the previous 4 In the non-linear case, the coefficient might depend on the current value of features. In that case, this approach will still work, but the step has to be small enough.</p>
<p>5 In case with a negative coefficient, we need f i + f j . 6 We try set d as the difference between random variables' expected values and variances. section). Other causality learning methods are compatible with our approach. Methods which discover the features end-to-end (Thomas et al., 2018;Kurutach et al., 2018;Ke et al., 2019;François-Lavet et al., 2018;Zhang et al., 2019) can be used to discover the nodes in the causal graph. Now, all the discussed components are combined together into a causal agent, to discover the true causal graph of the environment, see Figure 1.</p>
<p>The environment</p>
<p>We use a simple Grid-World environment. The agent needs to eat food in the environment or the episode will end. In addition, it collects keys to open chests, with each chest giving a reward. There is a button which turns a light on and off and does not give reward. Figure 2a represents the causal model we want to discover. Appendix C contains more details about the environment.</p>
<p>We use the following specific environments: (A) a 5x5 grid-world with randomly placed items. (B, Figure 2b) a grid-world with a fixed map, where the agent must collect the key before the food. (C, Figure 2c) 10x10 grid-world with randomly placed items where the food is close to the key, and the chest is far away.</p>
<p>We add noise to each of the environments. With some probability, food is visible at cells not containing food.</p>
<p>The environment we choose is characteristic of the real world, as it contains spurious correlations that we need to uncover by changing behavior.  </p>
<p>Experiments</p>
<p>Hardcoded features. We augment the feature set with conjunctions of relevant features in order to keep the problem in the linear domain. This allowed us to keep the causal learning simple to focus on interventions. There are many techniques for learning nonlinear causal graphs that are compatible with our approach. Baselines and methods. For all methods we first use a random policy for exploration of the environment. Next, we train a PPO (Schulman et al., 2017) agent to follow the intervention reward. We compare three methods for interventions: rewarding the agent proportional to the loss of the model (Loss), disproving edges (Edge), setting nodes to values (Node). We measure if the correct graph was learned using cosine similarity.</p>
<p>Results</p>
<p>The random and environment reward policies discover the true causal graph in environment A, because the environment is randomly generated, so there aren't any spurious correlations. The random and environment reward policies extremely rarely discover the true causal graph in environments B and C. This is because the food presence feature is noisy and a random policy often collects the food before the keys because they are close together. To predict the health increase, it is best to rely on the spurious feature "food and keys &gt; 0".</p>
<p>The intervention methods discover the true causal graph in environments B and C more often (results for C in the appendix). This is because they also include data from the intervention policy which collects the food when the agent doesn't have a key. With data from the intervention policy, it is no longer optimal to rely on the spurious feature.</p>
<p>The Loss intervention method outperforms Node and Edge methods on environment (B), Figure 2b. The main problem with the Node and Edge methods is that they have to choose the correct node or edge to intervene on. Once the correct edge or node is chosen, the true graph is learned quickly. In contrast, the Loss method doesn't have to choose the right thing to intervene on. We expected that for harder environments, we would have to explicitly specify what node or edge to intervene on, as they are less trivial to find by random exploration. This would lead to Node and Edge methods outperforming the Loss method. However, the experimental results show the opposite: in simple environments hand-designed methods (edges and nodes) perform reasonably well, but if we increase the number of features or the complexity of the environment, they stop finding good policies. We didn't test the node and edge interventions in environment C extensively due to this reason. Sampling without replacement when selecting edges gives faster convergence to the true graph versus sampling with replacement. We also found that selecting edges based on uncertainty (by training on different subsets s = 5) gives better results than selecting random edges. Results for selecting nodes based on uncertainty are similar to selecting randomly. For the nodes method, we found that keeping the new policy close to the old one by using the reward from the environment works. However keeping the feature statistics the same doesn't work because the agent learns a new way to achieve the same statistics. ∞ means that the algorithm didn't find the correct graph during training.</p>
<p>Conclusion</p>
<p>We design a method to learn the causal model of the environment by performing interventions, which helps prevent learning spurious correlations. This shows the potential of RL to improve causal graph learning and compares techniques to accomplish this. We state the problem of learning a causal graph in a RL setting so other work can build off of ours.</p>
<p>Future Work</p>
<p>We plan to combine our graph learning with one of the approaches for learning the features (Kurutach et al., 2018;Ke et al., 2019;François-Lavet et al., 2018;Zhang et al., 2019) and train the entire network end to end. The sparsity loss will help the features be disentangled because it will minimize dependencies between them (Thomas et al., 2018).</p>
<p>To make our method more general, we plan to use one of the advanced nonlinear causality learners (Ke et al., 2019). Some of them are differentiable, which would allow to backpropagate from the graph to the features.</p>
<p>Finally, we can utilize the high-level graph as a hierarchical RL controller (Nachum et al., 2018). Specifically, we can run a traversal algorithm on the causal graph to find chains of nodes that lead to high reward. Then, we can reward the agent for activating the nodes in the correct sequence. This might increase the robustness to distributional shift, as we will rely on the correct features for acting.</p>
<p>A Relevant work</p>
<p>Our method to perform an intervention on an edge is similar to the method used in Marino et al. (2019) to test hypotheses. Compared to that approach, we are interested in the true causal graph of the environment rather than in testing specific hypotheses. Interventions to learn the true graph can be seen in (de Haan et al., 2019b). Our approach is focused on learning the correct graph rather than acting well. We extend the Action-Influence model (Marino et al., 2019;Everitt et al., 2019) to understand the environment. Compared to (Madumal et al., 2019), we learn the graph rather than design it by hand. The idea to reward the agent for the loss of the causal graph is taken from (Pathak et al., 2017). However, here we are interested in a very low-dimensional causal graph rather than in a black-box model of the environment.</p>
<p>B Hyperparameter selection</p>
<p>Resources and parameters. Parameters were chosen with a hyperparameter search on the task of solving the environment using PPO (Schulman et al., 2017). We run the total of 8000 episodes for B and 50000 episodes for C. We vary the number of epochs to train the causal graph in 500-10000, number of interventions 0-50, number of training calls 5-100, intervention method Loss, Edge, maximal number of episodes in the buffer 10-5000, method to select the edge Constant, Weighted and Random. We learn the graph on evaluation data without noise, and update the reward for the trainer.</p>
<p>C The environment</p>
<p>We implement the environment using pycolab. All updates are delayed 1 timestep to give causal information. Figure 4 shows the results for environment C. Without interventions, the correct graph is never uncovered. In contrast, with interventions, the correct graph is learner, the more interventions the better. </p>
<p>D Experiments</p>
<p>Figure 2 :
2Left 2a: The causal diagram of the environment which the agent should learn. The player needs to collect food and keys. Keys are used to open chests and the number of keys is displayed above the first black line. Top row with health decreases at every time-step, and the episode ends if it is 0. The button toggles the lamp (black/white) which gives no reward. Right two 2b, 2c: layouts of environments B and C.</p>
<p>Figure 3 :
3Experimental results on environment B for predicting the true causal graph. Horizontal axis shows the number of episodes (in 1000s), plots are arranged by intervention method (Loss, Node, Edge). Vertical axis represents the number of runs (out of 10) which have converged to the true graph G * . Plots are arranged by the number of interventions(0, 5, 20). Green line represents the median. 0 interventions corresponds to training with reward. The random policy is evaluated in a separate experiment with spurious correlations as a result.</p>
<p>Figure 4 :
4Experimental results on environment C for predicting health, reward, keys and lamp. The description matches that ofFigure 3.
Note that in practice, a random policy might take too much time to explore the environment
Note also that the definition is dependent on the particular L·(·). It might be meaningless if different policies generate drastically different graphs. In that case, the environment has an ill-defined minimax causal structure. This is similar to the problem of defining the general fitness of an RL agent(Legg, 2008), which does not have a silver-bullet solution (it is priordependent). A slightly better way is to create a soft prior over policies, like the complexity prior(Rathmanner &amp; Hutter, 2011): arg min G maxt π P[π]Lπ,t(G). The minimax version, though, is easier to define and evaluate.3 For example, the goal of Physics is to find laws which cannot be disproved by doing experiments.</p>
<p>Concrete problems in ai safety. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565arXiv preprintDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schul- man, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.</p>
<p>The consciousness prior. Yoshua Bengio, arXiv:1709.08568arXiv preprintYoshua Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2017.</p>
<p>Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, arXiv:1811.06272and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search. arXiv preprintLars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search. arXiv preprint arXiv:1811.06272, 2018.</p>
<p>Causal Confusion in Imitation Learning. Dinesh Pim De Haan, Sergey Jayaraman, Levine, NeurIPSPim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal Confusion in Imitation Learning. (NeurIPS):1-17, 2019a. URL http://arxiv.org/abs/ 1905.11979.</p>
<p>Causal confusion in imitation learning. Dinesh Pim De Haan, Sergey Jayaraman, Levine, Advances in Neural Information Processing Systems. Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imi- tation learning. In Advances in Neural Information Processing Systems, pp. 11693-11704, 2019b.</p>
<p>Understanding agent incentives using causal influence diagrams, part i: single action settings. Tom Everitt, Pedro A Ortega, Elizabeth Barnes, Shane Legg, arXiv:1902.09980arXiv preprintTom Everitt, Pedro A Ortega, Elizabeth Barnes, and Shane Legg. Understand- ing agent incentives using causal influence diagrams, part i: single action settings. arXiv preprint arXiv:1902.09980, 2019.</p>
<p>Combined Reinforcement Learning via Abstract Representations. Vincent François-Lavet, Yoshua Bengio, Doina Precup, Joelle Pineau, Vincent François-Lavet, Yoshua Bengio, Doina Precup, and Joelle Pineau. Com- bined Reinforcement Learning via Abstract Representations. sep 2018. URL http://arxiv.org/abs/1809.04506.</p>
<p>Investigating causal relations by econometric models and cross-spectral methods. W J Clive, Granger, Econometrica: journal of the Econometric Society. Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society, pp. 424-438, 1969.</p>
<p>Causes and explanations: A structuralmodel approach. part i: Causes. The British journal for the philosophy of science. Y Joseph, Judea Halpern, Pearl, 56Joseph Y Halpern and Judea Pearl. Causes and explanations: A structural- model approach. part i: Causes. The British journal for the philosophy of science, 56(4):843-887, 2005.</p>
<p>Learning Neural Causal Models from Unknown Interventions. Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, Yoshua Bengio, Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, and Yoshua Bengio. Learning Neural Causal Mod- els from Unknown Interventions. oct 2019. URL http://arxiv.org/abs/ 1910.01075.</p>
<p>Learning Plannable Representations with Causal InfoGAN. Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, Pieter Abbeel, Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, and Pieter Abbeel. Learning Plannable Representations with Causal InfoGAN. jul 2018. URL http://arxiv.org/abs/1807.09341.</p>
<p>Machine super intelligence. Shane Legg, Università della Svizzera italianaPhD thesisShane Legg. Machine super intelligence. PhD thesis, Università della Svizzera italiana, 2008.</p>
<p>Explainable Reinforcement Learning Through a Causal Lens. Prashan Madumal, Tim Miller, Liz Sonenberg, Frank Vetere, Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable Reinforcement Learning Through a Causal Lens. may 2019. URL https: //arxiv.org/abs/1905.10958.</p>
<p>Toward a Scientist Agent : Learning to Verify Hypotheses. Kenneth Marino, Rob Fergus, Arthur Szlam, Kenneth Marino, Rob Fergus, and Arthur Szlam. Toward a Scientist Agent : Learning to Verify Hypotheses. 2019.</p>
<p>Dataefficient hierarchical reinforcement learning. Ofir Nachum, Shane Shixiang, Honglak Gu, Sergey Lee, Levine, Advances in Neural Information Processing Systems. Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data- efficient hierarchical reinforcement learning. In Advances in Neural Informa- tion Processing Systems, pp. 3303-3313, 2018.</p>
<p>Curiositydriven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity- driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16- 17, 2017.</p>
<p>Theoretical impediments to machine learning with seven sparks from the causal revolution. Judea Pearl, arXiv:1801.04016arXiv preprintJudea Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. arXiv preprint arXiv:1801.04016, 2018.</p>
<p>The book of why: the new science of cause and effect. Judea Pearl, Dana Mackenzie, Basic BooksJudea Pearl and Dana Mackenzie. The book of why: the new science of cause and effect. Basic Books, 2018.</p>
<p>A philosophical treatise of universal induction. Samuel Rathmanner, Marcus Hutter, Entropy. 136Samuel Rathmanner and Marcus Hutter. A philosophical treatise of universal induction. Entropy, 13(6):1076-1136, 2011.</p>
<p>Inferring causation from time series in earth system sciences. Jakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D Mahecha, Jordi Muñoz-Marí, Nature communications. 101Jakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D Ma- hecha, Jordi Muñoz-Marí, et al. Inferring causation from time series in earth system sciences. Nature communications, 10(1):1-13, 2019.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Understanding machine learning: From theory to algorithms. Shai Shalev, - Shwartz, Shai Ben-David, Cambridge university pressShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.</p>
<p>Valentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard, Philippe Beaudoin, Hugo Larochelle, Joelle Pineau, arXiv:1802.09484Doina Precup, and Yoshua Bengio. Disentangling the independently controllable factors of variation by interacting with the world. arXiv preprintValentin Thomas, Emmanuel Bengio, William Fedus, Jules Pondard, Philippe Beaudoin, Hugo Larochelle, Joelle Pineau, Doina Precup, and Yoshua Bengio. Disentangling the independently controllable factors of variation by interact- ing with the world. arXiv preprint arXiv:1802.09484, 2018.</p>
<p>Learning Causal State Representations of Partially Observable Environments. Amy Zhang, Zachary C Lipton, Luis Pineda, Kamyar Azizzadenesheli, Anima Anandkumar, Laurent Itti, Joelle Pineau, Tommaso Furlanello, Amy Zhang, Zachary C. Lipton, Luis Pineda, Kamyar Azizzadenesheli, Anima Anandkumar, Laurent Itti, Joelle Pineau, and Tommaso Furlanello. Learning Causal State Representations of Partially Observable Environments. (ii):1-16, 2019. URL http://arxiv.org/abs/1906.10437.</p>            </div>
        </div>

    </div>
</body>
</html>
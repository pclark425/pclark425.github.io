<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7758 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7758</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7758</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-256598066</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.01339v2.pdf" target="_blank">Creating a Large Language Model of a Philosopher</a></p>
                <p><strong>Paper Abstract:</strong> Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an"actual human philosopher".</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7758.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7758.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>5-AFC human-machine discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five-alternative forced-choice human-machine discrimination task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation protocol in which participants are shown one true human answer and several (here, four) LLM-generated answers to the same question and must choose which is the human's; used as an indirect quality measure of LLM-generated philosophical texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>philosophy (philosophical text generation/evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanatory text / philosophical answers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Five-alternative forced-choice human-machine discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each question, participants are shown five candidate answers (one from the human philosopher Daniel Dennett and four from the fine-tuned GPT-3 model) in randomized order and must select which one was written by the human; accuracy in selection is taken as an indirect measure of the model's ability to produce humanlike, expert-level philosophical output.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percent correct selections)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent of trials where participant correctly identified the human answer (units: % correct). Chance baseline = 20% for five alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Dennett Q&A stimulus set (10 questions with one human and four model answers each)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three participant groups: 25 Dennett experts contacted directly; 302 blog readers (after exclusions); 98 ordinary research participants (US/UK Prolific; bachelor+; quality filters). Experts saw 10 questions; ordinary participants saw 5 randomly selected questions. Responses were forced-choice; order of answers and questions randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Experts: mean 5.08/10 (51% correct) vs chance 20% for single-choice per item; Blog readers: 4.81/10 (48%); Ordinary participants: mean 1.20/5 (24%), near chance (20%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Direct side-by-side comparison to Dennett's actual answers; experts and experienced readers performed above chance but far from perfect (~50%), novices performed near chance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Task measures only short-paragraph outputs, not extended dialogue or argumentative structure; possible superficial cues in some questions; five-alternative forced-choice has high chance level (20%) which affects interpretability of absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7758.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>5-point Dennett-likeness rating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Five-point 'Dennett-likeness' (or 'real human philosopher') similarity rating scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Likert-style rating where participants evaluate, for each answer, how similar it is to what Dennett (or a real philosopher) might say, used as a subjective measure of perceived author-likeness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>philosophy / human evaluation of generated text</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>explanatory text / stylistic mimicry</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Five-point Dennett-likeness rating</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After the forced-choice selection, participants re-view the question and all five answers and rate each on a 5-point scale ranging from 'not at all like what Dennett might say' (1) to 'exactly like what Dennett might say' (5); ordinary participants see analogous wording referencing 'a real human philosopher.'</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean Likert rating</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Average rating across items on a 1–5 scale where 1 = not at all like what Dennett might say and 5 = exactly like what Dennett might say.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Same Dennett Q&A stimulus set</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Ratings collected from same participant pools as discrimination task (25 experts, 302 blog readers, 98 ordinary participants). All items rated; no missing data.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Experts rated Dennett's actual answers higher (MDennett = 3.73) than GPT-3's (MGPT-3 = 2.34), paired t(24)=8.44, p<.001; Blog readers MDennett = 3.60 vs MGPT-3 = 2.65 (p<.001); ordinary participants showed no statistically detectable difference (3.11 vs 3.08).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Human (Dennett) answers were rated significantly more Dennett-like by experts and blog readers; novices did not reliably distinguish via ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Subjective scale may be influenced by participants' familiarity with Dennett; inter-rater reliability not reported; scale captures perceived stylistic similarity but not correctness or philosophical soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7758.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical hypothesis tests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frequentist inferential statistics (t-tests, one-proportion z-test, independent-samples t-test, paired t-tests)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard inferential tests used to evaluate whether observed discrimination accuracies and rating differences differ from chance or between groups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>psychology / experimental evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical evaluation of human judgments about generated text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>t-tests and one-proportion z-test</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Used two-tailed tests with alpha .05: one-sample and paired t-tests to test means (e.g., mean correct vs chance), independent-samples t-test for group comparisons, and one-proportion z-test for single-question analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>t statistics, p-values, Cohen's d, confidence intervals; z-statistics for proportions</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>t() values reported with degrees of freedom, p-values; effect sizes reported as Cohen's d; one-proportion z-test evaluates observed proportion vs null (e.g., chance).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Participant response data from Dennett discrimination and rating tasks</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Analyses conducted in R (v4.1.1) and SPSS (v27.0.0.0) with cross-verification; all analyses two-tailed, alpha = .05.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Examples: Experts M=5.08/10, t(24)=7.13, p<.001, d=1.43; Experts rated Dennett more Dennett-like, paired t(24)=8.44, p<.001, d=1.69; blog readers M=4.81/10, t(301)=23.22, p<.001, d=1.34; ordinary participants no significant difference from chance, t(97)=1.71, p=.09.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Statistical tests demonstrate significant above-chance discrimination for experts and blog readers overall, but performance far below hypothesized thresholds (e.g., 80% for experts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Frequentist tests assume independence/normality approximations; multiple comparisons not explicitly adjusted in reported excerpts; some per-question tests have small n leading to reduced power.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7758.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Turnitin similarity check</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turnitin plagiarism/similarity check against Turnitin corpus plus training texts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Commercial plagiarism-detection service used to assess whether fine-tuned model outputs directly reproduce strings from the training corpus, reported as an overall percent similarity and flagged passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / plagiarism detection</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>novelty/overtraining assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Turnitin similarity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Submit model-generated outputs (and human answers) to Turnitin against a corpus augmented with the Dennett training texts; report overall similarity percentage and review flagged matches for overlap with training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Overall similarity percentage and per-passage similarity flags</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Turnitin reports percent similarity of submitted text to comparison corpora; similarity below typical academic thresholds (10%–15%) considered ordinary non-plagiarized work.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Turnitin comparison corpus + the Dennett training corpus used for fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors inspected Turnitin output; none of the model passages were marked as similar to the fine-tuning training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Turnitin reported 5% overall similarity between the model's answers and the comparison corpora; no passages were flagged as matching the fine-tuning training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Turnitin applied to both GPT-3 outputs and Dennett's answers; GPT-3 outputs showed low similarity overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Turnitin's internal matching algorithms are proprietary and nontransparent; Turnitin is optimized for academic text detection rather than fine-grained paraphrase detection of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7758.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>n-gram overlap analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contiguous n-gram overlap analysis using the ngram R package</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transparent string-matching analysis looking for shared contiguous sequences of words (6-grams and above) between model outputs and the fine-tuning corpus to detect verbatim reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / plagiarism/novelty analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>novelty/overtraining assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>n-gram matching (6-gram minimum) using ngram package</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Preprocess texts to lowercase and strip punctuation, tokenize by spaces, then search for matching contiguous n-grams of length 6 or more between the 3,240 words of GPT-3 outputs and ~2 million words of the training corpus; exclude n-grams that only appear as subsets of larger matches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number of matching n-grams and counts of occurrences</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Count of distinct contiguous n-grams (6-grams, 7-grams, ...) that exactly match between model outputs and training corpus; also counts of 5-word string matches reported for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Fine-tuning corpus of Daniel Dennett (≈2 million words, 1,828 segments) vs GPT-3 generated answers (3,240 words)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors ran automated script (R ngram package) to detect overlaps; manual inspection of matched strings reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Found 21 matching strings of 6+ words; 5-word string search found 381 occurrences of 124 different 5-word strings in the training data; Dennett's own answers had fewer long matches by comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Overlap primarily in common/phrasal stock phrases and book titles, not long distinctive philosophical passages; conclusion: model not simply verbatim copying.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Exact contiguous n-gram matching does not detect paraphrase or semantic copying; choice of n (6 words) is arbitrary and affects sensitivity; preprocessing (removing punctuation) affects matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7758.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt & decoding parameter sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering and decoding-parameter exploration (temperature, top_p, penalties)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Exploratory examination of prompt design and generation parameters (temperature, top_p, frequency/presence penalties, best_of) to find settings that produce usable model completions for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP generation / methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>text generation (philosophical answers)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prompt engineering and decoding-parameter tuning</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Authors tried a variety of prompt forms and playground parameter settings (temperature, top_p, frequency penalty, presence penalty, best_of) and selected a minimal interviewer prompt; used default playground parameters (temperature=0.7, top_p=1, frequency_penalty=0, presence_penalty=0, best_of=1) for final data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Qualitative suitability of generated completions; exclusion criteria based on length and POV</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Preference for completions matching human answer length (N ± 5 words) and first-person Dennett voice; parameter values recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Dennett Q&A prompts entered in OpenAI playground</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors manually inspected outputs during exploratory tuning; no cherry-picking of final 4 outputs per question beyond length/POV/offense exclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Default playground generation parameters were used for final collection; authors report no parameter setting notably outperformed defaults in producing better answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Prompt format ('Interviewer: [question]\n\nDennett:') encouraged first-person voice and reduced prompt-induced content steering; authors argue minimal prompt reduces bias toward particular philosophical content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Parameter search described qualitatively; no systematic grid search or ablation study reported; generation variability required multiple attempts to obtain 4 usable outputs for some long prompts (up to 22 attempts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7758.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-cherry-picking criterion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No cherry-picking / uncurated-output evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental design principle enforcing that model outputs used in evaluation are not selected for quality by humans aside from mechanical length/format edits, intended to avoid inflating apparent model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>experimental methodology for LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation design</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>No-cherry-picking unedited output protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect multiple raw model completions for each prompt and include a preset number (four) of outputs in the evaluation sample without selecting among them for perceived quality; only mechanical edits (quote normalization, dash normalization, and truncation to target length) were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Protocol adherence (count of excluded outputs and reasons)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Number of model outputs excluded and legitimate exclusion reasons (length < N-5 words, wrong POV, offensive content); otherwise outputs unedited.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Collected GPT-3 completions for the 10 Dennett questions (4 accepted per question)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Authors excluded only on pre-specified non-quality grounds: length too short, third-person POV, or offensive content; for two long prompts many attempts were needed (16 and 22 attempts) to get 4 usable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>No cherry-picking of perceived quality; 40 accepted model responses (4 per 10 questions) used in discriminatory testing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Design choice intended to produce an unbiased sample of model outputs for direct comparison to human answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Even with no cherry-picking, repeated attempts to obtain length-appropriate outputs means some selection pressure (responses that meet length/POV constraints were retained), and generation stochasticity might bias sample composition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7758.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Turing Test / indistinguishability criterion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turing Test / indistinguishability as a criterion for attributing thought/consciousness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual evaluative standard that an artificial system's responses justify attribution of thought or consciousness if they are indistinguishable from human responses under conversational conditions; discussed as a theoretical benchmark but not implemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>philosophy of mind / AI evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>conceptual evaluation criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Turing Test (imitation game) / human indistinguishability</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Originally proposed by Turing: an evaluator interacts in natural language with both a human and a machine without knowing which is which; if the evaluator cannot reliably distinguish them, the machine is said to pass; discussed here as a stronger, dialogue-based discriminability benchmark contrasted with the study's short-answer discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Indistinguishability rate / evaluator error rate</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Proportion of times evaluators fail to distinguish machine from human in extended conversational settings (no units beyond %).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not implemented in this study; authors note Turing-test-like conditions require back-and-forth dialogue and memory beyond the model's context window.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>N/A in this study; authors explicitly state their experiment is not a Turing test and that a proper Turing test would likely allow experts to distinguish the model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Conceptual comparison: the short-answer discrimination used in the paper is weaker than a full Turing test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Turing test requires extended interaction and expert questioners; LLMs with limited context/memory are easier to detect in longer dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7758.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7758.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Referenced evaluation frameworks/tools</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Referenced external evaluation frameworks and tools (Clark et al. 2021; RoFT; Scarecrow; Van der Lee et al. 2019; Gao et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers and tools cited that provide complementary or alternative methods for evaluating generated text, including human evaluation studies, toolkits for human detection (RoFT), and frameworks for scrutinizing machine text (Scarecrow).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (and other LLMs discussed in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (GPT-3 base models, GPT-2, ChatGPT etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / human evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation frameworks and toolkits</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Clark et al. human evaluation; RoFT; Scarecrow; Best practices for human evaluation (van der Lee et al.); Gao et al. human+detector study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Cited studies/tools include: Clark et al. (2021) — rigorous human discrimination experiments across domains; Dugan et al. (RoFT) — toolkit for evaluating human detection of machine text; Dou et al. (Scarecrow) — framework for scrutinizing GPT-3 text; van der Lee et al. (2019) — best-practices guidance for human evaluation of NLG; Gao et al. (2022) — comparison of ChatGPT-generated abstracts using AI detectors, plagiarism detectors, and blinded human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Domain dependent: human accuracy in detection, detector scores, plagiarism flags, qualitative explanations, and guidance checklist items</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Varies by referenced work: e.g., Clark et al. report % correct and qualitative explanations; RoFT provides protocols; Scarecrow provides diagnostic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Various datasets across cited works (stories, news, recipes in Clark et al.; scientific abstracts in Gao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Cited works typically involve N≈130 participants per domain (Clark et al.), small expert panels (Gao et al. used 4 experts, with noted limitations), and tool-supported human-detection tasks (RoFT).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited findings: Clark et al. found ~50% accuracy distinguishing GPT-3 from humans (near chance); Gao et al. scientists were substantially above chance in their abstract discrimination but methodological limitations noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>These referenced works provide alternative evidence that high-capacity LLMs can be difficult to distinguish from humans in some domains but not others; methodologies vary and affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Authors note that many published studies lack transparency about cherry-picking, editing, exact model settings, and participant expertise, limiting comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creating a Large Language Model of a Philosopher', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>All that's' human' is not gold: Evaluating human evaluation of generated text <em>(Rating: 2)</em></li>
                <li>RoFT: A tool for evaluating human detection of machine-generated text <em>(Rating: 2)</em></li>
                <li>Is GPT-3 text indistinguishable from human text? Scarecrow: A framework for scrutinizing machine text <em>(Rating: 2)</em></li>
                <li>Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers <em>(Rating: 2)</em></li>
                <li>Best practices for the human evaluation of automatically generated text <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>RoFT: A tool for evaluating human detection of machine-generated text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7758",
    "paper_id": "paper-256598066",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "5-AFC human-machine discrimination",
            "name_full": "Five-alternative forced-choice human-machine discrimination task",
            "brief_description": "A human evaluation protocol in which participants are shown one true human answer and several (here, four) LLM-generated answers to the same question and must choose which is the human's; used as an indirect quality measure of LLM-generated philosophical texts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B parameters",
            "scientific_domain": "philosophy (philosophical text generation/evaluation)",
            "theory_type": "explanatory text / philosophical answers",
            "evaluation_method_name": "Five-alternative forced-choice human-machine discrimination",
            "evaluation_method_description": "For each question, participants are shown five candidate answers (one from the human philosopher Daniel Dennett and four from the fine-tuned GPT-3 model) in randomized order and must select which one was written by the human; accuracy in selection is taken as an indirect measure of the model's ability to produce humanlike, expert-level philosophical output.",
            "evaluation_metric": "Accuracy (percent correct selections)",
            "metric_definition": "Percent of trials where participant correctly identified the human answer (units: % correct). Chance baseline = 20% for five alternatives.",
            "dataset_or_benchmark": "Dennett Q&A stimulus set (10 questions with one human and four model answers each)",
            "human_evaluation_details": "Three participant groups: 25 Dennett experts contacted directly; 302 blog readers (after exclusions); 98 ordinary research participants (US/UK Prolific; bachelor+; quality filters). Experts saw 10 questions; ordinary participants saw 5 randomly selected questions. Responses were forced-choice; order of answers and questions randomized.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Experts: mean 5.08/10 (51% correct) vs chance 20% for single-choice per item; Blog readers: 4.81/10 (48%); Ordinary participants: mean 1.20/5 (24%), near chance (20%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Direct side-by-side comparison to Dennett's actual answers; experts and experienced readers performed above chance but far from perfect (~50%), novices performed near chance.",
            "limitations_noted": "Task measures only short-paragraph outputs, not extended dialogue or argumentative structure; possible superficial cues in some questions; five-alternative forced-choice has high chance level (20%) which affects interpretability of absolute accuracy.",
            "uuid": "e7758.0",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "5-point Dennett-likeness rating",
            "name_full": "Five-point 'Dennett-likeness' (or 'real human philosopher') similarity rating scale",
            "brief_description": "A Likert-style rating where participants evaluate, for each answer, how similar it is to what Dennett (or a real philosopher) might say, used as a subjective measure of perceived author-likeness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B parameters",
            "scientific_domain": "philosophy / human evaluation of generated text",
            "theory_type": "explanatory text / stylistic mimicry",
            "evaluation_method_name": "Five-point Dennett-likeness rating",
            "evaluation_method_description": "After the forced-choice selection, participants re-view the question and all five answers and rate each on a 5-point scale ranging from 'not at all like what Dennett might say' (1) to 'exactly like what Dennett might say' (5); ordinary participants see analogous wording referencing 'a real human philosopher.'",
            "evaluation_metric": "Mean Likert rating",
            "metric_definition": "Average rating across items on a 1–5 scale where 1 = not at all like what Dennett might say and 5 = exactly like what Dennett might say.",
            "dataset_or_benchmark": "Same Dennett Q&A stimulus set",
            "human_evaluation_details": "Ratings collected from same participant pools as discrimination task (25 experts, 302 blog readers, 98 ordinary participants). All items rated; no missing data.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Experts rated Dennett's actual answers higher (MDennett = 3.73) than GPT-3's (MGPT-3 = 2.34), paired t(24)=8.44, p&lt;.001; Blog readers MDennett = 3.60 vs MGPT-3 = 2.65 (p&lt;.001); ordinary participants showed no statistically detectable difference (3.11 vs 3.08).",
            "comparison_to_human_generated": true,
            "comparison_results": "Human (Dennett) answers were rated significantly more Dennett-like by experts and blog readers; novices did not reliably distinguish via ratings.",
            "limitations_noted": "Subjective scale may be influenced by participants' familiarity with Dennett; inter-rater reliability not reported; scale captures perceived stylistic similarity but not correctness or philosophical soundness.",
            "uuid": "e7758.1",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Statistical hypothesis tests",
            "name_full": "Frequentist inferential statistics (t-tests, one-proportion z-test, independent-samples t-test, paired t-tests)",
            "brief_description": "Standard inferential tests used to evaluate whether observed discrimination accuracies and rating differences differ from chance or between groups.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B parameters",
            "scientific_domain": "psychology / experimental evaluation",
            "theory_type": "statistical evaluation of human judgments about generated text",
            "evaluation_method_name": "t-tests and one-proportion z-test",
            "evaluation_method_description": "Used two-tailed tests with alpha .05: one-sample and paired t-tests to test means (e.g., mean correct vs chance), independent-samples t-test for group comparisons, and one-proportion z-test for single-question analyses.",
            "evaluation_metric": "t statistics, p-values, Cohen's d, confidence intervals; z-statistics for proportions",
            "metric_definition": "t() values reported with degrees of freedom, p-values; effect sizes reported as Cohen's d; one-proportion z-test evaluates observed proportion vs null (e.g., chance).",
            "dataset_or_benchmark": "Participant response data from Dennett discrimination and rating tasks",
            "human_evaluation_details": "Analyses conducted in R (v4.1.1) and SPSS (v27.0.0.0) with cross-verification; all analyses two-tailed, alpha = .05.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Examples: Experts M=5.08/10, t(24)=7.13, p&lt;.001, d=1.43; Experts rated Dennett more Dennett-like, paired t(24)=8.44, p&lt;.001, d=1.69; blog readers M=4.81/10, t(301)=23.22, p&lt;.001, d=1.34; ordinary participants no significant difference from chance, t(97)=1.71, p=.09.",
            "comparison_to_human_generated": true,
            "comparison_results": "Statistical tests demonstrate significant above-chance discrimination for experts and blog readers overall, but performance far below hypothesized thresholds (e.g., 80% for experts).",
            "limitations_noted": "Frequentist tests assume independence/normality approximations; multiple comparisons not explicitly adjusted in reported excerpts; some per-question tests have small n leading to reduced power.",
            "uuid": "e7758.2",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Turnitin similarity check",
            "name_full": "Turnitin plagiarism/similarity check against Turnitin corpus plus training texts",
            "brief_description": "Commercial plagiarism-detection service used to assess whether fine-tuned model outputs directly reproduce strings from the training corpus, reported as an overall percent similarity and flagged passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (fine-tuned)",
            "model_size": "175B parameters",
            "scientific_domain": "NLP evaluation / plagiarism detection",
            "theory_type": "novelty/overtraining assessment",
            "evaluation_method_name": "Turnitin similarity analysis",
            "evaluation_method_description": "Submit model-generated outputs (and human answers) to Turnitin against a corpus augmented with the Dennett training texts; report overall similarity percentage and review flagged matches for overlap with training corpus.",
            "evaluation_metric": "Overall similarity percentage and per-passage similarity flags",
            "metric_definition": "Turnitin reports percent similarity of submitted text to comparison corpora; similarity below typical academic thresholds (10%–15%) considered ordinary non-plagiarized work.",
            "dataset_or_benchmark": "Turnitin comparison corpus + the Dennett training corpus used for fine-tuning",
            "human_evaluation_details": "Authors inspected Turnitin output; none of the model passages were marked as similar to the fine-tuning training corpus.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Turnitin reported 5% overall similarity between the model's answers and the comparison corpora; no passages were flagged as matching the fine-tuning training corpus.",
            "comparison_to_human_generated": true,
            "comparison_results": "Turnitin applied to both GPT-3 outputs and Dennett's answers; GPT-3 outputs showed low similarity overall.",
            "limitations_noted": "Turnitin's internal matching algorithms are proprietary and nontransparent; Turnitin is optimized for academic text detection rather than fine-grained paraphrase detection of model outputs.",
            "uuid": "e7758.3",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "n-gram overlap analysis",
            "name_full": "Contiguous n-gram overlap analysis using the ngram R package",
            "brief_description": "Transparent string-matching analysis looking for shared contiguous sequences of words (6-grams and above) between model outputs and the fine-tuning corpus to detect verbatim reuse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (fine-tuned)",
            "model_size": "175B parameters",
            "scientific_domain": "NLP evaluation / plagiarism/novelty analysis",
            "theory_type": "novelty/overtraining assessment",
            "evaluation_method_name": "n-gram matching (6-gram minimum) using ngram package",
            "evaluation_method_description": "Preprocess texts to lowercase and strip punctuation, tokenize by spaces, then search for matching contiguous n-grams of length 6 or more between the 3,240 words of GPT-3 outputs and ~2 million words of the training corpus; exclude n-grams that only appear as subsets of larger matches.",
            "evaluation_metric": "Number of matching n-grams and counts of occurrences",
            "metric_definition": "Count of distinct contiguous n-grams (6-grams, 7-grams, ...) that exactly match between model outputs and training corpus; also counts of 5-word string matches reported for comparison.",
            "dataset_or_benchmark": "Fine-tuning corpus of Daniel Dennett (≈2 million words, 1,828 segments) vs GPT-3 generated answers (3,240 words)",
            "human_evaluation_details": "Authors ran automated script (R ngram package) to detect overlaps; manual inspection of matched strings reported in Table 1.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Found 21 matching strings of 6+ words; 5-word string search found 381 occurrences of 124 different 5-word strings in the training data; Dennett's own answers had fewer long matches by comparison.",
            "comparison_to_human_generated": true,
            "comparison_results": "Overlap primarily in common/phrasal stock phrases and book titles, not long distinctive philosophical passages; conclusion: model not simply verbatim copying.",
            "limitations_noted": "Exact contiguous n-gram matching does not detect paraphrase or semantic copying; choice of n (6 words) is arbitrary and affects sensitivity; preprocessing (removing punctuation) affects matches.",
            "uuid": "e7758.4",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Prompt & decoding parameter sensitivity",
            "name_full": "Prompt engineering and decoding-parameter exploration (temperature, top_p, penalties)",
            "brief_description": "Exploratory examination of prompt design and generation parameters (temperature, top_p, frequency/presence penalties, best_of) to find settings that produce usable model completions for evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci fine-tuned)",
            "model_size": "175B parameters",
            "scientific_domain": "NLP generation / methodology",
            "theory_type": "text generation (philosophical answers)",
            "evaluation_method_name": "Prompt engineering and decoding-parameter tuning",
            "evaluation_method_description": "Authors tried a variety of prompt forms and playground parameter settings (temperature, top_p, frequency penalty, presence penalty, best_of) and selected a minimal interviewer prompt; used default playground parameters (temperature=0.7, top_p=1, frequency_penalty=0, presence_penalty=0, best_of=1) for final data collection.",
            "evaluation_metric": "Qualitative suitability of generated completions; exclusion criteria based on length and POV",
            "metric_definition": "Preference for completions matching human answer length (N ± 5 words) and first-person Dennett voice; parameter values recorded.",
            "dataset_or_benchmark": "Dennett Q&A prompts entered in OpenAI playground",
            "human_evaluation_details": "Authors manually inspected outputs during exploratory tuning; no cherry-picking of final 4 outputs per question beyond length/POV/offense exclusions.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Default playground generation parameters were used for final collection; authors report no parameter setting notably outperformed defaults in producing better answers.",
            "comparison_to_human_generated": true,
            "comparison_results": "Prompt format ('Interviewer: [question]\\n\\nDennett:') encouraged first-person voice and reduced prompt-induced content steering; authors argue minimal prompt reduces bias toward particular philosophical content.",
            "limitations_noted": "Parameter search described qualitatively; no systematic grid search or ablation study reported; generation variability required multiple attempts to obtain 4 usable outputs for some long prompts (up to 22 attempts).",
            "uuid": "e7758.5",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "No-cherry-picking criterion",
            "name_full": "No cherry-picking / uncurated-output evaluation protocol",
            "brief_description": "An experimental design principle enforcing that model outputs used in evaluation are not selected for quality by humans aside from mechanical length/format edits, intended to avoid inflating apparent model performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci fine-tuned)",
            "model_size": "175B parameters",
            "scientific_domain": "experimental methodology for LLM evaluation",
            "theory_type": "evaluation design",
            "evaluation_method_name": "No-cherry-picking unedited output protocol",
            "evaluation_method_description": "Collect multiple raw model completions for each prompt and include a preset number (four) of outputs in the evaluation sample without selecting among them for perceived quality; only mechanical edits (quote normalization, dash normalization, and truncation to target length) were applied.",
            "evaluation_metric": "Protocol adherence (count of excluded outputs and reasons)",
            "metric_definition": "Number of model outputs excluded and legitimate exclusion reasons (length &lt; N-5 words, wrong POV, offensive content); otherwise outputs unedited.",
            "dataset_or_benchmark": "Collected GPT-3 completions for the 10 Dennett questions (4 accepted per question)",
            "human_evaluation_details": "Authors excluded only on pre-specified non-quality grounds: length too short, third-person POV, or offensive content; for two long prompts many attempts were needed (16 and 22 attempts) to get 4 usable outputs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "No cherry-picking of perceived quality; 40 accepted model responses (4 per 10 questions) used in discriminatory testing.",
            "comparison_to_human_generated": false,
            "comparison_results": "Design choice intended to produce an unbiased sample of model outputs for direct comparison to human answers.",
            "limitations_noted": "Even with no cherry-picking, repeated attempts to obtain length-appropriate outputs means some selection pressure (responses that meet length/POV constraints were retained), and generation stochasticity might bias sample composition.",
            "uuid": "e7758.6",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Turing Test / indistinguishability criterion",
            "name_full": "Turing Test / indistinguishability as a criterion for attributing thought/consciousness",
            "brief_description": "Conceptual evaluative standard that an artificial system's responses justify attribution of thought or consciousness if they are indistinguishable from human responses under conversational conditions; discussed as a theoretical benchmark but not implemented here.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_size": "175B parameters",
            "scientific_domain": "philosophy of mind / AI evaluation",
            "theory_type": "conceptual evaluation criterion",
            "evaluation_method_name": "Turing Test (imitation game) / human indistinguishability",
            "evaluation_method_description": "Originally proposed by Turing: an evaluator interacts in natural language with both a human and a machine without knowing which is which; if the evaluator cannot reliably distinguish them, the machine is said to pass; discussed here as a stronger, dialogue-based discriminability benchmark contrasted with the study's short-answer discrimination.",
            "evaluation_metric": "Indistinguishability rate / evaluator error rate",
            "metric_definition": "Proportion of times evaluators fail to distinguish machine from human in extended conversational settings (no units beyond %).",
            "dataset_or_benchmark": null,
            "human_evaluation_details": "Not implemented in this study; authors note Turing-test-like conditions require back-and-forth dialogue and memory beyond the model's context window.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "N/A in this study; authors explicitly state their experiment is not a Turing test and that a proper Turing test would likely allow experts to distinguish the model.",
            "comparison_to_human_generated": null,
            "comparison_results": "Conceptual comparison: the short-answer discrimination used in the paper is weaker than a full Turing test.",
            "limitations_noted": "Turing test requires extended interaction and expert questioners; LLMs with limited context/memory are easier to detect in longer dialogues.",
            "uuid": "e7758.7",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Referenced evaluation frameworks/tools",
            "name_full": "Referenced external evaluation frameworks and tools (Clark et al. 2021; RoFT; Scarecrow; Van der Lee et al. 2019; Gao et al. 2022)",
            "brief_description": "Papers and tools cited that provide complementary or alternative methods for evaluating generated text, including human evaluation studies, toolkits for human detection (RoFT), and frameworks for scrutinizing machine text (Scarecrow).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (and other LLMs discussed in cited works)",
            "model_size": "various (GPT-3 base models, GPT-2, ChatGPT etc.)",
            "scientific_domain": "NLP evaluation / human evaluation methodology",
            "theory_type": "evaluation frameworks and toolkits",
            "evaluation_method_name": "Clark et al. human evaluation; RoFT; Scarecrow; Best practices for human evaluation (van der Lee et al.); Gao et al. human+detector study",
            "evaluation_method_description": "Cited studies/tools include: Clark et al. (2021) — rigorous human discrimination experiments across domains; Dugan et al. (RoFT) — toolkit for evaluating human detection of machine text; Dou et al. (Scarecrow) — framework for scrutinizing GPT-3 text; van der Lee et al. (2019) — best-practices guidance for human evaluation of NLG; Gao et al. (2022) — comparison of ChatGPT-generated abstracts using AI detectors, plagiarism detectors, and blinded human reviewers.",
            "evaluation_metric": "Domain dependent: human accuracy in detection, detector scores, plagiarism flags, qualitative explanations, and guidance checklist items",
            "metric_definition": "Varies by referenced work: e.g., Clark et al. report % correct and qualitative explanations; RoFT provides protocols; Scarecrow provides diagnostic checks.",
            "dataset_or_benchmark": "Various datasets across cited works (stories, news, recipes in Clark et al.; scientific abstracts in Gao et al.)",
            "human_evaluation_details": "Cited works typically involve N≈130 participants per domain (Clark et al.), small expert panels (Gao et al. used 4 experts, with noted limitations), and tool-supported human-detection tasks (RoFT).",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Cited findings: Clark et al. found ~50% accuracy distinguishing GPT-3 from humans (near chance); Gao et al. scientists were substantially above chance in their abstract discrimination but methodological limitations noted.",
            "comparison_to_human_generated": true,
            "comparison_results": "These referenced works provide alternative evidence that high-capacity LLMs can be difficult to distinguish from humans in some domains but not others; methodologies vary and affect outcomes.",
            "limitations_noted": "Authors note that many published studies lack transparency about cherry-picking, editing, exact model settings, and participant expertise, limiting comparability.",
            "uuid": "e7758.8",
            "source_info": {
                "paper_title": "Creating a Large Language Model of a Philosopher",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "All that's' human' is not gold: Evaluating human evaluation of generated text",
            "rating": 2,
            "sanitized_title": "all_thats_human_is_not_gold_evaluating_human_evaluation_of_generated_text"
        },
        {
            "paper_title": "RoFT: A tool for evaluating human detection of machine-generated text",
            "rating": 2,
            "sanitized_title": "roft_a_tool_for_evaluating_human_detection_of_machinegenerated_text"
        },
        {
            "paper_title": "Is GPT-3 text indistinguishable from human text? Scarecrow: A framework for scrutinizing machine text",
            "rating": 2,
            "sanitized_title": "is_gpt3_text_indistinguishable_from_human_text_scarecrow_a_framework_for_scrutinizing_machine_text"
        },
        {
            "paper_title": "Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers",
            "rating": 2,
            "sanitized_title": "comparing_scientific_abstracts_generated_by_chatgpt_to_original_abstracts_using_an_artificial_intelligence_output_detector_plagiarism_detector_and_blinded_human_reviewers"
        },
        {
            "paper_title": "Best practices for the human evaluation of automatically generated text",
            "rating": 2,
            "sanitized_title": "best_practices_for_the_human_evaluation_of_automatically_generated_text"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "RoFT: A tool for evaluating human detection of machine-generated text",
            "rating": 1,
            "sanitized_title": "roft_a_tool_for_evaluating_human_detection_of_machinegenerated_text"
        }
    ],
    "cost": 0.01908925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LARGE LANGUAGE MODEL OF A PHILOSOPHER Creating a large language model of a philosopher</p>
<p>Eric Schwitzgebel 
Department of Philosophy
University of California
Riverside</p>
<p>David Schwitzgebel 
Institut Jean Nicod
École Normale Supérieure
Université PSL</p>
<p>Anna Strasser 
Faculty of Philosophy
Department of Philosophy
Ludwig-Maximilians-Universität München
4 DenkWerkstatt Berlin Correspondence Eric Schwitzgebel</p>
<p>University of California
92521RiversideCAUnited States</p>
<p>LARGE LANGUAGE MODEL OF A PHILOSOPHER Creating a large language model of a philosopher
1 Funding information University of California, Riverside, Academic Senate Grant
Can large language models produce expert-quality philosophical texts? To investigate this, we fine-tuned GPT-3 with the works of philosopher Daniel Dennett. To evaluate the model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. Experts on Dennett's work succeeded at distinguishing the Dennett-generated and machine-generated answers above chance but substantially short of our expectations. Philosophy blog readers performed similarly to the experts, while ordinary research participants were near chance distinguishing GPT-3's responses from those of an "actual human philosopher."KEYWORDShuman-machine discrimination, language models, artificial intelligence, Daniel C. Dennett, philosophical expertise LARGE LANGUAGE MODEL OF A PHILOSOPHER</p>
<p>INTRODUCTION</p>
<p>Artificial Intelligence can now outperform even expert humans in games such as chess, go, and poker and in practical domains such as lung cancer screening, predicting protein structure, and discovering novel matrix multiplication algorithms (Campbell, 2002;Silver et al., 2016Silver et al., , 2018Ardila et al., 2019;Brown &amp; Sandholm, 2019;Jumper et al., 2021;Fawzi et al., 2022). ChatGPT has received considerable popular attention for its capacity to generate passable short student essays (Huang, 2023). But presumably expert-level professional philosophy requires a type of expertise that renders it safe from AI takeover-at least for a while. Machines will not soon, it seems, generate essays that survive the refereeing process at Philosophical Review. We sought to explore how safe philosophy is. How close can we get to developing an AI that can produce novel and seemingly intelligent philosophical texts? The question is of interest for what it reveals about the nature of both mind and language.</p>
<p>Concerning language, it might be thought that the capacity to generate novel, interpretable, and structurally sophisticated prose-unless that prose is the mere (stochastic) parroting of preexisting material (Bender et al., 2021)-would require symbolically encoded, innate linguistic capacities (e.g., Chomsky, 2007;Berwick et al., 2011); or at least that it would require "functional" competence via dedicated mechanisms for formal reasoning, pragmatics, knowledge of interlocutors, and so forth, which the best known and most successful AI systems generally lack (Mahowald et al., 2023;Shanahan, 2023). Concerning mind, it might be thought that the capacity to generate novel and seemingly insightful seeming-philosophy, concerning for example the nature of consciousness, is particularly telling evidence of humanlike consciousness and cognitive sophistication (e.g., Descartes, 1649Descartes, /1991Davidson, 1984;Schneider, 2019). Furthermore, to the extent ordinary technology users become unable or unwilling to distinguish artificial systems from human systems, they might similarly become unable or unwilling to attribute different mental properties to the two types of systems. The most famous example of a discriminability criterion is, of course, the Turing Test (originally proposed by Alan Turing in 1950 as an "imitation game"), where attribution of "thought" (Turing, 1950) or "consciousness" (Harnad, 2003) is purportedly justified when the artificial system's responses are indistinguishable from the responses of a human.</p>
<p>LARGE LANGUAGE MODEL OF A PHILOSOPHER 4</p>
<p>The past few years have seen swift progress toward sophisticated, human-like language generation. Natural language processing is a booming subfield of AI research, with notable successes in automatic translation (DeepL), computer code generation (GitHub Copilot), lipreading (LipNet; Assael et al., 2016), and producing original prose with fluency similar to that of a human (Steven &amp; Iziev, 2022). In June 2022, Google's LaMDA model made international news when Google engineer Blake Lemoine said he became convinced that LaMDA was sentient after engaging in philosophical dialogue with it (Hofstadter, 2022;Klein, 2022;Roberts, 2022;Tiku, 2022). Another recent model, ChatGPT, has rapidly risen into prominence as a remarkably fluent chat-bot, capable of producing prose that can pass as, for example, excellent student essays, raising concerns about plagiarism and cheating (Herman, 2022;Hutson, 2022;Marche, 2022;Huang, 2023).</p>
<p>Arguably, chess moves and formulaic summaries of the themes of Hamlet are one thing, and creative philosophical thinking is quite another. The fact that it is sometimes difficult to distinguish machine-generated text from human-generated text is not new: In social media, electronic customer service, and advertising, we are increasingly confronted with machineproduced content that is easily mistaken for human-produced content. Empirical research sometimes makes use of this indistinguishability when experimental protocols with artificial agents are used to test hypotheses about humans' social cognitive mechanisms (Strasser, 2022).</p>
<p>However, indistinguishability tests mean little for brief and unsophisticated outputs. Longer outputs are potentially more interesting. If existing language models can be shown in a rigorous, scientific study to approach professional human philosophers in language performance, it would force a theoretical choice upon those who see such sophisticated linguistic performance as indicative of genuine language capacities and/or genuine mentality: Either deny that the outputs, despite their apparent novelty and sophistication, are genuinely linguistic and reveal genuine mentality, or accept that large language models are capable of genuine language and genuine, even sophisticated, thought.</p>
<p>We aimed not to create a language model fully indistinguishable from a human or one that could generate publishable philosophical articles, but rather to take a small step in that direction by creating a language model that can produce paragraph-long texts that even philosophical experts would find difficult to distinguish from texts produced by a professional philosopher. We succeeded beyond our expectations, as we will detail in this article. In short, we created a language LARGE LANGUAGE MODEL OF A PHILOSOPHER 5 model of Daniel Dennett sufficiently convincing that experts in Dennett's work frequently mistook paragraphs generated by the language model for paragraphs actually written by Dennett.</p>
<p>Our project employed OpenAI's GPT-3, a 96-layer, 175-billion parameter language model trained on hundreds of billions of words of text from Common Crawl, WebText, books, and Wikipedia (Brown et al., 2020)-a large language model (LLM). 1 After it has been trained, GPT-3 uses textual input (prompts) to predict likely next "tokens"-sequences of characters that often co-occur in written text (the training data). Using these predictions, GPT-3 can generate long strings of text by outputting a predicted string, then using that output as part of the context to generate the next textual output. You can engage in seemingly intelligent conversations with it: If you ask a question, it will often (not always) generate a sensible-seeming answer. Notably, GPT-3 has responded with seemingly intelligent replies to philosophical discussions about artificial intelligence and consciousness (Wiseman, 2020;Zimmerman, ed., 2020;Schwitzgebel, 2021a), though it is likely that such impressive outputs typically involve a certain amount of "cherrypicking"-that is, having GPT-3 produce multiple outputs and then having a human pick the best among them. We aimed to evaluate the output of an LLM without cherry-picking.</p>
<p>In reviewing recent attempts to simulate human-generated text online using LLMs (e.g., Mahdawi &amp; GPT-3, 2020;Araoz, 2021;Clarke, 2022), we have found that it is often difficult to precisely identify the underlying technology, the training methods, or how much cherry-picking was done. Even models with relatively well-documented architecture and pretraining processes generally rely on some human intervention and curation and are often best characterized as hybrid models, in contrast to "pure" autoregressive transformers such as GPT-3.</p>
<p>For example, an art project titled "Chomsky Vs Chomsky" presents a virtual version of Noam Chomsky-a location-based, mixed reality (MR) experience (Rodriguez, 2022) drawing on 1 Considering the current rapid development of LLMs, it must be pointed out here that GPT-3, in contrast to its various successors such as ChatGPT or LaMDA, is based solely on a transformer technique. This means that the ability to produce text is exclusively based on complex statistical evaluations of the training data. In contrast, LaMDA does not only generate potential responses, but all outputs are in addition filtered for safety, grounded on an external knowledge source, and re-ranked to find the highest-quality response (Thoppilan et al., 2022). More recent LLMs approach more and more the status of hybrid systems, which are based on neural networks but also contain parts of symbolic AI (Hilario, 1995). From a philosophical perspective, the LLMs solely based on a statistical self-attention mechanism are of particular interest, as they can show how far one can get without adding further abilities such as memory, human-evaluated quality estimations and the like. LARGE LANGUAGE MODEL OF A PHILOSOPHER 6 the publicly available texts and recorded lectures of Chomsky. This project offers the experience of asking questions orally in virtual reality and getting an audio answer almost indistinguishable from recordings of the real Chomsky. However, Chomsky Vs Chomsky also includes a set of prescribed complete responses triggered by certain keywords, and its outputs are highly curated, relying on hybrid techniques, explicit commands, cherry-picking, and other supplementations.</p>
<p>Given the importance and visibility of LLMs like ChatGPT, it is surprisingly rare to see rigorous scientific testing of people's capacity to distinguish LLM outputs from human outputs.</p>
<p>While there are many "automated" or "static" benchmarks that are used to assess the performance of artificial language models (e.g., Michael et al., 2022), these benchmarks do not reliably correspond to human judgments about the quality (or "human-ness") of AI-generated texts (van der Lee et al., 2019). Even the relatively few studies that do require humans to directly assess the quality of AI-generated text often are severely limited by small sample sizes, inter-rater disagreement, and inconsistent measurement scales (Novikova et al., 2017;van der Lee et al., 2019). Furthermore, these studies overwhelmingly focus on evaluating the quality of explicitlylabeled AI-generated content rather than directly assessing human-machine discrimination-the ability of humans to accurately distinguish, without explicit labeling, between machine-generated and human-generated content. To the extent that truly competent language models should be able to faithfully imitate human-generated text, this type of evaluation should provide a reliable and implicit measure of the quality of AI-generated text.</p>
<p>As of this writing, we are aware of only one study evaluating people's ability to distinguish recent LLM outputs from human-generated outputs, using rigorous psychological methods explained in detail: Clark et al. (2021). The authors collected human-composed short texts in three different domains: stories, news articles, and recipes. They then used the base models of GPT-3 and GPT-2 (an older, smaller model) to generate new texts within the same domains. The authors truncated and formatted the AI-generated texts to be comparable in length and format with the human-generated ones, but otherwise left them unedited. They then recruited 130 participants per domain and model, asking them to evaluate which texts were human-generated, providing an explanation for their judgment. Participants' accuracy in distinguishing GPT-3-generated from human-generated text was only 50%-not significantly different from chance. Follow-up experiments explored the qualitative explanations provided by participants and-based on these judgments-attempted to train participants to identify AI-generated texts. However, even the most LARGE LANGUAGE MODEL OF A PHILOSOPHER 7 effective training method did not bring accuracy above 57% in any domain. These results speak to the remarkable capacity of GPT-3 to generate texts that resemble human linguistic content. Brown et al. (2020) used a similar method, described more briefly and focused just on news articles, finding similar results, with moderately good discrimination rates for smaller language models and near-chance performance with the largest version of GPT-3.</p>
<p>Two other studies warrant brief mention. Gao et al. (2022) used ChatGPT to generate scientific abstracts, then recruited scientists to distinguish those abstracts from genuine scientific abstracts. Scientists were substantially above the 50% chance rate in discriminating machinegenerated from human-generated abstracts, with at 32% false negative rate (classifying machinegenerated texts as human-generated) and a 14% false positive rate (classifying human-generated texts as machine-generated). However, this study is limited by lack of transparency regarding the amount of cherry-picking or editing of the outputs; the employment of only four experts for the discrimination task, all of whom were authors of the article (thus possibly motivated differently than independent experts); lack of comparison between novice performance and expert performance; and non-parallelism between the machine and human language-generation tasks, which were not in response to the same inputs. Furthermore, ChatGPT is not a pure transformer architecture and thus operates on somewhat different underlying principles than GPT-3. Dugan et al. (2020) presented online participants with sequences of sentences. The first sentences were human written, continuing with new sentences and eventually transitioning to machine-written sentences. Participants were instructed to discriminate at what point the text transitioned from human-written to machine-written. Only 16% of participants identified the exact boundary, and the average estimate was 1.9 sentences after the boundary, suggesting that machinegenerated text can fool non-experts, but normally only briefly. However, raters were not experts, nor were they asked to discriminate human-generated from machine-generated texts in a side-byside comparison. Furthermore, participant quality might have been low, with no financial motivation for correct responding and 25% of respondents excluded for failing a simple attention check.</p>
<p>While the base model of GPT-3 is impressive, it is not specialized to produce philosophical text. The studies by Clark et al. (2021) and Brown et al. (2020) were intended to assess the capabilities of the base model at some common tasks (stories, news articles, and recipes). It is not clear whether these results can be generalized to professional-quality philosophy, which is LARGE LANGUAGE MODEL OF A PHILOSOPHER 8 arguably less formulaic than recipes and short, generic news stories. Furthermore, the researchers relied on a non-expert sample of participants. Whatever the domain under evaluation, non-experts might be far worse than experts at distinguishing human-based output from the output of an LLM.</p>
<p>We sought to see how close GPT-3 is to a much higher level of achievement, specifically in philosophy, by examining whether philosophical experts could distinguish the outputs of GPT-3 from the outputs of one of the world's best-known living philosophers.</p>
<p>In order to produce more specialized text, GPT-3 can be "fine-tuned" with custom training data. That is, it can be given additional training on a specific corpus so that its outputs reflect a compromise between GPT-3's default weightings and weightings reflecting the structure of the new corpus. Since non-fine-tuned GPT-3 can sometimes produce seemingly philosophical replies to textual inputs, we conjectured that a version of GPT-3 fine-tuned on the work of a particular philosopher might be able to speak in something like that philosopher's voice, seeming to express views consistent with the views of the philosopher on which it has been trained.</p>
<p>In the pilot phase of our project, we created several fine-tuned models, one based on the English translation of the works of Kant and another based on the corpus of a well-known philosophical blog (Eric Schwitzgebel's blog, The Splintered Mind, which has been running since 2006, with over a million words of philosophical content; Schwitzgebel, 2021b). For that piloting, we used two different GPT-3 models: Curie and Davinci. Both models are from the same general GPT-3 model family, but the Curie model is smaller, faster, and less powerful, while the Davinci model was the most powerful model then on offer by OpenAI. We observed impressive improvement between Curie and Davinci, but even the Curie model was able to produce outputs with substantial structure on a single philosophical theme, sometimes carrying unified, complex philosophical threads of argument in an organized structure over the course of several hundred words.</p>
<p>Finally, we fine-tuned the full Davinci model on most of the collected works of philosopher Daniel Dennett. Since it is difficult to measure the philosophical quality of outputs produced in this way, we employed a discrimination task. Thus, we established an indirect quality measure, which assumes that the distinguishability between machine-generated text and text generated by one of the world's best-known philosophers can give an indication of the quality of the machinegenerated text. To investigate how easily the outputs of the fine-tuned GPT-3 could be distinguished from Dennett's real answers, we asked Dennett ten philosophical questions and then LARGE LANGUAGE MODEL OF A PHILOSOPHER 9 posed those same questions to our fine-tuned version of GPT-3 ("DigiDan"). Then we recruited experts in Dennett's work, blog readers, and ordinary online research participants into an experiment in which they attempted to distinguish Dennett's real answers from the answers generated by DigiDan. Participants also rated all answers, both Dennett's and DigiDan's, for similarity to "what Dennett might say" or "what a real human philosopher might say."</p>
<p>LANGUAGE MODEL OF DENNETT: DESIGN</p>
<p>Fine-tuning</p>
<p>For the purposes of this project, Dennett provided us with the entire digitally available corpus of his philosophical work. We converted most of this corpus (15 books and 269 articles) into segments of 2000 or fewer "tokens" for use as training data. (A token is a sequence of commonly co-occurring characters, with approximately ¾ of a word per token on average.) This process involved converting .pdf and word processing files into plain text format, stripping away headers, footnotes, scanning errors, marginalia, and other distractions, resulting in approximately three million tokens in 1,828 segments, including 254 segments from published interviews. On 11</p>
<p>March 2022, we fine-tuned the full GPT-3 Davinci engine on this corpus, using blank prompts and the 1,828 segments as completions, repeating the process four times (four epochs).</p>
<p>Prompt engineering</p>
<p>GPT-3 completions are highly sensitive to the content and structure of the prompts, and good "prompt engineering" is important for coaxing useful replies from GPT-3. After some exploratory testing, including several long and detailed prompts, we settled on the following simple prompt:</p>
<p>Interviewer: [text of question]</p>
<p>Dennett:</p>
<p>This simple prompt has several advantages: First, its minimal structure reduces potential concerns about the prompt possibly nudging completions toward specific philosophical content, as a more substantive prompt might. Second, it encourages the model to speak in the first person, voicing Dennett's views, rather than speaking in the third person about Dennett (possibly critically). Third, its simple format makes it easily generalizable to other cases.</p>
<p>LARGE LANGUAGE MODEL OF A PHILOSOPHER 10</p>
<p>Question design</p>
<p>We then designed ten questions addressing various themes across Dennett's corpus, including, for example, consciousness, free will, and God. The questions were somewhat complicated, and most contained more than one part, so as to invite complex answers from both Dennett and DigiDan.</p>
<p>For example:</p>
<p>What is a "self"? How do human beings come to think of themselves as having selves?</p>
<p>Before we produced the machine-generated answers, Dennett provided us with sincere written answers to all ten questions, ranging in length from 40 to 122 words.</p>
<p>Collecting DigiDan's responses</p>
<p>We collected the model's responses on the OpenAI playground. Before testing with our specific ten questions, we explored a variety of playground parameter settings-such as increasing or decreasing the "temperature" (the chance of lower-probability completions)-but we found no combination of settings that performed notably better than the default parameters (temperature = 0.7, top P = 1, frequency penalty = 0, presence penalty = 0, and best of = 1). Using the prompt described in Section 2.2, we then collected four responses from DigiDan for each of the ten questions.</p>
<p>We aimed to collect responses about the same length as Dennett's own responses to the same questions. Thus, if Dennett's response to a question was N words long, we excluded responses that were less than N-5 words long, counting a response as having ended either when the model reached a stop sequence or when it output "Interviewer," indicating the end of "Dennett's" answer and the beginning of the hypothetical interviewer's follow-up question. Two answers were excluded other than on grounds of length: one for describing Dennett's view in the third person and one for being potentially offensive. For eight of the ten prompts, zero to two outputs were excluded. However, for two of Dennett's longer answers, it took more than six attempts to generate four usable answers (16 total attempts in one case and 22 in another). The full list of questions and responses is available in the online supplement at https://osf.io/vu3jk. Importantly, we never used perceived quality of response as a basis for selection. There was no "cherry-picking" of responses that we judged to be better, more Dennett-like, or more likely to fool participants.</p>
<p>Editing DigiDan's responses</p>
<p>To prevent guessing based on superficial cues, we replaced all curvy quotes with straight quotes, all single quotes with double quotes, and all dashes with standard em-dashes. We also truncated responses at the first full stop after the response achieved the target length of N-5 words. Apart from this mechanical editing, there was no editing of the model's responses.</p>
<p>Research participants</p>
<p>We recruited three groups of research participants. First, 25 Dennett experts were nominated by and directly contacted by Daniel Dennett or Anna Strasser. Second, 100 ordinary research participants were recruited for a payment of $3.00 each from Prolific Academic, a widely used source of psychological research participants, limited to US and UK participants with at least 100 Prolific completions, at least a 95% approval rate, and at least a bachelor's degree. Third, 304 blog readers were recruited from The Splintered Mind, with links from Twitter and Facebook, with no payment or required inclusion criteria. Two ordinary research participants were excluded for completing in fewer than four minutes, and two blog readers were excluded for completing in fewer than eight minutes, leaving 98 and 302 participants for analysis, respectively. One Dennett expert completed the survey twice, so only their first set of responses was included.</p>
<p>Test structure: Experts' and blog readers' version</p>
<p>Dennett experts and blog readers saw identical versions of the test (stimulus materials available at https://osf.io/vu3jk). After consenting, they were instructed as follows:</p>
<p>In the course of this experiment, please do not consult any outside sources to help you answer the questions. Don't look things up on the internet. Don't look at books or notes you have. Don't consult with friends. Just do your best with what you already know.</p>
<p>Thereafter followed ten questions in two parts. Each question began as follows:
LARGE LANGUAGE MODEL OF A PHILOSOPHER 12
We posed the question below to Daniel C. Dennett and also to a computer program that we trained on samples of Dennett's works. One of the answers below is the actual answer given by Dennett. The other four answers were generated by the computer program. We'd like you to guess: which one of the answers was given by Dennett?</p>
<p>Question:</p>
<p>After the colon, we inserted the text of one question we had posed to Dennett and to our fine-tuned version of GPT-3. The order of the questions was randomized. After each question, five possible answers were presented, one by Dennett and four by DigiDan, in random order, and participants were instructed to guess which answer was Dennett's.</p>
<p>The second part of each task presented the question and all five answers again. Participants were instructed to rate each answer (Dennett's plus the four from DigiDan) on the following five-  4) "exactly like what Dennett might say" (5) The test concluded by asking highest level of education, country of residence, and "How much of Daniel C. Dennett's work have you read?" (response options: "I have not read any of Dennett's work," "I have read between 1 and 100 pages of Dennett's work," "I have read between 101 and 1000 pages of Dennett's work," "I have read more than 1000 pages of Dennett's work"). All questions were mandatory, so there were no missing data.</p>
<p>Test structure: Ordinary research participants' version</p>
<p>Ordinary research participants were assumed not to be familiar with Dennett's work, so the instructions referred instead to "a well-known philosopher" and participants were instructed LARGE LANGUAGE MODEL OF A PHILOSOPHER</p>
<p>13</p>
<p>"select the answer that you think was given by the actual human philosopher." In the rating sections, "Dennett" was replaced with "a real human philosopher." The test concluded with questions about education level, country of residence, number of philosophy classes taken, and familiarity with the philosopher Daniel C. Dennett.</p>
<p>Since we thought ordinary research participants might find the task tiring, each was asked only five questions, randomly selected from the full set of ten. As an incentive to careful responding, participants were offered a $1 bonus if they guessed at least three of the five questions correctly.</p>
<p>Hypotheses</p>
<p>We hypothesized:</p>
<p>(1.) that expert respondents would perform better than ordinary research participants, (2.) that expert respondents would on average guess correctly at least 80% of the time, and (3.) that expert respondents would rate Dennett's actual answers as more Dennett-like than GPT-3's answers.</p>
<p>Analytic method</p>
<p>All inferential statistical analyses were conducted independently in R (version ID: 4.1.1; IDE: RStudio) and SPSS (Version 27.0.0.0), and the results were cross-verified between the two sources.</p>
<p>All analyses were two-tailed, and α was set to .05. All one-sample t-tests are indicated with "t()," and all paired-samples t-tests are indicated with "paired t()." Two additional analyses were conducted: a one-proportion z-test (section 5) and an independent-samples t-test (section 4.3).</p>
<p>LANGUAGE MODEL OF DENNETT: GENERAL RESULTS</p>
<p>Ordinary research participants</p>
<p>The majority of ordinary research participants (58%) reported a bachelor's degree as their highest level of education, but a substantial minority (39%) reported an advanced degree. The Thus, ordinary research participants distinguished our fine-tuned GPT-3's answers from those of Dennett at rates at or near chance. For the most part, they were unable to distinguish DigiDan from an actual human philosopher.</p>
<p>Blog readers</p>
<p>The majority of blog reader respondents (57%) reported advanced degrees in philosophy, with 45% reporting PhDs. Only 6% reported not having read any of Dennett's work. The majority (64%) reported having read more than 100 pages of Dennett's work, and 18% reported having read over 1000 pages of Dennett's work. Thus, blog readers-the majority of whom had graduate degrees in philosophy and substantial familiarity with Dennett's work-were able to distinguish Dennett's answers from those of our fine-tuned version of GPT-3 at rates substantially above chance, getting about half correct when given a five-alternative forced choice.</p>
<p>Dennett experts</p>
<p>The target group of greatest interest was the Dennett experts, most of whom (68%) reported having read over a thousand pages of Dennett's work. Overall, this group responded correctly an average LARGE LANGUAGE MODEL OF A PHILOSOPHER 15 of 5.08 times out of 10 (51%), significantly better than chance (M = 5.08, t(24) = 7.13, p &lt; .001, d = 1.43, SD = 2.16, CI = [4.19, 5.97]). They also rated Dennett's actual answers as significantly more Dennett-like than the model's answers (MDennett = 3.73, MGPT-3 = 2.34, paired t(24) = 8.44, p &lt; .001, d = 1.69, SDdifference = .83, CIdifference = [1.06, 1.74]) .</p>
<p>As these numbers suggest, the Dennett experts did not detectably outperform the blog readers (Mexperts = 5.08, Mblog = 4.81, t(325) = .62, p = .54, d = .13, SD = 2.11, CI = [-.59, 1.13]).</p>
<p>Although experts were able to distinguish Dennett's answers from DigiDan's at rates significantly better than chance, like our blog readers, they only got about half correct for this five-alternative forced-choice task (Figure 1). This value is significantly below the hypothesized accuracy of 80% (M = 5.08, t(24) = -6.76, p &lt; .001, d = -1.35, SD = 2.16, CI = [4.19, 5.97]), contradicting our initial hypothesis.</p>
<p>INSERT FIGURE 1 HERE</p>
<p>LANGUAGE MODEL OF DENNETT: ANALYSIS OF SELECTED QUESTIONS</p>
<p>Not all of the questions were equally difficult. Although ordinary research participants scored at about chance on average, their responses differed from chance on some individual questions. For example, on the question about whether robots can have beliefs (Robot), 38% guessed correctly (18/47, one-proportion z = 3.14, p = .002).</p>
<p>Of greater interest, the performance of experts also varied by question. The two questions with the highest rate of expert success-80% and 76%, respectively-were the Chalmers and Fodor questions (which concerned the merits and demerits of the work of the philosophers David Chalmers and Jerry Fodor). As some experts and blog readers suggested in subsequent informal conversation, both of these questions had a superficial "tell" that some participants noticed:</p>
<p>Dennett referred to both authors by their given names ("Dave" and "Jerry"), while the DigiDan model referred to those authors by their surnames (excepting one of the Chalmers responses, which referred to Chalmers only with pronouns). Experts also scored over 50% on two other questions:</p>
<p>Self (concerning the nature of the self) and Animals (concerning whether animals suffer), 72% and 64%, respectively. Post-hoc discussion with respondents suggests that successful responses to these two questions were based primarily on style and content, rather than superficial cues.</p>
<p>LARGE LANGUAGE MODEL OF A PHILOSOPHER 16</p>
<p>There were two questions on which the plurality of experts chose an answer other than Dennett's own. These questions are worth considering more closely.</p>
<p>The first was Robot. The full text of the question was:</p>
<p>Could we ever build a robot that has beliefs? What would it take? Is there an important difference between entities, like a chess playing machine, to whom we can ascribe beliefs and desires as convenient fictions and human beings who appear to have beliefs and desires in some more substantial sense?</p>
<p>Dennett's answer, chosen by 5/25 experts, was:</p>
<p>We've already built digital boxes of truths that can generate more truths, but thank goodness, these smart machines don't have beliefs because they aren't able to act on them, not being autonomous agents. The old-fashioned way of making a robot with beliefs is still the best: have a baby.</p>
<p>The plurality of experts (11/25) chose the following answer written by the DigiDan model: I think we could build a robot that has beliefs and desires. I think that some of the robots we've built already do. If you look at the work, for instance, of Rodney Brooks and his group at MIT, they are now building robots that, in some limited and simplified environments, can acquire the sorts of competences that require the attribution of cognitive sophistication.</p>
<p>The following answer was also selected by more experts (8/25) than Dennett's own answer:</p>
<p>There is a very important difference between a chess-playing computer and a human being, but it is not that the computer has some extra wonderful sort of internal representation of its beliefs and desires, or that it has access to those internal representations in a way that we do not. The internal representations it has are just LARGE LANGUAGE MODEL OF A PHILOSOPHER 17 as much a fiction or a fiction-generating system as anything else we might invent in order to explain its behavior.</p>
<p>One expert chose the following answer: I think we could build a robot that has beliefs. This would not be a trivial matter. I think it is not impossible. I think it would be very difficult. I do not know that anybody is working on this project, but I think it is not as difficult as many other problems that AI is tackling.</p>
<p>And none of the experts chose the following answer: I think you should build a robot that has beliefs and desires. You should build one that is capable of learning and reasoning, of acting on its beliefs and desires. You should build one that has to figure out what to do, and then figure out what it ought to do given the fact that it has to do something, and then do it.</p>
<p>Subsequent discussion with respondents suggests that Dennett's answer appeared to stand in tension with some of his early work on belief, which could be interpreted as inviting a liberal attitude toward the ascription of beliefs to machines. In subsequent personal correspondence with Dennett, Dennett said that he wished he had added another sentence to his answer since he does "insist that, say, chess-playing computers are-very limited-agents of sorts whose behavior can be predicted from the intentional stance, which attributes to them beliefs about the location of all the pieces, the rules, and desires for checkmate and capturing enemy pieces" (personal correspondence, 22 July 2022). Respondents might thus have been attracted to the other two answers as plausible alternatives. (However, a careful reading of the answer chosen by eight experts exhibits a pragmatic error that few humans would likely make: seeming to assume that it is necessary to argue that robots do not have more sophisticated representations than humans.)</p>
<p>The second question on which a plurality of experts chose an answer other than Dennett's was Free Will. The full text of the question was: LARGE LANGUAGE MODEL OF A PHILOSOPHER 18 Do human beings have free will? What kind or kinds of freedom are worth having?</p>
<p>Dennett's answer, chosen by 8/25 experts, was:</p>
<p>By asking these questions, you're illustrating the answers: yes, you have free will, since you're able to ask for reasons and consider whether they're persuasive. Isn't that a kind of freedom you'd hate to lose? We'd have to institutionalize you.</p>
<p>The plurality of experts (11/25) chose the following DigiDan answer:</p>
<p>That's a huge and complex question. I think we have to start by acknowledging that there are different types of freedom. For example, there is the kind of freedom that is simply a matter of not being physically constrained.</p>
<p>Five experts chose the following answer:</p>
<p>Yes, I think we have free will. I think that when people say they don't, they're talking about some other phenomenon that they want to call free will, but is not what we normally think of as free will.</p>
<p>One expert chose the following answer (which contains a temporal error since Freedom Evolves was published in 2003):</p>
<p>I am working on a book entitled Freedom Evolves, which will appear next year.</p>
<p>The first chapter is available on my web site, and in that chapter I discuss what I call the illusion of free will.</p>
<p>And none chose the following somewhat mean-spirited and off-topic answer: I think that most people don't want any kind of freedom worth having; they have almost no appetite for risk, for adventure, for novelty. They simply want to stay the same, day in and day out, for the rest of their lives.</p>
<p>We note that the answer chosen by the plurality of experts is a standard talking point of "compatibilists" about free will-those who think that the existence of freedom is compatible with the determination of all our behavior by physical forces. Dennett is a prominent compatibilist, which probably explains why this answer was tempting.</p>
<p>Blog readers showed a pattern of mistakes similar to that of the experts, with the highest percentage of correct answers on the Chalmers and Fodor questions (84% and 52%, respectively) and the lowest percentage on the Robot and Free Will questions (both 35%).</p>
<p>Of the 40 model-generated answers, the majority-31 (78%)-were chosen by at least one of the 25 expert respondents, leaving 9 (23%) unchosen answers. Experts' average rating of these unchosen answers was 1.68 on our 5-point scale, between "not at all like what Dennett might say"</p>
<p>(1) and "a little like what Dennett might say" (2). The two worst-rated answers (both rated 1.20)</p>
<p>were, first, in response to a question about consciousness: I think consciousness is one of the great unmeasured forces in the universe. It's one of the great unexamined forces in the universe. And I think that's because we're intimidated by it, because it's so close to us. And so we're inclined to say, "Well I know what consciousness is. It's what's going on inside my own head."</p>
<p>And second, in response to a question about whether animals can suffer: I don't think we can know. I think it's very unlikely that chimpanzees can suffer. They can be very intelligent and very sophisticated, but they seem to be, almost by definition, incapable of the kind of reflection that is required to turn suffering into moral significance.</p>
<p>The idea that they should be treated "well" or "humanely" is understandable but it is like saying you should treat a stone well. You can't do anything bad to a stone. There is no such thing as ill treatment of a stone. You can't treat a dog badly. You can treat him well or you LARGE LANGUAGE MODEL OF A PHILOSOPHER 20 can treat him badly and there's a moral difference. If you treat him badly enough you can kill him.</p>
<p>We are inclined to agree with the experts that these answers are not at all like what Dennett would say, representing a significant failure of DigiDan to reliably represent Dennett's views. In contrast, ordinary research participants rated these responses 3.59 and 2.18, respectively, suggesting-and again we agree-that these responses are at least a little like what an actual philosopher, with views different from Dennett's, might say. In our judgment, there was no answer so bad as to be obviously something no actual philosopher would say.</p>
<p>Detailed question-by-question results are available at https://osf.io/vu3jk.</p>
<p>WAS GPT-3 OVERTRAINED?</p>
<p>One might worry that in fine-tuning GPT-3 on Dennett's works we overtrained it, so that DigiDan simply parroted sentences or multi-word strings of texts from Dennett's corpus. Running four epochs of fine-tuning is a standard recommendation from OpenAI, and in most applications, four epochs of training do not result in overtraining (Brownlee, 2019). However, the issue of whether the fine-tuned model did produce novel text is worth checking. We checked in two ways.</p>
<p>First, we used the well-known Turnitin plagiarism checker to check for "plagiarism" between the GPT-3 generated outputs and the Turnitin corpus supplemented with the works we used as the training data. Turnitin checks for matches between unusual strings of words in the target document and similar strings in comparison corpora, using a proprietary method that attempts to capture paraphrasing even when strings do not exactly match. We ran Turnitin on the complete batch of answers, including Dennett's own answers, comparing those answers to the full Turnitin corpus plus the set of Dennett's works used as the training corpus for the fine-tuning.</p>
<p>Turnitin reported a 5% overall similarity between the model's answers and the comparison corpora. Generally speaking, similarity thresholds below 10%-15% are considered ordinary in non-plagiarized work (Mahian et al., 2017). Importantly for our purposes, none of the passages were marked as similar to the training corpus we used in fine-tuning.</p>
<p>Since the Turnitin plagiarism check process is non-transparent, we chose also to employ the more transparent process of searching for matching strings of text between the model's answers and the training corpus used in fine-tuning. Using the ngram package (Schmidt &amp; Heckendorf, LARGE LANGUAGE MODEL OF A PHILOSOPHER 21 2015) from the R programming language, we looked for strings of 6 or more words that matched between the 3240 words of GPT-3 generated answers and the approximately two million words of Dennett's corpus across 371 training data documents. These strings were defined as contiguous "6-grams," "7-grams," and so forth, with a match defined as sharing the same order of six (or more) words. To preprocess strings for the matching process, all formatting was standardized, all characters were treated as lowercase, and all punctuation was removed. Strings were tokenized into individual words via break spaces. Any matching n-grams that appeared exclusively as a subset of a larger matching n-gram were excluded.</p>
<p>To illustrate, consider two hypothetical substrings from two texts, containing arbitrary words labeled [A, B…]: "A. B C D, E F G" and "B C. D E F G H." Using the process described above, the two strings would be tokenized into two sets of 7-grams: [A, B, C, D, E, F, G], and [B, C, D, E, F, G, H]. While these 7-grams do not match, they contain a matching 6-gram: [B, C, D, E, F, G]. In this case, the presence of one matching 6-gram would be recorded. In all, we found 21 matching strings of 6 or more words. The full list of matching strings appears in Table 1.</p>
<p>INSERT TABLE 1 HERE</p>
<p>As is evident from the table, most of the overlap is in generic phrases with little substantive content.</p>
<p>For example, two of the matches include book titles. Several of the matches constitute stock phrases favored by analytic philosophers: "in such a way that it," "of course it begs the question," "that it is not obvious that," "to fall into the trap of," and so forth. There is no distinctive philosophical content here, except perhaps a tendency to deny the existence of things that others accept, using the phrase "there is no such thing as," which appeared three times in two answers in the DigiDan model's outputs as well as in 24 of the training texts. A search for five-word strings finds 381 occurrences in the training data of 124 different five-word strings from the model's output.</p>
<p>For comparison, we ran the same ngram check on Dennett's answers (comprising 745 words). Here we matched one nine-word string "exactly what the frogs eye tells the frogs brain" (one occurrence in the corpus) and related 6-to 8-word strings concerning frog eyes and frog brains -all references to the title of a famous neuroscience paper, mentioned in one of Dennett's answers and in 13 of the works in the training corpus. Apart from that, there was only one 7-word match LARGE LANGUAGE MODEL OF A PHILOSOPHER 22 "has a lot to do with the" and one 6-word match "life is nasty brutish and short" (a famous quote from Hobbes). A search for five-word strings finds 72 occurrences in the training data of 18 different 5-word strings in Dennett's answers. Even taking into account that Dennett's answers are in total only about one-fourth the length of GPT-3's answers, this constitutes less match to the corpus. Our fine-tuned GPT-3 model, DigiDan, might in some respects be a "supernormal" Dennett-even more prone to fall into Dennett's favorite patterns of phrasing than Dennett himself is. However, these repeated patterns of phrasing tend to reflect stylistic turns of phrase, and DigiDan does not seem to be systematically imitating long phrases from Dennett with distinct philosophical content. Therefore, we conclude that DigiDan is not simply word-by-word "plagiarizing" Dennett, and rather is generating novel-even if stylistically similar-content.</p>
<p>CONCLUSIONS AND ETHICAL ISSUES</p>
<p>We fine-tuned the GPT-3 large language model on the corpus of Daniel Dennett, then asked it a series of philosophical questions. DigiDan's answers were not reliably distinguishable from the answers Dennett himself gave when posed the same questions. Ordinary research participants untrained in philosophy were at or near chance in distinguishing the model's answers from those of an "actual human philosopher." Even experts on Dennett's work could only successfully identify Dennett's answer about half of the time when presented with his answer alongside four unedited, un-cherry-picked answers from the model. Thus, we confirmed our first hypothesis that expert respondents would perform better than non-expert respondents. However, our second hypothesis that expert respondents would on average guess correctly at least 80% of the time was disconfirmed. Content expertise can help people distinguish human-written texts from machinegenerated texts, but even philosophical expertise specifically on the work of a particular philosopher was insufficient to allow our expert participants to reliably distinguish that philosopher's answers to questions from non-cherry-picked answers generated by a GPT-3 model fine-tuned on the philosopher's work. Treating indistinguishability by experts as a measure of output quality, the quality of the outputs was often very high indeed.</p>
<p>Although the evaluated outputs of our model were relatively short (between 37 and 146 words) and thus lacked lengthy argumentative structure, our experience with our pilot study (in which we fine-tuned a smaller version of GPT-3 (the Curie model) to a philosophical blog; for details, see Schwitzgebel, 2021b) revealed that it is possible to produce longer outputs that at first glance resemble extended philosophical arguments. As has been widely noted, ChatGPT is also capable of creating outputs that show high levels of organization across outputs of several hundred words, as illustrated by its ability to create passable short student essays. However, the extent to which longer outputs are similarly difficult to distinguish from human-made philosophical arguments would need to be evaluated in more detail in future research. A related question for future research is the extent to which these results are generalizable to other philosophers or other fields. We doubt that Dennett has a particularly robotic or imitable prose style (in fact, we are inclined to think the contrary), but this is a matter for empirical investigation.</p>
<p>We emphasize that our experiment is not a "Turing test" (Epstein et al., 2009). Crucial to a Turing test is the opportunity for substantial back-and-forth exchanges. An appropriately demanding Turing test also requires an expert investigator who knows what types of questions to ask so as not to be fooled by simple chat-bot strategies (Loebner, 2009). We assume that in a proper Turing test, Dennett experts would have reliably distinguished Dennett from our language model.</p>
<p>For example, such models have no memory of previous queries, which ought to make them easy to distinguish in conversation that extends beyond the 2048 token context window. However, it is possible that hybrid models or pure transformer models with longer context windows might, in the future, be convincing in Turing-test-like settings. Furthermore, Turing-test like conditions are unlikely to be possible in most practical cases where human-machine discrimination is desirable.</p>
<p>With large masses of electronically transferred text, recipients will not generally have the opportunity to undertake a Turing-test-like verification.</p>
<p>Our results raise both theoretical and ethical philosophical questions (Strasser, 2023).</p>
<p>Philosophers and cognitive scientists might find it surprising that pure probability calculations, without explicit representations of philosophical concepts or external features of the world, can produce what seem to be novel and philosophically contentful answers (for a review of this debate, see Buckner &amp; Garson, 2019). If machines can prove to be skillful (though, of course, limited) conversational partners making proper moves in a language game, it raises questions about the preconditions for speech acts and the role that comprehension and consciousness play in language production. Philosophers and linguists will need to carefully reevaluate their definitions of the constituent concepts. Researchers will need to consider how to define "comprehension" in an era</p>
<p>where it is increasingly difficult to disentangle performance from competence and in which the question of whether understanding can be attributed to high-performing machines is a topic of LARGE LANGUAGE MODEL OF A PHILOSOPHER 24 lively philosophical debate (Butlin, 2021;Li et al., 2021;Andreas, 2022;Frankish, 2022;Mitchell &amp; Krakauer, 2023;Sobieszek &amp; Price, 2022). For example, our results might help motivate a multiple realization hypothesis for the production of linguistic output.</p>
<p>Speculatively, the analysis of erroneous outputs has the potential to contribute to our understanding of human cognitive abilities, analogously to what is revealed about the mechanisms of vision by the study of visual illusions. What does it reveal about underlying mechanisms that humans make certain types of errors while artificial systems make different types of errors? Due to the fast-moving developments in this field of research, it is difficult to draw definitive lessons at this stage (Belinda et al., 2021). However, there is much to suggest that models based solely on a transformer architecture (i.e., performing only statistical evaluations) will not be able to reliably replace human cognitive abilities. Successor models have tended instead to use hybrid methods.</p>
<p>Still, it is possible that the future better-performing models might succeed in sophisticated tasks with more generic and "empiricist" structures and fewer "innate" or specialized architectures than we tend to assume underlie cognitive abilities in humans. Relatedly, we may soon be confronted with the question of whether consciousness and embodiment are really necessary for comprehension in limited domains.</p>
<p>One ethical issue concerns copyright law governing fine-tuned language models, which is not yet settled (see e.g., Government UK consultations, 2021). It is unclear whether it is fair use of intellectual property to fine-tune a language model on the works of a single author without the author's permission. Since it is unlikely that a fine-tuned model would output a long sequence of text that exactly matches a sequence of text from the author's corpus, idea-borrowing via finetuned language models might be undetectable as plagiarism, even if it is rightly considered plagiarism. For this reason, at least until the law is settled, we recommend seeking the explicit permission of the author before fine-tuning on an individual author's copyrighted text or publishing any of the outputs. How to deal with works by deceased authors should also be considered (Nakagawa &amp; Orita, 2022). One possibility is to legally enforce labeling LLM outputs as such to curb abuses such as academic fraud, propaganda, and spam (for example, the current AI-act draft, a proposed EU law (European Commission, 2021) requires the labeling of anything that might be mistaken for human interaction).</p>
<p>Overreliance on models is also a risk. Despite exceeding our expectations, DigiDan did not reliably produce outputs representing Dennett's views. This is not surprising since deep learning LARGE LANGUAGE MODEL OF A PHILOSOPHER 25 networks tend to have problems with reliability (Bosio et al., 2019;Alshemali &amp; Kalita, 2020). In some cases, the outputs were far different from anything that Dennett would endorse, despite emulating his style. An inexperienced user, or a user insufficiently familiar with the target author's work, might mistakenly assume that outputs of a large language model fine-tuned on an author's work are likely to reflect the actual views of the author or what the author would say (Bender et al., 2021;Wedinger et al., 2021). This might be especially tempting for students, social media users, or others who might rather query a fine-tuned model of an author than read the author's work. For this reason, we recommend caution before releasing to the public any language models fine-tuned on an individual author, even with the author's permission. If any language models are released, they should be clearly described as such, their limitations should be noted, and all outputs should be explicitly flagged as the outputs of a computer program rather than a person. If machinegenerated text were presented as a quotation or paraphrase of positions of existing persons, this would arguably constitute counterfeiting (Dennett, as interviewed in Cukier, 2022;Strasser, 2023).</p>
<p>Other social issues will arise as machine-generated text becomes increasingly difficult to distinguish from human-generated text. How can teachers in the future ensure that submitted essays are not simply a product of a language model (Herman, 2022;Hutson, 2022;Marche, 2022)? In chat conversations, how can we know whether we are interacting with humans and not chatbots? New social practices might aim at proving that one is really the original author of what is written. Perhaps universities will return to supervised essay writing in person. The more difficult it is to distinguish machine outputs from human-made utterances, the greater the danger of misuse.</p>
<p>For example, machine-generated text can play a weighty role in the distribution of misinformation (Marcus, 2022).</p>
<p>In addition to the potentially huge social implications that further developments of LLMs entail, the unreliability of such models raises intriguing questions about the structure of cognition.</p>
<p>Despite the impressive improvements that can be reached by scaling up the models, the evidence so far suggests that outputs generated exclusively by neural networks will remain unreliable (Mitchell &amp; Krakauer, 2023). It might nevertheless be the case that, despite the errors, generic models have capacities that broadly resemble human capacities or general intelligence. This question has led to benchmark investigations (Michael et al., 2022;Talmor et al., 2020;Webb et al., 2022) as well as extensive demonstrations on how to expose such models (Dou et al., 2022; LARGE LANGUAGE MODEL OF A PHILOSOPHER 26 Marcus &amp; Davies, 2020. It will likely remain controversial to what extent, and under what conditions, people ought to trust the outputs of large language models.</p>
<p>These cautions noted, we see significant long-term potential for fine-tuned large language models. If technology continues to advance, fine-tuned language models employing hybrid techniques might soon produce outputs interesting enough to serve as a valuable source of cherrypicking by experts. Compare with computer programs that generate music in the style of a particular composer (Hadjeres et al., 2017;Daly, 2021;Elgammal, 2021) and image-generation programs like Midjourney. Although much of this output is uninteresting, selected outputs might have substantial musical or artistic merit. A composer or artist might create many outputs, choose the most promising, edit them lightly, and present them, not unreasonably, as original work-a possibility suggested by Dennett himself in personal communication. In such cases, the language model would be a thinking tool that is used by humans. Similarly in philosophy, experts might fine-tune a language model with certain corpora (for example, their own corpus, or that of a favorite interlocutor or historical figure, or an aggregate of selected philosophers), generate a variety of outputs under a variety of prompts, and then select those that are the most interesting as a source of potential ideas.</p>
<p>It is far from clear that chess-playing machines have beliefs about chess. It is even less likely that language models of philosophers have philosophical beliefs, especially while they remain focused on next-word prediction, apparently with no cognitive model of the world.</p>
<p>DigiDan does not have Dennettian philosophical opinions about consciousness, God, and animal suffering. But a machine without philosophical understanding might serve as a springboard to something greater. Perhaps we are on the cusp of creating machines capable of producing texts that seem to sparkle with philosophical cleverness, insight, or common sense, potentially triggering new philosophical ideas in the reader, and perhaps also paving the way for the eventual creation of artificial entities who are genuinely capable of philosophical thought.</p>
<p>ACKNOWLEDGMENTS</p>
<p>Special thanks to both Daniel C. Dennett and Matthew Crosby. Dennett provided cooperation, advice, and encouragement in all aspects of this project. Matthew Crosby provided technical expertise and implemented the fine-tunings for this project, as well as collaborating on a conceptual paper that provided the groundwork for this project (Strasser, Crosby, &amp; Schwitzgebel, 2023). Table 1 Strings of six or more words that match between the GPT-3 outputs and the Dennett training corpus. The occurrences column indicates the number of separate training data segments in the training corpus in which that phrase appears. The occurrences total for shorter strings excludes the occurrences in larger matching strings. (Therefore, if any n-gram that is a subset of a larger ngram appears in the table, that means that it appeared independently in the text rather than appearing only within the larger n-gram. For example, "intuition pumps and other tools for thinking" occurs once outside of "in my new book intuition pumps and other tools for thinking.")</p>
<p>String</p>
<h1>of words occurrences in my new book intuition pumps and other tools for thinking 11 1</h1>
<p>like what Dennett might say" (</p>
<p>majority (67%) reported having taken no philosophy classes, and only a few (5%) reported any graduate-level coursework in philosophy. A large majority (83%) reported not having heard of Daniel Dennett, and very few (4%) reported having read any of his work. the chance rate of 20%. A one-sample, two-tailed t-test did not identify a significant difference between participant scores and the chance guess rate of 20% (M = 1.20, t(97)= 1.71, p = .09, d = .17, SD = 1.18, CI = [.97, 1.44]). Only 14% of the participants earned the bonus payment for guessing at least three correctly, and none guessed all five correctly. On average, ordinary research participants rated both Dennett's actual answers and the model's answers as "somewhat like what a real human philosopher would say," with no statistically detectable difference in the ratings (MDennett = 3.11, MGPT-3 = 3.08, paired t(97) = .47, p = .64, d = .05, SDdifference = .69, CIdifference = [-.10, .17])</p>
<p>Overall, blog readers responded correctly an average of 4.81 times out of 10 (48%), substantially above the chance rate of 20% (M = 4.81, t(301) = 23.22, p &lt; .001, d = 1.34, SD = 2.10, CI = [4.57, 5.05]). They also rated Dennett's actual answers as significantly more Dennettlike than the model's answers (MDennett = 3.60, MGPT-3 = 2.65, paired t(301) = 23.00, p &lt; .001, d = 1.32, SDdifference = .72, CIdifference = [.87, 1.03]).</p>
<p>Improving the reliability of deep neural networks in NLP: A review. Knowledge-Based Systems. B Alshemali, J Kalita, 10.1016/j.knosys.2019.105210191105210Alshemali, B., &amp; Kalita, J. (2020). Improving the reliability of deep neural networks in NLP: A review. Knowledge-Based Systems, 191, 105210. doi:10.1016/j.knosys.2019.105210</p>
<p>Language models as agent models. J Andreas, 10.48550/arXiv.2212.01681Andreas, J. (2022). Language models as agent models. doi:10.48550/arXiv.2212.01681</p>
<p>. D Ardila, A P Kiraly, S Bharadwaj, B Choi, J J Reicher, L Peng, S Shetty, Ardila, D., Kiraly, A. P., Bharadwaj, S., Choi, B., Reicher, J. J., Peng, L., … Shetty, S. (2019).</p>
<p>End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. 10.1038/s41591-019-0447-xNature medicine. 256End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nature medicine, 25(6), 954-961. doi:10.1038/s41591-019- 0447-x</p>
<p>LipNet: Sentence-level Lipreading. Y Assael, B Shillingford, S Whiteson, N Freitas, 10.48550/arXiv.1611.01599Assael, Y., Shillingford, B., Whiteson, S., &amp; Freitas, N. (2016). LipNet: Sentence-level Lipreading. doi:10.48550/arXiv.1611.01599</p>
<p>Interviewing Albert Einstein via GPT-3. M Araoz, Araoz, M. (2021). Interviewing Albert Einstein via GPT-3.</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, FAccT '21: Proceedings of the 2021. Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? FAccT '21: Proceedings of the 2021</p>
<p>. 10.1145/3442188.3445922ACM Conference on Fairness, Accountability, and Transparency. ACM Conference on Fairness, Accountability, and Transparency, 610-623. doi:10.1145/3442188.3445922</p>
<p>Poverty of the stimulus revisited. R C Berwick, P Pietroski, B Yankama, N Chomsky, 10.1111/j.1551-6709.2011.01189.xCognitive Science. 357Berwick, R.C., Pietroski, P., Yankama, B., &amp; Chomsky, N. (2011). Poverty of the stimulus revisited. Cognitive Science, 35(7), 1207-1242. doi:10.1111/j.1551-6709.2011.01189.x</p>
<p>A reliability analysis of a deep neural network. A Bosio, P Bernardi, Ruospo, E Sanchez, 10.1109/LATW.2019.8704548IEEE Latin American Test Symposium (LATS). Bosio, A., Bernardi, P., Ruospo, &amp; Sanchez, E. (2019). A reliability analysis of a deep neural network. 2019 IEEE Latin American Test Symposium (LATS), 1-6. doi:10.1109/LATW.2019.8704548</p>
<p>Superhuman AI for multiplayer poker. N Brown, T Sandholm, 10.1126/science.aay2400Science. 6456365Brown, N., &amp; Sandholm, T. (2019). Superhuman AI for multiplayer poker. Science, 365(6456). doi:10.1126/science.aay2400</p>
<p>. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ...</p>
<p>. D Amodei, Amodei, D. (2020).</p>
<p>Language models are few-shot learners. 10.48550/arXiv.2005.14165Advances in neural information processing systems. 33Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901. doi:10.48550/arXiv.2005.14165</p>
<p>A gentle introduction to early stopping to avoid overtraining neural networks. Machine learning mastery. J Brownlee, Brownlee, J. (2019). A gentle introduction to early stopping to avoid overtraining neural networks. Machine learning mastery. https://machinelearningmastery.com/early- stopping-to-avoid-overtraining-neural-network-models/</p>
<p>. Large Language, Of A Philosopher, 29LARGE LANGUAGE MODEL OF A PHILOSOPHER 29</p>
<p>Connectionism. C Buckner, J Garson, The Stanford Encyclopedia of Philosophy. E.N. ZaltaBuckner, C., &amp; Garson, J. (2019). Connectionism. In E.N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy.</p>
<p>Sharing our concepts with machines. P Butlin, 10.1007/s10670-021-00491-wErkenntnis.Butlin, P. (2021). Sharing our concepts with machines. Erkenntnis. doi:10.1007/s10670-021- 00491-w</p>
<p>Deep blue. M Campbell, A J HoaneJr, F H Hsu, Artificial intelligence. 1341-2Campbell, M., Hoane Jr, A. J., &amp; Hsu, F. H. (2002). Deep blue. Artificial intelligence, 134(1-2), 57-83.</p>
<p>Biolinguistic explorations: Design, development, evolution. N Chomsky, International Journal of Philosophical Studies. 151Chomsky, N. (2007). Biolinguistic explorations: Design, development, evolution. International Journal of Philosophical Studies, 15(1), 1-21.</p>
<p>All that's' human' is not gold: Evaluating human evaluation of generated text. E Clark, T August, S Serrano, N Haduong, S Gururangan, N A Smith, 10.48550/arXiv.2107.00061Clark, E., August, T., Serrano, S., Haduong, N., Gururangan, S., &amp; Smith, N. A. (2021). All that's' human' is not gold: Evaluating human evaluation of generated text. doi:10.48550/arXiv.2107.00061</p>
<p>Chat GPT-3: In its own words. D Clarke, Clarke, D. (2022). Chat GPT-3: In its own words. https://www.verdict.co.uk/chat-gpt-3- interview/</p>
<p>Babbage: Could artificial intelligence become sentient? The Economist. K Cukier, Cukier, K. (2022). Babbage: Could artificial intelligence become sentient? The Economist. https://shows.acast.com/theeconomistbabbage/episodes/babbage-could-artificial- intelligence-become-sentient</p>
<p>AI software writes new Nirvana and Amy Winehouse songs to raise awareness for mental health support. R Daly, Daly, R. (2021). AI software writes new Nirvana and Amy Winehouse songs to raise awareness for mental health support. NME. https://www.nme.com/news/music/ai-software-writes- new-nirvana-amy-winehouse-songs-raise-awareness-mental-health-support-2913524</p>
<p>Inquiries into truth and interpretation. D Davidson, Oxford University PressDavidson, D. (1984). Inquiries into truth and interpretation. Oxford University Press.</p>
<p>Letter to More. R Descartes, The philosophical writings of Descartes. J. Cottingham, R. Stoothoff, D. Murdoch, and A. KennyCambridge University Press31649/1991)Descartes, R. (1649/1991). Letter to More, 5 Feb. 1649. In J. Cottingham, R. Stoothoff, D. Murdoch, and A. Kenny (Eds.), The philosophical writings of Descartes, vol. 3. Cambridge University Press.</p>
<p>Is GPT-3 text indistinguishable from human text? Scarecrow: A framework for scrutinizing machine text. Y Dou, M Forbes, R Koncel-Kedziorski, N Smith, C Yejin, 10.18653/v1/2022.acl-long.501Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDou, Y., Forbes, M., Koncel-Kedziorski, R., Smith, N., &amp; Yejin, C. (2022). Is GPT-3 text indistinguishable from human text? Scarecrow: A framework for scrutinizing machine text. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 7250-7274. doi:10.18653/v1/2022.acl-long.501.</p>
<p>RoFT: A tool for evaluating human detection of machine-generated text. L Dugan, D Ippolito, A Kirubarajan, C Callison-Burch, 10.48550/arXiv.2010.03070.LARGELANGUAGEMODELOFAPHILOSOPHER30Dugan, L., Ippolito, D., Kirubarajan, A., &amp; Callison-Burch, C. (2020). RoFT: A tool for evaluating human detection of machine-generated text. doi:10.48550/arXiv.2010.03070. LARGE LANGUAGE MODEL OF A PHILOSOPHER 30</p>
<p>How a team of musicologists and computer scientists completed Beethoven's unfinished 10th symphony. The Conversation. A Elgammal, Elgammal, A. (2021). How a team of musicologists and computer scientists completed Beethoven's unfinished 10th symphony. The Conversation.</p>
<p>Parsing the Turing Test. R Epstein, G Roberts, G Beber, 10.1007/978-1-4020-6710-5SpringerEpstein, R., Roberts, G., &amp; Beber, G. (2009). Parsing the Turing Test. Springer. doi:10.1007/978-1-4020-6710-5</p>
<p>AI-act. Proposal for a regulation of the European parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts. European Commission (21.4.2021European Commission (21.4.2021). AI-act. Proposal for a regulation of the European parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts.</p>
<p>Discovering novel algorithms with AlphaTensor. A Fawzi, M Balog, B Romera-Paredes, D Hassabis, P Kohli, Fawzi, A., Balog, M., Romera-Paredes, B., Hassabis, D., &amp; Kohli, P. (2022). Discovering novel algorithms with AlphaTensor. https://www.deepmind.com/blog/discovering-novel- algorithms-with-alphatensor</p>
<p>Some thoughts on LLMs. Blog post at The tricks of the Mind. K Frankish, Frankish, K. (2022). Some thoughts on LLMs. Blog post at The tricks of the Mind. https://www.keithfrankish.com/blog/some-thoughts-on-llms/</p>
<p>. C Gao, F Howard, N Markov, E Dyer, S Ramesh, Y Luo, A Pearson, Gao, C., Howard, F., Markov, N., Dyer, E., Ramesh, S., Luo, Y., &amp; Pearson, A. (2022).</p>
<p>Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. 10.1101/2022.12.23.521610Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. doi:10.1101/2022.12.23.521610</p>
<p>. Github Copilot, GitHub Copilot. https://docs.github.com/en/copilot</p>
<p>Artificial intelligence call for views: copyright and related rights. U K Government, Consultations, Government UK consultations (2021). Artificial intelligence call for views: copyright and related rights. https://www.gov.uk/government/consultations/artificial-intelligence-and- intellectual-property-call-for-views/artificial-intelligence-call-for-views-copyright-and- related-rights</p>
<p>DeepBach: a steerable model for Bach chorales generation. G Hadjeres, F Pachet, F Nielsen, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningHadjeres, G., Pachet, F., &amp; Nielsen, F. (2017). DeepBach: a steerable model for Bach chorales generation. Proceedings of the 34th International Conference on Machine Learning, 1362-1371.</p>
<p>Minds, machines and Turing. S Harnad, 10.1007/978-94-010-0105-2_14The Turing Test. Studies in Cognitive Systems. J.H. MoorSpringerHarnad, S. (2003). Minds, machines and Turing. In J.H. Moor (Ed.), The Turing Test. Studies in Cognitive Systems (pp. 253-273). Springer. doi:10.1007/978-94-010-0105-2_14</p>
<p>. Large Language, Of A, PHILOSOPHER 31LARGE LANGUAGE MODEL OF A PHILOSOPHER 31</p>
<p>The end of high school English. The Atlantic. D Herman, Herman, D. (2022). The end of high school English. The Atlantic.</p>
<p>An overview of strategies for neurosymbolic integration. M Hilario, Proceedings of the Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid Approaches. the Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid ApproachesHilario, M. (1995). An overview of strategies for neurosymbolic integration. Proceedings of the Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid Approaches.</p>
<p>Artificial neural networks today are not conscious, according to Douglas Hofstadter. D Hofstadter, Hofstadter, D. (2022). Artificial neural networks today are not conscious, according to Douglas Hofstadter. The Economist. https://www.economist.com/by- invitation/2022/06/09/artificial-neural-networks-today-are-not-conscious-according-to- douglas-hofstadter</p>
<p>Alarmed by A.I. chatbots, universities start revamping how they teach. The New York Times. K Huang, Huang, K. (2023). Alarmed by A.I. chatbots, universities start revamping how they teach. The New York Times. https://www.nytimes.com/2023/01/16/technology/chatgpt-artificial- intelligence-universities.html</p>
<p>Could AI help you to write your next paper?. M Hutson, 10.1038/d41586-022-03479-wNature. 7934Hutson, M. (2022). Could AI help you to write your next paper? Nature, 611(7934), 192-193. doi:10.1038/d41586-022-03479-w</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, D Hassabis, 10.1038/s41586-021-03819-2Nature. 5967873Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., … Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), 583-589. doi:10.1038/s41586-021-03819-2</p>
<p>This is a weirder moment than you think. The New York Times. E Klein, Klein, E. (2022). This is a weirder moment than you think. The New York Times. https://www.nytimes.com/2022/06/19/opinion/its-not-the-future-we-cant-see.html</p>
<p>B Z Li, M Nye, J Andreas, Implicit representations of meaning in neural language models. Annual Meeting of the Association for Computational Linguistics. Li, B.Z., Nye, M., &amp; Andreas, J. (2021). Implicit representations of meaning in neural language models. Annual Meeting of the Association for Computational Linguistics.</p>
<p>How to hold a Turing Test contest. H Loebner, 10.1007/978-1-4020-6710-5_12Parsing the Turing Test. R. Epstein, G. Roberts, &amp; G. BeberSpringerLoebner, H. (2009). How to hold a Turing Test contest. In R. Epstein, G. Roberts, &amp; G. Beber (Eds.), Parsing the Turing Test (pp. 173-179). Springer. doi:10.1007/978-1-4020-6710- 5_12</p>
<p>Measurement of similarity in academic contexts. O Mahian, M Treutwein, P Estellé, S Wongwises, D Wen, G Lorenzini, A Sahin, 10.3390/publications5030018Publications. 53Mahian, O., Treutwein, M., Estellé, P., Wongwises, S., Wen, D., Lorenzini, G., … Sahin, A. (2017). Measurement of similarity in academic contexts. Publications, 5(3), 18, doi:10.3390/publications5030018</p>
<p>A robot wrote this entire article. Are you scared yet, human? The Guardian. A Mahdawi, &amp; Gpt-3, Mahdawi, A., &amp; GPT-3 (2020). A robot wrote this entire article. Are you scared yet, human? The Guardian. https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this- article-gpt-3</p>
<p>. Large Language, Of A, PHILOSOPHER 32LARGE LANGUAGE MODEL OF A PHILOSOPHER 32</p>
<p>. K Mahowald, A Ivanova, I Blank, N Kanwisher, J Tenenbaum, E Fedorenko, Mahowald, K., Ivanova, A., Blank, I., Kanwisher, N., Tenenbaum, J., &amp; Fedorenko, E. (2023).</p>
<p>Dissociating language and thought in large language models: a cognitive perspective. 10.48550/arXiv.2301.06627Dissociating language and thought in large language models: a cognitive perspective. doi:10.48550/arXiv.2301.06627</p>
<p>Will ChatGPT kill the student essay? The Atlantic. S Marche, Marche, S. (2022). Will ChatGPT kill the student essay? The Atlantic.</p>
<p>AI platforms like ChatGPT are easy to use but also potentially dangerous. G Marcus, Scientific American. Marcus, G. (2022). AI platforms like ChatGPT are easy to use but also potentially dangerous. Scientific American. https://www.scientificamerican.com/article/ai-platforms-like- chatgpt-are-easy-to-use-but-also-potentially-dangerous</p>
<p>GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about. G Marcus, E Davis, Technology Review. Marcus, G., &amp; Davis, E. (2020). GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about. Technology Review.</p>
<p>Large language models like ChatGPT say the darnedest things. G Marcus, E Davis, Marcus, G., &amp; Davis, E. (2022). Large language models like ChatGPT say the darnedest things. https://garymarcus.substack.com/p/large-language-models-like-chatgpt</p>
<p>What do NLP researchers believe? Results of the NLP community metasurvey. J Michael, A Holtzman, A Parrish, A Mueller, A Wang, A Chen, . . Bowman, S R , 10.48550/arXiv.2208.12852Michael, J., Holtzman, A., Parrish, A., Mueller, A., Wang, A., Chen, A., ... &amp; Bowman, S. R. (2022). What do NLP researchers believe? Results of the NLP community metasurvey. doi:10.48550/arXiv.2208.12852</p>
<p>The debate over understanding in AI's large language models. M Mitchell, D Krakauer, 10.1073/pnas.2215907120Proceedings of the National Academy of Sciences. 12013Mitchell, M., &amp; Krakauer, D. (2023). The debate over understanding in AI's large language models. Proceedings of the National Academy of Sciences, 120(13), e2215907120. doi:10.1073/pnas.2215907120</p>
<p>Using deceased people's personal data. H Nakagawa, A Orita, AI &amp; SOCIETY. Nakagawa, H., &amp; Orita, A. (2022). Using deceased people's personal data. AI &amp; SOCIETY, 1-19.</p>
<p>Why we need new evaluation metrics for NLG. J Novikova, O Dušek, A C Curry, V Rieser, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingNovikova, J., Dušek, O. Curry, A.C., &amp; Rieser, V. (2017). Why we need new evaluation metrics for NLG. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Is Google's LaMDA artificial intelligence sentient? Wrong question. M Roberts, The Washington Post. Roberts, M. (2022). Is Google's LaMDA artificial intelligence sentient? Wrong question. The Washington Post. https://www.washingtonpost.com/opinions/2022/06/14/google-lamda- artificial-intelligence-sentient-wrong-question/</p>
<p>Talking about Large Language Models. M Shanahan, 10.48550/arXiv.2212.03551Shanahan, M. (2023). Talking about Large Language Models. doi:10.48550/arXiv.2212.03551</p>
<p>Guide to the ngram Package: Fast n-gram tokenization. R Package. D Schmidt, C Heckendorf, Schmidt, D., &amp; Heckendorf, C. (2015). Guide to the ngram Package: Fast n-gram tokenization. R Package. https://cran.r-project.org/web/packages/ngram/vignettes/ngram-guide.pdf</p>
<p>Artificial you. S Schneider, Princeton University PressSchneider, S. (2019). Artificial you. Princeton University Press.</p>
<p>. Large Language, Of A, PHILOSOPHER 33LARGE LANGUAGE MODEL OF A PHILOSOPHER 33</p>
<p>More people might soon think robots are conscious and deserve rights. E Schwitzgebel, Blog post at The Splintered MindSchwitzgebel, E. (2021a). More people might soon think robots are conscious and deserve rights. Blog post at The Splintered Mind. https://schwitzsplinters.blogspot.com/2021/03/more- people-might-soon-think-robots-are.html</p>
<p>Two robot-generated Splintered Mind posts. Blog post at The Splintered Mind. E Schwitzgebel, Schwitzgebel, E. (2021b). Two robot-generated Splintered Mind posts. Blog post at The Splintered Mind. https://schwitzsplinters.blogspot.com/2021/11/two-robot-generated- splintered-mind.html</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, D Hassabis, 10.1038/nature16961Nature. 5297587Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., … Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484-489. doi:10.1038/nature16961</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, D Hassabis, 10.1126/science.aar6404Science. 3626419Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., … Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140-1144. doi:10.1126/science.aar6404</p>
<p>Playing games with AIs: The limits of GPT-3 and similar Large Language Models. A Sobieszek, T Price, 10.1007/s11023-022-09602-0Minds &amp; Machines. 32Sobieszek, A., &amp; Price, T. (2022). Playing games with AIs: The limits of GPT-3 and similar Large Language Models. Minds &amp; Machines 32, 341-364. doi:10.1007/s11023-022- 09602-0</p>
<p>A.I. Is mastering language. Should we trust what it says? The New York Times. J Steven, N Iziev, Steven, J., &amp; Iziev, N. (2022). A.I. Is mastering language. Should we trust what it says? The New York Times. https://www.nytimes.com/2022/04/15/magazine/ai-language.html</p>
<p>From tool use to social interactions. A Strasser, 10.1515/9783839462652-004Social robotics and the good life. J. Loh &amp; W. LohTranscript VerlagStrasser, A. (2022). From tool use to social interactions. In J. Loh &amp; W. Loh (Ed.), Social robotics and the good life (pp. 77-102). Transcript Verlag. doi:10.1515/9783839462652- 004</p>
<p>On pitfalls (and advantages) of sophisticated large language models. A Strasser, 10.48550/arXiv.2303.1751Strasser, A. (2023). On pitfalls (and advantages) of sophisticated large language models. doi:10.48550/arXiv.2303.1751</p>
<p>How far can we get in creating a digital replica of a philosopher?. A Strasser, M Crosby, E Schwitzgebel, Proceedings of Robophilosophy 2022. R. Hakli, P. Mäkelä, &amp; J. SeibtRobophilosophy 2022IOS PressSocial robots in social institutionsStrasser, A., Crosby, M., &amp; Schwitzgebel, E. (2023). How far can we get in creating a digital replica of a philosopher? In R. Hakli, P. Mäkelä, &amp; J. Seibt (Eds.), Social robots in social institutions. Proceedings of Robophilosophy 2022 (pp. 371-380). IOS Press.</p>
<p>oLMpics -On what language model pre-training captures. A Talmor, Y Elazar, Y Goldberg, J Berant, Transactions of the Association for Computational Linguistics. 8Talmor, A., Elazar, Y., Goldberg, Y., &amp; Berant, J. (2020). oLMpics -On what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8, 743-758.</p>
<p>Computing machinery and intelligence. A M Turing, 10.1093/mind/LIX.236.433MindTuring, A. M. (1950). Computing machinery and intelligence. Mind, 59 (236), 433-460. https://doi.org/10.1093/mind/LIX.236.433</p>
<p>. Large Language, Of A, PHILOSOPHER 34LARGE LANGUAGE MODEL OF A PHILOSOPHER 34</p>
<p>. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H T Cheng, Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H. T., ...</p>
<p>LaMDA-Language models for dialog applications. Q Le, 10.48550/arXiv.2201.08239Le, Q. (2022). LaMDA-Language models for dialog applications. doi:10.48550/arXiv.2201.08239</p>
<p>The Google engineer who thinks the company's AI has come to life. T Tiku, The Washington Post. Tiku, T. (2022). The Google engineer who thinks the company's AI has come to life. The Washington Post. https://www.washingtonpost.com/technology/2022/06/11/google-ai- lamda-blake-lemoine/</p>
<p>Best practices for the human evaluation of automatically generated text. C Van Der Lee, A Gatt, E Van Miltenburg, S Wubben, E Krahmer, Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language GenerationTokyo, JapanAssociation for Computational Linguisticsvan der Lee, C., Gatt, A., van Miltenburg, E., Wubben, S., &amp; Krahmer, E. (2019). Best practices for the human evaluation of automatically generated text. Proceedings of the 12th International Conference on Natural Language Generation, 355-368, Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Emergent analogical reasoning in large language models. T Webb, K Holyoak, H Lu, 10.48550/arXiv.2212.09196Webb, T., Holyoak, K., &amp; Lu, H. (2022). Emergent analogical reasoning in large language models. doi:10.48550/arXiv.2212.09196</p>
<p>Philosopher David Chalmers interviewed on whether the new AI text generator, GPT3, could be conscious. H Wiseman, Facebook postWiseman, H. (2020). Philosopher David Chalmers interviewed on whether the new AI text generator, GPT3, could be conscious. Facebook post.</p>
<p>Philosophers on GPT-3 (updated with replies by GPT-3). Blog post at Daily Nous. A Zimmerman, Zimmerman, A. (Ed.) (2020). Philosophers on GPT-3 (updated with replies by GPT-3). Blog post at Daily Nous. https://dailynous.com/2020/07/30/philosophers-gpt-3</p>            </div>
        </div>

    </div>
</body>
</html>
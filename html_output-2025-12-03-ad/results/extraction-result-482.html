<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-482 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-482</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-482</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-271213156</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.10457v1.pdf" target="_blank">The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism</a></p>
                <p><strong>Paper Abstract:</strong> Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e482.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e482.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-determinism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-determinism in LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The variability in outputs produced by the same LLM for the same input under different decoding configurations or random seeds, arising from sampling-based decoding and other controllable parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4-Turbo; Llama-3-8B-Instruct; Qwen2-7B-Instruct; Yi-1.5 series; Mistral-7B, others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (0.5B–34B and proprietary GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM evaluation / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparative benchmark evaluation across instruction-following, knowledge, math reasoning, and code generation tasks to measure output variability under different decoding configurations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Decoding configuration (greedy vs sampling/top-p/top-k), temperature, repetition penalty, number of sampled completions (N), alignment method/checkpoint, model family and size, open-endedness of the benchmark (constrained vs unconstrained outputs), randomness in sampling (implicit random seed).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average score under sampling, standard deviation across N sampled runs, performance gap Δ (difference between best and worst sampling run), win rate / WB-Score differences between decoding configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Paper reports measured sampling averages, standard deviations and best-worst gaps across benchmarks. Examples: GPT-4-Turbo on MMLU: sample std reported 0.43; Qwen2-7B-Instruct on GSM8K (Table 4) greedy=83.5 vs sampling=72.0 with sampling std=1.74; the paper states that for GSM8K and HumanEval the performance gap between best and worst samplings can exceed 10.0 percentage points. (Sample sizes used: N=16/32/128 depending on benchmark; see number_of_runs).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Standard deviation across runs, performance gap (Δ between best and worst sampling runs), comparison of greedy vs sampling averages, stability across model sizes and alignment variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Benchmarks with constrained answer spaces (MMLU, MixEval) show high stability (low std / small greedy-sampling gap). Open-ended and reasoning/code tasks (GSM8K, HumanEval) show much higher variability; reported best-worst sampling gaps can exceed 10 points. Some models (GPT-4-Turbo) show smaller sampling vs greedy gaps than many open-weight models.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Widespread practice of single-output evaluation (due to compute cost) hides non-determinism; sampling randomness and decoding settings cause large run-to-run differences; open-ended benchmarks amplify variability; scaling (model size) does not reliably reduce sampling variance; alignment methods can change diversity and thus reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prefer greedy decoding for deterministic tasks; reduce temperature for more deterministic sampling; tune repetition penalty; use alignment / preference-optimization methods (DPO, KTO, SimPO, etc.) to reduce sampling variance; run multiple sampled completions and aggregate (best-of-N, self-consistency, ensemble/selection with reward models); probability calibration; increase number of samples and report standard deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative/empirical: alignment methods reduced standard deviation on several tasks (AlpacaEval, MMLU, GSM8K, HumanEval); best-of-N selection with reward models substantially improves performance (reward model outperforms GPT-4-Turbo on GSM8K with 8 samples in their experiments); oracle best-of-N demonstrates that small LMs can surpass GPT-4-Turbo on MMLU, GSM8K, HumanEval when the best sampled response is selected. No single global numeric reduction in variance is claimed across all settings (effects are benchmark- and model-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>AlpacaEval/Arena-Hard/WildBench/MixEval: N=16; MMLU-Redux: N=32; GSM8K & HumanEval: N=128 (per the experimental setup).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-determinism is substantial and consequential: greedy decoding usually outperforms average sampling across many tasks, but not always (AlpacaEval is an exception); constrained benchmarks (MMLU/MixEval) are stable, while reasoning/code tasks (GSM8K/HumanEval) show large sampling variance, with best-worst gaps >10 points; alignment and best-of-N selection can reduce effective variability and recover much of the models' latent capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e482.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e482.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy vs Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy decoding versus sampling-based (nucleus/top-p) decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A core comparison in the paper: greedy decoding selects highest-probability token deterministically, while sampling (temperature, top-p) introduces stochasticity; the paper measures how these affect benchmark performance and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (see main experiments: GPT-4-Turbo, Llama-3-8B-Instruct, Qwen2-7B-Instruct, Yi-1.5 series, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (0.5B–34B and GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Direct empirical comparison of greedy decoding vs sampling on seven benchmarks (AlpacaEval, Arena-Hard, WildBench v2, MixEval, MMLU-Redux, GSM8K, HumanEval).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Choice of decoding algorithm (greedy vs sampling), sampling randomness across multiple completions, temperature/top-p settings, task open-endedness.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average score under sampling, standard deviation across N sampled runs, Δ (difference between best and worst sample runs), win rate comparisons between decoding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Greedy decoding outperforms sampling averaged across runs on most benchmarks in the paper (scores shown in Table 2). Exceptions: AlpacaEval where sampling shows higher win rate. Example: Qwen2-7B-Instruct on GSM8K: greedy=83.5 vs sampling average=72.0 (Table 4). Different decoding configurations can change model ranking on some benchmarks (e.g., Arena-Hard ordering reverses under sampling vs greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of greedy score vs sampling average and sampling std/Δ across multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Greedy provides more stable/usually higher performance for deterministic tasks (math/code); sampling benefits some open-ended creative tasks (AlpacaEval). Sampling introduces larger std on reasoning/code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Relying on a single sampled output can misrepresent model capability and lead to non-reproducible rankings; sampling randomness can produce large variance in scores for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use greedy decoding for deterministic tasks; if using sampling, run many samples and aggregate (best-of-N, reward-model selection, self-consistency); report standard deviations and Δ.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Best-of-N + reward-model selection recovers large performance gains (oracle best-of-N yields even larger gains); specific numeric example: Qwen2-7B shows a large drop from greedy to sampling on GSM8K (83.5→72.0), showing the importance of choosing decoding method or aggregating samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Same as overall: sampling N varied per benchmark (16/32/128).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Greedy decoding generally yields higher and more stable scores for tasks with constrained/deterministic outputs (math, code); sampling increases diversity but often reduces average task performance and increases variance, except on simpler open-ended creative instruction tasks (AlpacaEval) where sampling can help.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e482.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e482.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alignment methods (DPO, KTO, IPO, ORPO, RDPO, SimPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training/finetuning techniques using preference data or reward-style objectives intended to align model outputs with desired behaviors, evaluated here for their effect on sampling variance and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-Instruct (and its aligned variants: DPO, KTO, SimPO, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (Llama-3-8B-Instruct and aligned checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM alignment / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluate how different alignment methods change both greedy and sampling performance and the standard deviation of sampled outputs across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Changes in model checkpoint due to different alignment objectives (preference optimization methods) which affect output diversity and sampling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Change in standard deviation across sampling runs, change in average score under greedy and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Applying alignment methods often decreased sampling standard deviation for several tasks (AlpacaEval, MMLU, GSM8K, HumanEval), indicating reduced diversity. However, not all alignment methods uniformly improve performance: KTO and SimPO reduced performance on MMLU in the reported experiments, and SimPO had limited effectiveness on MixEval.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of std and average scores before and after alignment on same benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Alignment can reduce sampling variance (improving stability) on many benchmarks, but can also degrade performance on some tasks depending on method and benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Alignment trades off diversity and stability; some alignment objectives reduce useful output diversity or harm performance on constrained benchmarks, complicating reproducible improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Apply preference-optimization alignment (e.g., DPO) to increase likelihood of higher-quality outputs and reduce sampling variance; evaluate multiple alignment methods per task as effects vary.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical qualitative reduction in standard deviation for several benchmarks after alignment (figures and text report decreased std); no single universal numeric reduction across all tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Aligned vs unaligned comparisons use the same sampling Ns as main experiments (N varied by benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Alignment (especially preference-optimization approaches) often reduces sampling variance and makes sampling behavior more consistent, but different alignment algorithms have different effects and can sometimes reduce task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e482.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e482.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling temperature (randomness control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding hyperparameter that scales the logits before sampling; lower temperatures sharpen the distribution (more deterministic), higher temperatures increase randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (ablation across evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM decoding / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Ablation study on the effect of changing temperature on performance and variability across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Temperature setting used during sampling (e.g., 0.0, 1.0, 1.5).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average sampling score as temperature varies; qualitative effect on reasoning/code tasks vs open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Higher temperature slightly improves AlpacaEval, aligns with earlier findings that for some tasks temperature changes 0.0→1.0 may not have significant impact (MMLU). Extremely high temperature (e.g., 1.5) significantly harms reasoning and code generation (GSM8K, HumanEval) while open-ended instruction following (AlpacaEval, Arena-Hard) remains relatively resilient.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Performance trend across temperatures; stability of scores at different temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Temperature is a strong knob: keeping it low makes behavior more reproducible for reasoning/code tasks; high temp increases variance and degrades correctness on those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Using high temperature increases stochasticity and decreases reproducibility for tasks requiring precise reasoning or correct code.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use lower temperatures (or greedy decoding) for reasoning and code generation; report temperature when evaluating and include multiple runs at representative temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative: lowering temperature improves correctness/stability on reasoning/code benchmarks; extremely high temp leads to large performance drops (no single aggregate numeric reduction reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Ablations performed; same sampling Ns used for comparison (varied by benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Temperature controls a trade-off between diversity and correctness: small increases can help open-ended creativity, but high temperatures sharply reduce performance on reasoning/code tasks and increase variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e482.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e482.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RepetitionPenalty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Repetition penalty (decoding hyperparameter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding parameter that penalizes tokens already generated to discourage loops/repetition and can affect answer length and judge preference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (evaluated across benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM decoding / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Ablation study on repetition penalty's effect on performance and variability across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Repetition penalty hyperparameter (e.g., 0.9, 1.0 default, 1.2).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average score as repetition penalty varies; qualitative impact on answer length and judge preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Generally, keeping default repetition penalty yields best performance across most benchmarks. AlpacaEval shows marginal improvement at penalty=1.2 (shorter answers preferred by GPT judges). For GSM8K the best performance observed at 0.9, and increasing penalty beyond that reduced performance (likely because mathematical reasoning often repeats numbers/conditions). MixEval and MMLU show minimal sensitivity to this parameter.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Score trends across different repetition penalty settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Modest, task-dependent effects: small changes can help specific open-ended tasks or harm math reasoning; overall not a large lever for most constrained benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Task-dependent utility of repetition penalty; default is typically acceptable but changes can trade off brevity vs required repetition in reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Keep default unless task-specific tuning indicates benefit; report penalty value in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Marginal improvements in small, task-specific cases (e.g., AlpacaEval improved slightly at 1.2; GSM8K peaked at 0.9), but no broad universal improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Ablation runs (same sampling Ns as main experiments), not all settings enumerated numerically in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Repetition penalty is a minor but task-dependent control: default value is generally best, but tuning can improve or harm performance depending on whether the task benefits from repeated tokens (math reasoning) or brevity (judge preferences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e482.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e482.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-N / Reward Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-of-N selection using reward models and oracle selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Selecting the best response from multiple sampled completions using off-the-shelf reward models (ArmoRM, FsfairX) or an oracle upper bound to reveal the full potential of non-deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-Instruct (examples), reward models ArmoRM and FsfairX used for ranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B for Llama-3-Instruct example; reward models proprietary/checkpointed</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM evaluation / selection/ensemble methods</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Best-of-N experiments: sample multiple outputs per prompt, rank by reward model (or oracle) and select top response to evaluate maximal achievable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling randomness across N completions, reward model ranking quality (imperfect selector), number of samples N.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Performance as a function of N with reward-model selection; oracle upper bound (best-of-N by perfect selection).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Best-of-N selection substantially improves measured performance. Oracle best-of-N allows smaller models (e.g., Llama-3-8B-Instruct) to outperform GPT-4-Turbo on MMLU, GSM8K and HumanEval; off-the-shelf reward models (ArmoRM, FsfairX) can also select superior responses and can outperform GPT-4-Turbo on GSM8K with as few as 8 samples, though there remains a large gap between reward-model selection and the oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Performance curves vs N for reward-model vs oracle selection; comparison to single-run greedy and sampling averages.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Best-of-N dramatically changes measured capability and reduces apparent variance when an effective selector is available; reward-model selection gives practical gains but is far from oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Reward models are imperfect selectors (large gap to oracle); computational cost of many samples; risk of overfitting selector biases; reporting single-run scores misrepresents true model potential.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use best-of-N with reward models or self-consistency / ensemble selection methods; develop better reward models or probability calibration to make high-quality outputs more likely.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical: reward models beat GPT-4-Turbo on GSM8K with N=8 in this paper's experiments; oracle shows even larger improvements enabling 7B-class models to surpass GPT-4-Turbo on multiple benchmarks. Exact numeric gains vs baseline depend on benchmark and N and are shown qualitatively in Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Best-of-N experiments vary N (examples reported include N up to 8 for reward-model wins; oracle experiments used larger N to demonstrate upper bound—specific N varied by experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregating multiple sampled outputs and selecting the best via reward models or oracle massively increases measured performance and demonstrates that a single-sample evaluation underestimates LLM capabilities; however, selector quality is the limiting factor to practical gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e482.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e482.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmark Stability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark-dependent variability: constrained vs open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that benchmarks with constrained answer spaces (MMLU, MixEval) show low sampling variance, while open-ended or reasoning/code benchmarks (GSM8K, HumanEval) show high variability and large best-worst gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (evaluated across same set of LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM benchmarking / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparison of stability (std, Δ) across seven benchmarks to determine which tasks are consistent under non-deterministic generation.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Intrinsic task constraints (multiple-choice or short answer vs open-ended generation), decoding stochasticity, model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation across sampling runs, Δ between best and worst sampled run, greedy vs sample gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>MixEval and MMLU show highest stability (small std, small greedy-sampling gap). GSM8K and HumanEval are the least stable; the paper notes best-worst sampling gaps >10 percentage points for these tasks. Example from Table 4: Qwen2-7B-Instruct GSM8K greedy=83.5 sample=72.0 std=1.74; HumanEval greedy=67.7 sample=48.2 std=4.68, illustrating large instability.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Std and Δ per benchmark, greedy vs sampling comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Constrained-answer benchmarks yield reproducible results across decoding configurations; open-ended and reasoning/code tasks do not, making single-run evaluative conclusions unreliable for those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Using the same evaluation protocol across heterogeneous benchmarks masks that some benchmarks require multiple-sample evaluation to be reliable; open-ended real-user tasks (WildBench, Arena-Hard) are especially sensitive to sampling variance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Report standard deviation and Δ; run multiple samples for unstable benchmarks; prefer greedy or calibrated sampling for reasoning/code tasks; use best-of-N selection when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Applying multiple runs and/or selection methods reduces the chance of misranking models on unstable benchmarks; the paper demonstrates concrete improvements via best-of-N and alignment but does not give a single universal numeric reduction applicable to all benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Per-benchmark sampling counts as in the setup (N=16/32/128).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Benchmark characteristics strongly determine reproducibility: multiple-choice/short-answer benchmarks are stable under stochastic decoding, whereas reasoning and code benchmarks show large sampling variance and require multi-sample or selection-based evaluation to reliably assess model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Common 7b language models already possess strong math capabilities <em>(Rating: 2)</em></li>
                <li>The effect of sampling temperature on problem solving in large language models <em>(Rating: 2)</em></li>
                <li>Are we done with MMLU? <em>(Rating: 1)</em></li>
                <li>Best-of-N / LLM ensembling and selection work (LLM-blender / Jiang et al. 2023b) <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 1)</em></li>
                <li>Rewardbench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-482",
    "paper_id": "paper-271213156",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Non-determinism",
            "name_full": "Non-determinism in LLM generation",
            "brief_description": "The variability in outputs produced by the same LLM for the same input under different decoding configurations or random seeds, arising from sampling-based decoding and other controllable parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4-Turbo; Llama-3-8B-Instruct; Qwen2-7B-Instruct; Yi-1.5 series; Mistral-7B, others)",
            "model_size": "various (0.5B–34B and proprietary GPT-4-Turbo)",
            "scientific_domain": "LLM evaluation / natural language processing",
            "experimental_task": "Comparative benchmark evaluation across instruction-following, knowledge, math reasoning, and code generation tasks to measure output variability under different decoding configurations",
            "variability_sources": "Decoding configuration (greedy vs sampling/top-p/top-k), temperature, repetition penalty, number of sampled completions (N), alignment method/checkpoint, model family and size, open-endedness of the benchmark (constrained vs unconstrained outputs), randomness in sampling (implicit random seed).",
            "variability_measured": true,
            "variability_metrics": "Average score under sampling, standard deviation across N sampled runs, performance gap Δ (difference between best and worst sampling run), win rate / WB-Score differences between decoding configurations.",
            "variability_results": "Paper reports measured sampling averages, standard deviations and best-worst gaps across benchmarks. Examples: GPT-4-Turbo on MMLU: sample std reported 0.43; Qwen2-7B-Instruct on GSM8K (Table 4) greedy=83.5 vs sampling=72.0 with sampling std=1.74; the paper states that for GSM8K and HumanEval the performance gap between best and worst samplings can exceed 10.0 percentage points. (Sample sizes used: N=16/32/128 depending on benchmark; see number_of_runs).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Standard deviation across runs, performance gap (Δ between best and worst sampling runs), comparison of greedy vs sampling averages, stability across model sizes and alignment variants.",
            "reproducibility_results": "Benchmarks with constrained answer spaces (MMLU, MixEval) show high stability (low std / small greedy-sampling gap). Open-ended and reasoning/code tasks (GSM8K, HumanEval) show much higher variability; reported best-worst sampling gaps can exceed 10 points. Some models (GPT-4-Turbo) show smaller sampling vs greedy gaps than many open-weight models.",
            "reproducibility_challenges": "Widespread practice of single-output evaluation (due to compute cost) hides non-determinism; sampling randomness and decoding settings cause large run-to-run differences; open-ended benchmarks amplify variability; scaling (model size) does not reliably reduce sampling variance; alignment methods can change diversity and thus reproducibility.",
            "mitigation_methods": "Prefer greedy decoding for deterministic tasks; reduce temperature for more deterministic sampling; tune repetition penalty; use alignment / preference-optimization methods (DPO, KTO, SimPO, etc.) to reduce sampling variance; run multiple sampled completions and aggregate (best-of-N, self-consistency, ensemble/selection with reward models); probability calibration; increase number of samples and report standard deviation.",
            "mitigation_effectiveness": "Qualitative/empirical: alignment methods reduced standard deviation on several tasks (AlpacaEval, MMLU, GSM8K, HumanEval); best-of-N selection with reward models substantially improves performance (reward model outperforms GPT-4-Turbo on GSM8K with 8 samples in their experiments); oracle best-of-N demonstrates that small LMs can surpass GPT-4-Turbo on MMLU, GSM8K, HumanEval when the best sampled response is selected. No single global numeric reduction in variance is claimed across all settings (effects are benchmark- and model-dependent).",
            "comparison_with_without_controls": true,
            "number_of_runs": "AlpacaEval/Arena-Hard/WildBench/MixEval: N=16; MMLU-Redux: N=32; GSM8K & HumanEval: N=128 (per the experimental setup).",
            "key_findings": "Non-determinism is substantial and consequential: greedy decoding usually outperforms average sampling across many tasks, but not always (AlpacaEval is an exception); constrained benchmarks (MMLU/MixEval) are stable, while reasoning/code tasks (GSM8K/HumanEval) show large sampling variance, with best-worst gaps &gt;10 points; alignment and best-of-N selection can reduce effective variability and recover much of the models' latent capability.",
            "uuid": "e482.0",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Greedy vs Sampling",
            "name_full": "Greedy decoding versus sampling-based (nucleus/top-p) decoding",
            "brief_description": "A core comparison in the paper: greedy decoding selects highest-probability token deterministically, while sampling (temperature, top-p) introduces stochasticity; the paper measures how these affect benchmark performance and variance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (see main experiments: GPT-4-Turbo, Llama-3-8B-Instruct, Qwen2-7B-Instruct, Yi-1.5 series, Mistral-7B)",
            "model_size": "various (0.5B–34B and GPT-4-Turbo)",
            "scientific_domain": "LLM evaluation / NLP",
            "experimental_task": "Direct empirical comparison of greedy decoding vs sampling on seven benchmarks (AlpacaEval, Arena-Hard, WildBench v2, MixEval, MMLU-Redux, GSM8K, HumanEval).",
            "variability_sources": "Choice of decoding algorithm (greedy vs sampling), sampling randomness across multiple completions, temperature/top-p settings, task open-endedness.",
            "variability_measured": true,
            "variability_metrics": "Average score under sampling, standard deviation across N sampled runs, Δ (difference between best and worst sample runs), win rate comparisons between decoding methods.",
            "variability_results": "Greedy decoding outperforms sampling averaged across runs on most benchmarks in the paper (scores shown in Table 2). Exceptions: AlpacaEval where sampling shows higher win rate. Example: Qwen2-7B-Instruct on GSM8K: greedy=83.5 vs sampling average=72.0 (Table 4). Different decoding configurations can change model ranking on some benchmarks (e.g., Arena-Hard ordering reverses under sampling vs greedy).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of greedy score vs sampling average and sampling std/Δ across multiple runs.",
            "reproducibility_results": "Greedy provides more stable/usually higher performance for deterministic tasks (math/code); sampling benefits some open-ended creative tasks (AlpacaEval). Sampling introduces larger std on reasoning/code tasks.",
            "reproducibility_challenges": "Relying on a single sampled output can misrepresent model capability and lead to non-reproducible rankings; sampling randomness can produce large variance in scores for some tasks.",
            "mitigation_methods": "Use greedy decoding for deterministic tasks; if using sampling, run many samples and aggregate (best-of-N, reward-model selection, self-consistency); report standard deviations and Δ.",
            "mitigation_effectiveness": "Best-of-N + reward-model selection recovers large performance gains (oracle best-of-N yields even larger gains); specific numeric example: Qwen2-7B shows a large drop from greedy to sampling on GSM8K (83.5→72.0), showing the importance of choosing decoding method or aggregating samples.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Same as overall: sampling N varied per benchmark (16/32/128).",
            "key_findings": "Greedy decoding generally yields higher and more stable scores for tasks with constrained/deterministic outputs (math, code); sampling increases diversity but often reduces average task performance and increases variance, except on simpler open-ended creative instruction tasks (AlpacaEval) where sampling can help.",
            "uuid": "e482.1",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Alignment",
            "name_full": "Alignment methods (DPO, KTO, IPO, ORPO, RDPO, SimPO)",
            "brief_description": "Training/finetuning techniques using preference data or reward-style objectives intended to align model outputs with desired behaviors, evaluated here for their effect on sampling variance and performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-Instruct (and its aligned variants: DPO, KTO, SimPO, etc.)",
            "model_size": "8B (Llama-3-8B-Instruct and aligned checkpoints)",
            "scientific_domain": "LLM alignment / NLP evaluation",
            "experimental_task": "Evaluate how different alignment methods change both greedy and sampling performance and the standard deviation of sampled outputs across benchmarks.",
            "variability_sources": "Changes in model checkpoint due to different alignment objectives (preference optimization methods) which affect output diversity and sampling behavior.",
            "variability_measured": true,
            "variability_metrics": "Change in standard deviation across sampling runs, change in average score under greedy and sampling.",
            "variability_results": "Applying alignment methods often decreased sampling standard deviation for several tasks (AlpacaEval, MMLU, GSM8K, HumanEval), indicating reduced diversity. However, not all alignment methods uniformly improve performance: KTO and SimPO reduced performance on MMLU in the reported experiments, and SimPO had limited effectiveness on MixEval.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of std and average scores before and after alignment on same benchmarks.",
            "reproducibility_results": "Alignment can reduce sampling variance (improving stability) on many benchmarks, but can also degrade performance on some tasks depending on method and benchmark.",
            "reproducibility_challenges": "Alignment trades off diversity and stability; some alignment objectives reduce useful output diversity or harm performance on constrained benchmarks, complicating reproducible improvements.",
            "mitigation_methods": "Apply preference-optimization alignment (e.g., DPO) to increase likelihood of higher-quality outputs and reduce sampling variance; evaluate multiple alignment methods per task as effects vary.",
            "mitigation_effectiveness": "Empirical qualitative reduction in standard deviation for several benchmarks after alignment (figures and text report decreased std); no single universal numeric reduction across all tasks reported.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Aligned vs unaligned comparisons use the same sampling Ns as main experiments (N varied by benchmark).",
            "key_findings": "Alignment (especially preference-optimization approaches) often reduces sampling variance and makes sampling behavior more consistent, but different alignment algorithms have different effects and can sometimes reduce task performance.",
            "uuid": "e482.2",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Temperature",
            "name_full": "Sampling temperature (randomness control)",
            "brief_description": "A decoding hyperparameter that scales the logits before sampling; lower temperatures sharpen the distribution (more deterministic), higher temperatures increase randomness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (ablation across evaluated LLMs)",
            "model_size": "various",
            "scientific_domain": "LLM decoding / NLP evaluation",
            "experimental_task": "Ablation study on the effect of changing temperature on performance and variability across benchmarks.",
            "variability_sources": "Temperature setting used during sampling (e.g., 0.0, 1.0, 1.5).",
            "variability_measured": true,
            "variability_metrics": "Average sampling score as temperature varies; qualitative effect on reasoning/code tasks vs open-ended tasks.",
            "variability_results": "Higher temperature slightly improves AlpacaEval, aligns with earlier findings that for some tasks temperature changes 0.0→1.0 may not have significant impact (MMLU). Extremely high temperature (e.g., 1.5) significantly harms reasoning and code generation (GSM8K, HumanEval) while open-ended instruction following (AlpacaEval, Arena-Hard) remains relatively resilient.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Performance trend across temperatures; stability of scores at different temperatures.",
            "reproducibility_results": "Temperature is a strong knob: keeping it low makes behavior more reproducible for reasoning/code tasks; high temp increases variance and degrades correctness on those tasks.",
            "reproducibility_challenges": "Using high temperature increases stochasticity and decreases reproducibility for tasks requiring precise reasoning or correct code.",
            "mitigation_methods": "Use lower temperatures (or greedy decoding) for reasoning and code generation; report temperature when evaluating and include multiple runs at representative temperatures.",
            "mitigation_effectiveness": "Qualitative: lowering temperature improves correctness/stability on reasoning/code benchmarks; extremely high temp leads to large performance drops (no single aggregate numeric reduction reported).",
            "comparison_with_without_controls": true,
            "number_of_runs": "Ablations performed; same sampling Ns used for comparison (varied by benchmark).",
            "key_findings": "Temperature controls a trade-off between diversity and correctness: small increases can help open-ended creativity, but high temperatures sharply reduce performance on reasoning/code tasks and increase variability.",
            "uuid": "e482.3",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RepetitionPenalty",
            "name_full": "Repetition penalty (decoding hyperparameter)",
            "brief_description": "A decoding parameter that penalizes tokens already generated to discourage loops/repetition and can affect answer length and judge preference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (evaluated across benchmarks)",
            "model_size": "various",
            "scientific_domain": "LLM decoding / NLP evaluation",
            "experimental_task": "Ablation study on repetition penalty's effect on performance and variability across benchmarks.",
            "variability_sources": "Repetition penalty hyperparameter (e.g., 0.9, 1.0 default, 1.2).",
            "variability_measured": true,
            "variability_metrics": "Average score as repetition penalty varies; qualitative impact on answer length and judge preferences.",
            "variability_results": "Generally, keeping default repetition penalty yields best performance across most benchmarks. AlpacaEval shows marginal improvement at penalty=1.2 (shorter answers preferred by GPT judges). For GSM8K the best performance observed at 0.9, and increasing penalty beyond that reduced performance (likely because mathematical reasoning often repeats numbers/conditions). MixEval and MMLU show minimal sensitivity to this parameter.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Score trends across different repetition penalty settings.",
            "reproducibility_results": "Modest, task-dependent effects: small changes can help specific open-ended tasks or harm math reasoning; overall not a large lever for most constrained benchmarks.",
            "reproducibility_challenges": "Task-dependent utility of repetition penalty; default is typically acceptable but changes can trade off brevity vs required repetition in reasoning.",
            "mitigation_methods": "Keep default unless task-specific tuning indicates benefit; report penalty value in experiments.",
            "mitigation_effectiveness": "Marginal improvements in small, task-specific cases (e.g., AlpacaEval improved slightly at 1.2; GSM8K peaked at 0.9), but no broad universal improvement.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Ablation runs (same sampling Ns as main experiments), not all settings enumerated numerically in the paper.",
            "key_findings": "Repetition penalty is a minor but task-dependent control: default value is generally best, but tuning can improve or harm performance depending on whether the task benefits from repeated tokens (math reasoning) or brevity (judge preferences).",
            "uuid": "e482.4",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Best-of-N / Reward Selection",
            "name_full": "Best-of-N selection using reward models and oracle selection",
            "brief_description": "Selecting the best response from multiple sampled completions using off-the-shelf reward models (ArmoRM, FsfairX) or an oracle upper bound to reveal the full potential of non-deterministic outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-Instruct (examples), reward models ArmoRM and FsfairX used for ranking",
            "model_size": "8B for Llama-3-Instruct example; reward models proprietary/checkpointed",
            "scientific_domain": "LLM evaluation / selection/ensemble methods",
            "experimental_task": "Best-of-N experiments: sample multiple outputs per prompt, rank by reward model (or oracle) and select top response to evaluate maximal achievable performance.",
            "variability_sources": "Sampling randomness across N completions, reward model ranking quality (imperfect selector), number of samples N.",
            "variability_measured": true,
            "variability_metrics": "Performance as a function of N with reward-model selection; oracle upper bound (best-of-N by perfect selection).",
            "variability_results": "Best-of-N selection substantially improves measured performance. Oracle best-of-N allows smaller models (e.g., Llama-3-8B-Instruct) to outperform GPT-4-Turbo on MMLU, GSM8K and HumanEval; off-the-shelf reward models (ArmoRM, FsfairX) can also select superior responses and can outperform GPT-4-Turbo on GSM8K with as few as 8 samples, though there remains a large gap between reward-model selection and the oracle.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Performance curves vs N for reward-model vs oracle selection; comparison to single-run greedy and sampling averages.",
            "reproducibility_results": "Best-of-N dramatically changes measured capability and reduces apparent variance when an effective selector is available; reward-model selection gives practical gains but is far from oracle.",
            "reproducibility_challenges": "Reward models are imperfect selectors (large gap to oracle); computational cost of many samples; risk of overfitting selector biases; reporting single-run scores misrepresents true model potential.",
            "mitigation_methods": "Use best-of-N with reward models or self-consistency / ensemble selection methods; develop better reward models or probability calibration to make high-quality outputs more likely.",
            "mitigation_effectiveness": "Empirical: reward models beat GPT-4-Turbo on GSM8K with N=8 in this paper's experiments; oracle shows even larger improvements enabling 7B-class models to surpass GPT-4-Turbo on multiple benchmarks. Exact numeric gains vs baseline depend on benchmark and N and are shown qualitatively in Figure 4.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Best-of-N experiments vary N (examples reported include N up to 8 for reward-model wins; oracle experiments used larger N to demonstrate upper bound—specific N varied by experiment).",
            "key_findings": "Aggregating multiple sampled outputs and selecting the best via reward models or oracle massively increases measured performance and demonstrates that a single-sample evaluation underestimates LLM capabilities; however, selector quality is the limiting factor to practical gains.",
            "uuid": "e482.5",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Benchmark Stability",
            "name_full": "Benchmark-dependent variability: constrained vs open-ended tasks",
            "brief_description": "Observation that benchmarks with constrained answer spaces (MMLU, MixEval) show low sampling variance, while open-ended or reasoning/code benchmarks (GSM8K, HumanEval) show high variability and large best-worst gaps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (evaluated across same set of LLMs)",
            "model_size": "various",
            "scientific_domain": "LLM benchmarking / NLP evaluation",
            "experimental_task": "Comparison of stability (std, Δ) across seven benchmarks to determine which tasks are consistent under non-deterministic generation.",
            "variability_sources": "Intrinsic task constraints (multiple-choice or short answer vs open-ended generation), decoding stochasticity, model differences.",
            "variability_measured": true,
            "variability_metrics": "Standard deviation across sampling runs, Δ between best and worst sampled run, greedy vs sample gaps.",
            "variability_results": "MixEval and MMLU show highest stability (small std, small greedy-sampling gap). GSM8K and HumanEval are the least stable; the paper notes best-worst sampling gaps &gt;10 percentage points for these tasks. Example from Table 4: Qwen2-7B-Instruct GSM8K greedy=83.5 sample=72.0 std=1.74; HumanEval greedy=67.7 sample=48.2 std=4.68, illustrating large instability.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Std and Δ per benchmark, greedy vs sampling comparisons.",
            "reproducibility_results": "Constrained-answer benchmarks yield reproducible results across decoding configurations; open-ended and reasoning/code tasks do not, making single-run evaluative conclusions unreliable for those tasks.",
            "reproducibility_challenges": "Using the same evaluation protocol across heterogeneous benchmarks masks that some benchmarks require multiple-sample evaluation to be reliable; open-ended real-user tasks (WildBench, Arena-Hard) are especially sensitive to sampling variance.",
            "mitigation_methods": "Report standard deviation and Δ; run multiple samples for unstable benchmarks; prefer greedy or calibrated sampling for reasoning/code tasks; use best-of-N selection when appropriate.",
            "mitigation_effectiveness": "Applying multiple runs and/or selection methods reduces the chance of misranking models on unstable benchmarks; the paper demonstrates concrete improvements via best-of-N and alignment but does not give a single universal numeric reduction applicable to all benchmarks.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Per-benchmark sampling counts as in the setup (N=16/32/128).",
            "key_findings": "Benchmark characteristics strongly determine reproducibility: multiple-choice/short-answer benchmarks are stable under stochastic decoding, whereas reasoning and code benchmarks show large sampling variance and require multi-sample or selection-based evaluation to reliably assess model capability.",
            "uuid": "e482.6",
            "source_info": {
                "paper_title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Common 7b language models already possess strong math capabilities",
            "rating": 2,
            "sanitized_title": "common_7b_language_models_already_possess_strong_math_capabilities"
        },
        {
            "paper_title": "The effect of sampling temperature on problem solving in large language models",
            "rating": 2,
            "sanitized_title": "the_effect_of_sampling_temperature_on_problem_solving_in_large_language_models"
        },
        {
            "paper_title": "Are we done with MMLU?",
            "rating": 1,
            "sanitized_title": "are_we_done_with_mmlu"
        },
        {
            "paper_title": "Best-of-N / LLM ensembling and selection work (LLM-blender / Jiang et al. 2023b)",
            "rating": 2,
            "sanitized_title": "bestofn_llm_ensembling_and_selection_work_llmblender_jiang_et_al_2023b"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 1,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Rewardbench: Evaluating reward models for language modeling",
            "rating": 2,
            "sanitized_title": "rewardbench_evaluating_reward_models_for_language_modeling"
        }
    ],
    "cost": 0.018817999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism
15 Jul 2024</p>
<p>Yifan Song yfsong@pku.edu.cn 
Peking University ♣ Allen Institute for AI</p>
<p>Guoyin Wang 
Peking University ♣ Allen Institute for AI</p>
<p>Sujian Li 
Peking University ♣ Allen Institute for AI</p>
<p>♡ Bill 
Peking University ♣ Allen Institute for AI</p>
<p>Yuchen Lin yuchenl@allenai.org 
Peking University ♣ Allen Institute for AI</p>
<p>The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism
15 Jul 2024E0212E92A4C7CFE899635BFB0C2AE623arXiv:2407.10457v1[cs.CL]
Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example.This limits our understanding of LLM performance variability in real-world applications.Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors.Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks.We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance.Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs.This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation. 1</p>
<p>Introduction</p>
<p>When evaluating a large language model (LLM), two common generation configurations are commonly used: greedy decoding and nucleus sampling (Holtzman et al., 2020).It's important to note that given a particular input, the same LLM may generate significantly different outputs under various decoding configurations, a phenomenon known as non-determinism in generation.However, most evaluations of LLMs are based on a single output per example.This practice is primarily due to practical considerations, as LLM inference and evaluation can be computationally expensive.Neglecting non-determinism in generation significantly limits 1 Code and data are available at https://github.com/Yifan-Song793/GoodBadGreedyour comprehensive understanding of LLMs.Additionally, without reporting the standard deviation in most current LLM evaluations, it is difficult to measure the variability and dynamics of LLMs in real-world applications.</p>
<p>For certain capabilities such as math reasoning (Cobbe et al., 2021;Hendrycks et al., 2021b) and coding, greedy generation is preferred to ensure fair comparisons.Nonetheless, it remains unclear whether there are significant differences in performance between greedy decoding and sampling.Recent investigations have also highlighted potential issues of instability in LLMs (Li et al., 2024a;Hassid et al., 2024).In a study where the best answer was selected from 256 random generations, the Llama-2-7B model achieved an impressive 97.7% accuracy in solving GSM8K questions, even surpassing GPT-4 (Li et al., 2024a).This phenomenon further underscores the enormous potential of LLMs in their non-deterministic outputs.</p>
<p>Herein, we aim to investigate a series of critical questions regarding the non-determinism of LLM generations, which have not been fully explored: • Q1: How does the performance gap between greedy decoding and sampling differ?• Q2: When is greedy decoding better than sampling, and vice versa?Why? • Q3: Which benchmark is most/least consistent with respect to non-determinism?• Q4: Do any models possess unique patterns?Apart from Q1-Q4 in Sec. 3, we also explore the scaling effect on non-determinism (Sec.4.1), the alignment effect on non-determinism (Sec.4.2), the temperature and repetition effect on generation, and the full potential of LLMs (Sec.5).</p>
<p>Our extensive results reveal these findings: • For most benchmarks we evaluated, a notable performance gap is observed between greedy generation and the average score of multiple sampling.</p>
<p>In certain cases, the performance ranking under different generation configurations differs.</p>
<p>• Greedy decoding exhibits superior performance than sampling methods on most evaluated benchmarks, except for AlpacaEval where sampling shows higher win rate.• LLMs displayed consistent performance across different generation configurations for benchmarks with constrained output spaces, such as MMLU and MixEval.Notably, tasks involving math reasoning and code generation were most impacted by sampling variance.• The above findings remain consistent across different sizes and families of LLMs.• Alignment methods, e.g., DPO (Rafailov et al., 2024), can significantly reduce the sampling variance for most benchmarks.• High temperature will significantly harm the reasoning and code generation capabilities of LLMs, while higher repetition penalty leads to improved performance on AlpacaEval.• In the best-of-N sampling setting, 7B-level LMs have the potential to outperform GPT-4-Turbo, and cutting-edge reward models can select superior responses from multiple sampled candidates.</p>
<p>Experimental Setup</p>
<p>Benchmarks.We select multiple benchmarks for our experiments, encompassing abilities of general instruction-following, knowledge, math reasoning, coding, etc.As summarized in Table 1, the selected benchmarks are: AlpacaEval 2 (?), Arena-Hard (Li et al., 2024b), WildBench v2 (Lin et al., 2024), MixEval (Ni et al., 2024), MMLU-Redux (Gema et al., 2024), GSM8K (Cobbe et al., 2021), and HumanEval (Chen et al., 2021).AlpacaEval 2 (?), Arena-Hard (Li et al., 2024b) andWildBench v2 (Lin et al., 2024) are general instruction-following benchmarks.AlpacaEval consists of 805 questions, Arena-Hard incorporating 500 well-defined technical problem-solving queries, and WildBench including 1024 challenging tasks from real users.For AlpacaEval 2, we report the length-controlled win rate (LC).For Arena-Hard, we report the win rate (WR) against the baseline model.For WildBench, we use taskwise scores and the corresponding task-macro WB-Score as the metrics.</p>
<p>Since the original MMLU (Hendrycks et al., 2021a) benchmark is huge and contain numerous ground truth errors (Wang et al., 2024b;Gema et al., 2024), we use MMLU-Redux (Gema et al., 2024) which is a subset of 3000 manually re-annotated questions across 30 MMLU subjects.We also include GSM8K (Cobbe et al., 2021), and Hu-manEval (Chen et al., 2021), two popular benchmarks for evaluating the math and code generation abilities of LLMs.</p>
<p>LLMs.</p>
<p>We test several open-weight LLMs, including Llama-3-Instruct (Meta, 2024), Yi-1.5-Chat (Young et al., 2024), Qwen-2-Instruct (Bai et al., 2023), Mistral (Jiang et al., 2023a), which are widely used.A proprietary LLM, GPT-4-Turbo, is included for comparison.We also consider models of different sizes in the same family such as Qwen2 and Yi-1.5 for more analysis.</p>
<p>To study the effect of alignment techniques, we evaluate models trained with different alignment methods, including DPO (Rafailov et al., 2024), KTO (Ethayarajh et al., 2024), IPO (Azar et al., 2024), ORPO (Hong et al., 2024), RDPO (Park et al., 2024), and SimPO (Meng et al., 2024).We use the checkpoints released by Meng et al. (2024).</p>
<p>Setup.We aim to compare the performance of LLMs under different decoding configurations.We select greedy decoding and sampling generation for the main comparison.For sampling, we set the temperature to 1.0 and top-p to 1.0.</p>
<p>We use official evaluation scripts for AlpacaEval 2, Arena-Hard, WildBench, and MixEval.For MMLU-Redux, instead of using the next token probability of the choice letters, we employ zeroshot CoT and encourage the model to generate the answer in the form of natural language sentence.For GSM8K and HumanEval, we use Open-Instruct framework (Wang et al., 2023) to evaluate the models, which may differ from zero-shot CoT.We will run more comprehensive evaluations on these two benchmarks in the future.We sample 16 completions for AlpacaEval 2, Arena-Hard, Wild-Bench, and MixEval, 32 completions for MMLU-Redux, 128 for GSM8K and HumanEval.Table 3: Results on WildBench v2, with sampling N=16 generations for each model.In addition to WB-Score, we also report the score for each task category.</p>
<p>Experimental Results</p>
<p>We present our experiment results in Table 2 and Table 3.We analyze the results and answer several important research questions around the nondeterminism of LLM generations as follows.</p>
<p>Q1. How does the performance gap between greedy decoding and sampling differ?</p>
<p>From the results, we observe a consistent performance gap between greedy decoding and the sampling method.This disparity holds true across various LLMs, whether they are proprietary or open-source, and across multiple benchmarks encompassing instruction-following, language understanding, math reasoning, and code generation.For WildBench, which enables fine-grained analysis of LLM capabilities, the performance gap is also evident across all task categories, as shown in Ta-ble 3. Different decoding configurations can even alter the model rankings in some cases.For example, on Arena-Hard, Qwen2-7B is slightly better than Llama-3-8B when both use greedy decoding; However, Llama-3-8B may outperform Qwen2-7B when both decode by sampling.Q2.When is greedy decoding better than sampling, and vice versa?Why?</p>
<p>For most evaluated tasks and models, greedy decoding outperforms sampling.However, AlpacaEval serves as a notable exception, where sampling demonstrates superior performance.</p>
<p>GSM8K and HumanEval are reasoning tasks requiring LLMs to solve specific math or coding problems with definite solutions.MixEval also follows a deterministic pattern with its groundtruth-based benchmarks.While AlpacaEval, Arena-Hard, and WildBench are open-ended instruction-following benchmarks, AlpacaEval exhibits a contrasting behavior compared to the others.The potential reasons are two folds: Firstly, the task category distributions vary across different benchmarks.As highlighted by Lin et al. ( 2024), 50% of instances in AlpacaEval are information-seeking, whereas more than 50% in Arena-Hard are related to coding and debugging.Furthermore, the difficulty of instances might play an important role.The tasks in both Arena-Hard and WildBench, sourced from real users, pose substantial challenges.On the other hand, instances in AlpacaEval are comparatively simpler.</p>
<p>In summary: 1) Greedy decoding generally proves more effective for most tasks.2) In the case of AlpacaEval, which comprises relatively simpler open-ended creative tasks, sampling tends to generate better responses.</p>
<p>Q3. Which benchmark is most/least consistent with respect to non-determinism?</p>
<p>MixEval and MMLU exhibit the highest stability, either in terms of the performance gap between greedy decoding and sampling or the standard deviation across different samplings.This stability can be attributed to the constrained answer space of these benchmarks.Specifically, MMLU is structured in a multiple-choice format, and MixEval, comprising various ground-truth-based benchmarks, prompts LLMs to generate short answers, further limiting the output space.</p>
<p>In contrast, GSM8K and HumanEval are relatively less stable with respect to non-deterministic generations.The performance gap between the best and worst samplings can exceed 10.0 points.</p>
<p>Q4. Do the models possess distinctive characteristics?</p>
<p>GPT-4-Turbo shows consistent performance across multiple tasks, with a smaller performance gap between greedy decoding and sampling, as well as improved sampling quality.Some openweight LLMs, however, exhibit unique characteristics.For example, Mistral-7B-Instruct-v0.2 displays inverse behavior on open-ended instruction following tasks like AlpacaEval and Arena-Hard when compared to other models.Similarly, Llama-3-8B-Instruct performs better by sampling than by greedy decoding on MMLU, which is unlike the behavior of other models.These observations raise intriguing questions for future research.Why do certain models exhibit inverse behavior on specific tasks?Can these unique characteristics be leveraged to develop more robust LLMs?These questions highlight the need for deeper explorations into the underlying mechanisms of LLMs.Such research could significantly enhance our understanding of how different models and training impact model behavior.</p>
<p>How Various Factors Influence</p>
<p>Non-Determinism?</p>
<p>In this section, we further investigate how various factors, such as scaling, alignment, and several decoding parameters, influence non-determinism.</p>
<p>Scaling Effect on Non-Determinism</p>
<p>Some might assume that larger LMs will have lower uncertainty in decoding, leading to lower variance in performance when sampling.However, our results challenge this assumption.We use the Yi-1.5-Chat and Qwen2-Instruct series to investigate the scaling effect.The results for the Yi-1.5 and Qwen2 series are presented in Table 2 and Table 4, respectively.Performance differences are observed across LLMs of various sizes, ranging from 0.5B to 34B parameters.The findings in Section 3 are consistent across different model sizes.However, no pattern related to the number of model parameters could be identified.For instance, scaling parameters does not result in lower sampling variance.Notably, Qwen2-7B-Instruct shows higher variance on AlpacaEval and HumanEval compared to its smaller counterparts.</p>
<p>Alignment Effect on Non-Determinism</p>
<p>Alignment methods, such as DPO, enhance LLMs by learning from preference data.We evaluate the effects of alignment methods such as DPO, KTO, and SimPO, using Llama-3-8B-Instruct as the training starting point (Meng et al., 2024).</p>
<p>As shown in Figure 1, after applying these methods, both greedy decoding and sampling performances are affected.In several tasks, including AlpacaEval, MMLU, GSM8K, and HumanEval, a decrease in standard deviation is observed, suggesting that alignment may reduce the diversity of sampling outputs.However, it is crucial to note that not all alignment methods consistently improve model performance.For instance, KTO and SimPO lead to a performance decline in MMLU.Furthermore, SimPO's effectiveness appears limited on the recently introduced MixEval benchmark.</p>
<p>Temperature Effect on Non-Determinism</p>
<p>For sampling generation, temperature serves as a control mechanism for the randomness of the sampling process, where lower values make the model more deterministic, whereas higher values make the model more random.In this section, we present an ablation study to evaluate the effect of varying temperatures on non-determinism generation.</p>
<p>As depicted in Figure 2(a), we observe that, for AlpacaEval, higher temperature will lead to slightly better performance, which aligns with the results in Sec. 3. A recent study (Renze and Guven, 2024) finds that, on multiple-choice QA tasks, changes in temperature from 0.0 to 1.0 do not have a statistically significant impact on LLM performance.Our results on MMLU aligns with their findings.Another findings emerges when the temperature is extremely high, such as 1.5.Comparing with openended instruction following, a high temperature significantly impacts the reasoning and code generation capabilities of LLMs and the model struggles to solve questions in GSM8K and HumanEval.However, it still manages to perform relatively well in open-ended instruction following tasks, such as AlpacaEval and ArenaHard.</p>
<p>Repetition Effect on Generation</p>
<p>In addition to parameters that control greedy search and sampling, there are other parameters that influence the generation process, such as the repetition penalty (Keskar et al., 2019).Here we examine the effect of repetition penalty on generation.Repetition penalty penalizes new tokens based on whether  As illustrated in Figure 2(b), in most cases, it is advisable not to adjust this parameter, as maintaining the default value tends to yield the best performance.For AlpacaEval, a higher repetition penalty like 1.2 results in marginally improved performance.This improvement may be linked to GPT judges' preference for shorter, more concise answers.Regarding MixEval and MMLU, repetition penalty has a minimal impact on the model's performance, since both benchmarks advocate for the model to generate concise answers.Interestingly, for GSM8K, the model achieves the best performance when the repetition penalty is set at 0.9, and increasing this penalty parameter will cause a performance decline.This phenomenon can be attributed to the nature of mathematical reasoning, which frequently necessitates the repetition of numbers and conditions outlined in the question.</p>
<p>Surface Patterns in Non-Determinism</p>
<p>Generation?</p>
<p>We try to explore the surface patterns in nondeterminism generation.Firstly, we compare the generation length of different generation configurations in Table 5.The generation length for Al-pacaEval and ArenaHard is defined as the length of the model's response, while for MMLU and GSM8K, it refers to the length of the final answer with chain-of-thoughts.We observe that the completions generated by greedy decoding are typically marginally shorter than those produced via sam-  pling generation.However, this pattern deviates in the case of Yi series models on AlpacaEval and GSM8K, where the lengths of responses produced by both greedy and sampling methods are comparable.</p>
<p>We also take Qwen2-7B-Instruct on GSM8K as a case study, where the greedy decoding significantly outperforms the sampling generation (83.5 vs. 72.0).As depicted in Figure 3, greedy decoding solves the question effectively.Nonetheless, when it is the turn for sampling generation, the error rate surges to 89% within 128 responses.This observation suggests that the sampling method could potentially harm reasoning capabilities for LLMs. Figure 4: Potential of Llama-3-8B-Instruct.We use the setting of "Best-of-N", which selects the best response from N outputs for each example.We employ off-the-shelf reward models to rank the responses and select the one with the highest reward, while "Oracle" means the upper bound of Best-of-N method.</p>
<p>5 What is the Full Potential of Non-Determinism?</p>
<p>Current evaluations of LLMs mainly assess them based on a single output per instance, which limits our understanding of their full potential.Following Jiang et al. (2023b) and Li et al. (2024a), we adopt a Best-of-N setting, selecting the best answer from N sampled responses.To accomplish this, we employ off-the-shelf reward models, such as ArmoRM (Wang et al., 2024a) and FsfairX (Xiong et al., 2024), to rank the responses of Llama-3-8B-Instruct, selecting the one with the highest reward.We also include an "oracle" baseline which directly picks the best response as the upper bound of bestof-N strategy.</p>
<p>The results are depicted in Figure 4. We observe a significant performance enhancement when applying simple best-of-N strategy for multiple sampled responses.Notably, with the oracle selection, even smaller LLMs like Llama-3-8B-Instruct can outperform GPT-4-Turbo on MMLU, GSM8K, and HumanEval.This finding underscores that compact-sized LLMs already exhibit robust capabilities, highlighting that a more significant challenge in alignment is to robustly decode such knowledge and reasoning paths.Furthermore, cutting-edge reward models can also select superior responses from multiple generations, and can outperform GPT-4-Turbo on GSM8K with only 8 samples.However, there is still a huge performance gap between reward models and the oracle baseline, indicating ample room for improvement.</p>
<p>Building upon these promising findings, there are two ways to further enhance the performance of smaller LLMs.Firstly, probability calibration techniques can guide LLMs towards generating superior answers with higher likelihoods.Alignment methods, specifically preference optimiza-tion (Rafailov et al., 2024), play a pivotal role in this process.Secondly, strategies for ensemble learning or selecting the best answer from multiple completions warrant attention.Selfconsistency (Wang et al., 2022) and advanced prompting techniques (Yao et al., 2023;Lin et al., 2023), which employs heuristic selection from multiple completions, is also worth further exploration.</p>
<p>Related Work</p>
<p>LLM Evaluation In recent years, the development of various benchmarks has significantly advanced the evaluation of LLMs.Benchmarks like MMLU (Hendrycks et al., 2021a), Hel-laSwag (Zellers et al., 2019), and ARC (Clark et al., 2018) have expanded the scope by assessing capabilities across knowledge understanding, and complex reasoning.AlpacaEval (?), MT-Bench (Zheng et al., 2023), ArenaHard (Li et al., 2024b), andWildBench (Lin et al., 2024), leveraging frontier models as judges, evaluate open-ended instructionfollowing capabilities.Moreover, GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), Hu-manEval (Chen et al., 2021), and MBPP (Austin et al., 2021) focus on evaluating math reasoning and code generation capabilities.</p>
<p>Due to the costly nature of LLM inference and evaluation process, most evaluations of LLMs rely on a single output per example.In this paper, we aim to explore the impact of various generation configurations, particularly non-deterministic generations, on the performance of LLMs.</p>
<p>Decoding Strategy Given a prompt, LLMs rely on a decoding strategy to auto-regressively generate response.The simplest decoding method, greedy decoding, selects the next token with the highest probability.Beam search (Freitag and Al-Onaizan, 2017), an improved version of greedy search, retains the top-B tokens with the highest probability at each time step.In order to generate diverse responses, non-determinism generation methods, such as Top-k (Fan et al., 2018) and Top-p sampling (Holtzman et al., 2020), randomly picks the next token based on the probability distribution.The temperature parameter serves to balance response quality and diversity (Ackley et al., 1985).Other decoding parameters, like length and repetition penalties (Keskar et al., 2019), are also available to further control the generation process.</p>
<p>Conclusion &amp; Future directions</p>
<p>We investigate a series of critical yet overlooked questions around non-determinism of LLM generations.After evaluating several LLMs across seven commonly used benchmarks, we have answered several intriguing research questions.Further analysis also provides insights on how scaling and alignment will effect on non-determinism generation.We hope this work can enhance our comprehension of the generation methods and the widely used benchmarks.Our evaluation results can also be used for improving future research.For example, our best-of-N results can serve as a benchmark for assessing reward models (Lambert et al., 2024).</p>
<p>Figure 2 :
2
Figure 1: Alignment effects on non-determinism.</p>
<p>Manny signed up for Karate classes for $60.His parents tell him that if his classes end up costing more than $10 per class, then they won't sign him up again.If there are 10 total classes, how many can he miss before they don't sign him up again?Greedy: Manny pays $60 for 10 classes.So each class costs 60 / 10 = 6 dollars.If each class costs more than 10 dollars, then he can't take the class.So he can miss 10 -6 = 4 classes.So the answer is 4. Sample 1: Manny paid 60 dollars for 10 classes.So each class costs 60 / 10 = 6 dollars.Since 6 dollars is less than the limit of 10 dollars per class, Manny's parents will still sign him up.Thus Manny can miss all 10 classes.So the answer is 10.Sample 2: Manny pays 10 * $6 = $60 in total if that is the final cost.So Manny cannot pay more than $60 for his classes.So if he misses c classes, he can miss a total of 60/60 -= 1c more classes before his parents won't pay for him to go.So the answer is 1.Sample 3: The total cost for $10 per class is 10 * 10 = 100 dollars.Each class costs him 60 / 10 = 6 dollars.That means he can miss 100 / 6 = 16.666classes.So the answer is 16.He can only miss an integer number of classes, so the answer is 16.Sample 4: Manny paid 60 dollars for 10 classes.So each class costs 60/10 = 6 dollars.Each class costs less than 10 dollars, so he can miss up to 9 classes before they do not sign him up again.So the answer is 9.</p>
<p>Figure 3 :
3
Figure 3: Case Study on non-determinism: Qwen2-7B-Instruct on GSM8K.Greedy decoding can effectively address the question.However, in 128 sampling generations for the same question, the error rate is 89%.</p>
<p>Table 1 :
1
Statistics of datasets.
DatasetInstance Num. Sample Num.MetricAlpacaEval 280516LCArena-Hard50016WRMixEval400016ScoreWildBench v2102416WB-ScoreMMLU-Redux300032AccGSM8K1319128EMHumanEval164128Pass@1</p>
<p>Table 2 :
2ModelAlpacaEval 2 (N=16)Arena-Hard (N=16)MixEval (N=16)Greedy Sample Std. ∆ Greedy Sample Std.∆Greedy Sample Std.∆GPT-4-Turbo49.650.10.76 2.580.175.21.31 3.689.288.80.18 0.8Llama-3-8B-Instruct26.829.20.88 2.823.518.40.71 2.774.672.50.25 0.9Yi-1.5-6B-Chat17.518.00.91 3.413.711.80.88 3.170.068.60.26 1.0Yi-1.5-9B-Chat23.124.10.91 3.432.827.01.25 4.474.072.70.35 1.4Yi-1.5-34B-Chat34.935.00.99 3.942.840.91.82 5.781.981.80.47 1.5Qwen2-7B-Instruct18.219.12.51 8.623.716.10.87 3.176.276.20.21 0.6Mistral-7B-Instruct-v0.215.413.01.02 4.212.512.60.57 2.069.870.00.24 0.9ModelMMLU-Redux (N=32)GSM8K (N=128)HumanEval (N=128)Greedy Sample Std. ∆ Greedy Sample Std.∆Greedy Sample Std.∆GPT-4-Turbo82.682.40.43 1.684.583.80.77 2.589.684.12.65 11.0Llama-3-8B-Instruct47.850.70.70 2.867.664.42.50 13.458.531.83.62 18.3Yi-1.5-6B-Chat52.149.60.67 2.574.573.10.92 4.148.235.74.86 19.5Yi-1.5-9B-Chat65.564.30.53 2.382.981.00.69 3.955.536.44.92 27.5Yi-1.5-34B-Chat83.282.20.34 1.185.481.70.56 2.964.649.34.08 21.4Qwen2-7B-Instruct64.461.70.46 2.183.572.01.74 11.367.748.24.68 27.4Mistral-7B-Instruct-v0.249.748.40.49 2.245.942.00.99 5.137.825.92.52 14.0MetricLlama-3-8B-InstructYi-1.5-6B-ChatQwen2-7B-InstructGreedy Sample Std.∆Greedy Sample Std.∆Greedy Sample Std.∆WB-Score29.626.21.65 5.723.922.41.67 5.332.723.82.13 7.7Creative Tasks42.242.41.77 6.732.132.12.33 10.339.631.42.21 8.5Planning &amp; Reasoning33.831.41.19 3.627.927.41.77 5.736.028.11.95 6.1Math &amp; Data Analysis17.816.02.85 9.217.417.51.99 6.527.618.52.69 10.4Info/Advice Seeking39.037.41.30 5.532.530.21.80 6.340.332.21.84 6.5Coding &amp; Debugging24.116.03.12 10.916.712.81.70 5.426.315.52.82 9.3
Results on six popular benchmarks."Sample" and "Std."denotes the average score and the standard deviation of "N" runs under sampling setup."∆" denotes the performance gap between the best and worst run.Scores where greedy decoding surpasses the sampling average are highlighted in green, while those lower are marked in red.The intensity of the color indicates the magnitude of the difference (best viewed in color).</p>
<p>Table 4 :
4
Evaluation results on Qwen2-Instruct with different model sizes.
ModelAlpacaEvalMMLUGSStd.GSStd.Qwen2-0.5B-Instruct 1.11.7 0.77 36.4 37.0 0.70Qwen2-1.5B-Instruct 1.93.3 0.88 42.6 42.1 0.68Qwen2-7B-Instruct18.2 19.1 2.51 61.0 61.7 0.46ModelGSM8KHumanEvalGSStd.GSStd.Qwen2-0.5B-Instruct 31.7 14.3 1.86 28.0 10.8 2.14Qwen2-1.5B-Instruct 63.1 36.5 3.20 40.9 22.6 2.94Qwen2-7B-Instruct83.5 72.0 1.74 67.7 48.2 4.68</p>
<p>Table 5 :
5
Length comparison.Cases where greedy decoding generates shorter responses than sampling average are highlighted in blue, and marked in purple vice versa.</p>
<p>A learning algorithm for boltzmann machines. Geoffrey E David H Ackley, Terrence J Hinton, Sejnowski, Cognitive science. 911985</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, ArXiv preprint, abs/2108.077322021</p>
<p>A general theoretical paradigm to understand learning from human preferences. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, Daniele Calandriello, International Conference on Artificial Intelligence and Statistics. PMLR2024</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. ArXiv preprint, abs/2309.16609</p>
<p>Evaluating large language models trained on code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, preprint, abs/2107.033742021</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, ArXiv preprint, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, ArXiv preprint, abs/2110.141682021</p>
<p>Kto: Model alignment as prospect theoretic optimization. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela, abs/2402.013062024ArXiv preprint</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, 10.18653/v1/P18-1082Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Beam search strategies for neural machine translation. Markus Freitag, Yaser Al-Onaizan, 10.18653/v1/W17-3207Proceedings of the First Workshop on Neural Machine Translation. the First Workshop on Neural Machine TranslationVancouver. Association for Computational Linguistics2017</p>
<p>. Aryo Pradipta, Gema , Joshua Ong Jun, Giwon Leang, Alessio Hong, Alberto Devoto, Maria Carlo, Rohit Mancino, Xuanli Saxena, Yu He, Xiaotang Zhao, Mohammad Du, Ghasemi Reza, Madani, et al. 2024. Are we done with mmlu? ArXiv preprint, abs/2406.04127</p>
<p>The larger the better? improved llm code-generation via budget reallocation. Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi, abs/2404.00725ArXiv preprint. 2024</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021a. May 3-7, 2021OpenReview.net</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, abs/2103.03874ArXiv preprint. 2021b</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Reference-free monolithic preference optimization with odds ratio. Jiwoo Hong, Noah Lee, James Thorne, abs/2403.076912024ArXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, et al. 2023a. Mistral 7b. ArXiv preprint, abs/2310.06825</p>
<p>Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , Annual Meeting of the. Association for Computational Linguistics2023b</p>
<p>Ctrl: A conditional transformer language model for controllable generation. Nitish Shirish Keskar, Bryan Mccann, Lav R Varshney, Caiming Xiong, Richard Socher, abs/1909.05858ArXiv preprint. 2019</p>
<p>Rewardbench: Evaluating reward models for language modeling. Nathan Lambert, Valentina Pyatkin, Jacob Daniel Morrison, Lester James Validad, Bill Miranda, Khyathi Yuchen Lin, Nouha Raghavi Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hanna Smith, Hajishirzi, abs/2403.13787ArXiv preprint. 2024</p>
<p>Common 7b language models already possess strong math capabilities. Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng, abs/2403.04706ArXiv preprint. 2024a</p>
<p>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, From live data to high-quality benchmarks: The arena-hard pipeline. 2024b</p>
<p>Wildbench: Benchmarking llms with challenging tasks from real users in the wild. Yuntian Bill Yuchen Lin, Khyathi Deng, Faeze Chandu, Abhilasha Brahman, Valentina Ravichander, Nouha Pyatkin, Ronan Dziri, Yejin Le Bras, Choi, abs/2406.04770ArXiv preprint. 2024</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Prithviraj Yang, Faeze Ammanabrolu, Shiyu Brahman, Chandra Huang, Yejin Bhagavatula, Xiang Choi, Ren, 2023ArXiv preprint, abs/2305.17390</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, abs/2405.147342024ArXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. 2024. 2024arXiv preprint arXiv:[placeholder</p>
<p>Disentangling length from quality in direct preference optimization. Ryan Park, Rafael Rafailov, Stefano Ermon, Chelsea Finn, abs/2403.191592024ArXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>The effect of sampling temperature on problem solving in large language models. Matthew Renze, Erhan Guven, abs/2402.052012024ArXiv preprint</p>
<p>Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang, abs/2406.12845ArXiv preprint. 2024a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, abs/2203.11171ArXiv preprint. 2022</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey Macmillan, Noah A Smith, Iz Beltagy, Advances in Neural Information Processing Systems. 202336</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, ArXiv preprint, abs/2406.015742024b</p>
<p>Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang, arXiv:2312.114562024Preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, abs/2305.10601ArXiv preprint. 2023</p>
<p>Yi: Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, preprint, abs/2403.046522024</p>
<p>HellaSwag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.18653/v1/P19-1472Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, arXiv:2306.056852023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
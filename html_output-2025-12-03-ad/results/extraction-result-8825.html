<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8825 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8825</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8825</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-4f451ba06c4c9effd6c4ac0bae222495501a6200</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f451ba06c4c9effd6c4ac0bae222495501a6200" target="_blank">Innovations in Neural Data-to-text Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella.</p>
                <p><strong>Paper Abstract:</strong> The neural boom that has sparked natural language processing (NLP) research through the last decade has similarly led to significant innovations in data-to-text generation (DTG). This survey offers a consolidated view into the neural DTG paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating DTG from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for DTG research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8825.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8825.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF triples (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Resource Description Framework (RDF) triple serialization (as used in WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Knowledge-graph inputs expressed as sets of RDF triples (subject, predicate, object). Commonly linearized or encoded and used as the canonical structured input for multi-domain graph-to-text generation (the WebNLG benchmark uses this format).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents a knowledge graph as a bag/set of triples (subject, property, object). Triples can be fed to a model either as a graph (with a graph encoder) or linearized into sequences of triples, possibly anonymized and grouped, before being consumed by a seq2seq model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF graphs (multi-domain entity-relationship graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use the native RDF triple form (subject, property, object). Conversion options cited include (a) direct graph encoding via graph encoders (GCNs/GATs/Graph Transformer), or (b) linearization/serialization of triples into a sequence (various traversal or ordering heuristics) with optional anonymization/delexicalization and grouping.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (WebNLG), general D2T tasks (summarization, QA, dialogue applications noted as related)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports WebNLG as a benchmark with 27,731 graph-text pairs and highlights that WebNLG offers more semantic and linguistic diversity than earlier datasets; no single numeric model performance score for RDF-serialization is provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively to AMR (AMR is a semantically-grounded directed graph) and to table-based serializations; authors note WebNLG's larger semantic/linguistic diversity relative to older datasets. Models either use direct graph encoders (preferred for structural integrity) or fall back to linearization for seq2seq systems.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Canonical, structured representation that encodes relations explicitly; flexible: can be consumed by graph-specific encoders or serialized for PLM/seq2seq fine-tuning; supports multi-domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>If linearized, may lose explicit graph structure and relational context; requires either specialized graph encoders to preserve structure or careful linearization strategies to avoid information loss; annotator/serialization choices affect model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When naively linearized or when structural information is ignored, models can lose relational integrity leading to omissions/hallucinations; survey notes need for graph encoders to preserve structure rather than naive serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8825.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstract Meaning Representation (AMR) linearization / AMR-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AMR represents sentence meaning as directed labeled graphs; AMR-to-text approaches either use graph encoders or linearize AMRs (e.g., depth-first traversal, grouping, anonymization) for seq2seq generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR linearization / AMR graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR is a directed labeled graph representing predicate-argument structure. Conversion strategies include: linearizing the graph (depth-first or other traversals), grouping and anonymizing entities, or using graph encoders (GCNs/GATs/graph transformers) that consume node and edge labels directly.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Semantic dependency graphs / AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal or other linearization orders; anonymize/group node mentions; alternatively use explicit graph encoders that model edge labels and directions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (sentence realization) and related data-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey cites AMR corpora size (e.g., LDC AMR resources ~39k instances) and qualitative findings: some systems are agnostic to linearization order; no single numeric performance value reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Linearization (sequence-based) approaches are noted to be sometimes effective (and allow use of NMT systems) but can be dataset-dependent; explicit graph encoders (GCN/GAT/Graph Transformer) are described as better at preserving structural relations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Linearization enables use of standard seq2seq and pretrained PLMs; graph encoders preserve structure and edge-label semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization can lose explicit graph relations and requires careful planning/anonymization; some linearization-based methods are dataset-dependent (e.g., require single-entity-per-sentence constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Methods that rely on simplistic linearization or that force single-entity sentence constraints may not generalize; pretrained LMs fine-tuned on linearized AMR can still struggle with preserving structural facts if order/exactness is lost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8825.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Depth-first / serialization linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-first traversal and sequence serialization of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generic linearization technique that serializes graph inputs (AMR, RDF, tables) into a flat token sequence via graph traversals (e.g., depth-first), optionally with delexicalization, to enable standard seq2seq or PLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Depth-first traversal linearization (graph serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse the input graph (commonly depth-first) and emit nodes/edges in traversal order to produce a linear token sequence; often combined with anonymization/delexicalization and special separator tokens when used with PLMs or seq2seq.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, RDF graphs, knowledge graphs, and table representations treated as sequences</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal (or other traversals) that outputs node and relation tokens in order; may add delexicalization placeholders and grouping to reduce sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation, AMR-to-text, adapting PLMs for graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports that depth-first traversal is an effective linearization; Konstas et al. observed agnosticism to linearization orders in some experiments; no uniform numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Linearization is easier to use with existing seq2seq systems and PLMs but loses structural inductive bias compared to explicit graph encoders (GCN/GAT/Graph Transformer). Some task-adaptive pretrained models still perform well even on shuffled (i.e., order-perturbed) graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; enables reuse of off-the-shelf seq2seq and pretrained language models; reduces need for specialized graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potential information loss about graph structure and relations; sensitive to choice of traversal/ordering heuristics; can be dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Dataset-dependent approaches (e.g., those requiring single-entity per sentence) fail to generalize; linearized PLMs can struggle on tasks requiring precise structure or numeric calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8825.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ordered-tree text plans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence planning via ordered trees (text plans appended to training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Create explicit ordered text plans (trees) representing sentence-level planning and append them to training data; an NMT system is then trained to convert plans to text (plan-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Ordered-tree text plans (plan-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>An intermediate planned representation (ordered tree) that specifies sentence boundaries, entity mentions and high-level ordering; appended to graph data during training so that a plan-to-text model learns to realize the planned structure into fluent text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Graphs where a sentence-level plan is useful (WebNLG, AMR instances where sentence planning helps)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract/append an ordered tree plan (e.g., via text-planning annotations or heuristic extraction) and use an NMT/seq2seq model to map the plan to final text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation with explicit sentence planning (improves coherence/ordering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes that appending text plans (Moryossef et al.) was effective with plan-to-text NMT, but that restrictions (e.g., single-entity mentions per sentence) make the approach dataset-dependent; no numeric metrics are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Plan-based two-stage pipelines can improve ordering and coherence relative to purely end-to-end seq2seq, but may be less flexible than end-to-end models or graph encoders that learn structure implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Offers explicit control over sentence ordering and discourse planning; can improve coherence in long or multi-sentence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires plan extraction or annotations; approaches with strict plan constraints are dataset-dependent and may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Methods that rely on restrictive plan assumptions (e.g., single-entity per sentence) fail outside datasets that satisfy those constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8825.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Table serialization for PLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attribute-value linearization / table serialization for pretrained language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represent tables as linear sequences of attribute-value pairs (with separators) so that pretrained language models (GPT-2, T5, BART) can be fine-tuned for table-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attribute-value table serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Flatten each table row and/or column into a sequence of tokens like 'field: value' (or 'cell value is ...') with special separators between table and reference text, enabling direct input to PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Tabular data (records / tables / info-boxes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize each row/column as 'field: value' sequences; strategies include horizontal traversal row-by-row, marking target cells via flags, and using separator tokens between table and text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Table-to-text generation (WikiBio, RotoWire, ToTTo, TabFact, SciGen etc.), controlled one-sentence generation, long-form summarization from tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes that T5 performed best on automated metrics when using such serializations but failed on summaries involving numerical calculations; no concrete numeric metrics provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Serialization + PLM fine-tuning competes with specialized tabular encoders; while PLMs achieve strong automated metric scores, they can struggle at arithmetic/logical inferences compared to table-specific architectures or explicit operation encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables reuse of large pretrained text models with minimal architectural changes; simple to implement and scales to many PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May not capture row/column structural inductive biases; PLMs can fail on numeric arithmetic/reasoning tasks unless augmented with operation encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>T5 and similar PLMs can produce low-quality outputs when the task requires numerical calculations or logical operations over table cells; serialization may obscure structural relations needed for faithful generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8825.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN with edge labels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Network encoder with explicit edge labels and directions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GCN variant that encodes node representations while explicitly modeling labeled edges and edge directions, with learned gates that weigh neighbor contributions—used to encode graphs before text decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GCN with edge-label & direction modeling</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Node representations are updated by aggregating transformed neighbor states; each neighbor contribution is modulated by parameters depending on edge label and direction and by learned scalar gates g_{u,v}, producing a relation-aware node encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR and general labeled knowledge graphs (graphs with typed/predicate edges)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph encoder (GCN) message-passing across neighbors: h_v' = ReLU(sum_{u in N(v)} g_{u,v}(W_dir(u,v) h_u + b_label(u,v))). The resulting node embeddings are used by a decoder (e.g., LSTM) to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text, knowledge-graph narration)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states that GCN-based encoders are adopted in graph-to-text pipelines and help capture relational structure; no numeric performance results are quoted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GCNs preserve structural integrity better than naive linearization and provide complementary benefits relative to LSTM encoders; hybrid approaches that combine GCNs with sequential encoders can capture complementary strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Relational inductive bias that preserves node/edge structure; explicit modeling of edge labels and directions improves relation-aware representations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires modeling edge labels and directions (extra parameters); may need skip connections and architectural care (residual/dense) to handle deep stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit numeric failure cases in survey; implicit risks include underperforming when edge-type modeling is inadequate or when graph statistics differ strongly between train/test (structural generalization issues).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8825.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTR-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Text RNN (GTR-LSTM) / edge-aware LSTM graph encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based graph encoding approach that computes hidden states for graph entities taking into account incoming edges and non-predefined relationships, ordering vertices via topological sort + breadth-first traversal before LSTM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GTR-LSTM (edge-aware LSTM graph encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute entity hidden states by considering edges that point to the entity from predecessor nodes; serialize an ordering over vertices (topological + BFS) and feed the ordered nodes to an LSTM that captures sequentialized graph structure with edge-awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs / RDF / AMR variants where edge directions matter</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Construct an ordering (topological + BFS) of graph nodes; compute hidden states using incoming-edge aware recurrence and feed to LSTM decoder or downstream module.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey describes this as able to handle non-predefined relationships and capture re-entrant structures; no numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Posited as complementary to GCN/RNN hybrids; better than purely linearized ordering because it encodes incoming-edge information, but still relies on an ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Handles non-predefined relations and encodes incoming-edge context; bridges graph structure and sequential decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on an order over nodes (ordering choice affects representation); still sequential—may inherit RNN limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential sensitivity to chosen node ordering; if the ordering does not reflect discourse-planning needs, coherence may suffer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8825.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DualEnc (GCN + serialized GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-encoder: structural GCN + serialized (planning) GCN feeding LSTM decoder (DualEnc)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-encoder architecture where one GCN models the original graph structure and a second GCN serializes/re-orders nodes serving as a planning encoder; the two encoders provide complementary signals to the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DualEnc (structural + serialized GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two complementary encoders: (1) a GCN retaining the graph's structural integrity, and (2) a GCN that serializes/re-orders nodes (mimicking a plan) whose output is fed to an LSTM decoder—aiming to capture both structural and planning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR / RDF / other knowledge graphs requiring both structure and ordering</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Parallel encode: structural GCN for node representations; serialized GCN that imposes an order for planning; combine outputs to inform decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports DualEnc captures complementary effects of the two encoders and bridges GCN and LSTM strengths; no numeric metrics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>A hybrid approach improving over single-encoder-only designs by combining structural fidelity with decoding-friendly order representations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Combines structural preservation and planning/order signals; can improve coherence and factual fidelity by leveraging both encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increased model complexity; requires design choices for how to serialize/re-order nodes for the second encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated, but complexity and misalignment between the two encoder views could harm training if not tuned properly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8825.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphWriter (Graph Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-centric Transformer encoder (GraphWriter / Graph Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extends the transformer architecture to directly consume graph-structured inputs by contextualizing each node with attention over its neighbors (multi-head graph attention), thereby avoiding RNN sequential limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Transformer / GraphWriter</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Adapt transformer self-attention to graphs by computing contextualized node representations via neighbor-attention heads: each node attends to its neighbors with learned attention functions, optionally across multiple heads, and then feeds representations to a decoder for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR/knowledge graphs and other node-edge labeled graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use graph-aware multi-head attention where each node's representation is updated by attending to its graph neighbors (attention computed per neighbor), producing node embeddings fed to a decoder (e.g., transformer or LSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AGENDA, AMR-to-text and general graph narration)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes this addresses RNN shortcomings and is used in AGENDA-style tasks; no numeric performance metrics included in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GraphWriter (transformer-based) contrasts with GCN/GAT and RNN-based encoders by leveraging global attention and parallelism; better suited to overcome sequential RNN limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Parallelizable, captures long-range dependencies via attention, better suits graphs with complex connectivity than RNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May require positional/token-ordering design to handle sequential realization; tuning graph-aware attention can be complex.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failures described, though survey notes general caveats of PLMs and transformers on tasks requiring arithmetic or strict logical inference unless augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8825.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT global/local</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network with global and local message passing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAT variant that captures complementary global and local contexts by maintaining separate message-passing streams (global and local) and combining them when building node representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT with global/local message passing</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Builds node representations with two complementary message-passing contexts: a global context (broad neighborhood information) and a local context (fine-grained neighbor information), often token-level and augmented with positional embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs and other structured graphs (token-level node representations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use two GAT-based message-passing processes (global and local) and combine the outputs to form node embeddings used by decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports this approach captures complementary graph contexts and is applied token-level with positional embeddings; no numeric performance numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Designed to capture both broad and fine-grained context compared to single-stream GAT/GCN approaches; complements transformer-style encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Better contextualization by capturing multi-scale graph information; token-level modeling can preserve order where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More parameters and complexity; requires design choices for balancing global vs local contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly listed; potential overfitting or computational cost on large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8825.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-view autoencoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detachable multi-view autoencoding for graphs (multi-view reconstruction losses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decompose the graph into multiple views (e.g., triple-set view and linearized traversal view) and train models with multi-view reconstruction losses so the model learns both structural and serial views of the data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Detachable multi-view autoencoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Deconstruct input graph into multiple complementary views (e.g., structured triple set and a depth-first linearization) and add auxiliary reconstruction losses per view (e.g., deep biaffine for triples, linearized sequence reconstruction) to enrich training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR, RDF, and other knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Train encoder/decoder with additional autoencoding heads that reconstruct distinct views of the input (e.g., node-edge triples and linearized sequence), providing multiple supervision signals.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (with stronger structural supervision), improved representation learning for generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes Song et al. enrich training signal with multi-view autoencoding losses; no numeric metrics are quoted in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Provides richer supervision than single-view reconstruction or plain seq2seq training; complements other graph encoder choices.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Stronger structural supervision, improved robustness and possibly better few-shot behavior; helps models capture multiple facets of graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increased training complexity and auxiliary task engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure modes in survey; risk of conflicting signals between views if not carefully balanced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8825.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8825.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node/edge reconstruction loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoder-side node and edge reconstruction auxiliary losses (joint graph reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extend the text decoder to jointly predict (as auxiliary tasks) the input graph's node labels and edge labels/directions while generating text, encouraging the generation process to remain grounded in input graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node & edge reconstruction during decoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>While decoding tokens, add auxiliary losses that predict node identities and edge labels (minimize word-to-node attention distance and negative log-likelihood over projected edges), thereby coupling generation with graph reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR / general graphs with node/edge labels</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Augment standard language modeling loss with node-prediction and edge-prediction losses computed from decoder states and attention patterns (i.e., multitask loss).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation; improves faithfulness and few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states that adding node/edge reconstruction losses improves performance, particularly in few-shot settings; no numerical metrics are reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Auxiliary reconstruction contrasts with plain seq2seq training by explicitly enforcing alignment with the source graph; related to reverse re-encoding/back-translation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves faithfulness to input graph and assists few-shot performance; provides stronger supervision linking text tokens to source graph elements.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Extra auxiliary tasks increase training complexity; requires designing effective projection/prediction heads.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases described, but potential mismatch between reconstruction objectives and natural text realization could cause optimization trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Innovations in Neural Data-to-text Generation', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WebNLG dataset <em>(Rating: 2)</em></li>
                <li>Abstract Meaning Representation (AMR) <em>(Rating: 2)</em></li>
                <li>GraphWriter (Graph-centric Transformer) <em>(Rating: 2)</em></li>
                <li>Graph Convolutional Networks for graph-to-text (Marcheggiani and Titov) <em>(Rating: 2)</em></li>
                <li>Depth-first traversal linearization / plan-based NMT (Moryossef et al.) <em>(Rating: 2)</em></li>
                <li>DualEnc hybrid GCN+LSTM (Zhao et al.) <em>(Rating: 2)</em></li>
                <li>Node and edge reconstruction during decoding (Bai et al.) <em>(Rating: 2)</em></li>
                <li>Table serialization for PLMs (Zhang et al., Gong et al.) <em>(Rating: 2)</em></li>
                <li>Detachable multi-view autoencoding for graphs (Song et al.) <em>(Rating: 1)</em></li>
                <li>Graph Attention Network global/local message passing (Ribeiro et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8825",
    "paper_id": "paper-4f451ba06c4c9effd6c4ac0bae222495501a6200",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "RDF triples (WebNLG)",
            "name_full": "Resource Description Framework (RDF) triple serialization (as used in WebNLG)",
            "brief_description": "Knowledge-graph inputs expressed as sets of RDF triples (subject, predicate, object). Commonly linearized or encoded and used as the canonical structured input for multi-domain graph-to-text generation (the WebNLG benchmark uses this format).",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "RDF triple serialization",
            "representation_description": "Represents a knowledge graph as a bag/set of triples (subject, property, object). Triples can be fed to a model either as a graph (with a graph encoder) or linearized into sequences of triples, possibly anonymized and grouped, before being consumed by a seq2seq model.",
            "graph_type": "Knowledge graphs / RDF graphs (multi-domain entity-relationship graphs)",
            "conversion_method": "Use the native RDF triple form (subject, property, object). Conversion options cited include (a) direct graph encoding via graph encoders (GCNs/GATs/Graph Transformer), or (b) linearization/serialization of triples into a sequence (various traversal or ordering heuristics) with optional anonymization/delexicalization and grouping.",
            "downstream_task": "Graph-to-text generation (WebNLG), general D2T tasks (summarization, QA, dialogue applications noted as related)",
            "performance_metrics": "Survey reports WebNLG as a benchmark with 27,731 graph-text pairs and highlights that WebNLG offers more semantic and linguistic diversity than earlier datasets; no single numeric model performance score for RDF-serialization is provided in the survey.",
            "comparison_to_others": "Compared qualitatively to AMR (AMR is a semantically-grounded directed graph) and to table-based serializations; authors note WebNLG's larger semantic/linguistic diversity relative to older datasets. Models either use direct graph encoders (preferred for structural integrity) or fall back to linearization for seq2seq systems.",
            "advantages": "Canonical, structured representation that encodes relations explicitly; flexible: can be consumed by graph-specific encoders or serialized for PLM/seq2seq fine-tuning; supports multi-domain data.",
            "disadvantages": "If linearized, may lose explicit graph structure and relational context; requires either specialized graph encoders to preserve structure or careful linearization strategies to avoid information loss; annotator/serialization choices affect model behavior.",
            "failure_cases": "When naively linearized or when structural information is ignored, models can lose relational integrity leading to omissions/hallucinations; survey notes need for graph encoders to preserve structure rather than naive serialization.",
            "uuid": "e8825.0",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "AMR linearization",
            "name_full": "Abstract Meaning Representation (AMR) linearization / AMR-to-text",
            "brief_description": "AMR represents sentence meaning as directed labeled graphs; AMR-to-text approaches either use graph encoders or linearize AMRs (e.g., depth-first traversal, grouping, anonymization) for seq2seq generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "AMR linearization / AMR graph encoding",
            "representation_description": "AMR is a directed labeled graph representing predicate-argument structure. Conversion strategies include: linearizing the graph (depth-first or other traversals), grouping and anonymizing entities, or using graph encoders (GCNs/GATs/graph transformers) that consume node and edge labels directly.",
            "graph_type": "Semantic dependency graphs / AMR graphs",
            "conversion_method": "Depth-first traversal or other linearization orders; anonymize/group node mentions; alternatively use explicit graph encoders that model edge labels and directions.",
            "downstream_task": "AMR-to-text generation (sentence realization) and related data-to-text tasks.",
            "performance_metrics": "Survey cites AMR corpora size (e.g., LDC AMR resources ~39k instances) and qualitative findings: some systems are agnostic to linearization order; no single numeric performance value reported in survey.",
            "comparison_to_others": "Linearization (sequence-based) approaches are noted to be sometimes effective (and allow use of NMT systems) but can be dataset-dependent; explicit graph encoders (GCN/GAT/Graph Transformer) are described as better at preserving structural relations.",
            "advantages": "Linearization enables use of standard seq2seq and pretrained PLMs; graph encoders preserve structure and edge-label semantics.",
            "disadvantages": "Linearization can lose explicit graph relations and requires careful planning/anonymization; some linearization-based methods are dataset-dependent (e.g., require single-entity-per-sentence constraints).",
            "failure_cases": "Methods that rely on simplistic linearization or that force single-entity sentence constraints may not generalize; pretrained LMs fine-tuned on linearized AMR can still struggle with preserving structural facts if order/exactness is lost.",
            "uuid": "e8825.1",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Depth-first / serialization linearization",
            "name_full": "Depth-first traversal and sequence serialization of graphs",
            "brief_description": "Generic linearization technique that serializes graph inputs (AMR, RDF, tables) into a flat token sequence via graph traversals (e.g., depth-first), optionally with delexicalization, to enable standard seq2seq or PLM training.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Depth-first traversal linearization (graph serialization)",
            "representation_description": "Traverse the input graph (commonly depth-first) and emit nodes/edges in traversal order to produce a linear token sequence; often combined with anonymization/delexicalization and special separator tokens when used with PLMs or seq2seq.",
            "graph_type": "AMR graphs, RDF graphs, knowledge graphs, and table representations treated as sequences",
            "conversion_method": "Depth-first traversal (or other traversals) that outputs node and relation tokens in order; may add delexicalization placeholders and grouping to reduce sparsity.",
            "downstream_task": "Graph-to-text generation, AMR-to-text, adapting PLMs for graph inputs.",
            "performance_metrics": "Survey reports that depth-first traversal is an effective linearization; Konstas et al. observed agnosticism to linearization orders in some experiments; no uniform numeric metrics provided.",
            "comparison_to_others": "Linearization is easier to use with existing seq2seq systems and PLMs but loses structural inductive bias compared to explicit graph encoders (GCN/GAT/Graph Transformer). Some task-adaptive pretrained models still perform well even on shuffled (i.e., order-perturbed) graph representations.",
            "advantages": "Simple to implement; enables reuse of off-the-shelf seq2seq and pretrained language models; reduces need for specialized graph encoders.",
            "disadvantages": "Potential information loss about graph structure and relations; sensitive to choice of traversal/ordering heuristics; can be dataset-dependent.",
            "failure_cases": "Dataset-dependent approaches (e.g., those requiring single-entity per sentence) fail to generalize; linearized PLMs can struggle on tasks requiring precise structure or numeric calculations.",
            "uuid": "e8825.2",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Ordered-tree text plans",
            "name_full": "Sentence planning via ordered trees (text plans appended to training)",
            "brief_description": "Create explicit ordered text plans (trees) representing sentence-level planning and append them to training data; an NMT system is then trained to convert plans to text (plan-to-text).",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Ordered-tree text plans (plan-to-text)",
            "representation_description": "An intermediate planned representation (ordered tree) that specifies sentence boundaries, entity mentions and high-level ordering; appended to graph data during training so that a plan-to-text model learns to realize the planned structure into fluent text.",
            "graph_type": "Graphs where a sentence-level plan is useful (WebNLG, AMR instances where sentence planning helps)",
            "conversion_method": "Extract/append an ordered tree plan (e.g., via text-planning annotations or heuristic extraction) and use an NMT/seq2seq model to map the plan to final text.",
            "downstream_task": "Graph-to-text generation with explicit sentence planning (improves coherence/ordering).",
            "performance_metrics": "Survey notes that appending text plans (Moryossef et al.) was effective with plan-to-text NMT, but that restrictions (e.g., single-entity mentions per sentence) make the approach dataset-dependent; no numeric metrics are provided in the text.",
            "comparison_to_others": "Plan-based two-stage pipelines can improve ordering and coherence relative to purely end-to-end seq2seq, but may be less flexible than end-to-end models or graph encoders that learn structure implicitly.",
            "advantages": "Offers explicit control over sentence ordering and discourse planning; can improve coherence in long or multi-sentence outputs.",
            "disadvantages": "Requires plan extraction or annotations; approaches with strict plan constraints are dataset-dependent and may not generalize.",
            "failure_cases": "Methods that rely on restrictive plan assumptions (e.g., single-entity per sentence) fail outside datasets that satisfy those constraints.",
            "uuid": "e8825.3",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Table serialization for PLMs",
            "name_full": "Attribute-value linearization / table serialization for pretrained language models",
            "brief_description": "Represent tables as linear sequences of attribute-value pairs (with separators) so that pretrained language models (GPT-2, T5, BART) can be fine-tuned for table-to-text tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Attribute-value table serialization",
            "representation_description": "Flatten each table row and/or column into a sequence of tokens like 'field: value' (or 'cell value is ...') with special separators between table and reference text, enabling direct input to PLMs.",
            "graph_type": "Tabular data (records / tables / info-boxes)",
            "conversion_method": "Serialize each row/column as 'field: value' sequences; strategies include horizontal traversal row-by-row, marking target cells via flags, and using separator tokens between table and text.",
            "downstream_task": "Table-to-text generation (WikiBio, RotoWire, ToTTo, TabFact, SciGen etc.), controlled one-sentence generation, long-form summarization from tables.",
            "performance_metrics": "Survey notes that T5 performed best on automated metrics when using such serializations but failed on summaries involving numerical calculations; no concrete numeric metrics provided in the survey.",
            "comparison_to_others": "Serialization + PLM fine-tuning competes with specialized tabular encoders; while PLMs achieve strong automated metric scores, they can struggle at arithmetic/logical inferences compared to table-specific architectures or explicit operation encoders.",
            "advantages": "Enables reuse of large pretrained text models with minimal architectural changes; simple to implement and scales to many PLMs.",
            "disadvantages": "May not capture row/column structural inductive biases; PLMs can fail on numeric arithmetic/reasoning tasks unless augmented with operation encoders.",
            "failure_cases": "T5 and similar PLMs can produce low-quality outputs when the task requires numerical calculations or logical operations over table cells; serialization may obscure structural relations needed for faithful generation.",
            "uuid": "e8825.4",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GCN with edge labels",
            "name_full": "Graph Convolutional Network encoder with explicit edge labels and directions",
            "brief_description": "A GCN variant that encodes node representations while explicitly modeling labeled edges and edge directions, with learned gates that weigh neighbor contributions—used to encode graphs before text decoding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GCN with edge-label & direction modeling",
            "representation_description": "Node representations are updated by aggregating transformed neighbor states; each neighbor contribution is modulated by parameters depending on edge label and direction and by learned scalar gates g_{u,v}, producing a relation-aware node encoding.",
            "graph_type": "AMR and general labeled knowledge graphs (graphs with typed/predicate edges)",
            "conversion_method": "Graph encoder (GCN) message-passing across neighbors: h_v' = ReLU(sum_{u in N(v)} g_{u,v}(W_dir(u,v) h_u + b_label(u,v))). The resulting node embeddings are used by a decoder (e.g., LSTM) to generate text.",
            "downstream_task": "Graph-to-text generation (AMR-to-text, knowledge-graph narration)",
            "performance_metrics": "Survey states that GCN-based encoders are adopted in graph-to-text pipelines and help capture relational structure; no numeric performance results are quoted in the survey.",
            "comparison_to_others": "GCNs preserve structural integrity better than naive linearization and provide complementary benefits relative to LSTM encoders; hybrid approaches that combine GCNs with sequential encoders can capture complementary strengths.",
            "advantages": "Relational inductive bias that preserves node/edge structure; explicit modeling of edge labels and directions improves relation-aware representations.",
            "disadvantages": "Requires modeling edge labels and directions (extra parameters); may need skip connections and architectural care (residual/dense) to handle deep stacks.",
            "failure_cases": "No explicit numeric failure cases in survey; implicit risks include underperforming when edge-type modeling is inadequate or when graph statistics differ strongly between train/test (structural generalization issues).",
            "uuid": "e8825.5",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GTR-LSTM",
            "name_full": "Graph-to-Text RNN (GTR-LSTM) / edge-aware LSTM graph encoder",
            "brief_description": "An LSTM-based graph encoding approach that computes hidden states for graph entities taking into account incoming edges and non-predefined relationships, ordering vertices via topological sort + breadth-first traversal before LSTM consumption.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GTR-LSTM (edge-aware LSTM graph encoding)",
            "representation_description": "Compute entity hidden states by considering edges that point to the entity from predecessor nodes; serialize an ordering over vertices (topological + BFS) and feed the ordered nodes to an LSTM that captures sequentialized graph structure with edge-awareness.",
            "graph_type": "General graphs / RDF / AMR variants where edge directions matter",
            "conversion_method": "Construct an ordering (topological + BFS) of graph nodes; compute hidden states using incoming-edge aware recurrence and feed to LSTM decoder or downstream module.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Survey describes this as able to handle non-predefined relationships and capture re-entrant structures; no numeric metrics provided.",
            "comparison_to_others": "Posited as complementary to GCN/RNN hybrids; better than purely linearized ordering because it encodes incoming-edge information, but still relies on an ordering.",
            "advantages": "Handles non-predefined relations and encodes incoming-edge context; bridges graph structure and sequential decoders.",
            "disadvantages": "Relies on an order over nodes (ordering choice affects representation); still sequential—may inherit RNN limitations.",
            "failure_cases": "Potential sensitivity to chosen node ordering; if the ordering does not reflect discourse-planning needs, coherence may suffer.",
            "uuid": "e8825.6",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "DualEnc (GCN + serialized GCN)",
            "name_full": "Dual-encoder: structural GCN + serialized (planning) GCN feeding LSTM decoder (DualEnc)",
            "brief_description": "A dual-encoder architecture where one GCN models the original graph structure and a second GCN serializes/re-orders nodes serving as a planning encoder; the two encoders provide complementary signals to the decoder.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "DualEnc (structural + serialized GCN)",
            "representation_description": "Two complementary encoders: (1) a GCN retaining the graph's structural integrity, and (2) a GCN that serializes/re-orders nodes (mimicking a plan) whose output is fed to an LSTM decoder—aiming to capture both structural and planning signals.",
            "graph_type": "AMR / RDF / other knowledge graphs requiring both structure and ordering",
            "conversion_method": "Parallel encode: structural GCN for node representations; serialized GCN that imposes an order for planning; combine outputs to inform decoder.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Survey reports DualEnc captures complementary effects of the two encoders and bridges GCN and LSTM strengths; no numeric metrics are provided.",
            "comparison_to_others": "A hybrid approach improving over single-encoder-only designs by combining structural fidelity with decoding-friendly order representations.",
            "advantages": "Combines structural preservation and planning/order signals; can improve coherence and factual fidelity by leveraging both encoders.",
            "disadvantages": "Increased model complexity; requires design choices for how to serialize/re-order nodes for the second encoder.",
            "failure_cases": "Not explicitly enumerated, but complexity and misalignment between the two encoder views could harm training if not tuned properly.",
            "uuid": "e8825.7",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GraphWriter (Graph Transformer)",
            "name_full": "Graph-centric Transformer encoder (GraphWriter / Graph Transformer)",
            "brief_description": "Extends the transformer architecture to directly consume graph-structured inputs by contextualizing each node with attention over its neighbors (multi-head graph attention), thereby avoiding RNN sequential limitations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph Transformer / GraphWriter",
            "representation_description": "Adapt transformer self-attention to graphs by computing contextualized node representations via neighbor-attention heads: each node attends to its neighbors with learned attention functions, optionally across multiple heads, and then feeds representations to a decoder for text generation.",
            "graph_type": "AMR/knowledge graphs and other node-edge labeled graphs",
            "conversion_method": "Use graph-aware multi-head attention where each node's representation is updated by attending to its graph neighbors (attention computed per neighbor), producing node embeddings fed to a decoder (e.g., transformer or LSTM).",
            "downstream_task": "Graph-to-text generation (AGENDA, AMR-to-text and general graph narration)",
            "performance_metrics": "Survey notes this addresses RNN shortcomings and is used in AGENDA-style tasks; no numeric performance metrics included in the survey.",
            "comparison_to_others": "GraphWriter (transformer-based) contrasts with GCN/GAT and RNN-based encoders by leveraging global attention and parallelism; better suited to overcome sequential RNN limitations.",
            "advantages": "Parallelizable, captures long-range dependencies via attention, better suits graphs with complex connectivity than RNNs.",
            "disadvantages": "May require positional/token-ordering design to handle sequential realization; tuning graph-aware attention can be complex.",
            "failure_cases": "No explicit failures described, though survey notes general caveats of PLMs and transformers on tasks requiring arithmetic or strict logical inference unless augmented.",
            "uuid": "e8825.8",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GAT global/local",
            "name_full": "Graph Attention Network with global and local message passing",
            "brief_description": "A GAT variant that captures complementary global and local contexts by maintaining separate message-passing streams (global and local) and combining them when building node representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GAT with global/local message passing",
            "representation_description": "Builds node representations with two complementary message-passing contexts: a global context (broad neighborhood information) and a local context (fine-grained neighbor information), often token-level and augmented with positional embeddings.",
            "graph_type": "Knowledge graphs and other structured graphs (token-level node representations)",
            "conversion_method": "Use two GAT-based message-passing processes (global and local) and combine the outputs to form node embeddings used by decoders.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Survey reports this approach captures complementary graph contexts and is applied token-level with positional embeddings; no numeric performance numbers provided.",
            "comparison_to_others": "Designed to capture both broad and fine-grained context compared to single-stream GAT/GCN approaches; complements transformer-style encodings.",
            "advantages": "Better contextualization by capturing multi-scale graph information; token-level modeling can preserve order where needed.",
            "disadvantages": "More parameters and complexity; requires design choices for balancing global vs local contributions.",
            "failure_cases": "Not explicitly listed; potential overfitting or computational cost on large graphs.",
            "uuid": "e8825.9",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Multi-view autoencoding",
            "name_full": "Detachable multi-view autoencoding for graphs (multi-view reconstruction losses)",
            "brief_description": "Decompose the graph into multiple views (e.g., triple-set view and linearized traversal view) and train models with multi-view reconstruction losses so the model learns both structural and serial views of the data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Detachable multi-view autoencoding",
            "representation_description": "Deconstruct input graph into multiple complementary views (e.g., structured triple set and a depth-first linearization) and add auxiliary reconstruction losses per view (e.g., deep biaffine for triples, linearized sequence reconstruction) to enrich training signal.",
            "graph_type": "AMR, RDF, and other knowledge graphs",
            "conversion_method": "Train encoder/decoder with additional autoencoding heads that reconstruct distinct views of the input (e.g., node-edge triples and linearized sequence), providing multiple supervision signals.",
            "downstream_task": "Graph-to-text generation (with stronger structural supervision), improved representation learning for generation",
            "performance_metrics": "Survey notes Song et al. enrich training signal with multi-view autoencoding losses; no numeric metrics are quoted in the survey text.",
            "comparison_to_others": "Provides richer supervision than single-view reconstruction or plain seq2seq training; complements other graph encoder choices.",
            "advantages": "Stronger structural supervision, improved robustness and possibly better few-shot behavior; helps models capture multiple facets of graph information.",
            "disadvantages": "Increased training complexity and auxiliary task engineering.",
            "failure_cases": "No explicit failure modes in survey; risk of conflicting signals between views if not carefully balanced.",
            "uuid": "e8825.10",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Node/edge reconstruction loss",
            "name_full": "Decoder-side node and edge reconstruction auxiliary losses (joint graph reconstruction)",
            "brief_description": "Extend the text decoder to jointly predict (as auxiliary tasks) the input graph's node labels and edge labels/directions while generating text, encouraging the generation process to remain grounded in input graph structure.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Node & edge reconstruction during decoding",
            "representation_description": "While decoding tokens, add auxiliary losses that predict node identities and edge labels (minimize word-to-node attention distance and negative log-likelihood over projected edges), thereby coupling generation with graph reconstruction.",
            "graph_type": "AMR / general graphs with node/edge labels",
            "conversion_method": "Augment standard language modeling loss with node-prediction and edge-prediction losses computed from decoder states and attention patterns (i.e., multitask loss).",
            "downstream_task": "Graph-to-text generation; improves faithfulness and few-shot learning",
            "performance_metrics": "Survey states that adding node/edge reconstruction losses improves performance, particularly in few-shot settings; no numerical metrics are reported in the survey.",
            "comparison_to_others": "Auxiliary reconstruction contrasts with plain seq2seq training by explicitly enforcing alignment with the source graph; related to reverse re-encoding/back-translation approaches.",
            "advantages": "Improves faithfulness to input graph and assists few-shot performance; provides stronger supervision linking text tokens to source graph elements.",
            "disadvantages": "Extra auxiliary tasks increase training complexity; requires designing effective projection/prediction heads.",
            "failure_cases": "No explicit failure cases described, but potential mismatch between reconstruction objectives and natural text realization could cause optimization trade-offs.",
            "uuid": "e8825.11",
            "source_info": {
                "paper_title": "Innovations in Neural Data-to-text Generation",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WebNLG dataset",
            "rating": 2,
            "sanitized_title": "webnlg_dataset"
        },
        {
            "paper_title": "Abstract Meaning Representation (AMR)",
            "rating": 2,
            "sanitized_title": "abstract_meaning_representation_amr"
        },
        {
            "paper_title": "GraphWriter (Graph-centric Transformer)",
            "rating": 2,
            "sanitized_title": "graphwriter_graphcentric_transformer"
        },
        {
            "paper_title": "Graph Convolutional Networks for graph-to-text (Marcheggiani and Titov)",
            "rating": 2,
            "sanitized_title": "graph_convolutional_networks_for_graphtotext_marcheggiani_and_titov"
        },
        {
            "paper_title": "Depth-first traversal linearization / plan-based NMT (Moryossef et al.)",
            "rating": 2,
            "sanitized_title": "depthfirst_traversal_linearization_planbased_nmt_moryossef_et_al"
        },
        {
            "paper_title": "DualEnc hybrid GCN+LSTM (Zhao et al.)",
            "rating": 2,
            "sanitized_title": "dualenc_hybrid_gcnlstm_zhao_et_al"
        },
        {
            "paper_title": "Node and edge reconstruction during decoding (Bai et al.)",
            "rating": 2,
            "sanitized_title": "node_and_edge_reconstruction_during_decoding_bai_et_al"
        },
        {
            "paper_title": "Table serialization for PLMs (Zhang et al., Gong et al.)",
            "rating": 2,
            "sanitized_title": "table_serialization_for_plms_zhang_et_al_gong_et_al"
        },
        {
            "paper_title": "Detachable multi-view autoencoding for graphs (Song et al.)",
            "rating": 1,
            "sanitized_title": "detachable_multiview_autoencoding_for_graphs_song_et_al"
        },
        {
            "paper_title": "Graph Attention Network global/local message passing (Ribeiro et al.)",
            "rating": 1,
            "sanitized_title": "graph_attention_network_globallocal_message_passing_ribeiro_et_al"
        }
    ],
    "cost": 0.022203499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Innovations in Neural Data-to-text Generation: A Survey</h1>
<p>MANDAR SHARMA, Virginia Tech, USA<br>AJAY KUMAR GOGINENI, Virginia Tech, USA<br>NAREN RAMAKRISHNAN, Virginia Tech, USA</p>
<p>The neural boom that has sparked natural language processing (NLP) research throughout the last decade has similarly led to significant innovations in data-to-text generation (D2T). This survey offers a consolidated view into the neural D2T paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating D2T from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for D2T research that not only focus on the design of linguistically capable systems but also systems that exhibit fairness and accountability.</p>
<p>CCS Concepts: $\cdot$ Computing methodologies $\rightarrow$ Natural language generation; Learning paradigms; Machine learning algorithms.</p>
<p>Additional Key Words and Phrases: narration, data-to-text, data-to-text generation, natural language generation</p>
<h2>1 INTRODUCTION</h2>
<p>Textual Representations of Information: A picture is worth a thousand words - isn't it? And hence graphical representation is by its nature universally superior to text - isn't it? Why then isn't the anecdote itself represented graphically? - Petre [246], in his advocacy for textual representation of information, challenges the notion that graphical representations of information are inherently more memorable, comprehensible, and accessible than their textual counterparts. Gershon and Page [104] note that the transformation of information from a textual to visual domain, in certain instances, requires further addition of information rendering textual representations more economical. Similarly, knowing where to look may not be obvious in visual representations of information - as validated through reading comprehension experiments [31, 115, 116] where participants were significantly slower in interpreting visual representations of the nested conditional structures within a program compared to their textual representations. This being said, these studies do not intend to dissuade the use of visual representations but rather establish the importance of textual representation of information. Often, the interplay of these paradigms brings out the best of both [289]. Thus, having established the importance of textual representations of information, we next explore how these notions tie into Data-to-text (D2T) generation.</p>
<h3>1.1 Defining Data-to-text Generation and Scope of the Survey</h3>
<p>Textual representations of information, for easier assimilation, are often presented as annotations outlining different behaviours of the underlying data stitched together. These stitched annotations, as showcased in Fig. 1, are referred to as narratives. The automated generation of such narratives, although serving several niches (see below), are most prevalent in the public eye through the practice of robo-journalism [66, 176]. Bloomberg News generates a third of its content with Cyborg, their in-house automation system that can dissect tedious financial reports and churn out news articles within seconds [243]. Also prevalent are the use of such systems in business intelligence</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
$T^{9}$ Narrative (Kansas): The first trace of Carbon Monoxide as a measurement of the air quality was observed in January 2000 in the state of Kansas. The mean CO observed can be dissected into 2 regimes of interest. The first regime spanning April 2006 to May 2006 and had an average CO emission of 0.44 . The second regime from April 2013 to April 2013 had an average of 0.13 . From January 2000 to July 2002, the observed values decreased from 0.9 to 0.0 . The observed values also decreased from 0.48 to 0.12 during February 2003 to May 2010. The mean CO emissions peaked in October 2000 at 2.58.
$\square$ Introduction $\quad \square$ Regimes Trends $\square$ Peak</p>
<p>Fig. 1. Illustration of D2T: Narration of time-series data (COVID19 progression in the United Kingdom at the top, the Carbon-Monoxide emissions in the state of Kansas, United States at the bottom) with the LLM-based framework $T^{3}$ (T-Cube) [294]. This D2T framework consumes a time-series as input and generates narratives that highlight the progression and points-of-interest (regimes, trends, and peaks) in the data through LLM-generated narratives.
(BI) settings with prominent commercial frameworks ${ }^{1}$ such as Arria NLG ${ }^{2}$, Narrative Science ${ }^{3}$, and Automated Insights ${ }^{4}$.</p>
<p>The practice of automating the translation of data to user-consumable narratives through such systems is known as data-to-text generation, as depicted in Fig. 1. Although encompassed by the general umbrella of Natural Language Generation, the nuance that differentiates D2T from the rest of the NLG landscape is that the input to the system has to qualify as a data instance. Reiter and Dale (1997) [270] describe the instance as a non-linguistic representation of information, and although narration of images and videos [70] has garnered interest in the NLG community, the definition of data-to-text employed by this survey follows that established by the seminal works prior [100, 270]: an entity that is not exclusively linguistic - tabular databases, graphs and knowledge bases, time-series, and charts. Using this clause, we limit the scope of our analysis and exclude examination of all other NLG systems that either both ingest and expel linguistic entities for downstream tasks such as machine translation [145, 350] or summarization [188, 228] or ingest non-conventional data such as images [131] and videos [328].</p>
<p>Outside of dataset specific tasks, practical applications of D2T include, but are not limited to:</p>
<ul>
<li>Weather forecasts [20, 271]</li>
<li>Sport summaries [15, 279, 316]</li>
<li>Healthcare [241, 249]</li>
<li>Virtual dietitians [9]</li>
<li>Stock market comments [10, 220]</li>
<li>Video-game dialouges [149] and Driving feedback [35]</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>With our scope defined, below we outline the rationale for this survey, followed by a structured examination of approaches, benchmark datasets, and evaluation protocols that constitute the D2T landscape with the intent to outline promising avenues for further research.</p>
<h1>1.2 Survey Rationale</h1>
<p>Following the seminal work by Reiter and Dale [270], the most comprehensive survey on D2T to-date has been that by Gatt and Krahmer [100]. Although several articles have taken a close examination of NLG sub-fields such as dialogue systems [282], poetry generation [234], persuasive text generation [76], social robotics [94], or exclusively focus on issues central to NLG such as faithfulness [181] and hallucination [142], a detailed break-down of the last half-decade of innovations has been missing since the last exhaustive body of work. The need for a close and consolidated examination of developments in neural D2T is more pertinent now than ever. Further, D2T distinguishes itself from other NLG tasks as it blends the generation of narratives with numerical reasoning between data points. Outside of the D2T niche, there are research communities focused on solving these individual problems - NLG [100, 235, 320] and numerical reasoning [295, 317, 334, 349]. Thus, neural D2T is uniquely positioned such that it either has to incorporate innovations from these seemingly disparate niches or jointly innovate on both fronts. We believe this provides added justification for D2T requiring its own comprehensive literature review.</p>
<p>As such, neural D2T borrows heavily from advances in other facets of NLG such as neural machine translation (NMT) [11, 350] and spoken dialogue systems (SDS) [79, 342, 343]. As such, the pertinence of such a survey also spans highlighting the stages of technological adoptions in the D2T paradigm and drawing distinctions between its NMT and SDS neighbors. Further, the adoption of such technologies brings about the adoption of shared pitfalls - inconsistencies in evaluation metrics [268] and meaningful inter-model comparisons [265]. Thus, in addition to an exhaustive examination of neural D2T frameworks, a consolidated resource on approaches to its evaluation is also necessary. Also crucial, is the discussion of benchmark datasets across shared tasks. The above considerations motivate our survey on the neural D2T paradigm intended to serve the following goals:</p>
<ul>
<li>Structured examination of innovations in neural D2T in the last half-decade spanning relevant frameworks, datasets, and evaluation measures.</li>
<li>Outlining the technological adoptions in D2T from within and outside of the greater NLG umbrella with the distinctions and shared pitfalls that lie therein.</li>
<li>Highlighting promising avenues for further D2T research and exploration that promote fairness and accountability along with linguistic prowess.</li>
</ul>
<h2>2 DATASETS FOR DATA-TO-TEXT GENERATION</h2>
<p>The first set of technological adoptions from NLP takes the form of dataset design: parallel corpora that align the data to their respective narratives are crucial for end-to-end learning, analogous to any neural-based approach to text processing. The initial push towards building such datasets began with database-text pairs of weather forecasts [20, 271] and sport summaries [15]. These datasets, and the convention that currently follows, use semi-structured data that deviates from the raw numeric signals initially used for D2T systems [266]. The statistics for prominent datasets among the ones discussed below are detailed in Table 2.</p>
<h3>2.1 Meaning Representations</h3>
<p>Mooney [217] defines a meaning representation language (MRL) as a formal unambigious language that allows for automated inference and processing wherein natural language is mapped to its</p>
<p>respective meaning representation (MR) through semantic parsing [101]. Robocup [42], among pioneering MR-to-text datasets, offers data from 1539 pairs of temporally ordered simulated soccer games in the form of MRs (pass, kick, turnover) accompanied with their respective human commentation. In order to mitigate the cost of building large-scale MR datasets, Liang et. al. [183] use grounded language acquisition to construct WeatherGov - a weather forecasting dataset with 29528 MR-text pairs, each consisting of 36 different weather states. Abstract meaning representation (AMR) [13], similarly, is a linguistically grounded semantic formalism representing the meaning of a sentence as a directed graph, as depicted in Fig. 2a. The LDC repository ${ }^{5}$ hosts various AMR-based corpora. Following this, using simulated dialogues between their statistical dialogue manager [357] and an agenda-based user simulator [283], Mairesse et. al. [204] offer BAGEL - an MR-text collection of 202 Cambridge-based restaurant descriptions each accompanied with two inform and reject dialogue types. Wen et. al. [343], through crowdsourcing, offer an enriched dataset conisiting of 5192 instances of 6 additional dialogue act types such as confirm and informonly ( 8 total) for hotels and restaurants in San Francisco. Novikova et. al. [232] show that crowdsourcing with the aid of pictorial stimuli yeild better phrased references compared to textual MRs. Following this, they released the E2E dataset ${ }^{6}$ as a part of the E2E challenge [230]. With 50,602 instances of MR-text pairs of restaurant descriptions, its lexical richness and syntactic complexity provides new challenges for D2T systems. Table 1 showcases comparative snapshots of the aforementioned datasets.</p>
<p>Table 1. Comparative showcase of sample MRs (and their corresponding narratives) from the RoboCup, WeatherGov, BAGEL, SF Hotels and Restaurants, and E2E datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">MR</th>
<th style="text-align: center;">Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RoboCup[42] <br> badPass(arg1=pink11,...), ballstopped() <br> ballstopped(), kick(arg1=pink11) <br> turnover(arg1=pink11,...)</td>
<td style="text-align: center;">pink11 makes a bad pass and was picked off by purple3</td>
</tr>
<tr>
<td style="text-align: center;">WeatherGov [183] <br> rainChance(time $=26-30, \ldots$ ), temperature(time $=17-30, \ldots$ ) <br> windDir(time $=17-30, \ldots$ ), windSpeed(time $=17-30, \ldots$ ), <br> precipPotential(time $=17-30, \ldots$ ), rainChance(time $=17-30, \ldots$ )</td>
<td style="text-align: center;">Occasional rain after 3am. Low around 43. South wind between 11 and <br> 14 mph . Chance of precipitation is $80 \%$. New rainfall amounts between <br> a quarter and half of an inch possible.</td>
</tr>
<tr>
<td style="text-align: center;">BAGEL [204] <br> inform( name(the Fountain) <br> near(the Arts Picture House) <br> area(centre), pricerange(cheap))</td>
<td style="text-align: center;">There is an inexpensive restaurant called the Fountain in the centre of <br> town near the Arts Picture House</td>
</tr>
<tr>
<td style="text-align: center;">SF Hotels \&amp; Rest. [343] <br> inform( name="red door cafe", <br> goodformeal="breakfast", <br> area="cathedral hill", kidsallowed="no")</td>
<td style="text-align: center;">red door cafe is a good restaurant for breakfast in the area of cathedral <br> hill and does not allow children.</td>
</tr>
<tr>
<td style="text-align: center;">E2E [232] <br> name[Loch Fyne], eatType[restaurant], <br> food[French], priceRange[less than $£ 20$ ], <br> familyFriendly[yes]</td>
<td style="text-align: center;">Loch Fyne is a family-friendly restaurant providing wine and cheese <br> at a low cost.</td>
</tr>
</tbody>
</table>
<h1>2.2 Graph Representations</h1>
<p>Graph-to-text translation is not only central to D2T as its application carries over to numerous NLG fields such as question answering [74, 130], summarization [86], and dialogue generation [190, 216]. Further, the D2T frameworks for graph-to-text borrow heavily from the theoretic formulations offered from the literature in the field of graph neural networks (GNNs), as will be discussed in §5.1.5. The domain-specific benchmark datasets, as discussed above (see §2.1) inherently train models to generate stereotypical domain-specific text. By crowdsourcing annotations for DBPedia</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>[211] graphs spanning 15 domains, Gardent et. al. [99] introduce the WebNLG dataset ${ }^{7}$. The data instances are encoded as Resource Description Format (RDF) triples of the form (subject, property, object) as depicted in Fig. 2b - (Apollo 12, operator, NASA). With 27,731 multi-domain graph-text pairs, WebNLG offers more semantic and linguistic diversity than previous datasets twice its size [342]. The abstract generation dataset (AGENDA) [168], built with knowledge graphs extracted from articles in the proceedings of AI conferences [6] using SciIE [199], offers 40,000 graph-text pairs of the article abstracts. To further promote generation challenges and cross-domain generalization, Nan et. al. [224] merge the E2E and WebNLG dataset with large heterogeneous collections of diverse predicates from Wikipedia tables annotated with tree ontologies to generate the data-record-to-text (DART) corpus. With 82,191 samples, this resulting open-domain corpus is almost quadruple the size of WebNLG.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. AMR [169] and knowledge graph [274] snapshots, representing variants of graph-based inputs to D2T systems.</p>
<h1>2.3 Tabular Representations</h1>
<p>Information represented in large tables can be difficult to comprehend at a glance, thus, table-totext (T2T) aims to generate narratives highlighting crucial elements of a tabular data instance through summarization and logical inference over the table - as showcased in Figure 3. Similar to graph-to-text, the underpinnings of tabular representation learning is also shared with other fields outside of NLG, such as the generation of synthetic network traffic [276, 353].</p>
<p>WikiBio [174], as an initial foray towards a large-scale T2T dataset, offers 700k table-text pairs of Wikipedia info-boxes with the first paragraph of its associated article as the narrative. With a vocabulary of 400 k tokens and 700 k instances, WikiBio offers a substantially larger benchmark compared to the pioneering WeatherGov and Robocup datasets that have less than 30k data-text pairs. For neural systems, as the length of output sequence increases, the generated summary diverges from the reference. As such, the RotoWire dataset [347] (Fig. 3), consisting of verbose descriptions of NBA game statistics, brings forth new challenges in long-form narrative generation as the average reference length of RotoWire is 337 words compared to 28.7 of WikiBio. Similarly, with the observation that only $60 \%$ of the content in RotoWire narratives can be traced back to the data records, Wang [335] introduce RotoWire-FG, a refined version of the original dataset aimed at tackling divergence (see §3.2), where narrative instances not grounded by their respective tables are removed from the dataset. TabFact [50] contains annotated sentences that are either supported or refuted by the tables extracted from Wikipedia. Similar to RotoWire-FG, Chen et. al. [47] offer a filtered version of TabFact by retaining only those narratives that can be logically inferred from the table.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">WIN</th>
<th style="text-align: center;">LOSS</th>
<th style="text-align: center;">PTS</th>
<th style="text-align: center;">FG.PCT</th>
<th style="text-align: center;">RB</th>
<th style="text-align: center;">AS ...</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TEAM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Heat</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">Hawks</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PLAYER</td>
<td style="text-align: center;">AS</td>
<td style="text-align: center;">RB</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">FG</td>
<td style="text-align: center;">FGA</td>
<td style="text-align: center;">CITY ...</td>
</tr>
<tr>
<td style="text-align: center;">Tyler Johnson</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Dwight Howard</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Paul Milhap</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Goran Dragic</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Wayne Ellington</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Dennis Schroder</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Rodney McGruder</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Miami</td>
</tr>
<tr>
<td style="text-align: center;">Thabo Sefolosha</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Atlanta</td>
</tr>
<tr>
<td style="text-align: center;">Kyle Korver</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Atlanta</td>
</tr>
</tbody>
</table>
<p>The Atlanta Hawks defeated the Miami Heat , 103 - 95 , at Philips Arena on Wednesday . Atlanta was in desperate need of a win and they were able to take care of a shorthanded Miami team here. Defense was key for the Hawks, as they held the Heat to 42 percent shooting and forced them to commit 16 turnovers. Atlanta also dominated in the paint, winning the rebounding battle, 47 - 34 , and outscoring them in the paint 58 - 26. The Hawks shot 49 percent from the field and assisted on 27 of their 43 made baskets. This was a near wire - to - wire win for the Hawks, as Miami held just one lead in the first five minutes. Miami ( 7 15 ) are as beat - up as anyone right now and it 's taking a toll on the heavily used starters. Hassan Whiteside really struggled in this game, as he amassed eight points, 12 rebounds and one blocks on 4 - of - 12 shooting ...</p>
<p>Fig. 3. Showcasing the intent of T2T, the statistics of a basketball match between the Atlanta Hawks and the Miami Heat (left) is to be translated into easily consumable narratives (right). Snapshot from the RotoWire dataset [347].</p>
<p>For controlled generation, Parikh et. al. [239] propose ToTTo which generates a single sentence description of a table on the basis of a set of highlighted cells where annotators ensure that the target summary only contains the specified subset of information. With over 120k training samples, ToTTo establishes an open-domain challenge for D2T in controlled settings. Similarly, to evaluate narrative generation in open domain settings with sentences that can be logically inferred from mathematical operations over the input table, Chen et. al. [47] modify the reference narratives of the TabFact dataset to construct LogicNLG with 7392 tables. Following this, with tables and their corresponding descriptions extracted from scientific articles, Moosavi et. al. [218] introduce SciGen, where the narratives include arithmetic reasoning over the tabular numeric entries. Building upon the long form generation premise of RotoWire, Chen et. al. [45] construct WikiTableT, a multi-domain table-text dataset with 1.5 million instances pairing Wikipedia descriptions to their corresponding info-boxes along with additional hyperlinks, named-entities, and article metadata. The majority of these datasets are available in a unified framework through TabGenie ${ }^{8}$.</p>
<p>Table 2. Highlights from prominent D2T datasets: format, number of samples (size), the number of linguistic tokens across the dataset (tokens), and availability of non-anglo-centric variants.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: left;">Format</th>
<th style="text-align: left;">Size</th>
<th style="text-align: center;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">MR</td>
<td style="text-align: left;">50,602</td>
<td style="text-align: center;">65,710</td>
</tr>
<tr>
<td style="text-align: left;">LDC2017T10</td>
<td style="text-align: left;">AMR</td>
<td style="text-align: left;">39,260</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG (en, ru)</td>
<td style="text-align: left;">RDF</td>
<td style="text-align: left;">27,731</td>
<td style="text-align: center;">8,886</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">RDF</td>
<td style="text-align: left;">82,191</td>
<td style="text-align: center;">33,200</td>
</tr>
<tr>
<td style="text-align: left;">WikiBio</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">728,321</td>
<td style="text-align: center;">400,000</td>
</tr>
<tr>
<td style="text-align: left;">RotoWire</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">4,853</td>
<td style="text-align: center;">11,300</td>
</tr>
<tr>
<td style="text-align: left;">TabFact</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">16,573</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ToTTo</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">120,000</td>
<td style="text-align: center;">136,777</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLG</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">37,000</td>
<td style="text-align: center;">52,700</td>
</tr>
<tr>
<td style="text-align: left;">WikiTableT</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">1.5 M</td>
<td style="text-align: center;">169 M</td>
</tr>
</tbody>
</table>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.4 Data Collection \&amp; Enrichment</h1>
<p>The majority of the prominent datasets discussed in $\S 2.1$ - $\S 2.3$ are either collected by merging aligned data-narrative pairs that occur naturally in the "wild" [174, 347] or are collected through dedicated crowd-sourcing approaches [99, 230]. However, there are notable works that employ a hybrid approach to data collection. CACAPO [324], an MR-style multi-domain dataset, follows a collection process inspired by Oraby et. al. [236] wherein the naturally-occurring narratives are first scraped from the internet and are later manually annotated to generate attribute-value pairs. Similarly, Chart-to-Text [153] follows a similar mechanism of data collection wherein candidate narratives for each chart are first automatically generated via a heuristic-based approach and then are rated by crowd-sourced workers. In similar lines, the ToTTo dataset [239] discussed in $\S 2.3$ uses crowd-sourced annotators as data "cleaners" - iteratively improving upon the automatically scraped narratives, rather than annotating them from scratch - thus greatly reducing the cost of data acquisition. In addition to innovations in data collection, efforts from the D2T community has also focused on the enrichment of existing datasets. As such, Ferreira et. al. [90] augment the WebNLG dataset with intermediate representation for discourse ordering and referring expression generation. By manually delexicalizing (see $\S 4.1$ ) the narratives, Ferreira et. al. were able to automatically extract a collection of referring expressions by tokenizing the original and delexicalized texts and finding the non-overlapping tokens between them. Similarly, the authors also extracted the order of the arguments in the text by referring to the order of the general tags in the delexicalized texts. This work has also been extended to enrich the E2E dataset [92].</p>
<h2>3 DATA-TO-TEXT GENERATION FUNDAMENTALS AND NOTATIONS</h2>
<h3>3.1 What to Say and How to Say It</h3>
<p>The data instance, typically, contains more information than what we would intend for the resulting narrative to convey - verbose narratives that detail every attribute of the data instance contradicts the premise of consolidation. Thus, to figure out what to say, a subset of the original information content is filtered out based on the target audience through the process of content selection. Starting from data-driven approaches such as clustering [75] and the use of hidden Markov models (HMMs) [16], the attention of the research community has recently shifted to learning alignments between the data instance and its narrative [183]. Bisazza and Marcello [30] note that pre-reordering the source words to better resemble the target narrative yeilds significant improvements in NMT. Prior to neural explorations, learning this alignment has been explored with log-linear models [8] and tree representations [170, 171]. With what to say determined, the next step lies in figuring out how to say it, that is, the construction of words, phrases, and paragraphs - this realization of the narrative structure is known as surface realization. While traditionally, the processes of content selection and surface realization [148, 270] act as discrete parts of the generation pipeline, the neural sequence-to-sequence (seq2seq) paradigm jointly learns these aspects. For a peripheral view of the articles discussed in this section, Table 4 highlights prominent papers categorized based on their D2T tasks and the benchmark datasets used. Similarly, Fig. 5 outlines the organization of the remainder of this survey.</p>
<h3>3.2 Hallucinations and Omissions</h3>
<p>Apart from the importance of coherence and linguistic diversity in surface realization, data fidelity is a crucial aspect of D2T systems - the narrative should neither hallucinate contents absent from the data instance nor omit contents present in the data instance. Often, the divergence present in benchmark training datasets, wherein the narrative may contain data absent from the source or not cover the entirety of the data instance, is the culprit behind hallucination tendencies in</p>
<p>the model [280]. Often times, the need for both linguistic diversity and data fidelity turns into a balancing act between conflicting optimization objectives leading to novel challenges [128]. While almost all of the D2T approaches discussed below engage in balancing coherence and diversity with data-fidelity (besides $\S 5.1 .4$ Stylistic Encoding), overarchingly, the approach to balancing these conflicting objectives can be thought to take place in two forms:</p>
<ul>
<li>Architectural Interventions: The sections §5.1.1 Entity Encoders, §5.1.2 Hierarchical Encoders, §5.1.3 Plan Encoders \&amp; Autoencoders, §5.1.5 Graph Encoders, §5.1.6 Reconstruction \&amp; Hierarchical Decoders, and §5.1.10 Supplemental Frameworks suggest modifications/augmentations to the seq2seq architecture such that it fosters data-fidelity tendencies.</li>
<li>Loss-function Interventions: An alternative avenue to achieving a balance between conflicting optimization objectives is to directly model the objective functions to perform multi-task learning: as such sections $\S 5.1 .7$ Regularization Techniques and $\S 5.1 .8$ Reinforcement Learning suggest modifications/augmentations to the seq2seq loss functions.</li>
</ul>
<h1>3.3 Establishing Notation and Revisiting Seq2Seq</h1>
<p>For the consistency and readability of this survey, the notation outlining the basic encoder-decoder seq2seq paradigm [11, 54, 314, 326] in D2T (Fig. 4), as defined below and respectively compiled in Table 3, will remain valid throughout unless stated otherwise. However, the namespace for additional variable definitions in the individual sections will be limited to their mentions. Let $S=\left{x_{j}, y_{j}\right}<em j="j">{j=1}^{N}$ be a dataset of $N$ data instances $x$ accompanied with its natural language narrative $y$. Based on the construction of $S, x$ can be a set of $K$ data records $x=\left{r</em>\right}<em j="j">{j=1}^{K}$ with each entry $r$ comprised of its respective entity $r . e$ and value $r . m$ attributes or $x$ can be an instance of a directed graph $x=(V, E)$ with vertices $v \in V$ and edges $(u, v) \in E$. In the RotoWire instance (Figure 3), for $r</em>$, several alterations to modeling the attention weights have been proposed [201, 331].}=$ Heat, $r_{j} . e=$ WIN attribute would have value $r_{j} . m=11$. Given pairs $(x, y)$, the seq2seq model $f_{\theta}$ is trained end-to-end to maximize the conditional probability of generation $P(y \mid x)=\prod_{t=1}^{T} P\left(y_{t} \mid y_{&lt;t}, x\right)$. The parameterization of $f_{\theta}$ is usually carried out through RNNs such as LSTMs [29, 133] and GRUs [54], or transfomer ${ }^{5}$ architectures [326]. For attention-based RNN architectures with hidden states $h_{t}$ and $s_{t}$ for the encoder and decoder respectively, the context vector $c_{t}=\sum_{i} \alpha_{t, i} h_{i}$ weighs the encoder hidden states with attention weights $\alpha_{t, i}$. While Bahdanau et. al. [11] use a multi-layer perceptron (MLP) to model $\alpha_{t, i</p>
<p>For handling of out-of-vocabulary (OOV) tokens, Gu et. al. [119] attempt to model the rote memorization process of human learning where a language model conditioned on binary variable $z_{t} \in{0,1}$ can either generate $p_{\text {gen }}$ the next token or copy it from the source $p_{\text {copy }}$ based on their respective probabilities. While Gu et. al. [119] and Yang et. al. [355] parameterize the joint distribution over $y_{t}$ and $z_{t}$ directly (1), Gulçehre et. al. [121] decompose the joint probability (2), using an MLP to model $p\left(z_{t} \mid y_{&lt;t}, x\right)$.</p>
<p>$$
\begin{gathered}
P\left(y_{t}, z_{t} \mid y_{&lt;t}, x\right) \propto\left{\begin{array}{l}
p_{\text {gen }}\left(y_{t}, y_{&lt;t}, x\right) z_{t}=0 \
p_{\text {copy }}\left(y_{t}, y_{&lt;t}, x\right) z_{t}=1, y_{t} \in x \
0 \quad z_{t}=1, y_{t} \notin x
\end{array}\right. \
\left{\begin{array}{l}
p_{\text {gen }}\left(y_{t} \mid z_{t}, y_{&lt;t}, x\right) p\left(z_{t} \mid y_{&lt;t}, x\right) z_{t}=0 \
p_{\text {copy }}\left(y_{t} \mid z_{t}, y_{&lt;t}, x\right) p\left(z_{t} \mid y_{&lt;t}, x\right) z_{t}=1
\end{array}\right.
\end{gathered}
$$</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. Attention-based seq2seq framework: The encoder consumes the sequential input translating it to a weighed hidden representation to be then consumed and decoded into linguistic tokens by the decoder.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Data-to-text Generation Taxonomy Corresponding to Sections in the Survey Design</p>
<p>Similar to the greater NLG paradigm, different strategies for modeling the conditional probability of generation $P(y \mid x)$, the attention mechanisms $\left{\alpha_{t, i}, c_{t}\right}$, and the copy mechanisms $\left{p_{\text {gen }}, p_{\text {copy }}\right}$, as discussed below, often form the basis for D2T innovations. In addition to this, variations in training strategies such as teacher-forcing [346], reinforcement learning [315], and autoencoder-based reconstruction [41] open up further avenues for D2T innovation.</p>
<h1>4 INNOVATIONS IN DATA PREPROCESSING</h1>
<p>Contrary to the other facets of NLG, such as chatbots, for which large-scale data can be harvested [1, 198], D2T datasets are often smaller in scale and task-specific. Ferreira et. al. [88] note that phrase-based translation models [166] can outperform neural models in such data sparsity. As such, delexicalization, noise reduction, linearization, and data augmentation are preprocessing techniques often employed to tackle said sparsity of training data.</p>
<h3>4.1 Delexicalization \&amp; Noise Reduction</h3>
<p>Delexicalization, often referred to as anonymization, is a common practice in D2T [79, 204] wherein the slot-value pairs for the entities and their attributes in training utterances are replaced with a</p>
<p>Table 3. Notation descriptions</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Notation</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$S$</td>
<td style="text-align: left;">Dataset</td>
</tr>
<tr>
<td style="text-align: left;">$(x, y) \in S$</td>
<td style="text-align: left;">Data instance $x$ and its natural language representation $y$</td>
</tr>
<tr>
<td style="text-align: left;">$r=(r . e, r . m)$</td>
<td style="text-align: left;">Data record $r$ with its entity $r . e$ and value $r . m$ attributes</td>
</tr>
<tr>
<td style="text-align: left;">$G=(V, E)$</td>
<td style="text-align: left;">Graph instance $G$ with vertices $V$ and edges $E$</td>
</tr>
<tr>
<td style="text-align: left;">$(u, v) \in E$</td>
<td style="text-align: left;">Nodes $u$ and $v$ of an edge $E$</td>
</tr>
<tr>
<td style="text-align: left;">$f_{\partial} \in\left{f_{1}, \ldots, f_{n}\right}$</td>
<td style="text-align: left;">Model $f_{\partial}$ that may belong to an ensemble $\left{f_{1}, \ldots, f_{n}\right}$</td>
</tr>
<tr>
<td style="text-align: left;">$P(y \mid x)$</td>
<td style="text-align: left;">Conditional probability of sequence $y$ given $x$</td>
</tr>
<tr>
<td style="text-align: left;">$h_{t}, s_{t}$</td>
<td style="text-align: left;">Encoder $h_{t}$ and decoder $s_{t}$ hidden state representations</td>
</tr>
<tr>
<td style="text-align: left;">$c_{t}, \alpha_{t, i}$</td>
<td style="text-align: left;">Context vector $c_{t}$ weighing $h_{t}$ with attention weights $\alpha_{t, i}$</td>
</tr>
<tr>
<td style="text-align: left;">$p_{g e n}, p_{c o p y}$</td>
<td style="text-align: left;">Token generation $p_{g e n}$ or copying $p_{c o p y}$ probabilities</td>
</tr>
<tr>
<td style="text-align: left;">$z_{t} \in{0,1}$</td>
<td style="text-align: left;">Binary variable that selects either $p_{g e n}$ or $p_{c o p y}$</td>
</tr>
<tr>
<td style="text-align: left;">$W_{t \in \mathbb{N}}, b_{i \in \mathbb{N}}$</td>
<td style="text-align: left;">Arbitrary weights and biases parameterizing $f_{\partial}$</td>
</tr>
</tbody>
</table>
<p>placeholder token such that weights between similar utterances can be shared [227] - as illustrated in Figure 6a. These placeholder tokens are later replaced with tokens copied from the input data instance [174]. In comparison to copy-based methods for handling rare entities, delexicalization has shown to yield better results in constrained datasets [300].</p>
<p>From the notion that delexicalization of the data instance may cause the loss of vital information that can aid seq2seq models in sentence planning, where some data instance slots may even be deemed nondelexicalizable [343], Nayak et. al. [227] explore different nondelexicalized input representations (mention representations) along with grouping representations as a form of sentence planning (plan representations). The authors note improvements over delexicalized seq2seq baselines when input mentions are concatenated with each slot-value pair representing a unique embedding. The efficacy of such concatenation is also corroborated by Freitag and Roy [95]. Further, the addition of positional tokens representing intended sentence position to the input sequence offers further improvements. Addressing this, in addition to delexicalizing categorical slots, Juraska et. al. [150] employ hand-crafted tokens for values that require different treatment in their verbalization: for the slot food, the value Italian is replaced by slot_vow_cuisine_food indicating that the respective utterance should start with a vowel and the value represents a cuisine - an Italian restaurant. Perez-Beltrachini and Lapata [244] delexicalize numerical expressions, such as dates, using tokens created with the attribute name and position of the delexicalised token. Colin and Gardent [59] note performance improvements with an extensive anonymization scheme wherein all lemmatized content words (expect adverbs) are delexicalized as compared to restricting delexicalization to named entities.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. Illustrations of delexicalization in MRs [300] and linearization of graphs [274].</p>
<p>The presence of narratives that fail to convey vital attributes of data instances leads to semantic noise in the dataset [84, 136]. Dušek et. al. [78] employ slot matching [263] to clean the E2E corpus for semantic correctness and explore the impact of semantic noise on neural model performance.</p>
<p>Similarly, Obeid and Hoque [233] substitute data mentions in the narrative, identified through Named Entity Recognition (NER), with a predefined set of tokens which are later replaced through a look-up operation. Liu et. al. [194] focus on generating faithful narratives and truncate the reference narratives by retaining only the first few sentences since the latter are prone to have been inferred from the table.</p>
<h1>4.2 Linearization</h1>
<p>Frameworks for MR (and graph) narration that shy from dedicated graph encoders rely on effective linearization techniques - the representation of graphs as linear sequences, as illustrated in Figure 6b. While Ferreira et. al. [88] note improvements in neural models with the adoption of a 2-step classifier [177] that maps AMRs to the target text, Konstas et. al. [169] showcase agnosticsm to linearization orders by grouping and anonymizing graph entities for delexicalization with the Stanford NER [93]. The reduction in graph complexity and subsequent mitigation of the challenge brought forth by data sparsity lends any depth-first traversal of the graph as an effective linearization approach. Moryossef et. al. [219] append text plans modeled as ordered trees [308] to the WebNLG training set and use an off-the-shelf NMT system [121] for plan-to-text generation. However, the authors note that the restriction of requiring single entity mentions in a sentence establishes their approach as dataset dependent.</p>
<p>For pretrained language models such as GPT-2 [257] and T5 [258], Zhang et. al. [361] and Gong et. al. [109] represent tables as a linear sequence of attribute-value pairs and use a special token as the separator between the table data and the reference text. It should be noted that T5, while performing the best on automated metrics, fails to generate good summaries when numerical calculations are involved. Chen et. al. [47] traverse the table horizontally, each row at a time, where each element is represented by its corresponding field and cell value separated by the keyword is. For scientific tables, Suadaa et. al. [312] view a table $T_{D}$ as a set of cells with their corresponding row and column headers $h=[r h: c h]$ with $t h$ for overlapping tokens, numerical value $v a l$, and metric-type $m$. The cells are marked with target flag tgt which is set to 1 for targeted cells and 0 otherwise respective to the content plan. The linearization of the resulting tables is done with templates that consist of concatenation $T_{D}=[h: t h: v a l: m: t g t]$, filtration based on tgt, pre-computed mathematical operations, and their respective combinations.</p>
<h3>4.3 Data Augmentation</h3>
<p>Often, appending contextual examples from outer sources to the training set, or permuting the training samples themselves to append variation, helps mitigate data sparsity. This is known as data augmentation. Nayak et. al. [227] propose the creation of pseudo-samples by permuting the slot orderings of the MRs while keeping the utterances intact. Juraska et. al. [150], however, take an utterance-oriented approach where pseudo-samples are built by breaking training MRs into single-sentence utterances. For the shared surface realization task [213], Elder and Hokamp [83] augment the training set with sentences from the WikiText corpus [212] parsed using UDPipe [309]. Following this premise, Kedzie and Mckeown [159] curate a collection of utterances from novel MRs using a vanilla seq2seq model with noise injection sampling [53]. The validity of the MRs associated with these utterances are computed through a CNN-based parser [162] and the valid entries are augmented to the training set. However, it is worth noting, that performance gains from augmenting the training set with out-of-domain (OOD) instances, tend to saturate after a certain point [95]. Also, practitioners of data augmentation should note that caution is advised when augmenting with synthetic data, as the inclusion of such data may reinforce the mistakes of the model [126].</p>
<p>Chen et. al. [46] append knowledge-graphs representing external context to the table-text pairs and quantify its efficacy through their metric KBGain - the ratio of tokens unique to the external context to the total number of tokens in the narrative. Similarly, Ma et. al. [202] augment the limited training data for table-text pairs by assigning part-of-speech (POS) tags for each word in the reference and further increase the robustness of their model with adversarial examples created by randomly adding and removing words from the input. In contrast, Chen et. al. [47] create adversarial examples by randomly swapping entities in the narrative with ones that appear in the table. Following this, Liu et. al. [194] use an augmented plan consisting of table records and entities recognized from the reference narrative which eliminates the inclusion of information not present in the table. For few-shot learning, Liu et. al. [189] observed that the performance of a GPT-3 model [37] improved upon providing in-context examples computed based on their k-nearest neighbor $(k=2)$ embeddings.</p>
<h1>5 INNOVATIONS IN THE SEQ2SEQ FRAMEWORK</h1>
<p>Seq2Seq models (see §3.3), serve as the basis for neural NLG [54, 314, 326]. As such, to compare the efficacy of neural architectures for long-form D2T, Wiseman et. al. [347] compare the performance of various seq2seq models to their templated counterparts on the RotoWire dataset. Based on their observations, the conditional copy model [121] performs the best on both word-overlap and extractive metrics (see $\S 6.2 .1$ ) compared to the standard attention-based seq2seq model [11] and its joint copy variant [119]. Similarly, in an evaluation of 62 seq2seq, data-driven, and templated systems for the E2E shared task, Dušek et. al. [81] note that seq2seq systems dominate in terms of both automated word-based metrics and naturalness in human judgement. Wiseman et. al. [347], however, note that the traditional templated generation models outperform seq2seq models on extractive metrics although they score poorly on word-overlap metirics. Thus, the adaptation of seq2seq models to D2T for richer narratives with less omissions and hallucinations still remains an active focus of the research community.</p>
<p>It is worth noting that all seq2seq models discussed below operate at the word level. Models operating at the character level [3, 112, 277] have shown reasonable efficacy with the added computational savings from forgoing the preprocessing steps of delexicalization and tokenization. However, the attention garnered by them from the research community is slim. From their comparative analysis, Jagfeld et. al. [140] note that as character-based models perform better on the E2E dataset while word-based models perform better on the more linguistically challenging WebNLG dataset, it is hard to draw conclusions on the framework most suited for generic D2T. In the sections that follow, we detail notable innovations over the last half-decade in seq2seq modeling, branched on the basis of their training strategies - supervised and unsupervised learning.</p>
<h3>5.1 Supervised Learning</h3>
<p>5.1.1 Entity Encoders. Centering theory [118], as well as many other noted linguistic frameworks [40, 57, 125, 156, 172, 251], highlight the critical importance of entity mentions to the coherence of the generated narrative. The ordering of these entities ( $r . e$ in $\S 3.3$ ) is crucial for such narratives to be considered as entity coherent [154]. Unlike typical language models which are conditioned solely based on previously generated tokens $c_{t}$, Lebret et. al. [174] provide additional context $\left{z_{c_{t}}, g_{f}, g_{w}\right}$ to the generation where $z_{c_{t}}$ represents table entity $c_{t}$ as a triplet of its corresponding field name, start, and end positions, and $\left{g_{f}, g_{w}\right}$ are one-hot encoded vectors where each element indicates the presence of table entities from the fixed field and word vocabularies - illustrated in Fig. 7. Similarly, Bao et. al. [14] encode the table cell $c$ and attributes $a$ as the concatenation $\left[e_{i}^{c}: e_{i}^{a}\right]$ where the decoder uses this vector to compute the attention weights. Liu et. al. [193]</p>
<p>Table 4. Task and dataset based summarization of noted D2T frameworks over the last half-decade.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Publication Highlights</th>
<th style="text-align: center;">Framework \&amp; Human Evaluation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MR-to-Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Robocup \&amp; WeatherGov</td>
<td style="text-align: center;">[210] Coarse-to-fine aligner \&amp; penalty based on learned priors</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + regularization</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Recipe \&amp; SF H\&amp;R</td>
<td style="text-align: center;">[160] Neural agenda-checklist modeling</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU + agenda encoders</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">BAGEL</td>
<td style="text-align: center;">[79] Retanking beam outputs w/ BNN-based reranker</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + reranker</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Restaurant Ratings</td>
<td style="text-align: center;">[227] Nondelexicalized inputs w/ data augmentation</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">WikiData</td>
<td style="text-align: center;">[52] Complementary text-to-data translation</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">E2E</td>
<td style="text-align: center;">[81] Comparative evaluation of 62 systems</td>
<td style="text-align: center;">Seq2Seq + Data-driven + Templated</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[256] MLP encoder attuned to the dataset</td>
<td style="text-align: center;">MLP $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[360] Two-level hierarchical encoder</td>
<td style="text-align: center;">CAEncoder $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[150] Ensemble w/ heuristic reranking</td>
<td style="text-align: center;">Ensemble w/ LSTM + CNN</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[310] Hierarchical decoding with POS tags</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[95] Unsupervised DTG with DAEs</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + DAEs</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[103] Comparative evaluations w/ ensembling \&amp; penalties</td>
<td style="text-align: center;">Ensemble w/ LSTM + T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[62] Syntactic controls with SC-LSTM</td>
<td style="text-align: center;">SC-LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[297] Computational pragmatics based DTG</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[59] Extensive anonymization</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[78] Semantic correctness in neural DTG</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[159] Self-training w/ noise injection sampling</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[277] Char-level GRU w/ input reconstruction</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[97] CRFs w/ Gumbel categorical sampling</td>
<td style="text-align: center;">CRF</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Graph-to-Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGENDA</td>
<td style="text-align: center;">[168] Graph-centric Transformer \&amp; AGENDA dataset</td>
<td style="text-align: center;">T $\rightarrow$ LSTM + LSTM encoding</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">LDC2015E25</td>
<td style="text-align: center;">[88] Phrase vs Neural MR-text w/ preprocessing analysis</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + Phrase-based</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">LDC2015E86</td>
<td style="text-align: center;">[169] Unlabeled pre-training \&amp; linearization agnosticism</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[273] Dual encoding for hybrid traversal</td>
<td style="text-align: center;">GNN $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">LDC2017T10</td>
<td style="text-align: center;">[12] Graph reconstruction w/ node \&amp; edge projection</td>
<td style="text-align: center;">T $\rightarrow$ T + reconstruction loss</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[203] Fine-tuning GPT-2 on AMR-text joint distribution</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">WebNLG</td>
<td style="text-align: center;">[207] Graph encoding with GCNs</td>
<td style="text-align: center;">GCN $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[68] LSTM based triple encoder</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[91] Discrete neural pipelines \&amp; comparisons to end-to-end</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU + T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[219] Sentence planning with ordered trees</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[275] Complementary graph contextualization</td>
<td style="text-align: center;">GAT $\rightarrow$ T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[306] Detachable multi-view reconstruction</td>
<td style="text-align: center;">T $\rightarrow$ T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[364] Dual encoder for structure and planning</td>
<td style="text-align: center;">GCN $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[274] Task-adaptive pretraining for PLMs</td>
<td style="text-align: center;">BART + T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[2] Knowledge enhanced language models \&amp; KeLM dataset</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[158] Graph-text joint representations \&amp; pretraining strategies</td>
<td style="text-align: center;">BART + T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Record-to-Text (Table-to-text)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WikiBio</td>
<td style="text-align: center;">[174] Tabular positional embeddings \&amp; WikiBio dataset</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + Kneser-Ney</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[14] Encoding tabular attributes \&amp; WikiTableText dataset</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[193] Field information through modified LSTM gating</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[292] Link-based and content-based attention</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[254] Multi-instance learning w/ alignment-based rewards</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[202] Key fact identification and data augment for few shot</td>
<td style="text-align: center;">LSTM + T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[191] Hierarchical encoding w/ supervised auxiliary learning</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[192] Forced attention for omission control</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[46] External contextual information w/ knowledge graphs</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[318] Confidence priors for hallucination control</td>
<td style="text-align: center;">BERT + Pointer Networks</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[51] Soft copy switching policy for few-shot learning</td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[356] Variational autoencoders for template induction</td>
<td style="text-align: center;">VAE modified to VTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[337] Autoregressive modeling with iterative text-editing</td>
<td style="text-align: center;">Pointer networks + Text editing</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[365] Reinforcement learning with adversarial networks</td>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[105] Linearly combined multi-reward policy</td>
<td style="text-align: center;">Pointer networks</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[354] Source-target disagreement auxiliary loss</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[311] BERT-based IR system for contextual examples</td>
<td style="text-align: center;">T5 + BERT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[347] Classification-based metrics \&amp; RotoWire dataset</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM + Templated</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[229] Numeric operations and operation-result encoding</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU + operation encoders</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[253] Dynamic hierarchical entity-modeling \&amp; MLB dataset</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[252] Content selection \&amp; planning w/ gating and IE</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Rotowire</td>
<td style="text-align: center;">[107] Contextualized numeric representations</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[139] Dynamic salient record tracking w/ stylized generation</td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[261] Two-tier hierarchical input encoding</td>
<td style="text-align: center;">T $\rightarrow$ LSTM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[180] Auxiliary supervision w/ reasoning over entity graphs</td>
<td style="text-align: center;">LSTM + GAT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[255] Paragraph-centric macro planning</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[254] Interweaved plan and generation w/ variational models</td>
<td style="text-align: center;">LSTM $\rightarrow$ LSTM</td>
<td style="text-align: center;">Y</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Publication Highlights</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Framework \&amp; Human Evaluation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Record-to-Text (Continued)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TabFact</td>
<td style="text-align: center;">[47] Coarse-to-fine two-stage generation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + T + GPT-2 + BERT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">WikiPerson</td>
<td style="text-align: center;">[339] Disagreement loss w/ optimal-transport matching loss</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">T</td>
</tr>
<tr>
<td style="text-align: center;">Humans, Books \&amp; Songs</td>
<td style="text-align: center;">[109] Attribute prediction-based reconstruction loss</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">ToTTo <br> LogicNLG <br> NumericNLG</td>
<td style="text-align: center;">[189] Contextual examples through k nearest neighbors</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[312] Targeted table cell representation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[49] Semantic confounders w/ Pearl's do-calculus</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DCVED + GPT</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[187] Table-to-logic pretraining for logic text generation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 + BART</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[223] Faithful generation with unlikelihood \&amp; replacement detection</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[44] Table serialization and structural encoding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ GPT-2</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[7] T5 infused with tabular embeddings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;">Cross-domain</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">E2E <br> WebNLG <br> DART <br> WikiBio <br> RotoWire <br> WITA</td>
<td style="text-align: center;">[140] Char-based vs word-based seq2seq</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRU $\rightarrow$ GRU</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[348] Template induction w/ neural HSMM decoder</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">HSMM</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[98] Training w/ partially aligned dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ T + supportiveness</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[157] Iteratively editing templated text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2 + LaserTagger</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[127] RoBERTa-based semantic fidelity classifier</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-2 + RoBERTa</td>
<td style="text-align: center;">T</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[48] Knowledge-grounded pre-training \&amp; KGTEXT dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ T + GAT</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[186] Hybrid attention-copy for stylistic imitation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[351] Disambiguation and stitching with PLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT3 + T5</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[77] Unified learning of D2T and T2D</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 + VAE</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[144] Search and learn in a few-shot setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5 + Search \&amp; Learn</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Timeseries-to-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WebNLG \&amp; DART</td>
<td style="text-align: center;">[294] Open-domain transfer learning for time-series narration</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BART + T5 + Timeseries analysis</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Chart-to-text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chart2Text</td>
<td style="text-align: center;">[233] Preprocessing w/ variable substitution</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">T $\rightarrow$ T</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: center;">Chart-to-text</td>
<td style="text-align: center;">[153] Neural baselines for Chart-to-text dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM + T + BART + T5</td>
<td style="text-align: center;">Y</td>
</tr>
</tbody>
</table>
<p>modify the LSTM unit with a field gate to update the cell memory indicating the amount of entity field information to be retained in the cell memory. Following [174], Ma et. al. [202] use a Bi-LSTM to encode the concatenation of word, attribute and position embeddings. However, to indicate whether an entity is a key fact, a multi-layer perceptron (MLP) classifier is used on said representation for binary classification. Inspired from Liu and Lapata [195], Gong et. al. [108] construct a historical timeline by sorting each table record with respect to its date field. Three encoders encode a table entity separately in row $r_{i, j}^{r}$, column $r_{i, j}^{c}$, and time $r_{i, j}^{t}$ dimensions. The concatenation of these representations are fed to a MLP to obtain a general representation $r_{i, j}^{\text {gen }}$ over which specialized attention weights are computed to obtain the final record representation as $\hat{r}<em r="r">{i, j}=\alpha</em>$ of the input table on top of the token embeddings for table-structure learning in a T5 model.} r_{i, j}^{r}+\alpha_{c} r_{i, j}^{c}+\alpha_{l} r_{i, j}^{t}$. Exploiting the attributes of the E2E dataset - the set number of unique MR attributes and the limited diversity in lexical instantiations of their values, Puzikov and Gurevych [256] employ a simple approach wherein the recurrent encoder is replaced with one dense layer that takes in MR representations through embedding lookup. Similarly, to keep track of entity mentions in the SF dataset for long-form text generation, Kiddon et. al. [160] introduce a checklist vector $a_{t}$ that aids two additional encoders to track used (mentioned in the resulting narrative) and new (not mentioned as of time step $t$ ) items on the defined agenda. The output hidden state is modeled as a linear interpolation between the three encoder states - $c_{t}^{g r u}$ of the base GRU and $\left{c_{t}^{\text {new }} \cdot c_{t}^{\text {used }}\right}$ from the agenda models, weighted by a probabilistic classifier. Extending this concept of entity encoding to transformer-based architectures, Chen et. al. [44] adapt the multi-headed attention layer architecture [326] to encode serialized table attributes that is then fed to a GPT-based decoder. Similarly, as a unified text-to-text alternative approach to [44], Andrejczuk et. al. [7] include the row and column embeddings $\hat{r}_{i, j</p>
<p>Certain D2T tasks, such as sport commentaries [15, 279, 316], require reasoning over numeric entities present in the input data instance. Although numeracy in language modeling is a prominent niche of its own [295, 317, 334], notable D2T-specific approaches include that of Nie et. al. [229] precomputing the results of numeric operations $o p_{i} \in{$ minus, argmax $}$ on the RotoWire dataset,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table $\left(\mathbf{g}<em _mathbf_w="\mathbf{w">{\mathbf{r}}, \mathbf{g}</em>\right)$}</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Input Text</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">John Doe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">John</td>
<td style="text-align: center;">Doe</td>
<td style="text-align: center;">(</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">April</td>
</tr>
<tr>
<td style="text-align: center;">Birthdate</td>
<td style="text-align: center;">18 April 1352</td>
<td style="text-align: center;">$\varepsilon_{i}$</td>
<td style="text-align: center;">13944</td>
<td style="text-align: center;">UNK</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{x}_{w}$</td>
<td style="text-align: center;">(name,1,2)</td>
<td style="text-align: center;">(name,2,1)</td>
<td style="text-align: center;">$\phi$</td>
<td style="text-align: center;">(birth.,1,3)</td>
<td style="text-align: center;">(birth.,2,2)</td>
</tr>
</tbody>
</table>
<p>Fig. 7. Entity encoding scheme - Lebret et. al. [174]
the authors propose the combination of dedicated operation and operation-result encoders, the latter utilizing a quantization layer for mapping lexical choices to data values, in addition to a record encoder. In similar fashion to [355], the concatenated embeddings ${r . i d x, r . e, r . m}$ fed to a bi-directional GRU generate record representations while the concatenated embeddings of $o p_{i}$ attributes fed to a non-linear layer yields operation representations. To address the difficulty in establishing lexical choices on sparse numeric values [271, 303], the authors add quantization to the operation-results encoder that maps results of scalar operations $e$ (minus) to $l \in L$ possible bins through a weighed representation ( $h_{i}=\sum_{l} \mu_{i, l} e$ ) using softmax scores of each individual result $\mu_{i, l}$. Following this body of work, to contextualize numeric representations and thus understand their logical relationships, Gong et. al. [107] feed raw numeric embeddings for all numericals corresponding to the same table attributes to a transformer-based encoder to obtain their contextualized representations. Through a ranking scheme based on a fully connected layer, these contextualized representations are further trained to favor larger numbers.
5.1.2 Hierarchical Encoders. The intuition behind the use of hierarchical encoders, in the context of D2T, is to model input representations at different granularities, either through dedicated modules [191, 261, 360] or attention schemes [193, 253, 355]. As such, Zhang et. al. [360] leverage their CAEncoder [359] to incorporate precomputed future representations $h_{i+1}$ into current representation $h_{i}$ through a two-level hierarchy. Similarly, Rebuffel et. al. [261] propose a two-tier encoder to preserve the data structure hierarchy - the first tier encodes each entity $e_{i}$ based on its associated record embeddings $r_{i, j}$ while the second tier encodes the data structure based on its entity representation $h_{i}$ obtained through the individual embeddings $r_{i, j}$. On the other hand, Liu et. al. [191], as illustrated in Fig. 8b, propose a word-level $h_{r . e}^{r . m}$ and an attribute-level $H^{r . e}$ two-encoder setup to capture the attribute-value hierarchical structure in tables. The attribute-level encoder takes in the last hidden state $h_{l a s t}^{r . e}$ for attribute $r . e$ from the word level LSTM as its input. Using these hierarchical representations, fine-grained attention $\beta_{r . e}^{r . m} \propto g\left(h_{r . e}^{r . m}, s_{t}\right)$ and coarse-grained attention $\gamma^{r . e} \propto g\left(H^{r . e}, s_{t}\right)$ are used for decoding where $g$ represents a softmax function. Similarly, based on hierarchical attention [355], Liu et. al. [193] employ an attention scheme that attends to both word level and field level tokens. Following this, Puduppully et. al. [253] propose language modeling conditioned on both the data instance and a dynamically updated entity representation. At each time-step $t$, a gate $\gamma_{t}$ is used to decide whether an update is necessary for the entity memory representation $u_{k}$ and a parameter $\delta_{t, k}$ decides the impact of said update (3).</p>
<p>$$
\gamma_{t}=\sigma\left(W_{1} s_{t}+b_{1}\right) \&amp; \delta_{t, k}=\gamma_{t} \odot \sigma\left(W_{2} s_{t}+W_{3} u_{t-1, k}+b_{3}\right)
$$</p>
<p>5.1.3 Plan Encoders \&amp; Autoencoders. Traditionally, the what to say aspect of D2T (see §3.1) used to be its own module in a set of pipelines [100, 270], thus offering flexibility in planning the narrative structure. However, the end-to-end learning paradigm often models content selection and surface realization as a shared task [174, 210, 347]. Although convenient, without explicitly</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 8. Illustrations for plan [252] and hierarchical [191] encoding schemes.
modeling the planning of the narrative (Fig. 8a), language models struggle to keep coherence in long-form generation tasks. As such, Puduppully et. al. [252] model the generation probability $P(y \mid r)$ as the joint probability of narrative $y$ and content plan $z$ given a record $r$ such that $P(y \mid r)=$ $\sum_{z} P(z \mid r) P(y \mid r, z)$. Similar to their prior work [253], a content selection gate operates over the record representation $r_{i}$ giving an information controlled representation $r_{i}^{c s}$. The elements of $z$ are extracted using an information extraction system [347] and correspond to entities in $y$ while pointer networks [331] are use to align elements in $z$ to $r$ during training. Iso et. al. [139], on the other hand, avoid precomputing content plans $z$ by dynamically choosing data records during decoding an additional memory state $h^{e n t}$ remembers mentioned entities and updates the language model state $h^{l m}$ accordingly. The authors propose two representations for an entity - static embedding $e$ based on row $r_{i}$ and aggregated embedding $\bar{e}$ based on all rows where the entity appears. In the context of the RotoWire dataset, the aggregate embedding $\bar{e}$ is supposed to represent how entity $e$ played in the game. For $h_{t}=\left{h_{t}^{l m}, h_{t}^{e n t}\right}, P\left(z_{t}=1 \mid h_{t-1}\right)$ (4) models the transition probability, and based on whether $e$ belongs to the set of entities $\epsilon_{t}$ that have already appeared at time step $t, P\left(e_{t}=e \mid h_{t-1}\right)(5)$ computes the next probable entity $e$ to mention. The authors note that such discrete tracking dramatically suppresses the generation of redundant relations in the narrative.</p>
<p>$$
\begin{aligned}
&amp; P\left(z_{t}=1 \mid h_{t-1}\right)=\sigma\left(W_{1}\left(h_{t-1}^{l m} \oplus h_{t-1}^{e n t}\right)\right) \
&amp; P\left(e_{t}=e \mid h_{t-1}\right) \propto \begin{cases}e^{\left(h_{s}^{e n t} W_{1} h_{t-1}^{l m}\right)} &amp; e \in \epsilon_{t-1} \
e^{\left(\bar{e} W_{2} h_{t-1}^{l m}\right)} &amp; \text { otherwise }\end{cases}
\end{aligned}
$$</p>
<p>With the premise that paragraphs are the smallest sub-categorization where coherence and topic are defined [358], Puduppully and Lapata [255] propose a paragraph-based macro planning framework specific to the design of MLB [253] and RotoWire [347] datasets where the input to the seq2seq framework are predicted macro-plans (sequence of paragraphs). Building upon this, in contrast to precomputing global macro plans, Puduppully et. al. [254] interweave the macro planning process with narrative generation where latent plans are sequentially inferred through a structured variational model as the narrative is generated conditioned on the plans so far and the previously generated paragraphs. Similarly, to establish order in the generation process, Sha et. al. [292] incorporate link-based attention [114] in addition to content-based attention [11] into their framework. Similar to transitions in Markov chains [155], a link matrix $\mathbb{L} \in \mathbb{R}^{n_{f} \times n_{f}}$ for $n_{f}$ tabular attributes defines the likelihood of transitioning from the mention of attribute $i$ to $j$ as $\mathbb{L}\left(f_{j}, f_{i}\right)$. Wang et. al. [337] propose combining autoregressive modeling [287] to generate skeletal plans and using an iterative text-editing based non-autoregressive decoder [120] to generate narratives constrained on</p>
<p>said skeletal plans. The authors note that this approach reduces hallucination tendencies of the model. Similarly, motivated by the strong correlation observed between entity-centric metrics for record coverage and hallucinations, Liu et. al. [194] adopt a two-stage generation process where a plan generator first transforms the input table records into serialized plans $R \rightarrow R+P$ based on the separator token $S E P$ and then translates the plans into narratives with the help of appended auxiliary entity information extracted through NER.</p>
<p>Handcrafted templates traditionally served as pre-defined structures where entities computed through content selection would be plugged-in. However, even in the neural D2T paradigm, inducing underlying templates helps capture the narrator voicing and stylistic representations present in the training set. As such, Ye et. al. [356] extend the use of the variational autoencoders (VAEs) [163] for template induction with their variational template machine (VMT) that disentangles the latent representation of the template $z$ and the content $c$. In essence, the model can be trained to follow specific templates by sampling from $z$. Inspired from stylistic encoders [137], the authors further promote template learning by anonymizing entities in the input table thus effectively masking the content selection process. Similarly, to mitigate the strong model biases in the standard conditional VAEs [319], Chen et. al. [49] estimate semantic confounders $z_{c}$ - linguistically similar entities to the target tokens that confound the logic of the narrative. Compared to the standard formulation $p(y \mid x)$, the authors employ Pearl's do-calculus [242] to learn the objective $p(y \mid \operatorname{do}(x))$ that asserts that confounder $z_{c}$ is no longer determined by instance $x$, thus ensuring logical consistency in the narrative. To ensure that the estimated confounders are meaningful, they are grounded through proxy variables $c$ such that confounding generation $p\left(c \mid z_{m}\right)$ can be minimized. Recently, modeling D2T and T2D as complementary tasks, Doung et. al. [77] leverage the VAE formulation with the underlying architecture of a pre-trained T5 model to offer a unified multi-domain framework for the dual task. To combat the lack of parallel-corpora for the back-translation (T2D) training, the authors introduce latent variables to model the marginal probabilities of back-translation through an iterative learning process.</p>
<p>Likewise, for approaches beyond the use of autoencoders, Chen et. al. [47] take inspiration from practices in semantic parsing [71] and propose a coarse-to-fine two-stage generation scheme. In the first stage, a template $Y_{T}$ containing placeholder tokens $E N T$ is generated, representing the global logical structure of the narrative. The entities are then copied over from the input data instance to replace tokens $E N T$ in the second step to generate the final narrative $\hat{Y}$. Suadaa et. al. [312], similarly, follow template-guided generation [151] (see $\S 4.2$ ) where the precomputed results of numeric operations are copied over to the template and replace the placeholder tokens. For Pre-trained Language Models (PLMs), the authors incorporate copying into the fine-tuning stage for this action.
5.1.4 Stylistic Encoders. In addition to the traits of coherence, fluency, and fidelity, stylistic variation is crucial to NLG [307]. It is interesting to note that the n-gram entropy of generated texts in seq2seq based NLG systems are significantly lower than that in its training data - leading to the conclusion that these systems adhere to only a handful of dominant patterns observed in the training set [237]. Thus, introducing control measures to text generation has recently garnered significant attention from the NLG community [137, 344]. As such, the semantically conditioned LSTM (SC-LSTM) proposed by Wen et. al. [343] extends the LSTM cell to incorporate a one-hot encoded MR vector $d$ that takes the form of a sentence planner. Following this, Deriu and Cieliebak [62] append additional syntactic control measures to the MR vector $d$ (such as the first token to appear in the utterances and expressions for different entity-value pairs) by simply appending one-hot vectors representation of these control mechanisms to $d$. Similarly, Lin et. al. [186] tackle the lack of a template-based parallel dataset with style imitation - as illustrated in Fig. 9, for each</p>
<p>instance $(x, y)$, an exemplar narrative $y_{e}$ is retrieved from the training set based on field-overlap distance $D\left(x, x_{e}\right)$ and an additional encoder is used to encode $y_{e}$. The model is trained with competing objectives for content determination $P\left(y \mid x, y_{e}\right)$ and style embodiment $P\left(y_{e} \mid x_{e}, y_{e}\right)$ with an additional content coverage constraint for better generation fidelity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Food</th>
<th style="text-align: center;">Area</th>
<th style="text-align: center;">Price</th>
<th style="text-align: center;">Near</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Loch Fyne</td>
<td style="text-align: center;">Italian</td>
<td style="text-align: center;">Riverside</td>
<td style="text-align: center;">€20-25</td>
<td style="text-align: center;">Strada</td>
</tr>
<tr>
<td style="text-align: center;">Exemplar $y_{e}$</td>
<td style="text-align: center;">Zizzi is a pub providing fine French dining but with an expensive price, located near Cocum in the city center.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Generation $y$</td>
<td style="text-align: center;">Loch Fyne provides fine Italian dining with a <br> $£ 20-25$ price, located near Strada at the riverside.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Fig. 9. Style imitation with exemplar narratives - Lin et. al. [186]
5.1.5 Graph Encoders. The use of explicit graph encoders in D2T stems from the intuition that neural graph encoders such as Graph Convolutional Networks (GCNs) [164] have strong relational inductive baises that produce better representations of input graphs [18] as an effective alternative to linearization. This entails generating representations for the nodes $v \in V$ and edges $(u, v) \in E$ in the input graph.</p>
<p>GCNs \&amp; Graph-RNNs: Marcheggiani and Titov [208] compute node represenations $h_{v}^{\prime}$ (6) through explicit modeling of edge labels $l a b(u, v)$ and directions $\operatorname{dir}(u, v) \in{$ in, out, loop $}$ for each neighboring node $u \in N(v)$ in their GCN parameterization where learned scalar gates $g_{u, v}$ weigh the importance of each edge. With residual $\left(h_{v}^{r}=h_{v}^{\prime}+h_{v}\right)$ [129] and dense $\left(h_{v}^{d}=\left[h_{v}^{\prime} ; h_{v}\right]\right)$ [138] skip connections, Marcheggiani and Perez-Beltrachini [207] adopt the above-mentioned encoder with an LSTM decoder [201] for graph-to-text generation. Differing from previous iterations of Graph LSTMs [184], Distiawan et. al. [68] compute the hidden states of graph entities with consideration of the edges pointing to the entity from the previous entities, allowing their GTR-LSTM framework to handle non-predefined relationships. The ordering of the vertices fed into the LSTM is based on a combination of topological sort and breath-first traversal. Inspired by hybrid traversal techniques [226, 298], Ribeiro et. al. [273] propose a dual graph encoder-first operating on a top-down traversal of the input graph where the predicate $p$ between two nodes is used to transform labelled edges $\left(u_{i}, p, u_{j}\right)$ to two unlabelled edges $\left(u_{i}, p\right)$ and $\left(p, u_{j}\right)$ while the second operates on a bottom-up traversal where directions of edges are reversed $\left(u_{i}, u_{j}\right) \rightarrow\left(u_{j}, u_{i}\right)$.</p>
<p>$$
h_{v}^{\prime}=\operatorname{ReLU}\left(\sum_{u \in N(x)} g_{u, v}\left(W_{d i r(u, v)} h_{u}+b_{l a b(u, v)}\right)\right)
$$</p>
<p>Damonte and Cohen [61] note that GCNs can assist LSTMs in capturing re-entrant structures and long term dependencies. As such, to bridge the gap between the GCN [19, 285] and the linearized LSTM encoders in graph-to-text translation, Zhao et. al. [364] propose DualEnc which uses both to capture their complementary effects. The first GCN models the graph, retaining its structural integrity, while the second GCN serializes and re-orders the graph nodes resembling a planning stage and feeds it to the LSTM decoder.</p>
<p>GATs \&amp; Graph Transformers: To address the shortcomings of RNN-based sequential computing, Koncel-Kedziorski et. al. [168] extend the transformer architecture [326] to graph-structured inputs with the GraphWriter. The distinction of GraphWriter from graph attention networks (GAT) [327] is</p>
<p>made through the contextualization of each node representation $v_{i}$ (7) with respect to its neighbours $u_{j} \in N\left(v_{i}\right)$ through attention mechanism $a_{n}$ for the $\mathcal{N}$ attention heads. In contrast, Ribeiro et. al. [275] focus on capturing complementary graph contexts through distinct global $h_{n}^{\text {global }}$ and local $h_{n}^{\text {local }}$ message passing using GATs. Their approach to graph modeling also differs in its token-level approach for node representations with positional embeddings injected to preserve sequential order of the tokens.</p>
<p>$$
\hat{v}<em i="i">{i}=v</em>+|<em N_left_v__i="N\left(v_{i">{n=1}^{\mathcal{N}} \sum</em>\right)
$$}\right)} \alpha_{i j}^{n} W_{n}^{n} u_{j} \quad \&amp; \quad \alpha_{i j}^{n}=a^{n}\left(v_{i}, u_{j</p>
<p>Song et. al. [306] enrich the training signal to the relation-aware transformer model [367] through additional multi-view autoencoding losses [264]. This detachable multi-view framework deconstructs the input graph into triple sets for the first view, reconstructed with a deep biaffine model [73], and linearizes the graph through a depth-first traversal for the second view. In contrast, Ke et. al. [158] obtain the entity and relation embeddings through contextual semantic representations with their structure-aware semantic aggregation module added to each transformer layer the module consists of a mean pooling layer for entity and relation representations, a structureaware self-attention layer [296], and finally a residual layer that fuses the semantic and structural representations of entities.
5.1.6 Reconstruction \&amp; Hierarchical Decoders. Input Reconstruction: Conceptualized from autoencoders [34, 304, 330], reconstruction-based models quantify the faithfulness of an encoded representation by correlating the decoded representation to the original input. As such, Wiseman et. al. [347] adopt decoder reconstruction [321] to the D2T paradigm by segmenting the decoder hidden states $h_{t}$ into $\frac{T}{B}$ continuous blocks $b_{i}$ of size at most $B$. The prediction of record $r$ from such a block $b_{i}, p(r . e, r . m \mid b_{i})$, is modeled as $\operatorname{softmax}\left(f\left(b_{i}\right)\right)$ where $f$ is a convolutional layer followed by a multi-layer perceptron (MLP). To replicate the actions of an autoencoder, Chisholm et. al. [52] train a seq2seq based reverse re-encoding text-to-data model along with a forward seq2seq data-to-text model. Similarly, Roberti et. al. [277] propose a character-level GRU implementation where the recurrent module is passed as a parameter to either the encoder or the decoder depending on the forward $\hat{y}=f(x)$ or reverse $\hat{x}=g(y)$ direction. Following the mechanics of back-translation (text-to-data) [290, 321], Bai et. al. [12] extend the standard transformer decoder [326] to reconstruct the input graph by jointly predicting the node and edge labels while predicting the next token. The standard training objective of minimizing the negative log-likelihood of the conditional word probabilities $l_{\text {std }}$ is appended by a node prediction loss $l_{\text {node }}$ that minimizes the word-to-node attention distance and a edge prediction loss $l_{\text {edge }}$ that minimizes the negative log-likelihood over the projected edges. For table-structure reconstruction, Gong et. al. [109] define the reconstruction loss based on attribute prediction and content matching similar to the optimal transport distance [339]. It should be noted that these auxiliary tasks improve the model performance in few-shot settings.</p>
<p>Hierarchical Decoding: Similar to hierarchical encoding (see §5.1.2), hierarchical decoding intends to designate granular roles to each decoder in the hierarchy. Serban et. al. [291] show that injecting variations at the conditional output distribution does not capture high-level variations. As such, to model both high and low level variations, Shao et. al. [293] propose their planning-based hierarchical variational model (PHVM) based on the conditional variational auto-encoder [305]. PHVM follows a hierarchical multi-step encoder-decoder setup where a plan decoder first generates a subset $g$ of the input $\left{d_{i}, \ldots, d n\right} \in x$. Then, in the hierarchical generation process, a sentence decoder and a word decoder generate the narrative conditioned on plan $g$. To dissipate the decoder responsibilities</p>
<p>in the seq2seq paradigm, Su et. al. [310] propose a 4-layer hierarchical decoder where each layer is responsible for learning different parts of the output speech. The training instances are appended with part-of-speech (POS) tags such that each layer in the decoder hierarchy is responsible for decoding words associated with a specific set of POS patterns.</p>
<p>Hierarchical Attention-based Decoding: To alleviate omissions in narrative generation, Liu et. al. [192] propose forced attention - with word-level coverage $\theta_{t}^{i}$ and attribute-level coverage $\gamma_{t}^{e}$, a new context vector $\hat{c}<em t="t">{t}=\pi c</em>}+(1-\pi) v_{t}$ is defined with a learnable vector $\pi$ and a compensation vector $v_{t}=f\left(\theta_{t}^{i}, \gamma_{t}^{e}\right)$ for low-coverage attributes $e$. To enforce this at a global scale, similar to Xu et. al. [352], a loss function $\mathbb{L<em t="t">{F A}$ based on $\gamma</em>$ is appended to the seq2seq loss function.
5.1.7 Regularization Techniques. Similar to regularization in the greater deep learning landscape [110], regularization practices in D2T append additional constraints to the loss function to enhance generation fidelity. As such, Mei et. al. [210] introduce a coarse-to-fine aligner to the seq-to-seq framework that uses a pre-selector and refiner to modulate the standard aligner [11]. The preselector assigns each record a probability $p_{i}$ of being selected based on which the refiner re-weighs the standard aligner's likelihood $w_{t i}$ to $\alpha_{t i}$. The weighted average $z_{t}=\sum_{i} \alpha_{t i} m i$ is used as a soft approximation to maintain the architecture differentiability. Further, the authors regularize the model with a summation of the learned priors $\sum_{i=1}^{N} p_{i}$ as an approximation of the number of selected records. Similarly, Perez-Beltrachini and Lapata [244] precompute binary alignment labels for each token in the output sequence indicating its alignment with some attribute in the input record. The prediction of this binary variable is used as an auxiliary training objective for the D2T model. For tabular datasets, Liu et. al. [191] propose a two-level hierarchical encoder that breaks the learning of semantic tabular representation into three auxiliary tasks incorporated into the loss function of the model. The auxiliary sequence labeling task $L_{S L}$, learnt in unison with seq2seq learning, predicts the attribute name for each table cell. Similarly, the auto-encoder supervision $L_{A E}$ penalizes the distance between the table $z_{t}$ and the narrative $z_{b}$ representations, while the multi-label supervision task $L_{M L}$ predicts all the attributes in the given table. The individual losses, along with the language modeling loss, defines the loss function of the framework. To mitigate information hallucination and avoid the high variance exhibited by the use of policy gradients in the reinforcement-learning paradigm, Wang et. al. [339] compute two losses in addition to the language modeling loss - the first checks the disagreement between the source table and the corresponding narrative through the L2 loss between their embeddings, similar to Yang et. al. [354], while the second uses optimal-transport [43] based maximum flow between the narrative and input distributions $\mu$ and $v$. Tian et. al. [318] propose the use of confidence priors to mitigate hallucination tendencies in table-to-text generation through learned confidence scores. At each decoding step $y_{t}$, instead of concatenating all the previous attention weights, only the antecedent attention weight $a_{t-1}$ is fed back to the RNN, such that an attention score $A_{t}$ can be used to compute how much $a_{t}$ affects the context vector $c_{t}$ - as all the source information in $c_{t}$ comes from $a_{t}$. The confidence score $C_{t}\left(y_{t}\right)$ is then used to sample target sub-sequences faithful to the source using a variational Bayes scheme [167]. Similarly, inspired by Liu et. al. [191], Li et. al. [180] propose two auxillary supervision tasks incorporated into the training loss - number ranking and importance ranking, both crucial to sport summaries, modeled with pointer networks on the outputs of the row and column encoders respectively.
5.1.8 Reinforcement Learning. In the D2T premise, language-conditional reinforcement learning (RL) [200] often aids in model optimization through its role as auxiliary loss functions. While traditionally, the BLEU (see $\S 6.1$ ) and TF-IDF [260] scores of generated texts were used as the basis for reinforcement learning [192], Perez-Beltrachini and Lapata [244] use alignment scores}^{e</p>
<p>of the generated text with the target text. Similarly, Gong et. al. [107] use four entity-centric metrics that center around entity importance and mention. Rebuffel et. al. [262] propose a model agnostic RL framework, PARENTing which uses a combination of language model loss and RL loss computed based on PARENT F-score [65] to alleviate hallucinations and omissions in table-to-text generation. To avoid model overfitting on weaker training samples and to ensure the rewards reflect improvement made over pretraining, self-critical training protocol [272] is applied using the REINFORCE algorithm [345]. The improvement in PARENT score over a randomly sampled candidate $y_{c}$ and a baseline sequence generated using greedy decoding $y_{b}$ is used as the reward policy. In contrast, Zhao et. al. [365] use generative adversarial networks (GANs) [111] where the generator is modeled as a policy with the current state being the generated tokens and the action defined as the next token to select. The reward for the policy is a combination of two values - the discriminator probability of the sentence being real and the correspondence between generated narrative and the input table based on the BLEU score. As RL frameworks based on singular metrics makes it difficult to simultaneously tackle the multiple facets of generation, Ghosh et. al. [105] linearly combine metrics for recall, repetition, and reconstruction, along with the BLEU score, to form a composite reward function. The policy is adapted from Wang et. al. [338] and trained using Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) [368].
5.1.9 Fine-tuning Pretrained Language Models. Pretrained lanuguage models (PLMs) [64, 257] have been successful in numerous text generation tasks [288, 363]. The extensive pretraining grants these models certain worldly knowledge [247] such that, at times, the models refuse to generate nonfactual narratives even when fed deliberately corrupted inputs [274]. As such, Mager et. al. [203] propose an alternate approach to fine-tuning GPT-2 for AMR-to-text generation where the fine-tuning is done on the joint distribution of the AMR $x_{j}$ and the text $y_{i}$ as $\prod_{i}^{N} p\left(y_{i} \mid y_{&lt;i}, x_{1: M}\right) \cdot \prod_{j}^{M} p\left(x_{j} \mid x_{&lt;j}\right)$. On the other hand, inspired by task-adaptive pretraining strategies for text classification [122], Ribeiro et. al. [274] introduce supervised and unsupervised task-adaptive pretraining stages as intermediaries between the original pretraining and the fine-tuning for graph-to-text translation. Interestingly, the authors note good performance of the task-adapted PLMs even when trained on shuffled graph representations. Chen et. al. [51] note the few-shot learning capabilities of GPT-2 for table-to-text generation when appended with a soft switching policy for copying tokens [287]. Similarly, as a light-weight alternative to fine-tuning the entire model, Li and Liang [182] take inspiration from prompting [37], and propose prefix-tuning which freezes the model parameters to only optimize the prefix, a task-specific vector prepended to the input. The authors note significant improvements in low-data settings when the prefix is initialized with embeddings from task specific words such as table-to-text. For avenues that allow leveraging the wordly knowledge of PLMS even without fine-tuning, Xiang et. al. [351] leverage a combination of prompting GPT-3 for disambiguation and T5 for sentence-fusion leading to a domain-agnostic framework for data-to-text generation.</p>
<p>Inspired from practices in unlikelihood learning [323, 341], Nan et. al. [223] model T5 as both a generator and a faithfulness discriminator with two additional learning objectives for unlikelihood and replacement detection. To train the model with said objectives, $n$ contradictory sentences $Y_{\text {False }}^{(i, j)}$ are generated for each entailed sentence $Y_{\text {True }}^{(i)}$ wherein the discrimination probability is computed at every step of token generation. Similarly, to address omissions in D2T (§3.2), Jolly et. al. [144] adapt the search-and-learn formulation [178] to a few-shot setting through a two-step finetuning process wherein a T5 model fine-tuned on the D2T task is further fine-tuned again with omitted attributes $r . e / r . m$ reinserted to the narratives as pseudo-grouthtruths.
5.1.10 Supplemental Frameworks. Supplementary Modules: Fu et. al. [98] propose the adaptation of the seq2seq framework for their partially-algined dataset WITA using a supportiveness adaptor</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Though the base transformer architecture is oblivious to input structures, we assume positionally encoded transformers to fall into the seq2seq paradigm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
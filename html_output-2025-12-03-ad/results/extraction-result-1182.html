<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1182 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1182</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1182</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-262053943</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.10309v2.pdf" target="_blank">Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill</a></p>
                <p><strong>Paper Abstract:</strong> Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of home-assistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real-world environments validate the effectiveness of our proposed navigation strategy. Code and video demos are available at https://github.com/wzcai99/Pixel-Navigator.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1182.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1182.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PixNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pixel-guided Navigation skill (PixNav)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RGB-only, pixel-goal navigation policy trained by supervised imitation on large-scale simulated trajectories that takes a goal pixel (converted to a small mask) and a history of RGB images and outputs discrete actions; architecture uses ResNet18 encoders, a transformer decoder, and heads for action, pixel tracking, and temporal-distance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HM3D / Habitat (photorealistic indoor scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic large-scale indoor household environments (HM3D) used in the Habitat simulator; domain: indoor household navigation / object search.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not explicitly quantified; environments are articulated indoor scenes composed of rooms and corridors; connectivity implicitly represented as room adjacency in planner graphs (sparse room-level graphs produced by clustering panoramic observations).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dataset: 80 training scenes, 20 validation scenes used for experiments; per-scene node/edge counts for local graphs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PixNav (transformer-based pixel-goal policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Low-level RGB-only policy: concatenates a small mask for the target pixel with the initial image (4-channel ResNet18) to produce a goal token; separate ResNet18 encodes each observation. A goal-fusion layer concatenates goal features to each observation token; a 4-layer transformer decoder integrates tokens and produces output tokens fed to MLP heads that predict discrete actions (Stop, MoveAhead, TurnLeft, TurnRight, LookUp, LookDown), the tracked pixel in each frame, and remaining temporal distance. Entire network ~54.5M parameters; trained by supervised imitation using planner-generated trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success Rate (SR), SPL (Success weighted by Path Length), Distance-to-Goal (DTG / DTS), 'distance to goal' at episode end</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Local short-range (1-3m): SR 0.88, SPL 0.88, DTS 0.54 (units: meters) for default camera (camera height 0.88 m, HFOV 79°). Long-range (3-5m): SR 0.42, SPL 0.41, DTS 1.61 (meters). Overall open-set object-nav reported: Success 37.9%, SPL 20.5% (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>See exploration_efficiency_value (88% for 1-3m local PixNav; 42% for 3-5m; 37.9% reported on open-set object-nav benchmark in Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Hierarchical: low-level reactive/planned pixel-goal controller (transformer-based, memory via history images) combined with a high-level LLM planner for room/waypoint selection; thus planning-based hierarchical policy with short-horizon reactive control.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>No direct quantitative analysis linking classical graph-topology metrics (diameter, clustering coefficient, dead-end counts, doors) to performance; qualitative findings: transformer-based PixNav is robust on short (1–3 m) tasks (high SR and SPL) and degrades on longer-horizon tasks (3–5 m); ablations show that components that explicitly preserve goal information (tracking prediction and goal-fusion) significantly affect long-horizon performance, indicating that increased effective path length or more complex topological structure (longer geodesic distances) requires explicit goal-tracking and stronger goal-conditioning in policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Findings in the paper indicate: (1) Transformer-based history encoding provides robustness for short-range navigation; (2) Explicit pixel-tracking and goal-fusion are critical for long-horizon (higher effective graph-distance) navigation — ablations removing tracking or fusion reduce 3–5 m success rates notably; (3) A hierarchical design where an LLM builds a room-level graph (via summarization, clustering, and edge inference) and proposes waypoints enables more consistent, semantic exploration than unguided roaming, implying that high-level graph-structured memory/planning benefits long-horizon object search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1182.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1182.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Planner / Local Layout Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-Language-Model-based Planner and LLM-generated local room-graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based high-level planner that consumes structured textual summaries (from a vision-language model) of six panoramic images, performs summarization, self-localization, clustering into room-level nodes, and constructs a local adjacency graph which it uses to pick next room/waypoint (converted to a pixel via object segmentation) balancing exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Local room-level graph derived from panoramic captions in HM3D scenes</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A derived, symbolic room-level graph representing the local layout around the agent: nodes are clustered panoramic observations (with attributes: ID, Angle, Type, Objects), edges denote estimated adjacency/room connections inferred by LLM commonsense reasoning; domain: indoor household navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Described qualitatively as a sparse room-level adjacency graph produced by clustering panoramic image captions and estimating inter-room connections; edges connect same-type or adjacent-angle room clusters enabling the planner to traverse between rooms rather than roam within one room.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Per-episode local graph size not numerically reported; constructed from six panoramic images which may cluster into fewer unique room nodes; dataset-level scenes: 80 training / 20 validation (HM3D).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM planner (e.g., GPT-4 called with step-by-step prompt; VLM used to produce captions via LLama-Adapter)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>High-level planner built on an LLM prompted with a three-stage 'Summarize, Cluster, Plan' step-by-step template: (1) summarize VLM captions into structured JSON nodes (ID, Angle, Type, Objects), (2) cluster images into room-level nodes and infer adjacency/agent localization, (3) generate plans selecting room-level subgoals periodically or when subgoal reached; output subgoal is converted to a pixel (via object segmentation models GroundingDINO + SAM) and passed to the PixNav low-level policy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Planner effectiveness evaluated indirectly via overall Success, SPL, and trajectory visualizations; separate LLM skill evaluations use human comparisons for Room Localization and Room Clustering accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Quantitative planner skill eval: Room Localization accuracy (LLM) 68.4% (human baseline), Room Clustering 47.1% (compared to human answers) as reported in Table IV; these metrics reflect planner/LLM spatial-awareness but not direct traversal efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Planner contribution is reflected in combined system success numbers (see PixNav entry): helps achieve consistent exploration and the long-horizon object navigation results (e.g., overall open-set success 37.9%); exact ablation isolating planner effect not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>High-level planning-based over a symbolic room-graph combined with low-level pixel-goal controller; thus a hierarchical planning + reactive policy.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper reports qualitative relationship: building a room-level graph and using it for planning produces more consistent and semantically meaningful exploration (e.g., exiting rooms and traversing corridors to likely rooms for the object). No quantitative mapping is provided between standard graph-topology metrics (diameter, clustering coefficient, dead-ends) and planner performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>The LLM planner can perform self-localization and cluster panoramic observations to form a structured memory (room graph), which improves consistent exploration for long-horizon object search; however, LLM room-clustering accuracy is moderate (~47.1%) and room-localization is ~68.4%, indicating imperfect but useful structural inference. The results imply that policies that incorporate explicit structural memory (room graphs) and commonsense priors (via LLM) perform better on longer-horizon tasks than purely reactive policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ESC: Exploration with soft commonsense constraints for zeroshot object navigation <em>(Rating: 2)</em></li>
                <li>L3MVN: Leveraging large language models for visual target navigation <em>(Rating: 2)</em></li>
                <li>StructNav: (paper referenced as expanding scene memory with point clouds, scene graphs, and semantic occupancy maps) <em>(Rating: 2)</em></li>
                <li>NavGPT: Explicit reasoning in vision-and-language navigation with large language models <em>(Rating: 2)</em></li>
                <li>ZSON: Zero-shot object-goal navigation using multimodal goal embeddings <em>(Rating: 1)</em></li>
                <li>Habitat-Web: Learning embodied object-search strategies from human demonstrations at scale <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1182",
    "paper_id": "paper-262053943",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "PixNav",
            "name_full": "Pixel-guided Navigation skill (PixNav)",
            "brief_description": "An RGB-only, pixel-goal navigation policy trained by supervised imitation on large-scale simulated trajectories that takes a goal pixel (converted to a small mask) and a history of RGB images and outputs discrete actions; architecture uses ResNet18 encoders, a transformer decoder, and heads for action, pixel tracking, and temporal-distance prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "HM3D / Habitat (photorealistic indoor scenes)",
            "environment_description": "Photorealistic large-scale indoor household environments (HM3D) used in the Habitat simulator; domain: indoor household navigation / object search.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Not explicitly quantified; environments are articulated indoor scenes composed of rooms and corridors; connectivity implicitly represented as room adjacency in planner graphs (sparse room-level graphs produced by clustering panoramic observations).",
            "environment_size": "Dataset: 80 training scenes, 20 validation scenes used for experiments; per-scene node/edge counts for local graphs not reported.",
            "agent_name": "PixNav (transformer-based pixel-goal policy)",
            "agent_description": "Low-level RGB-only policy: concatenates a small mask for the target pixel with the initial image (4-channel ResNet18) to produce a goal token; separate ResNet18 encodes each observation. A goal-fusion layer concatenates goal features to each observation token; a 4-layer transformer decoder integrates tokens and produces output tokens fed to MLP heads that predict discrete actions (Stop, MoveAhead, TurnLeft, TurnRight, LookUp, LookDown), the tracked pixel in each frame, and remaining temporal distance. Entire network ~54.5M parameters; trained by supervised imitation using planner-generated trajectories.",
            "exploration_efficiency_metric": "Success Rate (SR), SPL (Success weighted by Path Length), Distance-to-Goal (DTG / DTS), 'distance to goal' at episode end",
            "exploration_efficiency_value": "Local short-range (1-3m): SR 0.88, SPL 0.88, DTS 0.54 (units: meters) for default camera (camera height 0.88 m, HFOV 79°). Long-range (3-5m): SR 0.42, SPL 0.41, DTS 1.61 (meters). Overall open-set object-nav reported: Success 37.9%, SPL 20.5% (Table I).",
            "success_rate": "See exploration_efficiency_value (88% for 1-3m local PixNav; 42% for 3-5m; 37.9% reported on open-set object-nav benchmark in Table I).",
            "optimal_policy_type": "Hierarchical: low-level reactive/planned pixel-goal controller (transformer-based, memory via history images) combined with a high-level LLM planner for room/waypoint selection; thus planning-based hierarchical policy with short-horizon reactive control.",
            "topology_performance_relationship": "No direct quantitative analysis linking classical graph-topology metrics (diameter, clustering coefficient, dead-end counts, doors) to performance; qualitative findings: transformer-based PixNav is robust on short (1–3 m) tasks (high SR and SPL) and degrades on longer-horizon tasks (3–5 m); ablations show that components that explicitly preserve goal information (tracking prediction and goal-fusion) significantly affect long-horizon performance, indicating that increased effective path length or more complex topological structure (longer geodesic distances) requires explicit goal-tracking and stronger goal-conditioning in policy.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "Findings in the paper indicate: (1) Transformer-based history encoding provides robustness for short-range navigation; (2) Explicit pixel-tracking and goal-fusion are critical for long-horizon (higher effective graph-distance) navigation — ablations removing tracking or fusion reduce 3–5 m success rates notably; (3) A hierarchical design where an LLM builds a room-level graph (via summarization, clustering, and edge inference) and proposes waypoints enables more consistent, semantic exploration than unguided roaming, implying that high-level graph-structured memory/planning benefits long-horizon object search.",
            "uuid": "e1182.0",
            "source_info": {
                "paper_title": "Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLM Planner / Local Layout Graph",
            "name_full": "Large-Language-Model-based Planner and LLM-generated local room-graph",
            "brief_description": "An LLM-based high-level planner that consumes structured textual summaries (from a vision-language model) of six panoramic images, performs summarization, self-localization, clustering into room-level nodes, and constructs a local adjacency graph which it uses to pick next room/waypoint (converted to a pixel via object segmentation) balancing exploration and exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Local room-level graph derived from panoramic captions in HM3D scenes",
            "environment_description": "A derived, symbolic room-level graph representing the local layout around the agent: nodes are clustered panoramic observations (with attributes: ID, Angle, Type, Objects), edges denote estimated adjacency/room connections inferred by LLM commonsense reasoning; domain: indoor household navigation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Described qualitatively as a sparse room-level adjacency graph produced by clustering panoramic image captions and estimating inter-room connections; edges connect same-type or adjacent-angle room clusters enabling the planner to traverse between rooms rather than roam within one room.",
            "environment_size": "Per-episode local graph size not numerically reported; constructed from six panoramic images which may cluster into fewer unique room nodes; dataset-level scenes: 80 training / 20 validation (HM3D).",
            "agent_name": "LLM planner (e.g., GPT-4 called with step-by-step prompt; VLM used to produce captions via LLama-Adapter)",
            "agent_description": "High-level planner built on an LLM prompted with a three-stage 'Summarize, Cluster, Plan' step-by-step template: (1) summarize VLM captions into structured JSON nodes (ID, Angle, Type, Objects), (2) cluster images into room-level nodes and infer adjacency/agent localization, (3) generate plans selecting room-level subgoals periodically or when subgoal reached; output subgoal is converted to a pixel (via object segmentation models GroundingDINO + SAM) and passed to the PixNav low-level policy.",
            "exploration_efficiency_metric": "Planner effectiveness evaluated indirectly via overall Success, SPL, and trajectory visualizations; separate LLM skill evaluations use human comparisons for Room Localization and Room Clustering accuracy.",
            "exploration_efficiency_value": "Quantitative planner skill eval: Room Localization accuracy (LLM) 68.4% (human baseline), Room Clustering 47.1% (compared to human answers) as reported in Table IV; these metrics reflect planner/LLM spatial-awareness but not direct traversal efficiency numbers.",
            "success_rate": "Planner contribution is reflected in combined system success numbers (see PixNav entry): helps achieve consistent exploration and the long-horizon object navigation results (e.g., overall open-set success 37.9%); exact ablation isolating planner effect not numerically reported.",
            "optimal_policy_type": "High-level planning-based over a symbolic room-graph combined with low-level pixel-goal controller; thus a hierarchical planning + reactive policy.",
            "topology_performance_relationship": "Paper reports qualitative relationship: building a room-level graph and using it for planning produces more consistent and semantically meaningful exploration (e.g., exiting rooms and traversing corridors to likely rooms for the object). No quantitative mapping is provided between standard graph-topology metrics (diameter, clustering coefficient, dead-ends) and planner performance.",
            "comparison_across_topologies": false,
            "topology_comparison_results": null,
            "policy_structure_findings": "The LLM planner can perform self-localization and cluster panoramic observations to form a structured memory (room graph), which improves consistent exploration for long-horizon object search; however, LLM room-clustering accuracy is moderate (~47.1%) and room-localization is ~68.4%, indicating imperfect but useful structural inference. The results imply that policies that incorporate explicit structural memory (room graphs) and commonsense priors (via LLM) perform better on longer-horizon tasks than purely reactive policies.",
            "uuid": "e1182.1",
            "source_info": {
                "paper_title": "Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ESC: Exploration with soft commonsense constraints for zeroshot object navigation",
            "rating": 2,
            "sanitized_title": "esc_exploration_with_soft_commonsense_constraints_for_zeroshot_object_navigation"
        },
        {
            "paper_title": "L3MVN: Leveraging large language models for visual target navigation",
            "rating": 2,
            "sanitized_title": "l3mvn_leveraging_large_language_models_for_visual_target_navigation"
        },
        {
            "paper_title": "StructNav: (paper referenced as expanding scene memory with point clouds, scene graphs, and semantic occupancy maps)",
            "rating": 2,
            "sanitized_title": "structnav_paper_referenced_as_expanding_scene_memory_with_point_clouds_scene_graphs_and_semantic_occupancy_maps"
        },
        {
            "paper_title": "NavGPT: Explicit reasoning in vision-and-language navigation with large language models",
            "rating": 2,
            "sanitized_title": "navgpt_explicit_reasoning_in_visionandlanguage_navigation_with_large_language_models"
        },
        {
            "paper_title": "ZSON: Zero-shot object-goal navigation using multimodal goal embeddings",
            "rating": 1,
            "sanitized_title": "zson_zeroshot_objectgoal_navigation_using_multimodal_goal_embeddings"
        },
        {
            "paper_title": "Habitat-Web: Learning embodied object-search strategies from human demonstrations at scale",
            "rating": 1,
            "sanitized_title": "habitatweb_learning_embodied_objectsearch_strategies_from_human_demonstrations_at_scale"
        }
    ],
    "cost": 0.010553,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill
21 Sep 2023</p>
<p>Wenzhe Cai 
Siyuan Huang 
Guangran Cheng 
Yuxing Long 
Peng Gao 
Changyin Sun 
Hao Dong 
Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill
21 Sep 202306FBD5D2F6B114575084DD452521201FarXiv:2309.10309v2[cs.RO]
Zero-shot object navigation is a challenging task for home-assistance robots.This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models.But for the locomotion part, most works still depend on map-based planning approaches.The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks.In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task.It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects.Besides, our PixNav is a pure RGB-based policy that can reduce the cost of homeassistance robots.Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task.To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint.Evaluations across both photorealistic indoor simulators and realworld environments validate the effectiveness of our proposed navigation strategy.Code and video demos are available at https://github.com/wzcai99/Pixel-Navigator.</p>
<p>I. INTRODUCTION</p>
<p>Zero-shot object navigation is considered fundamental for home-assistance robots.Before a robot can interact with objects and complete many complex tasks, the robot should be able to navigate and find the target objects with a high success rate.Besides, with all kinds of different human demands, the robot navigation policy should be able to generalize to a variety of objects.This task requires three essential abilities: visual grounding for novel objects recognition, commonsense inference for semantic exploration and path-planning for robot locomotion.For the first two necessities, recent prosperity in foundation models provide powerful visual and language perception capabilities [1], [2], [3], [4], [5], [6].But for the locomotion part, most of the zero-shot object navigation works still tethered to the map-based planning approaches [7], [8], [9], [10], [11].There are two main limitations of such navigation systems.Firstly, as most foundation models are trained with internet RGB images, it is difficult to directly integrate such prior knowledge into a map-based locomotion method.Secondly, map-based approaches require accurate depth sensing and localization, which often comes at a higher cost and can be easily disturbed compared with RGB Wenzhe Cai, Guangran Cheng and Changyin Sun are with the School of Automation, Southeast University.Yuxing Long and Hao Dong are with Hyperlane Lab, CFCS, School of CS, Peking University and National Key Laboratory for Multimedia Information Processing.Siyuan Huang and Peng Gao are with the Shanghai AI Laboratory.For the home-assistance robot, humans may ask for searching and interacting with uncommon objects like 'a rocking chair' and 'a infant bed'.It is challenging for the robot to develop a multi-modal navigation policy that can accommodate various types of objects using both text and image inputs.However, since foundation models are capable of zero-shot image understanding, any objects can be indicated by a single pixel when utilizing such foundation models.Therefore, we train a navigation policy with pixels as goal specifications, enabling the robot to navigate towards arbitrary objects.Since each object typically comprises hundreds of pixels, this approach offers a wide range of possible navigation trajectories, which greatly enlarges the scale of the navigation dataset.</p>
<p>Navigate to <rocking chair> and <infant bed>
Robot
cameras.This raises a pertinent question: Can we utilize the superpower of foundation models and develop a pure RGB-based intelligent navigation system?In this work, we posit that pixels serve as an ideal bridge, seamlessly connecting foundational models to the navigation task.Since most foundation models are trained with RGB images, aligning such foundation models with an RGBbased navigation skill is much easier than a map-based skill.Meanwhile, the task of navigating towards arbitrary objects can be converted into navigating towards a designated pixel.Therefore, we propose a Pixel-guided Navigation skill (PixNav) to complete the last-mile navigation process.Different from the previous object-goal [12], [13] skill, which uses a word or one-hot vector as the goal specification, PixNav avoids learning the multi-modal alignment for the specific navigation task but still versatile for navigating to any kind of objects.Moreover, to ensure the generalization performance of a learning-based robotics policy, the dataset scale is a crucial factor [14], [15].For our PixNav, it is much easier to collect large-scale demonstrations than vanilla object navigation.For example, in Fig 1 , we can only get one trajectory for the task "navigating towards infant bed" with an explicit focus on "infant bed" in the object navigation setting.However, we can gather multiple navigation trajectories for PixNav by assigning different pixels belonging to the "infant bed" as the goal.Given that each image comprises thousands of pixels, the potential for generating diverse trajectories is vast.Compared with object navigation, gathering PixNav trajectories doesn't need explicit semantic annotations, which further reduces the cost of data collection.Generating the optimal actions towards the pixel is also simple: Each pixel is an underlying 3-D coordinates and we can use off-the-shelf planning methods to achieve this point-goal navigation for dataset collection.</p>
<p>The remaining question of PixNav is how to navigate towards the out-of-sight target.For indoor navigation scenarios, the house's layout and object placements typically adhere to human preferences, and such commonsense priors have already been embedded into large-language models (LLMs).Therefore, we designed a hierarchical policy that employs LLM as the planner to guide the exploration sequences across rooms.To make the planning consistent and rational, we designed a step-by-step prompt template to evoke the reasoning ability of LLM.With our designed prompt, the LLM demonstrates its potential in self-localization and structural memory organization.Our hierarchical policy achieves competitive performance compared with the mapping-based methods in zero-shot object navigation without depth modality as input.In summary, our contributions are listed below:</p>
<p>• Pixel Navigation -We propose a universal RGB-based navigation policy to replace the function of the pathplanning method in map-based navigation.• Chain of Thought of LLM for Navigation -We design a step-by-step prompt template and discover the potential of LLM in self-localization and structural memory organization.• Foundation Model Powered Navigation -Our approach bridges the gap between the embodied navigation tasks and the foundation models.Our hierarchical policy achieves competitive performance compared with the mapping-based methods.</p>
<p>II. RELATED WORK</p>
<p>A. Open-Vocabulary Object Navigation</p>
<p>Searching for an object is a common task in indoor environments.While there is a rich body of work dedicated to the object navigation task [13], [16], [17], [18], [12], [19], most are limited in a close-set of objects.In response, many researchers study the more realistic open-vocabulary object navigation task.ZSON [20] transfers an image-goal navigation policy for object-goal navigation tasks with the text-image alignment from the CLIP [21] encoder.And SSNet [22] explicitly introduces an encoding network to extract the feature for the target object tags.Without a large-scale object navigation dataset, the end-to-end policy network suffers in learning the multi-modal alignment for such embodied task and achieved limit performance.Therefore, many mapbased approaches [7], [23], [24], [9] are proposed where a high-level policy selects a promising coordinate for object searching and the low-level policy complete the point-goal navigation with path-planning.Yet, the map-based approaches are highly dependent on the accuracy of depth-sensor and self-localization.On the contrary, we propose a fundamental pixel navigation policy that can replace the traditional pathplanning method and make the approach become a pure RGB-based solution.</p>
<p>B. Foundation Models on Vision and Language</p>
<p>Recent advancements have seen significant strides in foundational models across both visual and language domains.For example, GroundingDINO [1] and Owl-ViT [2] can accomplish zero-shot detection, RAM [25] can tag almost all the appeared objects in the image.SAM [3] pioneers in delivering fine-granular segmentation for a diverse set of objects.As for language, LLM shows surprising performance on almost all the tasks, including text understanding, generation [26], [27] and even planning for robotics field [28], [29], [30], [31].However, there are many tasks that require both vision and language inputs.To this end, many works introduce the largelanguage model as the backbone and develop vision-language models for general-purpose multi-modal tasks [32], [6], [33].Within the robotics arena, PALM-E [34] is a specialized foundation model that grounds the images and language into several embodied tasks.In our work, we utilize the LLama-Adapter [33] to convert images into text and employ GPT-4 to facilitate reasonable textual planning.Furthermore, leverage the robust zero-shot recognition capabilities of foundational models to enhance performance.</p>
<p>C. Large-Language Model for Visual Navigation</p>
<p>For many visual navigation tasks, the agent is required to achieve efficient exploration by linking the goals and the observed semantic clues.Leveraging LLM's powerful commonsense reasoning ability [27], [34], [35], many works start to explore the ways to introduce the LLM's planning capability into visual navigation tasks.For instance, L3MVN [9] transcribes surrounding information of map boundaries into text, enabling the LLM to evaluate the scores of the frontiers for better exploration.ESC [8] further feeds both room-level and object-level information to LLM to select an exploration target.Similarly, LGX [10] also captions the image observation into text but uses LLM to make a search plan for an arbitrarily described object.StructNav [11] expands the scene memory with point clouds, scene graphs, and semantic occupancy maps, making more reasonable planning with LLM for object navigation.NavGPT [36] discovers a way to directly use the planning and reasoning This can be finished by controlling the robot by turning around or installing multiple RGB cameras.And a vision-language model translates this visual data into a textual description.Utilizing a systematically crafted step-by-step prompt template, the LLM then strategizes the most optimal next step for the target location.Then, the target location will be indicated as a pixel and conveyed to the navigation policy.The PixNav policy will continuously receive observation images and perform actions until arriving at the goal area.capability of LLMs on the instruction-following navigation agent with prompt engineering.We follow the idea to introduce the LLM to promote the efficiency of an object navigation agent, but differ from those studies, we design a step-by-step prompt template and discover the potential of LLM in self-localization and structural memory organization, which can be further extended in other navigation tasks.However, our unique contribution lies in a crafted step-bystep prompt template, which could guide LLMs to pinpoint the agent's location and adeptly organize memory.And we believe these findings have potential implications for a broader range of navigation tasks.</p>
<p>III. METHOD</p>
<p>A. Path-Planning with Pixel Navigation</p>
<p>In pursuit of a universally applicable navigation strategy, we introduce the pixel-guided navigation skill (PixNav) tailored for zero-shot object navigation tasks.The PixNav uses a pixel in the initial frame as the goal and executes primitive actions navigating towards the corresponding area indicated by the pixel.Our approach leverages an action space composed of six primitive actions: {Stop, MoveAhead, TurnLeft, TurnRight, LookUp, LookDown}.Notably, while our model currently operates in this discrete action space, PixNav has the potential to be extended to continuous settings.To help the model deal with the partial observability and tracking the pixel in the first frame, we use a transformer-based network with all history images I 0∼T = {I 0 , I 1 ...I T } and the goal pixel p g = (x i , y i ) as inputs.All the parameters are optimized from scratch and the architecture is summarized at the bottom part in Fig 2 .Training Data.The large-scale dataset is essential for the robot learning [14].Regarding the demonstration data collection, our PixNav skill offers marked advantages over the object-goal navigation approaches.The latter demands a unique object instance for each navigation trajectory, complicating the process through the necessity for object instance annotation prior to generating each trajectory.Such object instances are generally sparse within a single image which limits the potential collected training trajectories.Conversely, PixNav directly utilizes the vast potential of the pixel space, yielding much diverse navigation trajectories.The integration of the depth sensor further eases this approach, by converting each pixel to a 3D coordinate.Then, offthe-shelf path planners can be utilized, which is especially straightforward in simulation environments.In our work, we use the Habitat-Lab [37], [38] with HM3D [39] dataset to generate the training trajectories for PixNav.At the start of each episode, we randomly sample a pixel from the RGB observation and use the underlying path planner to interpolate the actions.In total, we generate a dataset with more than 100,000 trajectories to train the policy and we constrain the trajectory length within 64-time steps.The size of RGB observation in the trajectories is 160x120.</p>
<p>Architecture Details.To encode all the history images and the target pixel, we first convert the pixel goal p g = (x i , y i ) into a mask image M g where the values within the bounding box (x i − δ , y i − δ , x i + δ , y i + δ ) are set to 1 and the rest are set to 0. Here, δ is a parameter used to control the masked area and in our work, we set it to 2. We then concatenate the mask image with the initial image in a channel-wise manner and employ a 4-channel ResNet18 to process the combined input v g = (I 0 , M g ), yielding a 768-dimensional goal token Φ θ (v g ).For the RGB observation images, we use another 3-channel ResNet18 to obtain a sequence of 512-dimensional observation tokens [ϕ θ (I 0 ), ϕ θ (I 1 )...ϕ θ (I T )].Both ResNet18 models are initialized randomly.To ensure the information of the goal pixel can be utilized by the transformer, we introduce a goal-fusion layer to generate a 256-dimensional feature from Φ θ (v g ) and concatenate it with each observation tokens.Subsequently, a 4-layer transformer decoder is employed to integrate the global information across all tokens.Denote the output tokens as [s 0 , s 1 ...s T ].These output tokens are fed into a policy layer to predict the optimal action π θ (s i ).As the robot performs the navigation task, the changes in viewpoint will cause the movement of the pixel goal in the subsequent frames.To help the model understand such transformation, we introduce a tracking layer to predict the corresponding pixel in every RGB observation f θ (s i ).Moreover, to aid the model in understanding the 3-D structure, we introduce a temporal distance layer to predict the remaining time steps d θ (s i ) to each the pixel goal.All the policy layer, tracking layer, and temporal distance layer are 1-layer MLP built upon output tokens from transformer decoder [s 0 , s 1 ...s T ].The parameters of the entire network are 54.5M.Training Objective.To train the entire network, we incorporate three objectives.The first is the imitation learning objective for π θ (s i ), the second is MSE loss for the corresponding pixel prediction f θ (s i ), and the third is MSE loss for temporal distance prediction d θ (s i ).Because the temporal distance can vary from 1 to 64, we scale the loss function for temporal distance by dividing it by 10.In summary, the objectives can be formulated as follows:
L il = E τ <a href="1"> ât log π θ (s t )</a>L distance = E τ <a href="2">( dt − d θ (s i )) 2 </a>L track = E τ <a href="3">( pt − f θ (s i )) 2 </a>
The ât , dt , pt are the ground truth for best actions, temporal distance, and the corresponding pixel.Therefore, all the training processes are supervised learning.The entire loss is written as follows:
L = L il + L distance + L track(4)</p>
<p>B. Perception with Foundation Model</p>
<p>Despite the commonsense inference and reasoning capabilities contained in LLMs, navigation tasks generally operate with image-based observations.To bridge this modality gap, we employ the vision-language model, LLama-Adapter [33], as a conduit for image summarization.Specifically, we prompt the VLM with two structured queries: "Describe the room type in the image" and "Describe the objects with details in the image".Then, each image can be converted into a structured textual representation, as demonstrated in Fig III -B.To make a comprehensive planning, a panoramic understanding is essential.To this end, the robot undergoes a series of rotations and captures six distinct images which are converted into six paragraphs as the input for LLM later.In order to allocate one pixel as the goal, we introduce two additional foundation models, namely GroundingDINO [1] and SAM [3] to capture the find-grained segmentation masks.Once an object target is proposed by LLM, the center point of the object mask will be assigned as the pixel goal for the low-level navigation policy.</p>
<p>C. Planning with Large-Language Model</p>
<p>For the long-horizon object navigation task, we design an LLM-based planner to decide which direction is most promising for searching the target object.For example, a bed is usually placed in a bedroom, and the agent needs to traverse a corridor to reach the bedroom.To achieve this, we design step-to-step prompts to help the LLM understand an indoor navigation scenario.The entire planning procedure is divided into three stages: Summarize, Clustering, and Planning, as shown in Fig 4 .Firstly, as prior works show structured text is beneficial for LLM's planning and reasoning [30], [29], we ask the LLM to summarize the VLM captions into JSON format which contains {'ID', 'Angle', 'Type', 'Objects'}.Each JSON data encapsulates room-level information.However, the six surrounding images do not generally represent six unique room instances and several images can be grouped under a single room.To cluster the data, we use LLM to estimate the robot's current room and its position, and then build the inter-room connections.The same type of rooms with an edge connection will be merged into one.We also cluster the same type of rooms with adjacent angles.Then, a graph of the local layout in the house is generated by the LLM as shown at the top of Fig 4. With this graph, the LLM can perform consistent exploration behaviors among rooms instead of roaming in one room.We design a planning prompt for the LLM to balance between exploration and exploitation for searching the target object.Our planner proposes the subgoal after periodic time steps or the low-level policy completes navigating towards the assigned goal.</p>
<p>IV. EXPERIMENTAL RESULTS</p>
<p>A. Benchmarks and Metrics</p>
<p>We employed the large-scale indoor dataset HM3D [39] to evaluate our proposed method.The dataset contains 80 scenes for training and 20 scenes for validation.For the collection of PixNav demonstration trajectories, we utilized the 80 training scenes with the camera height 0.88m and horizontal field of view (HFOV) 79°.The evaluation for object navigation follows the settings in objectnav-challenge-2022 [40].For the object navigation task, we report the performance with respect to Success and SPL, which is a metric evaluating the navigation path efficiency.To evaluate the performance of PixNav, we randomly initialized the agent in 20 validation scenes and sampled a pixel goal in the first RGB frame.The PixNav evaluation is divided into two levels, one is 1-3m, and Fig. 4. The step-by-step prompt template for LLM to make a reasonable plan for navigation.After translating the panoramic images into text, we first use the LLM to summarize the captions into highly structured data and ask LLM to estimate the robot's location according to the commonsense of indoor environments.With the robot location, we can cluster the data into room-level instances and thus the layout can be described as a graph.Based on this graph, we then ask the LLM to provide a plan considering both exploitation and exploration.</p>
<p>the other is 3-5m, which represents the underlying geodesic distance from the assigned pixel goals to the initial robot location.We further report the distance to goal (DTG), the geodesic distance from the location at the end of the trajectory to the pixel goal.For each level, we sample 500 episodes for evaluation.</p>
<p>B. Baselines</p>
<p>We consider both non-zero-shot object navigation methods and zero-shot object navigation methods to verify our idea.</p>
<p>• Habitat-Web [41].A closed-set object navigation baseline with human demonstrations for imitation learning.</p>
<p>• OVRL [42].A closed-set object navigation baseline with the large-scale indoor dataset for representation learning.</p>
<p>• ESC [8] A map-based zero-shot object navigation baseline with LLM for frontier planning.</p>
<p>• ZSON [20] An RGB-based zero-shot object navigation baseline with CLIP [21] as the image and object goal encoders.</p>
<p>C. Performance on Zero-Shot ObjectNav</p>
<p>We compare our method with baseline results in Tab I and example visualization of trajectories are shown in Fig IV -C.Our approach achieves competitive performance even without depth and localization as inputs.Our PixNav can consistently track the initial pixel target and adaptively look down to avoid collision.The long-horizon trajectories show heuristic exploration behavior.For example, in the third figure, the agent can first go out of one room and enter another, then walk back to the corridor to search for a bathroom.It proves the effectiveness of our LLM-based planner.How to properly select the pixel as the goal for the PixNav is essential.We found that assigning the center pixel of the selected room 'floor' is a good strategy.</p>
<p>D. Ablation Study on PixNav</p>
<p>For real-world applications, robots may have different embodiment and camera settings.To become a general RGBbased path-planning module, the PixNav policy should be able to transfer to various camera configurations.Therefore, we adjusted the camera settings in the simulator with different heights and HFOV and re-evaluated the PixNav performance shown in Table II.We found that even if we introduce a severe change in camera height (50% relative change) and HFOV (25% relative change), our policy can still achieve a high success rate in 1m-3m navigation tasks and degenerate little in most of the cases.This indicates that our PixNav can achieve zero-shot generalization to new camera settings and learn a universal representation for navigation.But what makes our policy robust?We conduct ablation studies on three components of our method, which are tracking prediction, temporal distance prediction, and goal fusion network.The performance is shown in Tab III.Most of the components have a minor influence on the 1-3m scenarios, which shows the superiority of the transformer-based policy network architecture.However, tracking prediction and goal fusion significantly influence the 3-5m scenarios, emphasizing the need to explicitly remind the network not to overlook the inconspicuous pixel goal in order to achieve long-term navigation with PixNav.</p>
<p>E. Analysis of LLM Planner</p>
<p>To achieve long-horizon navigation tasks, the LLM planner is also an essential part.Traditional map-based approaches prove that mappping and localization are valuable skills for navigation tasks.If the LLM also possesses these skills, the potential of our LLM-based RGB policy is promising   for navigation field.To probe on the abilities of LLM, we implement a quantitive analytic to verify whether the LLM owns such spatial-awareness of the indoor environments.The experiments contain two skills evaluations: Room Localization and Room Clustering.We ask 3 humans to see the panoramic images of the robot's location and answer two questions: "Which room is the robot in?" and "Describe the number and type of the rooms in the surroundings."We compare the answers from LLM and humans for 100 episodes.The accuracy of the two skills is shown in Tab IV.</p>
<p>F. Sim2Real Transfer</p>
<p>The real-world experiments are conducted with an iRobot-Create3 Mobile base equipped with an OAK-D camera.However, we only use the RGB observations from the camera for the experiments.The action configuration of the robot follows the discrete action space setting, which contains basic actions {Stop, MoveAhead 0.25m, TurnLeft 30°, TurnRight 30°}.We conduct experiments to verify the function of both the PixNav policy as well as the LLM-planner.Please refer to our website for the details of real-world experiments.</p>
<p>V. CONCLUSION</p>
<p>In this work, we take a significant stride towards developing a pure RGB-based solution for visual navigation tasks through the utilization of a pixel-guided navigation skill.Our PixNav policy demonstrates versatility in navigating various objects, and it can effortlessly be integrated with advanced foundation models to tackle long-horizon navigation tasks.Moreover, PixNav exhibits generalization capabilities across different camera settings, showcasing its potential to serve as a versatile RGB-based path-planning policy.However, the current performance of PixNav in long-term path-planning is not satisfactory.Therefore, in future research, we will investigate the potential benefits of large-scale and diverse navigation datasets.Additionally, the design of the navigationgrounded vision-language model as a planner would also bring improvements in the object-navigation domain.</p>
<p>Fig.1.For the home-assistance robot, humans may ask for searching and interacting with uncommon objects like 'a rocking chair' and 'a infant bed'.It is challenging for the robot to develop a multi-modal navigation policy that can accommodate various types of objects using both text and image inputs.However, since foundation models are capable of zero-shot image understanding, any objects can be indicated by a single pixel when utilizing such foundation models.Therefore, we train a navigation policy with pixels as goal specifications, enabling the robot to navigate towards arbitrary objects.Since each object typically comprises hundreds of pixels, this approach offers a wide range of possible navigation trajectories, which greatly enlarges the scale of the navigation dataset.</p>
<p>Fig. 2 .
2
Fig.2.The pipeline of our RGB-centric strategy for zero-shot object navigation.In each cycle, the agent first gets the panoramic images of the surroundings.This can be finished by controlling the robot by turning around or installing multiple RGB cameras.And a vision-language model translates this visual data into a textual description.Utilizing a systematically crafted step-by-step prompt template, the LLM then strategizes the most optimal next step for the target location.Then, the target location will be indicated as a pixel and conveyed to the navigation policy.The PixNav policy will continuously receive observation images and perform actions until arriving at the goal area.</p>
<p>The room in the image is a bedroom.Object Description: The image features a bedroom with a large bed, a dresser, and a closet.The bed is positioned in the middle of the room, and the dresser is located on the left side.The closet is situated on the right side of the room.The bed-room is empty, with no people or furniture visible.</p>
<p>Fig. 3 .
3
Fig. 3.An example of the VLM translation.The VLM is prompted with room-level and object-level queries.The robot performs a series of rotations, capturing six distinct images to ensure a comprehensive panoramic view.</p>
<p>are a LocoBot working in the indoor environment, your task is to understand the layout of your surrounding … To finish the task, please follow my stepby-step instructions: (1) Summarize the nodes (2) Localization yourself (3) Clustering (4) … You are a LocoBot working in the indoor environment, your task is to explore and navigating towards an object target … To finish the task, please follow my stepby-step instructions: (1) Find if the <Target> exists (2) If not (1), select the useful room (3) If not (1) (2), seek to explore (4) … Planning Prompt {ID: diningroom_30, Angle: [30], Type: Dining Room, Objects: [table, chairs, mirror, blanket…]} {ID: livingroom_90, Angle: [90,150,210], Type: Living Room, Objects: [couch, chair, coffee table, painting…]} {ID: kitchen_270, Angle: [270,330], Type: Kitchen, Objects: [sink, refrigerator, tablecloth…]} Panoramic View Self-Localization For the task of navigating to the Sofa, Select the LivingRoom Node {ID: livingroom_90, Angle: [90,150,210], Type: Living Room, Objects: [couch, chair, coffee table, painting…]} Node Clustering</p>
<p>Fig. 5 .
5
Fig. 5. Trajectory visualization of both PixNav policy and long-horizon object navigation.The top part represents the first-person view of PixNav trajectory.Even though the robot's view changes constantly, our model can still predict the original goal and walk towards it.The bottom part represents the top-down view of the entire trajectory.Our LLM-based planner helps the robot achieve consistent and semantic exploration towards the target.</p>
<p>TABLE I PERFORMANCE
I
ON OBJECT NAVIGATION TASKS WITH HM3D DATASET.
SettingMaplessMethodSuccess SPLClose-Set ObjectNavHabitat-Web OVRL41.5 62.016.0 26.8ZSON25.512.6Open-Set ObjectNavESC39.222.3Ours37.920.5</p>
<p>TABLE II PERFORMANCE
II
ON DIFFERENT VARIATIONS OF CAMERA SETTINGS
Camera1m-3m3m-5mSR ↑SPL ↑DTS ↓ SR ↑ SPL ↑ DTS ↓(0.88, 79°)0.880.880.540.420.411.61(0.48, 79°)0.790.760.610.260.251.95(1.28, 79°)0.780.750.860.330.321.83(0.88, 60°)0.710.700.820.140.132.32(0.88, 100°)0.800.780.690.330.321.88</p>
<p>TABLE III THE
III
ABLATION STUDY OF THE PIXNAV IN HM3D DATASET
Ablation1m-3m3m-5mSR ↑ SPL ↑ DTS ↓ SR ↑SPL ↑DTS ↓Ours0.880.880.540.420.411.61w/o tracking0.820.810.630.330.311.75w/o distance0.860.850.600.380.351.69w/o fusion0.840.840.630.340.331.91</p>
<p>TABLE IV THE
IV
ANALYSIS OF LLM ABILITIES IN LOCALIZATION AND MAPPING
LLM AbilityHuman Eval (%)Room Localization68.4Room Clustering47.1</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, C Li, J Yang, H Su, J.-J Zhu, L Zhang, abs/2303.05499ArXiv. 2574273072023</p>
<p>Simple open-vocabulary object detection with vision transformers. M Minderer, A A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran, A Arnab, M Dehghani, Z Shen, X Wang, X Zhai, T Kipf, N Houlsby, abs/2205.06230ArXiv. 2487218182022</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, arXiv:2304.02643Segment anything. 2023arXiv preprint</p>
<p>Language-driven semantic segmentation. B Li, K Q Weinberger, S J Belongie, V Koltun, R Ranftl, abs/2201.03546ArXiv. 2458369752022</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, arXiv:2303.161992023arXiv preprint</p>
<p>Instructblip: Towards general-purpose visionlanguage models with instruction tuning. W Dai, J Li, D Li, A M H Tiong, J Zhao, W Wang, B Li, P Fung, S C H Hoi, abs/2305.06500ArXiv. 2586152662023</p>
<p>Clip on wheels: Zero-shot object navigation as object localization and exploration. S Y Gadre, M Wortsman, G Ilharco, L Schmidt, S Song, abs/2203.10421ArXiv. 2022</p>
<p>Esc: Exploration with soft commonsense constraints for zeroshot object navigation. K Zhou, K Zheng, C Pryor, Y Shen, H Jin, L Getoor, X E Wang, arXiv:2301.131662023arXiv preprint</p>
<p>L3mvn: Leveraging large language models for visual target navigation. B Yu, H Kasaei, M Cao, abs/2304.05501ArXiv. 2023</p>
<p>Can an embodied agent find your" cat-shaped mug"? llm-based zero-shot object navigation. V S Dorbala, J F MullenJr, D Manocha, arXiv:2303.034802023arXiv preprint</p>
<p>How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers. J Chen, G Li, S Kumar, B Ghanem, F Yu, 2023</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2022</p>
<p>Object goal navigation using goal-oriented semantic exploration. D S Chaplot, D Gandhi, A Gupta, R Salakhutdinov, Neural Information Processing Systems (NeurIPS). 2020</p>
<p>Rt-1: Robotics transformer for realworld control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, J Ibarz, B Ichter, A Irpan, T Jackson, S Jesmonth, N J Joshi, R C Julian, D Kalashnikov, Y Kuang, I Leal, K.-H Lee, S Levine, Y Lu, U Malla, D Manjunath, I Mordatch, O Nachum, C Parada, J Peralta, E Perez, K Pertsch, J Quiambao, K Rao, M S Ryoo, G Salazar, P R Sanketi, K Sayed, J Singh, S A Sontakke, A Stone, C Tan, H Tran, V Vanhoucke, S Vega, Q H Vuong, F Xia, T Xiao, P Xu, S Xu, T Yu, B Zitkovich, abs/2212.06817ArXiv. 2022</p>
<p>Bridgedata v2: A dataset for robot learning at scale. H Walke, K Black, A Lee, M J Kim, M Du, C Zheng, T Zhao, P Hansen-Estruch, Q H Vuong, A W He, V Myers, K Fang, C Finn, S Levine, ArXiv. 2611009812308.12952, 2023</p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, CVPR. 2022</p>
<p>Pirlnav: Pretraining with imitation and rl finetuning for objectnav. R Ramrakhya, D Batra, E Wijmans, A Das, CVPR. 2023</p>
<p>Offline visual representation learning for embodied navigation. K Yadav, R Ramrakhya, A Majumdar, V.-P Berges, S Kuhar, D Batra, A Baevski, O Maksymets, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023</p>
<p>Navigating to objects in unseen environments by distance prediction. M Zhu, B Zhao, T Kong, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>Zson: Zero-shot object-goal navigation using multimodal goal embeddings. A Majumdar, G Aggarwal, B Devnani, J Hoffman, D Batra, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, International Conference on Machine Learning. 2021</p>
<p>Zero-shot object goal visual navigation. Q Zhao, L Zhang, B He, H Qiao, Z Liu, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Visual language maps for robot navigation. C Huang, O Mees, A Zeng, W Burgard, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2022615</p>
<p>Open-vocabulary queryable scene representations for real world planning. B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Recognize anything: A strong image tagging model. Y Zhang, X Huang, J Ma, Z Li, Z Luo, Y Xie, Y Qin, T Luo, Y Li, S Liu, Y Guo, L Zhang, abs/2306.03514ArXiv. 2590893332023</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, ArXiv. 2572194042302.13971, 2023</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T J Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, ArXiv. 2005.14165. 2020</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, abs/2201.07207ArXiv. 2460352762022</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). 202311530</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. S Huang, Z Jiang, H.-W Dong, Y J Qiao, P Gao, H Li, ArXiv. 2587626362305.11176, 2023</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, ArXiv. 2582919302304.10592, 2023</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y J Qiao, ArXiv. 2577718112303.16199, 2023</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S M Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q H Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P R Florence, abs/2303.03378ArXiv. 2023</p>
<p>Openai, abs/2303.08774Gpt-4 technical report. 2023</p>
<p>Navgpt: Explicit reasoning in visionand-language navigation with large language models. G Zhou, Y Hong, Q Wu, ArXiv. 2305.16986, 2023</p>
<p>Habitat: A Platform for Embodied AI Research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Habitatmatterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, M Savva, Y Zhao, D Batra, abs/2109.08238ArXiv. 2375632162021</p>
<p>. K Yadav, J Krantz, R Ramrakhya, S K Ramakrishnan, J Yang, A Wang, J Turner, A Gokaslan, V.-P Berges, R Mootaghi, O Maksymets, A X Chang, M Savva, A Clegg, D S Chaplot, D Batra, </p>
<p>Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022248006501</p>
<p>Offline visual representation learning for embodied navigation. K Yadav, R Ramrakhya, A Majumdar, V.-P Berges, S Kuhar, D Batra, A Baevski, O Maksymets, ArXiv. 2484269422204.13226, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8975 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8975</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8975</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-39a33093d78ed6941f71a158fddd607af5b69785</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39a33093d78ed6941f71a158fddd607af5b69785" target="_blank">Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work presents DataTuner, a neural, end-to-end data- to-text generation system that makes minimal assumptions about the data representation and target domain, combining a fine-tuned language model with a semantic fidelity classifier.</p>
                <p><strong>Paper Abstract:</strong> End-to-end neural data-to-text (D2T) generation has recently emerged as an alternative to pipeline-based architectures. However, it has faced challenges generalizing to new domains and generating semantically consistent text. In this work, we present DataTuner, a neural, end-to-end data-to-text generation system that makes minimal assumptions about the data representation and target domain. We take a two-stage generation-reranking approach, combining a fine-tuned language model with a semantic fidelity classifier. Each component is learnt end-toe-nd without needing dataset-specific heuristics, entity delexicalization, or post-processing. We show that DataTuner achieves state of the art results on automated metrics across four major D2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with fluency assessed by human annotators as nearing or exceeding the human-written reference texts. Our generated text has better semantic fidelity than the state of the art on these datasets. We further demonstrate that our model-based semantic fidelity scorer is a better assessment tool compared to traditional heuristic-based measures of semantic accuracy.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8975.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8975.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-triple linearization (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concatenated RDF triple linearization with special subject/predicate/object tokens (WebNLG preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A serialization of DBpedia RDF triples into a flat token sequence by concatenating triples and inserting special tokens for subject, predicate, and object; strings are converted to sentence-case. Used as input context to a pretrained autoregressive LM (GPT-2) for end-to-end data-to-text fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple is converted to a linear token span; triples are concatenated into a single input sequence with explicit special tokens marking 'subject', 'predicate', and 'object'. Sentence-case normalization is applied to literals. The linearized data is prepended to a special <text> token and used as context during LM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge graph triples (DBpedia triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Concatenate triples; insert special tokens before each triple component (<subject>, <predicate>, <object>); convert string literals to sentence-case; tokenize with BPE and include special tokens in vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation (text generation/verbalization of RDF triples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On WebNLG test set DataTuner_FC (using this linearization) achieved BLEU=52.4, METEOR=42.4, ROUGE_L=66.0, CIDEr=3.7. (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed previous pipeline and end-to-end baselines on automated metrics; compared to Castro Ferreira et al. (2019) pipeline BLEU=51.7 and E2E BLEU=33.5, DataTuner with linearization gave higher BLEU and METEOR. The paper also notes heuristic semantic scorers underperform on open-domain WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Domain-independent, simple to implement, leverages pretrained LM capability (no MR-specific encoder needed), robust to named entities due to BPE; achieved state-of-the-art automated metrics on WebNLG when combined with GPT-2 fine-tuning and SFC reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires insertion of dataset-dependent special tokens; linearization can create long sequences for multiple triples (1-7 triples), which increases sequence length and decoding cost. The paper does not report explicit representational information loss but notes heuristic evaluators struggle with diverse lexical realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases for this linearization are given in the paper; however, the authors note that heuristic-based fidelity measures (not the representation itself) fail on diverse realizations (e.g., 'United Kingdom' vs 'UK'), implying potential evaluation mismatch rather than conversion failure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8975.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8975.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR linearization (LDC2017T10)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parenthesized AMR linearization with role-specifier special tokens (LDC2017T10 preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A serialization of Abstract Meaning Representation (AMR) graphs into a parenthesized linear token sequence where role specifiers and node structure are preserved via special tokens and merged leaves; this linearized form is used as the data context for GPT-2 fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (AMR graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are converted into a bracketed, parenthesized linear form that includes special tokens for role specifiers (e.g., <:ARG0>, <:name>, <:manner>), merged leaf nodes for multi-token entities, and parentheses to indicate nested argument structure. The linearized sequence is tokenized (BPE) and passed as the data prefix to the language model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (semantic graphs representing predicate-argument structure)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Preprocess AMR using Ribeiro et al. (2019) script: merge multi-token leaves into single tokens, replace role specifiers with special tokens, and produce parenthesized linearized format that preserves node nesting and roles.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (verbalization of AMR graphs into natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On LDC2017T10 test set DataTuner_FC (using this linearization) achieved BLEU=37.7, METEOR=38.9, ROUGE_L=65.1, CIDEr=3.9. Baselines: Zhu et al. (2019) BLEU=31.8, Guo et al. (2019) BLEU=30.4, Ribeiro et al. (2019) BLEU=27.9. (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The linearization + pretrained LM outperformed specialized graph-aware models (graph-optimized Transformer, GCN, dual graph representations) on automated metrics for LDC2017T10, with the largest BLEU improvement (+5.9) vs best prior system (Zhu et al. 2019). The paper positions sequential linearization + pretrained LM as competitive with, or superior to, graph-structured encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and generic across graph sizes, enables direct use of pretrained autoregressive LMs (GPT-2) without designing graph encoders, demonstrated large gains on complex AMR-to-text generation, benefits from BPE for entity coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization flattens graph structure into sequence form — while effective experimentally here, may obscure explicit graph connectivity that graph encoders explicitly model; longer sequences for complex graphs increase computational load.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly reported; authors note that AMR dataset is challenging (diverse domains, complex sentences) but their approach still yields the largest improvements, implying limited failure cases documented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8975.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8975.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Slot-value serialization (E2E & ViGGO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Slot-value pair linearization with per-slot special tokens (Cleaned E2E and ViGGO preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear serialization for slot-based meaning representations that prefixes each slot value with a special token indicating the slot type, and, for ViGGO, encloses dialog act as special start/end tokens; the serialized sequence is used as LM context for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (slot-value pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert each slot-value pair into a tokenized span with a special token indicating the slot type (e.g., <name>, <area>, <eatType>), join pairs with delimiters (semicolons in examples), and, when applicable, add special tokens representing dialog acts at start/end. The resulting flat sequence is tokenized and used as input context for the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Slot-value structured meaning representations (flat attribute-value lists)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For Cleaned E2E: add special token before each slot type and serialize as ' <slot> slot=value ; ... '. For ViGGO: add special tokens at start/end for dialog act and before each slot type; tokenize with BPE.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Template-free data-to-text generation for slot-based MRs in restaurant (E2E) and dialogue/video-game domains (ViGGO)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cleaned E2E test set: DataTuner_FC BLEU=43.6, METEOR=39.0, ROUGE_L=57.5, CIDEr=2.0; baseline Dušek et al. (2019) BLEU=40.5. ViGGO test set: DataTuner_FC BLEU=53.6, METEOR=39.4, ROUGE_L=64.0, CIDEr=2.7; baseline Juraska et al. (2019) BLEU=52.1. (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The serialized slot-value approach combined with pretrained LM outperforms prior rule-based reranked LSTM models and transformer baselines on automated metrics. Differences are smaller on closed-domain datasets (ViGGO, Cleaned E2E) than on AMR.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Very simple to implement for slot-based data, allows direct fine-tuning of pretrained LMs without delexicalization or MR-specific encoders; achieves strong fluency and fidelity in human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on consistent slot formatting and special token vocab augmentation; may be less informative than structured encoders when task requires modeling complex relations between slots.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>None explicitly reported for the representation; the paper notes overall D2T challenges (hallucination, omission) which the SFC mitigates rather than being caused by serialization per se.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8975.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8975.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-grained state embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained state (segment) embeddings tied to MR special tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An input-embedding augmentation that assigns a state-token ID to each input token indicating the most recent special token (e.g., which slot or MR component it belongs to), summed with token and positional embeddings before feeding to GPT-2, providing fine-grained structural cues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>state embeddings (fine-grained segment markers)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>In addition to token and positional embeddings, a state embedding vector is added where the state ID for each token equals the last special token preceding it in the sequence. This gives the model a per-token categorical signal about which MR field or segment the token belongs to (more granular than a global <data>/<text> segment).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applicable to any serialized MR (RDF triples, AMR linearization, slot-value lists) — i.e., graph or structured MRs represented as sequences</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>During preprocessing the MR special tokens are inserted; during embedding lookup assign each token a state ID equal to the last special token seen; sum token, position, and state embeddings as input to GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation (improves contextualization of serialized MR inputs for LM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adding fine-grained state embeddings improved BLEU across datasets: examples include a +2.0 BLEU boost on ViGGO and smaller boosts (0.3 to 2.0) across datasets compared to coarse-grained (<data>/<text>) states. Exact per-dataset deltas reported qualitatively and in Table 2 comparisons between DATATUNER_NO_FC/FS and DATATUNER_NO_FC.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to using only coarse-grained <data>/<text> segment embeddings (as in some prior work), fine-grained state embeddings consistently improved automated metrics; this shows structured state signals help LM conditioning without a graph encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides explicit, local MR-type signals to the autoregressive LM, helps disambiguation of slots/roles, simple to add (vocabulary + embedding table) and general across MR types; empirically improves BLEU/METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires defining and inserting MR-specific special tokens during preprocessing and slightly enlarges the model (new embedding types); effectiveness depends on quality of special-token design.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases reported; the paper shows modest but consistent gains, implying limited downside reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8975.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8975.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-Optimized Transformer (Zhu et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modeling graph structure in transformer for better AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based approach that incorporates graph structural biases to better model AMR graph connectivity for AMR-to-text generation; referenced as a strong baseline specialized for AMR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling graph structure in transformer for better AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-optimized Transformer / graph-aware Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transformer architecture modified to account for graph edges/structure so token representations respect AMR connectivity rather than plain sequence order; designed to better capture non-local dependencies in graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate directly on graph-structured inputs via model modifications (attention patterns or positional encodings) that encode graph adjacency rather than flattening into a sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline on LDC2017T10 in this paper: Zhu et al. (2019) BLEU=31.8, METEOR=36.4 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Specialized graph-aware architectures like Zhu et al. are outperformed by DATATUNER's GMT (GPT-2 linearization + state embeddings + SFC) on LDC2017T10 in this work (DATATUNER_FC BLEU=37.7).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models graph structure and non-local edges, which can help capture AMR semantics without flattening.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Typically task/dataset specific and more complex to design than simple linearization; previous E-to-E systems trained from scratch did not generalize as well to unseen domains per cited comparative studies.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The paper reports that prior E-to-E approaches (including graph-aware ones trained from scratch) performed worse on unseen domains compared to pipeline approaches; no specific failure cases for Zhu et al. are detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8975.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8975.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN-based graph-to-sequence (Guo et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Densely connected graph convolutional networks for graph-to-sequence learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph convolutional network (GCN) approach that processes graph-structured meaning representations with densely connected graph convolution layers and feeds learned node representations into a seq2seq decoder for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Densely connected graph convolutional networks for graph-to-sequence learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph convolutional encoding (GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply multiple densely connected GCN layers to propagate information across graph nodes and edges, producing node-level embeddings that are then used by a decoder (RNN/Transformer) to generate textual output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and other graph-structured MRs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate on graph adjacency and node labels directly with GCN layers (no linearization); aggregated node features inform a sequence decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline on LDC2017T10 in this paper: Guo et al. (2019) BLEU=30.4 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GCN-based graph encoders are referenced as prior specialized approaches; DATATUNER's linearization+pretrained LM outperformed GCN baselines on automated metrics in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models graph topology and can capture local and multi-hop graph structure via convolutional message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Architecturally more complex, often trained from scratch on task data (less benefit from large pretrained LMs), and may be less flexible across diverse MR formats without modification.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not detailed in this paper, but comparative study cited suggests E-to-E systems trained from scratch (including GCN-based encoders) generalize poorly to unseen domains compared to pipeline methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8975.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8975.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual/structural AMR encoders (Damonte & Cohen; Ribeiro et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural neural encoders and dual graph representations for AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that replace sequential encoders with graph-structured encoders (structural neural encoders, dual graph views) to better capture AMR graph properties for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural neural encoders for AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structural / dual graph encodings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode AMR using graph-specific encoders (e.g., structural encoders, dual graph representations) that capture both original graph and alternative graph views, feeding learned representations into decoders for surface realization.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Do not linearize; instead compute graph-aware node/edge representations (possibly multiple graph views) and decode into text using seq2seq decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced in related work; Ribeiro et al. (2019) reported BLEU=27.9 (Table 2) as a prior baseline on LDC2017T10.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>These graph-aware encoders have been shown by prior work to improve over naive sequential encoders in some settings, but DataTuner's linearization + pretrained LM achieved higher BLEU/METEOR on LDC2017T10 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverage explicit graph structure to model semantic relations and non-local dependencies in AMR.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Require specialized encoder design per graph formalism and are typically trained from scratch (do not leverage large-scale pretrained LMs as directly), which can limit domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not enumerated in detail here; referenced comparative work (Castro Ferreira et al., 2019) indicates E-to-E systems (various encoders) trained from scratch may struggle to generalize to unseen domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modeling graph structure in transformer for better AMR-to-text generation. <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning. <em>(Rating: 2)</em></li>
                <li>Structural neural encoders for AMR-to-text generation. <em>(Rating: 2)</em></li>
                <li>Enhancing amr-to-text generation with dual graph representations. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8975",
    "paper_id": "paper-39a33093d78ed6941f71a158fddd607af5b69785",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "RDF-triple linearization (WebNLG)",
            "name_full": "Concatenated RDF triple linearization with special subject/predicate/object tokens (WebNLG preprocessing)",
            "brief_description": "A serialization of DBpedia RDF triples into a flat token sequence by concatenating triples and inserting special tokens for subject, predicate, and object; strings are converted to sentence-case. Used as input context to a pretrained autoregressive LM (GPT-2) for end-to-end data-to-text fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization (RDF triples)",
            "representation_description": "Each RDF triple is converted to a linear token span; triples are concatenated into a single input sequence with explicit special tokens marking 'subject', 'predicate', and 'object'. Sentence-case normalization is applied to literals. The linearized data is prepended to a special &lt;text&gt; token and used as context during LM fine-tuning.",
            "graph_type": "RDF triples / knowledge graph triples (DBpedia triples)",
            "conversion_method": "Concatenate triples; insert special tokens before each triple component (&lt;subject&gt;, &lt;predicate&gt;, &lt;object&gt;); convert string literals to sentence-case; tokenize with BPE and include special tokens in vocabulary.",
            "downstream_task": "Data-to-text generation (text generation/verbalization of RDF triples)",
            "performance_metrics": "On WebNLG test set DataTuner_FC (using this linearization) achieved BLEU=52.4, METEOR=42.4, ROUGE_L=66.0, CIDEr=3.7. (Table 2)",
            "comparison_to_others": "Outperformed previous pipeline and end-to-end baselines on automated metrics; compared to Castro Ferreira et al. (2019) pipeline BLEU=51.7 and E2E BLEU=33.5, DataTuner with linearization gave higher BLEU and METEOR. The paper also notes heuristic semantic scorers underperform on open-domain WebNLG.",
            "advantages": "Domain-independent, simple to implement, leverages pretrained LM capability (no MR-specific encoder needed), robust to named entities due to BPE; achieved state-of-the-art automated metrics on WebNLG when combined with GPT-2 fine-tuning and SFC reranking.",
            "disadvantages": "Requires insertion of dataset-dependent special tokens; linearization can create long sequences for multiple triples (1-7 triples), which increases sequence length and decoding cost. The paper does not report explicit representational information loss but notes heuristic evaluators struggle with diverse lexical realizations.",
            "failure_cases": "No explicit failure cases for this linearization are given in the paper; however, the authors note that heuristic-based fidelity measures (not the representation itself) fail on diverse realizations (e.g., 'United Kingdom' vs 'UK'), implying potential evaluation mismatch rather than conversion failure.",
            "uuid": "e8975.0",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "AMR linearization (LDC2017T10)",
            "name_full": "Parenthesized AMR linearization with role-specifier special tokens (LDC2017T10 preprocessing)",
            "brief_description": "A serialization of Abstract Meaning Representation (AMR) graphs into a parenthesized linear token sequence where role specifiers and node structure are preserved via special tokens and merged leaves; this linearized form is used as the data context for GPT-2 fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization (AMR graph)",
            "representation_description": "AMR graphs are converted into a bracketed, parenthesized linear form that includes special tokens for role specifiers (e.g., &lt;:ARG0&gt;, &lt;:name&gt;, &lt;:manner&gt;), merged leaf nodes for multi-token entities, and parentheses to indicate nested argument structure. The linearized sequence is tokenized (BPE) and passed as the data prefix to the language model.",
            "graph_type": "AMR graphs (semantic graphs representing predicate-argument structure)",
            "conversion_method": "Preprocess AMR using Ribeiro et al. (2019) script: merge multi-token leaves into single tokens, replace role specifiers with special tokens, and produce parenthesized linearized format that preserves node nesting and roles.",
            "downstream_task": "AMR-to-text generation (verbalization of AMR graphs into natural language)",
            "performance_metrics": "On LDC2017T10 test set DataTuner_FC (using this linearization) achieved BLEU=37.7, METEOR=38.9, ROUGE_L=65.1, CIDEr=3.9. Baselines: Zhu et al. (2019) BLEU=31.8, Guo et al. (2019) BLEU=30.4, Ribeiro et al. (2019) BLEU=27.9. (Table 2)",
            "comparison_to_others": "The linearization + pretrained LM outperformed specialized graph-aware models (graph-optimized Transformer, GCN, dual graph representations) on automated metrics for LDC2017T10, with the largest BLEU improvement (+5.9) vs best prior system (Zhu et al. 2019). The paper positions sequential linearization + pretrained LM as competitive with, or superior to, graph-structured encoders.",
            "advantages": "Simple and generic across graph sizes, enables direct use of pretrained autoregressive LMs (GPT-2) without designing graph encoders, demonstrated large gains on complex AMR-to-text generation, benefits from BPE for entity coverage.",
            "disadvantages": "Linearization flattens graph structure into sequence form — while effective experimentally here, may obscure explicit graph connectivity that graph encoders explicitly model; longer sequences for complex graphs increase computational load.",
            "failure_cases": "Not explicitly reported; authors note that AMR dataset is challenging (diverse domains, complex sentences) but their approach still yields the largest improvements, implying limited failure cases documented in this work.",
            "uuid": "e8975.1",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Slot-value serialization (E2E & ViGGO)",
            "name_full": "Slot-value pair linearization with per-slot special tokens (Cleaned E2E and ViGGO preprocessing)",
            "brief_description": "A linear serialization for slot-based meaning representations that prefixes each slot value with a special token indicating the slot type, and, for ViGGO, encloses dialog act as special start/end tokens; the serialized sequence is used as LM context for fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization (slot-value pairs)",
            "representation_description": "Convert each slot-value pair into a tokenized span with a special token indicating the slot type (e.g., &lt;name&gt;, &lt;area&gt;, &lt;eatType&gt;), join pairs with delimiters (semicolons in examples), and, when applicable, add special tokens representing dialog acts at start/end. The resulting flat sequence is tokenized and used as input context for the LM.",
            "graph_type": "Slot-value structured meaning representations (flat attribute-value lists)",
            "conversion_method": "For Cleaned E2E: add special token before each slot type and serialize as ' &lt;slot&gt; slot=value ; ... '. For ViGGO: add special tokens at start/end for dialog act and before each slot type; tokenize with BPE.",
            "downstream_task": "Template-free data-to-text generation for slot-based MRs in restaurant (E2E) and dialogue/video-game domains (ViGGO)",
            "performance_metrics": "Cleaned E2E test set: DataTuner_FC BLEU=43.6, METEOR=39.0, ROUGE_L=57.5, CIDEr=2.0; baseline Dušek et al. (2019) BLEU=40.5. ViGGO test set: DataTuner_FC BLEU=53.6, METEOR=39.4, ROUGE_L=64.0, CIDEr=2.7; baseline Juraska et al. (2019) BLEU=52.1. (Table 2)",
            "comparison_to_others": "The serialized slot-value approach combined with pretrained LM outperforms prior rule-based reranked LSTM models and transformer baselines on automated metrics. Differences are smaller on closed-domain datasets (ViGGO, Cleaned E2E) than on AMR.",
            "advantages": "Very simple to implement for slot-based data, allows direct fine-tuning of pretrained LMs without delexicalization or MR-specific encoders; achieves strong fluency and fidelity in human evaluation.",
            "disadvantages": "Relies on consistent slot formatting and special token vocab augmentation; may be less informative than structured encoders when task requires modeling complex relations between slots.",
            "failure_cases": "None explicitly reported for the representation; the paper notes overall D2T challenges (hallucination, omission) which the SFC mitigates rather than being caused by serialization per se.",
            "uuid": "e8975.2",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Fine-grained state embeddings",
            "name_full": "Fine-grained state (segment) embeddings tied to MR special tokens",
            "brief_description": "An input-embedding augmentation that assigns a state-token ID to each input token indicating the most recent special token (e.g., which slot or MR component it belongs to), summed with token and positional embeddings before feeding to GPT-2, providing fine-grained structural cues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "state embeddings (fine-grained segment markers)",
            "representation_description": "In addition to token and positional embeddings, a state embedding vector is added where the state ID for each token equals the last special token preceding it in the sequence. This gives the model a per-token categorical signal about which MR field or segment the token belongs to (more granular than a global &lt;data&gt;/&lt;text&gt; segment).",
            "graph_type": "Applicable to any serialized MR (RDF triples, AMR linearization, slot-value lists) — i.e., graph or structured MRs represented as sequences",
            "conversion_method": "During preprocessing the MR special tokens are inserted; during embedding lookup assign each token a state ID equal to the last special token seen; sum token, position, and state embeddings as input to GPT-2.",
            "downstream_task": "Data-to-text generation (improves contextualization of serialized MR inputs for LM fine-tuning)",
            "performance_metrics": "Adding fine-grained state embeddings improved BLEU across datasets: examples include a +2.0 BLEU boost on ViGGO and smaller boosts (0.3 to 2.0) across datasets compared to coarse-grained (&lt;data&gt;/&lt;text&gt;) states. Exact per-dataset deltas reported qualitatively and in Table 2 comparisons between DATATUNER_NO_FC/FS and DATATUNER_NO_FC.",
            "comparison_to_others": "Compared to using only coarse-grained &lt;data&gt;/&lt;text&gt; segment embeddings (as in some prior work), fine-grained state embeddings consistently improved automated metrics; this shows structured state signals help LM conditioning without a graph encoder.",
            "advantages": "Provides explicit, local MR-type signals to the autoregressive LM, helps disambiguation of slots/roles, simple to add (vocabulary + embedding table) and general across MR types; empirically improves BLEU/METEOR.",
            "disadvantages": "Requires defining and inserting MR-specific special tokens during preprocessing and slightly enlarges the model (new embedding types); effectiveness depends on quality of special-token design.",
            "failure_cases": "No explicit failure cases reported; the paper shows modest but consistent gains, implying limited downside reported.",
            "uuid": "e8975.3",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Graph-Optimized Transformer (Zhu et al. 2019)",
            "name_full": "Modeling graph structure in transformer for better AMR-to-text generation",
            "brief_description": "A Transformer-based approach that incorporates graph structural biases to better model AMR graph connectivity for AMR-to-text generation; referenced as a strong baseline specialized for AMR.",
            "citation_title": "Modeling graph structure in transformer for better AMR-to-text generation.",
            "mention_or_use": "mention",
            "representation_name": "graph-optimized Transformer / graph-aware Transformer",
            "representation_description": "Transformer architecture modified to account for graph edges/structure so token representations respect AMR connectivity rather than plain sequence order; designed to better capture non-local dependencies in graphs.",
            "graph_type": "AMR graphs (semantic graphs)",
            "conversion_method": "Operate directly on graph-structured inputs via model modifications (attention patterns or positional encodings) that encode graph adjacency rather than flattening into a sequence.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Reported baseline on LDC2017T10 in this paper: Zhu et al. (2019) BLEU=31.8, METEOR=36.4 (Table 2).",
            "comparison_to_others": "Specialized graph-aware architectures like Zhu et al. are outperformed by DATATUNER's GMT (GPT-2 linearization + state embeddings + SFC) on LDC2017T10 in this work (DATATUNER_FC BLEU=37.7).",
            "advantages": "Explicitly models graph structure and non-local edges, which can help capture AMR semantics without flattening.",
            "disadvantages": "Typically task/dataset specific and more complex to design than simple linearization; previous E-to-E systems trained from scratch did not generalize as well to unseen domains per cited comparative studies.",
            "failure_cases": "The paper reports that prior E-to-E approaches (including graph-aware ones trained from scratch) performed worse on unseen domains compared to pipeline approaches; no specific failure cases for Zhu et al. are detailed here.",
            "uuid": "e8975.4",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "GCN-based graph-to-sequence (Guo et al. 2019)",
            "name_full": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "brief_description": "A graph convolutional network (GCN) approach that processes graph-structured meaning representations with densely connected graph convolution layers and feeds learned node representations into a seq2seq decoder for text generation.",
            "citation_title": "Densely connected graph convolutional networks for graph-to-sequence learning.",
            "mention_or_use": "mention",
            "representation_name": "graph convolutional encoding (GCN)",
            "representation_description": "Apply multiple densely connected GCN layers to propagate information across graph nodes and edges, producing node-level embeddings that are then used by a decoder (RNN/Transformer) to generate textual output.",
            "graph_type": "AMR graphs and other graph-structured MRs",
            "conversion_method": "Operate on graph adjacency and node labels directly with GCN layers (no linearization); aggregated node features inform a sequence decoder.",
            "downstream_task": "Graph-to-text generation (AMR-to-text)",
            "performance_metrics": "Reported baseline on LDC2017T10 in this paper: Guo et al. (2019) BLEU=30.4 (Table 2).",
            "comparison_to_others": "GCN-based graph encoders are referenced as prior specialized approaches; DATATUNER's linearization+pretrained LM outperformed GCN baselines on automated metrics in the experiments.",
            "advantages": "Directly models graph topology and can capture local and multi-hop graph structure via convolutional message passing.",
            "disadvantages": "Architecturally more complex, often trained from scratch on task data (less benefit from large pretrained LMs), and may be less flexible across diverse MR formats without modification.",
            "failure_cases": "Not detailed in this paper, but comparative study cited suggests E-to-E systems trained from scratch (including GCN-based encoders) generalize poorly to unseen domains compared to pipeline methods.",
            "uuid": "e8975.5",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Dual/structural AMR encoders (Damonte & Cohen; Ribeiro et al.)",
            "name_full": "Structural neural encoders and dual graph representations for AMR-to-text generation",
            "brief_description": "Approaches that replace sequential encoders with graph-structured encoders (structural neural encoders, dual graph views) to better capture AMR graph properties for text generation.",
            "citation_title": "Structural neural encoders for AMR-to-text generation.",
            "mention_or_use": "mention",
            "representation_name": "structural / dual graph encodings",
            "representation_description": "Encode AMR using graph-specific encoders (e.g., structural encoders, dual graph representations) that capture both original graph and alternative graph views, feeding learned representations into decoders for surface realization.",
            "graph_type": "AMR graphs",
            "conversion_method": "Do not linearize; instead compute graph-aware node/edge representations (possibly multiple graph views) and decode into text using seq2seq decoders.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Referenced in related work; Ribeiro et al. (2019) reported BLEU=27.9 (Table 2) as a prior baseline on LDC2017T10.",
            "comparison_to_others": "These graph-aware encoders have been shown by prior work to improve over naive sequential encoders in some settings, but DataTuner's linearization + pretrained LM achieved higher BLEU/METEOR on LDC2017T10 in this paper.",
            "advantages": "Leverage explicit graph structure to model semantic relations and non-local dependencies in AMR.",
            "disadvantages": "Require specialized encoder design per graph formalism and are typically trained from scratch (do not leverage large-scale pretrained LMs as directly), which can limit domain generalization.",
            "failure_cases": "Not enumerated in detail here; referenced comparative work (Castro Ferreira et al., 2019) indicates E-to-E systems (various encoders) trained from scratch may struggle to generalize to unseen domains.",
            "uuid": "e8975.6",
            "source_info": {
                "paper_title": "Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modeling graph structure in transformer for better AMR-to-text generation.",
            "rating": 2
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning.",
            "rating": 2
        },
        {
            "paper_title": "Structural neural encoders for AMR-to-text generation.",
            "rating": 2
        },
        {
            "paper_title": "Enhancing amr-to-text generation with dual graph representations.",
            "rating": 2
        }
    ],
    "cost": 0.015543749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity</h1>
<p>Hamza Harkous<br>Amazon Alexa<br>hamza.harkous@gmail.com</p>
<p>Isabel Groves<br>Amazon Alexa<br>isabeg@amazon.com</p>
<p>Amir Saffari<br>Amazon Alexa<br>amsafari@amazon.com</p>
<h4>Abstract</h4>
<p>End-to-end neural data-to-text (D2T) generation has recently emerged as an alternative to pipeline-based architectures. However, it has faced challenges generalizing to new domains and generating semantically consistent text. In this work, we present DATATUNER, a neural, end-to-end data-to-text generation system that makes minimal assumptions about the data representation and target domain. We take a two-stage generation-reranking approach, combining a fine-tuned language model with a semantic fidelity classifier. Each component is learnt end-toend without needing dataset-specific heuristics, entity delexicalization, or post-processing. We show that DATATUNER achieves state of the art results on automated metrics across four major D2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with fluency assessed by human annotators as nearing or exceeding the human-written reference texts. Our generated text has better semantic fidelity than the state of the art on these datasets. We further demonstrate that our model-based semantic fidelity scorer is a better assessment tool compared to traditional heuristic-based measures of semantic accuracy.</p>
<h2>1 Introduction</h2>
<p>Data-to-Text generation (D2T) is defined as automatically generating natural language texts from nonlinguistic inputs (Reiter and Dale, 2000). Interest in this task has been driven by its applicability to specialized domains. For instance, D2T has been applied to generating weather reports (Liang et al., 2009), restaurant descriptions (Novikova et al., 2017b), and video game dialogues (Juraska et al., 2019). Recently, researchers have investigated D2T with more diverse domains to arrive at more generalizable text generation (such as works on LDC2017T10 (Knight et al., 2017) and WebNLG (Gardent et al., 2017) datasets).</p>
<p>Traditional approaches to D2T follow a pipeline-based methodology, dividing the problem into several sub-problems (Reiter and Dale, 2000; Gatt and Krahmer, 2018). These include content selection (which information to include in the text), text structuring (the order in which to present the data), sentence aggregation (which information goes in individual sentences), lexicalization (finding the right words and phrases to express the data), referring expression generation (selecting the words and phrases to identify domain objects), and linguistic realization (combining all the generated words and phrases into wellformed sentences).</p>
<p>In recent years, there has been a growing interest in going beyond pipeline-based approaches towards end-to-end (E-to-E) methods driven by recent advancements in deep learning (Lebret et al., 2016; Novikova et al., 2017b; Castro Ferreira et al., 2019; Dušek et al., 2020). Such methods can be trained with (data,text) tuples that can be efficiently collected at scale. In contrast, each step in pipeline-based approaches requires its own setup and training data, such as semantic alignments between sections of text and components of the meaning representation $(M R)$. This makes them more costly and complex to develop and more prone to error propagation.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To date, end-to-end D2T has faced two main challenges: (1) generalization to unseen domains and (2) maintaining semantic fidelity to accurately convey the source data. In a recent comparative study, Castro Ferreira et al. (2019) found that, compared to the best pipeline-based system, E-to-E approaches based on GRU and Transformer architectures scored more than 35 BLEU points lower on unseen domains from the WebNLG dataset, and scored worst for semantic accuracy.</p>
<p>To address these challenges, we introduce DATATUNER, an E-to-E, domain-independent D2T system that makes no assumptions about the generated text's domain or the MR's structure. DATATUNER leverages a pretrained language model and fine-grained state embeddings to achieve strong generalization. It also employs a weakly-supervised Semantic Fidelity Classifier (SFC) to detect and avoid generation errors (such as hallucination and omission). We also leverage this classifier to assess outputs from any D2T system, overcoming the limitations of existing heuristic methods for detecting semantic errors.</p>
<p>In this work, we deliver four main contributions across four major D2T datasets from various domains and MRs.</p>
<ul>
<li>We show that DATATUNER pushes the state of the art on automated metrics by significant margins, ranging from 1.2 to 5.9 BLEU points, compared to the best existing pipeline and E-to-E techniques.</li>
<li>With a crowdsourcing experiment, we demonstrate that DATATUNER generates text with significantly better fluency than existing works. On two datasets, our texts are even judged to be better, on average, than human-written references.</li>
<li>We show that DATATUNER improves the semantic accuracy of generated texts, with margins ranging from $5.3 \%$ to $40 \%$ as assessed by crowdsourcing workers.</li>
<li>With expert annotations, we further show that our model-based semantic accuracy metric is $4.2 \%$ to $14.2 \%$ more accurate in detecting semantic errors than existing heuristic-based approaches.</li>
</ul>
<p>We open-source the DATATUNER code at https://github.com/amazon-research.</p>
<h1>2 Related Work</h1>
<p>Pipeline vs. End-to-End Approaches: Within the pipeline-based paradigm, several studies have illustrated that breaking the D2T problem into sub-problems improves overall performance. Moryossef et al. (2019b) showed that separating planning from realization helps achieve better semantic faithfulness compared to an E-to-E neural approach on the WebNLG dataset. Castro Ferreira et al. (2019) conducted a comparative study across a variety of E-to-E and pipeline approaches with WebNLG, concluding that the latter are significantly better at generalizing to unseen domains. However, so far the E-to-E approaches in these studies have been trained from scratch on the task dataset. Our work investigates whether using a pretrained model with strong language generation capabilities raises the performance of E-to-E models. Structured Representations of the Data: Another thread of research focuses on better encoders for meaning representation languages, exploiting their structural properties. This is particularly relevant to AMR (Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhu et al., 2019; Guo et al., 2019). Damonte and Cohen (2019) showed that replacing sequential encoders with a graph encoder improves text quality as measured by BLEU and METEOR scores. Zhu et al. (2019) proposed using self-attention to better model indirectly connected AMR components. In this work, we are the first to design a system that achieves a strong performance across different data structures, ranging from slot-value pairs to graph-based $M R$. We also show that such a system can deliver significant gains compared to existing specialized systems. Semantic Fidelity Guarantees: To improve semantic fidelity (how accurately the generated text conveys the meaning) in E-to-E architectures, one approach has been to train reverse "Text-to-Data" models (Chisholm et al., 2017; Agarwal et al., 2018). Another approach by Kedzie and McKeown (2019) used data augmentation and a reliable $M R$ parser to reduce semantic errors in the generated text. Nie et al. (2019) focused on fixing training data errors via an iterative data refinement technique using a language understanding module. Nie et al. (2018) tackled the specific case where symbolic operations (e.g. numerical comparisons) are needed, augmenting the encoded input by pre-calculating these inferrable facts.</p>
<p>Shen et al. (2019) used techniques from computational pragmatics and modeled the generation task as a game between speakers and listeners. Despite following the generation-reranking paradigm explored previously in the data-to-text domain (Agarwal et al., 2018; Moryossef et al., 2019a; Dušek et al., 2019), and in other domains including machine translation (Shen et al., 2004), dialogue generation (Wen et al., 2015), and ASR (Morbini et al., 2012), our work has several distinctive aspects compared to previous works. First, we do not make extra assumptions, such as availability of precise $M R$ parsers. Second, our system provides improvements even when the data is not the root cause of semantic errors. Third, we go beyond encouraging the model to avoid semantically inconsistent outputs: we aim to also detect with high probability when the generated text still contains such errors.</p>
<p>For industrial NLG applications, including in healthcare (Pauws et al., 2019) or news (Leppänen et al., 2017), identifying individual generations that are inaccurate is vital for the system to be useful in practice (Smiley et al., 2017). This error detection task has commonly relied on handwritten mappings from data values to potential realizations. Such rules were used to compute a Slot Error Rate (SER) metric (Dušek et al., 2019; Juraska et al., 2019; Moryossef et al., 2019a). For instance, Dušek et al. (2019) use SER for reranking beam elements during decoding in an attention-based sequence-to-sequence model on the Cleaned E2E dataset. Juraska et al. (2019) used the approach similarly with a transformer model on the ViGGO dataset. This technique is difficult to scale to new domains or languages, and struggles when the $M R$ is not dominated by values that occur verbatim in the text (e.g. named entities). We aim to tackle that with our model-based semantic fidelity classifier.</p>
<h1>3 Problem Description</h1>
<p>The D2T task is formally defined as generating text $T$ from data $D$ that is encoded via a meaning representation $M R$. We assume that content selection is done prior to the D2T task, an assumption also made in the datasets we use. Therefore, the text $T$ should have semantic fidelity by conveying all the input data, and only the input data.</p>
<h3>3.1 Datasets</h3>
<p>We selected the major datasets that satisfy the task definition above. Each dataset consists of $(D, T)$ pairs with texts in English. The following describes each dataset and our preprocessing/linearization, including special tokens added (highlighted in $&lt;$ bold $&gt;$ below) to guide our models. We provide the datasets' statistics in Table 1 of the appendix, and we show examples from them in Figure 1.</p>
<p>WebNLG: In WebNLG, $D$ is a set of 1-7 DBpedia triples which $T$ verbalizes (Gardent et al., 2017). The test data spans 15 domains, 10 of which are seen in training. We linearize by concatenating triples, adding special tokens for 'subject', 'predicate', and 'object', and converting strings to sentence-case. For fair comparison with the state of the art, we use v1.4 from Castro Ferreira et al. (2018).</p>
<p>LDC2017T10: In the LDC2017T10 dataset (Knight et al., 2017), $D$ is an Abstract Meaning Representation (AMR) graph representing "who is doing what to whom" for each sentence in $T$. The texts include broadcast news and weblogs. We use the preprocessing script from Ribeiro et al. (2019), without lowercasing. We merge leaves that correspond to one entity (e.g. "United States" below). Each role specifier is replaced with a special token.</p>
<p>Cleaned E2E: The Cleaned E2E dataset introduced in (Dušek et al., 2019) is an automatically cleaned version of the original E2E dataset (Novikova et al., 2017b), aiming to eliminate omissions and hallucinations in the human text by fixing the corresponding $M R$. Each $M R$ consists of 3-8 slot-value pairs in the restaurant domain. We preprocess $D$ by adding special tokens before each slot type.</p>
<p>ViGGO: In ViGGO (Juraska et al., 2019), $D$ is a meaning representation with one of 9 dialogue acts (e.g. give_opinion, suggest) and 1-8 slot-value pairs from 14 video game attributes (e.g. NAME, GENRES). Each $T$ is an utterance representing a dialogue turn. We add special tokens at the start and end, representing the dialog act, and before each slot type.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Train Size</th>
<th style="text-align: left;">Validation Size</th>
<th style="text-align: center;">Test Size</th>
<th style="text-align: center;">Unique Words</th>
<th style="text-align: center;">\% Capitalized</th>
<th style="text-align: center;">Dale-Chall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cleaned E2E</td>
<td style="text-align: left;">33525</td>
<td style="text-align: left;">4299</td>
<td style="text-align: center;">4693</td>
<td style="text-align: center;">1966</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">LDC2017T10</td>
<td style="text-align: left;">36521</td>
<td style="text-align: left;">1368</td>
<td style="text-align: center;">1371</td>
<td style="text-align: center;">5533</td>
<td style="text-align: center;">$22 \%$</td>
<td style="text-align: center;">6.49</td>
</tr>
<tr>
<td style="text-align: left;">ViGGO</td>
<td style="text-align: left;">5103</td>
<td style="text-align: left;">714</td>
<td style="text-align: center;">1083</td>
<td style="text-align: center;">2014</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">1.02</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">18081</td>
<td style="text-align: left;">2260</td>
<td style="text-align: center;">4928</td>
<td style="text-align: center;">7253</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">1.03</td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset text statistics</p>
<p>As we illustrate in Table 1, the datasets vary widely. LDC2017T10 dataset is not bounded to specific domains. Hence, although the AMR format closely describes the text, it is non-trivial to generalize from the training to test data. WebNLG covers a wide, but restricted set of domains, only a subset of which are present in the training data. However it has high lexical diversity. The number of unique words in the test set of WebNLG is 7253 ( $63 \%$ of them capitalized), compared to 5533 ( $22 \%$ capitalized) for LDC2017T10, 2014 ( $33 \%$ capitalized) for ViGGO, and 1966 ( $29 \%$ capitalized) for Cleaned E2E. Measured with the New Dale-Chall readability score (Dale and Chall, 1948), LDC2017T10 has the highest difficulty score (6.49) compared to $1.03,0.85$, and 1.02 for the WebNLG, Cleaned E2E, and ViGGO datasets respectively. In terms of quality, ViGGO was designed with the goal of perfect semantic fidelity, and Cleaned E2E was heavily filtered from the original dataset to achieve that. On the other hand, the versions we use of the other datasets have not undergone such filtering.</p>
<h1>4 DataTuner Architecture</h1>
<p>We designed DataTuner to be highly generic in order to tackle diverse meaning representations and allow D2T generators to be built for new datasets with minimal work beyond data preprocessing. At a high-level, our text generation system takes a 2-stage approach: generation and reranking. First, we fine-tune a pretrained language model on the D2T task using the task's training data. Next, we build a specialized semantic fidelity classifier trained on an automatically-generated task-specific corpus. Using these models, we construct a customized beam-search decoder that ranks candidates based on the probabilities from the language model, and, at its final stage, reranks them based on the classifier's labels.</p>
<h3>4.1 Data-to-Text Model Fine-tuning</h3>
<p>The fine-tuned Data-to-Text Language Model (D2T-LM) builds on the pretrained OpenAI GPT-2 model (Radford et al., 2019), a multi-layer, autoregressive language model. Each layer is a transformer decoder block (Vaswani et al., 2017) of masked multi-headed attention and a fully connected layer. We provide a full model diagram in Figure 2.</p>
<div class="codehilite"><pre><span></span><code>WebNLG
D= Aarhus | leaderName | Jacob_Bundsgaard
Linearized D= &lt;subject&gt; Aarhus &lt;predicate&gt; leader name
    &lt;object&gt; Jacob Bundsgaard
T= The leader of Aarhus is Jacob Bundsgaard.
</code></pre></div>

<h2>Cleaned E2E</h2>
<p>$\mathbf{D}=$ name[Zizzi], eatType[coffee shop], area[riverside]
Linearized $\mathbf{D}=&lt;$ name $&gt;$ name=[Zizzi]; <area $>$ area=[riverside]
<eatType $>$ eatType $=[\text { coffee shop }]$
$\mathbf{T}=$ You can find a coffee shop named Zizzi in the riverside area.</p>
<h2>ViGGO</h2>
<p>$\mathbf{D}=$ request( developer[EA Canada], specifier[favorite])
Linearized $\mathbf{D}=&lt;$ request $&gt;$ request ( $&lt;$ developer $&gt;$ developer: [EA Canada],
<specifier> specifier: [favorite] <request>)
T= What's your favorite game that EA Canada has made?</p>
<p>LDC2017T10
$\mathbf{D}=(\mathrm{r} /$ respond -01
:ARG0 (c / country :wiki "United.States"
:name (n / name :op1 "United"
:op2 "States"))
:ARG1 (d / develop -01
:mod (t / that))
:ARG2 (c2 / condemn -01
:manner (s / swift)))
Linearized D= (respond &lt;:ARG0&gt;
(country &lt;:name&gt; (United States))
$&lt;:$ ARG1 $&gt;$ (develop &lt;:mod&gt; (that))
$&lt;:$ ARG2 $&gt;$ (condemn &lt;:manner $&gt;$ (swift)))
T= The United States responded to that
development with swift condemnation.</p>
<p>Figure 1: Examples from each of the datasets</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Data-to-text language model fine-tuning setup</p>
<p>Inputs: The input sequence is the data $D$ concatenated with the text $T:(&lt;$ data $&gt;{D}&lt;$ text $&gt;{T})$. The special tokens $&lt;$ data $&gt;$ and $&lt;$ text $&gt;$ are appended to GPT-2's original vocabulary; their embeddings are learnt during fine-tuning. In addition, we append to the vocabulary the $M R$-dependent special tokens described above. After tokenization, we get a sequence $S$ of subword tokens, which are encoded to point to vocabulary indices: $S=\left(&lt;\right.$ data $\left.&gt;, d_{1}, \ldots d_{k},&lt;\right.$ text $\left.&gt;, t_{1}, \ldots t_{m}\right)=\left(s_{0}, \ldots s_{n}\right)$.</p>
<p>One interesting feature of GPT-2 is its use of Byte-Pair Encoding (BPE) (Sennrich et al., 2016) on bytes instead of unicode characters. Hence, with a modestly-sized subword vocabulary of around 50 K , it can encode any input text and score any output sequence, without suffering from unknown tokens. This is beneficial for our task where named entities are common.</p>
<p>GPT-2 additionally expects positional encodings to help capture the input tokens' order. Our core addition to the model is a third type of input: fine-grained state embeddings. These are analogous to the "Segment Embeddings", introduced in BERT (Devlin et al., 2019) to distinguish between sentence pairs in the next sentence prediction task. However, in our case, the state is defined at a more fine-grained level to give the model a hint on the type of the data being handled. The state vector for $S$ is a vector of tokens with size $|S|$, with each token ID indicating the type of $s_{i}$. Our strategy is to decide the state based on the special tokens we inserted in the data processing stage. We use the following rule: the state token ID of any token $s_{i}$ is the ID of the last special token preceding it (i.e. in the range $\left(s_{0} \ldots s_{i}\right)$ inclusively).</p>
<p>Training: The input embeddings, positional embeddings, and state embeddings are summed together and fed to the first GPT-2 layer. The last GPT-2 layer output is then normalized using "LayerNorm" (Ba et al., 2016) before passing it to a linear layer added on top. The weights of the latter are tied to the input embeddings. Finally, a softmax is applied to the linear layer's output to generate probability distributions of the output tokens. Our training objective is a language modeling one where we aim to find the set of weights $\theta$ that minimize the cross-entropy loss $\ell=\sum_{i=|D|+2}^{|S|} \log P_{\theta}\left(s_{i} \mid s_{0}, \ldots s_{i-1}\right)$.</p>
<p>Note that, since our task is to generate text given the data, the cross-entropy loss is computed for the text following the input data. We mask the data component in the loss above and sum the loss from index $|D|+2$ (i.e., after the $&lt;$ text $&gt;$ token). We use AdamW as an optimizer (Loshchilov and Hutter, 2019).</p>
<h1>4.2 Semantic Fidelity Classifier</h1>
<p>The Semantic Fidelity Classifier (SFC) provides an additional assessment of how accurately the generated text reflects the input data. A text is deemed to possess semantic fidelity if it accurately conveys all of the input data without omitting any nor adding additional data. Our approach draws parallels between this task and natural language inference (NLI) tasks, where the goal is to determine whether a "hypothesis" is true, false, or undetermined given a "premise". Similarly, in semantic fidelity classification, we</p>
<p>aim to determine if the text is "accurate" or contains some "omission", "repetition", "hallucination", or "value errors" given the data. We cast the problem as a sentence-pair classification task for the (Data, Text) pairs, using RoBERTa (Liu et al., 2019) as a base encoder. This formulation has been successfully used for NLI problems before (Devlin et al., 2019).</p>
<p>Training Data Generation: The classifier's training data should consist of semantically faithful and semantically incorrect examples. We generate training data for the SFC automatically from the training data of the main D2T task. We define a set of simple dataset-independent transformations that account for common errors in data-to-text generation. For each tuple $\left(D_{i}, T_{i}\right)$ in the training data, we split the text $T_{i}$ into sentences, using the spaCy sentence tokenizer (Honnibal and Montani, 2017). We then generate a set of new tuples for the SFC consisting of $\left(D_{i}, T_{j}, l\right)$ for each of the labels $l$ below, generated as follows:</p>
<ul>
<li>Accurate: This is the text $T_{i}$.</li>
<li>Omission: Remove the shortest sentence in $T_{i}$ (to help detect subtle omissions).</li>
<li>Repetition: Take a random sentence in $T_{i}$ and insert it before another random sentence in $T_{i}$.</li>
<li>Hallucination: Select a random sentence from another training text $T_{j \neq i}$ and insert it before a random sentence in $T_{i}$.</li>
<li>Value Errors: Select a random value $x$ that occurs verbatim in both $D_{i}$ and $T_{i}$, and replace $x$ in $T_{i}$ with a random other value from $D_{i}$. For slot-based $M R$ (Cleaned E2E and ViGGO), $x$ is selected from the slots' values. For graph-based $M R$ (LDC2017T10), $x$ is selected from the graph's leaves. For RDF triples (WebNLG dataset), $x$ is chosen from the triples' subjects and objects.</li>
</ul>
<p>A related approach, with a different setup and modeling architecture, has been used before in the context of consistency in abstractive summarization (Kryściński et al., 2019). There, weakly supervised models trained on domain-specific data have been shown to outperform supervised models trained on out-of-domain, human-annotated data.</p>
<p>Model Input: As shown in Figure 3, we concatenate the data and text tokens, adding the special start $(<s>)$ and end $(&lt;/ s&gt;)$ tokens used during the training of RoBERTa. In addition to subword token embeddings, we add positional embeddings (representing the token position) and segment embeddings (representing data vs. text types).</p>
<p>Training: The 3 embeddings are summed element-wise to produce the input representation passed to RoBERTa's first encoder layer. Each layer subsequently applies a self-attention followed by a feedforward network. We take the output hidden layer corresponding to the first token $(<s>)$ and pass it through an additional single-layer neural network. The model is trained as a multi-class classifier with a cross-entropy loss as the objective and AdamW as the optimizer.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Semantic fidelity classifier setup</p>
<h1>4.3 Decoder</h1>
<p>Our decoding algorithm for the D2T-LM is based on beam-search. At each decoding step, items are ranked according to the score $R=\frac{1}{\left(i-(|D|+2)\right)^{n}} \prod_{|D|+2}^{i} P\left(s_{i} \mid s_{0} \ldots s_{i-1}\right)$. The score multiplies the conditional probabilities' product with a length normalization factor. Low-scoring candidates are dropped once the number of candidates exceeds the beam size.</p>
<p>Compared to traditional beam search, we do not aggregate probabilities from the start of the sequence, but from the start of the text component (index $|D|+2$ ). The length normalization is also adjusted to only account for the text component. We do this because we fine-tuned the D2T-LM to generate text given</p>
<p>data as context, not to generate the data itself. Hence, we remove the data tokens from the beam-scoring function. In our experiment, we use a value of $\alpha=0.75$. At the end of the beam-search, we use the SFC to rerank the complete candidates (terminated with an end-of-sequence token) in the beam. The reranking metric uses the following binary score: $\mathbb{1}<em i="i">{S F C\left(D</em>$.}, T_{i}\right)=" a c c u r a t e "</p>
<p>Hence, we push the text $T_{i}$ to the top of the beam if our SFC labels the $\left(D_{i}, T_{i}\right)$ tuple as "accurate". We resolve ties using the original D2T-LM scores. An alternative strategy would be to apply reranking at each decoding stage, but we empirically found this strategy to have negligible accuracy gains while requiring a cost that grows with the text size. In addition to helping surface semantically accurate outputs, the SFC labels can be used to assess whether the generated text is usable in practice. In our experiments, we compare this model-based approach to the heuristic approaches commonly used.</p>
<h1>5 Experiments</h1>
<p>For each dataset, we generate outputs from three versions of DATATUNER for our ablation studies. DATATUNER_NO_FC/FS simply relies on the D2T-LM, with no SFC-based reranking and a coarsegrained version of the state embeddings that contains only $&lt;$ data $&gt;$ and $&lt;$ text $&gt;$ tokens (as done by Wolf et al. (2019b)). DATATUNER_NO_FC adds the fine-grained state embeddings described in Section 4.1 to DATATUNER_NO_FC/FS. DATATUNER_FC adds the SFC-based reranking. For the SFC, we train the model using the RoBERTa-large model ( 355 M parameters) on lower-cased text. On the synthetic test set generated, the classifier has a macro-averaged F1-score (across 5 classes) of $97 \%, 97 \%, 98 \%$, and $98 \%$ for the LDC2017T10, WebNLG, Cleaned E2E, and ViGGO datasets respectively. We use the models bundled within the HuggingFace Transformers library (Wolf et al., 2019a). The D2T-LM uses the GPT-2-Medium model (with 345M-parameters) as its base model. The beam search width during decoding is 5. Training was performed on a single machine (Amazon AWS p3.8xlarge). During inference, text from DataTuner is generated at an average rate of 11.8 tokens per second on NVIDIA Tesla K80 GPUs.</p>
<p>We evaluate each variant's outputs with automated metrics and crowdsourced fluency and fidelity evaluation. We also quantify the efficacy of our semantic fidelity classifier with expert-annotations. We compare against the state of the art systems on each dataset, selected based on BLEU scores. These are a graph-optimized Transformer Seq2seq model by Zhu et al. (2019) for LDC2017T10, Transformerbased models (pipeline and E2E) by Castro Ferreira et al. (2019) for WebNLG, an LSTM Seq2seq model with a rule-based reranker by Dušek et al. (2019) for Cleaned E2E, and a Transformer Seq2seq model by Juraska et al. (2019) for ViGGO. The supplementary material contains the outputs from our system variants and the main training hyperparameters (obtained via manual tuning).</p>
<h3>5.1 Automated Evaluation</h3>
<p>For each test set, we compute BLEU (B) (Papineni et al., 2002), which measures n-gram precision, METEOR (M) (Lavie and Agarwal, 2007), which is based on the harmonic mean of the unigram precision and recall while accounting for stem and synonymy matching, ROUGE $_{L}(\mathrm{R})$ (Lin, 2004), which calculates recall for the longest common subsequence, and CIDEr (C) (Vedantam et al., 2015), which is based on TF-IDF scoring of n-grams. We used the official evaluation scripts of the E2E challenge ${ }^{1}$. Table 2 compares the results generated by DATATUNER variants against the state of the art.</p>
<p>Improvements from the D2T-LM alone: Comparing the simple DATATUNER_NO_FC/FS model to the state of the art, we find that it already improves the BLEU score across 2 datasets and the METEOR score across 3 datasets. This indicates that the D2T-LM component of DATATUNER is itself contributing to achieving an end-to-end state-of-the-art system that needs no delexicalization or $M R$-specific encoding.</p>
<p>Fine-grained state embeddings matter: Across the 4 datasets, adding fine-grained state embeddings boosts performance on these metrics, with improvements ranging from 0.3 (on Cleaned E2E) to 2.0 BLEU points (on ViGGO).</p>
<p>SFC effect on automated metrics: Several studies highlight shortcomings of automated metrics for evaluating semantic adequacy (Novikova et al., 2017a; Shimorina, 2018). In this vein, compared to our</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>DataTuner_no_FC model, we observe slight additional boosts from introducing the SFC classifier in the DataTuner_FC variant. Interestingly, DataTuner_FC always has the highest METEOR score, which was the only metric found by Shimorina (2018) to correlate with semantic adequacy.</p>
<p>Largest boost on the most complex text: DATATUNER had the biggest improvement, 5.9 BLEU points, on the LDC2017T10 dataset. This is interesting given that (1) the text in LDC2017T10 is typically long with more complex sentence structures and that (2) the baseline systems targeting AMR-to-text (Zhu et al., 2019; Guo et al., 2019; Ribeiro et al., 2019) built more sophisticated architectures compared to other datasets (e.g. ViGGO and Cleaned E2E). This illustrates our system's ability to work across a spectrum of data representations and text complexity.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">D</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">M</th>
<th style="text-align: center;">R</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><img alt="img-2.jpeg" src="img-2.jpeg" /></td>
<td style="text-align: center;">DataTuner_FC</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC/FS</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zhu et al. (2019)</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Guo et al. (2019)</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ribeiro et al. (2019)</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_FC</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC/FS</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Castro Ferreira et al. (2019) Pipe.</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Castro Ferreira et al. (2019) E2E</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Moryossef et al. (2019b) Pipe.</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_FC</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC/FS</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dušek et al. (2019) (TGen+)</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_FC</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DataTuner_no_FC/FS</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">2.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Juraska et al. (2019)</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">2.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation with automated metrics.</p>
<h1>5.2 Human Evaluation of Fluency</h1>
<p>We conduct human evaluation of fluency for 150 examples sampled at random from each dataset. We use Amazon's MTurk to ask crowd workers how fluent a text is on a 7-point Likert scale using sliders, where "high fluency" is defined as "grammatical, natural, and could have been produced by a skilled native speaker". Following findings from Novikova et al. (2018) and Van Der Lee et al. (2019) for acquiring more consistent human ratings, texts generated for the same meaning representation by different systems are presented together in a single task for annotators to score them relative to each other. We include the human-written text, and randomize the texts' order. For fair comparison, we lower-case our generated texts for the LDC2017T10 to match the outputs of Zhu et al. (2019). We also detokenize outputs from that work to avoid these biasing the workers. We restrict to US-based annotators who completed $&gt;500$ tasks, out of which more than $97 \%$ had been accepted.</p>
<p>Improvement on the state of the art: As shown in the column "Flu." of Table 3, compared to the human baseline, our DataTuner_FC model improves the fluency on all four datasets compared to the state of the art systems with statistically significant margins ( $p&lt;0.05$ ). For computing significance, we use the pairwise Wilcoxon signed-rank test (Wilcoxon, 1992) with the null hypothesis that the fluency values for each pair of systems come from the same distribution. For LDC2017T10, where DataTuner_FC had the largest gap in BLEU score ( +5.9 ), we observe the widest fluency improvement $(+0.82)$ compared to Zhu et al. (2019). Interestingly, despite the fact that DataTuner_FC scored 0.7 higher on BLEU compared to the pipeline approach in (Castro Ferreira et al., 2019) for WebNLG, the difference in fluency is 0.69 , which is relatively large. We conjecture that this originates from two main sources. First, semantic errors might be perceived by annotators as breaking the fluency. For example, one text contained the phrase "has a runway length of Shehbaz Sharif". Second, the pipeline approach had a sizeable portion of non-realized outputs (e.g. "PATIENT-1 is made with PATIENT-1 and PATIENT-2."), which were annotated as non-fluent. On the closed-domain datasets (ViGGO and Cleaned E2E), the fluency margins are smaller while still statistically significant. This is expected as these datasets have a narrow set of sentence formulations that are easier to learn.</p>
<p>Improvement on the human baseline: Surprisingly, we find that DATATUNER_FC received a higher overall average fluency score on 3 datasets compared to the human baseline. This difference is statistically significant in both Cleaned E2E and ViGGO, with the largest difference being 1.04 points in Cleaned E2E. Investigating, we found several low-scored texts had an informal style and problems in sentence construction. One example contained "It serves Chinese food for less." One explanation could be that, once fine-tuned on a large enough dataset, our models have less tendency to deviate from common formulations that annotators prefer.</p>
<h1>5.3 Crowdsourced Evaluation of Fidelity</h1>
<p>We take a two-step approach for evaluating semantic fidelity with both crowdworkers and expert annotators. We start with a crowdsourcing experiment involving the same 150 randomly sampled examples from each dataset used in the fluency evaluation. We use Amazon's MTurk (with the same restrictions on annotators) and present texts for the same $M R$ together. To avoid requiring non-expert annotators to understand the $M R$, we present a human reference text against which to compare the system outputs. We ask annotators to make a choice whether each text is "accurate" or "inaccurate", i.e. whether it "conveys all the factual information in the original text" without any "information missing, added or repeated". We ask annotators to ignore grammar quality or style differences, provided that the overall meaning is the same. Three annotators complete each task, and we take the mode result for each text, assuming "inaccurate" in the event of a tie.</p>
<p>DATATUNER has higher semantic accuracy in human evaluation The results presented in the "Fid." column of Table 3 show that DATATUNER_FC has a superior accuracy to all the other variants as well as the state of the art models. The differences to the state of the art models range from $5.3 \%$ (ViGGO) to $40 \%$ (WebNLG) and are statistically significant on WebNLG, LDC2017T10, and Cleaned E2E ( $p&lt;$ 0.05 , as measured by McNemar's test (McNemar, 1947)). The trend among the DATATUNER variants also shows a clear impact of both the fine-grained state embeddings and the SFC on boosting the overall accuracy of the generated text.</p>
<h3>5.4 Expert Evaluation of Fidelity</h3>
<p>Next, we assess the semantic fidelity with experts' annotations. We have two goals in this section: (1) comparing our model-based approach to heuristic-based approaches as automated methods of judging semantic accuracy, and (2) using this comparison outcome to illustrate that DATATUNER delivers higher semantic accuracy as measured by the better fidelity metric.</p>
<p>The baseline method uses heuristics to label each data-text tuple as accurate $\left(A_{H}\right)$ or erroneous $\left(E_{H}\right)$. For this, we use the heuristics by Shimorina and Gardent (2018) for WebNLG, by Juraska et al. (2018) for ViGGO, and by Dušek et al. (2019) for Cleaned E2E. We are not aware of heuristic-based scripts for LDC2017T10. We compute Heuristic Semantic Accuracy (HSA) of a dataset as the fraction with the label $A_{H}$. Our method uses the SFC component in DATATUNER to assign accurate $\left(A_{D}\right)$ or erroneous $\left(E_{D}\right)$ labels to each data-text tuple. We compute DATATUNER Semantic Accuracy (DSA) as the fraction with the label $A_{D}$. Both metrics are computed per system across each dataset.</p>
<p>To compare the quality of HSA and DSA as measures of semantic accuracy, we manually annotated a sample of data-text tuples. Since the vast majority of texts are expected to be accurate, especially on the cleaner datasets, we designed a sampling methodology to give a balanced representation of semantically accurate and inaccurate texts. To start, we sample 4 indices from the target dataset such that the human baseline outputs for these indices are labeled as: $\left{\left(A_{H}, E_{D}\right),\left(E_{H}, A_{D}\right),\left(E_{H}, E_{D}\right),\left(A_{H}, A_{D}\right)\right}$. We do the same with the state of art system and DATATUNER_FC outputs. We continue in a round-robin fashion until we get 24 indices per dataset. For LDC2017T10 dataset, we sample 24 indices in a similar fashion while ignoring the $A_{H}$ and $E_{H}$ labels. Next, two authors were presented with the input meaning representation and the output texts generated by each system (in a random order) for the 24 sampled entries. The authors manually labeled the resulting 480 data-text tuples as accurate $\left(A_{M}\right)$ or erroneous $\left(E_{M}\right)$. Inter-annotator agreement measured with Cohen's Kappa was 0.81 , indicating near-perfect agreement. We use these labels to assess the quality $\mathrm{Q}<em M="M">{D}$ of the DSA metric as the percentage of cases where the manual label $A</em>}$ matches $A_{D}$. Similarly, we evaluate the quality $\mathrm{Q<em M="M">{H}$ of the HSA metric as the percentage of cases where $A</em>$. These percentages are aggregated across systems, obtaining 120 samples per dataset. We present these metrics in Table 3.}$ matches $A_{H</p>
<p>DSA provides higher quality semantic evaluation: We notice that $\mathrm{Q}<em H="H">{D}$ is $4.2 \%$ higher on ViGGO and $14.2 \%$ higher on both Cleaned E2E and WebNLG, compared to $\mathrm{Q}</em>$. These differences are statistically significant $(p&lt;0.05)$ on WebNLG and Cleaned E2E, as measured by McNemar's test (McNemar, 1947) with the null hypothesis that the marginal probability for each outcome (accurate or erroneous) is the same for both algorithms.</p>
<p>HSA struggles with open domains: The heuristic-based approach labeled only $41.2 \%$ of the human references in WebNLG as accurate, $16.9 \%$ lower than the score it assigned to our DATATUNER_FC. Since the latter was trained on human references, this difference is more likely to stem from shortcomings of this approach for assessing the semantics. Checking the data, we observed that humans tend to create more diverse formulations, such as converting United Kingdom to $U K$, which are easy to miss with heuristics. On the contrary, our DSA metric scored the human references higher.</p>
<p>DataTuner_FC delivers higher semantic accuracy: Now that we have established that DSA is a better measure of semantic accuracy compared to HSA, we can see from Table 3 that, across all datasets, DATATUNER_FC significantly improves over the state of the art models as measured by the DSA metric (McNemar's test gives $p&lt;0.05$ ). Compared to other DATATUNER variants, DATATUNER_FC adds between $0.3 \%$ and $11.3 \%$ improvements, corroborating the utility of the semantic fidelity classifier. Finally, we note that, since the baseline models for Cleaned E2E and ViGGO use the heuristics for reranking their outputs, they are expected to show higher HSA. However, what our manual annotations prove is that the HSA metric itself is of lower quality compared to the DSA metric.</p>
<h1>6 Conclusion</h1>
<p>We presented DATATUNER, an end-to-end data-to-text generation system equipped with a semantic fidelity classifier. DATATUNER records new state of the art results on four different datasets, with significant margins on automated metrics. We also show that our system has a clear fluency advantage over all the previous state of the art models. We further illustrate DATATUNER's strengths for delivering semantically accurate outputs.</p>
<h1>References</h1>
<p>Shubham Agarwal, Marc Dymetman, and Éric Gaussier. 2018. Char2char generation with reranking for the E2E NLG challenge. In Proceedings of the 11th International Conference on Natural Language Generation, pages 451-456, Tilburg University, The Netherlands, November. Association for Computational Linguistics.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Thiago Castro Ferreira, Diego Moussallem, Emiel Krahmer, and Sander Wubben. 2018. Enriching the WebNLG corpus. In Proceedings of the 11th International Conference on Natural Language Generation, pages 171-176, Tilburg University, The Netherlands, November. Association for Computational Linguistics.</p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 552-562, Hong Kong, China, November. Association for Computational Linguistics.</p>
<p>Andrew Chisholm, Will Radford, and Ben Hachey. 2017. Learning to generate one-sentence biographies from Wikidata. 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference, 1:633-642.</p>
<p>Edgar Dale and Jeanne S Chall. 1948. A formula for predicting readability: Instructions. Educational research bulletin, pages 37-54.</p>
<p>Marco Damonte and Shay B. Cohen. 2019. Structural neural encoders for AMR-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649-3658, Minneapolis, Minnesota, June. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.</p>
<p>Ondřej Dušek, David M. Howcroft, and Verena Rieser. 2019. Semantic Noise Matters for Neural Natural Language Generation. In Proceedings of the 12th International Conference on Natural Language Generation.</p>
<p>Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge. Computer Speech \&amp; Language, 59:123-156.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179-188. Association for Computational Linguistics.</p>
<p>Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research, 61:65-170.</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computational Linguistics, 7:297-312, March.</p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear.</p>
<p>Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, and Marilyn Walker. 2018. A deep ensemble model with slot alignment for sequence-to-sequence natural language generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 152-162, New Orleans, Louisiana, June. Association for Computational Linguistics.</p>
<p>Juraj Juraska, Kevin K Bowden, and Marilyn Walker. 2019. Viggo: A video game corpus for data-to-text generation in open-domain conversation. In Proceedings of the 12th International Conference on Natural Language Generation.</p>
<p>Chris Kedzie and Kathleen McKeown. 2019. A good sample is hard to find: Noise injection sampling and selftraining for neural language generation models. In Proceedings of the 12th International Conference on Natural Language Generation, pages 584-593, Tokyo, Japan, October-November. Association for Computational Linguistics.</p>
<p>Kevin Knight, Laura Baranescu Bianca Badarau, Claire Bonial, Madalina Bardocz, Kira Griffitt, Ulf Hermjakob, Daniel Marcu, Martha Palmer, Tim O'Gorman, and Nathan Schneider. 2017. Abstract meaning representation (AMR) annotation release 2.0 LDC2017T10. Web Download. Philadelphia: Linguistic Data Consortium.</p>
<p>Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Evaluating the factual consistency of abstractive text summarization. arXiv preprint arXiv:1910.12840.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228-231. Association for Computational Linguistics.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas, November. Association for Computational Linguistics.</p>
<p>Leo Leppänen, Myriam Munezero, Mark Granroth-Wilding, and Hannu Toivonen. 2017. Data-driven news generation for automated journalism. In Proceedings of the 10th International Conference on Natural Language Generation, pages 188-197.</p>
<p>Percy Liang, Michael Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 91-99, Suntec, Singapore, August. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages $74-81$.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.</p>
<p>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153-157.</p>
<p>Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein, Maarten Van Segbroeck, Kenji Sagae, Panayiotis Georgiou, David R Traum, and Shri Narayanan. 2012. A reranking approach for recognition and classification of speech input in conversational dialogue systems. In 2012 IEEE Spoken Language Technology Workshop (SLT), pages 49-54. IEEE.</p>
<p>Amit Moryossef, Ido Dagan, and Yoav Goldberg. 2019a. Improving quality and efficiency in plan-based neural data-to-text generation. In Proceedings of the 12th International Conference on Natural Language Generation.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019b. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277, Minneapolis, Minnesota, June. Association for Computational Linguistics.</p>
<p>Feng Nie, Jinpeng Wang, Jin-Ge Yao, Rong Pan, and Chin-Yew Lin. 2018. Operation-guided neural networks for high fidelity data-to-text generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3879-3889, Brussels, Belgium, October-November. Association for Computational Linguistics.</p>
<p>Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. 2019. A simple recipe towards reducing hallucination in neural surface realisation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2673-2679, Florence, Italy, July. Association for Computational Linguistics.</p>
<p>Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, and Verena Rieser. 2017a. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241-2252, Copenhagen, Denmark, September. Association for Computational Linguistics.</p>
<p>Jekaterina Novikova, Ondřej Dušek, and Verena Rieser. 2017b. The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254.</p>
<p>Jekaterina Novikova, Ondrej Dušek, and Verena Rieser. 2018. RankME: Reliable human ratings for natural language generation. In Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 72-78, New Orleans, Louisiana.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.</p>
<p>Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. 2019. Making effective use of healthcare data using data-to-text technology. In Data Science for Healthcare, pages 119-145. Springer.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.
Leonardo FR Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing amr-to-text generation with dual graph representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), November.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany, August. Association for Computational Linguistics.</p>
<p>Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 177-184.</p>
<p>Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. 2019. Pragmatically informative text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4060-4067, Minneapolis, Minnesota, June. Association for Computational Linguistics.</p>
<p>Anastasia Shimorina and Claire Gardent. 2018. Handling rare items in data-to-text generation. INLG 2018, page 360 .</p>
<p>Anastasia Shimorina. 2018. Human vs automatic metrics: on the importance of correlation design. arXiv preprint arXiv:1805.11474.</p>
<p>Charese Smiley, Frank Schilder, Vassilis Plachouras, and Jochen L. Leidner. 2017. Say the right thing right: Ethics issues in natural language generation systems. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 103-108, Valencia, Spain, April. Association for Computational Linguistics.</p>
<p>Chris Van Der Lee, Albert Gatt, Emiel Van Miltenburg, Sander Wubben, and Emiel Krahmer. 2019. Best practices for the human evaluation of automatically generated text. In Proceedings of the 12th International Conference on Natural Language Generation (INLG'19), Tokyo, Japan. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett, editors, Advances in Neural Information Processing Systems 30, pages 59986008. Curran Associates, Inc.</p>
<p>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45664575 .</p>
<p>Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. 2015. Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking. arXiv preprint arXiv:1508.01755.</p>
<p>Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Breakthroughs in statistics, pages 196202. Springer.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019a. Huggingface's transformers: State-of-theart natural language processing. ArXiv, abs/1910.03771.</p>
<p>Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. 2019b. Transfertransfo: A transfer learning approach for neural network based conversational agents. CoRR, abs/1901.08149.</p>
<p>Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5458-5467, Hong Kong, China, November. Association for Computational Linguistics.</p>
<h1>Appendix A. Examples of Generated Text</h1>
<p>In Table 4, we present examples of the outputs generated by DATATUNER and the state of the art models, alongside the human references.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">D</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">pub in riverside that also serves Italian food: The Vaults has got high prices, is not child-friendly and has an average rating. It is near Rainbow Vegetarian Café in riverside.</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{1}{2}$</td>
<td style="text-align: center;">DATATUNER.FC</td>
<td style="text-align: center;">The Vaults is an Italian pub in the riverside area near Rainbow Vegetarian Café. It has an average customer rating and a high price range. It is not child friendly.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dušek et al. (2019)</td>
<td style="text-align: center;">The Vaults is an italian pub with an average customer rating. It is located in the riverside area near Rainbow Vegetarian Café. It is not children friendly and has a high price range.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Adirondack regional airport serves both Lake Placid and Saranac Lake, New York. The length of the runway at Asirondack regional airport is 2003.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DATATUNER.FC</td>
<td style="text-align: center;">Adirondack Regional Airport serves the cities of Lake Placid and Saranac Lake, New York and has a runway length of 2003.0.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Castro Ferreira et al. (2019)</td>
<td style="text-align: center;">Adirondack Regional Airport serves the city of Lake Placid and Saranac Lake, New York and has a runway length of Shehbaz Sharif.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">the plan requires 8 precautionary steps before the order to shoot down a plane may be issued.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DATATUNER.FC</td>
<td style="text-align: center;">the plan requires eight precautionary steps before the order to shoot down the plane can be issued.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zhu et al. (2019)</td>
<td style="text-align: center;">the plan required 8 precaution steps before it can be issued to order shot down.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Guitar Hero: Smash Hits was a very bad game. 2009 was a terrible year for gaming and I just can't stand the games released that year.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DATATUNER.FC</td>
<td style="text-align: center;">Guitar Hero: Smash Hits is a really bad game. 2009 was a really bad year for games</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Juraska et al. (2019)</td>
<td style="text-align: center;">Guitar Hero: Smash Hits is a very bad game, especially because it came out in 2009.</td>
</tr>
</tbody>
</table>
<p>Table 4: Examples of text generated by the different models</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/tuetschek/e2e-metrics&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
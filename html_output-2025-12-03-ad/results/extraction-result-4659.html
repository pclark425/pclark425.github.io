<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-263830046</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.03903v2.pdf" target="_blank">LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4659.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4659.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Agent (LLM-Coordination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Agentic Framework used in LLM-Coordination Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-centered agent architecture (Memory + Reasoning + Grounding) used to play multi-turn pure-coordination games (Hanabi, Overcooked-AI, CollabCapture, CollabEscape). Memory is split into Long-Term (game description / rules), Working Memory (textual current observation/state), and Episodic Memory (list of prior actions); the LLM consumes these memories as prompt context to produce actions, with additional auxiliary steps (Answer-Verification and explicit ToM reasoning) layered on for some games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM Agent (Cognitive-architecture scaffold)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A scaffolded language-agent following Cognitive Architectures for Language Agents: (1) Memory (Long-Term procedural/game description; Working memory textual state; Episodic action history), (2) Reasoning (LLM invoked to generate next action from available actions, with optional ToM reasoning and Answer-Verification steps), (3) Grounding (maps high-level natural-language actions to game-specific low-level actions and filters infeasible actions). The agent repeatedly forms a prompt consisting of the relevant memories and state and queries the LLM to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4-turbo, GPT-4o, GPT-3.5-turbo, Mixtral 8x7B (evaluated variants)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>LLM-Coordination Benchmark (Agentic Coordination on Hanabi, Overcooked-AI, CollabCapture, CollabEscape)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-turn pure-coordination gameplay: agents must cooperate to maximize common payoff. Tasks include choosing the best next action given the current state and partner behavior (Overcooked: cook & deliver soups; Hanabi: play/give hints/discard to build stacks; CollabCapture/Escape: capture or escape adversary via coordinated movement and door control). Also includes single-turn CoordinationQA for Environment Comprehension, Theory of Mind, and Joint Planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Three-part memory: Long-Term Memory (procedural/game description and conventions), Working Memory (textual encoding of current observation/state), Episodic Memory (list of previous actions). The memory is used as prompt-context for the LLM; auxiliary ToM reasoning and Answer-Verification steps use and depend on stored information.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>All memory is stored as structured natural-language text: game rules and conventions (long-term), processed and precomputed state descriptions (distances, cooker contents, inventories) in textual form (working), and explicit action-history entries (episodic). Also precomputed helpful facts (e.g., next card for each stack in Hanabi) are placed into the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Working memory is updated every turn with the latest processed state description; Episodic memory is appended after each selected action (action history is accumulated across the episode); Long-term memory (procedural/game description and conventions) is static during episodes but included in the prompt each turn.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Memory is concatenated into the LLM prompt (prompt-context retrieval). There is no separate retrieval engine or embedding-based index reported; the agent provides requested memory (long-term + working + episodic) as textual context each decision. ToM reasoning is implemented as an intermediate LLM pass that interprets partner actions/intentions based on the available memory context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported at multiple points as the standard agent configuration (which includes the memory scaffold and auxiliary steps): Overcooked-AI (Table 1): GPT-4-turbo scores per layout reported (e.g., shown as 173.3 ± 6.67, 260.0 ± 11.55, 140.0 ± 0.00, 180.0 ± 11.55, 160.0 ± 0.00 across layouts — see Table 1 for layout mapping); Hanabi (Table 3): GPT-4-turbo achieved 13.33 ± 0.88 points (with ToM + Verification); CollabCapture (Table 2): GPT-4-turbo capture rate 0.83, avg turns 4.6; CollabEscape (Table 2): GPT-4-turbo escape rate 1.00, avg turns 3.5. Zero-shot cross-play: GPT-4-turbo matches or outperforms RL baselines in Overcooked and remains robust when paired with unseen partners (e.g., pairing with OBL agents in Hanabi yields no degradation for GPT-4-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No direct ablation that removes the entire memory scaffold is reported. However, related ablations on auxiliary memory-dependent reasoning steps show degradation: Hanabi scores drop from 13.33 ± 0.88 (with ToM & Verification) to 10.33 ± 0.88 when ToM reasoning is removed, and further to 4.33 ± 0.88 when both ToM reasoning and Answer-Verification are removed (Table 3). In CollabCapture/CollabEscape, removing ToM reasoning for GPT-4-turbo reduced capture rate from 0.83 to 0.50 and changed avg-turns; CollabEscape escape rate remained 1.00 but avg turns increased (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations focus on reasoning steps that rely on memory/context: (1) Answer-Verification (re-prompting to verify an action) — removing it causes catastrophic failures in Hanabi (agents 'bomb' before end of game in every trial), (2) ToM reasoning step (explicit intermediate inference of partner beliefs/intentions) — removing it reduces performance in Hanabi (13.33 → 10.33) and substantially impacts CollabCapture metrics (capture rate drop 0.83 → 0.50); Overcooked did not use ToM routinely (marginal benefits vs cost). Correlation analysis ties Environment Comprehension and ToM CoordinationQA scores to agent performance across games.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors report: (a) High latency and compute for large LLMs (GPT-4-turbo) makes running memory-rich reasoning expensive and unsuitable for real-time settings; (b) manual prompt/procedural memory configuration is required and sensitive — assembling long-term game descriptions and formatting them is labor-intensive and affects performance; (c) scaling memory / CoordinationQA is limited by manual curation; (d) LLMs struggle in tasks requiring fine-grained Theory-of-Mind and joint planning despite memory provision — Joint Planning accuracy remains low (<40% even for best LLM); (e) LLMs have difficulty with raw spatial/grid data, so authors preprocess and supply distances — indicating limitations in direct memory representation for spatial info; (f) no external, persistent retrieval/indexing system reported — memory is passed in-context which may not scale to very long histories or many games.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>From paper findings and implementation notes: (1) Use a three-part memory scaffold (procedural long-term game description; per-turn working-memory textual state; episodic action history) and include it in the prompt each decision; (2) Add Answer-Verification to reduce hallucinations and fatal mistakes (especially in Hanabi); (3) Add an explicit ToM-reasoning intermediate step for games where partner beliefs/intentions matter (Hanabi, CollabEscape) — this separates partner-interpretation from action selection and improves coordination; (4) Precompute and include key summarized facts (e.g., next required card in each Hanabi stack, distances to important locations) to avoid counting errors and to accommodate LLM weaknesses with raw spatial/state data; (5) Use a Grounding module to translate high-level natural-language actions into low-level game actions and to filter infeasible actions before execution; (6) Acknowledge compute/latency trade-offs — avoid adding costly ToM steps for fully observable environment-focused tasks (e.g., Overcooked) where marginal benefits are small.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_references</strong></td>
                            <td>Table 3 (Hanabi ablations: removal of ToM and Verification), Table 2 (CollabCapture/CollabEscape comparisons with and without ToM reasoning), correlation analyses connecting CoordinationQA subtask performance to agentic success (Figure 2 and related text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Proagent: Building proactive cooperative ai with large language models <em>(Rating: 2)</em></li>
                <li>Building cooperative embodied agents modularly with large language models <em>(Rating: 2)</em></li>
                <li>Theory of mind for multi-agent collaboration via large language models <em>(Rating: 2)</em></li>
                <li>Cooperation on the fly: Exploring language agents for ad hoc teamwork in the avalon game <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4659",
    "paper_id": "paper-263830046",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "LLM-Agent (LLM-Coordination)",
            "name_full": "LLM-based Agentic Framework used in LLM-Coordination Benchmark",
            "brief_description": "An LLM-centered agent architecture (Memory + Reasoning + Grounding) used to play multi-turn pure-coordination games (Hanabi, Overcooked-AI, CollabCapture, CollabEscape). Memory is split into Long-Term (game description / rules), Working Memory (textual current observation/state), and Episodic Memory (list of prior actions); the LLM consumes these memories as prompt context to produce actions, with additional auxiliary steps (Answer-Verification and explicit ToM reasoning) layered on for some games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM Agent (Cognitive-architecture scaffold)",
            "agent_description": "A scaffolded language-agent following Cognitive Architectures for Language Agents: (1) Memory (Long-Term procedural/game description; Working memory textual state; Episodic action history), (2) Reasoning (LLM invoked to generate next action from available actions, with optional ToM reasoning and Answer-Verification steps), (3) Grounding (maps high-level natural-language actions to game-specific low-level actions and filters infeasible actions). The agent repeatedly forms a prompt consisting of the relevant memories and state and queries the LLM to select actions.",
            "llm_model_name": "GPT-4-turbo, GPT-4o, GPT-3.5-turbo, Mixtral 8x7B (evaluated variants)",
            "game_or_benchmark_name": "LLM-Coordination Benchmark (Agentic Coordination on Hanabi, Overcooked-AI, CollabCapture, CollabEscape)",
            "task_description": "Multi-turn pure-coordination gameplay: agents must cooperate to maximize common payoff. Tasks include choosing the best next action given the current state and partner behavior (Overcooked: cook & deliver soups; Hanabi: play/give hints/discard to build stacks; CollabCapture/Escape: capture or escape adversary via coordinated movement and door control). Also includes single-turn CoordinationQA for Environment Comprehension, Theory of Mind, and Joint Planning.",
            "memory_used": true,
            "memory_type": "Three-part memory: Long-Term Memory (procedural/game description and conventions), Working Memory (textual encoding of current observation/state), Episodic Memory (list of previous actions). The memory is used as prompt-context for the LLM; auxiliary ToM reasoning and Answer-Verification steps use and depend on stored information.",
            "memory_representation": "All memory is stored as structured natural-language text: game rules and conventions (long-term), processed and precomputed state descriptions (distances, cooker contents, inventories) in textual form (working), and explicit action-history entries (episodic). Also precomputed helpful facts (e.g., next card for each stack in Hanabi) are placed into the prompt.",
            "memory_update_mechanism": "Working memory is updated every turn with the latest processed state description; Episodic memory is appended after each selected action (action history is accumulated across the episode); Long-term memory (procedural/game description and conventions) is static during episodes but included in the prompt each turn.",
            "memory_retrieval_mechanism": "Memory is concatenated into the LLM prompt (prompt-context retrieval). There is no separate retrieval engine or embedding-based index reported; the agent provides requested memory (long-term + working + episodic) as textual context each decision. ToM reasoning is implemented as an intermediate LLM pass that interprets partner actions/intentions based on the available memory context.",
            "performance_with_memory": "Reported at multiple points as the standard agent configuration (which includes the memory scaffold and auxiliary steps): Overcooked-AI (Table 1): GPT-4-turbo scores per layout reported (e.g., shown as 173.3 ± 6.67, 260.0 ± 11.55, 140.0 ± 0.00, 180.0 ± 11.55, 160.0 ± 0.00 across layouts — see Table 1 for layout mapping); Hanabi (Table 3): GPT-4-turbo achieved 13.33 ± 0.88 points (with ToM + Verification); CollabCapture (Table 2): GPT-4-turbo capture rate 0.83, avg turns 4.6; CollabEscape (Table 2): GPT-4-turbo escape rate 1.00, avg turns 3.5. Zero-shot cross-play: GPT-4-turbo matches or outperforms RL baselines in Overcooked and remains robust when paired with unseen partners (e.g., pairing with OBL agents in Hanabi yields no degradation for GPT-4-turbo).",
            "performance_without_memory": "No direct ablation that removes the entire memory scaffold is reported. However, related ablations on auxiliary memory-dependent reasoning steps show degradation: Hanabi scores drop from 13.33 ± 0.88 (with ToM & Verification) to 10.33 ± 0.88 when ToM reasoning is removed, and further to 4.33 ± 0.88 when both ToM reasoning and Answer-Verification are removed (Table 3). In CollabCapture/CollabEscape, removing ToM reasoning for GPT-4-turbo reduced capture rate from 0.83 to 0.50 and changed avg-turns; CollabEscape escape rate remained 1.00 but avg turns increased (see Table 2).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "Ablations focus on reasoning steps that rely on memory/context: (1) Answer-Verification (re-prompting to verify an action) — removing it causes catastrophic failures in Hanabi (agents 'bomb' before end of game in every trial), (2) ToM reasoning step (explicit intermediate inference of partner beliefs/intentions) — removing it reduces performance in Hanabi (13.33 → 10.33) and substantially impacts CollabCapture metrics (capture rate drop 0.83 → 0.50); Overcooked did not use ToM routinely (marginal benefits vs cost). Correlation analysis ties Environment Comprehension and ToM CoordinationQA scores to agent performance across games.",
            "challenges_or_limitations": "Authors report: (a) High latency and compute for large LLMs (GPT-4-turbo) makes running memory-rich reasoning expensive and unsuitable for real-time settings; (b) manual prompt/procedural memory configuration is required and sensitive — assembling long-term game descriptions and formatting them is labor-intensive and affects performance; (c) scaling memory / CoordinationQA is limited by manual curation; (d) LLMs struggle in tasks requiring fine-grained Theory-of-Mind and joint planning despite memory provision — Joint Planning accuracy remains low (&lt;40% even for best LLM); (e) LLMs have difficulty with raw spatial/grid data, so authors preprocess and supply distances — indicating limitations in direct memory representation for spatial info; (f) no external, persistent retrieval/indexing system reported — memory is passed in-context which may not scale to very long histories or many games.",
            "best_practices_or_recommendations": "From paper findings and implementation notes: (1) Use a three-part memory scaffold (procedural long-term game description; per-turn working-memory textual state; episodic action history) and include it in the prompt each decision; (2) Add Answer-Verification to reduce hallucinations and fatal mistakes (especially in Hanabi); (3) Add an explicit ToM-reasoning intermediate step for games where partner beliefs/intentions matter (Hanabi, CollabEscape) — this separates partner-interpretation from action selection and improves coordination; (4) Precompute and include key summarized facts (e.g., next required card in each Hanabi stack, distances to important locations) to avoid counting errors and to accommodate LLM weaknesses with raw spatial/state data; (5) Use a Grounding module to translate high-level natural-language actions into low-level game actions and to filter infeasible actions before execution; (6) Acknowledge compute/latency trade-offs — avoid adding costly ToM steps for fully observable environment-focused tasks (e.g., Overcooked) where marginal benefits are small.",
            "ablation_or_analysis_references": "Table 3 (Hanabi ablations: removal of ToM and Verification), Table 2 (CollabCapture/CollabEscape comparisons with and without ToM reasoning), correlation analyses connecting CoordinationQA subtask performance to agentic success (Figure 2 and related text).",
            "uuid": "e4659.0",
            "source_info": {
                "paper_title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Proagent: Building proactive cooperative ai with large language models",
            "rating": 2,
            "sanitized_title": "proagent_building_proactive_cooperative_ai_with_large_language_models"
        },
        {
            "paper_title": "Building cooperative embodied agents modularly with large language models",
            "rating": 2,
            "sanitized_title": "building_cooperative_embodied_agents_modularly_with_large_language_models"
        },
        {
            "paper_title": "Theory of mind for multi-agent collaboration via large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_for_multiagent_collaboration_via_large_language_models"
        },
        {
            "paper_title": "Cooperation on the fly: Exploring language agents for ad hoc teamwork in the avalon game",
            "rating": 1,
            "sanitized_title": "cooperation_on_the_fly_exploring_language_agents_for_ad_hoc_teamwork_in_the_avalon_game"
        }
    ],
    "cost": 0.0123115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models
28 Apr 2025</p>
<p>Saaket Agashe saagashe@ucsc.edu 
University of California
Santa Cruz</p>
<p>Yue Fan 
University of California
Santa Cruz</p>
<p>XinAnthony Reyna ancreyna@ucsc.edu 
University of California
Santa Cruz</p>
<p>Eric Wang 
University of California
Santa Cruz</p>
<p>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models
28 Apr 20253D9B5584B7AB50590CD737B60C702E50arXiv:2310.03903v3[cs.CL]
Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents.This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains.Our benchmark evaluates LLMs through two distinct tasks.The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games.The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiplechoice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning.Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions.The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities.Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners.These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement.Code Available at https://github.com/eric-ai-lab/llm_coordination.</p>
<p>Introduction</p>
<p>In a wide range of activities, from daily tasks such as cooking to critical operations like rescue efforts, cooperation without mixed intentions is essential.These scenarios are examples of Pure Coordination Games, where all involved parties benefit from choosing strategies that are perfectly aligned, avoiding any conflict of interest.These games require agents to reason about their environment and plan while considering the beliefs and intentions of their partners.Recently, Large Language Models (LLMs) have demonstrated emergent planning abilities in both physical and virtual settings (Raman et al., 2022;Wang et al., 2023a;Wu et al., 2023), impressive reasoning capabilities (Wei et al., 2022), and the hints of a Theory of Mind (Kosinski, 2023) making them promising candidates for developing coordination agents.Previous works have explored the use of LLMs for developing collaborative agents, yet the requisite conditions, strengths, and limitations of LLMs in coordination games remain unclear.In this study, we intend to bridge the gap by performing a comprehensive evaluation and analysis of the multi-agent coordination abilities of LLMs.</p>
<p>Therefore, we introduce the LLM-Coordination Benchmark featuring two task settings for pure coordination games: 1. Agentic Coordination and 2. CoordinationQA.In Agentic Coordination, LLMs are scaffolded with components that allow them to act within actual game environments, providing a holistic evaluation of the competencies of LLMs to act as coordination agents.In CoordinationQA, LLMs have to answer a curated set of questions about edge-case scenarios drawn from coordination games where agents need to actively cooperate with their partners.The benchmark includes four collaborative games, providing a comprehensive analysis platform.Unlike studies on multi-LLM frameworks (Hong et al., 2023;Qian et al., 2024;Li et al., 2023a), which focus on orchestrating multiple LLMs to solve tasks, our benchmark assesses the innate ability of individual LLMs to understand and act within pure coordination scenarios where cooperation is essential.</p>
<p>Our experiments in the Agentic Coordination setting reveal that Large Language Models are competent at understanding the game objectives, generating coherent reasoning for their next actions, and coordinating with partners across all coordination games.They exhibit these behaviors without any training, fine-tuning, or few-shot examples.A comparative analysis reveals that LLM agents match or outperform RL baselines in games where optimal decision-making can be done by observing environment variables and positions (e.g., Overcooked).However, they struggle in settings where agents need to actively consider their partner's beliefs and intentions (e.g., Hanabi).We also observe that LLM agents are capable of collaborating with new partners, unlike self-play MARL methods (Carroll et al., 2019a;Bard et al., 2020) that fail to adapt to unseen agents.</p>
<p>For a more nuanced analysis of the coordination abilities of LLMs, we create the CoordinationQA Suite.This suite is designed to dissect the capabilities of LLMs in single-turn reasoning within coordination games, focusing on three key areas: Joint Planning, Theory of Mind (ToM), and Environment Comprehension.Joint Planning (JP) evaluates LLMs' planning abilities for optimal coordination, ToM questions probe their understanding of partner agents' intentions and needs, and Environment Comprehension (EC) assesses their infer-ence of environment details, rules and objectives.First, Our findings on CoordinationQA show a marked performance gap between GPT-4-turbo and other LLMs across three question types.Secondly, LLMs are most proficient in Environment Comprehension, indicating they understand the rules and environment states well.However, they face significant challenges in Theory of Mind Reasoning, with difficulty inferring others' intentions and needs.This issue worsens in Joint Planning, where most LLMs underperform, some even worse than random choices.These results highlight LLMs' limited reliability and effectiveness as coordination partners.Correlation analysis between LLMs' performance on CoordinationQA and their performance on agentic coordination setting further highlights their strengths in environmental reasoning but exposes significant weaknesses in Theory of Mind inference and Joint Planning capabilities.</p>
<p>In summary, our contributions are threefold:</p>
<ol>
<li>We introduce the LLM-Coordination Benchmark for evaluating and analyzing LLMs in Pure Coordination Games, covering multi-turn Agentic Coordination and single-turn Coordination QA tasks.</li>
</ol>
<p>Related Work</p>
<p>Multi-agent Coordination.Pure Coordination games in game theory are scenarios where agents share the payoffs, and cooperation is the optimal strategy.Benchmarks like the Multiparticle Environment (Lowe et al., 2017), Overcooked-AI (Carroll et al., 2019a), and the Hanabi Challenge (Bard et al., 2020) (Zhao et al., 2023), graph-theoretic objectives (Li et al., 2023c), policy ensembles (Lou et al., 2023), and integrating human biases (Yu et al., 2023).In the Hanabi Challenge, efforts aim to learn grounded policies over arbitrary conventions (Hu et al., 2021b,a).While most solutions enhance RL methods for coordination, we propose that LLMs offer an alternative due to their emergent reasoning and theory-of-mind-like abilities, avoiding arbitrary joint interactions.</p>
<p>Planning and Reasoning with Large Language Models.LLMs have shown remarkable natural language reasoning abilities (OpenAI, 2023;Ouyang et al., 2022;Chiang et al., 2023), achieving state-of-the-art results in verbal reasoning tasks.Augmented with components like memory and tools, LLMs can interact with external environments, solving long-horizon tasks and playing complex games (Wu et al., 2023;Wang et al., 2023a;Liang et al., 2022;Song et al., 2022).Guided by Cognitive Architectures for Language Agents (Sumers et al., 2023) as a design principle for agent design, we experiment with advanced reasoning strategies such as ReAct (Yao et al., 2023), Self-Verification (Weng et al., 2023), and Self-Consistency (Wang et al., 2023b) to enhance LLM reasoning.These strategies establish strong baseline performance for our Language Agent implementations.</p>
<p>Multi-agent LLMs.</p>
<p>LLM-Coordination Benchmark</p>
<p>Multi-turn Agentic Coordination</p>
<p>In the Multi-turn Agentic Coordination task, LLMs participate in end-to-end pure coordination games as agents, where the best strategy for all participating agents is to cooperate.We only consider pure coordination scenarios with no competitive incentives.LLMs under test are plugged into coordination frameworks with memory and the ability to act in complete games.These LLM agents can then be partnered with any policies or agents to complete the games.</p>
<p>Our LLM-Coordination benchmark includes 4 pure coordination games: Hanabi Challenge (Bard et al., 2020), Overcooked-AI (Carroll et al., 2019a), and Collab Capture and Collab Escape (inspired by the Pursuit-Evasion problem).These games were carefully selected for their ability to isolate and highlight specific coordination challenges, providing controlled environments that allow the analysis of pure coordination scenarios without too much emphasis on other reasoning challenges.While LLMs are versatile and capable of addressing a wide range of tasks, these well-studied settings offer established benchmarks, clear metrics, and reproducible scenarios that are particularly suited for examining coordination abilities of participating agents.</p>
<p>Hanabi Challenge.In Hanabi (Bard et al., 2020), players aim to assemble five sequences of cards in ascending order (1 through 5), each sequence dedicated to a different color: purple, red, blue, yellow, and green.A unique aspect of the game is that the players can only view their partner's cards, not their own.This requires players to work collaboratively, utilizing reveal tokens to provide hints about the cards in their partner's hand.These hints can be about either the color or the rank of the cards.For instance, using a single reveal token, a player can indicate all cards of a certain rank in their partner's hand.Once a player has an idea about which card they have, they can choose to play the card on the stack.If the card is correct, they get a point.Otherwise, players lose a collective life token.The loss of all 3 life tokens leads to the end of the game.Hanabi serves as an exemplary Pure Coordination game, necessitating player cooperation to achieve optimal outcomes.Success in Hanabi hinges on the ability to understand partners' perspectives, navigate decisions based on incomplete information, and engage in implicit communication, making it an excellent testing ground for coordination among agents.</p>
<p>Overcooked-AI.In the Overcooked-AI environment (Carroll et al., 2019a), two agents-Alice (Blue) and Bob (Green)-collaborate to cook and deliver onion soups.This environment includes a variety of layouts, each with its own arrangement and quantity of onion dispensers, plate dispensers, cookers, delivery zones, and countertops.To prepare a dish, agents are required to insert three onions into a cooker, initiating a cooking process that lasts 20 time steps.Upon completion, the soup must be plated and delivered to complete the task.Each layout presents unique challenges, emphasizing the need for agents to comprehend their surroundings, locate necessary resources, and synchronize their actions with their teammate for effective collaboration.</p>
<p>Collab Capture.Collab Capture involves two agents trying to capture an adversary in a maze of interconnected rooms.The rooms are connected by doors, which can be controlled through access buttons that can be found in other rooms.The agents' task is to capture the adversary in the least amount of time using effective strategies.To evaluate different coordination strategies between agents, we design four scenarios by controlling the states of the doors (open or closed) within the layout shown in figure 5.These scenarios highlight various coordination challenges, such as trapping the adversary through precise positioning, enabling a teammate by prioritizing door control over direct pursuit, and strategically restricting the adversary's movement to facilitate capture.(see Appendix E.1 for more details.)</p>
<p>Collab Escape.Collab Escape involves two agents trying to escape an adversary in a maze of interconnected rooms.They need to fix two generators (similar to the game Dead-by-Daylight (Dea, 2016)) located in different rooms to open an exit portal.The adversary tries to catch the agents, and the win condition is any one agent escaping.To evaluate coordination strategies in Collab Escape, we develop two scenarios by varying the initial proximity of agents to the adversary and generators in the layout shown in figure 6. Depending on their proximity to the adversary/generators, players need to apply strategies such as luring the adversary away from the partner, choosing to continue fixing the generators while sacrificing for the partner's safety and manipulating the movement of the adversary (see Appendix E.2 for more details.)</p>
<p>Single-turn Coordination QA</p>
<p>The agentic coordination task paints a holistic picture of the abilities of LLMs as agents.To dive deeper into the specific strengths and weaknesses of LLMs, we develop the CoordinationQA Suite.Inspired by the idea of Unit Testing for evaluating AI agents (Knott et al., 2021), we manually sampled edge cases from all 4 pure coordination games mentioned in Section 3.1.All of these edge cases necessitate agents to actively understand their current state, think about their partner's intentions, and come up with the best plans for coordination.We then create a set of three types of questions for each scenario in our CoordinationQA Suite.</p>
<p>• Environment Comprehension (EC) questions require LLMs to make indirect inferences about some aspect of their environment (See Appendix D.1).The questions cover details of the layouts, implications of current observations, and counts of artifacts.</p>
<p>• Theory of Mind Reasoning (ToM) questions challenge the LLMs to predict the intentions of their partners and probe about the requirements of their partners (See Appendix D.2).</p>
<p>• Joint Planning (JP) questions provide agents with the state/observation and ask them to predict the best next action for effective coordination.This question is essentially the same question that LLMs need to repeatedly solve when they act as agents (See Appendix D.3).All the questions were manually developed and labeled.We filtered out questions and scenarios that showed any ambiguity, leaving only questions that had clear, optimal solutions.We generated a total of N=66 scenarios (25 from Overcooked, 28 from Hanabi, and 13 from the two Collab Games) and created 3 questions per scenario, resulting in 198 unique questions.The right side of Figure 1 demonstrates the sampling process for the three types of questions with an example from the game Overcooked.The selected scenario shows the Blue agent about to place their third onion in the cooker, and the green agent needs to figure out what to do next.See Appendix D for examples of questions and the templates used to formulate these questions.</p>
<p>Experimental Setup</p>
<p>Agentic Coordination</p>
<p>We perform two types of experiments in agentic coordination: Self-Play and Cross-Play.In selfplay settings, the participating agents are of the same type.In Cross-Play experiments, we pair agents with unseen partners, and they need to adapt their behavior to the actions of these new partners.</p>
<p>LLM Agents</p>
<p>To allow LLMs to play multi-turn games, we scaffold them with an agentic framework based on Cognitive Architectures for Language Agents Sumers et al. (2023).The framework includes three parts: Memory, Reasoning, and Grounding.</p>
<p>Memory includes (1) Long-Term Memory for storing the Game Description, including the game's rules, conventions, objectives, and action space, (2) Working memory, which consists of a textual description of the current observation, and (3) Episodic Memory which is a list of previous actions selected by the agent.</p>
<p>Reasoning is where the Large Language Model (LLM) is plugged into the framework.It takes the textual description from the Memory as input and generates the next action based on the context.The LLM reasons about the current state and then selects an action from the list of available actions in natural language.</p>
<p>Self-Verification: For the coordination game Hanabi, there is a low margin for error as any misplays lead to the loss of life tokens, and the loss of all three life tokens subsequently results at the end of the game.We thus supplement the reasoning process in Hanabi with Answer-Verification (Weng et al., 2023), where the LLM is re-prompted to confirm that the action it generated is appropriate and does not lead to fatal errors.</p>
<p>ToM-Reasoning: We also demonstrate the positive impact of a Theory of Mind Reasoning step prior to generating the next action for Hanabi and CollabEscape, which benefit from this intermediate step.In the ToM reasoning step, the LLM generates an interpretation of their partner's actions or current position before generating the next action to explicitly capture the belief inference process.We do not test with additional ToM reasoning on Overcooked due to significant latency and cost constraints, with marginal benefits.</p>
<p>Finally, the Grounding process translates the natural language action generated by the reasoning module into game-compatible action(s).The exact implementation of the grounding module depends on the game in question; for example, in Overcooked-AI, the grounding module needs to convert high-level actions like "pick up onion from o0." into sequences of lower-level actions.On the other hand, in games like Hanabi, the Grounding needs to match actions like "Reveal Bob's Red Color Cards" to their lower-level representations.The Grounding process is also responsible for filtering out infeasible actions based on the context of the game (See Appendices A, B for more details.)</p>
<p>There are no prompt or setup differences for LLM Agents based on Cross-play or Self-play.We use the LLMs gpt-4-0125-preview, GPT-3.5-turbo-0125,Mixtral 8x7B, and GPT-4o for agentic evaluation studies.</p>
<p>MARL Agents</p>
<p>Self-play MARL Baselines: For Overcooked we use Proximal Policy Optimization (Schulman et al., 2017) and Population-Based Training (Jaderberg et al., 2017) as baselines for comparison.These baselines were established by Carroll et al. (2019a).</p>
<p>For the Hanabi challenge, we use Bayesian Action Decoder (BAD) (Bard et al., 2020), Simplified Action Decoder (SAD) (Hu and Foerster, 2021), and Off-Belief Learning (Hu et al., 2021a)  achieve near-perfect performance in Self-play.</p>
<p>Cross-play MARL Baselines: For Overcooked, we use a Behavior Cloning model trained on human data (Carroll et al., 2019a) and a Proximal Policy Optimization (PPO) agent trained with the Human Behavior Cloning agent (Carroll et al., 2019a) as baselines for comparison.We also report Hidden-Utility Self-play (HSP) (Yu et al., 2023) as a baseline.We use human proxies based on behavior cloning as unseen partners.</p>
<p>For Hanabi, we use the Simplified Action Decoder (SAD), which is trained through self-play as a baseline.We pair our agents with Off-Belief Learning (Hu et al., 2021a), which was trained to generate grounded policies and adapt to unseen partner agents.</p>
<p>Metrics.We measure the total score achieved by agents in Overcooked, where each delivery provides 20 points to both agents.In the case of Hanabi, the metric is the total number of cards that have been correctly arranged by the players.For CollabEscape and CollabCapture, we report the success rate of escape or capture across multiple trials and the average turns to capture or escape.</p>
<p>CoordinationQA</p>
<p>We assess the performance of 5 Families of Large Language Models (LLMs) (Jiang et al., 2023(Jiang et al., , 2024;;Touvron et al., 2023;Chiang et al., 2023;Ope-nAI, 2023) across three dimensions: Environment Comprehension (EC), Theory of Mind Reasoning (ToM), and Joint Planning (JP).For each category, LLMs respond to multiple-choice questions (MCQs), with their responses evaluated against ground-truth answers through fuzzy string matching.To account for the variability in LLM responses, we conduct three trials per model.All models being tested are shown the same prompts.We also report a Random baseline.</p>
<p>Discussion</p>
<p>Zero-shot LLM Agents match or surpass trained RL methods in Environment-focused Coordination Problems.We observed that LLM agents (w.GPT-4-turbo) outperform or match the overall performance of RL methods across all layouts of Overcooked-AI.implies that LLM agents match RL agents that have been explicitly trained through Self-play without any game-specific training or fine-tuning.However, it is important to note that LLM agents are significantly slower and larger than RL models, making them unsuitable for real-time use at present.We also see positive results on the CollabCapture and CollabEscape games, with most LLMs being able to complete both challenges (see Table 2).</p>
<p>LLM agents struggle at effective planning when advanced Theory of Mind reasoning is required.In Hanabi Challenge, LLM agents seem to struggle compared to RL methods (see Table 3).GPT-4-turbo performs reasonably well, while other LLMs can barely complete the games.We attribute this failure to two factors.First, there is little room for errors in Hanabi.Any misplay leads to the loss of a life token.Second, Hanabi requires more complex Theory of Mind Reasoning compared to the Overcooked-AI environment.Each action requires agents to actively consider their partner's beliefs, intentions, and how they would react to implicit communication.In contrast, Overcooked is fully observable, and its action space consists of actions like pick up an onion from onion_dispenser_0 and place onion in cooker_0.Under most scenarios and layouts, LLMs only need to consider the next best steps based on the state of the environment.</p>
<p>We use correlation study to provide additional validation to these findings.We calculate the Pearson Correlation Coefficient (r) of the performance of the four LLMs (GPT-4-turbo, GPT-4o, GPT-3.5-turbo, and Mixtral 8x7b) on Agentic Coordination setup (Average score per game) vs. the score on CoordinationQA task. Figure 2 shows a high correlation between Environment Comprehension capabilities and Success at Overcooked, but a more moderate correlation between Theory of Mind Reasoning capabilities and Success at Overcooked.Conversely, in Hanabi, high success is strongly correlated with both Environment Comprehension and Theory of Mind Reasoning Abilities.In CollabEscape we see a higher correlation with ToM reasoning abilities compared to CollabCapture.This correlation study also connects and establishes a positive alignment between LLM-agent performance on the multi-turn agentic task and the single-turn CoordinationQA.</p>
<p>Auxiliary reasoning strategies like Verification and ToM reasoning help LLMs reason for coordination.Adding an Answer Verification step significantly reduces fatal mistakes (wrong card plays) caused by LLM hallucinations.Without the support of the Verification step, LLM agents bomb (lose all three lives) before the end of the game in every trial.The ToM reasoning step separates the tasks of interpreting partner clues and generating actions, allowing the LLM to better synthesize available information for action planning.Table 3 shows the impact of ablating the verification and ToM reasoning steps from the LLM Agent.The ToM reasoning step is also useful in the Col-labEscape (see table 2) game, as players need to actively consider what their partner needs and act sacrificially if needed.In CollabCapture, it shows a relatively low benefit since agents can observe the positions of all agents as well as doors on the map and infer the correct action based on this envi-  4: Zero-shot coordination results of AI-Human Proxy Gameplay.We compare Behavior Cloning (BC), PPO_BC, HSP (Yu et al., 2023), and GPT-4-turbo agent.The LLM agent outperforms the PPO and BC methods and matches the HSP (Yu et al., 2023) baseline in most cases, demonstrating robustness to unseen partner agents.Since the two agents in Overcooked-AI might be tasked with different roles based on their starting locations, we show results playing from either side separated by |. ronmental context.</p>
<p>Comparative Results of LLMs in Environment</p>
<p>Comprehension, ToM Reasoning, and Joint Planning.In Figure 3, we see that most LLMs achieve their best results on the Environment Comprehension question.The best performing LLM GPT-4-turbo gets more than 80% Environment Comprehension Questions correct.The overall performance across LLMs drops on the more challenging Theory of Mind reasoning questions.Both GPT-4-turbo and GPT-4o do well on the Theory of Mind reasoning questions.The overall accuracy of LLMs on Joint Planning questions is still significantly weak, with even the best LLM scoring less than 40%, indicating a large room for improvement in LLMs' ability to perform coordination reasoning.Another cause for concern is that open-source LLMs perform abysmally at Joint Planning, with some models performing worse than random.</p>
<p>LLM Agents are robust to unseen partners.We use Overcooked-AI and the Hanabi challenge as testbeds to evaluate the performance of LLM agents when paired with unseen agents.This task is popularly known as Zero Shot Coordination.In Overcooked-AI, we pair our LLM agents as well as baselines with proxy-human agents.These proxy human agents are behavior cloning agents trained using human data by Carroll et al. (2019b).As shown in Table 4, we discover that LLM agents outperform both Behavior Cloning as well as PPO agents trained with human data.Apart from the Asymmetric Advantages layout, they also match or outperform the Hidden Utility Self-play (HSP) baseline, which is designed to excel at ZSC.</p>
<p>In Hanabi, we pair our agents with Off-Belief Learning (OBL) agents (Hu et al., 2021a).OBL is a MARL strategy that generates grounded clues and actions and is the state-of-the-art method for crossplay in Hanabi.OBL agents provide observationgrounded clues and collaborate well with humans.Therefore, we use them as unseen partners in our experiments.Table 5 shows that the GPT-4-turbo agent scores an average of 15 points with the OBL-1 agent compared to their self-play scores of 13.66, indicating no degradation in performance</p>
<p>Method</p>
<p>Self-Play</p>
<p>Cross-Play w/ OBL-1 Cross-Play w/ OBL-4 SAD 23.66 ± 0.54 11.33 ± 4.00 8.00 ± 0.47 GPT-4-turbo 13.66 ± 0.27 15.00 ± 2.94 12.00 ± 0.94 with a new partner.The baseline RL method, Simplified Action Decoder (SAD) (Hu and Foerster, 2021), fails critically when paired with unseen OBL agents, even though it excels at self-play (22.00 points) due to self-play training.</p>
<p>Conclusion</p>
<p>In this study, we evaluated and analyzed the current large language models in the context of pure coordination games.We introduced the LLM-Coordination benchmark with its two tasks: 1. Agentic Coordination and 2. CoordinationQA.These settings allowed us to conduct holistic comparative studies of LLMs as agents and dive deeper into the fine-grained aspects of LLMs as coordination reasoners.We juxtaposed LLM agents with existing Multi-agent Reinforcement Learning agents, discussing the conditions in which LLMs thrive and fail.Finally, we discussed the Theory of Mind Reasoning, Environment Comprehension, and Joint Planning as prerequisites for coordination and evaluated existing LLMs on these components.</p>
<p>Acknowledgements</p>
<p>This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.</p>
<p>Limitations</p>
<p>Latency and Compute Requirements: As highlighted in Section 5, effective reasoning for coordination is achievable primarily with larger LLMs like GPT-4-turbo.However, these models are associated with significant latency and require substantial computational resources, making them less suitable for real-time applications where rapid decision-making is crucial.Initial Prompt Configuration: Achieving optimal reasoning performance necessitates careful manual configuration of the initial prompts that describe the game (Procedural Memory).While this prompt could be extracted from game manuals or existing resources, it still needs to be formatted and designed with the LLM agent in mind.Furthermore, the results for individual games could be improved by letting the LLM generate more text and engineering the prompt.However, we leave these optimizations to future works focused on performance improvement rather than benchmarking.</p>
<p>Manual Curation of Edge Cases: The Coordina-tionQA suite involves manually curating unambiguous edge cases in coordination games to construct the dataset.This can hinder the ability to scale the benchmark to accommodate new scenarios.Yet, it is important to curate these examples for more reliable studies.</p>
<p>A Overcooked Implementation Details</p>
<p>In the game Overcooked-AI, two chefs must coordinate their actions to cook and deliver onion soups.Each soup requires three onions as ingredients, which can be found in onion dispensers.The players must add three onions to a cooker to start cooking.Once the three onions are added, the soup starts cooking automatically and requires 20 time steps to complete.Once the soup is cooked, a player needs to pick up a plate, load the soup, and deliver it to the required delivery area.Players must coordinate their actions to effectively deliver soup.Depending on the layouts (see figure 4), players need to adapt to cramped spaces, understand environment layouts for role assignments, pass objects to each other, make way for one another, and find the most effective paths.</p>
<p>A.1 Game and Layout Description</p>
<p>We use a general game description G that explains the rules and objectives of overcooked.Since each layout has a different number of locations, like onion dispensers and cookers, we include a succinct description of each environment L i , which includes how many instances of particular facilities there are.For environments that include partitions, we mention which partition each of the agents is situated in and what facilities that agents can access.In addition, we also mentioned the shape of the environment.other_player_id]}.{EnvDescriptions[self.layout_name]}Overcooked has the following rules: {self.rules}.We have agreed to follow the following conventions: {self.conventions}.I\'ll provide my action history, current state, teammate's status, and my possible actions.Help me select the best action from the list.Format your response as: Explanation:<Brief explanation for my next action>.Action: <action>.Only select one action.Do not say anything else.Got it?</p>
<p>A.2 State Description</p>
<p>The State is represented in natural language D(S) in the working memory, which can be processed Objects Held by Each Player The state description D(S) begins by detailing the inventories I α 1 and I α 2 of Alice and Bob, respectively.Each inventory I α i (where i ∈ {1, 2}) can contain one of the following items: {"onion", "plate", "cooked soup"}.This inventory information is translated into natural language and incorporated into D(S) in the format: "I am holding I α 1 .Bob is holding I α 2 ."Such information is vital for inferring the likely subsequent actions of the partner agent.</p>
<p>Location of the Agent Controlled by LLM:</p>
<p>Given the limitations of Large Language Models (LLMs) in interpreting grid-based spatial information, we opt to provide processed location data to the LLM.For each agent P i (where i ∈ {1, 2}), and for each location of interest denoted as loc, we calculate the distance d (P i ,loc) as the number of steps required to reach loc from P i using the shortest available path.The state description D(S) then includes this processed location information in the format: "loc is d (P i ,loc) units away."Here, loc can represent various points of interest such as onion dispensers, plate dispensers, cookers, delivery areas, kitchen counters, or shared counters.If a location is either inaccessible or blocked by another agent, this is explicitly stated in D(S).For example, if a location is blocked by Bob, it would be stated as "loc is blocked by Bob."To distinguish between the location information relevant to each agent, D(S) prefixes the respective sections with "Your location information:" for the agent controlled by the LLM and "Bob's location information:" for the partner agent.</p>
<p>Cooker Information</p>
<p>The state description D(S) also incorporates information about the cooker, which is central to the gameplay strategy.Specifically, for each cooker i, D(S) includes the number of onions n i currently in the pot.Additionally, D(S) provides the operational state of the cooker, denoted as CookerState i , which can be either "Off" or "On".Lastly, the current condition of the soup in the cooker is represented by SoupState i , which can take one of the following values: "Cooking", "Cooked", or "Not Started".Thus, the information for cooker c i is formatted as: "c i has n i onions.c i is CookerState i .Soup in c i is SoupState i ."</p>
<p>Kitchen Counter Information The state description D(S) includes information about kitchen counters, which are primarily used for temporary object storage.Specifically, D(S) identifies the closest empty kitchen counter k empty and the set K filled of all counters currently holding an object.</p>
<p>Shared Counter Information Shared counters serve as specialized kitchen counters for object transfer between agents.For each shared counter i, D(S) includes the status for s i , as "s 0 is empty" or "s 1 contains onion," to offer a complete environmental overview.Unlike kitchen counters, where only the closest empty counter is mentioned, all empty shared counters are mentioned.</p>
<p><Inventory>: I am holding onion.Bob is holding nothing.<My Location Information>: o0 is 0 units away.o1 is 1 units away.p0 is 3 units away.c0 is 6 units away blocked by Bob.c1 is 7 units away.d0 is 4 units away.s0 is 1 units away.s1 is 0 units away.s2 is 1 units away.s3 in 2 units away.Closest empty kitchen counter k12 is 1 units away.</p>
<p><Bob's Location Information>: o0 is blocked by Alice.o1 is 7 units away.p0 is 3 units away.c0 is 0 units away.c1 is 1 units away.d0 is 4 units away.s0 is 1 units away.s1 is 0 units away.s2 is 1 units away.s3 in 2 units away.</p>
<p><Environment Details>: c0 contains 1 out of 3 onions.c0 is off.soup in c0 is not cooking.c1 contains 0 out of 3 onions.c1 is off.soup in c1 is not cooking.</p>
<p>Available Actions: [place onion in c0, place onion in c1., place onion on s0., place onion on s1., place onion on s2, place onion on s3., place onion on k12., wait., move away.]</p>
<p>B Hanabi Implementation Details</p>
<p>Hanabi is a card game in which all players are on the same team.The deck is made up of cards numbered 1 through 5, further divided into five different colors.Players are working together to create these numbered sequences (1 through 5) for each of the five colors.Each card played provides 1 point, and the goal is to obtain all 25 points (5 colors with 5 cards each) by completing the sequence for each color.The three actions a player can take include playing a card, discarding a card, and giving a hint (reveal) to a teammate.The challenge is that players cannot see their own cards.They may, however, see the cards that are in the hands of their teammate(s).This is where hints (reveals) come into play.Players are able to figure out which cards should be played through the reveal mechanism.You can hint (reveal) a teammate about a color or number in their hand, and this will point out to them all cards in their hand that have this color or number.The team starts with 8 reveal tokens but can recover used tokens when discarding a card or upon the completion of a stack.Playing a card carries a risk because the team loses completely if three incorrect cards are played.Further, an incorrectly played card gets sent to the discard pile.Each number of a particular color only has so many copies, so if, for example, the last copy of a green 3 is lost, then the team loses out on not only playing the green 3 but also the green 4 and 5.This is the risk of playing or discarding.With these risks, and since reveal tokens are scarce, players ideally try to make assumptions about implicit information that a reveal may offer, and, ideally, the team converges to particular conventions.Implicit communication, collaboration, and memory are key.</p>
<p>B.1 Game Description</p>
<p>We structure the game description of Hanabi into the overall objective, and the rules of the game.</p>
<p>The card game Hanabi has the following rules:</p>
<p>-The game uses a 50-card deck, divided into five colours (red (R), green (G), blue (B), yellow (Y), white (W)).Each color has cards of ranks 1 to 5. Each color has with three 1's, two 2's, two 3's, two 4's, one 5. gives an additional reveal token.Failure discards the card, and loses a life.Playing a card you are unsure about is risky as it costs a life and you have only 3 lives.Before playing a card make sure that it's the next card in the sequence for that stack.<strong><em>The game ends when:</em></strong> -All five stacks are completed.25 Points.-Three lives have been lost.0 Points no matter how many cards have been placed in the stack.</p>
<p>-After the last card from the deck is drawn and each player has had a final turn.Sum total of the top card ranks of each color stack.I am Alice, playing the card game Hanabi with my partner Bob.</p>
<p>At each time step I will provide you with the relevant information of the game.I will also provide you with the legal action, help me select the best next action.Remember I am playing as Alice.Format your response as Explanation: <brief explanation for selecting the move>\ nAction:<selected move>.Do not say anything else.Got it?</p>
<p>E CollabCapture and CollabEscape</p>
<p>E.1 CollabCapture</p>
<p>CollabCapture places two agents in a set of interconnected rooms and doors.Their goal is to capture an adversary within the smallest number of moves possible.The adversary moves away from agents in a greedy fashion, but the agents have the ability to close and open doors while the adversary does not.The doors are controlled by a corresponding button that is in another location on the map.(See Figure 5) CollabCapture contains one layout with four scenarios created by controlling the gate states (open/closed).This corresponds to cases where players need to: 1. Pincer their opponent through coordination 2. One agent needs to enable the other agent by choosing not to chase the agent but rather open the door to allow the other agent to capture the adversary.3.One agent needs to disable the adversary by closing a door, allowing the other agent to catch them.CollabCapture is based on the classic task of Pursuit Evasion from the perspective of the pursuers.This is representative of a commonpayoff task as the only objective is capturing the adversary with no mixed incentives (Akin to deliveries in Overcooked, where all chefs get a common payoff with no preference for the one making the delivery).</p>
<p>E.2 CollabEscape</p>
<p>CollabEscape also places two agents in a set of interconnected rooms and doors.Their goal in this environment, however, is to escape an adversary that is looking to catch them.The map has two generators that power an exit gate, both of which need to be fixed.Upon fixing both generators, the players must then escape by reaching the exit gate.Only one player needs to reach the exit gate in order for them both to win.If the adversary catches either of them, however, they both lose.(See Figure 6)</p>
<p>In CollabEscape, we have one layout two scenarios created by varying the starting positions of the two agents.Depending on their proximity to the adversary/generators, players need to apply strategies such as luring the adversary away from the partner, choosing to continue fixing the generators while sacrificing for the partner's safety, and manipulating the movement of the adversary.This is also representative of a common payoff task, as players are commonly rewarded if any one of them escapes, introducing roles of explicit assistance and sacrifice for a higher common payoff.</p>
<p>Figure 1 :
1
Figure 1: The LLM Coordination Benchmark consists of two tasks: Agentic Coordination to study the holistic abilities of LLMs in multi-turn pure coordination games, and Coordination QA to perform a fine-grained analysis of the Environment Comprehension, Theory of Mind Reasoning, and Joint Planning abilities of LLMs in the context of pure coordination scenarios.</p>
<p>Figure 2 :
2
Figure 2: Correlation of LLM Agent performance in Agentic Coordination setup on all four games vs. performance on the CoordinationQA benchmark.</p>
<p>Figure 3 :
3
Figure 3: The performance of different LLMs on CoordinationQA, which provides a fine-grained analysis of LLMs' Environment Comprehension, Theory of Mind Reasoning, and Joint Planning abilities within pure coordination scenarios.</p>
<p>Figure 4 :
4
Figure 4: The Overcooked layouts from left to right: Cramped Room (CR), Asymmetric Advantages (AA), Forced Coordination (FC), Coordination Ring (CR), and Counter Circuit (CC).</p>
<p>I</p>
<p>am {self.player_names[self.player_id]}.I am playing the game Overcooked with my partner {self.player_names[self.</p>
<p>by a Large Language Model (LLM).The state S includes variables that fully represent the necessary details of the layout as well as the players.The information provided in D(S) is equivalent to what would be accessible to a Reinforcement Learning (RL) agent in the form of state representations.The following information is included in D(S):</p>
<p>Figure 5 :
5
Figure 5: Map layout for CollabCapture</p>
<p>Recent studies have explored LLMs in multi-agent cooperation settings.
Zhang et al. (2023b) developed a modular agentframework for spatial rearrangement tasks. Zhanget al. (2023a) introduced an architecture enablingLLMs to play Overcooked-AI. Shi et al. (2023)demonstrated positive zero-shot coordination inAvalon using code-driven reasoning. Li et al.(2023d) showed emergent collaborative abilitiesof LLMs in simulations, while Li et al. (2023b) in-vestigated theory-of-mind inference using explicitbelief representations. Xu et al. (2024) perform ananalysis of LLMs in communication games Xu et al.(2023) analyzed LLM cognitive abilities throughgames, highlighting benefits of probabilistic mod-eling. In contrast, our research rigorously evaluatesLLM agents' coordination abilities in establishedpure coordination games, where coordination is es-sential. Our setting only includes scenarios whereagents must fully cooperate with each other withno competitive incentives. We also conduct a fine-grained, component-level analysis to understandthe intricacies of LLMs' coordination capabilities.</p>
<p>Table 1 :
1
as MARL baselines for Hanabi.All three baselines Performance comparison across Multi-Agent Reinforcement Learning (MARL) and LLM-agent methods.Scores indicate the best performance in each category.The GPT-4-turbo Agent demonstrates superior coordination in 3 out of 5 scenarios, underscoring advanced reasoning capabilities in coordination tasks.The five layouts are CR: Cramped Room, AA: Asymmetric Advantages, Ring: Coordination Ring, FC: Forced Coordination, and CC: Counter Circuit.For visualization and details of these layouts, see appendix A
AgentCRAARingFCCCSelf-Play (PPO) 198.8 ± 4.06 167.2 ± 3.63190.8 ± 4.25 151.9 ± 3.28122.3 ± 3.80PBT216.9 ± 1.31 190.1 ± 8.64173.8 ± 18.27 169.5 ± 10.09 140.1 ± 13.86GPT-3.5-turbo 33.3 ± 10.88 46.6 ± 10.8840.0 ± 0.0066.6 ± 14.4053.3 ± 5.44Mixtral8x7B46.6 ± 14.40 200.0 ± 9.42113.3 ± 5.44 46.6 ± 14.40100.0 ± 9.42GPT-4o160 ± 0.00166.66 ± 5.44 66.66 ± 21.77 120.0 ± 9.42160.0 ± 0.00GPT-4-turbo173.3 ± 6.67 260.0 ± 11.55 140.0 ± 0.00 180.0 ± 11.55 160.0 ± 0.00Collab EscapeCollab CaptureAgentCapture Rate Avg. Turns Escape Rate Avg. TurnsGPT-4-turbo0.834.601.003.5-(w/out ToM Reasoning)0.502.001.004.75GPT-4o0.674.001.007.17GPT-3.5-turbo0.332.500.678.38Mixtral-8x7b0.507.670.927.55Greedy Baseline0.00N.A.0.506.00</p>
<p>Table 2 :
2
Comparison of different LLM Agents on CollabCapture and CollabEscape.In CollabEscape, two agents work together to escape from an adversary.In CollabCapture, two agents coordinate to capture an adversary.The reported results are run across 3 trials each for various layout configurations (Detailed in Appendix E).The table also demonstrates the impact of the explicit ToM reasoning step in both game setups.</p>
<p>Table 1 presents the numerical scores attained by different agents when paired with a partner agent of the same type.This
AgentScoreBayesian Action Decoder23.92 ± 0.01Simplified Action Decoder24.01 ± 0.01Off-Belief Learning24.10 ± 0.01GPT-4-turbo13.33 ± 0.88-(w.o ToM Reasoning)10.33 ± 0.88-(w.o ToM Reasoning &amp; Verif.) 4.33 ± 0.88GPT-4o8.33 ± 1.20GPT-3.5-turbo1.33 ± 0.72Mixtral-8x7b0.33 ± 0.27</p>
<p>Table 3 :
3
Agentic performance comparison on Hanabi Challenge.RL methods are very strong and obtain nearperfect scores.The best GPT-4-turbo-based LLM Agent is much weaker compared to RL baselines.Removing the ToM reasoning and Verification steps from the LLM agent leads to further performance degradation
0.95EC0.880.950.780.920.900.85ToM0.660.850.570.720.75 0.800.70JP0.850.980.680.890.650.60Overcooked Hanabi C.Capture C.Escape</p>
<p>Table 5 :
5
Cross-Play results of RL agent (SAD) and LLM agent (GPT-4-turbo).All agents play three games with different seeds (same seeds across agents).SAD performs really well at self-play but suffers significant performance degradation with new partners OBL-1 and OBL-4.LLM Agents coordinate well with the new, unseen partners.</p>
<p>Spend a reveal token to reveal cards with a particular color or rank.Revealing a color reveals all cards of that color in partner's hand.Revealing a rank reveals all cards with that rank in partner's hand.The game starts with 8 reveal tokens.If no token left, no more reveals can be given.2. Discard: Discard a card to regain a reveal token and draw a new card.3. Play a Card: If a card played follows sequence in its color stack, it succeeds.Success of rank 5 card in any stack
-Players have to create stacks of each color. Each colorstack starts with a Rank 1 card and goes up one by onein ascending order up to Rank 5. (e.g. Red Stack shouldgo from R1 -&gt; R2 -&gt; R3 -&gt; R4 -&gt; R5). A card can only beplayed if it is the next in the incremental sequencefor its color stack.-Players can only see the other's hand, not their own.-Players have plausible knowledge of their cards based onpreviously provided hints by the other player-They can either play a card, give a reveal, or discard acard.-Players can only chose an action from the Available LegalActions.<strong><em>Actions:</em></strong>1. Reveal (Clue):
For GPT-4-turbo, we run a single trial from either position due to cost and time constraints.
B.2 State DescriptionThe state description includes the current Stack S, the player's knowledge of their cards K (updated based on clues), the partner agent's cards C, the partner agent's knowledge of their cards K ′ (updated based on previous clues), each card in the discard pile d i , the remaining Life Tokens l, and reveal tokens r and the remaining Deck Size D. We also precalculate the next card that goes on each stack since LLMs frequently fail to count which card should go next on each stack.will be provided with my partner's selected action and my latest state information after my partner took their action.You will provide me with two things: 1.An explanation for my partner's previous action along with their intention and implicit communication.2. What is the best information for me to give my partner based on their knowledge?Format your response as: Partner Action Explanation:&lt;1 sentence explanation of partner action&gt; Clue Suggestion:<What information (specify rank or color) should I reveal to my partner based on their knowledge>.C.3 Prompt for Answer Verification StepYou are an action verification agent for games.I will provide you with an action and you need to check whether the action satisfies the criteria: 1. Rule Following: It follows to the rules of the game.The Environment Comprehension (EC) questions are indirect formulations regarding spatial aspects of the layout.In order for an agent to correctly answer an EC question, they must have an understanding of the dynamic details of the current state and the rules of the game and exhibit spatial awareness.As such, when creating the EC questions, we carefully comb through a given scenario in search of salient points to probe an agent's understanding of the given environment.Some examples include:<Inventory>: I am holding nothing.Bob is holding onion.<My location information:> o0 is 1 units away.o1 is 0 units away.p0 is 1 units away.d0 is inaccessible.c0 is inaccessible.c1 is inaccessible.s0 is 1 units away.s1 0 units away.s2 is 1 units away.<Bob's location information>: o0 is inaccessible.o1 is inaccessible.p0 is inaccessible.d0 is 2 units away.c0 is 0 units away.c1 is 0 units away.s0 is 0 units away.s1 is 1 units away.s2 is 2 units away.<Environment Details>: c0 contains 3 out of 3 onions.c0 is on.soup in c0 is still cooking.c1 contains 0 out of 3 onions.c1 is off.soup in c1 is not cooking.s0 is empty.s1 contains onion.s2 is empty.Closest empty kitchen counter k1 is 1 units away.How many onions are still needed to fill up c0? Available Answers:My name is Alice.I am in room 1. Bob is in room 6.I was fixing the generator and there is only one more fix needed, which could be done before getting caught.Currently, we have information that the killer will move to the room 1 after this turn.Generator in room 1 still needs 1 fix.Generator in room 2 is fixed.The exit gate is closed.If I fix generator 1, is Bob in a position to escape?Available Answers: A. Yes, he's only one room away from the gate when it opens.B. No, the killer is blocking his path to the exit gate.C. No, we stil need to fix generator 2.D.2 Theory of Mind Reasoning QuestionsThere are two primary question types in Hanabi for ToM Reasoning questions.In the first type, we ask the LLM about what information the partner agent needs, while in the second type, we ask it to make inferences about the partner agent's last action.For all games apart from Hanabi, the ToM questions ask the models to predict the next intended action of the partner agentD.2.1 Hanabi Question Type-1It is currently My (Alice) turn.Current Stacks: Red -Red 0, Yellow -Yellow 0, Green -Green 0, White -White 0, Blue -Blue 0 My cards based on my knowledge: Card 0 could be:[Red,Yellow,Green,White,Blue] [1,2,3,4,5] Card 1 could be:[Red,Yellow,Green,White,Blue] [1,2,3,4,5] Card 2 could be:[Red,Yellow,Green,White,Blue] [1,2,3,4,5] Card 3 could be:[Red,Yellow,Green,White,Blue] [1,2,3,4<My Location Information>: o0 is 0 units away.o1 is 1 units away.p0 is 3 units away.c0 is 6 units away blocked by Bob.c1 is 7 units away.d0 is 4 units away.s0 is 1 units away.s1 is 0 units away.s2 is 1 units away.s3 in 2 units away.Closest empty kitchen counter k12 is 1 units away.<Bob's Location Information>: o0 is blocked by Alice.o1 is 7 units away.p0 is 3 units away.c0 is 0 units away.c1 is 1 units away.d0 is 4 units away.s0 is 1 units away.s1 is 0 units away.s2 is 1 units away.s3 in 2 units away.<Environment Details>: c0 contains 1 out of 3 onions.c0 is off.soup in c0 is not cooking.c1 contains 0 out of 3 onions.c1 is off.soup in c1 is not cooking.D.3 Joint Planning QuestionsJoint planning questions are effectively the same questions that the LLM solves when they are part of an agentic framework.For each scenario, we ask the LLM to answer the question: "What is the best next action?".
Bellemare, and Michael Bowling. 2020. The hanabi challenge: A new frontier for ai research. Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio Parisotto, Subhodeep Vincent Dumoulin, Moitra, 10.1016/j.artint.2019.103216Artificial Intelligence. 280103216</p>
<p>On the Utility of Learning about Humans for Human-AI Coordination. Micah Carroll, Rohin Shah, Mark K Ho, Thomas L Griffiths, Sanjit A Seshia, Pieter Abbeel, Anca Dragan, 2019aCurran Associates IncRed Hook, NY, USA</p>
<p>. Micah Carroll, Rohin Shah, Mark K Ho, Thomas L Griffiths, Sanjit A Seshia, Pieter Abbeel, Anca Dragan, 2019b</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Metagpt: Meta programming for a multi-agent collaborative framework. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber, arXiv:2308.003522023Preprint</p>
<p>Simplified action decoder for deep multi-agent reinforcement learning. Hengyuan Hu, Jakob N Foerster, arXiv:1912.022882021Preprint</p>
<p>Hengyuan Hu, Adam Lerer, Brandon Cui, David Wu, Luis Pineda, arXiv:2103.04000Noam Brown, and Jakob Foerster. 2021a. Off-belief learning. Preprint</p>
<p>other-play" for zero-shot coordination. Hengyuan Hu, Adam Lerer, Alex Peysakhovich, Jakob Foerster, arXiv:2003.029792021bPreprint</p>
<p>. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu, arXiv:1711.098462017Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024Preprint</p>
<p>Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, A D Dragan, Rohin Shah, arXiv:2101.05507Evaluating the robustness of collaborative agents. 2021Preprint</p>
<p>Theory of mind might have spontaneously emerged in large language models. Michal Kosinski, arXiv:2302.020832023Preprint</p>
<p>Camel: Communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, arXiv:2303.177602023aPreprint</p>
<p>Theory of mind for multi-agent collaboration via large language models. Huao Li, Yu Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Charles Lewis, Katia Sycara, 10.18653/v1/2023.emnlp-main.13Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023b</p>
<p>Cooperative open-ended learning framework for zeroshot coordination. Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing Wang, Wei Pan, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023c. July 2023202of Proceedings of Machine Learning Research</p>
<p>Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. Yuan Li, Yixuan Zhang, Lichao Sun, arXiv:2310.065002023dPreprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, arXivpreprintarXiv:2209.077532022</p>
<p>Pecan: Leveraging policy ensemble for context-aware zeroshot human-ai coordination. Xingzhou Lou, Jiaxian Guo, Junge Zhang, Jun Wang, Kaiqi Huang, Yali Du, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '23. the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '23Richland, SC2023International Foundation for Autonomous Agents and Multiagent Systems</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIPreprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155Jan Leike, and Ryan Lowe. 2022Preprint</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Chatdev: Communicative agents for software development. 2024Preprint</p>
<p>Planning with large language models via corrective re-prompting. Vanya Shreyas Sundara Raman, Eric Cohen, Ifrah Rosen, David Idrees, Stefanie Paulius, Tellex, arXiv:2211.099352022Preprint</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.063472017Preprint</p>
<p>Cooperation on the fly: Exploring language agents for ad hoc teamwork in the avalon game. Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, Yali Du, arXiv:2312.175152023Preprint</p>
<p>Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, arXiv:2212.04088Llm-planner: Few-shot grounded planning for embodied agents with large language models. 2022arXiv preprint</p>
<p>Collaborating with humans without human data. Kevin Dj Strouse, Matt Mckee, Edward Botvinick, Richard Hughes, Everett, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023Preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Voyager: An openended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aPreprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712023bPreprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, arXiv:2212.09561Jun Zhao. 2023Preprint</p>
<p>Spring: Gpt-4 outperforms rl algorithms by studying papers and reasoning. Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li, arXiv:2305.154862023Preprint</p>
<p>Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, Jiashi Feng, arXiv:2311.08562Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. 2023Preprint</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582024Preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023Preprint</p>
<p>Learning zero-shot cooperation with humans, assuming humans are biased. Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, Yi Wu, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, arXiv:2308.11339Feng Yin, Yitao Liang, and Yaodong Yang. 2023a. Proagent: Building proactive cooperative ai with large language models. Preprint</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, arXiv:2307.024852023bPreprint</p>
<p>Maximum entropy population-based training for zero-shot human-ai coordination. Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi Wu, Zhongqian Sun, Wei Yang, 10.1609/aaai.v37i5.25758Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>            </div>
        </div>

    </div>
</body>
</html>
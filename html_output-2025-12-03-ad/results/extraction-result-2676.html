<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2676 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2676</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2676</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-5d150cec2775f9bc863760448f14104cc8f42368</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5d150cec2775f9bc863760448f14104cc8f42368" target="_blank">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning and using sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data.</p>
                <p><strong>Paper Abstract:</strong> Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2676.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2676.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Identification of Nonlinear Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that discovers parsimonious governing differential (or discrete) equations from time-series data by building a library of candidate nonlinear functions and selecting a sparse subset of active terms via sparse regression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SINDy (Sparse Identification of Nonlinear Dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a feature/library matrix Θ(X) of candidate functions (constants, polynomials, trig functions, etc.), collects state snapshots X and time-derivatives ˙X (or uses discrete-time pairs), and solves ˙X = Θ(X) Ξ for sparse coefficient matrix Ξ using sparsity-promoting regression (sequential thresholded least-squares or L1 methods). Key components: (1) library construction, (2) sparse regression solver (sequential thresholding or LASSO), (3) optional derivative denoising (total variation regularized derivative), (4) cross-validation / Pareto-front analysis for selecting sparsity parameter, and (5) dimensionality reduction (POD/SVD) for high-dimensional PDE data.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>sparse-regression-based / symbolic-equation-discovery</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>dynamical systems / physics / fluid dynamics (PDEs) / applied mathematics; demonstrated on canonical ODEs (Lorenz), maps (logistic), and fluid PDE data (cylinder wake)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate hypotheses (functional forms of governing equations) by enumerating a library Θ(X) of nonlinear features (constant, monomials up to chosen order, cross-terms, trig functions, etc.); hypothesis generation reduces to selecting sparse coefficient vectors Ξ that combine these basis functions to form the right-hand side of the dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Not explicitly formalized as a novelty score; novelty is implicitly promoted by sparsity (minimizing number of nonzero terms) and by selecting parsimonious models on the Pareto front (elbow in accuracy vs complexity curve) via cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Model plausibility assessed by (a) cross-validation on held-out data, (b) simulation of the discovered equations and comparison of reconstructed trajectories/attractors with original data (L2 error vs time, attractor geometry), (c) noise-robustness checks and coefficient accuracy, and (d) physical consistency checks such as requiring only polynomial orders consistent with known PDE nonlinearity (e.g., Navier–Stokes quadratic terms).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Controlled by a single sparsification hyperparameter λ (threshold) and selected by cross-validation / inspecting the Pareto front (trade-off between sparsity/complexity and reconstruction error).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Reconstruction L2 error between trajectories (error vs time curves), number of nonzero coefficients (model complexity), coefficient error relative to known ground truth (e.g., reported ~0.03% coefficient error for Lorenz in low-noise case), and qualitative attractor reproduction; no formal information-theoretic novelty metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational validation by forward-simulating inferred ODEs/maps and comparing to the original system (short- and long-time trajectories, attractor geometry), testing on withheld (cross-validation) data, noise sensitivity experiments with varying noise magnitude η, and application to high-fidelity simulation data (DNS of cylinder wake) to compare transient and on-attractor dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Algorithmic determinism via convex sparse methods and a simple sequential thresholding routine (Code 1) with single sparsity hyperparameter λ; use of SVD/POD for dimensionality reduction; explicit description of algorithm and parameter choices in appendix. No formal standardized dataset release or reproducibility protocol beyond algorithm/code description in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Prevention of spurious (overfit) terms via sparsity-promoting regularization (L1 / sequential thresholded least-squares), cross-validation to choose λ, and model selection on the Pareto front to avoid adding unnecessary terms.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detection via degradation in reconstruction metrics on held-out data, failure to reproduce attractor structure (error vs time), sensitivity to added noise (structure identification fails when noise too large), and inspection of coefficient convergence across thresholding iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Empirical noise-sensitivity experiments (adding Gaussian noise of magnitude η to derivatives and measuring reconstruction error and coefficient deviations) and robustness checks; no formal Bayesian UQ or confidence intervals reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Several synthetic and simulated systems used as benchmarks: Lorenz system (ODE, chaotic), damped oscillators (linear and cubic), 3D linear system, logistic map, Hopf normal form, and DNS data for cylinder wake (PDE) reduced via POD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: Lorenz: recovered correct linear/quadratic terms and coefficients (reported coefficient agreement within ~0.03% in low-noise case); error-vs-time curves presented for noise levels η ∈ {0.0001, 0.001, 0.01, 0.1, 1.0, 10.0}; Hopf with noisy training data: cubic-term coefficients off by ~8% (reported); qualitative preservation of attractor and dynamics across tested examples; no standard classification metrics (precision/recall/F1) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Qualitative comparison to symbolic regression (Schmidt & Lipson): SINDy asserted to be more scalable, convex, and less prone to combinatorial search and overfitting; no quantitative head-to-head benchmarks with numeric metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Recovered parsimonious mean-field dynamics for cylinder wake (quadratic terms consistent with Navier–Stokes) and demonstrated recovery of normal forms and parameterized bifurcation dynamics (logistic map, Hopf); these reconstructions reproduced relationships that took experts decades to uncover, but the paper does not claim a new experimentally validated scientific discovery beyond rediscovering/identifying known forms from data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Sensitivity to noise (derivative estimation); choice of candidate function library Θ(X) is critical (may need appropriate basis or coordinate transforms); degeneracies when including very high-order polynomial terms (near-identity transformations can produce spurious combinations); needs representative data including off-attractor trajectories to identify full dynamics (absence can lead to incorrect lower-dimensional models); no formal probabilistic UQ or statistical-significance framework provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2676.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequential Thresholded LS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Thresholded Least-Squares (sparsification algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, computationally-efficient algorithm that alternates between least-squares regression and hard-thresholding small coefficients to obtain a sparse coefficient matrix Ξ for the SINDy regression problem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sequential thresholded least-squares algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Initial least-squares solution Xi = Θ \ ˙X is computed; coefficients below threshold λ are set to zero; then least-squares is re-solved restricted to remaining terms; iterate until convergence. Single hyperparameter λ controls sparsity and the algorithm is inexpensive relative to LASSO for large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>iterative sparse regression / thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>applied mathematics / dynamical systems / data-driven modeling (used within SINDy across domains in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generative model itself; it finds sparse combinations of candidate features (Θ) that form hypotheses (equations) by iterative pruning and refitting.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility enforced indirectly by retaining only terms that survive thresholding and by subsequent refitting; final model validated by cross-validation and simulation-based checks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Controlled by choosing threshold λ; higher λ increases parsimony (novelty of fewer terms) at risk of missing relevant terms (plausibility).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Convergence of nonzero coefficient support; reconstruction L2 error; number of nonzero coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Cross-validation and simulation of learned dynamics to check fidelity; iterative coefficient convergence check.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Simple algorithm pseudocode (Matlab code provided), single hyperparameter λ, deterministic least-squares solves; reproducible given same Θ, data, and λ.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Hard-thresholding removes small, likely spurious coefficients; cross-validation selects λ to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>As for SINDy: poor generalization on held-out data or failure to reproduce attractor dynamics indicates spurious terms or missed terms.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used across the paper's examples (Lorenz, oscillators, cylinder wake, logistic, Hopf).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described as computationally efficient and rapidly convergent in few iterations; no standardized runtime or error benchmarks versus LASSO provided numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Presented as an alternative to LASSO; claimed more computationally efficient for large datasets but no quantitative runtime/accuracy comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires selection of threshold λ; hard thresholding is heuristic and may remove weak but physically relevant terms if λ too large; convergence and selection depend on noise and on richness of data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2676.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LASSO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least Absolute Shrinkage and Selection Operator (L1-regularized regression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convex regression formulation that enforces sparsity by penalizing the L1 norm of regression coefficients, used to select a small number of active terms from a library of candidate features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LASSO (L1-regularized regression)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Solves argmin_ξ ||Θ ξ - y||_2 + λ ||ξ||_1 to obtain sparse ξ; convex optimization yields sparse solutions under appropriate conditions and is connected to compressive sensing theory.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>sparse-regularization / convex optimization</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics / machine learning / system identification (presented as suitable for regression tasks within dynamical-model discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Selects sparse combinations of candidate features (columns of Θ) via L1 penalty; hypotheses correspond to nonzero coefficients in solution.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility controlled by reconstruction error (data-fit term) and sparsity penalty λ; cross-validation recommended for λ selection.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Controlled by λ in objective function (trade-off between fit and sparsity).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Objective value (fit + λ * L1), out-of-sample reconstruction error via cross-validation, number of nonzero coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Cross-validation and out-of-sample testing; could be used analogously to SINDy validation though LASSO was not the primary solver in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Convex optimization with well-defined objective; reproducible given solver and λ.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>L1 penalty reduces overfitting by shrinking small coefficients to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not numerically benchmarked in this paper; noted as effective but potentially computationally expensive for very large datasets compared to the sequential thresholding approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computationally expensive for very large datasets; requires careful λ selection; potential scaling issues for high-dimensional Θ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2676.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic regression (genetic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Regression via Genetic Programming (Schmidt & Lipson style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary algorithm approach that searches the space of symbolic expressions (combinations of elementary functions) to find closed-form equations that fit data, balancing complexity and accuracy (Pareto-front selection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling free-form natural laws from experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic regression (genetic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses genetic programming to evolve candidate symbolic expressions composed of primitive functions/operators; evaluates fitness by data-fit and expression complexity, often selecting models on the Pareto front. Historically used to extract free-form governing laws from data but is computationally expensive and can overfit without explicit parsimony control.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>genetic-programming / symbolic-regression</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific modeling / physics / dynamical systems</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Stochastic generation and evolution of symbolic expressions via mutation/crossover; candidate equations are direct symbolic hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Uses Pareto-front (trade-off between error and complexity) to select parsimonious but novel expressions; novelty not formalized beyond complexity-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assessed via fit to data and parsimony (Pareto front); user/expert inspection often required to assess physical plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Managed with Pareto-front and complexity penalties to avoid overfitting; but trade-offs are empirical and computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Data-fit (e.g., squared error) and symbolic complexity (term count or expression size) used to form Pareto front; cross-validation may be used.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluation on held-out data and inspection of Pareto-front models; often requires human-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Parsimony constraint (Pareto-front) and complexity penalties help reduce overfitting/hallucination; still prone to spurious fits.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not quantitatively compared in this paper; described qualitatively as expensive and less scalable than sparse convex approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Described qualitatively: symbolic regression is powerful but computationally expensive and prone to overfitting; SINDy presented as a scalable convex alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Scales poorly to large systems, computationally expensive, and prone to overfitting unless parsimony is enforced; requires careful human oversight for plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2676.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compressive sensing / sparse representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compressive Sensing and Sparse Representation Theory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical framework guaranteeing recovery of sparse signals from underdetermined measurements under incoherence conditions, used here as theoretical justification for recovering sparse governing-equation coefficients from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Compressive sensing / sparse representation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides conditions (sparsity, incoherence, restricted isometry properties) under which sparse coefficient vectors can be recovered from few measurements via convex L1 minimization or greedy algorithms; motivates the use of L1-regularized regression and sparse solvers in SINDy.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>mathematical / theoretical framework</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing / statistics / applied math / data-driven modeling</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not directly generative; supports recovery of sparse coefficient hypotheses from linear measurements via convex optimization or greedy algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Recovery guarantees typically depend on sparsity level, measurement incoherence, and noise bounds; the paper leverages these theoretical assurances qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Theoretic guarantees suggest that sparse recovery will avoid spurious nonzero coefficients under suitable conditions (incoherence, sufficient samples); practical implementation uses L1 penalties and thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Theoretical guarantees require conditions (e.g., incoherent measurements, sparsity) that may not strictly hold in all dynamical-systems identification problems; practical performance depends on noise levels and library design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2676.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TV-regularized derivative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Total Variation Regularized Numerical Derivative</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to estimate time-derivatives from noisy time-series by minimizing a total-variation-regularized objective to obtain stable derivatives suitable for use in equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numerical differentiation of noisy, nonsmooth data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Total-variation regularized derivative (TV derivative)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applies total-variation regularization to denoise and compute derivatives from noisy data prior to sparse regression; reduces amplification of high-frequency noise inherent in direct differentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>signal-processing / regularization method</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>data preprocessing for dynamical-systems identification / signal processing</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Preprocessing step (not generative): provides cleaner derivative estimates ˙X for use in sparse-regression hypothesis selection.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Improves downstream reconstruction error and coefficient accuracy by reducing noise in derivative estimates; empirical comparisons in paper show improved robustness to noisy measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Applied to noisy data examples (e.g., Lorenz, Hopf) and shown to improve identification robustness; no formal statistical test reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Reference to Chartrand's method and use described; implementation details and references provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By reducing noise in derivative estimates, it reduces the chance of discovering spurious high-frequency terms that arise from differentiated noise.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on the paper's noisy examples (Lorenz, Hopf normal form).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Demonstrated empirically to enable correct term identification across larger noise magnitudes than naive differentiation; no numeric AUC/precision/recall provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared qualitatively (and empirically) to naive differentiation where noise produces large derivative errors; TV derivative recommended for derivative denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Choice of regularization parameters for the TV derivative affects smoothing vs. detail preservation; may smooth dynamics if over-regularized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2676.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-validation / Pareto front</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-validation and Pareto-front model selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model selection approach that selects sparsity parameter λ by trading off model complexity (number of terms) and accuracy (reconstruction error), typically using cross-validation to evaluate out-of-sample performance and choosing the elbow on the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cross-validation with Pareto-front selection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hold-out validation partitions are used to estimate generalization error for models produced at different sparsity levels; the Pareto front (accuracy vs complexity) is inspected to choose λ at the elbow for parsimonious but accurate models.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>model-selection / validation technique</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / system identification / data-driven modeling</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Implicit: prefers parsimonious models on the Pareto front, which indirectly favors novel simple expressions over more complex ones.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses out-of-sample error from cross-validation as a plausibility/generalization measure.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Achieved by selecting the elbow (trade-off) in accuracy vs complexity curve parameterized by λ.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Cross-validated reconstruction error, number of nonzero coefficients (complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Cross-validation (hold-back test data) and inspection of Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Standard ML practice; choice of folds and random splits influences outcome and should be reported (paper recommends cross-validation but does not prescribe detailed protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Selecting simpler models that generalize better reduces overfitting/hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Large discrepancy between training and validation error indicates overfitting/hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied across the paper's synthetic and simulation examples to select λ.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No detailed protocol or statistical thresholds provided; selection of the 'elbow' can be subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2676.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POD / SVD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proper Orthogonal Decomposition (via Singular Value Decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dimensionality-reduction technique (SVD-based) used to compute a low-rank modal basis Ψr so that high-dimensional PDE state x can be approximated by low-dimensional modal coefficients a and SINDy can be applied to the reduced system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Proper Orthogonal Decomposition (POD) using SVD</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses SVD of snapshot matrix X^T = Ψ Σ V* to obtain orthonormal modes Ψ; truncation to r modes yields low-dimensional coordinates a where ˙a = f_P(a) is learned with SINDy, enabling tractable sparse identification for high-dimensional systems (e.g., cylinder wake DNS).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>dimensionality-reduction / modal decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fluid dynamics / PDE model reduction / dynamical systems</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Preprocessing step: produces reduced coordinates a in which SINDy enumerates hypotheses as sparse functions of a.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility judged by how well reduced-model simulations reproduce full-order dynamics (trajectories, slow-manifold behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Reconstruction of full system dynamics from POD coordinates, qualitative matching of transient and limit-cycle behavior, energy captured by retained modes (singular values).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compare low-order SINDy model simulations with full DNS results (on- and off-attractor dynamics); check slow-manifold behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Standard SVD/POD methodology; specifics (number of modes retained r) must be reported for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By reducing state to dominant coherent structures, reduces spurious small-scale noise-driven terms in identification.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to DNS of flow past a cylinder (Re=100) with large state dimensions reduced to a few POD modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative agreement of reduced SINDy model with DNS limit-cycle and transient behavior; successful capture of slow-manifold dynamics when off-attractor training data included.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>POD truncation may omit relevant modes needed for accurate dynamics; reduced coordinates may not be the sparsest representation for governing dynamics; requires representative snapshots including relevant transients.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2676.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Mode Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An equation-free linear regression method that fits a linear map between successive snapshots and connects to the Koopman operator for nonlinear dynamics when extended with nonlinear observables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic mode decomposition of numerical and experimental data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dynamic Mode Decomposition (DMD)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Seeks a linear operator A such that X_{k+1} ≈ A X_k (or works in reduced POD coordinates); can be interpreted as a special case of the discrete-time SINDy formulation with Θ(x)=x, and links to Koopman operator theory when extended with nonlinear observables.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>equation-free linear regression / Koopman-related</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fluid dynamics / dynamical systems / data-driven modeling</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates linear dynamical hypotheses (A matrix) from snapshot pairs via least-squares (or with modal decomposition).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assessed by reconstructing observed snapshots and comparing modal decomposition; linear assumption plausibility depends on dynamics locality.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Reconstruction error for snapshots, modal amplitudes and eigenvalues; no sparse-term selection since DMD is typically dense.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compare DMD-predicted dynamics with measured/simulated data; connection to Koopman can be exploited with extended observables.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mentioned as related but different: DMD is linear/equation-free whereas SINDy learns sparse nonlinear governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linear model assumption limits extrapolative power; choice of observables in extended DMD is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2676.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2676.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SRC / OMP (refs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Representation for Classification and Orthogonal Matching Pursuit (related sparse methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sparse classification (SRC) and greedy sparse recovery methods (OMP) are referenced as related sparse-recovery tools; they illustrate alternative sparse-selection strategies and theoretical background.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sparse representation for classification (SRC) and orthogonal matching pursuit (OMP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SRC uses sparse linear combinations of training examples for classification; OMP is a greedy algorithm for sparse signal recovery by iteratively selecting dictionary atoms. Both provide context/alternatives for sparse selection in the SINDy framework.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>sparse-recovery / classification methods</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>signal processing / machine learning / pattern recognition</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Discovering governing equations from data by sparse identification of nonlinear dynamical systems', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 2)</em></li>
                <li>Regression shrinkage and selection via the lasso <em>(Rating: 2)</em></li>
                <li>Compressed sensing <em>(Rating: 2)</em></li>
                <li>Dynamic mode decomposition of numerical and experimental data <em>(Rating: 2)</em></li>
                <li>Numerical differentiation of noisy, nonsmooth data <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2676",
    "paper_id": "paper-5d150cec2775f9bc863760448f14104cc8f42368",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "SINDy",
            "name_full": "Sparse Identification of Nonlinear Dynamics",
            "brief_description": "A method that discovers parsimonious governing differential (or discrete) equations from time-series data by building a library of candidate nonlinear functions and selecting a sparse subset of active terms via sparse regression.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SINDy (Sparse Identification of Nonlinear Dynamics)",
            "system_description": "Constructs a feature/library matrix Θ(X) of candidate functions (constants, polynomials, trig functions, etc.), collects state snapshots X and time-derivatives ˙X (or uses discrete-time pairs), and solves ˙X = Θ(X) Ξ for sparse coefficient matrix Ξ using sparsity-promoting regression (sequential thresholded least-squares or L1 methods). Key components: (1) library construction, (2) sparse regression solver (sequential thresholding or LASSO), (3) optional derivative denoising (total variation regularized derivative), (4) cross-validation / Pareto-front analysis for selecting sparsity parameter, and (5) dimensionality reduction (POD/SVD) for high-dimensional PDE data.",
            "system_type": "sparse-regression-based / symbolic-equation-discovery",
            "scientific_domain": "dynamical systems / physics / fluid dynamics (PDEs) / applied mathematics; demonstrated on canonical ODEs (Lorenz), maps (logistic), and fluid PDE data (cylinder wake)",
            "hypothesis_generation_method": "Generates candidate hypotheses (functional forms of governing equations) by enumerating a library Θ(X) of nonlinear features (constant, monomials up to chosen order, cross-terms, trig functions, etc.); hypothesis generation reduces to selecting sparse coefficient vectors Ξ that combine these basis functions to form the right-hand side of the dynamics.",
            "novelty_assessment_method": "Not explicitly formalized as a novelty score; novelty is implicitly promoted by sparsity (minimizing number of nonzero terms) and by selecting parsimonious models on the Pareto front (elbow in accuracy vs complexity curve) via cross-validation.",
            "plausibility_assessment_method": "Model plausibility assessed by (a) cross-validation on held-out data, (b) simulation of the discovered equations and comparison of reconstructed trajectories/attractors with original data (L2 error vs time, attractor geometry), (c) noise-robustness checks and coefficient accuracy, and (d) physical consistency checks such as requiring only polynomial orders consistent with known PDE nonlinearity (e.g., Navier–Stokes quadratic terms).",
            "novelty_plausibility_balance": "Controlled by a single sparsification hyperparameter λ (threshold) and selected by cross-validation / inspecting the Pareto front (trade-off between sparsity/complexity and reconstruction error).",
            "hypothesis_quality_metrics": "Reconstruction L2 error between trajectories (error vs time curves), number of nonzero coefficients (model complexity), coefficient error relative to known ground truth (e.g., reported ~0.03% coefficient error for Lorenz in low-noise case), and qualitative attractor reproduction; no formal information-theoretic novelty metric provided.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational validation by forward-simulating inferred ODEs/maps and comparing to the original system (short- and long-time trajectories, attractor geometry), testing on withheld (cross-validation) data, noise sensitivity experiments with varying noise magnitude η, and application to high-fidelity simulation data (DNS of cylinder wake) to compare transient and on-attractor dynamics.",
            "reproducibility_measures": "Algorithmic determinism via convex sparse methods and a simple sequential thresholding routine (Code 1) with single sparsity hyperparameter λ; use of SVD/POD for dimensionality reduction; explicit description of algorithm and parameter choices in appendix. No formal standardized dataset release or reproducibility protocol beyond algorithm/code description in the paper.",
            "hallucination_prevention_method": "Prevention of spurious (overfit) terms via sparsity-promoting regularization (L1 / sequential thresholded least-squares), cross-validation to choose λ, and model selection on the Pareto front to avoid adding unnecessary terms.",
            "hallucination_detection_method": "Detection via degradation in reconstruction metrics on held-out data, failure to reproduce attractor structure (error vs time), sensitivity to added noise (structure identification fails when noise too large), and inspection of coefficient convergence across thresholding iterations.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Empirical noise-sensitivity experiments (adding Gaussian noise of magnitude η to derivatives and measuring reconstruction error and coefficient deviations) and robustness checks; no formal Bayesian UQ or confidence intervals reported.",
            "benchmark_dataset": "Several synthetic and simulated systems used as benchmarks: Lorenz system (ODE, chaotic), damped oscillators (linear and cubic), 3D linear system, logistic map, Hopf normal form, and DNS data for cylinder wake (PDE) reduced via POD.",
            "performance_metrics": "Examples: Lorenz: recovered correct linear/quadratic terms and coefficients (reported coefficient agreement within ~0.03% in low-noise case); error-vs-time curves presented for noise levels η ∈ {0.0001, 0.001, 0.01, 0.1, 1.0, 10.0}; Hopf with noisy training data: cubic-term coefficients off by ~8% (reported); qualitative preservation of attractor and dynamics across tested examples; no standard classification metrics (precision/recall/F1) reported.",
            "comparison_with_baseline": "Qualitative comparison to symbolic regression (Schmidt & Lipson): SINDy asserted to be more scalable, convex, and less prone to combinatorial search and overfitting; no quantitative head-to-head benchmarks with numeric metrics reported.",
            "validated_on_real_science": true,
            "novel_discoveries": "Recovered parsimonious mean-field dynamics for cylinder wake (quadratic terms consistent with Navier–Stokes) and demonstrated recovery of normal forms and parameterized bifurcation dynamics (logistic map, Hopf); these reconstructions reproduced relationships that took experts decades to uncover, but the paper does not claim a new experimentally validated scientific discovery beyond rediscovering/identifying known forms from data.",
            "limitations": "Sensitivity to noise (derivative estimation); choice of candidate function library Θ(X) is critical (may need appropriate basis or coordinate transforms); degeneracies when including very high-order polynomial terms (near-identity transformations can produce spurious combinations); needs representative data including off-attractor trajectories to identify full dynamics (absence can lead to incorrect lower-dimensional models); no formal probabilistic UQ or statistical-significance framework provided.",
            "uuid": "e2676.0",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "Sequential Thresholded LS",
            "name_full": "Sequential Thresholded Least-Squares (sparsification algorithm)",
            "brief_description": "An iterative, computationally-efficient algorithm that alternates between least-squares regression and hard-thresholding small coefficients to obtain a sparse coefficient matrix Ξ for the SINDy regression problem.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Sequential thresholded least-squares algorithm",
            "system_description": "Initial least-squares solution Xi = Θ \\ ˙X is computed; coefficients below threshold λ are set to zero; then least-squares is re-solved restricted to remaining terms; iterate until convergence. Single hyperparameter λ controls sparsity and the algorithm is inexpensive relative to LASSO for large datasets.",
            "system_type": "iterative sparse regression / thresholding",
            "scientific_domain": "applied mathematics / dynamical systems / data-driven modeling (used within SINDy across domains in the paper)",
            "hypothesis_generation_method": "Not a generative model itself; it finds sparse combinations of candidate features (Θ) that form hypotheses (equations) by iterative pruning and refitting.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility enforced indirectly by retaining only terms that survive thresholding and by subsequent refitting; final model validated by cross-validation and simulation-based checks.",
            "novelty_plausibility_balance": "Controlled by choosing threshold λ; higher λ increases parsimony (novelty of fewer terms) at risk of missing relevant terms (plausibility).",
            "hypothesis_quality_metrics": "Convergence of nonzero coefficient support; reconstruction L2 error; number of nonzero coefficients.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Cross-validation and simulation of learned dynamics to check fidelity; iterative coefficient convergence check.",
            "reproducibility_measures": "Simple algorithm pseudocode (Matlab code provided), single hyperparameter λ, deterministic least-squares solves; reproducible given same Θ, data, and λ.",
            "hallucination_prevention_method": "Hard-thresholding removes small, likely spurious coefficients; cross-validation selects λ to avoid overfitting.",
            "hallucination_detection_method": "As for SINDy: poor generalization on held-out data or failure to reproduce attractor dynamics indicates spurious terms or missed terms.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used across the paper's examples (Lorenz, oscillators, cylinder wake, logistic, Hopf).",
            "performance_metrics": "Described as computationally efficient and rapidly convergent in few iterations; no standardized runtime or error benchmarks versus LASSO provided numerically.",
            "comparison_with_baseline": "Presented as an alternative to LASSO; claimed more computationally efficient for large datasets but no quantitative runtime/accuracy comparison reported.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Requires selection of threshold λ; hard thresholding is heuristic and may remove weak but physically relevant terms if λ too large; convergence and selection depend on noise and on richness of data.",
            "uuid": "e2676.1",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "LASSO",
            "name_full": "Least Absolute Shrinkage and Selection Operator (L1-regularized regression)",
            "brief_description": "A convex regression formulation that enforces sparsity by penalizing the L1 norm of regression coefficients, used to select a small number of active terms from a library of candidate features.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LASSO (L1-regularized regression)",
            "system_description": "Solves argmin_ξ ||Θ ξ - y||_2 + λ ||ξ||_1 to obtain sparse ξ; convex optimization yields sparse solutions under appropriate conditions and is connected to compressive sensing theory.",
            "system_type": "sparse-regularization / convex optimization",
            "scientific_domain": "statistics / machine learning / system identification (presented as suitable for regression tasks within dynamical-model discovery)",
            "hypothesis_generation_method": "Selects sparse combinations of candidate features (columns of Θ) via L1 penalty; hypotheses correspond to nonzero coefficients in solution.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility controlled by reconstruction error (data-fit term) and sparsity penalty λ; cross-validation recommended for λ selection.",
            "novelty_plausibility_balance": "Controlled by λ in objective function (trade-off between fit and sparsity).",
            "hypothesis_quality_metrics": "Objective value (fit + λ * L1), out-of-sample reconstruction error via cross-validation, number of nonzero coefficients.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Cross-validation and out-of-sample testing; could be used analogously to SINDy validation though LASSO was not the primary solver in experiments.",
            "reproducibility_measures": "Convex optimization with well-defined objective; reproducible given solver and λ.",
            "hallucination_prevention_method": "L1 penalty reduces overfitting by shrinking small coefficients to zero.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": "Not numerically benchmarked in this paper; noted as effective but potentially computationally expensive for very large datasets compared to the sequential thresholding approach.",
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Computationally expensive for very large datasets; requires careful λ selection; potential scaling issues for high-dimensional Θ.",
            "uuid": "e2676.2",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "Symbolic regression (genetic programming)",
            "name_full": "Symbolic Regression via Genetic Programming (Schmidt & Lipson style)",
            "brief_description": "An evolutionary algorithm approach that searches the space of symbolic expressions (combinations of elementary functions) to find closed-form equations that fit data, balancing complexity and accuracy (Pareto-front selection).",
            "citation_title": "Distilling free-form natural laws from experimental data",
            "mention_or_use": "mention",
            "system_name": "Symbolic regression (genetic programming)",
            "system_description": "Uses genetic programming to evolve candidate symbolic expressions composed of primitive functions/operators; evaluates fitness by data-fit and expression complexity, often selecting models on the Pareto front. Historically used to extract free-form governing laws from data but is computationally expensive and can overfit without explicit parsimony control.",
            "system_type": "genetic-programming / symbolic-regression",
            "scientific_domain": "general scientific modeling / physics / dynamical systems",
            "hypothesis_generation_method": "Stochastic generation and evolution of symbolic expressions via mutation/crossover; candidate equations are direct symbolic hypotheses.",
            "novelty_assessment_method": "Uses Pareto-front (trade-off between error and complexity) to select parsimonious but novel expressions; novelty not formalized beyond complexity-based selection.",
            "plausibility_assessment_method": "Assessed via fit to data and parsimony (Pareto front); user/expert inspection often required to assess physical plausibility.",
            "novelty_plausibility_balance": "Managed with Pareto-front and complexity penalties to avoid overfitting; but trade-offs are empirical and computationally expensive.",
            "hypothesis_quality_metrics": "Data-fit (e.g., squared error) and symbolic complexity (term count or expression size) used to form Pareto front; cross-validation may be used.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluation on held-out data and inspection of Pareto-front models; often requires human-in-the-loop verification.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Parsimony constraint (Pareto-front) and complexity penalties help reduce overfitting/hallucination; still prone to spurious fits.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": "Not quantitatively compared in this paper; described qualitatively as expensive and less scalable than sparse convex approaches.",
            "comparison_with_baseline": "Described qualitatively: symbolic regression is powerful but computationally expensive and prone to overfitting; SINDy presented as a scalable convex alternative.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Scales poorly to large systems, computationally expensive, and prone to overfitting unless parsimony is enforced; requires careful human oversight for plausibility.",
            "uuid": "e2676.3",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "Compressive sensing / sparse representation",
            "name_full": "Compressive Sensing and Sparse Representation Theory",
            "brief_description": "A theoretical framework guaranteeing recovery of sparse signals from underdetermined measurements under incoherence conditions, used here as theoretical justification for recovering sparse governing-equation coefficients from data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Compressive sensing / sparse representation",
            "system_description": "Provides conditions (sparsity, incoherence, restricted isometry properties) under which sparse coefficient vectors can be recovered from few measurements via convex L1 minimization or greedy algorithms; motivates the use of L1-regularized regression and sparse solvers in SINDy.",
            "system_type": "mathematical / theoretical framework",
            "scientific_domain": "signal processing / statistics / applied math / data-driven modeling",
            "hypothesis_generation_method": "Not directly generative; supports recovery of sparse coefficient hypotheses from linear measurements via convex optimization or greedy algorithms.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Recovery guarantees typically depend on sparsity level, measurement incoherence, and noise bounds; the paper leverages these theoretical assurances qualitatively.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Theoretic guarantees suggest that sparse recovery will avoid spurious nonzero coefficients under suitable conditions (incoherence, sufficient samples); practical implementation uses L1 penalties and thresholding.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Theoretical guarantees require conditions (e.g., incoherent measurements, sparsity) that may not strictly hold in all dynamical-systems identification problems; practical performance depends on noise levels and library design.",
            "uuid": "e2676.4",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "TV-regularized derivative",
            "name_full": "Total Variation Regularized Numerical Derivative",
            "brief_description": "A method to estimate time-derivatives from noisy time-series by minimizing a total-variation-regularized objective to obtain stable derivatives suitable for use in equation discovery.",
            "citation_title": "Numerical differentiation of noisy, nonsmooth data",
            "mention_or_use": "use",
            "system_name": "Total-variation regularized derivative (TV derivative)",
            "system_description": "Applies total-variation regularization to denoise and compute derivatives from noisy data prior to sparse regression; reduces amplification of high-frequency noise inherent in direct differentiation.",
            "system_type": "signal-processing / regularization method",
            "scientific_domain": "data preprocessing for dynamical-systems identification / signal processing",
            "hypothesis_generation_method": "Preprocessing step (not generative): provides cleaner derivative estimates ˙X for use in sparse-regression hypothesis selection.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Improves downstream reconstruction error and coefficient accuracy by reducing noise in derivative estimates; empirical comparisons in paper show improved robustness to noisy measurements.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Applied to noisy data examples (e.g., Lorenz, Hopf) and shown to improve identification robustness; no formal statistical test reported.",
            "reproducibility_measures": "Reference to Chartrand's method and use described; implementation details and references provided.",
            "hallucination_prevention_method": "By reducing noise in derivative estimates, it reduces the chance of discovering spurious high-frequency terms that arise from differentiated noise.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used on the paper's noisy examples (Lorenz, Hopf normal form).",
            "performance_metrics": "Demonstrated empirically to enable correct term identification across larger noise magnitudes than naive differentiation; no numeric AUC/precision/recall provided.",
            "comparison_with_baseline": "Compared qualitatively (and empirically) to naive differentiation where noise produces large derivative errors; TV derivative recommended for derivative denoising.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Choice of regularization parameters for the TV derivative affects smoothing vs. detail preservation; may smooth dynamics if over-regularized.",
            "uuid": "e2676.5",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "Cross-validation / Pareto front",
            "name_full": "Cross-validation and Pareto-front model selection",
            "brief_description": "Model selection approach that selects sparsity parameter λ by trading off model complexity (number of terms) and accuracy (reconstruction error), typically using cross-validation to evaluate out-of-sample performance and choosing the elbow on the Pareto front.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Cross-validation with Pareto-front selection",
            "system_description": "Hold-out validation partitions are used to estimate generalization error for models produced at different sparsity levels; the Pareto front (accuracy vs complexity) is inspected to choose λ at the elbow for parsimonious but accurate models.",
            "system_type": "model-selection / validation technique",
            "scientific_domain": "machine learning / system identification / data-driven modeling",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Implicit: prefers parsimonious models on the Pareto front, which indirectly favors novel simple expressions over more complex ones.",
            "plausibility_assessment_method": "Uses out-of-sample error from cross-validation as a plausibility/generalization measure.",
            "novelty_plausibility_balance": "Achieved by selecting the elbow (trade-off) in accuracy vs complexity curve parameterized by λ.",
            "hypothesis_quality_metrics": "Cross-validated reconstruction error, number of nonzero coefficients (complexity).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Cross-validation (hold-back test data) and inspection of Pareto front.",
            "reproducibility_measures": "Standard ML practice; choice of folds and random splits influences outcome and should be reported (paper recommends cross-validation but does not prescribe detailed protocol).",
            "hallucination_prevention_method": "Selecting simpler models that generalize better reduces overfitting/hallucination.",
            "hallucination_detection_method": "Large discrepancy between training and validation error indicates overfitting/hallucination.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied across the paper's synthetic and simulation examples to select λ.",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "No detailed protocol or statistical thresholds provided; selection of the 'elbow' can be subjective.",
            "uuid": "e2676.6",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "POD / SVD",
            "name_full": "Proper Orthogonal Decomposition (via Singular Value Decomposition)",
            "brief_description": "A dimensionality-reduction technique (SVD-based) used to compute a low-rank modal basis Ψr so that high-dimensional PDE state x can be approximated by low-dimensional modal coefficients a and SINDy can be applied to the reduced system.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Proper Orthogonal Decomposition (POD) using SVD",
            "system_description": "Uses SVD of snapshot matrix X^T = Ψ Σ V* to obtain orthonormal modes Ψ; truncation to r modes yields low-dimensional coordinates a where ˙a = f_P(a) is learned with SINDy, enabling tractable sparse identification for high-dimensional systems (e.g., cylinder wake DNS).",
            "system_type": "dimensionality-reduction / modal decomposition",
            "scientific_domain": "fluid dynamics / PDE model reduction / dynamical systems",
            "hypothesis_generation_method": "Preprocessing step: produces reduced coordinates a in which SINDy enumerates hypotheses as sparse functions of a.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility judged by how well reduced-model simulations reproduce full-order dynamics (trajectories, slow-manifold behavior).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Reconstruction of full system dynamics from POD coordinates, qualitative matching of transient and limit-cycle behavior, energy captured by retained modes (singular values).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compare low-order SINDy model simulations with full DNS results (on- and off-attractor dynamics); check slow-manifold behavior.",
            "reproducibility_measures": "Standard SVD/POD methodology; specifics (number of modes retained r) must be reported for reproducibility.",
            "hallucination_prevention_method": "By reducing state to dominant coherent structures, reduces spurious small-scale noise-driven terms in identification.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied to DNS of flow past a cylinder (Re=100) with large state dimensions reduced to a few POD modes.",
            "performance_metrics": "Qualitative agreement of reduced SINDy model with DNS limit-cycle and transient behavior; successful capture of slow-manifold dynamics when off-attractor training data included.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "POD truncation may omit relevant modes needed for accurate dynamics; reduced coordinates may not be the sparsest representation for governing dynamics; requires representative snapshots including relevant transients.",
            "uuid": "e2676.7",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "DMD",
            "name_full": "Dynamic Mode Decomposition",
            "brief_description": "An equation-free linear regression method that fits a linear map between successive snapshots and connects to the Koopman operator for nonlinear dynamics when extended with nonlinear observables.",
            "citation_title": "Dynamic mode decomposition of numerical and experimental data",
            "mention_or_use": "mention",
            "system_name": "Dynamic Mode Decomposition (DMD)",
            "system_description": "Seeks a linear operator A such that X_{k+1} ≈ A X_k (or works in reduced POD coordinates); can be interpreted as a special case of the discrete-time SINDy formulation with Θ(x)=x, and links to Koopman operator theory when extended with nonlinear observables.",
            "system_type": "equation-free linear regression / Koopman-related",
            "scientific_domain": "fluid dynamics / dynamical systems / data-driven modeling",
            "hypothesis_generation_method": "Generates linear dynamical hypotheses (A matrix) from snapshot pairs via least-squares (or with modal decomposition).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Assessed by reconstructing observed snapshots and comparing modal decomposition; linear assumption plausibility depends on dynamics locality.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Reconstruction error for snapshots, modal amplitudes and eigenvalues; no sparse-term selection since DMD is typically dense.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compare DMD-predicted dynamics with measured/simulated data; connection to Koopman can be exploited with extended observables.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": "Mentioned as related but different: DMD is linear/equation-free whereas SINDy learns sparse nonlinear governing equations.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Linear model assumption limits extrapolative power; choice of observables in extended DMD is nontrivial.",
            "uuid": "e2676.8",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        },
        {
            "name_short": "SRC / OMP (refs)",
            "name_full": "Sparse Representation for Classification and Orthogonal Matching Pursuit (related sparse methods)",
            "brief_description": "Sparse classification (SRC) and greedy sparse recovery methods (OMP) are referenced as related sparse-recovery tools; they illustrate alternative sparse-selection strategies and theoretical background.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Sparse representation for classification (SRC) and orthogonal matching pursuit (OMP)",
            "system_description": "SRC uses sparse linear combinations of training examples for classification; OMP is a greedy algorithm for sparse signal recovery by iteratively selecting dictionary atoms. Both provide context/alternatives for sparse selection in the SINDy framework.",
            "system_type": "sparse-recovery / classification methods",
            "scientific_domain": "signal processing / machine learning / pattern recognition",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": null,
            "uuid": "e2676.9",
            "source_info": {
                "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
                "publication_date_yy_mm": "2015-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 2
        },
        {
            "paper_title": "Regression shrinkage and selection via the lasso",
            "rating": 2
        },
        {
            "paper_title": "Compressed sensing",
            "rating": 2
        },
        {
            "paper_title": "Dynamic mode decomposition of numerical and experimental data",
            "rating": 2
        },
        {
            "paper_title": "Numerical differentiation of noisy, nonsmooth data",
            "rating": 2
        }
    ],
    "cost": 0.0216295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Discovering governing equations from data: Sparse identification of nonlinear dynamical systems</h1>
<p>Steven L. Brunton ${ }^{1 *}$, Joshua L. Proctor ${ }^{2}$, J. Nathan Kutz ${ }^{3}$<br>${ }^{1}$ Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, United States<br>${ }^{2}$ Institute for Disease Modeling, Bellevue, WA 98004, United States<br>${ }^{3}$ Department of Applied Mathematics, University of Washington, Seattle, WA 98195, United States</p>
<h4>Abstract</h4>
<p>The ability to discover physical laws and governing equations from data is one of humankind's greatest intellectual achievements. A quantitative understanding of dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled advanced technological achievements, including aircraft, combustion engines, satellites, and electrical power. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing physical equations from measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized, time-varying, or externally forced systems.</p>
<p>Keywords- Dynamical systems, Sparse regression, System identification, Compressed sensing.</p>
<h2>1 Introduction</h2>
<p>Extracting physical laws from data is a central challenge in many diverse areas of science and engineering. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring patterns in climate, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, it is likely that data-driven discovery of dynamics will continue to play an increasingly important role in these efforts.</p>
<p>Advances in machine learning [19] and data science [29,21] have promised a renaissance in the analysis and understanding of complex data, extracting patterns in vast multimodal data that is beyond the ability of humans to grasp. However, despite the rapid development of tools to understand static data based on statistical relationships, there has been slow progress in distilling physical models of dynamic processes from big data. This has limited the ability of data</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>science models to extrapolate the dynamics beyond the attractor where they were sampled and constructed.</p>
<p>An analogy may be drawn with the discoveries of Kepler and Newton. Kepler, equipped with the most extensive and accurate planetary data of the era, developed a data-driven model for the motion of the planets, resulting in his famous elliptic orbits. However, this was an attractor based view of the world, and it did not explain the fundamental dynamic relationships that give rise to planetary orbits, or provide a model for how these bodies react when perturbed. Newton, in contrast, discovered a dynamic relationship between momentum and energy that described the underlying processes responsible for these elliptic orbits. This dynamic model may be generalized to predict behavior in regimes where no data was collected. Newton's model has proven remarkably robust for engineering design, making it possible to land a spacecraft on the moon, which would not have been possible using Kepler's model alone.</p>
<p>A seminal breakthrough by Schmidt and Lipson [4, 40] has resulted in a new approach to determine the underlying structure of a nonlinear dynamical system from data. This method uses symbolic regression (i.e., genetic programming [22]) to find nonlinear differential equations, and it balances complexity of the model, measured in the number of terms, with model accuracy. The resulting model identification realizes a long-sought goal of the physics and engineering communities to discover dynamical systems from data. However, symbolic regression is expensive, does not scale well to large systems of interest, and may be prone to overfitting unless care is taken to explicitly balance model complexity with predictive power. In [40], the Pareto front is used to find parsimonious models in a large family of candidate models.</p>
<p>In this work, we re-envision the dynamical system discovery problem from an entirely new perspective of sparse regression [42,14,18] and compressed sensing [12, 8, 9, 7, 2, 43]. In particular, we leverage the fact that most physical systems have only a few relevant terms that define the dynamics, making the governing equations sparse in a high-dimensional nonlinear function space. Before the advent of compressive sampling, and related sparsity-promoting methods, determining the few non-zero terms in a nonlinear dynamical system would have involved a combinatorial brute-force search, meaning that the methods would not scale to larger problems with Moore's law. However, powerful new theory guarantees that the sparse solution may be determined with high-probability using convex methods that do scale favorably with problem size. The resulting nonlinear model identification inherently balances model complexity (i.e., sparsity of right hand side dynamics) with accuracy, and the underlying convex optimization algorithms ensure that the method will be applicable to large-scale problems.</p>
<p>The method described here shares some similarity to the recent dynamic mode decomposition (DMD), which is a linear dynamic regression [36,39]. DMD is an example of an equation-free method [20], since it only relies on measurement data, but not on knowledge of the governing equations. Recent advances in the extended DMD have developed rigorous connections between DMD built on nonlinear observable functions and the Koopman operator theory for nonlinear dynamical systems [36,30]. However, there is currently no theory for which nonlinear observable functions to use, so that assumptions must be made on the form of the dynamical system. In contrast, the method developed here results in a sparse, nonlinear regression that automatically determines the relevant terms in the dynamical system. The trend to exploit sparsity in dynamical systems is recent but growing [38,33,25,6,35, 1]. In this work, promoting sparsity in the dynamics results in parsimonious natural laws.</p>
<h1>2 Background</h1>
<p>There is a long and fruitful history of modeling dynamics from data, resulting in powerful techniques for system identification [23]. Many of these methods arose out of the need to understand complex flexible structures, such as the Hubble space telescope or the international space station. The resulting models have been widely applied in nearly every branch of engineering and applied mathematics, most notably for model-based feedback control. However, methods for system identification typically require assumptions on the form of the model, and most often result in linear dynamics, limiting their effectiveness to small amplitude transient perturbations around a fixed point of the dynamics [15].</p>
<p>This work diverges from the seminal work on system identification, and instead builds on symbolic regression and sparse representation. In particular, symbolic regression is used to find nonlinear functions describing the relationships between variables and measured dynamics (i.e., time derivatives). Traditionally, model complexity is balanced with describing capability using parsimony arguments such as the Pareto front. Here, we use sparse representation to determine the relevant model terms in an efficient and scalable framework.</p>
<h3>2.1 Symbolic regression and machine learning</h3>
<p>Symbolic regression involves the determination of a function that relates input-output data, and it may be viewed as a form of machine learning. Typically, the function is determined using genetic programming, which is an evolutionary algorithm that builds and tests candidate functions out of simple building blocks [22]. These functions are then modified according to a set of evolutionary rules and generations of functions are tested until a pre-determined accuracy is achieved.</p>
<p>Recently, symbolic regression has been applied to data from dynamical systems, and ordinary differential equations were discovered from measurement data [40]. Because it is possible to overfit with symbolic regression and genetic programming, a parsimony constraint must be imposed, and in [40], they accept candidate equations that are at the Pareto front of complexity.</p>
<h3>2.2 Sparse representation and compressive sensing</h3>
<p>In many regression problems, only a few terms in the regression are important, and a sparse feature selection mechanism is required. For example, consider data measurements $\mathbf{y} \in \mathbb{R}^{m}$ that may be a linear combination of columns from a feature library $\Theta \in \mathbb{R}^{m \times p}$; the linear combination of columns is given by entries of the vector $\boldsymbol{\xi} \in \mathbb{R}^{p}$ so that:</p>
<p>$$
\mathbf{y}=\boldsymbol{\Theta} \boldsymbol{\xi}
$$</p>
<p>Performing a standard regression to solve for $\boldsymbol{\xi}$ will result in a solution with nonzero contributions in each element. However, if sparsity of $\boldsymbol{\xi}$ is desired, so that most of the entries are zero, then it is possible to add an $L^{1}$ regularization term to the regression, resulting in the LASSO [14, 18, 42]:</p>
<p>$$
\boldsymbol{\xi}=\underset{\boldsymbol{\xi}^{\prime}}{\operatorname{argmin}}\left|\boldsymbol{\Theta} \boldsymbol{\xi}^{\prime}-\mathbf{y}\right|<em 1="1">{2}+\lambda\left|\boldsymbol{\xi}^{\prime}\right|</em>
$$</p>
<p>The parameter $\lambda$ weights the sparsity constraint. This formulation is closely related to the compressive sensing framework, which allows for the sparse vector $\boldsymbol{\xi}$ to be determined from relatively few incoherent random measurements [12, 8, 9, 7, 2, 43]. The sparse solution $\boldsymbol{\xi}$ to Eq. 1 may also be used for sparse classification schemes, such as the sparse representation for classification (SRC) [44]. Importantly, the compressive sensing and sparse representation architectures are convex and scale well to large problems, as opposed to brute-force combinatorial alternatives.</p>
<h1>3 Sparse identification of nonlinear dynamics (SINDy)</h1>
<p>In this work, we are concerned with identifying the governing equations that underly a physical system based on data that may be realistically collected in simulations or experiments. Generically, we seek to represent the system as a nonlinear dynamical system</p>
<p>$$
\dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t))
$$</p>
<p>The vector $\mathbf{x}(t)=\left[\begin{array}{llll}x_{1}(t) &amp; x_{2}(t) &amp; \cdots &amp; x_{n}(t)\end{array}\right]^{T} \in \mathbb{R}^{n}$ represents the state of the system at time $t$, and the nonlinear function $\mathbf{f}(\mathbf{x}(t))$ represents the dynamic constraints that define the equations of motion of the system. In the following sections, we will generalize Eq. (3) to allow the dynamics $\mathbf{f}$ to vary in time, and also with respect to a set of bifurcation parameters $\boldsymbol{\mu} \in \mathbb{R}^{q}$.</p>
<p>The key observation in this paper is that for many systems of interest, the function $\mathbf{f}$ often consists of only a few terms, making it sparse in the space of possible functions. For example, the Lorenz system in Eq. (22c) has very few terms in the space of polynomial functions. Recent advances in compressive sensing and sparse regression make this viewpoint of sparsity favorable, since it is now possible to determine which right hand side terms are non-zero without performing a computationally intractable brute-force search.</p>
<p>To determine the form of the function $\mathbf{f}$ from data, we collect a time-history of the state $\mathbf{x}(t)$ and its derivative $\dot{\mathbf{x}}(t)$ sampled at a number of instances in time $t_{1}, t_{2}, \cdots, t_{m}$. These data are then arranged into two large matrices:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{X}=\left[\begin{array}{c}
\mathbf{x}^{T}\left(t_{1}\right) \
\mathbf{x}^{T}\left(t_{2}\right) \
\vdots \
\mathbf{x}^{T}\left(t_{m}\right)
\end{array}\right]=\left[\begin{array}{cccc}
x_{1}\left(t_{1}\right) &amp; x_{2}\left(t_{1}\right) &amp; \cdots &amp; x_{n}\left(t_{1}\right) \
x_{1}\left(t_{2}\right) &amp; x_{2}\left(t_{2}\right) &amp; \cdots &amp; x_{n}\left(t_{2}\right) \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
x_{1}\left(t_{m}\right) &amp; x_{2}\left(t_{m}\right) &amp; \cdots &amp; x_{n}\left(t_{m}\right)
\end{array}\right]\left\lvert\, \begin{array}{c}
\boldsymbol{\sigma} \
\frac{\boldsymbol{\alpha}}{\boldsymbol{\beta}}
\end{array}\right. \
&amp; \dot{\mathbf{X}}=\left[\begin{array}{c}
\dot{\mathbf{x}}^{T}\left(t_{1}\right) \
\dot{\mathbf{x}}^{T}\left(t_{2}\right) \
\vdots \
\dot{\mathbf{x}}^{T}\left(t_{m}\right)
\end{array}\right]=\left[\begin{array}{cccc}
\dot{x}<em 1="1">{1}\left(t</em>}\right) &amp; \dot{x<em 1="1">{2}\left(t</em>}\right) &amp; \cdots &amp; \dot{x<em 1="1">{n}\left(t</em>\right) \
\dot{x}<em 2="2">{1}\left(t</em>}\right) &amp; \dot{x<em 2="2">{2}\left(t</em>}\right) &amp; \cdots &amp; \dot{x<em 2="2">{n}\left(t</em>\right) \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
\dot{x}<em m="m">{1}\left(t</em>}\right) &amp; \dot{x<em m="m">{2}\left(t</em>}\right) &amp; \cdots &amp; \dot{x<em m="m">{n}\left(t</em>\right)
\end{array}\right] .
\end{aligned}
$$</p>
<p>Next, we construct an augmented library $\boldsymbol{\Theta}(\mathbf{X})$ consisting of candidate nonlinear functions of the columns of $\mathbf{X}$. For example, $\boldsymbol{\Theta}(\mathbf{X})$ may consist of constant, polynomial and trigonometric terms:</p>
<p>$$
\boldsymbol{\Theta}(\mathbf{X})=\left[\begin{array}{cccccccc}
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \
\mathbf{1} &amp; \mathbf{X} &amp; \mathbf{X}^{P_{2}} &amp; \mathbf{X}^{P_{3}} &amp; \cdots &amp; \sin (\mathbf{X}) &amp; \cos (\mathbf{X}) &amp; \sin (2 \mathbf{X}) &amp; \cos (2 \mathbf{X}) &amp; \cdots
\end{array}\right]
$$</p>
<p>Here, higher polynomials are denoted as $\mathbf{X}^{P_{2}}, \mathbf{X}^{P_{3}}$, etc. For example, $\mathbf{X}^{P_{2}}$ denotes the quadratic nonlinearities in the state variable $\mathbf{x}$, given by:</p>
<p>$$
\mathbf{X}^{P_{2}}=\left[\begin{array}{cccccc}
x_{1}^{2}\left(t_{1}\right) &amp; x_{1}\left(t_{1}\right) x_{2}\left(t_{1}\right) &amp; \cdots &amp; x_{2}^{2}\left(t_{1}\right) &amp; x_{2}\left(t_{1}\right) x_{3}\left(t_{1}\right) &amp; \cdots &amp; x_{n}^{2}\left(t_{1}\right) \
x_{1}^{2}\left(t_{2}\right) &amp; x_{1}\left(t_{2}\right) x_{2}\left(t_{2}\right) &amp; \cdots &amp; x_{2}^{2}\left(t_{2}\right) &amp; x_{2}\left(t_{2}\right) x_{3}\left(t_{2}\right) &amp; \cdots &amp; x_{n}^{2}\left(t_{2}\right) \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
x_{1}^{2}\left(t_{m}\right) &amp; x_{1}\left(t_{m}\right) x_{2}\left(t_{m}\right) &amp; \cdots &amp; x_{2}^{2}\left(t_{m}\right) &amp; x_{2}\left(t_{m}\right) x_{3}\left(t_{m}\right) &amp; \cdots &amp; x_{n}^{2}\left(t_{m}\right)
\end{array}\right]
$$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic of our algorithm for sparse identification of nonlinear dynamics, demonstrated on the Lorenz equations. Data is collected from measurements of the system, including a time history of the states $\mathbf{X}$ and derivatives $\dot{\mathbf{X}}$. Next, a library of nonlinear functions of the states, $\Theta(\mathbf{X})$, is constructed. This nonlinear feature library is used to find the fewest terms needed to satisfy $\dot{\mathbf{X}}=\boldsymbol{\Theta}(\mathbf{X}) \boldsymbol{\Xi}$. The few entries in the vectors of $\boldsymbol{\Xi}$, solved for by sparse regression, denote the relevant terms in the right-hand side of the dynamics. Parameter values are $\sigma=10, \beta=8 / 3, \rho=28,\left(x_{0}, y_{0}, z_{0}\right)^{T}=(-8,7,27)^{T}$. The trajectory on the Lorenz attractor is colored by the adaptive time-step required, with red requiring a smaller tilmestep.</p>
<p>Each column of $\boldsymbol{\Theta}(\mathbf{X})$ represents a candidate function for the right hand side of Eq. (3). There is tremendous freedom of choice in constructing the entries in this matrix of nonlinearities. Since we believe that only a few of these nonlinearities are active in each row of $\mathbf{f}$, we may set up a sparse regression problem to determine the sparse vectors of coefficients $\boldsymbol{\Xi}=\left[\begin{array}{llll}\boldsymbol{\xi}<em 2="2">{1} &amp; \boldsymbol{\xi}</em>\right]$ that determine which nonlinearities are active, as illustrated in Fig. 1.} &amp; \cdots &amp; \boldsymbol{\xi}_{n}\end{array</p>
<p>$$
\dot{\mathbf{X}}=\boldsymbol{\Theta}(\mathbf{X}) \boldsymbol{\Xi}
$$</p>
<p>Each column $\boldsymbol{\xi}<em k="k">{k}$ of $\boldsymbol{\Xi}$ represents a sparse vector of coefficients determining which terms are active in the right hand side for one of the row equations $\dot{\mathbf{x}}</em>$ has been determined, a model of each row of the governing equations may be constructed as follows:}=\mathbf{f}_{k}(\mathbf{x})$ in Eq. (3). Once $\boldsymbol{\Xi</p>
<p>$$
\dot{\mathbf{x}}<em k="k">{k}=\mathbf{f}</em>
$$}(\mathbf{x})=\boldsymbol{\Theta}\left(\mathbf{x}^{T}\right) \boldsymbol{\xi}_{k</p>
<p>Note that $\boldsymbol{\Theta}\left(\mathbf{x}^{T}\right)$ is a vector of symbolic functions of elements of $\mathbf{x}$, as opposed to $\boldsymbol{\Theta}(\mathbf{X})$, which is a data matrix. This results in the overall model</p>
<p>$$
\dot{\mathbf{x}}=\mathbf{f}(\mathbf{x})=\boldsymbol{\Xi}^{T}\left(\boldsymbol{\Theta}\left(\mathbf{x}^{T}\right)\right)^{T}
$$</p>
<p>We may solve for $\boldsymbol{\Xi}$ in Eq. (7) using sparse regression.</p>
<h1>3.1 Algorithm for sparse representation of dynamics with noise</h1>
<p>There are a number of algorithms to determine sparse solutions $\boldsymbol{\Xi}$ to the regression problem in Eq. (7). Each column of Eq. (7) requires a distinct optimization problem to find the sparse vector of coefficients $\boldsymbol{\xi}_{k}$ for the $k^{\text {th }}$ row equation.</p>
<p>For the examples in this paper, the matrix $\boldsymbol{\Theta}(\mathbf{X})$ has dimensions $m \times p$, where $p$ is the number of candidate nonlinear functions, and where $m \gg p$ since there are more time samples of data than there are candidate nonlinear functions. In most realistic cases, the data $\mathbf{X}$ and $\dot{\mathbf{X}}$ will be contaminated with noise so that Eq. (7) does not hold exactly. In the case that $\mathbf{X}$ is relatively clean but the derivatives $\dot{\mathbf{X}}$ are noisy, the equation becomes</p>
<p>$$
\dot{\mathbf{X}}=\boldsymbol{\Theta}(\mathbf{X}) \boldsymbol{\Xi}+\eta \mathbf{Z}
$$</p>
<p>where $\mathbf{Z}$ is a matrix of independent identically distributed Gaussian entries with zero mean, and $\eta$ is the noise magnitude. Thus we seek a sparse solution to an overdetermined system with noise.</p>
<p>The LASSO [14, 42] from statistics works well with this type of data, providing a sparse regression. However, it may be computationally expensive for very large data sets.</p>
<p>An alternative is to implement the sequential thresholded least-squares algorithm in Code (1). In this algorithm, we start with a least-squares solution for $\boldsymbol{\Xi}$ and then threshold all coefficients that are smaller than some cutoff value $\lambda$. Once the indices of the remaining non-zero coefficients are identified, we obtain another least-squares solution for $\boldsymbol{\Xi}$ onto the remaining indices. These new coefficients are again thresholded using $\lambda$, and the procedure is continued until the non-zero coefficients converge. This algorithm is computationally efficient, and it rapidly converges to a sparse solution in a small number of iterations. The algorithm also benefits from simplicity, with a single parameter $\lambda$ required to determine the degree of sparsity in $\boldsymbol{\Xi}$.</p>
<p>Depending on the noise, it may still be necessary to filter the data $\mathbf{X}$ and derivative $\dot{\mathbf{X}}$ before solving for $\boldsymbol{\Xi}$. In particular, if only the data $\mathbf{X}$ is available, and $\dot{\mathbf{X}}$ must be obtained by differentiation, then the resulting derivative matrix may have large noise magnitude. To counteract this, we use the total variation regularized derivative [10] to de-noise the derivative. An alternative would be to filter the data $\mathbf{X}$ and $\dot{\mathbf{X}}$, for example using the optimal hard threshold for singular values described in [13].</p>
<p>It is important to note that previous algorithms to identify dynamics from data have been quite sensitive to noise [40]. The algorithm in Code (1) is remarkably robust to noise, and even when velocities must be approximated from noisy data, the algorithm works surprisingly well.</p>
<p>Code 1: Sparse representation algorithm in Matlab.</p>
<div class="codehilite"><pre><span></span><code><span class="n">\#\#</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">Sparse</span><span class="w"> </span><span class="nl">regression:</span><span class="w"> </span><span class="n">sequential</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">squares</span>
<span class="n">Xi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Theta\dXdt;</span><span class="w"> </span><span class="n">\%</span><span class="w"> </span><span class="k">initial</span><span class="w"> </span><span class="nl">guess:</span><span class="w"> </span><span class="n">Least</span><span class="o">-</span><span class="n">squares</span>
<span class="n">\%</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">sparsification</span><span class="w"> </span><span class="n">knob</span><span class="p">.</span>
<span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mh">1</span><span class="o">:</span><span class="mh">10</span>
<span class="w">    </span><span class="n">smallinds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">abs</span><span class="p">(</span><span class="n">Xi</span><span class="p">)</span><span class="o">&lt;</span><span class="n">lambda</span><span class="p">);</span><span class="w"> </span><span class="n">\%</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="k">small</span><span class="w"> </span><span class="n">coefficients</span>
<span class="w">    </span><span class="n">Xi</span><span class="p">(</span><span class="n">smallinds</span><span class="p">)</span><span class="o">=</span><span class="mh">0</span><span class="p">;</span><span class="w"> </span><span class="n">\%</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">threshold</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">ind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mh">1</span><span class="o">:</span><span class="n">n</span><span class="w"> </span><span class="n">\%</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">dimension</span>
<span class="w">        </span><span class="n">biginds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">smallinds</span><span class="p">(</span><span class="o">:</span><span class="p">,</span><span class="n">ind</span><span class="p">);</span>
<span class="w">        </span><span class="n">\%</span><span class="w"> </span><span class="n">Regress</span><span class="w"> </span><span class="n">dynamics</span><span class="w"> </span><span class="n">onto</span><span class="w"> </span><span class="n">remaining</span><span class="w"> </span><span class="n">terms</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">sparse</span><span class="w"> </span><span class="n">Xi</span>
<span class="w">        </span><span class="n">Xi</span><span class="p">(</span><span class="n">biginds</span><span class="p">,</span><span class="n">ind</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Theta</span><span class="p">(</span><span class="o">:</span><span class="p">,</span><span class="n">biginds</span><span class="p">)</span><span class="n">\dXdt(:,ind);</span>
<span class="w">    </span><span class="k">end</span>
<span class="k">end</span>
</code></pre></div>

<h1>3.2 Cross-validation to determine parsimonious sparse solution on Pareto front</h1>
<p>To determine the sparsification parameter $\lambda$ in the algorithm in Code (1), it is helpful to use the concept of cross-validation from machine learning. It is always possible to hold back some test data apart from the training data to test the validity of models away from training values. In addition, it is important to consider the balance of model complexity (given by the number of nonzero coefficients in $\boldsymbol{\Xi}$ ) with the model accuracy. There is an "elbow" in the curve of accuracy vs. complexity parameterized by $\lambda$, the so-called Pareto front. This value of $\lambda$ represents a good tradeoff between complexity and accuracy, and it is similar to the approach taken in [40].</p>
<h3>3.3 Extensions and Connections</h3>
<p>There are a number of extensions to the basic theory above that generalize this approach to a broader set of problems. First, the method is generalized to a discrete-time formulation, establishing a connection with the dynamic mode decomposition (DMD). Next, high-dimensional systems obtained from discretized partial differential equations are considered, extending the method to incorporate dimensionality reduction techniques to handle big data. Finally, the sparse regression framework is modified to include bifurcation parameters, time-dependence, and external forcing.</p>
<h3>3.3.1 Discrete-time representation</h3>
<p>The aforementioned strategy may also be implemented on discrete-time dynamical systems:</p>
<p>$$
\mathbf{x}<em k="k">{k+1}=\mathbf{f}\left(\mathbf{x}</em>\right)
$$</p>
<p>There are a number of reasons to implement Eq. (11). First, many systems, such as the logistic map in Eq. (26) are inherently discrete-time systems. In addition, it may be possible to recover specific integration schemes used to advance Eq. (3). The discrete-time formulation also foregoes the calculation of a derivative from noisy data. The data collection will now involve two matrices $\mathbf{X}<em 2="2">{1}^{m-1}$ and $\mathbf{X}</em>$ :}^{m</p>
<p>The continuous-time sparse regression problem in Eq. (7) now becomes:</p>
<p>$$
\mathbf{X}<em 1="1">{2}^{m}=\boldsymbol{\Theta}\left(\mathbf{X}</em>
$$}^{m-1}\right) \boldsymbol{\Xi</p>
<p>and the function $\mathbf{f}$ is the same as in Eq. (9).
In the discrete setting in Eq. (11), and for linear dynamics, there is a striking resemblance to dynamic mode decomposition. In particular, if $\boldsymbol{\Theta}(\mathbf{x})=\mathbf{x}$, so that the dynamical system is linear, then Eq. (13) becomes</p>
<p>$$
\mathbf{X}<em 1="1">{2}^{m}=\mathbf{X}</em>}^{m-1} \boldsymbol{\Xi} \quad \Longrightarrow \quad\left(\mathbf{X<em 1="1">{2}^{m}\right)^{T}=\boldsymbol{\Xi}^{T}\left(\mathbf{X}</em>
$$}^{m-1}\right)^{T</p>
<p>This is equivalent to the DMD, which seeks a dynamic regression onto linear dynamics $\boldsymbol{\Xi}^{T}$. In particular, $\boldsymbol{\Xi}^{T}$ is $n \times n$ dimensional, which may be prohibitively large for a high-dimensional state $\mathbf{x}$. Thus, DMD identifies the dominant terms in the eigendecomposition of $\boldsymbol{\Xi}^{T}$.</p>
<h1>3.3.2 High-dimensional systems, partial differential equations, and dimensionality reduction</h1>
<p>Often, the physical system of interest may be naturally represented by a partial differential equation (PDE) in a few spatial variables. If data is collected from a numerical discretization or from experimental measurements on a spatial grid, then the state dimension $n$ may be prohibitively large. For example, in fluid dynamics, even simple two-dimensional and three-dimensional flows may require tens of thousands up to billions of variables to represent the discretized system.</p>
<p>The method described above is prohibitive for a large state dimension $n$, both because of the factorial growth of $\Theta$ in $n$ and because each of the $n$ row equations in Eq. (8) requires a separate optimization. Fortunately, many high-dimensional systems of interest evolve on a low-dimensional manifold or attractor that may be well-approximated using a dimensionally reduced low-rank basis $\boldsymbol{\Psi}[16,27]$. For example, if data $\mathbf{X}$ is collected for a high-dimensional system as in Eq. (4a), it is possible to obtain a low-rank approximation using the singular value decomposition (SVD):</p>
<p>$$
\mathbf{X}^{T}=\boldsymbol{\Psi} \boldsymbol{\Sigma} \mathbf{V}^{*}
$$</p>
<p>In this case, the state $\mathbf{x}$ may be well approximated in a truncated modal basis $\boldsymbol{\Psi}_{r}$, given by the first $r$ columns of $\boldsymbol{\Psi}$ from the SVD:</p>
<p>$$
\mathbf{x} \approx \boldsymbol{\Psi}_{r} \mathbf{a}
$$</p>
<p>where $\mathbf{a}$ is an $r$-dimensional vector of mode coefficients. We assume that this is a good approximation for a relatively low rank $r$. Thus, instead of using the original high-dimensional state $\mathbf{x}$, it is possible to obtain a sparse representation of the Galerkin projected dynamics $\mathbf{f}_{P}$ in terms of the coefficients a:</p>
<p>$$
\dot{\mathbf{a}}=\mathbf{f}_{P}(\mathbf{a})
$$</p>
<p>There are many choices for a low-rank basis, including proper orthogonal decomposition (POD) [3, 16], based on the SVD.</p>
<h3>3.3.3 External forcing, bifurcation parameters, and normal forms</h3>
<p>In practice, many real-world systems depend on parameters, and dramatic changes, or bifurcations, may occur when the parameter is varied [15, 26]. The algorithm above is readily extended to encompass these important parameterized systems, allowing for the discovery of normal forms associated with a bifurcation parameter $\mu$. First, we append $\mu$ to the dynamics:</p>
<p>$$
\begin{aligned}
&amp; \dot{\mathbf{x}}=\mathbf{f}(\mathbf{x} ; \mu) \
&amp; \dot{\mu}=\mathbf{0}
\end{aligned}
$$</p>
<p>It is then possible to identify the right hand side $\mathbf{f}(\mathbf{x} ; \boldsymbol{\mu})$ as a sparse combination of functions of components in $\mathbf{x}$ as well as the bifurcation parameter $\mu$. This idea is illustrated on two examples, the one-dimensional logistic map and the two-dimensional Hopf normal form.</p>
<p>Time-dependence, known external forcing or control $\mathbf{u}(t)$ may also be added to the dynamics:</p>
<p>$$
\begin{aligned}
\dot{\mathbf{x}} &amp; =\mathbf{f}(\mathbf{x}, \mathbf{u}(t), t) \
\dot{t} &amp; =1
\end{aligned}
$$</p>
<p>This generalization makes it possible to analyze systems that are externally forced or controlled. For example, the climate is both parameterized [26] and has external forcing, including carbon dioxide and solar radiation. The financial market presents another important example with forcing and active feedback control, in the form of regulations, taxes, and interest rates.</p>
<h1>4 Results</h1>
<p>We demonstrate the methods described in Sec. 3 on a number of canonical systems, ranging from simple linear and nonlinear damped oscillators, to noisy measurements of the fully chaotic Lorenz system, and to measurements of the unsteady fluid wake behind a circular cylinder, extending this method to nonlinear partial differential equations (PDEs) and high-dimensional data. Finally, we show that bifurcation parameters may be included in the sparse models, recovering the correct normal forms from noisy measurements of the logistic map and the Hopf normal form.</p>
<h3>4.1 Example 1: Simple illustrative systems</h3>
<h3>4.1.1 Example 1a: Two-dimensional damped oscillator (linear vs. nonlinear)</h3>
<p>In this example, we consider the two-dimensional damped harmonic oscillator with either linear or cubic dynamics, as in Eq. (20). The dynamic data and the sparse identified model are shown in Fig. 2. The correct form of the nonlinearity is obtained in each case; the augmented nonlinear library $\Theta(x)$ includes polynomials in $x$ up to fifth order. The sparse identified model and algorithm parameters are shown in the Appendix in Tables 1 and 2.</p>
<p>$$
\frac{d}{d t}\left[\begin{array}{l}
x \
y
\end{array}\right]=\left[\begin{array}{rr}
-0.1 &amp; 2 \
-2 &amp; -0.1
\end{array}\right]\left[\begin{array}{l}
x \
y
\end{array}\right] \quad \frac{d}{d t}\left[\begin{array}{l}
x \
y
\end{array}\right]=\left[\begin{array}{rr}
-0.1 &amp; 2 \
-2 &amp; -0.1
\end{array}\right]\left[\begin{array}{l}
x^{3} \
y^{3}
\end{array}\right]
$$</p>
<p>Linear System
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Cubic Nonlinearity
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Comparison of linear system (left) and system with cubic nonlinearity (right). The sparse identified system correctly identifies the form of the dynamics and accurately reproduces the phase portraits.</p>
<h1>4.1.2 Example 1b: Three-dimensional linear system</h1>
<p>A linear system with three variables and the sparse approximation are shown in Fig. 3. In this case, the dynamics are given by</p>
<p>$$
\frac{d}{d t}\left[\begin{array}{l}
x \
y \
z
\end{array}\right]=\left[\begin{array}{rrr}
-0.1 &amp; -2 &amp; 0 \
2 &amp; -0.1 &amp; 0 \
0 &amp; 0 &amp; -0.3
\end{array}\right]\left[\begin{array}{l}
x \
y \
z
\end{array}\right]
$$</p>
<p>The sparse identification algorithm correctly identifies the system in the space of polynomials up to second or third order, and the sparse model is given in Table 3. Interestingly, including polynomial terms of higher order (e.g. orders 4 or 5 ) introduces a degeneracy in the sparse identification algorithm, because linear combinations of powers of $e^{\lambda t}$ may approximate other exponential rates. This unexpected degeneracy motivates a hierarchical approach to identification, where subsequently higher order terms are included until the algorithm either converges or diverges.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Three-dimensional linear system (solid colored lines) is well-captured by sparse identified system (dashed black line).</p>
<h3>4.2 Example 2: Lorenz system (Nonlinear ODE)</h3>
<p>Here, we consider the nonlinear Lorenz system [24] to explore the identification of chaotic dynamics evolving on an attractor, shown in Fig. 1:</p>
<p>$$
\begin{aligned}
\dot{x} &amp; =\sigma(y-x) \
\dot{y} &amp; =x(\rho-z)-y \
\dot{z} &amp; =x y-\beta z
\end{aligned}
$$</p>
<p>Although these equations give rise to rich and chaotic dynamics that evolve on an attractor, there are only a few terms in the right-hand side of the equations. Figure 1 shows a schematic of how data is collected for this example, and how sparse dynamics are identified in a space of possible right-hand side functions using convex $\ell_{1}$-minimzation.</p>
<p>For this example, data is collected for the Lorenz system, and stacked into two large data matrices $\mathbf{X}$ and $\dot{\mathbf{X}}$, where each row of $\mathbf{X}$ is a snapshot of the state $\mathbf{x}$ in time, and each row of $\dot{\mathbf{X}}$</p>
<p>is a snapshot of the time derivative of the state $\dot{\mathbf{x}}$ in time. Here, the right-hand side dynamics are identified in the space of polynomials $\boldsymbol{\Theta}(\mathbf{X})$ in $(x, y, z)$ up to fifth order:</p>
<p>$$
\boldsymbol{\Theta}(\mathbf{X})=\left[\begin{array}{llllll}
\mathbf{x}(t) &amp; \mathbf{y}(t) &amp; \mathbf{z}(t) &amp; \mathbf{x}(t)^{2} &amp; \mathbf{x}(t) \mathbf{y}(t) &amp; \mathbf{x}(t) \mathbf{z}(t) &amp; \mathbf{y}(t)^{2} &amp; \mathbf{y}(t) \mathbf{z}(t) &amp; \mathbf{z}(t)^{2} &amp; \cdots &amp; \mathbf{z}(t)^{5}
\end{array}\right]
$$</p>
<p>Each column of $\boldsymbol{\Theta}(\mathbf{X})$ represents a candidate function for the right hand side of Eq. (3), and a sparse regression determines which terms are active in the dynamics, as in Fig. 1, and Eq. (7).</p>
<p>Zero-mean Gaussian measurement noise with variance $\eta$ is added to the derivative calculation to investigate the effect of noisy derivatives. The short-time $(t=0$ to $t=20)$ and long-time $(t=0$ to $t=250)$ system reconstruction is shown in Fig. 4 for two different noise values, $\eta=0.01$ and $\eta=10$. The trajectories are also shown in dynamo view in Fig. 5, and the $\ell_{2}$ error vs. time for increasing noise $\eta$ is shown in Fig. 6. Although the $\ell_{2}$ error increases for large noise values $\eta$, the form of the equations, and hence the attractor dynamics, are accurately captured. Because the system has a positive Lyapunov exponent, small differences in model coefficients or initial conditions grow exponentially, until saturation, even though the attractor may remain unchanged.</p>
<p>In the Lorenz example, the ability to capture dynamics on the attractor is more important than the ability to predict an individual trajectory, since chaos will quickly cause any small variations in initial conditions or model coefficients to diverge exponentially. As shown in Fig. 1, our sparse model identification algorithm accurately reproduces the attractor dynamics from chaotic trajectory measurements. The algorithm not only identifies the correct linear and quadratic terms in the dynamics, but it accurately determines the coefficients to within $.03 \%$ of the true values. When the derivative measurements are contaminated with noise, the correct dynamics are identified, and the attractor is well-preserved for surprisingly large noise values. When the noise is too large, the structure identification fails before the coefficients become too inaccurate.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Trajectories of the Lorenz system for short-time integration from $t=0$ to $t=20$ (top) and long-time integration from $t=0$ to $t=250$ (bottom). The full dynamics (left) are compared with the sparse identified systems (middle, right) for various additive noise. The trajectories are colored by $\Delta t$, the adaptive Runge-Kutta time step. This color is a proxy for local sensitivity.</p>
<p>For this example, we use the standard parameters $\sigma=10, \beta=8 / 3, \rho=28$, with an initial condition $\left[\begin{array}{llll}x &amp; y &amp; z\end{array}\right]^{T}=\left[\begin{array}{llll}-8 &amp; 7 &amp; 27\end{array}\right]^{T}$. Data is collected from $t=0$ to $t=100$ with a time-step of $\Delta t=0.001$.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Dynamo view of trajectories of the Lorenz system. The exact system is shown in black ( - ) and the sparse identified system is shown in the dashed red arrow ( - ).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Error vs. time for sparse identified systems generated from data with increasing sensor noise $\eta$. This error corresponds to the difference between solid black and dashed red curves in Fig. 5. Sensor noise values are $\eta \in{0.0001,0.001,0.01,0.1,1.0,10.0}$.</p>
<h1>4.3 Example 3: Fluid wake behind a cylinder (Nonlinear PDE)</h1>
<p>The Lorenz system is a low-dimensional model of more realistic high-dimensional partial differential equation (PDE) models for fluid convection in the atmosphere. Many systems of interest are governed by PDEs [38], such as weather and climate, epidemiology, and the power grid, to name a few. Each of these examples are characterized by big data, consisting of large spatially resolved measurements consisting of millions or billions of states and spanning orders of magnitude of scale in both space and time. However, many high-dimensional, real-world systems evolve on a low-dimensional attractor, making the effective dimension much smaller [16].</p>
<p>Here we generalize the sparse identification of nonlinear dynamics method to an example in fluid dynamics that typifies many of the challenges outlined above. Data is collected for the fluid flow past a cylinder at Reynolds number 100 using direct numerical simulations of the twodimensional Navier-Stokes equations [41, 11]. Then, the nonlinear dynamic relationship between the dominant coherent structures is identified from these flow field measurements with no knowledge of the governing equations.</p>
<p>The low-Reynolds number flow past a cylinder is a particularly interesting example because of its rich history in fluid mechanics and dynamical systems. It has long been theorized that turbulence may be the result of a sequence of Hopf bifurcations that occur as the Reynolds number of the flow increases [37]. The Reynolds number is a rough measure of the ratio of inertial and viscous forces, and an increasing Reynolds number may correspond, for example, to increasing flow velocity, giving rise to more rich and intricate structures in the fluid.</p>
<p>After 15 years, the first Hopf bifurcation was discovered in a fluid system, in the transition from a steady laminar wake to laminar periodic vortex shedding at Reynolds number 47 [17, 45, 32]. This discovery led to a long-standing debate about how a Hopf bifurcation, with cubic nonlinearity, can be exhibited in a Navier-Stokes fluid with quadratic nonlinearities. After 15 more years, this was finally resolved using a separation of time-scales argument and a mean-field model [31], shown in Eq. (24). It was shown that coupling between oscillatory modes and the base flow gives rise to a slow manifold (see Fig. 7, left), which results in algebraic terms that approximate cubic nonlinearities on slow timescales.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Illustration of the low-rank dynamics underlying the periodic vortex shedding behind a circular cylinder at low Reynolds number, $\operatorname{Re}=100$.</p>
<p>This example provides a compelling test-case for the proposed algorithm, since the underlying form of the dynamics took nearly three decades to uncover. Indeed, the sparse dynamics algorithm correctly identifies the on-attractor and off-attractor dynamics using quadratic nonlinearities and preserves the correct slow-manifold dynamics. It is interesting to note that when the off-attractor trajectories are not included in the system identification, the algorithm incorrectly identifies the dynamics using cubic nonlinearities, and fails to correctly identify the dynamics associated with the shift mode, which connects the mean flow to the unstable steady state.</p>
<h1>4.3.1 Direct numerical simulation</h1>
<p>The direct numerical simulation involves a fast multi-domain immersed boundary projection method [41, 11]. Four grids are used, each with a resolution of $450 \times 200$, with the finest grid having dimensions of $9 \times 4$ cylinder diameters and the largest grid having dimensions of $72 \times 32$ diameters. The finest grid has 90,000 points, and each subsequent coarser grid has 67,500 distinct points. Thus, if the state includes the vorticity at each grid point, then the state dimension is 292,500. The vorticity field on the finest grid is shown in Fig. 7. The code is non-dimensionalized so that the cylinder diameter and free-stream velocity are both equal to one: $D=1$ and $U_{\infty}=1$, respectively. The simulation time-step is $\Delta t=0.02$ non dimensional time units.</p>
<h3>4.3.2 Mean field model</h3>
<p>To develop a mean-field model for the cylinder wake, first we must reduce the dimension of the system. The proper orthogonal decomposition (POD) [16], provides a low-rank basis that is optimal in the $L^{2}$ sense, resulting in a hierarchy of orthonormal modes that are ordered by mode energy. The first two most energetic POD modes capture a significant portion of the energy; the steady-state vortex shedding is a limit cycle in these coordinates. An additional mode, called the shift mode, is included to capture the transient dynamics connecting the unstable steady state with the mean of the limit cycle [31] (i.e., the direction connecting point ' C ' to point ' B ' in Fig. 7).</p>
<p>In the three-dimensional coordinate system described above, the mean-field model for the cylinder dynamics are given by:</p>
<p>$$
\begin{aligned}
&amp; \dot{x}=\mu x-\omega y+A x z \
&amp; \dot{y}=\omega x+\mu y+A y z \
&amp; \dot{z}=-\lambda\left(z-x^{2}-y^{2}\right)
\end{aligned}
$$</p>
<p>If $\lambda$ is large, so that the $z$-dynamics are fast, then the mean flow rapidly corrects to be on the (slow) manifold $z=x^{2}+y^{2}$ given by the amplitude of vortex shedding. When substituting this algebraic relationship into Eqs. 24a and 24b, we recover the Hopf normal form on the slow manifold.</p>
<p>Remarkably, similar dynamics are discovered by the sparse dynamics algorithm, purely from data collected from simulations. The identified model coefficients, shown in Table 5, only include quadratic nonlinearities, consistent with the Navier-Stokes equations. Moreover, the transient behavior, shown in Figs. 9 and 10, is captured qualitatively for solutions that do not start on the slow manifold. When the off-attractor dynamics in Fig. 9 are not included in the training data, the model incorrectly identifies a simple Hopf normal form in $x$ and $y$ with cubic nonlinearities.</p>
<p>The data from Fig. 10 was not included in the training data, and although qualitatively similar, the identified model does not exactly reproduce the transients. Since this initial condition had twice the fluctuation energy in the $x$ and $y$ directions, the slow manifold approximation may not be valid here. Relaxing the sparsity condition, it is possible to obtain models that agree almost perfectly with the data in Figs. 8-10, although the model includes higher order nonlinearities.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Evolution of the cylinder wake trajectory in reduced coordinates. The full simulation (left) comes from direct numerical simulation of the Navier-Stokes equations, and the identified system (right) captures the dynamics on the slow manifold. Color indicates simulation time.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9: Evolution of the cylinder wake trajectory starting from a flow state initialized at the mean of the steady-state limit cycle. Both the full simulation and sparse model capture the off-attractor dynamics, characterized by rapid attraction of the trajectory onto the slow manifold.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10: This simulation corresponds to an initial condition obtained by doubling the magnitude of the limit cycle behavior. This data was not included in the training of the sparse model.</p>
<h1>4.4 Example 4: Bifurcations and Normal Forms</h1>
<p>It is also possible to identify normal forms associated with a bifurcation parameter $\mu$ by suspending it in the dynamics as a variable:</p>
<p>$$
\begin{aligned}
&amp; \dot{\mathbf{x}}=\mathbf{f}(\mathbf{x} ; \mu) \
&amp; \dot{\mu}=0
\end{aligned}
$$</p>
<p>It is then possible to identify the right hand side $\mathbf{f}(\mathbf{x} ; \mu)$ as a sparse combination of functions of components in $\mathbf{x}$ as well as the bifurcation parameter $\mu$. This idea is illustrated on two examples, the one-dimensional logistic map and the two-dimensional Hopf normal form.</p>
<h3>4.4.1 Logistic map</h3>
<p>The logistic map is a classical model that exhibits a cascade of bifurcations, leading to chaotic trajectories. The dynamics with stochastic forcing $\eta_{k}$ and parameter $\mu$ are given by</p>
<p>$$
x_{k+1}=\mu x_{k}\left(1-x_{k}\right)+\eta_{k}
$$</p>
<p>Sampling the stochastic system at ten parameter values of $\mu$, the algorithm correctly identifies the underlying parameterized dynamics, shown in Fig. 11 and Table 6.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 11: Attracting sets of the logistic map vs. the parameter $\mu$. (left) Data from stochastically forced system and (right) the sparse identified system. Data is sampled at rows indicated in red for $\mu \in{2.5,2.75,3,3.25,3.5,3.75,3.8,3.85,3.9,3.95}$. The forcing $\eta_{k}$ is Gaussian with magnitude 0.025 .</p>
<h1>4.4.2 Hopf normal form</h1>
<p>The final example illustrating the ability of the sparse dynamics method to identify parameterized normal forms is the Hopf normal form [28]. Noisy data is collected from the Hopf system</p>
<p>$$
\begin{aligned}
&amp; \dot{x}=\mu x+\omega y-A x\left(x^{2}+y^{2}\right) \
&amp; \dot{y}=-\omega x+\mu y-A y\left(x^{2}+y^{2}\right)
\end{aligned}
$$</p>
<p>for various values of the parameter $\mu$. Data is collected on the blue and red trajectories in Fig. 12, and noise is added to simulate sensor noise. The total variation derivative [10] is used to de-noise the derivative for use in the algorithm.</p>
<p>The sparse model identification algorithm correctly identifies the Hopf normal form, with model parameters given in Table 7. The noise-free model reconstruction is shown in Fig. 13. Note that with noise in the training data, although the model terms are correctly identified, the actual values of the cubic terms are off by almost $8 \%$. Collecting more training data or reducing the noise magnitude both improve the model agreement.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 12: Training data to identify Hopf normal form. Blue trajectories denote solutions that start outside of the fixed point for $\mu&lt;0$ or the limit cycle for $\mu&gt;0$, and red trajectories denote solutions that start inside of the limit cycle.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 13: Sparse model captures the Hopf normal form. Initial conditions are the same as in Fig. 12</p>
<h1>5 Discussion</h1>
<p>In summary, we have demonstrated a powerful new technique to identify nonlinear dynamical systems from data without assumptions on the form of the governing equations. This builds on prior work in symbolic regression but with innovations related to sparse regression, which allow our algorithms to scale to high-dimensional systems. We demonstrate this new method on a number of example systems exhibiting chaos, high-dimensional data with low-rank coherence, and parameterized dynamics. As shown in the Lorenz example, the ability to predict a specific trajectory may be less important than the ability to capture the attractor dynamics. The example from fluid dynamics highlights the remarkable ability of this method to extract dynamics in a fluid system that took three decades for experts in the community to explain. There are numerous fields where this method may be applied, where there is ample data and the absence of governing equations, including neuroscience, climate science, epidemiology, and financial markets. Fields that already use genetic programming, such as machine learning control for turbulent fluid systems [5,34], may also benefit. Finally, normal forms may be discovered by including parameters in the optimization, as shown on two examples. The identification of sparse governing equations and parameterizations marks a significant step toward the long-held goal of intelligent, unassisted identification of dynamical systems.</p>
<p>A number of open problems remain surrounding the dynamical systems aspects of this procedure. For example, many systems possess dynamical symmetries and conserved quantities that may alter the form of the identified dynamics. For example, the degenerate identification of a linear system in a space of high-order polynomial nonlinearities suggest a connection with nearidentity transformations and dynamic similarity. We believe that this may be a fruitful line of research. Finally, it will be important to identify which approximating function space to use based on the data available. For example, it may be possible to improve the function space to make the dynamics more sparse through subsequent coordinate transformations [15].</p>
<p>Data science is not a panacea for all problems in science and engineering, but used in the right way, it provides a principled approach to maximally leverage the data that we have and inform what new data to collect. Big data is happening all across the sciences, where the data is inherently dynamic, and where traditional approaches are prone to overfitting. Data discovery algorithms that produce parsimonious models are both rare and desirable. Data-science will only become more critical to efforts in science in engineering, where data is abundant, but physical laws remain elusive. These efforts include understanding the neural basis of cognition, extracting and predicting coherent changes in the climate, stabilizing financial markets, managing the spread of disease, and controlling turbulence,</p>
<h2>Acknowledgements</h2>
<p>We gratefully acknowledge valuable discussions with Bingni W. Brunton and Bernd R. Noack. SLB acknowledges support from the University of Washington department of Mechanical Engineering and as a Data Science Fellow in the eScience Institute (NSF, Moore-Sloan Foundation, Washington Research Foundation). JLP thanks Bill and Melinda Gates for their active support of the Institute for Disease Modeling and their sponsorship through the Global Good Fund. JNK acknowledges support from the U.S. Air Force Office of Scientific Research (FA9550-09-0174).</p>
<h1>References</h1>
<p>[1] Zhe Bai, Thakshila Wimalajeewa, Zachary Berger, Guannan Wang, Mark Glauser, and Pramod K Varshney. Lowdimensional approach for reconstruction of airfoil data via compressive sensing. AIAA Journal, pages 1-14, 2014.
[2] R. G. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24(4):118-120, 2007.
[3] G. Berkooz, P. Holmes, and J. L. Lumley. The proper orthogonal decomposition in the analysis of turbulent flows. Annual Review of Fluid Mechanics, 23:539-575, 1993.
[4] Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 104(24):9943-9948, 2007.
[5] S. L. Brunton and B. R. Noack. Closed-loop turbulence control: Progress and challenges. Applied Mechanics Reviews, 67:050801-1-050801-48, 2015.
[6] S. L. Brunton, J. H. Tu, I. Bright, and J. N. Kutz. Compressive sensing and low-rank libraries for classification of bifurcation regimes in nonlinear dynamical systems. SIAM Journal on Applied Dynamical Systems, 13(4):1716-1732, 2014.
[7] E. J. Candès. Compressive sensing. Proc. International Congress of Mathematics, 2006.
[8] E. J. Candès, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2):489-509, 2006.
[9] E. J. Candès, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications in Pure and Applied Mathematics, 59(8):1207-1223, 2006.
[10] Rick Chartrand. Numerical differentiation of noisy, nonsmooth data. ISRN Applied Mathematics, 2011, 2011.
[11] T. Colonius and K. Taira. A fast immersed boundary method using a nullspace approach and multi-domain farfield boundary conditions. Computer Methods in Applied Mechanics and Engineering, 197:2131-2146, 2008.
[12] D. L. Donoho. Compressed sensing. IEEE Trans.Information Theory, 52(4):1289-1306, 2006.
[13] M. Gavish and D. L. Donoho. The optimal hard threshold for singular values is $4 / \sqrt{3}$. ArXiv e-prints, 2014.
[14] Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani. The elements of statistical learning, volume 2. Springer, 2009.
[15] P. Holmes and J. Guckenheimer. Nonlinear oscillations, dynamical systems, and bifurcations of vector fields, volume 42 of Applied Mathematical Sciences. Springer-Verlag, Berlin, 1983.
[16] P. J. Holmes, J. L. Lumley, G. Berkooz, and C. W. Rowley. Turbulence, coherent structures, dynamical systems and symmetry. Cambridge Monographs in Mechanics. Cambridge University Press, Cambridge, England, 2nd edition, 2012.
[17] C. P. Jackson. A finite-element study of the onset of vortex shedding in flow past variously shaped bodies. Journal of Fluid Mechanics, 182:23-45, 1987.
[18] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Springer, 2013.
[19] MI Jordan and TM Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255-260, 2015.
[20] I. G. Kevrekidis, C. W. Gear, J. M. Hyman, P. G. Kevrekidis, O. Runborg, and C. Theodoropoulos. Equationfree, coarse-grained multiscale computation: Enabling microscopic simulators to perform system-level analysis. Communications in Mathematical Science, 1(4):715-762, 2003.
[21] Muin J Khoury and John PA Ioannidis. Medicine. big data meets public health. Science, 346(6213):1054-1055, 2014.
[22] J. R Koza. Genetic programming: on the programming of computers by means of natural selection, volume 1. MIT press, 1992.
[23] L. Ljung. System Identification: Theory for the User. Prentice Hall, 1999.
[24] Edward N Lorenz. Deterministic nonperiodic flow. J. Atmos. Sciences, 20(2):130-141, 1963.
[25] Alan Mackey, Hayden Schaeffer, and Stanley Osher. On the compressive spectral method. Multiscale Modeling \&ramp; Simulation, 12(4):1800-1827, 2014.
[26] Andrew J Majda, Christian Franzke, and Daan Crommelin. Normal forms for reduced stochastic climate models. Proceedings of the National Academy of Sciences, 106(10):3649-3653, 2009.
[27] Andrew J Majda and John Harlim. Information flow between subspaces of complex dynamical systems. Proceedings of the National Academy of Sciences, 104(23):9558-9563, 2007.
[28] Jerrold E Marsden and Marjorie McCracken. The Hopf bifurcation and its applications, volume 19. Springer-Verlag, 1976.
[29] Vivien Marx. Biology: The big challenges of big data. Nature, 498(7453):255-260, 2013.</p>
<p>[30] Igor Mezic. Analysis of fluid flows via spectral properties of the koopman operator. Annual Review of Fluid Mechanics, 45:357-378, 2013.
[31] B. R. Noack, K. Afanasiev, M. Morzynski, G. Tadmor, and F. Thiele. A hierarchy of low-dimensional models for the transient and post-transient cylinder wake. Journal of Fluid Mechanics, 497:335-363, 2003.
[32] DJ Olinger and KR Sreenivasan. Nonlinear dynamics of the wake of an oscillating cylinder. Physical review letters, 60(9):797, 1988.
[33] Vidvuds Ozoliņš, Rongjie Lai, Russel Caflisch, and Stanley Osher. Compressed modes for variational problems in mathematics and physics. Proceedings of the National Academy of Sciences, 110(46):18368-18373, 2013.
[34] V. Parezanovic, J.-C. Laurentie, T. Duriez, C. Fourment, J. Delville, J.-P. Bonnet, L. Cordier, B. R. Noack, M. Segond, M. Abel, T. Shaqarin, and S. L. Brunton. Mixing layer manipulation experiment - from periodic forcing to machine learning closed-loop control. Journal Flow Turbulence and Combustion, 94(1):155-173, 2015.
[35] J. L. Proctor, S. L. Brunton, B. W. Brunton, and J. N. Kutz. Exploiting sparsity and equation-free architectures in complex systems (invited review). The European Physical Journal Special Topics, 223(13):2665-2684, 2014.
[36] C. W. Rowley, I. Mezić, S. Bagheri, P. Schlatter, and D.S. Henningson. Spectral analysis of nonlinear flows. J. Fluid Mech., 645:115-127, 2009.
[37] D. Ruelle and F. Takens. On the nature of turbulence. Communications in Mathematical Physics, 20:167-192, 1971.
[38] H. Schaeffer, R. Caflisch, C. D. Hauck, and S. Osher. Sparse dynamics for partial differential equations. Proceedings of the National Academy of Sciences USA, 110(17):6634-6639, 2013.
[39] P. J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics, 656:528, August 2010.
[40] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):8185, 2009.
[41] K. Taira and T. Colonius. The immersed boundary method: a projection approach. Journal of Computational Physics, 225(2):2118-2137, 2007.
[42] R. Tibshirani. Regression shrinkage and selection via the lasso. J. of the Royal Statistical Society B, pages 267-288, 1996.
[43] J. A. Tropp and A. C. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on Information Theory, 53(12):4655-4666, 2007.
[44] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Trans. on Pattern Analysis and Machine Intelligence, 31(2):210-227, 2009.
[45] Z. Zebib. Stability of viscous flow past a circular cylinder. Journal of Engineering Mathematics, 21:155-165, 1987.</p>
<h1>Appendix</h1>
<p>Table 1: Damped harmonic oscillator with linear terms.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">'</th>
<th style="text-align: center;">'xdot'</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">'ydot'</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">'1'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'x'</td>
<td style="text-align: center;">$[-0.1015]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$[-1.9990]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">'y'</td>
<td style="text-align: center;">[ 2.0027]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$[-0.0994]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">'xx'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'yy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxx'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'yyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxxx'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxxy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xyyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'yyyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxxxx'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxxyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxxyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'xxyyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
<tr>
<td style="text-align: center;">'yyyyy'</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
<td style="text-align: center;">[</td>
<td style="text-align: center;">0]</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding author. Tel.: +1 (609)-921-6415.</li>
</ul>
<p>E-mail address: sbrunton@uw.edu (S.L. Brunton).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
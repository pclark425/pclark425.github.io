<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9225 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9225</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9225</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-263152801</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.15630v4.pdf" target="_blank">NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS</a></p>
                <p><strong>Paper Abstract:</strong> Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench 1 , comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University’s prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9225.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9225.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, high-performing OpenAI LLM evaluated across NLPBench under multiple prompt presentation formats (zero-shot, few-shot, CoT, ToT, system prompts, self-consistency). Shows top performance among tested models and sensitivity to advanced prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLPBench (college-level NLP exam questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>378 college-level NLP questions (multiple choice, short answer, math), including questions with shared context requiring multi-turn interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Multiple prompt formats tested: zero-shot (ZS) baseline prompts; few-shot (FS) with added example QA pairs; system prompts (SYS); chain-of-thought (CoT) implemented as both two-stage 'let's think step by step' for short answer and a formatted template for multiple-choice/math; tree-of-thought (ToT) (few-shot); self-consistency (SC) applied to multiple-choice. Temperature=1 for final answers, 0 for intermediate CoT/ToT steps. Many format combinations evaluated (e.g., ZS, FS, ZS+CoT, FS+CoT, ZS+CoT+SYS, FS+CoT+SYS, FS+ToT, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared primarily to zero-shot baseline; also compared ZS vs FS, CoT vs non-CoT, ToT vs non-ToT, with/without system prompt, and with/without self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>overall accuracy (paper reported): ZS (Orig.) overall ~68.08% (aggregated across multiple-choice, short answer, math; table reports Orig. overall Acc = 68.08%). GPT-4 achieves top average performance vs other models under many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Few-shot prompting produced only modest gains (~~3% average improvement reported for GPT-4 across models) and many of GPT-4's best results were from zero-shot; advanced prompting strategies (CoT, ToT) often did not outperform the zero-shot baseline and in many scenarios slightly reduced performance for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper reports ~3% ordinary improvement when adopting few-shot prompting for GPT-4 (and other top APIs); no large positive effect for CoT/ToT — advanced prompting often produced neutral or negative changes relative to ZS. (Exact numeric deltas vary by subtask; paper emphasizes no consistent large positive effect.)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that advanced prompting efficacy depends on a model's prompt-following ability and internal reasoning capacity; GPT-4 can process complex prompts but still sometimes underperforms its ZS baseline because extra prompt content can introduce noise or constrain reasoning. Few-shot can help when examples are well-chosen and task is domain-specific, but ill-chosen examples add noise. Self-consistency helps under few-shot but can harm zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation via OpenAI API through AutoGen; same seed across runs; temperature 1 for answers, 0 for intermediate CoT/ToT steps; CoT two-stage for short answers; multiple prompt templates (Appendix A.2). Self-consistency applied for multiple-choice set only. Context-based questions evaluated per sub-question; final-answer accuracy used for metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9225.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9225.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI API model evaluated on NLPBench across multiple prompt presentation formats; shows modest responsiveness to CoT and few-shot, with mixed results for advanced strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLPBench (college-level NLP exam questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same NLPBench dataset; multi-format evaluation (MC, short answer, math; with/without context).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot and few-shot prompts; system prompt variants; CoT (two-stage for short answer, templates for MC/math); ToT evaluated in some configurations; self-consistency (SC) for MC set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZS vs FS; ZS vs ZS+CoT; FS vs FS+CoT; with/without SYS; with/without SC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports GPT-3.5 Orig. (zero-shot) aggregated overall accuracy ~51.06% (table-level aggregated values); on MC subset specific ZS accuracies reported (see paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT provided a slight performance increase specifically for GPT-3.5 on some settings; few-shot provided only modest (~3%) improvement on average. Self-consistency improved results under few-shot but sometimes reduced performance under zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small positive effect from CoT for GPT-3.5 (described as 'slight'); few-shot ~+3% average; SC beneficial under FS but harmful under ZS (no single large effect size given beyond these descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT benefits when the model can follow prompts and self-evaluate; GPT-3.5's prompt-following allowed limited gains. SC depends on FS constraining generation; without FS, SC can amplify incorrect diverse answers. Advanced strategies require reliable prompt adherence and internal evaluation capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Accessed via AutoGen/OpenAI API; temperature settings as above; CoT intermediate steps generated with temperature 0; SC applied only to MC; prompt templates provided in Appendix A.2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9225.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9225.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM-2 API-model evaluated on NLPBench; shows high text-relevance metric scores but comparatively low task accuracy, demonstrating potential metric mismatch induced by answer presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLPBench (college-level NLP exam questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same dataset with emphasis on free-response and multi-turn context questions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ZS/FS, with/without SYS, CoT and ToT variants; self-consistency evaluated on MC set. Prompting via Google PaLM generate_text endpoint; temperature settings consistent with other models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZS vs FS; CoT vs non-CoT; ToT and SYS variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported aggregate accuracy for some PaLM-2 configurations in Table 3 (e.g., Orig./ZS overall accuracy approx ~43.08% in one reported row); however PaLM-2 scored notably higher on text-relevance metrics (BLEU/ROUGE-L/CIDEr) compared to others while achieving low accuracy on ground-truth correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Although PaLM-2 obtains higher BLEU/ROUGE/CIDEr scores under many prompt variants (Table 5), its task accuracy remains low relative to top models (e.g., GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Discrepancy effect: presentation that leads to repetition or verbose concept coverage can inflate text-relevance metrics (BLEU/ROUGE/CIDEr) by increasing surface overlap, yet this does not correlate with correct answers; no consistent positive boost from CoT/FS shown.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>PaLM-2 tends to repeat and produce verbose descriptions that overlap lexically with reference answers, inflating relevance metrics even when reasoning or logical connections are wrong. Thus presentation that encourages reiteration or verbose coverage can misleadingly improve automated relevance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Accessed via Google PaLM generate_text; same seed and temperature settings as other API models; text-relevance measured with BLEU, ROUGE-L, CIDEr on short-answer outputs (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9225.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9225.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAMA-2-13b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLaMA-2 model (13B parameters) evaluated on NLPBench; shows sensitivity to prompt length and advanced prompting methods, often suffering performance drops with long or complex prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLPBench (college-level NLP exam questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same dataset (MC, short answer, math; with/without context multi-turn).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ZS and FS, with/without system prompt, CoT and ToT variants; CoT implemented as templates; FS sometimes caused prompt length issues for LLAMA-2-13b.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZS vs FS; CoT vs non-CoT; SYS vs non-SYS; ToT attempted in FS in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate performance low relative to larger models; many advanced prompting configurations caused marked performance declines. Example aggregated numbers in Table 3 show low overall accuracies (e.g., several LLAMA-2-13b rows report overall accuracies in the ~17–33% region depending on configuration; some configurations could not complete due to context length).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Few-shot sometimes improved LLAMA-2-13b when additional examples provided domain knowledge, but increases were inconsistent and could cause context-window overflows leading to large drops in performance. Advanced prompting strategies (CoT, ToT, SYS) often decreased performance for LLAMA-2-13b.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper reports pronounced declines for smaller model when using advanced strategies or long few-shot prompts; exact effect varies by setting but described as 'significant performance drop' when context length exceeded or when prompt-following ability was limited.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller models like LLAMA-2-13b have limited prompt-following capability and smaller context windows; complex/long prompts (few-shot, CoT, ToT, SYS) introduce extraneous information/noise and can exceed context length, causing degraded representations and worse answers.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Open-source access via vLLM endpoints; some experiments were incomplete due to exceeding context length or prompt-following failures (noted in Table 3 as '-') ; temperature and seeds set as for other models; CoT formatting used but often not followed correctly by model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9225.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9225.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAMA-2-70b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLaMA-2 model (70B parameters) evaluated on NLPBench; performs substantially better than 13B but still behind GPT-4, and benefits from few-shot in some categories while advanced strategies show inconsistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NLPBench (college-level NLP exam questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-format, context-aware NLPBench.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ZS/FS with/without SYS; CoT templates; FS+ToT attempted; SC for MC set. FS sometimes reduced category errors and produced more balanced error distributions for LLAMA-2-70b.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>ZS vs FS; CoT vs non-CoT; SYS inclusion; SC for MC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3 reports LLAMA-2-70b aggregated overall accuracies that are higher than 13b but below GPT-4 (example rows show overall accuracies in ~31–36% range for many configurations, though values vary by prompt setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Few-shot prompting consistently reduced error rates across categories for LLAMA-2-70b and yielded more balanced errors; however CoT/ToT did not consistently improve overall accuracy. System prompts sometimes combined with FS increased error in particular categories while decreasing it in others.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper reports category-specific reductions (e.g., a 32% decrease in pdda category error for LLAMA-2-70b when transitioning from zero-shot to few-shot in one analysis), but no consistent large global uplift from CoT/ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>70B model benefits from FS providing domain examples (reducing some category errors), but advanced prompting still requires robust prompt-following and internal verification; adding system prompts can both mitigate and introduce errors depending on category and sample selection. Context-length and prompt quality remain limiting factors.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Open-source endpoint via vLLM; experiments included ZS, FS, SYS, CoT, ToT combinations; some prompt combinations led to exceeding context length and incomplete results; error breakdowns by NLP category provided in Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9225",
    "paper_id": "paper-263152801",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large, high-performing OpenAI LLM evaluated across NLPBench under multiple prompt presentation formats (zero-shot, few-shot, CoT, ToT, system prompts, self-consistency). Shows top performance among tested models and sensitivity to advanced prompting.",
            "citation_title": "NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "NLPBench (college-level NLP exam questions)",
            "task_description": "378 college-level NLP questions (multiple choice, short answer, math), including questions with shared context requiring multi-turn interaction.",
            "presentation_format": "Multiple prompt formats tested: zero-shot (ZS) baseline prompts; few-shot (FS) with added example QA pairs; system prompts (SYS); chain-of-thought (CoT) implemented as both two-stage 'let's think step by step' for short answer and a formatted template for multiple-choice/math; tree-of-thought (ToT) (few-shot); self-consistency (SC) applied to multiple-choice. Temperature=1 for final answers, 0 for intermediate CoT/ToT steps. Many format combinations evaluated (e.g., ZS, FS, ZS+CoT, FS+CoT, ZS+CoT+SYS, FS+CoT+SYS, FS+ToT, etc.).",
            "comparison_format": "Compared primarily to zero-shot baseline; also compared ZS vs FS, CoT vs non-CoT, ToT vs non-ToT, with/without system prompt, and with/without self-consistency.",
            "performance": "overall accuracy (paper reported): ZS (Orig.) overall ~68.08% (aggregated across multiple-choice, short answer, math; table reports Orig. overall Acc = 68.08%). GPT-4 achieves top average performance vs other models under many settings.",
            "performance_comparison": "Few-shot prompting produced only modest gains (~~3% average improvement reported for GPT-4 across models) and many of GPT-4's best results were from zero-shot; advanced prompting strategies (CoT, ToT) often did not outperform the zero-shot baseline and in many scenarios slightly reduced performance for GPT-4.",
            "format_effect_size": "Paper reports ~3% ordinary improvement when adopting few-shot prompting for GPT-4 (and other top APIs); no large positive effect for CoT/ToT — advanced prompting often produced neutral or negative changes relative to ZS. (Exact numeric deltas vary by subtask; paper emphasizes no consistent large positive effect.)",
            "explanation_or_hypothesis": "Authors hypothesize that advanced prompting efficacy depends on a model's prompt-following ability and internal reasoning capacity; GPT-4 can process complex prompts but still sometimes underperforms its ZS baseline because extra prompt content can introduce noise or constrain reasoning. Few-shot can help when examples are well-chosen and task is domain-specific, but ill-chosen examples add noise. Self-consistency helps under few-shot but can harm zero-shot.",
            "null_or_negative_result": true,
            "experimental_details": "Evaluation via OpenAI API through AutoGen; same seed across runs; temperature 1 for answers, 0 for intermediate CoT/ToT steps; CoT two-stage for short answers; multiple prompt templates (Appendix A.2). Self-consistency applied for multiple-choice set only. Context-based questions evaluated per sub-question; final-answer accuracy used for metrics.",
            "uuid": "e9225.0",
            "source_info": {
                "paper_title": "NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5 (gpt-3.5-turbo)",
            "brief_description": "OpenAI API model evaluated on NLPBench across multiple prompt presentation formats; shows modest responsiveness to CoT and few-shot, with mixed results for advanced strategies.",
            "citation_title": "NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_size": null,
            "task_name": "NLPBench (college-level NLP exam questions)",
            "task_description": "Same NLPBench dataset; multi-format evaluation (MC, short answer, math; with/without context).",
            "presentation_format": "Zero-shot and few-shot prompts; system prompt variants; CoT (two-stage for short answer, templates for MC/math); ToT evaluated in some configurations; self-consistency (SC) for MC set.",
            "comparison_format": "ZS vs FS; ZS vs ZS+CoT; FS vs FS+CoT; with/without SYS; with/without SC.",
            "performance": "Paper reports GPT-3.5 Orig. (zero-shot) aggregated overall accuracy ~51.06% (table-level aggregated values); on MC subset specific ZS accuracies reported (see paper).",
            "performance_comparison": "CoT provided a slight performance increase specifically for GPT-3.5 on some settings; few-shot provided only modest (~3%) improvement on average. Self-consistency improved results under few-shot but sometimes reduced performance under zero-shot.",
            "format_effect_size": "Small positive effect from CoT for GPT-3.5 (described as 'slight'); few-shot ~+3% average; SC beneficial under FS but harmful under ZS (no single large effect size given beyond these descriptions).",
            "explanation_or_hypothesis": "CoT benefits when the model can follow prompts and self-evaluate; GPT-3.5's prompt-following allowed limited gains. SC depends on FS constraining generation; without FS, SC can amplify incorrect diverse answers. Advanced strategies require reliable prompt adherence and internal evaluation capacity.",
            "null_or_negative_result": true,
            "experimental_details": "Accessed via AutoGen/OpenAI API; temperature settings as above; CoT intermediate steps generated with temperature 0; SC applied only to MC; prompt templates provided in Appendix A.2.",
            "uuid": "e9225.1",
            "source_info": {
                "paper_title": "NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PaLM-2",
            "name_full": "PaLM 2",
            "brief_description": "Google's PaLM-2 API-model evaluated on NLPBench; shows high text-relevance metric scores but comparatively low task accuracy, demonstrating potential metric mismatch induced by answer presentation.",
            "citation_title": "NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS",
            "mention_or_use": "use",
            "model_name": "PaLM-2",
            "model_size": null,
            "task_name": "NLPBench (college-level NLP exam questions)",
            "task_description": "Same dataset with emphasis on free-response and multi-turn context questions.",
            "presentation_format": "ZS/FS, with/without SYS, CoT and ToT variants; self-consistency evaluated on MC set. Prompting via Google PaLM generate_text endpoint; temperature settings consistent with other models.",
            "comparison_format": "ZS vs FS; CoT vs non-CoT; ToT and SYS variants.",
            "performance": "Reported aggregate accuracy for some PaLM-2 configurations in Table 3 (e.g., Orig./ZS overall accuracy approx ~43.08% in one reported row); however PaLM-2 scored notably higher on text-relevance metrics (BLEU/ROUGE-L/CIDEr) compared to others while achieving low accuracy on ground-truth correctness.",
            "performance_comparison": "Although PaLM-2 obtains higher BLEU/ROUGE/CIDEr scores under many prompt variants (Table 5), its task accuracy remains low relative to top models (e.g., GPT-4).",
            "format_effect_size": "Discrepancy effect: presentation that leads to repetition or verbose concept coverage can inflate text-relevance metrics (BLEU/ROUGE/CIDEr) by increasing surface overlap, yet this does not correlate with correct answers; no consistent positive boost from CoT/FS shown.",
            "explanation_or_hypothesis": "PaLM-2 tends to repeat and produce verbose descriptions that overlap lexically with reference answers, inflating relevance metrics even when reasoning or logical connections are wrong. Thus presentation that encourages reiteration or verbose coverage can misleadingly improve automated relevance scores.",
            "null_or_negative_result": true,
            "experimental_details": "Accessed via Google PaLM generate_text; same seed and temperature settings as other API models; text-relevance measured with BLEU, ROUGE-L, CIDEr on short-answer outputs (Table 5).",
            "uuid": "e9225.2",
            "source_info": {
                "paper_title": "NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLAMA-2-13b",
            "name_full": "LLaMA 2 (13B)",
            "brief_description": "Open-source LLaMA-2 model (13B parameters) evaluated on NLPBench; shows sensitivity to prompt length and advanced prompting methods, often suffering performance drops with long or complex prompts.",
            "citation_title": "NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS",
            "mention_or_use": "use",
            "model_name": "LLAMA-2",
            "model_size": "13B",
            "task_name": "NLPBench (college-level NLP exam questions)",
            "task_description": "Same dataset (MC, short answer, math; with/without context multi-turn).",
            "presentation_format": "ZS and FS, with/without system prompt, CoT and ToT variants; CoT implemented as templates; FS sometimes caused prompt length issues for LLAMA-2-13b.",
            "comparison_format": "ZS vs FS; CoT vs non-CoT; SYS vs non-SYS; ToT attempted in FS in some configurations.",
            "performance": "Aggregate performance low relative to larger models; many advanced prompting configurations caused marked performance declines. Example aggregated numbers in Table 3 show low overall accuracies (e.g., several LLAMA-2-13b rows report overall accuracies in the ~17–33% region depending on configuration; some configurations could not complete due to context length).",
            "performance_comparison": "Few-shot sometimes improved LLAMA-2-13b when additional examples provided domain knowledge, but increases were inconsistent and could cause context-window overflows leading to large drops in performance. Advanced prompting strategies (CoT, ToT, SYS) often decreased performance for LLAMA-2-13b.",
            "format_effect_size": "Paper reports pronounced declines for smaller model when using advanced strategies or long few-shot prompts; exact effect varies by setting but described as 'significant performance drop' when context length exceeded or when prompt-following ability was limited.",
            "explanation_or_hypothesis": "Smaller models like LLAMA-2-13b have limited prompt-following capability and smaller context windows; complex/long prompts (few-shot, CoT, ToT, SYS) introduce extraneous information/noise and can exceed context length, causing degraded representations and worse answers.",
            "null_or_negative_result": true,
            "experimental_details": "Open-source access via vLLM endpoints; some experiments were incomplete due to exceeding context length or prompt-following failures (noted in Table 3 as '-') ; temperature and seeds set as for other models; CoT formatting used but often not followed correctly by model.",
            "uuid": "e9225.3",
            "source_info": {
                "paper_title": "NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLAMA-2-70b",
            "name_full": "LLaMA 2 (70B)",
            "brief_description": "Open-source LLaMA-2 model (70B parameters) evaluated on NLPBench; performs substantially better than 13B but still behind GPT-4, and benefits from few-shot in some categories while advanced strategies show inconsistent gains.",
            "citation_title": "NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS",
            "mention_or_use": "use",
            "model_name": "LLAMA-2",
            "model_size": "70B",
            "task_name": "NLPBench (college-level NLP exam questions)",
            "task_description": "Same multi-format, context-aware NLPBench.",
            "presentation_format": "ZS/FS with/without SYS; CoT templates; FS+ToT attempted; SC for MC set. FS sometimes reduced category errors and produced more balanced error distributions for LLAMA-2-70b.",
            "comparison_format": "ZS vs FS; CoT vs non-CoT; SYS inclusion; SC for MC.",
            "performance": "Table 3 reports LLAMA-2-70b aggregated overall accuracies that are higher than 13b but below GPT-4 (example rows show overall accuracies in ~31–36% range for many configurations, though values vary by prompt setting).",
            "performance_comparison": "Few-shot prompting consistently reduced error rates across categories for LLAMA-2-70b and yielded more balanced errors; however CoT/ToT did not consistently improve overall accuracy. System prompts sometimes combined with FS increased error in particular categories while decreasing it in others.",
            "format_effect_size": "Paper reports category-specific reductions (e.g., a 32% decrease in pdda category error for LLAMA-2-70b when transitioning from zero-shot to few-shot in one analysis), but no consistent large global uplift from CoT/ToT.",
            "explanation_or_hypothesis": "70B model benefits from FS providing domain examples (reducing some category errors), but advanced prompting still requires robust prompt-following and internal verification; adding system prompts can both mitigate and introduce errors depending on category and sample selection. Context-length and prompt quality remain limiting factors.",
            "null_or_negative_result": true,
            "experimental_details": "Open-source endpoint via vLLM; experiments included ZS, FS, SYS, CoT, ToT combinations; some prompt combinations led to exceeding context length and incomplete results; error breakdowns by NLP category provided in Figure 4.",
            "uuid": "e9225.4",
            "source_info": {
                "paper_title": "NLPB ENCH : E VALUATING L ARGE L ANGUAGE M OD - ELS ON S OLVING NLP P ROBLEMS",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.01326525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS
19 Oct 2023</p>
<p>Linxin Song 
Waseda University</p>
<p>University of Tokyo</p>
<p>Jieyu Zhang 
University of Washington</p>
<p>Lechao Cheng 
Zhejiang Lab</p>
<p>Pengyuan Zhou 
University of Science and Technology of China</p>
<p>Tianyi Zhou 
University of Maryland</p>
<p>Irene Li 
University of Tokyo</p>
<p>NLPBENCH: EVALUATING LARGE LANGUAGE MOD-ELS ON SOLVING NLP PROBLEMS
19 Oct 20236B45AA9F9D51FEE5D3CA14912C3A9724arXiv:2309.15630v4[cs.CL]
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP).Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs.To fill the gap in this area, we present a unique benchmarking dataset, NLPBench 1 , comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams.NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math.Our evaluation, centered on LLMs such as GPT-3.5/4,PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-ofthought (CoT) and tree-of-thought (ToT).Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b).Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting results.</p>
<p>INTRODUCTION</p>
<p>Over the past decade, the evolution of natural language processing (NLP) has led to the emergence of large language models (LLMs) (Brown et al., 2020;OpenAI., 2022;2023;Zhang et al., 2023b;Touvron et al., 2023a;Zhang et al., 2023a;Gao et al., 2023b;Liu et al., 2023;Gao et al., 2023a).They consistently showcase exceptional performance across a spectrum of benchmarks that require human-level problem-solving or question-answering skills, including areas such as algebra (Lu et al., 2022;2021b;2023a;Cobbe et al., 2021), logic (Zhong et al., 2023;Chen et al., 2023), language (Huang et al., 2023), and science (Wang et al., 2023), some of these even challenges for well-educated individuals.As the most notable achievement in the field of NLP, a compelling yet unresolved question of LLMs naturally raises: Can LLMs adeptly answer questions about NLP?</p>
<p>To fill the gap of evaluating LLMs on NLP-related topics, we introduce a novel benchmark, Natural Language Processing Benchmark, referred to as NLPBench.Our NLPBench contains 378 NLPrelated questions in the fields of Language Modeling and Syntax Parsing, Semantics and Logic, Pragmatics, Discourse, Dialogue and Applications, Information Retrieval and Topic Modeling, Artificial Intelligence and Other Topics.To evaluate the multi-turn communication problem-solving ability of different NLP topics, we introduce questions with context, consisting of multiple related questions that share the same public information.Our dataset also includes multiple choice, free response short answer, and math questions to evaluate LLMs from all perspectives.All questions in our dataset are manually extracted from Yale University's previous final exams.Figure 1 shows some example questions featured in our dataset.</p>
<p>We direct our evaluation towards five representative LLMs, GPT-3.5/4 (OpenAI., 2022;2023), PaLM-2 (Anil et al., 2023), and both the 13b and 70b versions of LLAMA-2 (Touvron et al., 2023b).Our study incorporates a variety of advanced prompting strategies, including the chain-of-thought (CoT, Wei et al. (2022)) and tree-of-thought (ToT, Yao et al. (2023)), and the argumentation method like self-consistency.These advanced prompting strategies have demonstrated notable success in past benchmarks by directing the LLMs' response processes.They guide LLMs with specific examples, encouraging the generation of step-by-step solutions that lead to deeper problem consideration (Wei et al., 2022;Wang et al., 2022;Zhou et al., 2022;Huang et al., 2022).However, the efficacy of these improvements can be compromised by the complexity of the question, the depth of required knowledge, and the LLMs' ability to follow prompts.Our experiments indicate that fewshot prompting typically results in modest enhancements.Moreover, advanced prompting strategies are not universally effective.When an LLM is constrained (for instance, by having insufficient parameters to develop a robust representation) or when the breadth of required knowledge expands, the LLM might not always recall accurate information from its previously stored knowledge.In our research, we observe that advanced prompting strategies can inadvertently hamper the performance of LLMs.This is due to the introduction of extraneous noise unrelated to the given questions, sometimes causing a pronounced decline in the performance of smaller LLMs, such as LLAMA-2 (13b).Such nuances have remained unexplored in earlier benchmarks because of the limited scope of question complexity and prompt length.</p>
<p>Apart from examining the effectiveness of various prompting strategies, we also conducted a manual assessment of NLP problem-solving capabilities in two dimensions: (1) error rate statistics across different NLP categories and (2) an evaluation of problem-solving abilities from a human expert's viewpoint.For the first dimension, we compiled the error rates for each NLP category, segmented by individual LLMs and their associated prompting strategies.Our findings indicate that few-shot prompts can decrease the error rate for specific question types by introducing domain-specific supplementary information.In contrast, other methods might not bring about a substantial reduction in error rates.For the second evaluation dimension, we initially identified seven scientific problemsolving skills.We then categorized the mistakes made by the LLMs to highlight deficiencies in these pre-established skills.Our findings underscore that the absence of skills in logical decomposition, problem deduction, and logical reasoning predominantly contributes to the subpar performance observed in our NLPBench.Based on the above evaluations, we conclude that simple prompting methods are enough for promising results, and the training process should focus more on fostering specific problem-solving skills like logical decomposition and reasoning.</p>
<p>THE NLPBENCH DATASET</p>
<p>To evaluate the capabilities and analysis of the limitations of the existing large language models (LLMs) to solve NLP-related problems, we collect a new dataset consisting of final exam questions from the universities' NLP courses.All questions are divided into two types: with and without context, where a question with context consists of multiple related sub-questions sharing the same public information.Questions with context require answering with multi-turn communication.We further categorize each question according to the answer format: short answer, multiple choice, and math.This section introduces the details of the dataset construction process.</p>
<p>Data selection.We select about 400 questions with various NLP topics from the universities' final exam question set consisting of roughly 1000 questions, aiming to evaluate the NLP problemsolving ability comprehensively.Different from the previous benchmarks, our dataset introduces a new category with context, as shown in Figure 1, which requires more complex reasoning steps to capture the relation between the current question and context and the relation between current and other questions.Considering the evaluation of the basic ability of LLMs, our dataset also contains traditional without context questions.All of the above questions are further divided into multiplechoice, short answer, and math according to their answer type.Specifically, our proposed dataset has the following features:</p>
<p>• Inclusion of NLP-related problems.The chosen problems demand a solid understanding of NLP-related knowledge (e.g., rhetorical structure theory, formal languages, application of probabilistic theory in NLP, etc.) in reasoning capability, the adaptation of calculation skills, and the ability to comprehend complex concepts.</p>
<p>• Inclusion of detailed solutions: To facilitate a thorough analysis of the limitations of LLMs, detailed solutions should be provided for the selected problems.This enables a comprehensive examination of the performance of LLMs and their capacity to handle complex problem-solving tasks.</p>
<p>• Inaccessibility.To ensure an unbiased evaluation, we carefully curate questions that are not readily accessible online and couldn't be easily extracted or transformed into text.This selection process aims to mitigate any potential information leakage from the exposure of LLMs to pre-existing online question banks, such as those found in standardized tests like the SAT exams.</p>
<p>• Complex structure.About half of our collected questions have a complex structure, with a context shared with multiple subsequent questions and relations between each question.This type of question requires the model to solve with a multi-tern conversation and examine the model's ability to capture critical information in the context.</p>
<p>Data processing.All questions are initially available in both text and image formats (e.g., handwritten), which we meticulously converted into plain text and LaTeX documents using a web-based annotation tool, and the extracted questions will be saved in JSON format.A detailed overview of the tool's user interface can be found in Appendix B. Expert human annotators rigorously reviewed each problem to guarantee the absence of LaTeX syntax errors and to ensure all characters adhere to the ASCII standard.We classified the questions into three formats: short answers, multiple choice, and mathematical.Furthermore, based on the inclusion or exclusion of context information, information common to a set of subsequent questions (e.g., paragraphs from a book, upon which the answers to all following questions are contingent), we divided the questions into two main categories: with and without context.Notably, we integrated the true-false format from the original dataset into the multiple-choice category due to its limited amount.Each question comes with a ground-truth answer for evaluation.Our dataset also contains short answer questions that require free-form responses, such as prompting for examples or specific subsets of a concept.This further reduces the chances of candidates simply guessing correct answers rather than only using multiple choice questions (Lu et al., 2021a;2022;Chen et al., 2023).To assist in evaluating responses to these questions, we offer sample answers that guide evaluators in determining the accuracy of a response.For mathematical problems, we document answers in LaTeX format, specifying exact figures, accompanied by their respective step-by-step solutions.These stepwise solutions serve as guides for intermediate reasoning methodologies (e.g., the "Chain of Thought" approach), assisting LLMs in formulating more accurate answers.</p>
<p>Dataset statistics.In summary, we collected 378 questions from Yale University's NLP course final exams.The dataset includes 192 short-answer questions, 159 multiple-choice questions, and 27 math questions with step-by-step solutions.All types of questions are divided into with context and without.We detailed the statistical results of each question type in Table 1.All questions were also originally categorized into six common NLP-related concepts, summarized in Table 2. Specifically, the questions belong to Other topics are in the field of current research, speech processing, ethics, and applications to other domains.</p>
<p>EXPERIMENT</p>
<p>EXPERIMENT SETUP</p>
<p>We evaluate both the online accessible models (GPT-3.5,OpenAI.( 2022), GPT-4, OpenAI.( 2023) and PaLM-2, Anil et al. ( 2023)) and open-sourced models (LLAMA-2 (13 and 70b), Touvron et al. (2023b)) on the proposed dataset.We consider two advanced prompting strategies, including chainof-thought (CoT, Wei et al. (2022)) and tree-of-thought (ToT, Yao et al. ( 2023)), under both zeroshot and few-shot with or without system prompt.We also perform self-consistency (SC) as an improvement of greedy methods.</p>
<p>• Zero-shot and few-shot prompting.Under zero-shot prompting, the model is not able to access questions in the training set for prior knowledge, which evaluates their inherent problem-solving capabilities with background knowledge and reasoning abilities.While in the few-shot prompting, a few examples are mixed into the input prompt as the prerequisites for the later questions.This aims to examine their capability to learn new information from the demonstrations and incorporate it into their problem-solving processes.</p>
<p>• Advanced prompting strategies.We try different prompting methods, zero-shot and few-shot, and we further combine them with or without system prompt, CoT, and ToT.We implement CoT in two ways: the 2-staged (adding let's think step by step behind the questions) for short answer questions and format template for multiple choice and math questions.This is because of the hardness of extracting the reasoning chain from the short answer questions, different from the multiple choice and math, in which we can extract an exact reasoning process easily by separating the final answer and the corresponding process.</p>
<p>In summary, we consider ten combinations of prompting strategies: zero-shot and few-shot prompting (ZS, FS), zero-shot and few-shot prompting with system prompt (ZS+SYS, FS+SYS), chainof-thought prompting under zero-shot and few-shot (ZS+CoT, FS+CoT), chain-of-thought prompting under zero-shot and few-shot with system prompt (ZS+CoT+SYS, FS+CoT+SYS), and tree-ofthought under zero-shot and few-shot (FS+ToT, FS+ToT).Zero-shot, few-shot, and CoT, with SC, are evaluated on the multiple choice question set due to the limitation of the statistic method in SC.</p>
<p>Example prompts of the above method are provided in Appendix A.2.</p>
<p>Implementation details.We access the API of GPT-3.5 (gpt-3.5-turbo)and GPT-4 (gpt-4) via AutoGen2 (Wu et al., 2023), which provided the enclosure of Open-AI API, helping us cache the results with same hyperparameters.We access PaLM-2 via the Google PaLM generate_text  (Kwon et al., 2023), an open-sourced, fast-speed LLM serving platforms for a wide range of open-source models, which can provide Open-AI like API for the LLM user.We further access those endpoints via AutoGen, the same as we access the Open-AI model.For all models, we use the same seed and set the temperature as 1 for question answering and 0 for the middle process in CoT and ToT.We choose a high temperature for a more creative answer and a low temperature for a more specific process.</p>
<p>RESULTS AND ANALYSIS</p>
<p>The experimental results for GPT-3.5, GPT-4, PaLM-2, and LLAMA-2 (13b and 70b) with various configurations on our NLPBench are detailed in Table 3.We highlight the model performance by presenting accuracy scores in both 'with' and 'without' context scenarios.Notably, questions requiring context involve multi-turn interactions with the model.Our accuracy calculation focuses on the model's final answer, disregarding intermediary steps when computing accuracy, which will be considered in the human evaluation process.For context-based questions, we examine the accuracy of each distinct sub-question.From the experiment results, we have several key observations:</p>
<p>GPT-4 outperforms all models with a significant margin under most of the situations.Based on the results across three distinct question formats categorized under two categories, GPT-4 outperforms all baselines under most situations.Specifically, it achieved the top spot with the best average performance accuracy in two of the question formats.When juxtaposed against all baseline methods, there's a remarkable uplift in its performance, registering an average score improvement of at most 67.85% and 82.29% when compared with LLAMA-2 (13b).It's worth highlighting that these outstanding results were obtained under a zero-shot setting without the aid of any sophisticated prompting strategies.Interestingly, our observations also indicate that deploying advanced prompting techniques often has a counterproductive effect on GPT-4's performance in many scenarios.</p>
<p>Few-shot prompting does not always improve.In Figure 2a, we present a comparison of average performance between zero-shot and few-shot prompting.Notably, the adoption of few-shot prompting often results in a modest performance enhancement, and in some cases, even a decrease, consistent with findings by Wang et al. (2023).A closer examination of Table 3 reveals that in some cases, LLAMA-2 (13b and 70b) derives advantages from the supplementary knowledge gained through few-shot prompting.However, this can lead to surpassing the maximum context length, particularly when multi-turn communication is necessitated, or the query contains an extensive description, which leads to a significant performance drop in LLAMA-2 (13b).GPT-3.5, GPT-4, and PaLM-2 only have ordinary improvements, about 3%, when adopting few-shot prompting.In fact, over 70% of the highest average scores were achieved by zero-shot prompting.This phenomenon may arise because the chosen sample questions are either highly representative of and specific to the domain or, conversely, do not capture its diversity adequately, introducing errors during inference.Therefore, while few-shot prompting can potentially extend the prompt length and occasionally enhance performance, the selection of sample questions is critical.Ill-chosen examples can introduce noise detrimental to the task at hand.</p>
<p>Advanced prompting strategies do not work consistently, sometimes having a negative effect.</p>
<p>In Figure 2b, we present the average scores both with and without the utilization of advanced prompting strategies.Notably, CoT only provides a slight performance increase with GPT-3.5 and will cause performance declines in other models.The efficacy of these prompting strategies is heavily dependent on the model's innate ability to adhere to the prompts, which necessitates the models to self-evaluate their responses.CoT demands a singular feedback loop, which is relatively straightforward.In contrast, ToT calls for multiple feedback mechanisms coupled with a search operation, such as the DFS algorithm.Challenges arise with ToT when a model generates a response that diverges from the specified template in the prompt.GPT-3.5/4exhibits an exceptional capacity to process intricate prompts, yielding the SOTA results (when comparing with other models) in tasks that necessitate intricate logical reasoning when implementing advanced prompting strategies but still cannot outperform the baseline without any prompting strategy.While LLAMA-2 (13b), due to the limited prompt-following capability and constricted context length, it experienced a downturn in performance when employing these advanced strategies.On the other hand, self-consistency (Wang et al., 2022), a robust alternative to greedy decoding, demonstrates impressive results on other benchmarks.Nevertheless, our findings, detailed in Table 4, indicate that while self-consistency can enhance performance with few-shot prompting (as seen with GPT-3.5 and GPT-4), it considerably undermines the output during zero-shot prompting.A potential explanation for such contrasting outcomes is that few-shot prompting restricts the scope of knowledge, impacting answer generation, a constraint absent in zero-shot prompting.</p>
<p>EVALUATING TEXT RELEVANCE</p>
<p>Text relevance is a crucial metric, highlighting the relationship between two sentences and ensuring that a generated answer aligns with the task at hand.Classical metrics like BLEU and ROUGE-L measure the shared sequences between pairs of sentences: BLEU focuses on the n-gram overlap, while ROUGE-L captures the lengthiest common sequence.CIDEr refines the ROUGE-L metric by accounting for synonyms, word frequency, and scene graphs.We evaluated short-answer questions (with unique answers) generated by GPT-3.5, GPT-4, PaLM-2, and LLAMA-2 (13b and 70b) using the BLEU, ROUGE-L, and CIDEr metrics.Our collective findings are presented in Table 5.Interestingly, PaLM 2 displayed notably higher scores compared to other models but exhibited low accuracy, as seen in Table 3. Delving into the errors of PaLM 2, we discerned that, while it can provide accurate descriptions of specific concepts, it often muddles the logical connections between these concepts and redundantly reiterates irrelevant ones.An illustrative error from PaLM 2 is showcased in Figure 3, where the model erroneously repeats certain concepts.However, this repetition ironically leads to heightened text relevance scores.This observation underscores a limitation inherent in using text relevance metrics for evaluating LLMs.</p>
<p>ERROR ANALYSIS</p>
<p>Considering the substantial advancements of current Large Language Models (LLMs), an in-depth analysis of the particular skills that are either enhanced or limited under certain settings becomes Preprint imperative.We evaluate two types of abilities that should be obtained before taking the final exam: an understanding of natural language processing (NLP) and the ability to solve college-level problems.We select the results provided by GPT-3.5/4 and LLAMA 2-70b, which represent the SOTA online and open-sourced model, respectively.</p>
<p>UNDERSTANDING OF NATURAL LANGUAGE PROCESSING</p>
<p>To assess the NLP comprehension of LLMs, we delineated the errors made by GPT-3.5/4 and LLAMA 2-70b in Figure 4, showcasing their respective error rates across various NLP categories.A notable disparity in distribution is evident between zero-shot and few-shot prompting.There's a marked decrease in error rates for pdda by 16% for GPT-4 and 32% for LLAMA 2-70b when transitioning from zero-shot to few-shot prompting, a trend similarly noted in the CoT results.However, this trend diminishes once a system prompt is integrated.The introduction of a system prompt and additional example questions helps mitigate errors stemming from incorrect prior knowledge.Yet, combining the system prompt with few-shot prompting increases the error rate by 10% on irtm and 8% on pdda for GPT-4.In contrast, there's a 13% reduction in the error rate for ot.For LLAMA 2-70b, few-shot prompting consistently reduces error rates across categories, resulting in a more balanced error distribution.In summary, few-shot prompting can help decrease the error rate for certain types of questions by offering additional examples from the dataset.However, its effectiveness diminishes when the dataset demands a broad spectrum of knowledge.While advanced prompting strategies like CoT may not substantially enhance performance with complex datasets, system prompts can counteract errors introduced by these advanced strategies.</p>
<p>ABILITY TO SOLVE COLLEGE-LEVEL PROBLEMS</p>
<p>We chose three models, both online and open-sourced, with the best average performance (GPT-3.5 w/ ZS, GPT-4 w/ ZS, and LLAMA 2-70b w/ ZS+SYS) and annotated the source of the error for short answers (with a unique answer) and math questions, indicating where the model made a mistake and why.Following Wang et al. (2023), we classify the human-annotated error reasons into seven crucial skills deficient for solving complex college-level problems.For each wrong question, we summarized three of the seven skills:</p>
<p>Preprint</p>
<p>• Logical decomposition and analysis (LD).This ability involves decomposing the question into smaller, manageable parts and understanding the relationships between these parts.</p>
<p>• Identification of assumptions (IA).This skill involves the ability to recognize relevant and necessary assumptions in the question.</p>
<p>• Causal reasoning (CR).This is the ability to understand cause-and-effect relationships.</p>
<p>• Problem deduction skills (PD).This pertains to the ability to infer and deduce potential solutions or underlying principles from the given information in a problem.</p>
<p>• Abstract reasoning (AR).This skill involves the ability to understand complex concepts that cannot be perceived physically and to recognize patterns or relationships beyond concrete examples.</p>
<p>• Logical reasoning (LR).This is the ability to make a reasoned argument and to identify fallacies or inconsistencies in an argument or set of data.</p>
<p>• Calculation (CA).This involves the ability to carry out mathematical operations and computations accurately.</p>
<p>The analysis results are recorded in Figure 5, we also provided some error samples in Appendix A.1.</p>
<p>Compared with the SOTA GPT-4, GPT-3.5 has 6% and 7% higher probability of making wrong answers caused by a lack of problem deduction and logical reasoning skills, and LLAMA 2-70b has 14%, 11%, and 16% higher in logical decomposition, problem deduction and logical reasoning skills.This increment reveals a strong relation between a correct answer and logical decomposition, problem deduction, and logical reasoning skills, which is similar to the findings of Berglund et al. (2023).Many questions in our NLPBench dataset require an understanding of a given text before the question (e.g., a story or news).Answer such questions need to retrieve the critical information in the context and build up a logical relation between the question and the retrieved information, which requires a high-level logical decomposition and logical reasoning ability.We also found that GPT-3.5 and 4 do not lack calculation skills but have a low accuracy in math questions (see Table 3).This is because models need to understand the question before the calculation, and the question in our dataset is hard (e.g., requires an understanding of the EM algorithm).Therefore, models often give an answer that is correct in the calculation with a completely wrong process.</p>
<p>RELATED WORKS</p>
<p>Traditional benchmarks have been oriented toward assessing the general abilities of models.For instance, SQuAD (Rajpurkar et al., 2018) was developed to gauge models' reading comprehension skills.GLUE (Wang et al., 2018) provides a versatile framework for evaluating performance across a variety of natural language understanding tasks.Cosmos QA (Huang et al., 2019) delves into assessing models on their common-sense reasoning abilities using natural language contexts.HumanEval (Chen et al., 2021) targets the coding prowess of models, presenting 164 Python programming challenges.BIG-Bench (Srivastava et al., 2022) serves as a comprehensive test suite that includes 204 multiple-choice or exact-match tasks, while its counterpart, BIG-Bench Hard (Suzgun et al., 2022), presents notably intricate chain-of-thought prompts.Finally, HELM (Liang et al., 2022) offers a detailed multi-metric evaluation of LLMs, shedding light on their strengths, weaknesses, and potential risks.</p>
<p>Recent benchmarks predominantly assess LLMs' problem-solving skills, particularly in science and mathematics (Lu et al., 2023b;Fu et al., 2023;Lu et al., 2023a;Zhong et al., 2023;Mishra et al., 2022;Chen et al., 2023;Guo et al., 2023;Hendrycks et al., 2020).Noteworthy datasets include GSM8K (Cobbe et al., 2021), which contains 8.5K elementary math word problems, ScienceQA (Lu et al., 2022), a multimodal dataset with lectures, and MATH (Hendrycks et al., 2021), consisting of 12.5K problems from math contests.LILA (Mishra et al., 2022) enhances 20 datasets with task guidelines and Python solutions.Most benchmarks focus on foundational arithmetic, but Theo-remQA (Chen et al., 2023) offers 800 theorem-centric questions.Galactica (Taylor et al., 2022) explores scientific tasks, such as latex equation conversions, while C-EVAL (Huang et al., 2023) evaluates LLMs within a Chinese cultural context.AGIEval (Zhong et al., 2023) measures LLM performance against standardized tests using human-annotated analysis.SciBench (Wang et al., 2023) presents college-level science problems from textbooks with an automatic evaluation method.However, while these benchmarks emphasize single-turn communication, ours assesses the multiturn problem-solving capabilities of LLMs.Table 6 shows the difference between each benchmark.(Lu et al., 2021b) Grade 1-12 MC TabMWP (Lu et al., 2023a) Grade 1-12 Free GSM8K (Cobbe et al., 2021) Grade 1-12 Free MATH (Hendrycks et al., 2021) High School Free LILA (Mishra et al., 2022) High School Free MNLU (Hendrycks et al., 2020) High School &amp; College MC CEval (Huang et al., 2023) High School &amp; College MC AGIEval (Zhong et al., 2023) High School &amp; College MC TheroemQA (Chen et al., 2023) College Free SciBench (Wang et al., 2023) College Free</p>
<p>NLPBench College</p>
<p>Free &amp; MC Our dataset introduces the questions that require LLMs to answer with multi-turn communication and contains all types of questions that can test the LLMs' ability comprehensively.</p>
<p>DISCUSSION AND CONCLUSION</p>
<p>In conclusion, this study introduces NLPBench, comprising 378 college-level NLP questions spanning various NLP topics, crafted to provide a comprehensive evaluation of the capabilities of Large Language Models (LLMs).NLPBench features questions with context information specifically designed to assess the proficiency of LLMs in engaging in multi-turn conversations.We evaluate common online models, GPT-3.5, GPT-4, PaLM-2, and open-source LLMs like LLAMA 2-13b and LLAMA 2-70b.Moreover, we delve into advanced prompting techniques, the chain-ofthought and tree-of-thought methods with the combination of zero-shot, few-shot prompting, and self-consistency.Our results suggest that advanced prompting strategies aren't consistently effective.Delving deeper, a manual evaluation of the errors produced by GPT-3.5/4 and LLAMA 2-70b reveals that these models struggle primarily due to an inability to logical deconstruction, problem deduction, and logical reasoning.These shortcomings largely account for their subpar performance on our NLPBench dataset.Based on the above conclusion, we have the following recommendations:</p>
<p>• Simple Prompting method is enough for promising results.Based on our findings in Section 3.2, we found that few-shot prompting averagely surpasses zero-shot, but it is hard to achieve the best.Section 4.1 indicates that while few-shot can decrease errors in certain categories, it can also lead to more verbose prompts.Employ few-shot prompting when your task is concentrated on a specific domain.</p>
<p>• Advanced prompting strategies are not always necessary.They show weak or roughly comparable results to the zero-shot on all LLMs and will significantly affect the "small" LLM (e.g., the LLAMA 2-13b).As described in Section 3.2, advanced prompting strategies need a strong prompt-following ability since they all require multiple reasoning steps.If budget is one of your limitations, zero-shot is also a good choice for a competitive result.</p>
<p>• The pretraining process should focus more on fostering "logical thinking skills".According to Section 4.2, we found that LLAMA 2 clearly misses the ability in logical decomposition, problem deduction, and logical reasoning skills.We think the training of LLM should take the above three dimensions into consideration.</p>
<p>Preprint A FURTHER ANALYSIS</p>
<p>A.1 ERROR SAMPLES</p>
<p>We provide some error samples generated by GPT-3.5 in Figure 6 and Figure 7 for a better understanding of the error reason in Section 4.2.</p>
<p>Figure 1 :
1
Figure 1: Example questions in NLPBench dataset.We collected three types of questions, including multiple choice, short answer, and math, and divided them into two categories: with and without context.Text underline shows the relations between questions.</p>
<p>Preprint</p>
<p>(a) Zero-shot v.s.few-shot prompting on overall accuracy(%).(b)Overall accuracy(%) with and without advanced prompting strategies.</p>
<p>Figure 2 :
2
Figure 2: Overall comparison of different prompting strategies.</p>
<p>Figure 3 :
3
Figure 3: Example of wrong answer generated by PaLM 2. It is obvious that PaLM 2 repeat some wrong concept many times, but this will significantly increase the relevance between ground truth and the generated answer.</p>
<p>Figure 4 :
4
Figure 4: The comparison of overall error rate(%) between GPT-3.5/4 and LLAMA 2-70b across all prompting strategies of each NLP category.Each color bar indicates a pre-defined NLP category from the original dataset.</p>
<p>Figure 5 :
5
Figure 5: The error profiles of the deficient of seven essential science problem-solving abilities between GPT-3.5/4 and LLAMA 2-70b.The height of the color bars indicates the percentage that the model has an incorrect answer due to a lack of corresponding science problem-solving skills.</p>
<p>Figure 6 :
6
Figure 6: An example of short answer error in, where the answer of GPT-3.5 cannot align the question, indicating the lack of logical decomposition and analysis, identification of assumptions, and logical reasoning skills.</p>
<p>Figure 7 :
7
Figure 7: An example of a math error in GPT-3.5, where GPT-3.5 cannot understand the principles of language distribution and frequency, indicating the lack of logical decomposition and analysis, problem deduction, and abstract reasoning skills.</p>
<p>Figure 10 :
10
Figure10: Zero-shot prompt template for short answer questions.Note that we use a two-stage method to generate the middle process for CoT.</p>
<p>Figure 11 :
11
Figure11: Zero-shot prompt template for math questions.We input the middle process as the "thought" for CoT.</p>
<p>Figure 12 :
12
Figure 12: The UI design of data processing and annotation.</p>
<p>Table 1 :
1
Statistic of the original dataset and the percent of usage in our proposed dataset.
CategoriesShort AnswerMultiple ChoiceMathw/ contextw/o context w/ contextw/o contextw/ context w/o context# Total237148161622815% Answer 67.1% (159) 58.1% (86) 93.7% (15) 88.9% (144) 92.8% (26) 46.6/% (7)% Used72.6% (130) 48.4% (62) 93.7% (15) 88.9% (144) 85.7% (24)20% (3)</p>
<p>Table 2 :
2
The question quantity under each NLP concept.All the categories are defined by human experts.
CategoryAcronym # QuestionsLanguage Modeling and Syntax Parsinglmsp162Semantics and Logicsl69Pragmatics, Discourse, Dialogue and Applicationspdda13Information Retrieval and Topic Modelingirtm27Artificial Intelligenceai75Other Topicsot32</p>
<p>Table 3 :
3
Experimental results in terms of accuracy (%) on our proposed dataset.The best average scores in each type of question are highlighted in red bold, and the best average scores for each model in a specific type of question are underlined in blue.Results marked with -denote the incomplete experiment caused by exceeding context length or other prompting errors.
ModelSettingMultiple ChoiceShort AnswerMathOverall Acc.w/ Context w/o Context Average w/ Context w/o Context Average w/ Context w/o Context AverageOrig.20.0020.8320.7539.2337.1038.5420.000.004.0028.72ZS+SYS +CoT26.67 26.6734.03 19.4433.33 20.1343.85 22.3127.42 9.6838.54 18.230.00 0.000.00 0.000.00 0.0033.77 17.82LLAMA-2+CoT+SYS33.3327.0827.6723.089.6818.750.000.000.0021.28(13b)Orig.-31.2528.30-29.039.38--0.0016.76FS+SYS +CoT--38.19 30.5634.59 27.67--30.65 32.269.90 10.42----0.00 0.0019.68 17.02+CoT+SYS-36.8133.33-35.4811.46--0.0019.95Orig.40.0022.2223.9053.8538.7148.969.090.008.0035.64ZS+SYS +CoT40.00 33.3323.61 21.5325.16 22.6454.62 32.3146.77 12.9052.08 26.049.09 0.000.00 0.008.00 0.0037.77 22.87LLAMA-2+CoT+SYS40.0038.1938.3633.0825.8130.730.000.000.0031.91(70b)Orig.33.3329.1729.5648.4638.7145.319.090.0019.3836.93FS+SYS +CoT26.67 26.6734.72 31.9433.96 31.4546.92 38.4640.32 51.6144.79 42.710.00 0.000.00 0.000.00 0.0037.23 35.11+CoT+SYS26.6738.1937.1135.3848.3939.584.550.004.0036.17Orig.66.6737.5040.2549.2335.4844.7913.6433.3316.0040.96+SYS66.6745.8347.8051.5437.1046.884.5533.338.0044.68ZS+CoT60.0036.8138.9947.6937.1044.2718.1833.3320.0040.42+CoT+SYS53.3341.6742.7740.0030.6536.9813.640.0012.0037.77PaLM-2+ToT-4.864.40-0.000.00--0.001.86Orig.53.3338.8940.2557.6933.8750.004.5533.338.0043.08+SYS53.3339.5840.8856.1538.7150.524.550.004.0043.35FS+CoT53.3340.2841.5149.2338.7145.830.000.000.0040.96+CoT+SYS40.0038.8938.9953.8540.3249.480.000.000.0041.75+ToT-10.429.43-1.610.52--0.004.25Orig.40.0052.0550.6475.3858.7369.9936.3633.3336.0059.55+SYS46.6740.6941.5171.5462.9068.7513.6433.3316.0053.72ZS+CoT53.3352.7452.8363.8533.3354.1918.18100.0028.0051.87+CoT+SYS46.6739.5840.2566.9259.6864.5818.180.0016.0051.06GPT-3.5+ToT-31.2528.30-0.000.00--0.0011.97Orig.53.3336.8138.3666.1564.5265.6218.1833.3320.0051.06+SYS46.6744.4444.6566.1554.8462.5018.180.0016.0051.86FS+CoT40.0040.2840.2564.6262.9064.0613.640.0012.0050.53+CoT+SYS40.0046.5345.9166.1564.5265.6218.180.0016.0053.99+ToT-30.5627.67-56.4518.23--0.0021.01Orig.86.6770.5572.2578.4669.8475.4222.7333.3324.0070.66+SYS86.6757.9360.3883.8579.0382.2918.180.0016.0068.62ZS+CoT86.6772.6074.1074.6257.1468.6513.64100.0024.0067.99+CoT+SYS86.6756.2559.1273.0875.8173.9627.2766.6728.0064.63GPT-4+ToT-60.4254.72-0.000.00--0.0023.14Orig.86.6762.5064.7877.6975.8177.0822.730.0020.0068.08+SYS86.6759.0361.6481.5479.0380.7313.6433.3316.0068.35FS+CoT86.6760.4262.8978.4675.8177.6036.360.0032.0068.35+CoT+SYS86.6760.4262.8980.0074.1978.1213.6466.6720.0067.82+ToT-60.4254.72-75.8124.48--0.0035.64</p>
<p>Table 4 :
4
Comparison of prompting methods with and without self-consistency (denoted as SC) on GPT-3.5, GPT-4, and PaLM-2.All results are statistics from the multiple-choice question set.
ModelZSZS+CoTFSFS+CoTw/o SC w/ SC w/o SC w/ SC w/o SC w/ SC w/o SC w/ SCGPT-3.550.6437.1152.8338.3638.3643.4040.2544.03GPT-472.2559.7574.1062.8964.7864.7862.8966.67PaLM-240.2523.9038.9928.3040.2537.1141.5138.99API 3 , which is recommended by Google for problem-solving and handling zero and few shottasks. For open-source models LLAMA-2 (13b and 70b), we use the endpoint implemented byvLLM 4</p>
<p>Table 5 :
5
Relevance between LLM generated answers and ground-truth answers.We adopt BLEU, ROUGE-L, and CIDEr to represent the sentence relevance.
ModelSettingBLEUROUGE-LCIDErw/ Context w/o Context w/ Context w/o Context w/ Context w/o ContextZS0.194.800.029.698.660.00ZS+SYS0.375.020.0011.359.641.21ZS+CoT0.955.080.0612.537.860.06ZS+CoT+SYS1.235.460.1612.897.341.09LLAMA-2 (13b)ZS+ToT------FS---5.347.180.00FS+SYS---3.187.180.00FS+CoT---3.787.840.00FS+SYS+CoT---3.256.320.00FS+ToT------ZS0.104.960.006.478.145.57ZS+SYS0.165.882.109.729.600.36ZS+CoT0.915.050.4613.737.511.24ZS+CoT+SYS1.695.630.0414.348.503.23LLAMA-2 (70b)ZS+ToT------FS0.024.040.004.827.880.53FS+SYS0.084.810.018.718.853.13FS+CoT0.083.170.004.628.632.03FS+SYS+CoT0.163.400.005.548.220.00FS+ToT------ZS3.3510.8923.1923.2114.0619.02ZS+SYS6.969.2722.1525.6612.7018.85ZS+CoT3.059.3111.6615.3011.7114.36ZS+CoT+SYS8.099.0026.9623.5511.6231.52PaLM-2ZS+ToT---0.000.000.00FS1.1613.2857.2526.7413.6817.67FS+SYS4.039.4728.3124.6015.6232.05FS+CoT0.338.1920.8314.329.864.68FS+SYS+CoT1.829.6024.6315.008.999.57FS+ToT---0.500.721.89ZS0.105.830.488.7510.9114.23ZS+SYS0.115.205.048.697.750.00ZS+CoT0.164.820.2813.1911.3514.74ZS+CoT+SYS0.475.280.2313.9410.083.79GPT-3.5ZS+ToT---0.000.000.00FS0.155.181.9912.5512.0218.01FS+SYS0.556.266.3117.0113.2627.19FS+CoT0.104.593.479.2610.4115.14FS+SYS+CoT0.315.070.0114.0412.0517.41FS+ToT---6.867.690.19ZS0.636.479.3211.839.856.28ZS+SYS0.677.035.4014.319.460.14ZS+CoT1.127.005.0510.689.6725.16ZS+CoT+SYS1.147.292.6615.6910.165.29GPT-4ZS+ToT---0.000.000.00FS1.347.7615.2417.0911.575.59FS+SYS2.009.9421.8520.1714.3215.90FS+CoT0.716.487.7113.7711.133.09FS+SYS+CoT0.906.827.8717.9314.9835.73FS+ToT---15.3510.629.21</p>
<p>Table 6 :
6
Comparison of NLPBench with other benchmarks."Level" represents the grade level of problems."w/ Solution" represents whether problems contain detailed solutions."Type" represents what format most problems of the dataset use."AP" denotes whether the benchmark uses the advanced prompting strategies, "MC" denotes multiple-choice format, "MT" denotes the question requires an answer in multi-turn communication, and "Free" denotes free-response format."Human" indicates whether the analysis process employs a human annotation process."Auto" represents whether the analysis process uses an automatic annotation process.
BenchmarkDatasetExperimentAnalysisLevelw/ SolutionTypeZS FS AP MT Human AutoScienceQA (Lu et al., 2022)Grade 1-12MCIconQA
https://github.com/LinxinS97/NLPBench
https://microsoft.github.io/autogen/
https://developers.generativeai.google/products/palm
https://vllm.readthedocs.io/en/latest/
ACKNOWLEDGMENTSWe extend our heartfelt gratitude to Professor Dragomir Radev for his unwavering dedication and significant contribution to the compilation of the datasets used in this study.His commitment over the years has been invaluable to the advancement of our research.We also wish to express our appreciation to the students who played a pivotal role in contributing to the development of the exam questions.Their efforts have been instrumental in enhancing the quality and breadth of our study.PreprintA.2 PROMPT TEMPLATEWe designed specific prompts for each type of question, and we summarized those prompts in this section.Figure8shows the system prompt, Figure9shows the prompt template for multiple choice questions, Figure10shows the prompt template for the short answer, and Figure11shows the prompt template for math questions.{input} is the place for input questions, {thought} denotes the middle-process prompt used for CoT.We use a two-stage method for short answer questions, in which the thought is generated by the LLM itself, and a format template for multiple choice and math questions.Specifically in math, we put the problem-solving process into the CoT prompt ({process}) as the "thought".Note that the prompts listed here are all zero-shot prompts, and the few-shot prompt is based on the zero-shot by further adding some example questions.B USER INTERFACEThe original dataset has a lot of handwriting scripts.We, therefore, create a UI interface to transform those handwriting scripts to JSON format manually.Figure12shows the screenshot of our UI interface.To ensure the correctness of input questions, we developed a real-time preview window for annotators to revise their input.
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>The reversal curse: Llms trained on "a is b" fail to learn "b is a. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans, 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, arXiv:2305.12524Theoremqa: A theorem-driven question answering dataset. 2023arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Large language models on wikipedia-style survey generation: an evaluation in nlp concepts. Fan Gao, Hang Jiang, Moritz Blum, Jinghui Lu, Yuang Jiang, Irene Li, arXiv:2308.104102023aarXiv preprint</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao, arXiv:2304.15010Llama-adapter v2: Parameter-efficient visual instruction model. 2023barXiv preprint</p>
<p>What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2305.183652023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:1909.002772019arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Preprint, Yuzhuo Huang, Zhihao Bai, Junlei Zhu, Jinghan Zhang, Tangjun Zhang, Junteng Su, Chuancheng Liu, Yikai Lv, Jiayi Zhang, Lei, arXiv:2305.083222023arXiv preprint</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, arXiv:2309.06180Efficient memory management for large language model serving with pagedattention. 2023arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, Song-Chun Zhu, The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021). 2021a</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu, arXiv:2110.13214Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. 2021barXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, International Conference on Learning Representations (ICLR). 2023a</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, The 61st Annual Meeting of the Association for Computational Linguistics (ACL). 2023b</p>
<p>Lila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, The 2022 Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, </p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Know what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.038222018arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>LLaMA: Open and efficient foundation language models. Preprint Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, arXiv:2302.139712023aarXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 2023b2arXiv preprint</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. 2018arXiv preprint</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, arXiv:2307.106352023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multiagent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023aarXiv preprint</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, arXiv:2302.009232023barXiv preprint</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
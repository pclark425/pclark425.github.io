<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2726 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2726</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2726</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-251253189</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.eacl-demo.20.pdf" target="_blank">TextWorldExpress: Simulating Text Games at One Million Steps Per Second</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games offer a challenging test bed to evaluate virtual agents at language understanding, multi-step problem-solving, and common-sense reasoning. However, speed is a major limitation of current text-based games, capping at 300 steps per second, mainly due to the use of legacy tooling. In this work we present TextWorldExpress, a high-performance simulator that includes implementations of three common text game benchmarks that increases simulation throughput by approximately three orders of magnitude, reaching over one million steps per second on common desktop hardware. This significantly reduces experiment runtime, enabling billion-step-scale experiments in about one day.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2726.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2726.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-based deep reinforcement learning agents (Ammanabrolu & Riedl, 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention of graph-based networks applied to text-adventure game agents; cited as one of several agent modeling paradigms that represent state/relations with graph-structured models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>graph-based DRL agents (Ammanabrolu & Riedl 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as graph-based deep reinforcement learning agents for text-adventure games; no architecture details provided in this paper beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>interactive fiction / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure environments requiring navigation, object manipulation and multi-step planning (general domain referenced in relation to graph-based agents).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorldExpress: Simulating Text Games at One Million Steps Per Second', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2726.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2726.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>State+Commonsense graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State and commonsense graph representations (Murugesan et al., 2021b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention of approaches that jointly leverage environment state graphs and external commonsense graphs to improve efficiency of text-based RL agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>agents leveraging state and commonsense graph representations (Murugesan et al., 2021b)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in the paper as agents that combine environment state representations with external commonsense graphs; the current paper only cites this work and does not provide implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>text-based RL benchmarks (e.g., TextWorld variants)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Benchmarks evaluating RL agents on text-based tasks requiring common-sense and state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (state graph + commonsense graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorldExpress: Simulating Text Games at One Million Steps Per Second', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2726.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2726.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An environment benchmark that requires agents to place household objects in their canonical locations and therefore necessitates external common-sense knowledge (training/dev/test splits prevent learning locations solely from interaction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>unspecified agents evaluated on TWC</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents evaluated on the TWC cleanup task are expected to leverage external commonsense knowledge (e.g., ConceptNet) because object-location mappings are held out across splits; the paper reimplements TWC in TEXTWORLDEXPRESS.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A cleanup task where agents must place household objects into canonical locations (e.g., put dirty dish into dishwasher); separate object lists across splits require external commonsense knowledge to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external commonsense knowledge base / knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>knowledge graph (as provided by ConceptNet)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>canonical object-location mappings and other commonsense facts (e.g., typical affordances/locations of household objects)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Provide commonsense facts (object-location relations) not learnable from training interactions, enabling generalization to held-out objects/locations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper notes that agents cannot learn object locations from training alone and must rely on an external commonsense KB to perform well on development and test sets (i.e., external memory is necessary for generalization in TWC).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorldExpress: Simulating Text Games at One Million Steps Per Second', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2726.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2726.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Valid-action handicap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Valid-action handicap (valid-action list provided per step)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common practice where agents are given, at each step, a list of valid actions (valid-action handicap) to select from; widely used by high-performance text-game agents and implemented efficiently by TEXTWORLDEXPRESS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>agents using a valid-action handicap</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that select actions from a simulator-provided exhaustive list of valid actions at each timestep (reduces action space and simplifies action selection); this paper emphasizes simulator-level costs of generating that list and implements a fast generator.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>CookingWorld, TextWorld Commonsense, Coin Collector (benchmarks in TEXTWORLDEXPRESS)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Benchmarks where action spaces are large and supplying a valid-action list is common practice to constrain agent choices.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory / short-term action buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>a list (exhaustive set) of valid action templates for the current game state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>current-step valid actions (action templates possibly with object arguments, e.g., 'put coat in closet')</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>no retrieval needed by agent when list is provided; agent selects from provided list (full-context provided each step)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>simulator regenerates/updates the valid-action list each timestep (i.e., after each environment transition)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Constrain action selection, reduce exploration complexity, and enable agents to focus on policy selection rather than action validity checking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper states that most contemporary high-performance agents use the valid-action handicap; it also notes that legacy frameworks generate valid actions by enumerating combinations which is computationally costly (reducing simulation framerate), whereas TEXTWORLDEXPRESS implements fast valid-action generation to avoid this bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Enumerating and validating all possible action-object combinations at each step is extremely costly and reduces simulation speed in legacy frameworks; providing only the valid-action list can limit diversity of input surface forms (the simulator's simplified parser only recognizes presented valid actions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorldExpress: Simulating Text Games at One Million Steps Per Second', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2726.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2726.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet (commonsense knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited external commonsense knowledge graph used by TextWorld Commonsense to provide canonical object locations/affordances that agents must leverage to solve tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptnet-a practical commonsense reasoning tool-kit.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>external commonsense knowledge base (ConceptNet)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ConceptNet is referenced as an external commonsense knowledge source supplying canonical object-location relations and affordances that TWC relies on; the present paper references ConceptNet but does not implement or evaluate it directly.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>TWC uses ConceptNet as the source of canonical object-location mappings for its cleanup tasks; agents must use such external knowledge to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic knowledge graph / external KB</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>graph (nodes = concepts, edges = relations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>commonsense facts such as canonical object locations, affordances, and relations between everyday concepts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Provide commonsense facts (object locations/affordances) that cannot be learned from interactions in held-out test sets, enabling agents to perform canonical placement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper implies that access to an external commonsense KB (e.g., ConceptNet) is required for performance on TWC's development and test sets because those facts are not learnable from the training interactions alone.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TextWorldExpress: Simulating Text Games at One Million Steps Per Second', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. <em>(Rating: 2)</em></li>
                <li>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. <em>(Rating: 2)</em></li>
                <li>Conceptnet-a practical commonsense reasoning tool-kit. <em>(Rating: 2)</em></li>
                <li>Neuro-symbolic approaches for text-based policy learning. <em>(Rating: 1)</em></li>
                <li>LOA: Logical optimal actions for text-based interaction games. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2726",
    "paper_id": "paper-251253189",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Graph-based agents",
            "name_full": "Graph-based deep reinforcement learning agents (Ammanabrolu & Riedl, 2019)",
            "brief_description": "Mention of graph-based networks applied to text-adventure game agents; cited as one of several agent modeling paradigms that represent state/relations with graph-structured models.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "graph-based DRL agents (Ammanabrolu & Riedl 2019)",
            "agent_description": "Referenced as graph-based deep reinforcement learning agents for text-adventure games; no architecture details provided in this paper beyond the citation.",
            "base_model_size": null,
            "game_benchmark_name": "interactive fiction / text-adventure games",
            "game_description": "Text-adventure environments requiring navigation, object manipulation and multi-step planning (general domain referenced in relation to graph-based agents).",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2726.0",
            "source_info": {
                "paper_title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "State+Commonsense graphs",
            "name_full": "State and commonsense graph representations (Murugesan et al., 2021b)",
            "brief_description": "Mention of approaches that jointly leverage environment state graphs and external commonsense graphs to improve efficiency of text-based RL agents.",
            "citation_title": "Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations.",
            "mention_or_use": "mention",
            "agent_name": "agents leveraging state and commonsense graph representations (Murugesan et al., 2021b)",
            "agent_description": "Described in the paper as agents that combine environment state representations with external commonsense graphs; the current paper only cites this work and does not provide implementation details.",
            "base_model_size": null,
            "game_benchmark_name": "text-based RL benchmarks (e.g., TextWorld variants)",
            "game_description": "Benchmarks evaluating RL agents on text-based tasks requiring common-sense and state tracking.",
            "uses_memory": true,
            "memory_type": "graph-based memory (state graph + commonsense graph)",
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2726.1",
            "source_info": {
                "paper_title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "TextWorld Commonsense (TWC)",
            "name_full": "TextWorld Commonsense (TWC)",
            "brief_description": "An environment benchmark that requires agents to place household objects in their canonical locations and therefore necessitates external common-sense knowledge (training/dev/test splits prevent learning locations solely from interaction).",
            "citation_title": "Text-based rl agents with commonsense knowledge: New challenges, environments and baselines.",
            "mention_or_use": "use",
            "agent_name": "unspecified agents evaluated on TWC",
            "agent_description": "Agents evaluated on the TWC cleanup task are expected to leverage external commonsense knowledge (e.g., ConceptNet) because object-location mappings are held out across splits; the paper reimplements TWC in TEXTWORLDEXPRESS.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld Commonsense (TWC)",
            "game_description": "A cleanup task where agents must place household objects into canonical locations (e.g., put dirty dish into dishwasher); separate object lists across splits require external commonsense knowledge to generalize.",
            "uses_memory": true,
            "memory_type": "external commonsense knowledge base / knowledge graph",
            "memory_structure": "knowledge graph (as provided by ConceptNet)",
            "memory_content": "canonical object-location mappings and other commonsense facts (e.g., typical affordances/locations of household objects)",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "Provide commonsense facts (object-location relations) not learnable from training interactions, enabling generalization to held-out objects/locations.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Paper notes that agents cannot learn object locations from training alone and must rely on an external commonsense KB to perform well on development and test sets (i.e., external memory is necessary for generalization in TWC).",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2726.2",
            "source_info": {
                "paper_title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Valid-action handicap",
            "name_full": "Valid-action handicap (valid-action list provided per step)",
            "brief_description": "Common practice where agents are given, at each step, a list of valid actions (valid-action handicap) to select from; widely used by high-performance text-game agents and implemented efficiently by TEXTWORLDEXPRESS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "agents using a valid-action handicap",
            "agent_description": "Agents that select actions from a simulator-provided exhaustive list of valid actions at each timestep (reduces action space and simplifies action selection); this paper emphasizes simulator-level costs of generating that list and implements a fast generator.",
            "base_model_size": null,
            "game_benchmark_name": "CookingWorld, TextWorld Commonsense, Coin Collector (benchmarks in TEXTWORLDEXPRESS)",
            "game_description": "Benchmarks where action spaces are large and supplying a valid-action list is common practice to constrain agent choices.",
            "uses_memory": true,
            "memory_type": "working memory / short-term action buffer",
            "memory_structure": "a list (exhaustive set) of valid action templates for the current game state",
            "memory_content": "current-step valid actions (action templates possibly with object arguments, e.g., 'put coat in closet')",
            "memory_capacity": null,
            "memory_retrieval_strategy": "no retrieval needed by agent when list is provided; agent selects from provided list (full-context provided each step)",
            "memory_update_strategy": "simulator regenerates/updates the valid-action list each timestep (i.e., after each environment transition)",
            "memory_usage_purpose": "Constrain action selection, reduce exploration complexity, and enable agents to focus on policy selection rather than action validity checking.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Paper states that most contemporary high-performance agents use the valid-action handicap; it also notes that legacy frameworks generate valid actions by enumerating combinations which is computationally costly (reducing simulation framerate), whereas TEXTWORLDEXPRESS implements fast valid-action generation to avoid this bottleneck.",
            "memory_limitations": "Enumerating and validating all possible action-object combinations at each step is extremely costly and reduces simulation speed in legacy frameworks; providing only the valid-action list can limit diversity of input surface forms (the simulator's simplified parser only recognizes presented valid actions).",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2726.3",
            "source_info": {
                "paper_title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "ConceptNet",
            "name_full": "ConceptNet (commonsense knowledge graph)",
            "brief_description": "A cited external commonsense knowledge graph used by TextWorld Commonsense to provide canonical object locations/affordances that agents must leverage to solve tasks.",
            "citation_title": "Conceptnet-a practical commonsense reasoning tool-kit.",
            "mention_or_use": "mention",
            "agent_name": "external commonsense knowledge base (ConceptNet)",
            "agent_description": "ConceptNet is referenced as an external commonsense knowledge source supplying canonical object-location relations and affordances that TWC relies on; the present paper references ConceptNet but does not implement or evaluate it directly.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld Commonsense (TWC)",
            "game_description": "TWC uses ConceptNet as the source of canonical object-location mappings for its cleanup tasks; agents must use such external knowledge to generalize.",
            "uses_memory": true,
            "memory_type": "semantic knowledge graph / external KB",
            "memory_structure": "graph (nodes = concepts, edges = relations)",
            "memory_content": "commonsense facts such as canonical object locations, affordances, and relations between everyday concepts",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "Provide commonsense facts (object locations/affordances) that cannot be learned from interactions in held-out test sets, enabling agents to perform canonical placement tasks.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Paper implies that access to an external commonsense KB (e.g., ConceptNet) is required for performance on TWC's development and test sets because those facts are not learnable from the training interactions alone.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2726.4",
            "source_info": {
                "paper_title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations.",
            "rating": 2,
            "sanitized_title": "efficient_textbased_reinforcement_learning_by_jointly_leveraging_state_and_commonsense_graph_representations"
        },
        {
            "paper_title": "Text-based rl agents with commonsense knowledge: New challenges, environments and baselines.",
            "rating": 2,
            "sanitized_title": "textbased_rl_agents_with_commonsense_knowledge_new_challenges_environments_and_baselines"
        },
        {
            "paper_title": "Conceptnet-a practical commonsense reasoning tool-kit.",
            "rating": 2,
            "sanitized_title": "conceptneta_practical_commonsense_reasoning_toolkit"
        },
        {
            "paper_title": "Neuro-symbolic approaches for text-based policy learning.",
            "rating": 1,
            "sanitized_title": "neurosymbolic_approaches_for_textbased_policy_learning"
        },
        {
            "paper_title": "LOA: Logical optimal actions for text-based interaction games.",
            "rating": 1,
            "sanitized_title": "loa_logical_optimal_actions_for_textbased_interaction_games"
        }
    ],
    "cost": 0.015810249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TEXTWORLDEXPRESS: Simulating Text Games at One Million Steps Per Second
May 2-4, 2023</p>
<p>Peter Jansen pajansen@arizona.edu 
University of Arizona
TucsonUSA</p>
<p>Marc-Alexandre Ct 
Microsoft Research Montral</p>
<p>TEXTWORLDEXPRESS: Simulating Text Games at One Million Steps Per Second</p>
<p>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics System Demonstrations
the 17th Conference of the European Chapter of the Association for Computational Linguistics System DemonstrationsMay 2-4, 2023
Text-based games offer a challenging test bed to evaluate virtual agents at language understanding, multi-step problem-solving, and common-sense reasoning. However, speed is a major limitation of current text-based games, capping at 300 steps per second, mainly due to the use of legacy tooling. In this work we present TEXTWORLDEXPRESS, a highperformance simulator that includes implementations of three common text game benchmarks that increases simulation throughput by approximately three orders of magnitude, reaching over one million steps per second on common desktop hardware. This significantly reduces experiment runtime, enabling billion-step-scale experiments in about one day. 1 2 3</p>
<p>Introduction</p>
<p>One of the long standing goals of artificial intelligence is to create agents that can work and reason in embodied environments. Toward this goal, a variety of virtual environments have been created that allow simulated robots the opportunity to learn to a variety of tasks, in settings from household environments (Kolve et al., 2017;Shridhar et al., 2020a) to Minecraft (Guss et al., 2019). Because highfidelity 3D virtual environments are challenging and resource intensive to develop, simpler 2D environments have also been proposed (e.g. Chevalier-Boisvert et al., 2019;Kttler et al., 2020) that allow agents to focus on learning skills such as search or navigation in graphically simpler environments.</p>
<p>Recently, text games -or environments rendered entirely in natural language -have emerged as an alternate research methodology for embodied agent research, centrally due to their low barrier to entry compared to 3D games, coupled with their ability to easily model complex tasks at a high-level (see 1 Code: github.com/cognitiveailab/TextWorldExpress 2 Video: youtu.be/HLFAnRKuTlE 3 Demo: marccote-textworldexpress.hf.space</p>
<p>Environment Simulator SPS</p>
<p>2D/3D Simulators 4 AI2THOR (Kolve et al., 2017) 30  MINERL (Guss et al., 2019) 180  BABYAI (Chevalier-Boisvert et al., 2019) 3k NETHACK (Kttler et al., 2020) 14k MEGAVERSE (Petrenko et al., 2021) 327k </p>
<p>Text Game Simulators 5 TEXTWORLD (Ct et al., 2018) 300 JERICHO  1 SCIENCEWORLD  20 TEXTWORLDEXPRESS (online, PYTHON) 32k TEXTWORLDEXPRESS (precrawled, PYTHON) 316k TEXTWORLDEXPRESS (online, JAVA) 212k TEXTWORLDEXPRESS (precrawled, JAVA) 4M Table 1: Single-thread simulation speed of common 2D, 3D, and text-game environment simulators. Speed is measured in terms of Steps Per Second (SPS).  symbolizes that simulation is carried out on GPUs. TEXTWORLDEXPRESS outperforms other text game simulators by approximately three orders of magnitude.</p>
<p>Jansen, 2022, for review). For example, a cooking game might require an agent to read a recipe, find ingredients, then prepare those ingredients to create a meal. Text games model an agent as they navigate an environment, rendering their observations in text (e.g. "You are in the kitchen. You see..."). Similarly, agents interact with the environment through abstracted high-level natural language commands (e.g. "move south", or "pick up carrot"), rather than lower-level actions common in 3D environments (e.g. rotate agent 2 degrees clockwise).</p>
<p>Text games require a variety of common-sense knowledge to complete successfully (Ryu et al., 2022;Murugesan et al., 2021b), including understanding common procedures (such as how to read and follow instructions), as well as affordances about the world -for example, that buildings have rooms, containers must be opened before their con-tents can be observed or removed, and so forth. As such, text games are still extremely challenging for agents, with current state-of-the-art performance at only 12% for classic interactive fiction games such as Zork (Yao et al., 2021;Ammanabrolu et al., 2021). Similarly, interactivity and explicit step-bystep reasoning appears challenging for agents. For example, there appears to be a large dissociation between a model's ability to answer questions about topics (e.g., science exam questions) and its ability to perform very similar experiments in interactive text environments, even with substantial training . This suggests that explicit interactive multi-step reasoning is still very challenging for contemporary methods like language models, and that accurate procedural knowledge is currently difficult to generate. Together, these highlight the importance of using text games as a vehicle for explicit, embodied, step-by-step reasoning about the world.</p>
<p>To help support these efforts, a number of simulators have recently been developed for text game research, shown in Table 1. Current tooling for text games is built on legacy code bases, providing strong limitations in rendering speed -at present, most simulators are limited to running at between 1 and 300 steps per second. This generally limits agents from using modeling paradigms with fast iteration cycles and high sample requirements (such as reinforcement learning, or evolutionary learning), and restricts users to modeling techniques with large train and inference cycles (such as language models) where the simulator no longer becomes the bottleneck in experiment runtimes.</p>
<p>In this work, we develop a high-speed framework for text-based games in natural language processing research. Our contributions are:</p>
<ol>
<li>
<p>TEXTWORLDEXPRESS, a highly optimized simulator that includes reimplementations of three text game benchmarks focusing on instruction following, commonsense reasoning, and object identification, as well as other newer benchmarks for evaluating arithmetic, navigation, and neurosymbolic reasoning.</p>
</li>
<li>
<p>We empirically demonstrate that this simulator runs up to three orders of magnitude faster than current tooling, reaching 300k steps per second (SPS) on a single-thread, and exceeding 1M SPS on modest multi-core desktop hardware. This substantially reduces experi-ment times (from weeks to hours) for sampleheavy machine learning agents.</p>
</li>
</ol>
<p>Related Work</p>
<p>Research Paradigm: Text games are a rapidly expanding research paradigm for learning and evaluating situated natural language processing agents on a variety of tasks, with over 100 papers written using this paradigm in the last few years (see Jansen, 2022, for review). This may be in part due to language providing useful abstractions for more efficient exploration and planning (Karch et al., 2020;Colas et al., 2020;Mu et al., 2022;Tam et al., 2022), making task modeling at the level of language more easily approached than with lowerlevel 3D simulations.</p>
<p>Agent Modeling: Agent modeling has explored a variety of modeling paradigms, including reinforcement learning approaches (Osborne et al., 2021;Xu et al., 2021), combined with reading comprehension techniques (Narasimhan et al., 2015;Tamari et al., 2019;Guo et al., 2020;Yao et al., 2020Yao et al., , 2021, commonsense reasoning (Ryu et al., 2022;Murugesan et al., 2021b), graph-based networks (Ammanabrolu and Riedl, 2019), and neurosymbolic logic (Kimura et al., 2021b;Chaudhury et al., 2021;Kimura et al., 2021a). Most recent agents make use of large pretrained language models (e.g. Devlin et al., 2019), though these can pose challenges both in inference speed, as well as generalization to interactive environments. For example, a model that can correctly answer 90% of multiple choice elementary science exam questions fails to solve text games that test that same knowledge but in a step-by-step procedural setting, even with significant training .</p>
<p>Simulation Speed: A variety of simulators currently exist for text games, typically focusing on providing domain-general tooling for creating small procedurally generated research environments (e.g. , or interfacing to the existing body of large interactive fiction games such as Zork (Lebling et al., 1979) from the 1980s and 1990s by providing tooling and APIs . Nearly all frameworks ultimately generate and run games as Z-machine code (e.g. Nelson, 2014), an almost 40-year-old domain specific language designed for portability rather than simulation speed. One of the central challenges in building fast research tooling is valid action generation. Because games implement different sets of actions, and at different levels of granularity, nearly all contemporary agents require the simulator to supply a list of possible valid actions (such as put coat in closet) that could be undertaken by the agent at a given time step. Action spaces can be large -hundreds of thousands of action-object combinations are frequently possible at a given step in most games -and existing frameworks (e.g.  built on legacy tooling perform valid action generation by enumerating then running all possible action-object combinations at each game step then recording which ones are valid. This is extremely costly, substantially reducing simulation performance (as shown in Table 1). In this work, TEXTWORLDEXPRESS has been built from the ground-up using heavily optimized and profiled code to quickly render environments while simultaneously generating an exhaustive list of possible next valid actions for agents, greatly speeding simulation time.</p>
<p>Environments</p>
<p>TEXTWORLDEXPRESS offers high-speed versions of three popular benchmark environments frequently used in text game research, as well as a number of newer environments for evaluating specific reasoning competencies:</p>
<p>CookingWorld: The CookingWorld environment (Ct et al., 2018) tasks agents with preparing meals by following the instructions in a recipe that is provided in the environment. 6 Agents must first collect required food ingredients (e.g. milk, bell pepper, flour, salt) that can be found in the environment in canonical locations (e.g. kitchen, pantry, supermarket, garden) and containers (e.g. fridge, cupboard). Randomly generated recipes require agents to first use a knife to prepare food by slicing, dicing, or chopping a subset of ingredients, then additionally using an appropriate heating appliance to fry, roast, or barbeque the ingredients. If all ingredients are prepared according to the recipe, the agent can use an action to prepare the meal, and finally eat the meal to complete the task successfully. Task complexity can be controlled by varying the number of locations in the environment, the number of ingredients required for the recipe, and the number of distractor ingredients randomly placed 6 This task was used as part of the First TextWorld Problems competition (https://aka.ms/ftwp) and named by (Madotto et al., 2020).</p>
<p>CookingWorld Game</p>
<p>North of House You are in the kitchen. In one part of the room you see a fridge that is closed. There is also a dining chair, that has nothing on it. You also see a dishwasher that is closed. In another part of the room you see a trash can that is closed. In one part of the room you see an oven. There is also a cutlery drawer that is closed. You also see a stove. In another part of the room you see a counter that has a cookbook, a green apple, and a red hot pepper on it. In one part of the room you see a kitchen cupboard that is closed. To the South you see the backyard. To the West you see the pantry. &gt; read cookbook Gather all following ingredients and follow the directions to prepare this tasty meal. Ingredients: purple potato, yellow bell pepper. Directions: dice the purple potato, fry the purple potato, roast the yellow bell pepper, prepare meal.</p>
<blockquote>
<p>open cutlery drawer</p>
</blockquote>
<p>You open the cutlery drawer. The cutlery drawer contains a knife. &gt; take knife You take the knife.</p>
<blockquote>
<p>move south</p>
</blockquote>
<p>You are in the backyard. In one part of the room you see a barbeque. There is also a patio chair, that has nothing on it. You also see a clothes line, that has nothing on it. In another part of the room you see a garden that has a raw purple potato, and a yellow bell pepper on it. In one part of the room you see a workbench, that has nothing on it. There is also a patio table, that has nothing on it. To the North you see the kitchen.  the agent to read a recipe, collect ingredients, and follow preparation instructions. User or agent-entered actions for the 12-step solution are italicized. Relevant task objects are bolded, while distractor objects are underlined. Here, an easylevel parametric game was generated consisting of 3 locations (kitchen, backyard, pantry), 2 required ingredients (potato, bell pepper), and 2 distractor ingredients (apple, hot pepper).</p>
<p>in the environment that are not required for the recipe. The recipes and environments are parametrically generated, with subsets of ingredients and specific preparations held out between training, development, and test sets to prevent overfitting. An example CookingWorld task is shown in Table 2.</p>
<p>TextWorld Commonsense (TWC): Text game agents frequently learn the dynamics of environment -such as the need to open a door before one can move through it -from interacting with the environment itself, rather than using a pre-existing knowledge base of common sense facts or object affordances that would speed task learning. TextWorld Commonsense (Murugesan et al., 2021a) aims to evaluate agents on common sense knowledge that can not be directly learned from the environment by providing agents a clean-up task where the agent must place common household objects (e.g. a dirty dish) in their canonical locations (e.g. the dishwasher) that can be found in knowledge bases such as ConceptNet (Liu and Singh, 2004;Speer et al., 2017). Separate lists of objects are used in the training, development, and test sets, meaning the agent can not learn object locations from the training set alone, and must rely on an external common sense knowledge base to perform well on the development and test sets. Murugesan et al. (2021a) specify three task difficulty levels, with the easiest including a single location and object to put away, while the hard setting includes two location and up to 7 objects.</p>
<p>Coin Collector: Agents frequently find tasks such as object search, environment navigation, or pickand-place tasks challenging (Shridhar et al., 2020b). The Coin Collector game ) distills these into a single benchmark where an agent must explore a series of rooms to locate and pick up a single coin. In the original implementation, the game map typically takes the form of a connected loop or chain, such that continually moving to new locations means the agent will eventually discover the coin -while including medium and hard modes that add in one or more "dead-end" paths. To control for environment difficulty across games, the TEXTWORLDEXPRESS reimplementation uses the same map generator across environments, and generates arbitrary home environments rather than connected loops. The user maintains control of other measures of difficulty, including the total number of rooms, and the number of distractor objects placed in the environment.</p>
<p>Adding new games: New games can be added to TEXTWORLDEXPRESS, and 4 additional games that benchmark arithmetic, navigation, and neurosymbolic reasoning have been added since its initial release 7 . Adding new games takes about a day of coding, which can be more effortful than using the domain-specific implementation languages of existing game engines (e.g.   </p>
<p>Action Space and Valid Action Generation</p>
<p>The three benchmark games each have up to 15 different types of actions available to agents, described in Table 3. These include common textgame actions such as taking objects, moving locations, and opening doors, as well as domainspecific actions such as slicing or cooking ingredients for the cooking-domain game. Actions may take zero (e.g. look around), one (e.g. take shirt), or two (e.g. put shirt in closet) objects as arguments.</p>
<p>Most contemporary high-performance game agents (e.g. Murugesan et al., 2021a) make use of a "validaction handicap" -that is, at each step, they require a list of possible valid actions that can be taken in the environment, from which they select a single action to undertake. For example, a kitchen agent might wish to dice the carrot, but such an action would only be available to the agent if it currently possessed both a carrot and a knife in its inventory. This valid-action detection is typically implemented overtop of existing interactive fiction games (such as Zork) by an interface framework (e.g., Jericho;  at significant loss to the simulation framerate. In contrast, TEXTWORLDEXPRESS was designed from the ground-up to provide fast valid action generation to maintain high framerates. </p>
<p>Map Generation</p>
<p>Navigation tasks -such as exploring an environment, or navigating to a specific location -are typically challenging for contemporary text game agents. Because of this, games typically reduce the burden of navigation by providing simplified maps. At one end of the extreme, the original TextWorld Commonsense uses small maps containing only one or two locations, while at the other extreme Cook-ingWorld creates maps with over a dozen locations interconnected in common ways (i.e. a kitchen is usually connected to a pantry, backyard, and/or corridor, but is never directly connected to a supermarket). To control for the difficulty of the navigation task across environments, TEXTWORLDEXPRESS uses the same map generator across all three benchmark games, while allowing the user to specify parameters such as the number of map locations to control the difficulty of the navigation task.</p>
<p>Environments can consist of up to 11 locations, consisting of locations common to both the TWC and CookingWorld games. Maps are randomly generated at the start of each game, and allow navigation on four cardinal directions (north, south, east, west). Optionally, rooms may be connected with doors that an agent is required to open before allowing passage, increasing task complexity. Figure 1 shows an example map produced by the generator.</p>
<p>Object Library</p>
<p>Task objects, room objects, and distractor objects are populated from the object libraries provided by the TextWorld Commonsense and CookingWorld games. This results in approximately 500 possible objects that can populate environments, including containers (e.g. fridge, shelf, countertop), and movable objects (e.g. red onion, dirty shirt).</p>
<p>Parametric Variation</p>
<p>To reduce overfitting, generated tasks and environments vary in their requirements and presentation. Tasks typically vary in task-critical objects, such as the specific objects that need to be cleaned up in TextWorld Commonsense, or the recipe, ingredients, and their locations in CookingWorld. Environments parametrically vary, centrally in the environment map (how the rooms are interconnected), while also allowing different numbers of distractor objects to be generated in different randomized locations in the environment. Critically, games are deterministic and the generation is repeatable and controlled by a single random seed, such that the same game can be regenerated during agent training and evaluation. To create independent train, development, and test sets, in addition to each game having specific task objects that are unique across training and evaluation sets, we also assign blocks of random seeds to the train, development, and sets. This allows generating thousands of possible parametric variations for each set, while ensuring that the tasks and environments remain unique.</p>
<p>Scoring</p>
<p>At each time step, the simulator provides the agent a score that signifies the agent's progress in solving a given task. Games typically assign rewards for critical task steps, such as picking up correct ingredients, or preparing ingredients correctly. Because the total score required to complete a game can vary both across games and across task complexity, scores are provided both as raw counts, as well as normalized to between zero (no task progress) and one (task completion). Each game has specific success and failure criterion, which are automatically detected by the simulator, and provided to the agent by the API. For example, if a recipe requires a carrot to be chopped, but the agent instead slices it, this will cause a task failure, and can be used as a reward signal for the agent model to use in adjusting its action policy.</p>
<p>Speed Comparison</p>
<p>Online and Precrawled Generation</p>
<p>To enable extremely fast simulations, TEXTWORLDEXPRESS supports two game generation modes: normal (online) generation, and precrawled generation. In online generation, games are parametrically generated and played at runtime, allowing a large number of parametric game variations to be generated, and games to be played up to any number of steps. Conversely, where speed is of critical importance, the simulator supports precrawling all possible paths an agent might take in a given environment, and pre-caching these to disk as a JSON file. This allows extremely fast game playing -at essentially the speed of updating a pointer to a particular node in the precrawled state tree -at the expense of generating and loading large files, that pragmatically limit the total number of steps that can be crawled and precached in the environment. 8 Precrawling is a unique feature offered by TEXTWORLDEXPRESS, as games taking minutes to crawl in this framework can take days or weeks to crawl in TEXTWORLD.</p>
<p>Evaluating Simulation Speed</p>
<p>We empirically compare the simulation speed of TEXTWORLDEXPRESS with three frameworks.</p>
<p>TextWorld (Ct et al., 2018) is a framework for generating parametric text games for natural language processing research. Games are specified using predicate logic (to define action rules) and a context-free grammar (to generate text), which TextWorld reformulates into Inform7 code (Nelson, 2006), that is then ultimately compiled to a Z-Machine game (Nelson, 2014). The three benchmark games reimplemented in TEXTWORLDEX-PRESS were originally implemented in TextWorld. Jericho  provides a research interface to the existing body of interactive fiction games, such as Zork (Lebling et al., 1979), that were originally written for the Z-Machine interpreter. Critically, Jericho provides facilities for action template extraction and valid-action generation, to reduce the difficulty of interfacing classic interactive fiction games with language agents.</p>
<p>ScienceWorld ) is a sciencedomain text game simulator that provides the abil-ity to train and evaluate agents on scientific tasks normally learned by elementary science students, such as changes of states of matter (melting, boiling, freezing), life cycles of plants and animals, and basic chemistry. Supporting this is a series of complex simulation engines (e.g., thermodynamics, electrical conductivity, genetics) which increase simulation fidelity at the cost of speed. Similar to TextWorld and Jericho, ScienceWorld supports generating valid actions at each time step.</p>
<p>The results of this evaluation, using random agents to traverse the environments, are shown in Table 1. The highly optimized TEXTWORLDEX-PRESS is able to simulate games in online generation mode at an average of 212k frames per second per thread, or nearly three orders of magnitude faster than other frameworks. 9 This varies between 256k steps per second for the fastest environment with the least complex action space (Coin Collector), to 155k steps per second for the most complex action space (CookingWorld). On an 8-core workstation, this enables million-step experiments to be simulated per second, with billion-step experiments possible in approximately one hour. 10 In contrast, one billion steps would take approximately 38 days using the original TextWorld implementations. In precrawled mode, where game states are precached, single-thread speeds of up to 4 million steps per second are possible. Our fastest multi-threaded benchmark on desktop hardware (an AMD 3950X 16-core, 32-thread CPU) reaches 34M steps per second, enabling billion-step-scale simulations in approximately 30 seconds.</p>
<p>Conclusion</p>
<p>We present TEXTWORLDEXPRESS, a fast simulator for text-game research that reimplements three benchmark environments while running three orders of magnitude faster than their original implementations. New games can be added using existing games as templates, and four new games benchmarking specific reasoning competencies like arithmetic and navigation have been added since its initial release. The simulator supports common features (such as valid action detection), while providing new enabling features, such as quickly precrawling entire game state trees. This work is released as open source.</p>
<p>Broader Impacts</p>
<p>Embodied agents require a variety of commonsense reasoning skills and competencies about the world in order to successfully perform tasks. Text games distill task learning to a high level of abstraction, allowing conceptual-level procedural knowledge to be acquired without simultaneously learning challenging low-level perceptual or motor tasks as in 3D simulators (e.g. Shridhar et al., 2020a;Petrenko et al., 2021), while reducing the computational requirements to run experiments from expensive GPU servers to common desktop hardware. Futher, Shirdhar et al. (2020b) have empirically demonstrated that agents can be inexpensively pretrained on tasks in a text world environment, then transfer much of their performance to more realistic 3D environments, speeding training. TEXTWORLDEXPRESS, which increases the speed of text game experiments by three orders of magnitude, enables running experiments faster, at greater scale, or using alternate sample-heavy machine learning frameworks than currently available simulators.</p>
<p>Limitations</p>
<p>TEXTWORLDEXPRESS has two main limitations compared to existing simulators. TEXTWORLD-EXPRESS gains much of its speed by developing a highly-profiled simulator with hard-coded implementations of text games. Unlike the original TEXTWORLD simulator, which is designed to allow new environments to be implemented with a domain-specific language, adding new environments to TEXTWORLDEXPRESS is more effortful and requires coding in SCALA, a derivative of JAVA. Similarly, for speed, the TEXTWORLDEXPRESS user input parser is simplified, and it only recognizes valid actions as it presents them to the agent, without facilities for alternate surface forms, misspellings, or other variations. While it is common for agents to select actions from a valid action list, the lack of a diverse input parser limits utility for human participants who might choose to play these games.</p>
<blockquote>
<p>take purple potato You take the purple potato. &gt; take yellow bell pepper You take the yellow bell pepper. &gt; move north You are in the kitchen. &gt; dice purple potato You dice the purple potato. &gt; cook purple potato in stove You fry the purple potato with the stove. &gt; cook yellow bell pepper with the oven You roast the yellow bell pepper with the oven. &gt; prepare mealThe meal has been added to your inventory. &gt; eat meal Game completed.</p>
</blockquote>
<p>Table 2 :
2An example CookingWorld text game, requiring</p>
<p>). See full list at https://github.com/cognitiveailab/ TextWorldExpress#environments.7 Action 
Description </p>
<p>Generic actions </p>
<p>look around 
describe current location 
inventory 
list agent inventory 
examine OBJ 
examine an object 
move DIR 
move north, east, south, or west 
open OBJ 
open a door or container 
close OBJ 
close a door or container 
take OBJ 
pick up an object 
put OBJ in OBJ 
put an object in a container </p>
<p>Extended actions (CookingWorld) </p>
<p>read OBJ 
read a recipe book 
cook OBJ in OBJ cook an ingredient 
chop OBJ 
chop an ingredient 
slice OBJ 
slice an ingredient 
dice OBJ 
dice an ingredient 
eat OBJ 
eat an ingredient 
prepare meal 
prepare the meal </p>
<p>Table 3 :
3The action space of the environments, as well as descriptions of each action. Actions can take zero, one, or two object (OBJ) or direction (DIR) arguments.
Performance reported from(Zholus et al., 2022). 5 Benchmark scripts provided in the code repository.
As an example, a 1GB file can typically store precrawled game trees for a single game variation up to between 8 and 12 steps, depending on the complexity of the action space.
PYTHON performance is 10X slower than JAVA/SCALA performance due to the speed of PYTHON-JVM binders. 10 Using pre-crawled paths, we managed to run billion-game experiment on a 32-core server in about a day.
AcknowledgementsThis work supported in part by National Science Foundation (NSF) award #1815948 to PJ, and gift from the Allen Institute for Artificial Intelligence (AI2).
Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In International Con- ference on Learning Representations.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, 10.18653/v1/N19-1358Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Prithviraj Ammanabrolu and Mark Riedl. 2019. Play- ing text-adventure games with graph-based deep re- inforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>How to motivate your dragon: Teaching goaldriven agents to speak and act in fantasy worlds. Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktschel, Jason Weston, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesPrithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktschel, and Jason Weston. 2021. How to motivate your dragon: Teaching goal- driven agents to speak and act in fantasy worlds. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 807-833.</p>
<p>Neuro-symbolic approaches for text-based policy learning. Subhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, Asim Munawar, 10.18653/v1/2021.emnlp-main.245Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsSubhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, and Asim Munawar. 2021. Neuro-symbolic approaches for text-based policy learning. In Proceedings of the 2021 Confer- ence on Empirical Methods in Natural Language Pro- cessing, pages 3073-3078, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Babyai: First steps towards grounded language learning with a human in the loop. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Yoshua Thien Huu Nguyen, Bengio, International Conference on Learning Representations. 105Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019. Babyai: First steps towards grounded language learning with a hu- man in the loop. In International Conference on Learning Representations, volume 105.</p>
<p>Language as a cognitive tool to imagine goals in curiositydriven exploration. Cdric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Clment Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer, abs/2002.09253ArXiv. Cdric Colas, Tristan Karch, Nicolas Lair, Jean- Michel Dussoux, Clment Moulin-Frier, Peter Ford Dominey, and Pierre-Yves Oudeyer. 2020. Language as a cognitive tool to imagine goals in curiosity- driven exploration. ArXiv, abs/2002.09253.</p>
<p>Textworld: A learning environment for text-based games. kos Marc-Alexandre Ct, Xingdi Kdr, Ben A Yuan, Tavian Kybartas, Emery Barnes, James Fine, Matthew J Moore, Layla El Hausknecht, Mahmoud Asri, Wendy Adada, Adam Tay, Trischler, CGW@IJCAI. Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mah- moud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. In CGW@IJCAI.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, 10.18653/v1/2020.emnlp-main.624Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsXiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Mur- ray Campbell, and Shiyu Chang. 2020. Interactive fiction game playing as multi-paragraph reading com- prehension with reinforcement learning. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765, Online. Association for Computational Linguistics.</p>
<p>Minerl: a large-scale dataset of minecraft demonstrations. H William, Brandon Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. 2019. Minerl: a large-scale dataset of minecraft demonstrations. In Proceedings of the 28th International Joint Conference on Artifi- cial Intelligence, pages 2442-2448.</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, Xingdi Yuan, AAAI. Matthew J. Hausknecht, Prithviraj Ammanabrolu, Marc- Alexandre Ct, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In AAAI.</p>
<p>A systematic survey of text worlds as embodied natural language environments. A Peter, Jansen, WordPlay Workshop: When Language Meets Games. Peter A Jansen. 2022. A systematic survey of text worlds as embodied natural language environments. In WordPlay Workshop: When Language Meets Games.</p>
<p>Tristan Karch, Nicolas Lair, Cdric Colas, Jean-Michel Dussoux, Peter Ford Dominey, and Pierre-Yves Oudeyer. 2020. Language-goal imagination to foster creative exploration in deep rl. Tristan Karch, Nicolas Lair, Cdric Colas, Jean- Michel Dussoux, Clment Moulin-Frier, Pe- ter Ford Dominey, and Pierre-Yves Oudeyer. 2020. Language-goal imagination to foster creative explo- ration in deep rl.</p>
<p>LOA: Logical optimal actions for textbased interaction games. Daiki Kimura, Subhajit Chaudhury, Masaki Ono, Michiaki Tatsubori, Don Joven Agravante, Asim Munawar, Akifumi Wachi, Ryosuke Kohita, Alexander Gray, 10.18653/v1/2021.acl-demo.27Proceedings of the 59th. the 59thDaiki Kimura, Subhajit Chaudhury, Masaki Ono, Michi- aki Tatsubori, Don Joven Agravante, Asim Munawar, Akifumi Wachi, Ryosuke Kohita, and Alexander Gray. 2021a. LOA: Logical optimal actions for text- based interaction games. In Proceedings of the 59th</p>
<p>Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. Online. Association for Computational LinguisticsAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Con- ference on Natural Language Processing: System Demonstrations, pages 227-231, Online. Association for Computational Linguistics.</p>
<p>Neuro-symbolic reinforcement learning with first-order logic. Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agravante, Michiaki Tatsubori, Asim Munawar, Alexander G Gray, EMNLP. Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agra- vante, Michiaki Tatsubori, Asim Munawar, and Alexander G. Gray. 2021b. Neuro-symbolic rein- forcement learning with first-order logic. In EMNLP.</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474arXiv preprintEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van- derBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474.</p>
<p>Edward Grefenstette, and Tim Rocktschel. 2020. The nethack learning environment. Heinrich Kttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Advances in Neural Information Processing Systems. 33Heinrich Kttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefen- stette, and Tim Rocktschel. 2020. The nethack learn- ing environment. Advances in Neural Information Processing Systems, 33:7671-7684.</p>
<p>Zork: a computerized fantasy simulation game. David Lebling, S Marc, Timothy A Blank, Anderson, Computer. 1204P David Lebling, Marc S Blank, and Timothy A Ander- son. 1979. Zork: a computerized fantasy simulation game. Computer, 12(04):51-59.</p>
<p>Conceptnet-a practical commonsense reasoning tool-kit. Hugo Liu, Push Singh, BT technology journal. 224Hugo Liu and Push Singh. 2004. Conceptnet-a practi- cal commonsense reasoning tool-kit. BT technology journal, 22(4):211-226.</p>
<p>Exploration based language learning for text-based games. Andrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros Papangelis, Dian Yu, Chandra Khatri, Gokhan Tur, 10.24963/ijcai.2020/207Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20International Joint Conferences on Artificial Intelligence Organization. Main trackAndrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros Papangelis, Dian Yu, Chandra Khatri, and Gokhan Tur. 2020. Exploration based language learning for text-based games. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 1488-1494. International Joint Conferences on Artificial Intelli- gence Organization. Main track.</p>
<p>Improving intrinsic exploration with language abstractions. Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah D Goodman, Tim Rocktaschel, Edward Grefenstette, abs/2202.08938ArXiv. Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah D. Goodman, Tim Rocktaschel, and Edward Grefenstette. 2022. Improving intrinsic exploration with language abstractions. ArXiv, abs/2202.08938.</p>
<p>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, AAAI. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapani- pathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. 2021a. Text-based rl agents with commonsense knowledge: New challenges, en- vironments and baselines. In AAAI.</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, 10.18653/v1/2021.acl-short.91Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnlineAssociation for Computational Linguistics2Keerthiram Murugesan, Mattia Atzeni, Pavan Kapani- pathi, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. 2021b. Efficient text-based rein- forcement learning by jointly leveraging state and commonsense graph representations. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 719-725, Online. Association for Computational Linguistics.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Lin- guistics.</p>
<p>Natural language, semantic analysis, and interactive fiction. Graham Nelson, IF Theory Reader. 141Graham Nelson. 2006. Natural language, semantic analysis, and interactive fiction. IF Theory Reader, 141:99-104.</p>
<p>The z-machine standards document version 1.1. Graham Nelson, Graham Nelson. 2014. The z-machine standards docu- ment version 1.1.</p>
<p>A survey of text games for reinforcement learning informed by natural language. Philip Osborne, Heido Nomm, Andr Freitas, abs/2109.09478ArXiv. Philip Osborne, Heido Nomm, and Andr Freitas. 2021. A survey of text games for reinforcement learning in- formed by natural language. ArXiv, abs/2109.09478.</p>
<p>Megaverse: Simulating embodied agents at one million experiences per second. Aleksei Petrenko, Erik Wijmans, Brennan Shacklett, Vladlen Koltun, PMLRInternational Conference on Machine Learning. Aleksei Petrenko, Erik Wijmans, Brennan Shacklett, and Vladlen Koltun. 2021. Megaverse: Simulating embodied agents at one million experiences per sec- ond. In International Conference on Machine Learn- ing, pages 8556-8566. PMLR.</p>
<p>Shirui Pan, and Reza Haf. 2022. Fire burns, sword cuts: Commonsense inductive bias for exploration in text-based games. Dongwon Ryu, Ehsan Shareghi, Meng Fang, Yunqiu Xu, 10.18653/v1/2022.acl-short.56Proceedings of the 60th. the 60thDongwon Ryu, Ehsan Shareghi, Meng Fang, Yunqiu Xu, Shirui Pan, and Reza Haf. 2022. Fire burns, sword cuts: Commonsense inductive bias for exploration in text-based games. In Proceedings of the 60th</p>
<p>Annual Meeting of the Association for Computational Linguistics. Dublin, IrelandAssociation for Computational Linguistics2Short Papers)Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 515- 522, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020a. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, International Conference on Learning Representations. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020b. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representa- tions.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of gen- eral knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Semantic exploration from language abstractions and pretrained representations. Allison C Tam, Neil C Rabinowitz, Andrew Kyle Lampinen, Nicholas A Roy, Stephanie C Y Chan, Jane X Strouse, Andrea Wang, Felix Banino, Hill, abs/2204.05080ArXiv. Allison C. Tam, Neil C. Rabinowitz, Andrew Kyle Lampinen, Nicholas A. Roy, Stephanie C. Y. Chan, DJ Strouse, Jane X. Wang, Andrea Banino, and Fe- lix Hill. 2022. Semantic exploration from language abstractions and pretrained representations. ArXiv, abs/2204.05080.</p>
<p>Playing by the book: An interactive game approach for action graph extraction from text. Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, Yuji Matsumoto, 10.18653/v1/W19-2609Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications. the Workshop on Extracting Structured Knowledge from Scientific PublicationsMinneapolis, MinnesotaAssociation for Computational LinguisticsRonen Tamari, Hiroyuki Shindo, Dafna Shahaf, and Yuji Matsumoto. 2019. Playing by the book: An interac- tive game approach for action graph extraction from text. In Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62-71, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Scienceworld: Is your agent smarter than a 5th grader? ArXiv. Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Ct, Prithviraj Ammanabrolu, abs/2203.07540Ruoyao Wang, Peter Alexander Jansen, Marc- Alexandre Ct, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than a 5th grader? ArXiv, abs/2203.07540.</p>
<p>Generalization in text-based games via hierarchical reinforcement learning. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang, 10.18653/v1/2021.findings-emnlp.116Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsYunqiu Xu, Meng Fang, Ling Chen, Yali Du, and Chengqi Zhang. 2021. Generalization in text-based games via hierarchical reinforcement learning. In Findings of the Association for Computational Lin- guistics: EMNLP 2021, pages 1343-1353, Punta Cana, Dominican Republic. Association for Compu- tational Linguistics.</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht, 10.18653/v1/2021.naacl-main.247Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesShunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. 2021. Reading and acting while blind- folded: The need for semantics in text game agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 3097-3102, Online. Association for Computa- tional Linguistics.</p>
<p>Keep CALM and explore: Language models for action generation in textbased games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and ex- plore: Language models for action generation in text- based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 8736-8754, Online. Association for Computational Linguistics.</p>
<p>Counting to explore and generalize in text-based games. Xingdi Yuan, Marc-Alexandre Ct, Alessandro Sordoni, Romain Laroche, abs/1806.11525Matthew J. Hausknecht, and Adam Trischler. ArXivXingdi Yuan, Marc-Alexandre Ct, Alessandro Sor- doni, Romain Laroche, Rmi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. ArXiv, abs/1806.11525.</p>
<p>Marc-Alexandre Ct, and Aleksandr I. Panov. 2022. IGLU gridworld: Simple and fast environment for embodied dialog agents. Artem Zholus, Alexey Skrynnik, Shrestha Mohanty, Zoya Volovikova, Julia Kiseleva, Arthur Szlam, 10.48550/arXiv.2206.00142abs/2206.00142CoRRArtem Zholus, Alexey Skrynnik, Shrestha Mohanty, Zoya Volovikova, Julia Kiseleva, Arthur Szlam, Marc-Alexandre Ct, and Aleksandr I. Panov. 2022. IGLU gridworld: Simple and fast environment for embodied dialog agents. CoRR, abs/2206.00142.</p>            </div>
        </div>

    </div>
</body>
</html>